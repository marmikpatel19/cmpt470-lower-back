[{"issue_number":1,"repository":"tensorflow\/tensorflow","title":"`gradient_checker.compute_gradient` can cause a crash","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20241025\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `Floating point exception` issue in TensorFlow when I used API `gradient_checker.compute_gradient`. I have confirmed that below code would crash on tf-nightly 2.19.0-dev20241025 (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1Ow6DQI7g-s7LpKUM1fy8OXbNcfw1ZH7r?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport math\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import nn_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.python.util.compat import collections_abc\nfrom tensorflow.python.eager import context\ndef DtypesToTest(use_gpu):\n  # double datatype is currently not supported for convolution ops\n  # on the ROCm platform\n  optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]\n  if use_gpu:\n    if not test_util.GpuSupportsHalfMatMulAndConv():\n      return optional_float64 + [dtypes.float32]\n    else:\n      # It is important that float32 comes before float16 here,\n      # as we will be using its gradients as reference for fp16 gradients.\n      return optional_float64 + [dtypes.float32, dtypes.float16]\n  else:\n    return optional_float64 + [dtypes.float32, dtypes.float16, dtypes.bfloat16]\ndef _ConstructAndTestGradientForConfig(\n    batch, input_shape, filter_shape, in_depth, out_depth, stride,\n    padding, test_input, data_format, use_gpu):\n  input_planes, input_rows, input_cols = input_shape\n  filter_planes, filter_rows, filter_cols = filter_shape\n  input_shape = [batch, input_planes, input_rows, input_cols, in_depth]\n  filter_shape = [\n      filter_planes, filter_rows, filter_cols, in_depth, out_depth\n  ]\n  if isinstance(stride, collections_abc.Iterable):\n    strides = [1] + list(stride) + [1]\n  else:\n    strides = [1, stride, stride, stride, 1]\n  if padding == \"VALID\":\n    output_planes = int(\n        math.ceil((input_planes - filter_planes + 1.0) \/ strides[1]))\n    output_rows = int(\n        math.ceil((input_rows - filter_rows + 1.0) \/ strides[2]))\n    output_cols = int(\n        math.ceil((input_cols - filter_cols + 1.0) \/ strides[3]))\n  else:\n    output_planes = int(math.ceil(float(input_planes) \/ strides[1]))\n    output_rows = int(math.ceil(float(input_rows) \/ strides[2]))\n    output_cols = int(math.ceil(float(input_cols) \/ strides[3]))\n  output_shape = [batch, output_planes, output_rows, output_cols, out_depth]\n  input_size = 1\n  for x in input_shape:\n    input_size *= x\n  filter_size = 1\n  for x in filter_shape:\n    filter_size *= x\n  input_data = [x * 1.0 \/ input_size for x in range(0, input_size)]\n  filter_data = [x * 1.0 \/ filter_size for x in range(0, filter_size)]\n  for data_type in DtypesToTest(use_gpu=use_gpu):\n    # TODO(mjanusz): Modify gradient_checker to also provide max relative\n    # error and synchronize the tolerance levels between the tests for forward\n    # and backward computations.\n    if data_type == dtypes.float64:\n      tolerance = 1e-8\n    elif data_type == dtypes.float32:\n      tolerance = 5e-3\n    elif data_type == dtypes.float16:\n      tolerance = 5e-3 if test.is_built_with_rocm() else 1e-3\n    elif data_type == dtypes.bfloat16:\n      tolerance = 1e-2\n    sess = tf.compat.v1.Session()\n    with sess.as_default():\n      orig_input_tensor = constant_op.constant(\n          input_data, shape=input_shape, dtype=data_type, name=\"input\")\n      filter_tensor = constant_op.constant(\n          filter_data, shape=filter_shape, dtype=data_type, name=\"filter\")\n      if data_format == \"NCDHW\":\n        input_tensor = test_util.NHWCToNCHW(orig_input_tensor)\n        new_strides = test_util.NHWCToNCHW(strides)\n      else:\n        input_tensor = orig_input_tensor\n        new_strides = strides\n      conv = nn_ops.conv3d(\n          input_tensor,\n          filter_tensor,\n          new_strides,\n          padding,\n          data_format=data_format,\n          name=\"conv\")\n      jacob_t, jacob_n = gradient_checker.compute_gradient(\n          orig_input_tensor, input_shape, conv, output_shape)\n\nwith context.graph_mode():\n  _ConstructAndTestGradientForConfig(data_format=\"NDHWC\",use_gpu=False,batch=2, input_shape=(3, 7, 6), filter_shape=(3, 3, 3), in_depth=2, out_depth=0, stride=3, padding='VALID', test_input=True)\n```\n\n### Relevant log output\n\n```shell\nFatal Python error: Floating point exception\n```","labels":["type:bug","type:support"],"created_at":"2025-02-11T16:38:56Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/87063"},{"issue_number":2,"repository":"tensorflow\/tensorflow","title":"TPU nan issue","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux (google colab default)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIn the 2.18.0 Tensorflow version, occur only when using TPU:\n- At the middle of any epoch, loss turns out to be nan for the rest of the epoch \n\n### Standalone code to reproduce the issue\n\n```shell\nShortest way: connect and connect to the colab tutorial on TPU, i.e. https:\/\/colab.research.google.com\/github\/tensorflow\/docs\/blob\/master\/site\/en\/guide\/tpu.ipynb#scrollTo=Tce3stUlHN0L\nIf the issue hasn't been fixed, the training loop will produce nan loss\n```\n\n### Relevant log output\n\n```shell\nEpoch 1\/5\n300\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 22s 47ms\/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan\nEpoch 2\/5\n300\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 30ms\/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan\nEpoch 3\/5\n300\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 30ms\/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan\nEpoch 4\/5\n231\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 23ms\/step - loss: nan - sparse_categorical_accuracy: nan\n```","labels":["type:bug","comp:tpus","TF 2.18"],"created_at":"2025-02-10T15:03:57Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86953"},{"issue_number":3,"repository":"tensorflow\/tensorflow","title":"`tf.summary_ops.write` aborts with \"Check failed: 1 == NumElements() (1 vs. 4)\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20250207\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.write` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/13pz-hZNK_BaOnbYo_9CT2YMdoncPXEA3?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops import summary_ops_v2 as summary_ops\nfrom tensorflow.python.ops import variables\nwriter = summary_ops.create_file_writer_v2(\"\/tmp\")\nmystep = variables.Variable(1, dtype=dtypes.int64)\nwith writer.as_default(step=[3, 0, 0, 2]):\n    summary_ops.write('tag', 1.0)\n```\n\n### Relevant log output\n\n```shell\n2025-02-09 04:47:35.482562: F tensorflow\/core\/framework\/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor\nAborted (core dumped)\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-09T04:49:39Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86918"},{"issue_number":4,"repository":"tensorflow\/tensorflow","title":"`tf.summary_ops.run_metadata_graphs` aborts with \"Check failed: 1 == NumElements() (1 vs. 4)\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20250207\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.run_metadata_graphs` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1qCZb2sPj2l79IecGA1tz9CywtNlevAjf?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.ops import summary_ops_v2 as summary_ops\nfrom tensorflow.core.protobuf import config_pb2\nwriter = summary_ops.create_file_writer_v2(\"\/tmp\")\nmeta = config_pb2.RunMetadata()\nwith writer.as_default([3, 0, 0, 2]):\n    summary_ops.run_metadata_graphs(name='my_name', data=meta)\n```\n\n### Relevant log output\n\n```shell\n2025-02-09 04:39:36.666278: F tensorflow\/core\/framework\/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor\nAborted (core dumped)\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-09T04:42:49Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86917"},{"issue_number":5,"repository":"tensorflow\/tensorflow","title":"`io_ops.restore_v2` aborts with \"Check failed: size >= 0 (0 vs. -3) \"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.19.0-dev20250207\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `aborted issue` in TensorFlow when I used API `tf.io_ops.restore_v2` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20250207` (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/17XwNF4WI3HqVJkLwk2VQjcTDAV42-6Bl?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import io_ops\n\ndtype = dtypes.uint4\nwith ops.Graph().as_default():\n    op = io_ops.restore_v2('model', ['var1', 'var2'], ['', '-3 4 0,1:-'], [dtype, dtype])\n```\n\n### Relevant log output\n\n```shell\n2025-02-09 04:25:19.857968: F tensorflow\/core\/framework\/tensor_shape.cc:413] Check failed: size >= 0 (0 vs. -3) \nAborted (core dumped)\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-09T04:34:07Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86916"},{"issue_number":6,"repository":"tensorflow\/tensorflow","title":"Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMacos 15.3 (worker 0) Macos 12.7.6(worker 1)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.20\n\n### Bazel version\n\n...\n\n### GCC\/compiler version\n\n16.0.0 (apple M3) 14.0.0 (intel iris)\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nApple M3 and Intel Iris Graphics 6100\n\n### Current behavior?\n\nWhen I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  #Build the model under the strategy\n\nNo error message is displayed, but the process no longer progresses after\n\n\u2022\t I followed the recommendations of the official documentation, but the problem persists.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport json\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nTF_CONFIG = {\n    \"cluster\": {\n        \"worker\": [\"192.168.0.68:12345\", \"192.168.0.68:12346\"]\n    },\n    \"task\": {\"type\": \"worker\", \"index\": 0}  # Modifier index pour chaque worker\n}\n\nos.environ[\"TF_CONFIG\"] = json.dumps(TF_CONFIG)\n\n# Manually Load the MNIST dataset\ndata = np.load(\"mnist.npz\")\nx_train, y_train = data[\"x_train\"], data[\"y_train\"]\nx_test, y_test = data[\"x_test\"], data[\"y_test\"]\n\n# Normalize images\nx_train, x_test = x_train \/ 255.0, x_test \/ 255.0\n\n# Add a dimension to match TensorFlow's expectations\nx_train = x_train[..., np.newaxis]\nx_test = x_test[..., np.newaxis]\n\n# Define the distribution strategy\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\n\n# Build the model under the strategy\nwith strategy.scope():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n        tf.keras.layers.MaxPooling2D((2, 2)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f\"Test accuracy: {test_acc:.4f}\")\n```\n\n### Relevant log output\n\n```shell\n2025-02-08 12:29:20.630247: I metal_plugin\/src\/device\/metal_device.cc:1154] Metal device set to: Apple M3\n2025-02-08 12:29:20.630286: I metal_plugin\/src\/device\/metal_device.cc:296] systemMemory: 16.00 GB\n2025-02-08 12:29:20.630293: I metal_plugin\/src\/device\/metal_device.cc:313] maxCacheSize: 5.33 GB\n2025-02-08 12:29:20.630324: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2025-02-08 12:29:20.630338: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:269] Created TensorFlow device (\/job:localhost\/replica:0\/task:0\/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n2025-02-08 12:29:20.631249: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2025-02-08 12:29:20.631259: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:269] Created TensorFlow device (\/job:worker\/replica:0\/task:0\/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n2025-02-08 12:29:20.632347: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:449] Started server with target: grpc:\/\/192.168.0.68:12345\n2025-02-08 12:29:20.637550: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:535] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 14287335759644278642\n2025-02-08 12:29:20.637654: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service_agent.cc:298] Coordination agent has successfully connected.\n2025-02-08 12:29:37.728182: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:535] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 15227140312468372989\n```","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.13"],"created_at":"2025-02-08T11:40:33Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86897"},{"issue_number":7,"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.Rsqrt``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 (google colab)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\nresult of ``tf.raw_ops.Rsqrt``` is inconsistent between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nx_0 = tf.constant([[[1.2031, 0.2168], [0.1177, 0.6758]], \n                    [[1.8203, 0.9688], [0.8828, 0.0767]]], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.Rsqrt(x=x_0)\n  print(\"Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.Rsqrt(x=x_0)\n  print(\"Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nOutput on CPU: tf.Tensor(\n[[[0.910156 2.14062]\n  [2.92188 1.21875]]\n\n [[0.742188 1.01562]\n  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)\n\nOutput on GPU: tf.Tensor(\n[[[0.914062 2.15625]\n  [2.90625 1.21875]]\n\n [[0.738281 1.01562]\n  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-05T06:12:29Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86607"},{"issue_number":8,"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.Tan``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\n```tf.raw_ops.Tan``` produces inconsistent results between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nreal = tf.constant([1.5634333], dtype=tf.float32)\nimag = tf.constant([0.020735], dtype=tf.float32)\n\ncomplex_tensor = tf.complex(real, imag)\n\nwith tf.device('\/CPU:0'):\n    result_cpu = tf.raw_ops.Tan(x=complex_tensor)\n    print(result_cpu)\n\nwith tf.device('\/GPU:0'):\n    result_gpu = tf.raw_ops.Tan(x=complex_tensor)\n    print(result_gpu)\n\n##Comparing whole complex numbers\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_cons = tf.experimental.numpy.allclose(result_cpu, result_gpu, rtol=1e-6,  atol=1e-5)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:\", is_cons.numpy())\n\n##Comparing by parts\nreal_part_cpu = tf.math.real(result_cpu)\nreal_part_gpu = tf.math.real(result_gpu)\nreal_part_diff = tf.reduce_max(tf.abs(real_part_cpu - real_part_gpu)).numpy()\nreal_part_cons = tf.experimental.numpy.allclose(real_part_cpu, real_part_gpu, rtol=1e-6,  atol=1e-5)\n\nimag_part_cpu = tf.math.imag(result_cpu)\nimag_part_gpu = tf.math.imag(result_gpu)\nimag_part_diff = tf.reduce_max(tf.abs(imag_part_cpu - imag_part_gpu)).numpy()\nimag_part_cons = tf.experimental.numpy.allclose(imag_part_cpu, imag_part_gpu, rtol=1e-6,  atol=1e-5)\n\nprint(\"Real parts absolute difference:\", real_part_diff)\nprint(\"Real parts Consistency check with atol=1e-5 and rtol=1e-6:\", real_part_cons.numpy())\n\nprint(\"Imag parts absolute difference:\", imag_part_diff)\nprint(\"Imag parts Consistency check with atol=1e-5 and rtol=1e-6:\", imag_part_cons.numpy())\n```\n\n### Relevant log output\n\n```shell\ntf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64)\ntf.Tensor([15.205605+42.834225j], shape=(1,), dtype=complex64)\n\nMax absolute difference: 8.5064334e-05\nConsistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False\n\nReal parts absolute difference: 2.861023e-05\nReal parts Consistency check with atol=1e-5 and rtol=1e-6: False\n\nImag parts absolute difference: 8.010864e-05\nImag parts Consistency check with atol=1e-5 and rtol=1e-6: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-04T03:18:15Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86506"},{"issue_number":9,"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.LogSoftmax``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 (google colab)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\nresult of ``tf.raw_ops.LogSoftmax``` is inconsistent between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nlogits = tf.constant([[0.0664, -2.3906]], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.LogSoftmax(logits=logits)\n  print(\"Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.LogSoftmax(logits=logits)\n  print(\"Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nOutput on CPU: tf.Tensor([[-0.0825195 -2.53125]], shape=(1, 2), dtype=bfloat16)\n\nOutput on GPU: tf.Tensor([[-0.0825195 -2.54688]], shape=(1, 2), dtype=bfloat16)\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-03T03:30:58Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86434"},{"issue_number":10,"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.L2Loss``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 (google colab)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\nresult of ```tw.raw_ops.L2Loss``` is inconsistent between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nt = tf.constant([\n    [[0.9922, -1.4922], \n     [0.0376,  0.1504], \n     [0.6172,  1.2266]],\n\n    [[-0.1387,  1.3047], \n     [0.3535, -0.0471], \n     [0.0437,  0.2637]]\n], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.L2Loss(t=t)\n  print(\"Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.L2Loss(t=t)\n  print(\"Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nOutput on CPU: tf.Tensor(3.51562, shape=(), dtype=bfloat16)\n\nOutput on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16)\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-02T07:05:17Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86406"},{"issue_number":11,"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.BiasAddGrad``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\n```tf.raw_ops.BiasAddGrad``` produces inconsistent results between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nout_backprop = tf.constant([\n    [\n        [\n            [[ 0.2207,  2.1094], [-0.3730, -1.0625], [ 1.7031,  0.7148]], \n            [[ 1.5078, -0.6719], [-0.6367,  0.5039], [-2.3281,  0.5078]]\n        ],\n        [\n            [[-0.3574,  0.0461], [ 2.3750, -2.9688], [-0.5703, -2.0156]],\n            [[ 0.8125,  1.7656], [-0.9570,  0.6250], [-0.6914, -0.4746]]\n        ],\n        [\n            [[-0.3750, -0.7383], [ 0.3691,  0.4570], [ 1.1641,  0.2715]],\n            [[-1.2969, -0.9844], [-0.4863,  1.0938], [-1.4297,  0.8086]]\n        ]\n    ],\n    [\n        [\n            [[ 0.3730,  0.8477], [-0.3887,  1.2266], [ 0.0859, -0.5742]],\n            [[-0.7383, -0.2432], [-0.7578, -0.8281], [-0.1660, -0.9336]]\n        ],\n        [\n            [[ 1.4297,  0.6797], [-1.6172,  0.4941], [-0.3047, -0.3711]],\n            [[-0.6250, -0.7617], [ 0.9453,  0.1064], [ 1.4062, -2.9531]]\n        ],\n        [\n            [[-1.4297, -0.1387], [ 0.0625,  1.0469], [-0.1953,  1.6406]],\n            [[-0.3047,  0.5117], [ 1.8125,  1.1797], [-0.8789, -0.4688]]\n        ]\n    ]\n], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=\"NCHW\")\n  print(\"BiasAddGrad Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=\"NCHW\")\n  print(\"BiasAddGrad Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nBiasAddGrad Output on CPU: tf.Tensor([0.09375 -3.96875 1.70312], shape=(3,), dtype=bfloat16)\n\nBiasAddGrad Output on GPU: tf.Tensor([0.078125 -4 1.70312], shape=(3,), dtype=bfloat16)\n\nMax absolute difference: 0.03125\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-01T10:43:06Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86378"},{"issue_number":12,"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.BatchMatMulV2``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\ngetting inconsistent results of ```tf.raw_ops.BatchMatMulV2``` between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nx_0 = tf.constant([\n    [[[[ -1.3594, -0.3027], [-1.4141,  0.2969]],\n      [[ -0.9141,  1.7812], [ 1.2266,  0.8594]]],\n\n     [[[  0.8359, -0.9414], [-1.7969, -0.7461]],\n      [[  0.3164,  0.3691], [ 0.7656,  0.2354]]]],\n\n    [[[[ -0.5898,  1.3516], [ 0.4902, -0.1045]],\n      [[ -0.1099,  1.5078], [ 0.2852, -0.0957]]],\n\n     [[[-0.9883,  1.3203], [-0.2715, -1.7578]],\n      [[ -0.1602, -0.4336], [-0.6875, -0.4492]]]]\n], dtype=tf.bfloat16)\n\ny = tf.constant([\n    [[[  0.6836, -0.6562], [-0.5508, -0.8438]], \n     [[  1.6094, -0.9883], [-0.1318,  1.1094]]],\n\n    [[[  0.4062, -1.1094], [-0.7188, -1.7578]], \n     [[ -1.0391, -0.6602], [ 0.8359, -0.6562]]]\n], dtype=tf.bfloat16) \n\nwith tf.device('CPU:0'):\n    result_cpu = tf.raw_ops.BatchMatMulV2(\n        x=x_0, \n        y=y,\n    )\n    print(result_cpu)\n\nwith tf.device('GPU:0'):\n    result_gpu = tf.raw_ops.BatchMatMulV2(\n        x=x_0, \n        y=y,\n    )\n    print(result_gpu)\n\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\ntf.Tensor(\n[[[[[-0.761719 1.14062]\n    [-1.125 0.675781]]\n\n   [[-1.70312 2.875]\n    [1.85938 -0.257812]]]\n\n\n  [[[1.01562 0.726562]\n    [-0.193359 3.29688]]\n\n   [[-0.0201416 -0.449219]\n    [-0.597656 -0.65625]]]]\n\n\n\n [[[[-1.14062 -0.75]\n    [0.392578 -0.233398]]\n\n   [[-0.375 1.78125]\n    [0.470703 -0.386719]]]\n\n\n  [[[-1.34375 -1.21875]\n    [1.14844 3.39062]]\n\n   [[-0.195312 0.388672]\n    [0.337891 0.746094]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)\n\ntf.Tensor(\n[[[[[-0.761719 1.14844]\n    [-1.13281 0.675781]]\n\n   [[-1.70312 2.875]\n    [1.85938 -0.259766]]]\n\n\n  [[[1.01562 0.726562]\n    [-0.193359 3.3125]]\n\n   [[-0.0201416 -0.451172]\n    [-0.597656 -0.660156]]]]\n\n\n\n [[[[-1.14844 -0.753906]\n    [0.392578 -0.233398]]\n\n   [[-0.375 1.78125]\n    [0.470703 -0.388672]]]\n\n\n  [[[-1.35156 -1.22656]\n    [1.15625 3.39062]]\n\n   [[-0.196289 0.390625]\n    [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)\n\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-01T07:13:36Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86350"},{"issue_number":13,"repository":"tensorflow\/tensorflow","title":"Stateful LSTM bug with batch size","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nWindows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid. \nBasically the error is the same as this https:\/\/github.com\/tensorflow\/tensorflow\/issues\/64061\n\nBelow is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\n\n# Set a fixed batch size\nbatch_size = 32\n\n# Create some random training data\n# We'll have sequences of length 5, with 1 feature per time step\nsequence_length = 5\nnum_features = 1\nnum_samples = 100  # Total number of samples (must be divisible by batch_size)\n\n# Ensure num_samples is a multiple of batch_size\nnum_samples = (num_samples \/\/ batch_size) * batch_size\n\nX_train = np.random.rand(num_samples, sequence_length, num_features)\ny_train = np.random.rand(num_samples, 1)  # Example target values\n\n# Reshape y_train to match expected output shape if needed\ny_train = y_train.reshape(-1,1)\n\n# Create the stateful LSTM model\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.LSTM(units=64,  # Number of LSTM units\n                               batch_input_shape=(batch_size, sequence_length, num_features),\n                               stateful=True,\n                               return_sequences=False)) #often false for a final prediction\n\nmodel.add(tf.keras.layers.Dense(units=1)) # Output layer with 1 unit\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')\n\n# Train the model\nepochs = 10\n\nfor epoch in range(epochs):\n    # Shuffle data indices for each epoch (important for stateful LSTMs)\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    X_train = X_train[indices]\n    y_train = y_train[indices]\n\n    model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False) # Shuffle must be false\n\n    # Reset states after each epoch (essential for stateful LSTMs)\n    model.reset_states()\n```\n\n### Relevant log output\n\n```shell\n\n```","labels":["stat:awaiting response","type:bug","comp:keras","TF 2.18"],"created_at":"2025-01-31T18:07:07Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86310"},{"issue_number":14,"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\ngetting inconsistent results of ```tf.image.adjust_hue``` between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nimages = tf.constant([\n    [[ 1.9720840,  2.1302242, -0.1902120],\n     [ 0.6557856, -1.3016001,  1.1452782]],\n    \n    [[-2.2193234,  0.3198028,  0.9568117],\n     [-0.3937407, -0.0503466, -0.3693791]]\n], dtype=tf.float32)\n\ndelta = tf.constant(-0.7441734, dtype=tf.float32)\n\nwith tf.device('CPU:0'):\n    adjusted_cpu = tf.image.adjust_hue(images, delta)\n    print(\"Adjusted Hue on CPU:\\n\", adjusted_cpu)\n\nwith tf.device('GPU:0'):\n    adjusted_gpu = tf.image.adjust_hue(images, delta)\n    print(\"Adjusted Hue on GPU:\\n\", adjusted_gpu)\n\n\nis_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e-5, rtol=1e-6)\n\nmax_abs_diff = tf.reduce_max(tf.abs(adjusted_cpu - adjusted_gpu)).numpy()\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nAdjusted Hue on CPU:\n tf.Tensor(\n[[[-0.190212    2.1302242   1.2092681 ]\n  [ 1.1452782  -0.48211157 -1.3016001 ]]\n\n [[ 0.11679006 -2.2193234   0.9568117 ]\n  [-0.3937407  -0.25841027 -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)\n\nAdjusted Hue on GPU:\n tf.Tensor(\n[[[-0.19021209  2.1302242   1.209268  ]\n  [ 1.1452781  -0.48211193 -1.3016001 ]]\n\n [[ 0.11678863 -2.2193234   0.95681167]\n  [-0.0503466  -0.0503466  -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)\n\nMax absolute difference: 0.3433941\nConsistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-01-31T07:40:34Z","comments":4,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86256"},{"issue_number":15,"repository":"tensorflow\/tensorflow","title":"Buffer allocation error in Tensorflow Lite with OpenCL backend on certain platforms","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nmacOS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI noticed a memory allocation error in clCreateBuffer. The issue seems to be caused by this:\n\n1) TFlite tries to alloca 0xa2000000 bytes of memory (value stored as size_t)\n\n2) The call ends up in this function (tensorflow\/lite\/experimental\/litert\/runtime\/opencl\/buffer.cc):\n\n```\nabsl::Status CreateClBuffer(cl_context context, int size_in_bytes,\n                            bool read_only, void* data, cl_mem* result) \n```\n\nwhere the size is now int (i.e. 32 bit signed integer... so 0xa2000000 is interpreted as a negative value).\n\n3) This function then calls clCreateBuffer, which takes the size argument as size_t again, and thus receives 0xffffffffa2000000, i.e. the signed 32 bit integer first sign extended to 64bit and then interpreted as unsigned, and thus resulting in a huge size.\n\nThe issue doesn't seem to appear with the same model on Android, probably because: The max buffer allocation size on macOS (M1) seems to be 9GB (according to clinfo), but on that android device it's only 1GB (so on android tflite never tries to allocate such a huge chunk of memory).\n\n### Standalone code to reproduce the issue\n\n```shell\nUnfortunately I'm not allowed to share the code\/model, but looking at the function signatures one can see the issue.\n```\n\n### Relevant log output\n\n```shell\nERROR: Failed to allocate device memory (clCreateBuffer): Invalid buffer size\n```","labels":["type:bug","comp:lite","TF 2.13"],"created_at":"2025-01-29T12:11:28Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86048"},{"issue_number":16,"repository":"tensorflow\/tensorflow","title":"TensorFlow warning shows whenever importing it","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.18.0-rc2-4-g6550e4bd802\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 24.10 x86_64\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nCUDA:12.6\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`cuFFT`, `cuDNN` and `cuBLAS` warnings showing after importing `TensorFlow`\n\n- OS: Ubuntu 24.10 x86_64\n- Host: G5 5590\n- Kernel: 6.11.0-13-generic\n- CPU: Intel i7-9750H (12) @ 4.500GHz\n- GPU: NVIDIA GeForce GTX 1650 Mobile \/ Max-Q\n- GPU: Intel CoffeeLake-H GT2 [UHD Graphics 630]\n> whenever running the following code it gives that warning also it outputs the predicted output but after the warning:\n```python\nimport tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))\n```\n> output:\n```\n2025-01-23 21:08:06.468437: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1737659286.484845  763412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1737659286.489647  763412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-23 21:08:06.505984: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n[PhysicalDevice(name='\/physical_device:GPU:0', device_type='GPU')]\n```\n> also i know that for that warning (`cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`) i must rebuild the tensor-flow from binaries enabling the AVX2 and FMA instructions but what about the others?\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))\n```\n\n### Relevant log output\n\n```shell\n\n```","labels":["type:bug","2.18.rc"],"created_at":"2025-01-23T21:56:39Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85604"},{"issue_number":17,"repository":"tensorflow\/tensorflow","title":"Tutorial \"Multi-worker training with Keras\" fails to complete","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv1.12.1-120353-gc5bd67bc56f 2.19.0-dev20250107\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nDebian 6.1.123-1 (2025-01-02) x86_64 GNU\/Linux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython 3.12.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nFollowing the tutorial everything goes well until you start the second worker. Then the below failure occures.\n\n2025-01-20 07:19:35.283801: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-01-20 07:19:35.290192: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-20 07:19:35.307721: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-01-20 07:19:36.510476: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2025-01-20 07:19:36.510494: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=\"-1\"\n2025-01-20 07:19:36.510499: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n2025-01-20 07:19:36.510501: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n2025-01-20 07:19:36.510505: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael\n2025-01-20 07:19:36.510507: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:190] hostname: michael\n2025-01-20 07:19:36.510562: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0\n2025-01-20 07:19:36.510572: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0\n2025-01-20 07:19:36.510574: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0\n2025-01-20 07:19:36.519175: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:637] Initializing CoordinationService\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc:\/\/localhost:12345\n2025-01-20 07:19:36.524874: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 4677280066871850635\n2025-01-20 07:19:36.524894: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 1\/2 tasks to connect.\n2025-01-20 07:19:36.524898: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:819] Example stragglers:\n\/job:worker\/replica:0\/task:1\nI0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.\n2025-01-20 07:22:27.996664: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 13530699364709055870\n2025-01-20 07:22:27.996686: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 0\/2 tasks to connect.\n\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/layers\/core\/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n  warnings.warn(\n2025-01-20 07:22:28.461733: W tensorflow\/core\/framework\/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\nTraceback (most recent call last):\n  File \"\/home\/chad\/Documents\/McCueFiles\/NeuralNetworks\/TensorFlowProject\/TensorFlowDocExample\/main.py\", line 21, in <module>\n    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 108, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: Attempt to convert a value (PerReplica:{\n  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=\narray([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>\n}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.\n\n### Standalone code to reproduce the issue\n\n```shell\npython main.py &> job_1.log\n```\n\n### Relevant log output\n\n```shell\n2025-01-20 07:19:35.283801: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-01-20 07:19:35.290192: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-20 07:19:35.307721: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-01-20 07:19:36.510476: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2025-01-20 07:19:36.510494: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=\"-1\"\n2025-01-20 07:19:36.510499: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n2025-01-20 07:19:36.510501: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n2025-01-20 07:19:36.510505: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael\n2025-01-20 07:19:36.510507: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:190] hostname: michael\n2025-01-20 07:19:36.510562: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0\n2025-01-20 07:19:36.510572: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0\n2025-01-20 07:19:36.510574: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0\n2025-01-20 07:19:36.519175: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:637] Initializing CoordinationService\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc:\/\/localhost:12345\n2025-01-20 07:19:36.524874: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 4677280066871850635\n2025-01-20 07:19:36.524894: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 1\/2 tasks to connect.\n2025-01-20 07:19:36.524898: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:819] Example stragglers:\n\/job:worker\/replica:0\/task:1\nI0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.\n2025-01-20 07:22:27.996664: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 13530699364709055870\n2025-01-20 07:22:27.996686: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 0\/2 tasks to connect.\n\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/layers\/core\/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n  warnings.warn(\n2025-01-20 07:22:28.461733: W tensorflow\/core\/framework\/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\nTraceback (most recent call last):\n  File \"\/home\/chad\/Documents\/McCueFiles\/NeuralNetworks\/TensorFlowProject\/TensorFlowDocExample\/main.py\", line 21, in <module>\n    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 108, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: Attempt to convert a value (PerReplica:{\n  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=\narray([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>\n}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.\n```","labels":["type:bug","TF 2.18"],"created_at":"2025-01-20T14:03:18Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85351"},{"issue_number":18,"repository":"tensorflow\/tensorflow","title":"Aborted  in `tf.raw_ops.RaggedGather`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific inputs, `tf.raw_ops.RaggedGather` triggers crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nparams_nested_splits = tf.constant(0, shape=[3], dtype=tf.int64)\nparams_dense_values = tf.constant(1, shape=[0], dtype=tf.float32)\nindices = tf.constant(0, shape=[], dtype=tf.int64)\nOUTPUT_RAGGED_RANK = 1\nPARAMS_RAGGED_RANK = 1\n\ntf.raw_ops.RaggedGather(\n    params_nested_splits=[params_nested_splits],\n    params_dense_values=params_dense_values,\n    indices=indices,\n    OUTPUT_RAGGED_RANK=1,\n    name=None\n)\n```\n\n### Relevant log output\n\n```shell\n2025-01-18 09:30:00.549762: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64\nAborted (core dumped)\n```","labels":["type:bug","comp:ops","2.17"],"created_at":"2025-01-18T09:32:16Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85242"},{"issue_number":19,"repository":"tensorflow\/tensorflow","title":"Segmentation fault (core dumped) in `RaggedTensorToTensor`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nshape = tf.constant(-1, shape=[], dtype=tf.int64)\nvalues = tf.constant(0, shape=[0], dtype=tf.int32)\ndefault_value = tf.constant(0, shape=[], dtype=tf.int32)\nrow_partition_tensors = tf.constant([0, 1, 6], shape=[3], dtype=tf.int64)\nrow_partition_types = [\"ROW_SPLITS\"]\n\ntf.raw_ops.RaggedTensorToTensor(\n    shape=shape,\n    values=values,\n    default_value=default_value,\n    row_partition_tensors=[row_partition_tensors],\n    row_partition_types=row_partition_types)\n```\n\n### Relevant log output\n\n```shell\nSegmentation fault (core dumped)\n```","labels":["type:bug","comp:ops","2.17"],"created_at":"2025-01-18T09:27:19Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85240"},{"issue_number":20,"repository":"tensorflow\/tensorflow","title":"Seg Fault when iterate dataset created from data service","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSegfault when trying to iterate dataset get from data service.\n\n### Standalone code to reproduce the issue\n\n```shell\n# start the data service file start_dataservice.py\n\nimport tensorflow as tf\n\ndispatcher = tf.data.experimental.service.DispatchServer(\n    tf.data.experimental.service.DispatcherConfig(port=50050), start=True\n)\ndispatcher_address = dispatcher.target.split(\":\/\/\")[1]\nworker = tf.data.experimental.service.WorkerServer(\n    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True\n)\nprint(\"Starting Worker\")\nworker.join()\n\n# test file test_dataset_service.py\nimport tensorflow as tf\nimport numpy as np\n\n\nflags = tf.compat.v1.app.flags\n\nflags.DEFINE_bool(\"local\", False, \"Run data service in process\")\nflags.DEFINE_bool(\"distribute\", False, \"Run data service in distributed_epoch mode\")\nFLAGS = flags.FLAGS\n\n\ndef local_service():\n    print(\"Starting Local Service\")\n    dispatcher = tf.data.experimental.service.DispatchServer(\n        tf.data.experimental.service.DispatcherConfig(port=50050), start=True\n    )\n    dispatcher_address = dispatcher.target.split(\":\/\/\")[1]\n    worker = tf.data.experimental.service.WorkerServer(\n        tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True\n    )\n    print(\"Dispatcher target is \", dispatcher.target)\n    return dispatcher, worker, dispatcher.target\n\n\ndef apply_transformations(ds_train):\n    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    ds_train = ds_train.cache()\n    ds_train = ds_train.shuffle(60000)\n    ds_train = ds_train.batch(128)\n    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n    return ds_train\n\n\n(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\nx_train = x_train \/ np.float32(255)\ny_train = y_train.astype(np.int64)\nds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n\n\ndef normalize_img(image, label):\n    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n    return tf.cast(image, tf.float32) \/ 255.0, label\n\n\nds_train = apply_transformations(ds_train)\n# Create dataset however you were before using the tf.data service.\ndataset = ds_train\nif FLAGS.local:\n    dispatcher, worker, service = local_service()\nelse:\n    dispatcher_address = \"localhost\"\n    dispatcher_port = \"50050\"\n    service = \"grpc:\/\/{}:{}\".format(dispatcher_address, dispatcher_port)\nif FLAGS.distribute:\n    processing_mode = \"distributed_epoch\"\nelse:\n    processing_mode = \"parallel_epochs\"\n\n# This will register the dataset with the tf.data service cluster so that\n# tf.data workers can run the dataset to produce elements. The dataset returned\n# from applying `distribute` will fetch elements produced by tf.data workers.\ndataset = dataset.apply(\n    tf.data.experimental.service.distribute(processing_mode=processing_mode, service=service)\n)\n\nfor (x1, y1), (x2, y2) in zip(dataset, ds_train):\n    np.allclose(x1, x2)\n    np.allclose(y1, y2)\n\nprint(\"verified mnist dataset locally vs over service\")\n\n# script to run \npython -m pip install --upgrade pip\npython -m pip install tensorflow==2.18.0\npython -m pip install 'protobuf<4'\nscreen -d -m python start_dataservice.py\npython3 test_dataset_service.py --local=False\n```\n\n### Relevant log output\n\n```shell\n2025-01-14 21:56:19.778399: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1736891779.795141    9168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1736891779.800177    9168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-14 21:56:19.815971: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nI0000 00:00:1736891783.518634    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 889 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.520395    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 37945 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1d.0, compute capability: 8.0\nI0000 00:00:1736891783.522012    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:2 with 37945 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.523626    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:3 with 37945 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1d.0, compute capability: 8.0\nI0000 00:00:1736891783.525222    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:4 with 37945 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.526807    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:5 with 37945 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1d.0, compute capability: 8.0\nI0000 00:00:1736891783.528377    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:6 with 37945 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.529933    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:7 with 37945 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1d.0, compute capability: 8.0\n\/test\/bin\/testDataservice: line 5:  9168 Segmentation fault      (core dumped) python ${BIN_DIR}\/test_dataset_service.py --local=False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-01-14T21:57:20Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84897"},{"issue_number":21,"repository":"tensorflow\/tensorflow","title":"GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\nnightly\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 22.04\r\n\r\n### Python version\r\n\r\nPython 3.12\r\n\r\n### CUDA\/cuDNN version\r\n\r\nCUDA 12.4\r\n\r\n### GPU model and memory\r\n\r\nA100 80GB\r\n\r\n### Current behavior?\r\n\r\nStart a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace. \r\nNo any memory profile events or OP profiler, but only trace view.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n**tf_allreduce.py**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.collective_ops import all_reduce, all_reduce_v2\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python.distribute import cluster_resolver as cluster_resolver_lib\r\n\r\ncluster_resolver = cluster_resolver_lib.TFConfigClusterResolver()\r\ncluster = cluster_resolver.cluster_spec()\r\ntask_type = cluster_resolver.task_type\r\ntask_id = cluster_resolver.task_id\r\n\r\nexperimental_config = config_pb2.ConfigProto.Experimental(\r\n    share_cluster_devices_in_session=False,\r\n    share_session_state_in_clusterspec_propagation=False\r\n)\r\nconfig = config_pb2.ConfigProto(experimental=experimental_config)\r\nconfig.experimental.collective_group_leader = '\/job:worker\/replica:0\/task:0'\r\nserver = tf.distribute.Server(cluster,\r\n                              job_name=task_type,\r\n                              task_index=task_id,\r\n                              protocol=\"grpc\", # \"grpc+verbs\"\r\n                              config=config)\r\nrun_options = config_pb2.RunOptions()\r\n\r\nwith tf.compat.v1.Session(target=server.target, config=config) as sess:\r\n    tensor = tf.Variable(tf.ones([2, 2]), dtype=tf.float32)\r\n    init = tf.compat.v1.global_variables_initializer()\r\n    sess.run(init)\r\n    sess.run(tf.print([\"tensor:\",tensor]))\r\n\r\n    reduced_tensor = all_reduce(tensor, group_size=2, group_key=4321, instance_key=1234, merge_op='Add', final_op='Id', communication_hint='auto')\r\n    run_options.experimental.collective_graph_key = 6\r\n    while True:\r\n        sess.run(tf.print([\"reduced_tensor:\",reduced_tensor]), options=run_options)\r\n```\r\n\r\nRun script to start server.\r\n```bash\r\nCUDA_VISIBLE_DEVICES=0 TF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2223\",\"localhost:2224\"]},\"task\":{\"type\":\"worker\",\"index\":0}}' python tf_allreduce.py&\r\nCUDA_VISIBLE_DEVICES=1 TF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2223\",\"localhost:2224\"]},\"task\":{\"type\":\"worker\",\"index\":1}}' python tf_allreduce.py&\r\n```\r\n\r\n use capture_profile in Tensorboard or tf.profiler.experimental.client.trace.\r\n```python\r\ntf.profiler.experimental.client.trace(\r\n  'grpc:\/\/localhost:2223,grpc:\/\/localhost:2224',\r\n   '\/tmp\/my_tb_dir',\r\n   2000,\r\n)\r\n```\r\n\r\nTry to convert xplane.pb to memory_profile, nothing show.\r\n```python\r\nfrom tensorflow.python.profiler.internal import _pywrap_profiler as profiler_wrapper\r\njson = profiler_wrapper.xspace_to_tools_data([\"xxx.xplane\"], \"memory_profile\")\r\n```\r\n\r\n**Relevant log output**\r\n```\r\n{\"memoryProfilePerAllocator\":{},\"numHosts\":1,\"memoryIds\":[]}\r\n```\r\n\r\nRelative issue: #48146 ","labels":["type:bug","comp:gpu","TF 2.18"],"created_at":"2025-01-09T09:26:20Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84460"},{"issue_number":22,"repository":"tensorflow\/tensorflow","title":"Unable to connect to TPU through Cloud VM (metadata issue?)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.18.0-rc2-4-g6550e4bd802 2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\ntpu-ubuntu2204-base\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.2\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI am on a VM instance trying to connect to a tpu v4-32 using a test script. I installed tensorflow-tpu on both the VM (in venv) and TPU (globally) as per the instructions from the google website.\r\n\r\nIt seems like there is an issue with getting TPU metadata.\r\n\r\nIt is able to connect to the metadata server when I request manually from the VM:\r\n\r\n```\r\n$ curl http:\/\/169.254.169.254\/computeMetadata\/v1\/ -H \"Metadata-Flavor: Google\"\r\ninstance\/\r\noslogin\/\r\nproject\/\r\n```\r\n\r\nAny help would be appreciated!\n\n### Standalone code to reproduce the issue\n\n```shell\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name)\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntry:\r\n    tf.tpu.experimental.initialize_tpu_system(resolver)\r\n    print(\"TPU initialized:\", resolver.master())\r\nexcept Exception as e:\r\n    print(\"Failed to initialize TPU:\", e)\n```\n\n\n### Relevant log output\n\n```shell\n$ python hello.py\r\n2025-01-08 23:49:33.189260: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2025-01-08 23:49:33.221197: I tensorflow\/core\/tpu\/tpu_api_dlsym_initializer.cc:95] Opening library: \/home\/ucsdwanglab\/test_tpu\/.venv\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2\r\n2025-01-08 23:49:33.221290: I tensorflow\/core\/tpu\/tpu_api_dlsym_initializer.cc:121] Libtpu path is: \/home\/ucsdwanglab\/test_tpu\/.venv\/lib\/python3.11\/site-packages\/libtpu\/libtpu.so\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable CHIPS_PER_HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable ALT: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable WRAP: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (accelerator-type) from instance metadata for variable TPU_ACCELERATOR_TYPE: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\n\r\nFailed to find host bounds for accelerator type: WARNING: could not determine TPU accelerator type, please set env var `TPU_ACCELERATOR_TYPE` manually, otherwise libtpu.so may not properly initialize.\r\nFailed to get TPU metadata (agent-worker-number) from instance metadata for variable TPU_WORKER_ID: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\n\r\nFailed to get TPU metadata (worker-network-endpoints) from instance metadata for variable TPU_WORKER_HOSTNAMES: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\n\r\nWARNING: Logging before InitGoogle() is written to STDERR\r\nE0000 00:00:1736380405.363400    3192 common_lib.cc:511] INVALID_ARGUMENT: Error: unexpected worker hostname 'WARNING: could not determine TPU worker hostnames or IP addresses' from env var TPU_WORKER_HOSTNAMES. Expecting a valid hostname or IP address without port number. (Full TPU workers' addr string: WARNING: could not determine TPU worker hostnames or IP addresses, please set env var `TPU_WORKER_HOSTNAMES` manually, otherwise libtpu.so may not properly initialize.)\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/libtpu_init_utils.cc:173\r\n2025-01-08 23:56:48.526584: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1736380609.730442    3192 context_distributed_manager.cc:762] unknown service tensorflow.WorkerService\r\nAdditional GRPC error information from remote target \/job:worker\/replica:0\/task:0 while calling \/tensorflow.WorkerService\/GetStatus:\r\n:{\"created\":\"@1736380609.730372913\",\"description\":\"Error received from peer ipv4:10.130.0.3:8470\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/surface\/call.cc\",\"file_line\":1056,\"grpc_message\":\"unknown service tensorflow.WorkerService\",\"grpc_status\":12}\r\nE0108 23:56:49.730822322    3192 completion_queue.cc:244]    assertion failed: queue.num_items() == 0\r\nhttps:\/\/symbolize.stripped_domain\/r\/?trace=7f1ccaf5cebc,7f1ccaf0e04f&map= \r\n*** SIGABRT received by PID 3192 (TID 3192) on cpu 4 from PID 3192; stack trace: ***\r\nPC: @     0x7f1ccaf5cebc  (unknown)  (unknown)\r\n    @     0x7f1caa302841       1888  (unknown)\r\n    @     0x7f1ccaf0e050   18460496  (unknown)\r\n    @     0x7f1ccaed1c60  (unknown)  (unknown)\r\nhttps:\/\/symbolize.stripped_domain\/r\/?trace=7f1ccaf5cebc,7f1caa302840,7f1ccaf0e04f,7f1ccaed1c5f&map= \r\nE0108 23:56:49.732558    3192 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked.\r\nE0108 23:56:49.732569    3192 coredump_hook.cc:355] RAW: Skipping coredump since rlimit was 0 at process start.\r\nE0108 23:56:49.732575    3192 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.\r\nE0108 23:56:49.732580    3192 coredump_hook.cc:411] RAW: Sending fingerprint to remote end.\r\nE0108 23:56:49.732595    3192 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket \/var\/google\/services\/logmanagerd\/remote_coredump.socket (Is the listener running?): No such file or directory\r\nE0108 23:56:49.732601    3192 coredump_hook.cc:472] RAW: Dumping core locally.\r\nE0108 23:56:49.745981    3192 process_state.cc:805] RAW: Raising signal 6 with default behavior\r\nAborted\n```\n","labels":["type:bug","comp:tpus","TF 2.18"],"created_at":"2025-01-09T00:04:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84413"},{"issue_number":23,"repository":"tensorflow\/tensorflow","title":"dictionaries in fit method of model load data in wrong order","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17; tf 2.18\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 22.04.3 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.12\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nthe code is running in google collab.\r\nThe code below is an example of a model with multiple inputs and multiple outputs.\r\nNOT working code with using **dictionaries** in method **fit** of model.\r\n\r\nthe link to collab:  https:\/\/colab.research.google.com\/drive\/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing\r\nthe link to gist: https:\/\/gist.github.com\/moprules\/def9b2bda642a064b35e51b8914a28dd\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\n# collab:  https:\/\/colab.research.google.com\/drive\/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing\r\n# gist:      https:\/\/gist.github.com\/moprules\/def9b2bda642a064b35e51b8914a28dd\r\n\r\n# fast code\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\nvocabulary_size = 10000\r\nnum_tags = 100\r\nnum_departments = 4\r\n\r\n# define three model inputs\r\ntitle = keras.Input(shape=(vocabulary_size,), name=\"title\")\r\ntext_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\r\ntags = keras.Input(shape=(num_tags,), name=\"tags\")\r\n\r\nfeatures = layers.Concatenate()([title, text_body, tags])\r\n# one intermediate layer\r\nfeatures = layers.Dense(64, activation=\"relu\")(features)\r\n\r\n# Define two model outputs\r\npriority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\r\ndepartment = layers.Dense(num_departments, activation=\"softmax\", name=\"department\")(features)\r\n\r\n# set the model\r\nmodel = keras.Model(inputs=[title, text_body, tags],\r\n                    outputs=[priority, department])\r\n# prepare data\r\nnum_samples = 1280\r\n# The data is filled in with zeros and ones\r\ntitle_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\r\ntext_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\r\ntags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\r\n\r\n# priority: [0., 1.]\r\npriority_data = np.random.random(size=(num_samples, 1))\r\n# class of 4 labels\r\ndepartment_data = np.random.randint(0, 2, size=(num_samples, num_departments))\r\n\r\n# compile model\r\nmodel.compile(optimizer=\"rmsprop\",\r\n              loss={\"priority\": \"mean_squared_error\",\r\n                    \"department\": \"categorical_crossentropy\"},\r\n              metrics={\"priority\": [\"mean_absolute_error\"],\r\n                       \"department\": [\"accuracy\"]})\r\n\r\n# It doesn't matter how the model is compiled\r\n# model.compile(optimizer=\"rmsprop\",\r\n#               loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\r\n#               metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\r\n\r\n\r\n# NOT WORKING\r\n# TRAIN MODEL WITH transferring the DICTIONARY to the method\r\nmodel.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\r\n          {\"priority\": priority_data, \"department\": department_data},\r\n          epochs=1\r\n)\r\n\r\n# WORK\r\n# TRAIN MODEL WITHOUT transferring the DICTIONARY to the method\r\nmodel.fit([title_data, text_body_data, tags_data],\r\n          [priority_data, department_data],\r\n          epochs=1\r\n)\r\n\r\n# ALSO WORK\r\n# TRAIN MODEL WITH transferring the DICTIONARY to the method\r\n# REPLACE priority and department\r\nmodel.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\r\n          {\"priority\": department_data, \"department\": priority_data},\r\n          epochs=1\r\n)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting response","type:bug","stale","comp:keras","TF 2.18"],"created_at":"2025-01-07T13:08:06Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84278"},{"issue_number":24,"repository":"tensorflow\/tensorflow","title":"keras model.save does not respect `include_optimizer=False`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.19.0-dev20250105\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSaving a model using keras with `include_optizer = False` results in a model being saved with optimizer\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1x5NJs9nFxmExhuy8_f_fOehHmIOmk-CZ?usp=sharing\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","comp:keras","TF 2.18"],"created_at":"2025-01-07T10:33:38Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84268"},{"issue_number":25,"repository":"tensorflow\/tensorflow","title":"Encountered unresolved custom op: XlaDynamicSlice","description":"Hi, i am doing a task in converting T5 model to TFLite for Android. Currently, i am using T5ModelForConditionalGeneration from Huggingface to convert. The conversion is done with some below logging but when load the `generator` from Interpretor, and run an inference example, i have faced this error. You can reproduce with the colab provided below. AFAIK, this XlaDynamicSlice is in TF ops but why this op cannot be resolved in this cases. \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): PyPI 24.0 on Python 3.11.5\r\n- TensorFlow version (or github SHA if from source): 2.18.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nIn colab version, tflite_convert doesn't log anything, below log is in my local version\r\n```\r\nINFO:tensorflow:Assets written to: \/tmp\/tmpaxxybw9x\/assets\r\nINFO:tensorflow:Assets written to: \/tmp\/tmpaxxybw9x\/assets\r\nW0000 00:00:1736157114.568747 1061359 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\r\nW0000 00:00:1736157114.568765 1061359 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\r\n2025-01-06 16:51:54.568997: I tensorflow\/cc\/saved_model\/reader.cc:83] Reading SavedModel from: \/tmp\/tmpaxxybw9x\r\n2025-01-06 16:51:54.645325: I tensorflow\/cc\/saved_model\/reader.cc:52] Reading meta graph with tags { serve }\r\n2025-01-06 16:51:54.645352: I tensorflow\/cc\/saved_model\/reader.cc:147] Reading SavedModel debug info (if present) from: \/tmp\/tmpaxxybw9x\r\n2025-01-06 16:51:55.085153: I tensorflow\/cc\/saved_model\/loader.cc:236] Restoring SavedModel bundle.\r\n2025-01-06 16:51:56.061632: I tensorflow\/cc\/saved_model\/loader.cc:220] Running initialization op on SavedModel bundle at path: \/tmp\/tmpaxxybw9x\r\n2025-01-06 16:51:56.517300: I tensorflow\/cc\/saved_model\/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 1948307 microseconds.\r\n2025-01-06 16:52:30.233639: W tensorflow\/compiler\/mlir\/lite\/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\r\nFlex ops: FlexStridedSlice\r\nDetails:\r\n\ttf.StridedSlice(tensor<?x?x?xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<?x1x?x?xf32>) : {begin_mask = 13 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 13 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}\r\nSee instructions: https:\/\/www.tensorflow.org\/lite\/guide\/ops_select\r\n2025-01-06 16:52:30.233666: W tensorflow\/compiler\/mlir\/lite\/flatbuffer_export.cc:3836] The following operation(s) need TFLite custom op implementation(s):\r\nCustom ops: XlaDynamicSlice\r\nDetails:\r\n\ttf.XlaDynamicSlice(tensor<1x12x?x?xf32>, tensor<4xi64>, tensor<4xi64>) -> (tensor<1x12x1x?xf32>) : {device = \"\"}\r\nSee instructions: https:\/\/www.tensorflow.org\/lite\/guide\/ops_custom\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab\/Jupyter\/any notebook.\r\nMy reproduce code in Colab: https:\/\/colab.research.google.com\/drive\/1Rmhc_vpJSa7M1Vt-4ugV5uORaEfRJMOw?usp=sharing\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info \/ logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n","labels":["stat:awaiting response","type:bug","comp:lite","TFLiteConverter","TF 2.18"],"created_at":"2025-01-06T10:46:54Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84203"},{"issue_number":26,"repository":"tensorflow\/tensorflow","title":"MFCC-Example-Model converted from TF to TFlite fails with IsPowerOfTwo-RuntimeError inside rfft2d","description":"### 1. System information\r\n\r\n- OS Platform and Distribution: Linux Mint 6.2.9\r\n- TensorFlow installation: pip\r\n- TensorFlow library: 2.18.0 (latest)\r\n\r\n### 2. Code\r\n\r\nBelow is a minimum example which triggers the rfft2d IsPowerOfTwo RuntimeError.\r\nThe MFCC-Calculation was directly taken from the tutorial from [tensorflow.org](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/signal\/mfccs_from_log_mel_spectrograms#for_example)\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass MFCCLayer(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        super(MFCCLayer, self).__init__(**kwargs)\r\n\r\n    def call(self, pcm):\r\n        # A 1024-point STFT with frames of 64 ms and 75% overlap.\r\n        stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256, fft_length=1024)\r\n        spectrograms = tf.abs(stfts)\r\n\r\n        # Warp the linear scale spectrograms into the mel-scale.\r\n        num_spectrogram_bins = stfts.shape[-1]\r\n        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\r\n        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\r\n            num_mel_bins,\r\n            num_spectrogram_bins,\r\n            sample_rate,\r\n            lower_edge_hertz,\r\n            upper_edge_hertz,\r\n        )\r\n        mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\r\n        mel_spectrograms.set_shape(\r\n            spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:])\r\n        )\r\n\r\n        # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\r\n        log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\r\n\r\n        # Compute MFCCs from log_mel_spectrograms and take the first 13.\r\n        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[\r\n            ..., :13\r\n        ]\r\n        print(\"mfccs.shape: \", mfccs.shape)\r\n        return mfccs\r\n\r\n\r\ndef build_model(input_shape):\r\n    input_layer = tf.keras.layers.Input(shape=input_shape)\r\n    output_layer = MFCCLayer()(input_layer)\r\n    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    batch_size, num_samples, sample_rate = 32, 32000, 16000.0\r\n    # A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].\r\n    pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)\r\n    print(\"pcm.shape: \", pcm.shape)\r\n\r\n    model = build_model(pcm.shape)\r\n    model.summary()\r\n\r\n    # Convert to TensorFlow Lite and Save\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    tflite_model = converter.convert()\r\n\r\n    with open(\"mfcc.tflite\", \"wb\") as f:\r\n        f.write(tflite_model)\r\n\r\n    # Load the model and run inference\r\n    with open(\"mfcc.tflite\", \"rb\") as f:\r\n        tflite_model = f.read()\r\n\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    pcm = tf.expand_dims(pcm, axis=0)  # Add batch dimension\r\n\r\n    interpreter.set_tensor(input_details[0][\"index\"], pcm)\r\n    interpreter.invoke()  # <-- RuntimeError: tensorflow\/lite\/kernels\/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.\r\n    mfccs = interpreter.get_tensor(output_details[0][\"index\"])\r\n    print(\"mfccs.shape: \", mfccs.shape)\r\n```\r\n\r\n\r\n### 3. Failure after conversion\r\nAs far as I know, the RuntimeError should't happen, as all supplied stft-function arguments are power of two's?\r\n\r\nI am unsure if this is just a user error from myself or this is a bug.\r\nI couldn't find any info online, hence i ask here.\r\n\r\nIs a MFCC-calculation model possible with TFlite?\r\n\r\nThanks for all help\r\n\r\n","labels":["stat:awaiting response","type:bug","stale","comp:lite","TFLiteConverter","TF 2.18"],"created_at":"2025-01-05T20:45:45Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84171"},{"issue_number":27,"repository":"tensorflow\/tensorflow","title":"Broken compatibility with tensorflow-metal in 2.18","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMacOS 15.2\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nApple M2 Max GPU 38-cores\n\n### Current behavior?\n\nApple silicone GPU with tensorflow-metal==1.1.0  and python 3.11 works fine with tensorboard==2.17.0\r\n\r\nThis is normal output:\r\n```\r\n\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/bin\/python \/Users\/mspanchenko\/VSCode\/cryptoNN\/ml\/core_second_window\/test_tensorflow_gpus.py \r\n[PhysicalDevice(name='\/physical_device:GPU:0', device_type='GPU')]\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\nBut if I upgrade tensorflow to 2.18 I'll have error, attached in \"Relevant log output\" issue section\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nif __name__ == '__main__':\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    print(gpus)\n```\n\n\n### Relevant log output\n\n```shell\n\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/bin\/python \/Users\/mspanchenko\/VSCode\/cryptoNN\/ml\/core_second_window\/test_tensorflow_gpus.py \r\nTraceback (most recent call last):\r\n  File \"\/Users\/mspanchenko\/VSCode\/cryptoNN\/ml\/core_second_window\/test_tensorflow_gpus.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow\/__init__.py\", line 437, in <module>\r\n    _ll.load_library(_plugin_dir)\r\n  File \"\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/load_library.py\", line 151, in load_library\r\n    py_tf.TF_LoadLibrary(lib)\r\ntensorflow.python.framework.errors_impl.NotFoundError: dlopen(\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow-plugins\/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii\r\n  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> \/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow-plugins\/libmetal_plugin.dylib\r\n  Expected in:     <2814A58E-D752-317B-8040-131217E2F9AA> \/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tensorflow_internal.so\r\n\r\nProcess finished with exit code 1\n```\n","labels":["stat:awaiting response","type:bug","stale","comp:gpu","TF 2.18"],"created_at":"2025-01-05T17:26:17Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84167"},{"issue_number":28,"repository":"tensorflow\/tensorflow","title":"The test case label_image .py of tensorflow2.4.1 source code fails to be execued.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.4.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.7.12\n\n### Bazel version\n\n3.7\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe test case label_image.py fails to be executed,and the message \"module 'tensorfle' has no attribute 'GrapDef'\" is displayed.\r\n![image](https:\/\/github.com\/user-attachments\/assets\/e4b7b56d-2589-41fe-8395-1743c941dd49)\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ngraph_def = tf.GraphDef()\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting response","type:bug","stale","TF 2.4"],"created_at":"2025-01-03T03:03:30Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84039"},{"issue_number":29,"repository":"tensorflow\/tensorflow","title":"Failing to convert MobileNetV3Large to TFLite w\/ Integer q","description":"### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Windows 10 WSL\r\n- TensorFlow installation (pip package or built from source):  2.10 (on Win 10) and 2.16.2 (on WSL)\r\n\r\n### 2. Code\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.applications import MobileNetV3Large\r\nfrom tensorflow.keras.applications.mobilenet_v3 import preprocess_input\r\nimport matplotlib.pyplot as plt\r\nfrom scipy.stats import pearsonr\r\n\r\n# Generate one sample image for testing\r\ntest_image = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))\r\ntest_image = np.clip(test_image, 0, 255).astype(np.float32)\r\npreprocessed_image = preprocess_input(test_image.copy())\r\n\r\n# Load model\r\nmodel = MobileNetV3Large(\r\n    weights='imagenet',\r\n    include_top=True,\r\n    input_shape=(224, 224, 3)\r\n)\r\n\r\n# Get original prediction\r\noriginal_pred = model.predict(preprocessed_image, verbose=0)\r\n\r\n# Convert to TFLite\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable dynamic range quantization\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\nconverter._experimental_disable_per_channel = True\r\nconverter.experimental_new_converter = True\r\n\r\n# Convert\r\ntflite_model = converter.convert()\r\n\r\n# Get TFLite prediction\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], preprocessed_image)\r\ninterpreter.invoke()\r\ntflite_pred = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n# Calculate correlation\r\ncorrelation, _ = pearsonr(original_pred.flatten(), tflite_pred.flatten())\r\n\r\n# Visualize\r\nplt.figure(figsize=(10, 5))\r\n\r\n# Scatter plot\r\nplt.subplot(1, 2, 1)\r\nplt.scatter(original_pred.flatten(), tflite_pred.flatten(), alpha=0.5)\r\nplt.plot([original_pred.min(), original_pred.max()],\r\n         [original_pred.min(), original_pred.max()],\r\n         'r--', label=f'Perfect Correlation\\nActual: {correlation:.4f}')\r\nplt.title('Original vs Quantized Predictions')\r\nplt.xlabel('Original Model')\r\nplt.ylabel('Quantized Model')\r\nplt.legend()\r\n\r\n# Distribution plot\r\nplt.subplot(1, 2, 2)\r\nplt.hist(np.abs(original_pred.flatten() - tflite_pred.flatten()),\r\n         bins=50, alpha=0.75, label='Prediction Differences')\r\nplt.title('Distribution of Prediction Differences')\r\nplt.xlabel('|Original - Quantized|')\r\nplt.ylabel('Count')\r\nplt.legend()\r\n\r\nplt.tight_layout()\r\nplt.show()\r\n\r\nprint(f\"\\nResults:\")\r\nprint(f\"Prediction correlation: {correlation:.4f}\")\r\nprint(f\"Original model size: {len(model.get_weights()) \/ 1024 \/ 1024:.2f} MB\")\r\nprint(f\"Quantized model size: {len(tflite_model) \/ 1024 \/ 1024:.2f} MB\")\r\nprint(f\"Size reduction: {(1 - len(tflite_model) \/ len(model.get_weights())) * 100:.1f}%\")\r\n```\r\n\r\n### 3. Failure after conversion\r\n1. TF 2.10 in Win10 Log:\r\nModel produces wrong results. See plot made from code:\r\n![image](https:\/\/github.com\/user-attachments\/assets\/8823cbfc-88d9-4e2d-9ef7-c8a2adc3ef0a)\r\n\r\n2. TF2.16 in WSL:\r\nModel fails to convert. Gets error: `LLVM ERROR: Failed to infer result type(s).` (see log)\r\n\r\n\r\n### 5. (optional) Any other info \/ logs\r\nI ran this on 2 systems:\r\n\r\n1. TF 2.10 in Win10 Log:\r\n```\r\nimport sys; print('Python %s on %s' % (sys.version, sys.platform))\r\nD:\\code\\ai_dev\\venv\\Scripts\\python.exe \"C:\/Program Files\/JetBrains\/PyCharm 2023.2.4\/plugins\/python\/helpers\/pydev\/pydevd.py\" --multiprocess --qt-support=auto --client 127.0.0.1 --port 54366 --file C:\\Users\\Administrator\\AppData\\Roaming\\JetBrains\\PyCharm2023.2\\scratches\\tfmodel_tflite.py \r\nConnected to pydev debugger (build 232.10203.26)\r\n2024-12-26 15:17:07.215039: I tensorflow\/core\/platform\/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-12-26 15:17:07.670016: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1616] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 7423 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6\r\n2024-12-26 15:17:11.111912: I tensorflow\/stream_executor\/cuda\/cuda_dnn.cc:384] Loaded cuDNN version 8906\r\n2024-12-26 15:17:12.036555: I tensorflow\/stream_executor\/cuda\/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 64). These functions will not be directly callable after loading.\r\n2024-12-26 15:17:44.746406: W tensorflow\/compiler\/mlir\/lite\/python\/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\r\n2024-12-26 15:17:44.746529: W tensorflow\/compiler\/mlir\/lite\/python\/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\r\n2024-12-26 15:17:44.747230: I tensorflow\/cc\/saved_model\/reader.cc:45] Reading SavedModel from: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprjeve5nr\r\n2024-12-26 15:17:44.771028: I tensorflow\/cc\/saved_model\/reader.cc:89] Reading meta graph with tags { serve }\r\n2024-12-26 15:17:44.771129: I tensorflow\/cc\/saved_model\/reader.cc:130] Reading SavedModel debug info (if present) from: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprjeve5nr\r\n2024-12-26 15:17:44.886049: I tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\r\n2024-12-26 15:17:44.904668: I tensorflow\/cc\/saved_model\/loader.cc:229] Restoring SavedModel bundle.\r\n2024-12-26 15:17:45.275249: I tensorflow\/cc\/saved_model\/loader.cc:213] Running initialization op on SavedModel bundle at path: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprjeve5nr\r\n2024-12-26 15:17:45.402632: I tensorflow\/cc\/saved_model\/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 655396 microseconds.\r\n2024-12-26 15:17:45.811466: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n```\r\n\r\n3. TF 2.16.2 in WSL log:\r\n```\r\n\/root\/ai_dev\/.venv\/bin\/python \/root\/.pycharm_helpers\/pydev\/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 55955 --file \/mnt\/c\/Users\/Administrator\/AppData\/Roaming\/JetBrains\/PyCharm2023.2\/scratches\/tfmodel_tflite.py \r\nConnected to pydev debugger (build 232.10203.26)\r\n2024-12-26 15:18:56.434366: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-26 15:18:57.803991: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-26 15:18:58.473972: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-12-26 15:18:58.972456: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-12-26 15:18:58.975123: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-12-26 15:18:59.910805: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-12-26 15:19:06.124533: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-12-26 15:19:15.989425: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:984] could not open file to read NUMA node: \/sys\/bus\/pci\/devices\/0000:0a:00.0\/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2024-12-26 15:19:17.013008: W tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https:\/\/www.tensorflow.org\/install\/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nW0000 00:00:1735255170.947341  469943 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\r\nW0000 00:00:1735255170.947401  469943 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\r\n2024-12-26 15:19:30.948060: I tensorflow\/cc\/saved_model\/reader.cc:83] Reading SavedModel from: \/tmp\/tmp9lkkwnp9\r\n2024-12-26 15:19:30.953309: I tensorflow\/cc\/saved_model\/reader.cc:51] Reading meta graph with tags { serve }\r\n2024-12-26 15:19:30.953334: I tensorflow\/cc\/saved_model\/reader.cc:146] Reading SavedModel debug info (if present) from: \/tmp\/tmp9lkkwnp9\r\n2024-12-26 15:19:31.011901: I tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\r\n2024-12-26 15:19:31.020594: I tensorflow\/cc\/saved_model\/loader.cc:234] Restoring SavedModel bundle.\r\n2024-12-26 15:19:31.231606: I tensorflow\/cc\/saved_model\/loader.cc:218] Running initialization op on SavedModel bundle at path: \/tmp\/tmp9lkkwnp9\r\n2024-12-26 15:19:31.297302: I tensorflow\/cc\/saved_model\/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 349244 microseconds.\r\n2024-12-26 15:19:31.779723: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nloc(fused[\"ReadVariableOp:\", callsite(\"MobileNetV3Large_1\/conv_1\/convolution\/ReadVariableOp@__inference_serving_default_5035\"(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":2199:1) at callsite(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":2181:1 at callsite(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":1493:1 at callsite(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":1500:1 at callsite(\"\/root\/.pycharm_helpers\/pydev\/_pydev_imps\/_pydev_execfile.py\":18:1 at callsite(\"\/mnt\/c\/Users\/Administrator\/AppData\/Roaming\/JetBrains\/PyCharm2023.2\/scratches\/tfmodel_tflite.py\":34:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1175:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1129:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1636:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1614:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/convert_phase.py\":205:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1537:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/layer.py\":58:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/layer.py\":112:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/layer.py\":899:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/operation.py\":46:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":156:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\":182:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/function.py\":171:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\":632:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/layer.py\":899:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/operation.py\":46:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":156:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/convolutional\/base_conv.py\":243:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/convolutional\/base_conv.py\":233:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/nn.py\":1183:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/nn.py\":301:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/nn.py\":274:1 at \"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/core.py\":85:1))))))))))))))))))))))))))))))))]): error: missing attribute 'value'\r\nLLVM ERROR: Failed to infer result type(s).\r\n\r\nProcess finished with exit code 134\r\n```\r\n\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:lite","TFLiteConverter","TF 2.16"],"created_at":"2024-12-26T23:21:09Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83754"},{"issue_number":30,"repository":"tensorflow\/tensorflow","title":"How to run TFLite benchmark with QNN delegate in Android","description":"### Issue type\r\n\r\nFeature Request\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.15.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nmacOS 15.2\r\n\r\n### Mobile device\r\n\r\nOne Plus 7 Pro, Android 11\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI have built\/installed\/run TFLite benchmark following this [instruction](https:\/\/github.com\/tensorflow\/tensorflow\/tree\/master\/tensorflow\/lite\/tools\/benchmark#on-android) for Android, and used TensorFlow 2.15.0 according to [issue#66015](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66015). I test the benchmark via the following commands and the output result seems correct.\r\n```shell\r\nadb push \/Users\/handleychen\/Github\/tensorflow\/tensorflow\/bazel-bin\/tensorflow\/lite\/tools\/benchmark\/benchmark_model \/data\/local\/tmp\r\nadb shell chmod +x \/data\/local\/tmp\/benchmark_model\r\nadb shell \"mkdir \/data\/local\/tmp\/models\"\r\nadb push \/Users\/handleychen\/Github\/tensorflow\/models\/*.tflite \/data\/local\/tmp\/models\r\nadb shell \/data\/local\/tmp\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224.tflite --num_threads=4 --enable_op_profiling=true\r\nadb shell \/data\/local\/tmp\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224.tflite --use_gpu=true --enable_op_profiling=true\r\nadb shell \/data\/local\/tmp\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224.tflite --use_nnapi=true --enable_op_profiling=true\r\n``` \r\n[benchmark result.txt](https:\/\/github.com\/user-attachments\/files\/18197819\/benchmark.result.txt)\r\n\r\nNow I want to run the benchmark with QNN delegate. I [setup the on device environment](https:\/\/docs.qualcomm.com\/bundle\/publicresource\/topics\/80-63442-50\/TfLite-Delegate_setup.html#on-device-environment-setup) and [run a QNN delegate using an external delegate](https:\/\/docs.qualcomm.com\/bundle\/publicresource\/topics\/80-70015-54\/sample-applications.html#run-a-qnn-delegate-using-an-external-delegate). The [model](https:\/\/storage.googleapis.com\/download.tensorflow.org\/models\/tflite\/task_library\/image_classification\/android_java\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite) being tested comes from tflite example [image_classification](https:\/\/github.com\/tensorflow\/examples\/tree\/master\/lite\/examples\/image_classification).  I tested the benchmark using the following commands, but the result was a failure.\r\n```shell\r\nadb shell \"mkdir \/data\/local\/tmp\/qnn_delegate\"\r\nadb push \/Users\/handleychen\/Github\/quic\/SDK\/qairt\/2.26.0.240828\/lib\/aarch64-android\/* \/data\/local\/tmp\/qnn_delegate\r\nadb shell\r\ncd \/data\/local\/tmp\r\nexport LD_LIBRARY_PATH=\/data\/local\/tmp\/qnn_delegate\r\nexport ADSP_LIBRARY_PATH=\"\/data\/local\/tmp\/qnn_delegate\"\r\n.\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:gpu;'\r\n.\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;'\r\n# I also tried setting htp_precision:1, but the result was the same.\r\n.\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;htp_precision:1'\r\n``` \r\n```shell\r\n# for gpu delegate\r\n\u2026\u2026\r\nINFO: Though EXTERNAL delegate is explicitly applied, the model graph will not be executed by the delegate.\r\n\u2026\u2026\r\n\r\n# for npu delegate\r\nINFO: STARTING!\r\nINFO: Log parameter values verbosely: [0]\r\nINFO: Graph: [\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]\r\nINFO: External delegate path: [\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so]\r\nINFO: External delegate options: [backend_type:htp;htp_precision:1]\r\nINFO: Loaded model \/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: EXTERNAL delegate created.\r\nERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008\r\nERROR: Restored original execution plan after delegate application failure.\r\nERROR: Failed to apply EXTERNAL delegate.\r\nERROR: Benchmarking failed.\r\n``` \r\nThe full output is attached. [benchmarkQNN result.txt](https:\/\/github.com\/user-attachments\/files\/18206356\/benchmarkQNN.result.txt)\r\n\r\nI have also tested it on Android phones equipped with Snapdragon 855 and Gen 3 chips, and the results were the same.\r\n\r\nCould anyone tell me how to deal with this?\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nas described above\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:lite"],"created_at":"2024-12-19T12:40:25Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83344"},{"issue_number":31,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `LearnedUnigramCandidateSampler`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific inputs,`tf.raw_ops.LearnedUnigramCandidateSampler` triggers crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntrue_classes = tf.constant([], dtype=tf.int64)\r\nnum_true = 3590707793247644003\r\nnum_sampled = 126\r\nunique = False\r\nrange_max = 186785497093039093\r\nseed = 8997\r\nseed2 = 0\r\n\r\ntf.raw_ops.LearnedUnigramCandidateSampler(\r\n    true_classes=true_classes,\r\n    num_true=num_true,\r\n    num_sampled=num_sampled,\r\n    unique=unique,\r\n    range_max=range_max,\r\n    seed=seed,\r\n    seed2=seed2,\r\n    name=None\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-12-17 11:36:29.305345: W tensorflow\/core\/framework\/op_kernel.cc:1816] OP_REQUIRES failed at candidate_sampler_ops.cc:37 : INVALID_ARGUMENT: Attr num_true has value 3590707793247644003 out of range for an int32\r\n2024-12-17 11:36:29.305378: F external\/local_tsl\/tsl\/lib\/random\/weighted_picker.cc:28] Check failed: N >= 0 (0 vs. -2090015755)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-12-17T12:05:50Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83164"},{"issue_number":32,"repository":"tensorflow\/tensorflow","title":"[XLA] `tf.keras.layers.LSTM` behaves differently on GPU","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nnightly\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen executing LSTM on **XLA**, it fails.\r\nHowever, when executing it without XLA, it passes.\r\nThe above failure is on GPU.\r\nIf I use CPU as backend, with or without XLA both pass the check.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport os\r\nimport tensorflow\r\nimport tensorflow as tf\r\ntf.random.set_seed(42)\r\nclass RecurrentModel(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(RecurrentModel, self).__init__()\r\n        self.lstm = tf.keras.layers.LSTM(units=64, return_sequences=True)\r\n\r\n    @tf.function(jit_compile=True)\r\n    def call(self, x):\r\n        return self.lstm(x)\r\n\r\n\r\nmodel = RecurrentModel()\r\n\r\n\r\ninput_shape = (10, 20, 1)\r\nx = tf.random.normal(shape=input_shape)\r\n\r\ninputs = [x]\r\n\r\noutput = model(*inputs)\r\nprint(output)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-0938fdccd1fa> in <cell line: 24>()\r\n     22 inputs = [x]\r\n     23 \r\n---> 24 output = model(*inputs)\r\n     25 print(output)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51   try:\r\n     52     ctx.ensure_initialized()\r\n---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                         inputs, attrs, num_outputs)\r\n     55   except core._NotOkStatusException as e:\r\n\r\nInvalidArgumentError: Exception encountered when calling RecurrentModel.call().\r\n\r\nDetected unsupported operations when trying to compile graph __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: CudnnRNNV3 (No registered 'CudnnRNNV3' OpKernel for XLA_GPU_JIT devices compatible with node {{node lstm_3_1\/CudnnRNNV3}}){{node lstm_3_1\/CudnnRNNV3}}\r\nThe op is created at: \r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/colab_kernel_launcher.py\", line 37, in <module>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/traitlets\/config\/application.py\", line 992, in launch_instance\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelapp.py\", line 619, in start\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/platform\/asyncio.py\", line 195, in start\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 685, in <lambda>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 738, in _run_callback\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 825, in inner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 361, in process_one\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 261, in dispatch_shell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 539, in execute_request\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 2975, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3030, in _run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/async_helpers.py\", line 78, in _pseudo_sync_runner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3257, in run_cell_async\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3473, in run_ast_nodes\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3553, in run_code\r\nFile \"<ipython-input-4-0938fdccd1fa>\", line 24, in <cell line: 24>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/layer.py\", line 826, in __call__\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/layer.py\", line 1376, in _maybe_build\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/core.py\", line 212, in compute_output_spec\r\nFile \"<ipython-input-1-0938fdccd1fa>\", line 13, in call\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/layer.py\", line 901, in __call__\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/ops\/operation.py\", line 46, in __call__\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 156, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/rnn\/lstm.py\", line 570, in call\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/rnn\/rnn.py\", line 406, in call\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/rnn\/lstm.py\", line 537, in inner_loop\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/rnn.py\", line 841, in lstm\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/rnn.py\", line 933, in _cudnn_lstm\r\n\ttf2xla conversion failed while converting __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=\/path\/to\/dump\/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_877]\r\n\r\nArguments received by RecurrentModel.call():\r\n  \u2022 x=tf.Tensor(shape=(10, 20, 1), dtype=float32)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","comp:xla","TF 2.18"],"created_at":"2024-12-16T13:57:22Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83063"},{"issue_number":33,"repository":"tensorflow\/tensorflow","title":"`tensorflow.experimental.numpy.kron` not working with multidimensional Arrays","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThis is a new issue in replacement for https:\/\/github.com\/tensorflow\/tensorflow\/issues\/59761 as suggested by @tilakrayal\r\n\r\nI tested the function against numpy and it throws an error when the `ndim` of the input tensors is greater than 2.\r\nI run the code on the latest TensorFlow version on PyPI and the nightly version, and I get the same failures.\r\n\r\nAlso, I am not getting as much debug information only this error\r\n\r\n`UnimplementedError: {{function_node __wrapped__Mul_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name: `\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\ntry:\r\n    a = tf.constant(np.arange(100).reshape(2, 5, 2, 5))\r\n    b = tf.constant(np.arange(24).reshape(2, 3, 4))\r\n    print(a.ndim) # 4\r\n    print(b.ndim) # 3\r\n\r\n    y = tf.experimental.numpy.kron(a, b)\r\n\r\n    print(y.shape)\r\nexcept:\r\n    print(\"Can't use tf.experimental.numpy.kron on multi-dimensional arrays\")\r\n\r\nx = np.arange(100).reshape(2, 5, 2, 5)\r\ny = np.arange(24).reshape(2, 3, 4)\r\n\r\nprint(x.ndim) # 4\r\nprint(y.ndim) # 3\r\n\r\nz = np.kron(x, y)\r\n\r\nprint(z.shape) # (2, 10, 6, 20)\n```\n\n\n### Relevant log output\n\n```shell\nUnimplementedError: {{function_node __wrapped__Mul_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-12-16T07:22:01Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83037"},{"issue_number":34,"repository":"tensorflow\/tensorflow","title":"error message is inconsistent with documentation in `tf.raw_ops.MaxPoolGradWithArgmax`","description":"### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nAccording to [Doc tf.raw_ops.MaxPoolGradWithArgmax](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/MaxPoolGradWithArgmax), the argument `argmax` can be `int32` or `int64`. However, after actual testing, the parameter `argmax` can only support tensor input of data type `int64`.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput_tensor = tf.constant(1, shape=[1, 2, 2, 1], dtype=tf.float32)\r\ngrad_tensor = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.float32)\r\nargmax_indices = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.int32)\r\nksize = [1, 2, 2, 1]\r\nstrides = [1, 1, 1, 1]\r\npadding = \"VALID\"\r\n\r\noutput_grad = tf.raw_ops.MaxPoolGradWithArgmax(\r\n    input=input_tensor,\r\n    grad=grad_tensor,\r\n    argmax=argmax_indices,\r\n    ksize=ksize,\r\n    strides=strides,\r\n    padding=padding,\r\n    include_batch_in_index=False\r\n)\n```\n\n\n### Relevant log output\n\n```shell\nNotFoundError: Could not find device for node: {{node MaxPoolGradWithArgmax}} = MaxPoolGradWithArgmax[T=DT_FLOAT, Targmax=DT_INT32, include_batch_in_index=false, ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 1, 1, 1]]\r\nAll kernels registered for op MaxPoolGradWithArgmax:\r\n  device='CPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT32]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT64]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_HALF]; Targmax in [DT_INT64]\r\n [Op:MaxPoolGradWithArgmax] name:\n```\n","labels":["type:docs-bug","stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-12-12T13:14:53Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82837"},{"issue_number":35,"repository":"tensorflow\/tensorflow","title":"Update Python in Docker images to 3.11.x and ditch `3.11.0rc1`","description":"### Issue type\n\nBuild\/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nDocker\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe Python version of the docker images is outdated and should be updated\n\n### Standalone code to reproduce the issue\n\n```shell\ndocker run -it tensorflow\/tensorflow python\n```\n\n\n### Relevant log output\n\n```shell\nPython 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","TF 2.18"],"created_at":"2024-12-09T13:35:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82529"},{"issue_number":36,"repository":"tensorflow\/tensorflow","title":"The warning \"The structure of `inputs` doesn't match the expected structure\" when training a functional model","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.13.1-0-gf841394b1b7 2.13.1 (Nightly: v1.12.1-119104-gf8fd6f53fa3 2.19.0-dev20241204)\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWindows 11 23H2 22631.4460\n\n### Mobile device\n\nWindows 11 23H2 22631.4460\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the model is functional, not Sequential, the warning has occured:\r\n\r\n```\r\nEpoch 1\/5\r\n<path-to-python>\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*\r\n  warnings.warn(\r\n```\r\n\r\nYes, the warning message has interrupted on parenthesis. When I've run the same code in Nightly, the warning message is:\r\n\r\n```\r\nEpoch 1\/5\r\n<path-to-python>\\lib\\site-packages\\keras\\src\\models\\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\r\nExpected: ['keras_tensor']\r\nReceived: inputs=Tensor(shape=(None, 10))\r\n  warnings.warn(msg)\r\n```\r\n\r\nAfter the warning, the training continues normally, but because of this warning, I can't be sure that the model works as I expect.\r\n\r\nI've traced the source and found that in `Lib\\site-packages\\keras\\src\\tree\\optree_impl.py` on line 95 comparasion of expected and actual structure failed. Now I place the traced variables here:\r\n\r\n```\r\n>>> a\r\n<tf.Tensor 'data:0' shape=(None, 10) dtype=float64>\r\n>>> b\r\n[<KerasTensor shape=(None, 10), dtype=float32, sparse=False, name=keras_tensor>]\r\n>>> a_structure\r\nPyTreeSpec(*, NoneIsLeaf)\r\n>>> b_structure\r\nPyTreeSpec([*], NoneIsLeaf)\r\n```\r\n\r\nThe data passed to the `fit` function fully corresponds to the [documentation](https:\/\/keras.io\/api\/models\/model_training_apis\/#fit-method). The warning appears independently of whether I use numpy array or PyDataset as dataset of `fit` function.\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom keras.models import Model\r\nfrom keras.layers import Dense, Input, Flatten, Concatenate\r\nfrom keras import utils\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass SamplesSet(utils.PyDataset):\r\n    \r\n    def __init__(self, batch_size, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.batch_size = batch_size\r\n        \r\n    def __len__(self):\r\n        return 1\r\n    \r\n    def __getitem__(self, idx):\r\n        x1 = np.random.uniform(size=10*self.batch_size).reshape((self.batch_size, 10))\r\n        y = np.arange(self.batch_size)\r\n        return x1, y\r\n    \r\ntrain = SamplesSet(100)\r\nx1_train = np.random.uniform(size=10*100).reshape((100, 10))\r\ny_train = np.arange(100)\r\n\r\ninput1 = Input(shape=(10,))\r\nl1 = Dense(1)(input1)\r\nd2 = Dense(1, activation='sigmoid')(l1)\r\nmodel = Model(inputs=[input1], outputs=[d2])\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\nprint(model.summary())\r\n\r\nhistory = model.fit(x1_train, y_train, epochs=5, verbose=1)\r\n# In the all cases below warning occures too\r\n# history = Model.fit(train, epochs=5, verbose=1) \r\n# ret = model.predict(np.arange(10)[np.newaxis,:])\r\n# ret = model.predict(tf.constant([[0,1,2,3,4,5,6,7,8,9]]))\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","comp:keras","TF 2.13"],"created_at":"2024-12-06T03:47:02Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82372"},{"issue_number":37,"repository":"tensorflow\/tensorflow","title":"[XLA] TF XLA outputs abnormal value when compiling `Embedding`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nnightly\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nFor `Embedding` operator, when I set the `input_dim=1` (which means it indexed from 0 to 0), the output always returns **0 without XLA**.\r\n\r\nAfter compilation, the outputs are usually some random tensors.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\ntf.random.set_seed(42)\r\nx = tf.constant([1])\r\n\r\n\r\n# uncompiled model\r\nclass Model(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.embedding = tf.keras.layers.Embedding(1, 1)\r\n\r\n    def call(self, x):\r\n        output = self.embedding(x)\r\n        return output\r\n\r\n\r\nm = Model()\r\n\r\noutput1 = m(x)\r\n\r\n\r\n# compiled model\r\nclass Model(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.embedding = tf.keras.layers.Embedding(1, 1)\r\n\r\n    @tf.function(jit_compile=True)\r\n    def call(self, x):\r\n        output = self.embedding(x)\r\n        return output\r\n\r\n\r\nm = Model()\r\noutput2 = m(x)\r\n\r\nprint(output1)\r\nprint(output2)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\ntf.Tensor([[0.]], shape=(1, 1), dtype=float32)\r\ntf.Tensor([[-0.00567592]], shape=(1, 1), dtype=float32)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.18"],"created_at":"2024-12-05T13:58:40Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82317"},{"issue_number":38,"repository":"tensorflow\/tensorflow","title":"Are checkpoints broken in >= 2.16?","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.16, 2.17\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe example given in https:\/\/www.tensorflow.org\/guide\/checkpoint does not seem to work as expected in 2.16 and 2.17, while working fine in 2.15. After restoring and restarting the training process, it starts training from the very beginning.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1n76Mu5BhdBJBSXc7cXYJMr0lMDER2JRa?usp=sharing\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:apis","2.17"],"created_at":"2024-12-04T15:11:53Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82209"},{"issue_number":39,"repository":"tensorflow\/tensorflow","title":"TPU not support TensorFlow 2.18 and 2.17.1","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.18 and tf. 2.17.1\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`import tensorflow as tf` results `segmentation fault core dumped`\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:tpus","TF 2.18"],"created_at":"2024-12-04T14:56:53Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82208"},{"issue_number":40,"repository":"tensorflow\/tensorflow","title":"Clarify the `constant_op.constant(2)` statement","description":"It would be helpful to clarify the `constant_op.constant(2)` statement by explaining the corresponding import statement.\r\n\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/5bc9d26649cca274750ad3625bd93422617eed4b\/tensorflow\/python\/ops\/summary_ops_v2.py#L1062-L1066","labels":["type:bug","comp:ops"],"created_at":"2024-12-02T18:05:41Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/81954"},{"issue_number":41,"repository":"tensorflow\/tensorflow","title":"MixedPrecision + XLA: Seen floating point types of different precisions","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nGoogle Colab\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\nGoogle Colab default\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen using bilinear interpolation + XLA + mixed_float16 policy issue raises during compilation.\r\nWithout bilinear interpolation or without XLA or without mixed_float16 there is no issue.\r\n\r\nIn Google Colab i have this issue only on CPU with TF 2.17 and both CPU & GPU with TF 2.18.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nhttps:\/\/colab.research.google.com\/drive\/1joSiScbM7Stc9bn1C4R_4sFkDzakrTsJ?usp=sharing\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-4-9cc273be2d5a> in <cell line: 28>()\r\n     26 model.compile(loss='mse', optimizer='adam', run_eagerly=False, jit_compile=True)\r\n     27 \r\n---> 28 model.fit(dataset)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51   try:\r\n     52     ctx.ensure_initialized()\r\n---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                         inputs, attrs, num_outputs)\r\n     55   except core._NotOkStatusException as e:\r\n\r\nInternalError: Graph execution error:\r\n\r\nDetected at node StatefulPartitionedCall defined at (most recent call last):\r\n  File \"\/usr\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\n\r\n  File \"\/usr\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/colab_kernel_launcher.py\", line 37, in <module>\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/traitlets\/config\/application.py\", line 992, in launch_instance\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelapp.py\", line 619, in start\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/platform\/asyncio.py\", line 195, in start\r\n\r\n  File \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\n\r\n  File \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\n\r\n  File \"\/usr\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 685, in <lambda>\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 738, in _run_callback\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 825, in inner\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 377, in dispatch_queue\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 250, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 748, in __init__\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 361, in process_one\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 261, in dispatch_shell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 539, in execute_request\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 2975, in run_cell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3030, in _run_cell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/async_helpers.py\", line 78, in _pseudo_sync_runner\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3257, in run_cell_async\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3473, in run_ast_nodes\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3553, in run_code\r\n\r\n  File \"<ipython-input-4-9cc273be2d5a>\", line 28, in <cell line: 28>\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 368, in fit\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 216, in function\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 129, in multi_step_on_iterator\r\n\r\nduring context [Unknown]: Seen floating point types of different precisions in %multiply.43589 = f32[2,8,8,1280]{3,2,1,0} multiply(f32[2,8,8,1280]{3,2,1,0} %add.43539, f16[2,8,8,1280]{3,2,1,0} %multiply.43588), metadata={op_type=\"Mul\" op_name=\"mul_9\" source_file=\"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/framework\/ops.py\" source_line=1196}, but mixed precision is disallowed.\r\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_124474]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.18"],"created_at":"2024-11-29T07:11:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/81273"},{"issue_number":42,"repository":"tensorflow\/tensorflow","title":"Some operators give different results on CPU and GPU when dealing with complex numbers that include `inf`.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04.3\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe outputs of TensorFlow mathematical APIs (`sin, cos, tan, sinh, cosh, exp, and reduce_mean`) are inconsistent between the CPU and GPU when applied to complex inputs containing `inf`. \n\n### Standalone code to reproduce the issue\n\n```shell\npython\r\nimport tensorflow as tf\r\n\r\ntest_inputs = [\r\n    tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128),\r\n]\r\n\r\ntest_apis = [\r\n    tf.math.sin, tf.math.cos, tf.math.tan,\r\n    tf.math.sinh, tf.math.cosh, tf.math.exp, tf.math.reduce_mean\r\n]\r\n\r\nfor api in test_apis:\r\n    print(f\"Testing {api.__name__}\")\r\n    for x in test_inputs:\r\n        try:\r\n            with tf.device('\/CPU'):\r\n              cpu_out = api(x)\r\n              print(f\"CPU Output: {cpu_out}\")\r\n            with tf.device('\/GPU:0'):\r\n              gpu_out = api(x)\r\n              print(f\"GPU Output: {gpu_out}\")\r\n        except Exception as e:\r\n            print(f\"Error in {api.__name__}: {e}\")\n```\n\n\n### Relevant log output\n\n```shell\nTesting sin\r\nCPU Output: [nan +0.j  0.+infj nan+infj]\r\nGPU Output: [nan+nanj nan+infj nan+nanj]\r\nTesting cos\r\nCPU Output: [nan +0.j inf -0.j inf+nanj]\r\nGPU Output: [nan+nanj inf+nanj nan+nanj]\r\nTesting tan\r\nCPU Output: [nan+0.j  0.+1.j  0.+1.j]\r\nGPU Output: [nan+nanj  0. +1.j  0. +1.j]\r\nTesting sinh\r\nCPU Output: [inf +0.j  0.+nanj inf+nanj]\r\nGPU Output: [inf+nanj nan+nanj nan+nanj]\r\nTesting cosh\r\nCPU Output: [inf +0.j nan +0.j inf+nanj]\r\nGPU Output: [inf+nanj nan+nanj nan+nanj]\r\nTesting exp\r\nCPU Output: [inf +0.j nan+nanj inf+nanj]\r\nGPU Output: [inf+nanj nan+nanj nan+nanj]\r\nTesting reduce_mean\r\nCPU Output: (inf+infj)\r\nGPU Output: (nan+nanj)\n```\n","labels":["stat:awaiting response","type:bug","stale","comp:ops","2.17"],"created_at":"2024-11-27T13:13:05Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80947"},{"issue_number":43,"repository":"tensorflow\/tensorflow","title":"When using `tf.math.log1p` and NumPy's `np.log1p` with the same complex input, the outputs are inconsistent.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04.3\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow's `tf.math.log1p` produces inconsistent results with NumPy's `np.log1p` for complex inputs containing `inf`, such as `[inf+0.j, 0+inf.j, inf+inf.j]`. TensorFlow outputs `[inf+0.j, nan+nanj, nan+nanj]`, while NumPy returns `[inf+0.j, inf+1.57079633j, inf+0.78539816j]`.\n\n### Standalone code to reproduce the issue\n\n```shell\npython\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntest_input = tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128)\r\n\r\n# TensorFlow computation\r\nwith tf.device('\/CPU:0'):\r\n    cpu_out = tf.math.log1p(test_input)\r\n\r\n# NumPy computation\r\nnumpy_out = np.log1p(test_input.numpy())\r\n\r\nprint(f\"CPU Output: {cpu_out}\")\r\nprint(f\"NumPy Output: {numpy_out}\")\n```\n\n\n### Relevant log output\n\n```shell\nCPU Output: [inf +0.j nan+nanj nan+nanj]\r\nNumPy Output: [inf+0.j         inf+1.57079633j inf+0.78539816j]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-26T13:36:51Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80850"},{"issue_number":44,"repository":"tensorflow\/tensorflow","title":"Heap-buffer-overflow in `SparseMatrixSparseCholesky`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen batch_index is scalar, `SparseMatrixSparseCholesky` triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nindices = tf.constant([[0, 0], [1, 1], [2, 1], [2, 2], [3, 3]], dtype=tf.int64)\r\nvalues = tf.constant([1.0, 2.0, 1.0, 3.0, 4.0], tf.float32)\r\ndense_shape = tf.constant([4, 4], dtype=tf.int64)\r\ninput = tf.raw_ops.SparseTensorToCSRSparseMatrix(\r\n    indices=indices, values=values, dense_shape=dense_shape, name=None\r\n)\r\npermutation = tf.constant([4,1,1,1], dtype=tf.int32)\r\n\r\ntf.raw_ops.SparseMatrixSparseCholesky(\r\n  input=input, permutation=permutation, type=tf.float32\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n=================================================================\r\n==3331846==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60700049d1d0 at pc 0x7faa202d6d87 bp 0x7ffd09bf8fe0 sp 0x7ffd09bf8fd0\r\nWRITE of size 4 at 0x60700049d1d0 thread T0\r\n    #0 0x7faa202d6d86 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2a2d86)\r\n    #1 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f373a)\r\n    #2 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f84e9)\r\n    #3 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x4201d41)\r\n    #4 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x26d8d71)\r\n    #5 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2adab6)\r\n    #6 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1cdc14a)\r\n    #7 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1862fe6)\r\n    #8 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x173f306)\r\n    #9 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x179cf74)\r\n    #10 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x17aa8b1)\r\n    #11 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c582eb)\r\n    #12 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aedc2c)\r\n    #13 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31af066a)\r\n    #14 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c2a939)\r\n    #15 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31addde4)\r\n    #16 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31ae1dd4)\r\n    #17 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aebd26)\r\n    #18 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x20ad4c33)\r\n    #19 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c1fe5e)\r\n    #20 0x7fa9f268645b in TFE_Execute (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x1065245b)\r\n    #21 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/_pywrap_tensorflow_internal.so+0x3c2274)\r\n    #22 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0xb7ccb)\r\n    #23 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0x1ce899)\r\n    #24 0x51ad66  (\/usr\/bin\/python3.11+0x51ad66)\r\n    #25 0x4e75db in _PyObject_MakeTpCall (\/usr\/bin\/python3.11+0x4e75db)\r\n    #26 0x4fb151 in _PyEval_EvalFrameDefault (\/usr\/bin\/python3.11+0x4fb151)\r\n    #27 0x531822 in _PyFunction_Vectorcall (\/usr\/bin\/python3.11+0x531822)\r\n    #28 0x541194 in PyObject_Call (\/usr\/bin\/python3.11+0x541194)\r\n    #29 0x4fefe0 in _PyEval_EvalFrameDefault (\/usr\/bin\/python3.11+0x4fefe0)\r\n    #30 0x62e1b3  (\/usr\/bin\/python3.11+0x62e1b3)\r\n    #31 0x4f3a66 in PyEval_EvalCode (\/usr\/bin\/python3.11+0x4f3a66)\r\n    #32 0x647c36  (\/usr\/bin\/python3.11+0x647c36)\r\n    #33 0x64534f  (\/usr\/bin\/python3.11+0x64534f)\r\n    #34 0x650d14  (\/usr\/bin\/python3.11+0x650d14)\r\n    #35 0x650a63 in _PyRun_SimpleFileObject (\/usr\/bin\/python3.11+0x650a63)\r\n    #36 0x650832 in _PyRun_AnyFileObject (\/usr\/bin\/python3.11+0x650832)\r\n    #37 0x64f786 in Py_RunMain (\/usr\/bin\/python3.11+0x64f786)\r\n    #38 0x61ee0c in Py_BytesMain (\/usr\/bin\/python3.11+0x61ee0c)\r\n    #39 0x7faaf80d1d8f  (\/lib\/x86_64-linux-gnu\/libc.so.6+0x29d8f)\r\n    #40 0x7faaf80d1e3f in __libc_start_main (\/lib\/x86_64-linux-gnu\/libc.so.6+0x29e3f)\r\n    #41 0x61ec94 in _start (\/usr\/bin\/python3.11+0x61ec94)\r\n\r\n0x60700049d1d0 is located 0 bytes to the right of 80-byte region [0x60700049d180,0x60700049d1d0)\r\nallocated by thread T0 here:\r\n    #0 0x7faaf84bf887 in __interceptor_malloc ..\/..\/..\/..\/src\/libsanitizer\/asan\/asan_malloc_linux.cpp:145\r\n    #1 0x7faa202d6803 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2a2803)\r\n    #2 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f373a)\r\n    #3 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f84e9)\r\n    #4 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x4201d41)\r\n    #5 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x26d8d71)\r\n    #6 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2adab6)\r\n    #7 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1cdc14a)\r\n    #8 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1862fe6)\r\n    #9 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x173f306)\r\n    #10 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x179cf74)\r\n    #11 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x17aa8b1)\r\n    #12 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c582eb)\r\n    #13 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aedc2c)\r\n    #14 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31af066a)\r\n    #15 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c2a939)\r\n    #16 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31addde4)\r\n    #17 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31ae1dd4)\r\n    #18 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aebd26)\r\n    #19 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x20ad4c33)\r\n    #20 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c1fe5e)\r\n    #21 0x7fa9f268645b in TFE_Execute (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x1065245b)\r\n    #22 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/_pywrap_tensorflow_internal.so+0x3c2274)\r\n    #23 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0xb7ccb)\r\n    #24 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0x1ce899)\r\n    #25 0x51ad66  (\/usr\/bin\/python3.11+0x51ad66)\r\n\r\nSUMMARY: AddressSanitizer: heap-buffer-overflow (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2a2d86) in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const\r\nShadow bytes around the buggy address:\r\n  0x0c0e8008b9e0: 00 00 00 00 00 00 fa fa fa fa fd fd fd fd fd fd\r\n  0x0c0e8008b9f0: fd fd fd fd fa fa fa fa 00 00 00 00 00 00 00 00\r\n  0x0c0e8008ba00: 00 fa fa fa fa fa 00 00 00 00 00 00 00 00 04 fa\r\n  0x0c0e8008ba10: fa fa fa fa 00 00 00 00 00 00 00 00 04 fa fa fa\r\n  0x0c0e8008ba20: fa fa 00 00 00 00 00 00 00 00 04 fa fa fa fa fa\r\n=>0x0c0e8008ba30: 00 00 00 00 00 00 00 00 00 00[fa]fa fa fa fa fa\r\n  0x0c0e8008ba40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba50: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba60: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba70: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\nShadow byte legend (one shadow byte represents 8 application bytes):\r\n  Addressable:           00\r\n  Partially addressable: 01 02 03 04 05 06 07\r\n  Heap left redzone:       fa\r\n  Freed heap region:       fd\r\n  Stack left redzone:      f1\r\n  Stack mid redzone:       f2\r\n  Stack right redzone:     f3\r\n  Stack after return:      f5\r\n  Stack use after scope:   f8\r\n  Global redzone:          f9\r\n  Global init order:       f6\r\n  Poisoned by user:        f7\r\n  Container overflow:      fc\r\n  Array cookie:            ac\r\n  Intra object redzone:    bb\r\n  ASan internal:           fe\r\n  Left alloca redzone:     ca\r\n  Right alloca redzone:    cb\r\n  Shadow gap:              cc\r\n==3331846==ABORTING\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-26T12:42:50Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80847"},{"issue_number":45,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `RaggedBincount`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the value of size is close to the maximum value for the dtype, an integer overflow error will occur in following multiplication calculations.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nsplits = tf.constant([0, 3, 5, 9], dtype=tf.int64)\r\nvalues = tf.constant(1, shape=[3,3], dtype=tf.int64)\r\nsize = tf.constant(6522107765268123892, dtype=tf.int64)\r\nweights = tf.constant(1, shape=[3,3], dtype=tf.float32)\r\ncounts = tf.raw_ops.RaggedBincount(splits=splits, values=values, size=size, weights=weights)\n```\n\n\n### Relevant log output\n\n```shell\nStatus: INVALID_ARGUMENT: Encountered overflow when multiplying 3 with 6522107765268123892, result: -1\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-26T03:18:31Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80812"},{"issue_number":46,"repository":"tensorflow\/tensorflow","title":"This method creates a model with a 100% memory leak loop using model. fit()","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nubuntu 2.2 or mac m1\r\n\r\n### Mobile device\r\n\r\nubuntu 2.2 or mac m1\r\n\r\n### Python version\r\n\r\n3.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport gc\r\n\r\nimport keras\r\nimport numpy as np\r\nimport psutil\r\nfrom keras.optimizers import Adam\r\nfrom keras.layers import Dense, Dropout, Input, LSTM\r\nimport tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom tensorflow.keras.models import Sequential, load_model\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\nimport time\r\nimport json\r\n\r\n\r\nnum_samples = 6\r\nnum_features = 3\r\nnum_classes = 4\r\nepochs = 50\r\nbatch_size = 2\r\nidentifier = \"test_model\"\r\nnum_iterations = 500  \r\n\r\ndef build_model(X, num_classes):\r\n    model = Sequential()\r\n    model.add(Input(shape=(X.shape[1], X.shape[2])))\r\n    model.add(LSTM(16, return_sequences=True))\r\n    model.add(LSTM(16))\r\n    model.add(Dropout(0.4))\r\n    model.add(Dense(8, activation='tanh'))\r\n    model.add(Dense(num_classes, activation='softmax'))\r\n\r\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])\r\n    return model\r\n\r\n\r\ndata_X = np.random.rand(num_samples, num_features)\r\ndata_Y = np.random.randint(0, num_classes, size=(num_samples, 1))  \r\n\r\n\r\ndata_Y = np.eye(num_classes)[data_Y.flatten()]  \r\nprint(type(data_X))\r\n\r\nscaler = MinMaxScaler()\r\ndata_X_scaled = scaler.fit_transform(data_X)\r\n\r\n\r\ntrain_X, test_X, train_Y, test_Y = train_test_split(data_X_scaled, data_Y, train_size=0.6, random_state=42)\r\n\r\n\r\ntrain_X = np.expand_dims(train_X, axis=1)\r\ntest_X = np.expand_dims(test_X, axis=1)\r\n\r\nbest_loss = np.inf\r\nbest_model_data = None\r\nfor iteration in range(num_iterations):\r\n   \r\n    tf.keras.backend.clear_session()\r\n    tf.compat.v1.reset_default_graph()\r\n    model = build_model(train_X, num_classes)\r\n \r\n    model_name = f\"model_{iteration}\"\r\n    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=0, restore_best_weights=True)\r\n    print(f\"Iteration {iteration + 1}\/{num_iterations}\")\r\n    process = psutil.Process()\r\n    mem_info = process.memory_info()\r\n    print(f\"start Current memory usage: {mem_info.rss \/ (1024 * 1024):.2f} MB\")  # RSS - Resident Set Size\r\n    try:\r\n        history = model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, shuffle=True,\r\n                            validation_data=(test_X, test_Y), verbose=0)\r\n        current_loss = history.history['loss'][-1]\r\n        print(f\"Training model: {model.name}\")\r\n    \r\n        del model\r\n        tf.keras.backend.clear_session()\r\n        gc.collect()\r\n    except Exception as e:\r\n        print(\"err:\", e)\r\n    finally:\r\n        process = psutil.Process()\r\n        mem_info = process.memory_info()\r\n        print(f\"end Current memory usage: {mem_info.rss \/ (1024 * 1024):.2f} MB\")  # RSS - Resident Set Size\r\n\r\nprint(\"end\uff01\")\r\n\r\n\r\nif best_model_data:\r\n    model_json = best_model_data[\"model_architecture\"]\r\n    model_weights = json.loads(best_model_data[\"model_weights\"], object_hook=lambda d: np.array(d))\r\n    model = tf.keras.models.model_from_json(model_json)\r\n    model.set_weights(model_weights)\r\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])\r\n    print(\"ok\")\r\nelse:\r\n    print(\"not found\")\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nIteration 1\/500\r\nstart Current memory usage: 450.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 524.41 MB\r\nIteration 2\/500\r\nstart Current memory usage: 524.52 MB\r\nTraining model: sequential\r\nend Current memory usage: 564.97 MB\r\nIteration 3\/500\r\nstart Current memory usage: 564.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 598.00 MB\r\nIteration 4\/500\r\nstart Current memory usage: 598.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 624.69 MB\r\nIteration 5\/500\r\nstart Current memory usage: 624.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 653.89 MB\r\nIteration 6\/500\r\nstart Current memory usage: 653.91 MB\r\nTraining model: sequential\r\nend Current memory usage: 679.45 MB\r\nIteration 7\/500\r\nstart Current memory usage: 679.45 MB\r\nTraining model: sequential\r\nend Current memory usage: 701.59 MB\r\nIteration 8\/500\r\nstart Current memory usage: 701.59 MB\r\nTraining model: sequential\r\nend Current memory usage: 726.83 MB\r\nIteration 9\/500\r\nstart Current memory usage: 726.84 MB\r\nTraining model: sequential\r\nend Current memory usage: 749.56 MB\r\nIteration 10\/500\r\nstart Current memory usage: 749.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 782.56 MB\r\nIteration 11\/500\r\nstart Current memory usage: 782.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 805.92 MB\r\nIteration 12\/500\r\nstart Current memory usage: 805.92 MB\r\nTraining model: sequential\r\nend Current memory usage: 833.17 MB\r\nIteration 13\/500\r\nstart Current memory usage: 833.17 MB\r\nTraining model: sequential\r\nend Current memory usage: 852.84 MB\r\nIteration 14\/500\r\nstart Current memory usage: 852.84 MB\r\nTraining model: sequential\r\nend Current memory usage: 875.05 MB\r\nIteration 15\/500\r\nstart Current memory usage: 875.06 MB\r\nTraining model: sequential\r\nend Current memory usage: 901.56 MB\r\nIteration 16\/500\r\nstart Current memory usage: 901.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 930.62 MB\r\nIteration 17\/500\r\nstart Current memory usage: 705.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 762.64 MB\r\nIteration 18\/500\r\nstart Current memory usage: 762.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 798.06 MB\r\nIteration 19\/500\r\nstart Current memory usage: 798.17 MB\r\nTraining model: sequential\r\nend Current memory usage: 824.98 MB\r\nIteration 20\/500\r\nstart Current memory usage: 824.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 850.34 MB\r\nIteration 21\/500\r\nstart Current memory usage: 850.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 876.81 MB\r\nIteration 22\/500\r\nstart Current memory usage: 876.81 MB\r\nTraining model: sequential\r\nend Current memory usage: 904.02 MB\r\nIteration 23\/500\r\nstart Current memory usage: 904.08 MB\r\nTraining model: sequential\r\nend Current memory usage: 929.70 MB\r\nIteration 24\/500\r\nstart Current memory usage: 929.73 MB\r\nTraining model: sequential\r\nend Current memory usage: 952.33 MB\r\nIteration 25\/500\r\nstart Current memory usage: 952.34 MB\r\nTraining model: sequential\r\nend Current memory usage: 952.28 MB\r\nIteration 26\/500\r\nstart Current memory usage: 952.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 980.39 MB\r\nIteration 27\/500\r\nstart Current memory usage: 978.78 MB\r\nTraining model: sequential\r\nend Current memory usage: 999.02 MB\r\nIteration 28\/500\r\nstart Current memory usage: 999.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1023.50 MB\r\nIteration 29\/500\r\nstart Current memory usage: 1023.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1047.80 MB\r\nIteration 30\/500\r\nstart Current memory usage: 1047.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1068.88 MB\r\nIteration 31\/500\r\nstart Current memory usage: 1068.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1095.78 MB\r\nIteration 32\/500\r\nstart Current memory usage: 1095.78 MB\r\nTraining model: sequential\r\nend Current memory usage: 1119.03 MB\r\nIteration 33\/500\r\nstart Current memory usage: 1119.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 1039.41 MB\r\nIteration 34\/500\r\nstart Current memory usage: 1022.78 MB\r\nTraining model: sequential\r\nend Current memory usage: 1040.88 MB\r\nIteration 35\/500\r\nstart Current memory usage: 1040.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 1054.58 MB\r\nIteration 36\/500\r\nstart Current memory usage: 1054.58 MB\r\nTraining model: sequential\r\nend Current memory usage: 1076.16 MB\r\nIteration 37\/500\r\nstart Current memory usage: 1076.19 MB\r\nTraining model: sequential\r\nend Current memory usage: 1097.02 MB\r\nIteration 38\/500\r\nstart Current memory usage: 1097.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 1113.70 MB\r\nIteration 39\/500\r\nstart Current memory usage: 1114.12 MB\r\nTraining model: sequential\r\nend Current memory usage: 1140.30 MB\r\nIteration 40\/500\r\nstart Current memory usage: 1140.33 MB\r\nTraining model: sequential\r\nend Current memory usage: 1163.81 MB\r\nIteration 41\/500\r\nstart Current memory usage: 1163.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 1195.83 MB\r\nIteration 42\/500\r\nstart Current memory usage: 1195.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1221.53 MB\r\nIteration 43\/500\r\nstart Current memory usage: 1221.55 MB\r\nTraining model: sequential\r\nend Current memory usage: 1231.09 MB\r\nIteration 44\/500\r\nstart Current memory usage: 1231.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1245.78 MB\r\nIteration 45\/500\r\nstart Current memory usage: 1199.55 MB\r\nTraining model: sequential\r\nend Current memory usage: 1221.59 MB\r\nIteration 46\/500\r\nstart Current memory usage: 1221.59 MB\r\nTraining model: sequential\r\nend Current memory usage: 1249.11 MB\r\nIteration 47\/500\r\nstart Current memory usage: 1249.22 MB\r\nTraining model: sequential\r\nend Current memory usage: 1275.50 MB\r\nIteration 48\/500\r\nstart Current memory usage: 1259.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1290.91 MB\r\nIteration 49\/500\r\nstart Current memory usage: 1285.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1296.75 MB\r\nIteration 50\/500\r\nstart Current memory usage: 1296.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1306.59 MB\r\nIteration 51\/500\r\nstart Current memory usage: 1306.59 MB\r\nTraining model: sequential\r\nend Current memory usage: 1287.53 MB\r\nIteration 52\/500\r\nstart Current memory usage: 1287.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1297.23 MB\r\nIteration 53\/500\r\nstart Current memory usage: 1297.25 MB\r\nTraining model: sequential\r\nend Current memory usage: 1285.45 MB\r\nIteration 54\/500\r\nstart Current memory usage: 1285.45 MB\r\nTraining model: sequential\r\nend Current memory usage: 1290.36 MB\r\nIteration 55\/500\r\nstart Current memory usage: 1282.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1302.14 MB\r\nIteration 56\/500\r\nstart Current memory usage: 1302.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1287.70 MB\r\nIteration 57\/500\r\nstart Current memory usage: 1287.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1282.77 MB\r\nIteration 58\/500\r\nstart Current memory usage: 1271.38 MB\r\nTraining model: sequential\r\nend Current memory usage: 1232.14 MB\r\nIteration 59\/500\r\nstart Current memory usage: 1212.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 1201.16 MB\r\nIteration 60\/500\r\nstart Current memory usage: 1200.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1169.45 MB\r\nIteration 61\/500\r\nstart Current memory usage: 1169.45 MB\r\nTraining model: sequential\r\nend Current memory usage: 1209.73 MB\r\nIteration 62\/500\r\nstart Current memory usage: 1207.19 MB\r\nTraining model: sequential\r\nend Current memory usage: 1226.28 MB\r\nIteration 63\/500\r\nstart Current memory usage: 1226.28 MB\r\nTraining model: sequential\r\nend Current memory usage: 1231.45 MB\r\nIteration 64\/500\r\nstart Current memory usage: 1210.11 MB\r\nTraining model: sequential\r\nend Current memory usage: 1176.00 MB\r\nIteration 65\/500\r\nstart Current memory usage: 1173.97 MB\r\nTraining model: sequential\r\nend Current memory usage: 1201.42 MB\r\nIteration 66\/500\r\nstart Current memory usage: 1201.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 1223.94 MB\r\nIteration 67\/500\r\nstart Current memory usage: 1222.50 MB\r\nTraining model: sequential\r\nend Current memory usage: 1229.80 MB\r\nIteration 68\/500\r\nstart Current memory usage: 1227.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1219.02 MB\r\nIteration 69\/500\r\nstart Current memory usage: 1210.48 MB\r\nTraining model: sequential\r\nend Current memory usage: 1247.17 MB\r\nIteration 70\/500\r\nstart Current memory usage: 1245.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1259.84 MB\r\nIteration 71\/500\r\nstart Current memory usage: 1259.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 1286.39 MB\r\nIteration 72\/500\r\nstart Current memory usage: 1286.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1316.52 MB\r\nIteration 73\/500\r\nstart Current memory usage: 1311.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1338.72 MB\r\nIteration 74\/500\r\nstart Current memory usage: 1338.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1348.45 MB\r\nIteration 75\/500\r\nstart Current memory usage: 1338.30 MB\r\nTraining model: sequential\r\nend Current memory usage: 1354.97 MB\r\nIteration 76\/500\r\nstart Current memory usage: 1353.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1385.67 MB\r\nIteration 77\/500\r\nstart Current memory usage: 1385.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1408.83 MB\r\nIteration 78\/500\r\nstart Current memory usage: 1408.88 MB\r\nTraining model: sequential\r\nend Current memory usage: 1430.91 MB\r\nIteration 79\/500\r\nstart Current memory usage: 1430.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1443.62 MB\r\nIteration 80\/500\r\nstart Current memory usage: 1428.00 MB\r\nTraining model: sequential\r\nend Current memory usage: 1436.50 MB\r\nIteration 81\/500\r\nstart Current memory usage: 1436.64 MB\r\nTraining model: sequential\r\nend Current memory usage: 1454.66 MB\r\nIteration 82\/500\r\nstart Current memory usage: 1440.91 MB\r\nTraining model: sequential\r\nend Current memory usage: 1461.81 MB\r\nIteration 83\/500\r\nstart Current memory usage: 1460.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 1481.19 MB\r\nIteration 84\/500\r\nstart Current memory usage: 1481.19 MB\r\nTraining model: sequential\r\nend Current memory usage: 1477.84 MB\r\nIteration 85\/500\r\nstart Current memory usage: 1477.84 MB\r\nTraining model: sequential\r\nend Current memory usage: 1493.55 MB\r\nIteration 86\/500\r\nstart Current memory usage: 1493.58 MB\r\nTraining model: sequential\r\nend Current memory usage: 1509.50 MB\r\nIteration 87\/500\r\nstart Current memory usage: 1509.50 MB\r\nTraining model: sequential\r\nend Current memory usage: 1543.94 MB\r\nIteration 88\/500\r\nstart Current memory usage: 1542.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1516.17 MB\r\nIteration 89\/500\r\nstart Current memory usage: 1516.20 MB\r\nTraining model: sequential\r\nend Current memory usage: 1470.17 MB\r\nIteration 90\/500\r\nstart Current memory usage: 1470.22 MB\r\nTraining model: sequential\r\nend Current memory usage: 1443.72 MB\r\nIteration 91\/500\r\nstart Current memory usage: 1444.36 MB\r\nTraining model: sequential\r\nend Current memory usage: 1486.23 MB\r\nIteration 92\/500\r\nstart Current memory usage: 1476.41 MB\r\nTraining model: sequential\r\nend Current memory usage: 1524.97 MB\r\nIteration 93\/500\r\nstart Current memory usage: 1524.97 MB\r\nTraining model: sequential\r\nend Current memory usage: 1534.94 MB\r\nIteration 94\/500\r\nstart Current memory usage: 1551.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1853.48 MB\r\nIteration 95\/500\r\nstart Current memory usage: 1853.48 MB\r\nTraining model: sequential\r\nend Current memory usage: 1790.12 MB\r\nIteration 96\/500\r\nstart Current memory usage: 1792.27 MB\r\nTraining model: sequential\r\nend Current memory usage: 1883.20 MB\r\nIteration 97\/500\r\nstart Current memory usage: 1879.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1759.69 MB\r\nIteration 98\/500\r\nstart Current memory usage: 1669.66 MB\r\nTraining model: sequential\r\nend Current memory usage: 1596.77 MB\r\nIteration 99\/500\r\nstart Current memory usage: 1597.12 MB\r\nTraining model: sequential\r\nend Current memory usage: 1568.83 MB\r\nIteration 100\/500\r\nstart Current memory usage: 1532.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1516.75 MB\r\nIteration 101\/500\r\nstart Current memory usage: 1465.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1486.66 MB\r\nIteration 102\/500\r\nstart Current memory usage: 1483.34 MB\r\nTraining model: sequential\r\nend Current memory usage: 1523.19 MB\r\nIteration 103\/500\r\nstart Current memory usage: 1523.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1532.77 MB\r\nIteration 104\/500\r\nstart Current memory usage: 1531.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1561.78 MB\r\nIteration 105\/500\r\nstart Current memory usage: 1555.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1586.70 MB\r\nIteration 106\/500\r\nstart Current memory usage: 1586.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1608.41 MB\r\nIteration 107\/500\r\nstart Current memory usage: 1603.81 MB\r\nTraining model: sequential\r\nend Current memory usage: 1629.00 MB\r\nIteration 108\/500\r\nstart Current memory usage: 1629.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1609.25 MB\r\nIteration 109\/500\r\nstart Current memory usage: 1609.31 MB\r\nTraining model: sequential\r\nend Current memory usage: 1630.09 MB\r\nIteration 110\/500\r\nstart Current memory usage: 1629.20 MB\r\nTraining model: sequential\r\nend Current memory usage: 1638.66 MB\r\nIteration 111\/500\r\nstart Current memory usage: 1620.30 MB\r\nTraining model: sequential\r\nend Current memory usage: 1642.81 MB\r\nIteration 112\/500\r\nstart Current memory usage: 1642.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1659.45 MB\r\nIteration 113\/500\r\nstart Current memory usage: 1655.17 MB\r\nTraining model: sequential\r\nend Current memory usage: 1687.80 MB\r\nIteration 114\/500\r\nstart Current memory usage: 1673.33 MB\r\nTraining model: sequential\r\nend Current memory usage: 1705.94 MB\r\nIteration 115\/500\r\nstart Current memory usage: 1699.95 MB\r\nTraining model: sequential\r\nend Current memory usage: 1708.22 MB\r\nIteration 116\/500\r\nstart Current memory usage: 1707.88 MB\r\nTraining model: sequential\r\nend Current memory usage: 1648.23 MB\r\nIteration 117\/500\r\nstart Current memory usage: 1634.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 1670.97 MB\r\nIteration 118\/500\r\nstart Current memory usage: 1671.97 MB\r\nTraining model: sequential\r\nend Current memory usage: 1649.69 MB\r\nIteration 119\/500\r\nstart Current memory usage: 1645.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1698.64 MB\r\nIteration 120\/500\r\nstart Current memory usage: 1699.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1737.67 MB\r\nIteration 121\/500\r\nstart Current memory usage: 1737.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1738.05 MB\r\nIteration 122\/500\r\nstart Current memory usage: 1721.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 1730.64 MB\r\nIteration 123\/500\r\nstart Current memory usage: 1729.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1766.12 MB\r\nIteration 124\/500\r\nstart Current memory usage: 1761.22 MB\r\nTraining model: sequential\r\nend Current memory usage: 1796.58 MB\r\nIteration 125\/500\r\nstart Current memory usage: 1796.73 MB\r\nTraining model: sequential\r\nend Current memory usage: 1709.02 MB\r\nIteration 126\/500\r\nstart Current memory usage: 1721.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1771.50 MB\r\nIteration 127\/500\r\nstart Current memory usage: 1771.50 MB\r\nTraining model: sequential\r\nend Current memory usage: 1777.38 MB\r\nIteration 128\/500\r\nstart Current memory usage: 1757.58 MB\r\nTraining model: sequential\r\nend Current memory usage: 1806.50 MB\r\nIteration 129\/500\r\nstart Current memory usage: 1758.81 MB\r\nTraining model: sequential\r\nend Current memory usage: 1812.45 MB\r\nIteration 130\/500\r\nstart Current memory usage: 1812.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 1811.14 MB\r\nIteration 131\/500\r\nstart Current memory usage: 1799.61 MB\r\nTraining model: sequential\r\nend Current memory usage: 1835.33 MB\r\nIteration 132\/500\r\nstart Current memory usage: 1716.38 MB\r\nTraining model: sequential\r\nend Current memory usage: 1759.75 MB\r\nIteration 133\/500\r\nstart Current memory usage: 1752.44 MB\r\nTraining model: sequential\r\nend Current memory usage: 1818.41 MB\r\nIteration 134\/500\r\nstart Current memory usage: 1811.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 1853.58 MB\r\nIteration 135\/500\r\nstart Current memory usage: 1853.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 1858.50 MB\r\nIteration 136\/500\r\nstart Current memory usage: 1858.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 1874.84 MB\r\nIteration 137\/500\r\nstart Current memory usage: 1862.92 MB\r\nTraining model: sequential\r\nend Current memory usage: 1768.23 MB\r\nIteration 138\/500\r\nstart Current memory usage: 1762.73 MB\r\nTraining model: sequential\r\nend Current memory usage: 1843.39 MB\r\nIteration 139\/500\r\nstart Current memory usage: 1843.52 MB\r\nTraining model: sequential\r\nend Current memory usage: 1885.88 MB\r\nIteration 140\/500\r\nstart Current memory usage: 1885.95 MB\r\nTraining model: sequential\r\nend Current memory usage: 1924.86 MB\r\nIteration 141\/500\r\nstart Current memory usage: 1925.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1946.80 MB\r\nIteration 142\/500\r\nstart Current memory usage: 1946.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1977.53 MB\r\nIteration 143\/500\r\nstart Current memory usage: 1974.27 MB\r\nTraining model: sequential\r\nend Current memory usage: 1995.17 MB\r\nIteration 144\/500\r\nstart Current memory usage: 1992.41 MB\r\nTraining model: sequential\r\nend Current memory usage: 1984.45 MB\r\nIteration 145\/500\r\nstart Current memory usage: 1963.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 1947.31 MB\r\nIteration 146\/500\r\nstart Current memory usage: 1944.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 1996.00 MB\r\nIteration 147\/500\r\nstart Current memory usage: 1996.08 MB\r\nTraining model: sequential\r\nend Current memory usage: 2008.41 MB\r\nIteration 148\/500\r\nstart Current memory usage: 1999.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1951.30 MB\r\nIteration 149\/500\r\nstart Current memory usage: 1942.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1992.28 MB\r\nIteration 150\/500\r\nstart Current memory usage: 1982.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 2008.83 MB\r\nIteration 151\/500\r\nstart Current memory usage: 2008.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1946.42 MB\r\nIteration 152\/500\r\nstart Current memory usage: 1946.92 MB\r\nTraining model: sequential\r\nend Current memory usage: 1992.48 MB\r\nIteration 153\/500\r\nstart Current memory usage: 1979.52 MB\r\nTraining model: sequential\r\nend Current memory usage: 2035.66 MB\r\nIteration 154\/500\r\nstart Current memory usage: 2023.91 MB\r\nTraining model: sequential\r\nend Current memory usage: 2030.31 MB\r\nIteration 155\/500\r\nstart Current memory usage: 1974.39 MB\r\nTraining model: sequential\r\nend Current memory usage: 2029.30 MB\r\nIteration 156\/500\r\nstart Current memory usage: 1997.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 2000.31 MB\r\nIteration 157\/500\r\nstart Current memory usage: 1964.38 MB\r\nTraining model: sequential\r\nend Current memory usage: 1979.45 MB\r\nIteration 158\/500\r\nstart Current memory usage: 1973.12 MB\r\nTraining model: sequential\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF 2.18"],"created_at":"2024-11-25T12:12:55Z","comments":7,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80753"},{"issue_number":47,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.SparseConcat`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific inputs, `SparseConcat` triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nindices1 = tf.constant(2, shape=[3,3], dtype=tf.int64)\r\nvalues1 = tf.constant(\"aaaabaaacaaadaaaeaaafaaagaaahaaaiaaajaaakaaalaaamaaanaaaoaaapaaaqaaaraaasaaataaauaaavaaawaaaxaaayaaazaabbaabcaabdaabeaabfaabgaabhaabiaabjaabkaablaabmaabnaaboaabpaabqaabraabsaabtaabuaabvaabwaabxaabyaabzaacbaaccaacdaaceaacfaacgaachaaciaacjaackaaclaacmaacnaacoaacpaacqaacraacsaactaacuaacvaacwaacxaacyaac\",\r\n    shape=[3], dtype=tf.string)\r\nshapes1 = tf.constant([5, 2, 2147483647], dtype=tf.int64)\r\n\r\nindices2 = tf.constant(-2, shape=[4,3], dtype=tf.int64)\r\nvalues2 = tf.constant(\" \", shape=[4], dtype=tf.string)\r\nshapes2 = tf.constant([5,1879048192,536870912], dtype=tf.int64)\r\n\r\nconcat_dim = 1\r\ntf.raw_ops.SparseConcat(\r\n    indices=[indices1, indices2], values=[values1, values2], shapes=[shapes1, shapes2], concat_dim=concat_dim, name=None\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-11-24 06:36:10.994508: F tensorflow\/core\/framework\/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements() status: INVALID_ARGUMENT: Shape [5,1879048194,2147483647] results in overflow when computing number of elements\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-24T06:40:09Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80669"},{"issue_number":48,"repository":"tensorflow\/tensorflow","title":"Floating point exception (core dumped) in `tf.raw_ops.Reshape`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific inputs,  `tf.raw_ops.Reshape` triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntensor = tf.constant(-3.5e+35, shape=[5], dtype=tf.float32)\r\nshape = tf.constant([0, 1879048192, 100000000, 1610612736, -1], dtype=tf.int32)\r\n\r\ntf.raw_ops.Reshape(tensor=tensor, shape=shape)\n```\n\n\n### Relevant log output\n\n```shell\nFloating point exception (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-22T02:58:01Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80529"},{"issue_number":49,"repository":"tensorflow\/tensorflow","title":"The documentation in `data_performance.ipynb` uses `py_function()` without an explanation","description":"### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nTF 2.18\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIn the [\"Better performance with the tf.data API\" guide](https:\/\/www.tensorflow.org\/guide\/data_performance), `tf.py_function()` is used several times in the mapping function. There are various performance boosts demonstrated by the guide with said mapping functions. However, there does not seem to be any practical reason why `tf.py_function()` is used inside the mapping functions. In fact, if you remove those, the behavior is the same; in other words, there doesn't seem to be a need for them at all.\r\n\r\nCuriously, if you remove them from the examples and then perform the time measurements, the speedup goes away. For example, consider the following mapping function from [the guide](https:\/\/github.com\/tensorflow\/docs\/blob\/bbc0b9c70fc0bd4411793d1b0bcc56ef1dbc2405\/site\/en\/guide\/data_performance.ipynb#L447-L450):\r\n\r\n```python\r\ndef mapped_function(s):\r\n    # Do some hard pre-processing\r\n    tf.py_function(lambda: time.sleep(0.03), [], ())\r\n    return s\r\n```\r\n\r\nIn the sequential case, it is shown that the following mapping code results in a time measurement of 0.49222556600034295:\r\n\r\n```python\r\nbenchmark(\r\n    ArtificialDataset()\r\n    .map(mapped_function)\r\n)\r\n```\r\n\r\nAnd, the optimized (parallel) version as follows results in a time measurement of 0.36392719900049997:\r\n\r\n```python\r\nbenchmark(\r\n    ArtificialDataset()\r\n    .map(\r\n        mapped_function,\r\n        num_parallel_calls=tf.data.AUTOTUNE\r\n    )\r\n)\r\n```\r\n\r\nBut, if I remove the `tf.py_function()` from the mapping function, I get comparable measurements from both examples, namely, 0.22448736599994845 and 0.2266392660001202:\r\n\r\n```python\r\ndef mapped_function(s):\r\n    # Do some hard pre-processing\r\n    lambda: time.sleep(0.03), [], ()\r\n    return s\r\n```\r\n\r\nIn fact, that measurement is even better, which makes me believe that this example is contrived to show a performance benefit by using `num_parallel_calls` when in fact TF is already optimizing the code without it. Frivolously wrapping the function in `tf.py_function()` is most likely  causing TensorFlow *not* to optimize the function. Thus, is `num_parallel_calls` even needed to achieve better performance?\n\n### Standalone code to reproduce the issue\n\n```shell\nMentioned in the above description.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:docs-bug","stat:awaiting tensorflower","type:bug"],"created_at":"2024-11-20T16:01:24Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80365"},{"issue_number":50,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.MatrixSolve`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04.3 LTS (x86_64)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen the shape of the matrix argument is empty and the gpu is available, tf.raw_ops.MatrixSolve triggers a crash.\r\nIt can be reproduced on tf-nightly when the gpu is available.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\ntf.raw_ops.MatrixSolve(matrix=tf.random.uniform([], dtype=tf.dtypes.double, maxval=1000000000), rhs=tf.random.uniform([1, 2], dtype=tf.dtypes.double, maxval=1000000000), adjoint=True)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-11-20 14:51:08.714846: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-11-20 14:51:08.775383: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-11-20 14:51:08.852267: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-11-20 14:51:08.876168: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-20 14:51:08.931206: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-11-20 14:51:16.385650: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-11-20 14:51:16.387914: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-11-20 14:51:16.542383: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-20T06:53:21Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80331"},{"issue_number":51,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.MatrixInverse`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the shape of the input argument is empty and the gpu is available, tf.raw_ops.MatrixInverse triggers a crash.\r\nIt can be reproduced on tf-nightly when the gpu is available.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ntf.raw_ops.MatrixInverse(input=tf.cast(tf.random.uniform([], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex128),adjoint=True)\n```\n\n\n### Relevant log output\n\n```shell\n2024-11-20 10:46:15.940818: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-11-20 10:46:16.001155: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-11-20 10:46:16.076386: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-11-20 10:46:16.100080: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-20 10:46:16.154057: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-11-20 10:46:23.889652: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 21903 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-11-20 10:46:23.891964: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-11-20 10:46:24.756574: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-20T02:47:58Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80316"},{"issue_number":52,"repository":"tensorflow\/tensorflow","title":"model.fit fails when the number of rows exceeds Int32.MaxValue","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0-dev20241117\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMacOS 15.1.0\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI would expect model.fit to handle training on extremely large NumPy arrays without limitations.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\r\nfrom keras import Sequential\r\nfrom keras.layers import Dense\r\n\r\nn = 2_147_483_648\r\nx = np.zeros(n).astype(np.float32)\r\ny = x\r\n\r\nmodel = Sequential([\r\n    Dense(64, activation=\"relu\", input_shape=(1,)),\r\n    Dense(1, activation=\"sigmoid\")\r\n])\r\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\r\nmodel.fit(x=x,y=y, epochs=1, batch_size=1024, verbose=1)\n```\n\n\n### Relevant log output\n\n```shell\nValueError: Invalid value in tensor used for shape: -2147483648\n```\n","labels":["type:bug","TF 2.18"],"created_at":"2024-11-19T09:24:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80241"},{"issue_number":53,"repository":"tensorflow\/tensorflow","title":"tf.range still miss some dtypes support","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.18.0-rc2-4-g6550e4bd802 2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nGoogle Colab\n\n### Mobile device\n\nNo\n\n### Python version\n\nGoogle Colab default\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSame issue as in https:\/\/github.com\/tensorflow\/tensorflow\/issues\/72365 but now with unsigned dtypes\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntf.range(10, delta=1, dtype='uint8')\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-7b6ccd0e0a16> in <cell line: 3>()\r\n      1 import tensorflow as tf\r\n      2 \r\n----> 3 tf.range(10, delta=1, dtype='uint8')\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/framework\/ops.py in raise_from_not_ok_status(e, name)\r\n   6000 def raise_from_not_ok_status(e, name) -> NoReturn:\r\n   6001   e.message += (\" name: \" + str(name if name is not None else \"\"))\r\n-> 6002   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n   6003 \r\n   6004 \r\n\r\nInvalidArgumentError: Value for attr 'Tidx' of uint8 is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64, uint16, uint32\r\n\t; NodeDef: {{node Range}}; Op<name=Range; signature=start:Tidx, limit:Tidx, delta:Tidx -> output:Tidx; attr=Tidx:type,default=DT_INT32,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT16, DT_UINT32]> [Op:Range] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-11-14T17:40:39Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80039"},{"issue_number":54,"repository":"tensorflow\/tensorflow","title":"tf.cast to int8 produce wrong number","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.15 - 2.17\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\ninput is a long list include random numbers.\r\ntf.cast to int8 output is  different from numpy cast.\r\n\r\nThe strange thing is if you truncate the long input list to a short list then things works fine.\r\n\r\nThe code is straight forward, you can reproduce it in the colab\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/18dYjvY6JQk79hVq8JsjcWEwIG5KBuF1v?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nNumPy Casted values (int8): \r\n [   0    0    0    0    1   -1    0    3   -3    2   -2    0   50   21\r\n -125   62   25   31  127   -9  117   61  123   47  -20   28   52   36\r\n  -43  -45  -84  118   37  -17   -6  -79    1   75  -45  -60 -103  -63\r\n   85 -112   76   96  -56   86  -32 -108 -105 -121   -2  121   86  -54\r\n   91  -55   36 -119   -3  -36   95  127 -105  -60   37   -9 -106    7\r\n  -31  105   13 -103 -123   79   17  -48 -108  -56  -87 -128   35  -94\r\n  -45  118  -91   86  -63  -43   77    1 -127  -16  -71  -73  -76  -15\r\n  -11]\r\nTensorFlow Casted values (int8): \r\n [   0 -128    0    0    1   -1    0    3   -3    2   -2    0  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  -76  -15\r\n  -11]\r\n```\n```\n","labels":["type:bug","comp:apis","2.17"],"created_at":"2024-11-08T11:33:35Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79689"},{"issue_number":55,"repository":"tensorflow\/tensorflow","title":"Unable to register CUDA plug-ins runnung docker image latest-gpu-jypyter","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\ndocker desktop 4.35.1 , ubuntu 24.04.1 LTS, WSL 2.3.24.0\n\n### Mobile device\n\nNone\n\n### Python version\n\nPython 3.11.0rc1 (provided by tensorflow docker image)\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.7\n\n### GPU model and memory\n\nNVIDIA GeForce RTX 4060 Ti 16G\n\n### Current behavior?\n\nI Strictly followed the instructions provided in:\r\nhttps:\/\/www.tensorflow.org\/install\/docker \r\nhttps:\/\/docs.nvidia.com\/datacenter\/cloud-native\/container-toolkit\/latest\/install-guide.html\r\nGot correct results running a sample workload (as suggested in nvidia contaner toolkit installation manual)\r\n\r\n![image](https:\/\/github.com\/user-attachments\/assets\/6f8c74e6-e23d-4395-8ffc-314f69af5bc7)\r\n\r\nDownloaded tensorflow\/tensorflow latest-gpu-jupyter image and ran the container.\r\nOpend a new jupyter notebook (http:\/\/127.0.0.1:8888\/tree?token=...)\r\nImporting tensorflow I wanted to check the GPU support.\r\nGot error messages and empy available gpu list.\r\n`2024-11-06 10:31:50.143673: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-06 10:31:50.712357: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ntf.__version__\r\n\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n```\n\n\n### Relevant log output\n\n```shell\n2024-11-06 10:31:50.143673: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-06 10:31:50.712357: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n'2.18.0'\r\n\r\nNum GPUs Available:  0\r\n2024-11-06 10:31:54.748888: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (34)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.18"],"created_at":"2024-11-06T11:15:27Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79520"},{"issue_number":56,"repository":"tensorflow\/tensorflow","title":"`tf.math.floormod` not throwing `Integer division by zero` error on GPU for tensor of int64 dtype","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.3\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nRunning `tf.math.floormod` with a tensor containing zeroes in the denominator tensor on GPU does not throw a division by zero error when the tensor has a dtype of `int64`.\r\n\r\n[colab](https:\/\/colab.research.google.com\/drive\/1e053_hcmu0sHQcMPop-_WZR6ya1bw6dy?usp=sharing)\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nA = tf.constant([[2,2],[2,2]], dtype=tf.int64)\r\nB = tf.constant([[0,0],[0,0]], dtype=tf.int64)\r\n\r\nwith tf.device(\"\/gpu:0\"):\r\n    output_gpu = tf.math.floormod(A, B) # No error\r\n    print(f\"\\nGPU: {output_gpu}\\n\") # GPU: [[2 2] [2 2]]\r\n\r\nwith tf.device(\"\/cpu:0\"):\r\n    output_cpu = tf.math.floormod(A, B) \r\n    # InvalidArgumentError: Integer division by zero\n```\n\n\n### Relevant log output\n\n```shell\nGPU: [[2 2]\r\n [2 2]]\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-2-ba17e88e7095> in <cell line: 9>()\r\n      8 \r\n      9 with tf.device(\"\/cpu:0\"):\r\n---> 10     output_cpu = tf.math.floormod(A, B)\r\n     11     # InvalidArgumentError: Integer division by zero\r\n\r\n2 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/framework\/ops.py in raise_from_not_ok_status(e, name)\r\n   5981 def raise_from_not_ok_status(e, name) -> NoReturn:\r\n   5982   e.message += (\" name: \" + str(name if name is not None else \"\"))\r\n-> 5983   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n   5984 \r\n   5985 \r\n\r\nInvalidArgumentError: {{function_node __wrapped__FloorMod_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Integer division by zero [Op:FloorMod] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-11-01T00:42:37Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79162"},{"issue_number":57,"repository":"tensorflow\/tensorflow","title":"`tf.linalg.lstsq` producing outputs with large inconsistencies between CPU and GPU with `float32` tensors","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n12.3\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nPassing tensors of `float32` to 'tf.linalg.lstsq' is producing very different output from CPU and GPU.\r\n\r\n### Standalone code to reproduce the issue\r\n[colab](https:\/\/colab.research.google.com\/drive\/1Fyh3HQs39NhGlOLEdzpqPKVkznBe6Fe9?usp=sharing)\r\n```shell\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nA = tf.constant([[[[0.37454012, 0.9507143, 0.7319939, 0.5986585, 0.15601864], [0.15599452, 0.05808361, 0.8661761, 0.601115, 0.7080726,], [0.02058449, 0.96990985, 0.83244264,\r\n                0.21233912, 0.18182497], [0.1834045, 0.30424225, 0.52475643, 0.43194503, 0.29122913], [0.6118529, 0.13949387, 0.29214466, 0.36636186, 0.45606998]]]], dtype=tf.float32)\r\n\r\nB = tf.constant([[[[0.59241456, 0.04645041], [0.94888556, 0.965632,], [0.684233, 0.4401525,], [\r\n                0.9093204, 0.25877997], [0.54671025, 0.18485446]]]], dtype=tf.float32)\r\n\r\nwith tf.device(\"\/gpu:0\"):\r\n    output_gpu = tf.linalg.lstsq(A, B)\r\n    \r\nwith tf.device( \"\/cpu:0\"):\r\n    output_cpu = tf.linalg.lstsq(A, B)\r\n\r\nnp.testing.assert_allclose(output_cpu, output_gpu.cpu(), atol=100) # AssertionError\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=100\r\n\r\nMismatched elements: 2 \/ 10 (20%)\r\nMax absolute difference: 112.95715\r\nMax relative difference: 0.2963447\r\n x: array([[[[-170.69518 ,   28.379017],\r\n         [ 164.85085 ,  -28.04222 ],\r\n         [-273.4264  ,   46.952198],...\r\n y: array([[[[-241.07059 ,   40.25023 ],\r\n         [ 232.80626 ,  -39.505226],\r\n         [-386.38354 ,   66.00629 ],...\r\n```\r\n","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2024-10-31T22:52:23Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79157"},{"issue_number":58,"repository":"tensorflow\/tensorflow","title":"Thread ID in TensorBoard Profiler Trace Viewer Could Be Negative","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Mint 21.2\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n6.5.0\n\n### GCC\/compiler version\n\nclang 14.0.0\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhile using TensorFlow's profiler, I found that some thread IDs displayed in the Trace Viewer are negative. As shown in the image below, thread names such as `tf_Compute\/-1030865605`, `tf_data_private_threadpool\/-102013...`, etc., have negative thread IDs.\r\n\r\n![image](https:\/\/github.com\/user-attachments\/assets\/71d32241-d617-4e52-bcbf-d3140ebef440)\r\n\r\nMy log file is also attached.\r\n\r\n[20241030-161104.zip](https:\/\/github.com\/user-attachments\/files\/17587809\/20241030-161104.zip)\n\n### Standalone code to reproduce the issue\n\n```shell\n(Just the profiler demo code) https:\/\/www.tensorflow.org\/tensorboard\/tensorboard_profiling_keras\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","awaiting PR merge","2.17"],"created_at":"2024-10-31T13:36:36Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79128"},{"issue_number":59,"repository":"tensorflow\/tensorflow","title":"Significant Discrepancy in `tf.linalg.triangular_solve` Results Between CPU and GPU","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\nv2.18.0-rc2-4-g6550e4bd802 2.18.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 22.04.4 LTS x86_64\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.0\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen using `tf.linalg.triangular_solve` with large matrices or specific triangular matrix conditions (e.g., `upper=True`, `transpose=True`, `unitriangular=True`), the GPU results significantly differ from the CPU results. \r\n\r\nThe discrepancy includes extremely large Mean Absolute Error (MAE) values and infinite Mean Squared Error (MSE) values of the results, indicating a possible issue in the GPU implementation of the function.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n[colab](https:\/\/colab.research.google.com\/drive\/1zXcOAkxHV6snaMyWGMeKIukf5IVBd0Bh?usp=sharing)\r\n[Safe Tensors](https:\/\/drive.google.com\/file\/d\/1n1JyugXNAS9M2wmnk9u9vCVuHz0uE3R6\/view?usp=sharing)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom safetensors.torch import load_file\r\n\r\ndef set_seed(seed_value=42):\r\n    \"\"\"Sets the random seed for reproducibility.\"\"\"\r\n    np.random.seed(seed_value)\r\n    tf.random.set_seed(seed_value)\r\n\r\ndef tensorflow_version(input, cpu=True):\r\n    set_seed()\r\n    if cpu:\r\n        device_string = \"\/cpu:0\"\r\n    else:\r\n        device_string = \"\/gpu:0\"\r\n\r\n    with tf.device(device_string):\r\n        b_tensor = tf.constant(input[\"b\"])\r\n        A_tensor = tf.constant(input[\"A\"])\r\n\r\n        upper = input.get(\"upper\", True)\r\n        transpose = input.get(\"transpose\", False)\r\n        unitriangular = input.get(\"unitriangular\", False)\r\n\r\n        solution = tf.linalg.triangular_solve(\r\n            A_tensor, b_tensor, lower=not upper, adjoint=transpose\r\n        )\r\n\r\n        return {\"triangular_solve_solution\": solution.numpy()}\r\n\r\ndef load_safe_tensor(file_path):\r\n    safe_tensor_data = load_file(file_path)\r\n    A_tensor = safe_tensor_data[\"A\"]\r\n    b_tensor = safe_tensor_data[\"b\"]\r\n\r\n    return {\r\n        \"A\": tf.convert_to_tensor(A_tensor.numpy()),\r\n        \"b\": tf.convert_to_tensor(b_tensor.numpy()),\r\n    }\r\n\r\ndef calculate_differences(cpu_result, gpu_result):\r\n    diff = np.abs(cpu_result - gpu_result)\r\n    mae = np.mean(diff)\r\n    mse = np.mean(diff**2)\r\n    rmse = np.sqrt(mse)\r\n    max_diff = np.max(diff)\r\n    mean_relative_diff = np.mean(diff \/ (np.abs(cpu_result) + 1e-10))\r\n\r\n    return {\r\n        \"Mean Absolute Error\": mae,\r\n        \"Mean Squared Error\": mse,\r\n        \"Root Mean Squared Error\": rmse,\r\n        \"Maximum Absolute Difference\": max_diff,\r\n        \"Mean Relative Difference\": mean_relative_diff,\r\n    }\r\n\r\ndef main():\r\n    file_path = \"tensorflow_triangular_solve_3.safetensors\"\r\n    input_data = load_safe_tensor(file_path)\r\n    input_data[\"upper\"] = True\r\n    input_data[\"transpose\"] = True\r\n    input_data[\"unitriangular\"] = True\r\n\r\n    result_cpu = tensorflow_version(input_data, cpu=True)\r\n    print(\"CPU Result:\")\r\n    print(result_cpu)\r\n\r\n    if tf.config.list_physical_devices(\"GPU\"):\r\n        result_gpu = tensorflow_version(input_data, cpu=False)\r\n        print(\"GPU Result:\")\r\n        print(result_gpu)\r\n\r\n        if result_gpu:\r\n            cpu_solution = result_cpu[\"triangular_solve_solution\"]\r\n            gpu_solution = result_gpu[\"triangular_solve_solution\"]\r\n            differences = calculate_differences(cpu_solution, gpu_solution)\r\n            for key, value in differences.items():\r\n                print(f\"{key}: {value}\")\r\n    else:\r\n        print(\"GPU not available.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n#### **Issue Summary:**\r\n- The `tf.linalg.triangular_solve` function produces vastly different results on the GPU compared to the CPU. The differences include an enormous Mean Absolute Error (MAE) and an infinite Mean Squared Error (MSE), indicating a severe discrepancy between the CPU and GPU results.\r\n- The issue appears to be related to the use of specific arguments like `upper=True`, `transpose=True`, and `unitriangular=True`, suggesting that there might be a numerical precision or stability issue in the GPU implementation.\r\n- It is unclear why these large discrepancies occur, but they point to a critical inconsistency that could impact numerical reliability for users depending on TensorFlow\u2019s triangular solve functionality.\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nMean Absolute Error: 1.566790629150662e+21\r\nMean Squared Error: inf\r\nRoot Mean Squared Error: inf\r\nMaximum Absolute Difference: 1.4265324671452624e+26\r\n```\r\n","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2024-10-31T04:34:40Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79090"},{"issue_number":60,"repository":"tensorflow\/tensorflow","title":"`lookup_ops.StaticVocabularyTable` and `lookup_ops.StaticVocabularyTableV1`  can cause a crash","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf-nightly  2.19.0-dev20241025\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04.3 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI encountered an `segmentation fault issue` in TensorFlow when I used API `lookup_ops.StaticVocabularyTable` or `lookup_ops.StaticVocabularyTableV1` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).\r\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1NGnFQqFl6Jy_Xt7wBL3ynmSVs9AiFw_l?usp=sharing) to reproduce the issue.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport os\r\nfrom tensorflow.python.ops import lookup_ops\r\n\r\ndef _createVocabFile(basename, values=('brain', 'salad', 'surgery')):\r\n    vocabulary_file = os.path.join(\"\/tmp\", basename)\r\n    with open(vocabulary_file, 'w') as f:\r\n        f.write('\\n'.join(values) + '\\n')\r\n    return vocabulary_file\r\n\r\nvocab_file = _createVocabFile('feat_to_id_1.txt')\r\nvocab_size = 9223372036854775807\r\noov_buckets = 1\r\ntable = lookup_ops.StaticVocabularyTable(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)\r\n#lookup_ops.StaticVocabularyTableV1(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nSegmentation fault (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-10-26T07:19:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78831"},{"issue_number":61,"repository":"tensorflow\/tensorflow","title":"`gen_list_ops.tensor_list_concat_v2` aborts with \"Check failed: size >= 0 (0 vs. -1)\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly  2.19.0-dev20241025\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\nLinux Ubuntu 20.04.3 LTS\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)\r\n\r\nPlease find the [[gist](https:\/\/colab.research.google.com\/drive\/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing)](https:\/\/colab.research.google.com\/drive\/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import gen_list_ops\r\nfrom tensorflow.python.ops import list_ops\r\n\r\nl = list_ops.tensor_list_reserve(element_dtype=dtypes.float32, element_shape=None, num_elements=3)\r\nt = gen_list_ops.tensor_list_concat_v2(l, element_dtype=dtypes.float32, element_shape=list_ops._build_element_shape((None, 3)), leading_dims=[-1, 3, 5])\n```\n\n\n### Relevant log output\n\n```shell\nF tensorflow\/core\/framework\/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-10-26T07:15:16Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78830"},{"issue_number":62,"repository":"tensorflow\/tensorflow","title":"`data_flow_ops.Barrier` aborts with \"Check failed: i >= 0 (0 vs. -100)\" ","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly  2.19.0-dev20241025\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)\r\n\r\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1OOOIvZ7brRDjRqshv36CA_55BlPbgI4e?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import data_flow_ops\r\nfrom tensorflow.python.eager import context\r\nimport tensorflow as tf\r\n\r\nwith context.graph_mode():\r\n    sess = tf.compat.v1.Session()\r\n    with sess.as_default():\r\n        b = data_flow_ops.Barrier((dtypes.float32, dtypes.float32), shapes=((), ()), name='B')\r\n        keys = [b'a', b'b', b'c', b'd']\r\n        values_0 = [10.0, 20.0, 30.0, 40.0]\r\n        values_1 = [100.0, 200.0, 300.0, 400.0]\r\n        insert_1_1_op = b.insert_many(-100, keys[0:2], values_1[0:2]) \r\n        insert_1_1_op.run()\n```\n\n\n### Relevant log output\n\n```shell\nF tensorflow\/core\/kernels\/barrier_ops.cc:286] Check failed: i >= 0 (0 vs. -100)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-10-26T07:04:16Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78828"},{"issue_number":63,"repository":"tensorflow\/tensorflow","title":"Does TFLite dequantize opertor support constant buffer input","description":"**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (or github SHA if from source): the latest version\r\n\r\n**Standalone code to reproduce the issue** \r\n<img width=\"724\" alt=\"image\" src=\"https:\/\/github.com\/user-attachments\/assets\/849a0952-90ce-42c4-b2dc-9a9e7333fb0b\">\r\nThe [tflite model for user input](https:\/\/github.com\/fujunwei\/tensorflow\/blob\/tflite_label_image\/tensorflow\/lite\/examples\/python\/dequantize_input.tflite) can work as expected\r\n\r\n<img width=\"713\" alt=\"image\" src=\"https:\/\/github.com\/user-attachments\/assets\/43cafc84-0ea6-4a06-84b4-4772c6cea8e8\">\r\n\r\nThe [tflite model with constant buffer](https:\/\/github.com\/fujunwei\/tensorflow\/blob\/tflite_label_image\/tensorflow\/lite\/examples\/python\/dequantize_constant.tflite) can't compute, so does [dequantize](https:\/\/www.tensorflow.org\/mlir\/tfl_ops#tfldequantize_tfldequantizeop) support constant input?\r\n\r\n**Any other info \/ logs**\r\n\r\nYou can also modify [dequantize_tester in the Repo](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/lite\/delegates\/xnnpack\/dequantize_tester.cc#L110) to reproduce this issue like below code:\r\n\r\n1, Create a constant buffer\r\n```\r\n  const auto buffer_data = builder.CreateVector( reinterpret_cast<const uint8_t*>(constant_buffer.data()),\r\n                                                                              constant_buffer.size());\r\n  const auto buffer_index = base::checked_cast<uint32_t>(buffers.size());\r\n  buffers.emplace_back(::tflite::CreateBuffer(builder, buffer_data));\r\n```\r\n\r\n2, Use the `buffer_index ` when creating tensor:\r\n```\r\nconst std::array<flatbuffers::Offset<::tflite::Tensor>, 2> tensors{{\r\n      ::tflite::CreateTensor(\r\n          builder,\r\n          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),\r\n          Unsigned() ? ::tflite::TensorType_UINT8 : ::tflite::TensorType_INT8,\r\n          \/*buffer=*\/buffer_index , \/*name=*\/0,\r\n          ::tflite::CreateQuantizationParameters(\r\n              builder, \/*min=*\/0, \/*max=*\/0,\r\n              builder.CreateVector<float>({InputScale()}),\r\n              builder.CreateVector<int64_t>({InputZeroPoint()}))),\r\n      ::tflite::CreateTensor(\r\n          builder,\r\n          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),\r\n          ::tflite::TensorType_FLOAT32,\r\n          \/*buffer=*\/0, \/*name=*\/builder.CreateString(\"dequantizeLinearOutput\")),\r\n  }};\r\n\r\n```","labels":["stat:awaiting tensorflower","type:bug","comp:lite","ModelOptimizationToolkit","2.17"],"created_at":"2024-10-25T08:28:42Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78748"},{"issue_number":64,"repository":"tensorflow\/tensorflow","title":"No kernels registered for op `Conv2DBackpropInputV2`","description":"### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe existence of operator `Conv2DBackpropInputV2` is described in the official website document.https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/Conv2DBackpropInputV2.\r\nHowever, during my actual execution, the following error message appears:\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput_sizes = tf.constant(1, shape=[4], dtype=tf.int32)\r\nfilter_tensor = tf.constant(2, shape=[4], dtype=tf.float32)\r\nout_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)\r\n\r\nstrides = [1, 1, 1, 1]\r\npadding = \"SAME\"\r\n\r\ntf.raw_ops.Conv2DBackpropInputV2(\r\n    input=input_sizes,\r\n    filter=filter_tensor,\r\n    out_backprop=out_backprop,\r\n    strides=strides,\r\n    padding=padding\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-10-21 12:30:18.492624: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nTraceback (most recent call last):\r\n  File \"\/mnt\/tests\/Conv2DBackpropInput.py\", line 10, in <module>\r\n    tf.raw_ops.Conv2DBackpropInputV2(\r\n  File \"\/mnt\/origin\/venv\/tensorflow-nightly\/lib\/python3.11\/site-packages\/tensorflow\/python\/util\/tf_export.py\", line 377, in wrapper\r\n    return f(**kwargs)\r\n           ^^^^^^^^^^^\r\n  File \"\/mnt\/origin\/venv\/tensorflow-nightly\/lib\/python3.11\/site-packages\/tensorflow\/python\/ops\/gen_nn_ops.py\", line 2030, in conv2d_backprop_input_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"\/mnt\/origin\/venv\/tensorflow-nightly\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 5983, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Conv2DBackpropInputV2}} = Conv2DBackpropInputV2[T=DT_INT32, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true]\r\nAll kernels registered for op Conv2DBackpropInputV2:\r\n  <no registered kernels>\r\n [Op:Conv2DBackpropInputV2] name:\n```\n","labels":["type:docs-bug","stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-10-21T12:34:35Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78431"},{"issue_number":65,"repository":"tensorflow\/tensorflow","title":"Overflow and Check fail in `tf.raw_ops.Conv2DBackpropInput`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOverflow : `input_sizes` is a one-dimensional tensor and contains maximum values\r\nCheck fail\uff1a`input_sizes`'s shape is [2] and The dimension of `filter` is less than 2\n\n### Standalone code to reproduce the issue\n\n```shell\nOverflow:\r\n\r\nimport tensorflow as tf\r\n\r\ninput_sizes = tf.constant([1, 5, 5, 999999999999], shape=[4], dtype=tf.int32)\r\nfilter_tensor = tf.constant(3, shape=[3, 3, 3, 2], dtype=tf.float32)\r\nout_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)\r\n\r\nstrides = [1, 1, 1, 1]\r\npadding = \"SAME\"\r\n\r\ntf.raw_ops.Conv2DBackpropInput(\r\n    input_sizes=input_sizes,\r\n    filter=filter_tensor,\r\n    out_backprop=out_backprop,\r\n    strides=strides,\r\n    padding=padding\r\n)\r\n\r\nCheck fail:\r\n```python\r\nimport tensorflow as tf\r\n\r\ninput_sizes = tf.constant(1, shape=[2], dtype=tf.int32)\r\nfilter_tensor = tf.constant(2, shape=[1], dtype=tf.float32)\r\nout_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)\r\n\r\nstrides = [1, 1, 1, 1]\r\npadding = \"SAME\"\r\n\r\ntf.raw_ops.Conv2DBackpropInput(\r\n    input_sizes=input_sizes,\r\n    filter=filter_tensor,\r\n    out_backprop=out_backprop,\r\n    strides=strides,\r\n    padding=padding\r\n)\r\n```\n```\n\n\n### Relevant log output\n\n```shell\nOverflow:\r\n\r\n2024-10-21 11:40:49.417611: F tensorflow\/core\/kernels\/mkl\/mkl_conv_grad_input_ops.cc:578] Non-OK-status: tensor::MakeShape(input_tensor, &input_tf_shape)\r\nStatus: INVALID_ARGUMENT: Dimension -727379969 must be >= 0\r\nAborted (core dumped)\r\n```\r\n\r\nCheck fail:\r\n```\r\n2024-10-21 11:16:29.937265: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 1)\r\nAborted (core dumped)\r\n```\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-21T12:17:20Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78428"},{"issue_number":66,"repository":"tensorflow\/tensorflow","title":"Overflow in `tf.raw_ops.Fill`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOverflow in `tf.raw_ops.Fill` when there are too large values in `dims`.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1GDBN4lheNUIW704hsXVt3lW55UJue97S?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nKill\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-21T11:10:34Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78427"},{"issue_number":67,"repository":"tensorflow\/tensorflow","title":"Multi-threaded execution throws an exception (using GPU).","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.19.0-dev20241018\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 24.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nMulti-threaded execution throws an exception (using GPU).\n\n### Standalone code to reproduce the issue\n\n```shell\nimport concurrent\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nexecutor = concurrent.futures.ThreadPoolExecutor()\r\n\r\n\r\ndef sum(x, axis):\r\n    return tf.reduce_sum(x, axis=axis)\r\n\r\n\r\nfutures = []\r\n\r\nfor i in range(1000):\r\n    futures.clear()\r\n    for _ in range(4):\r\n        x = tf.convert_to_tensor(np.random.rand(100, 100))\r\n        futures.append(executor.submit(sum, x, 1))\r\n        x = tf.convert_to_tensor(np.random.rand(100))\r\n        futures.append(executor.submit(sum, x, 0))\r\n    concurrent.futures.wait(\r\n        futures, return_when=concurrent.futures.ALL_COMPLETED\r\n    )\r\n    [future.result() for future in futures]\n```\n\n\n### Relevant log output\n\n```shell\nW tensorflow\/core\/framework\/op_kernel.cc:1840] OP_REQUIRES failed at reduction_ops_common.h:147 : INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)\r\nI tensorflow\/core\/framework\/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)\r\n\r\n```\n```\n","labels":["type:bug","comp:gpu"],"created_at":"2024-10-19T13:02:32Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78338"},{"issue_number":68,"repository":"tensorflow\/tensorflow","title":"[Incorrect Result] `tf.math.reciprocal` returns `NaN` on `inf` input on Linux.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nAlmaLinux 9.4\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`tf.math.reciprocal` returns `NaN` on Linux when input is `inf` or `-inf` and has dtype=complex128, shape >= 2.\r\nThe output is expected to be 0, since:\r\n1. This behavior is not consistent with dtype=float64, where the output will be 0.\r\n2. When input tensor contains only one value, the output will be 0.\r\n3. The same code snippet will return different result on macOS, where the output is also 0.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninput = tf.constant(np.inf, dtype=tf.float64)\r\nout = tf.math.reciprocal(input)\r\n# tf.Tensor(0.0, shape=(), dtype=float64)\r\nprint(out)\r\n\r\ninput = tf.constant(np.inf, dtype=tf.complex128)\r\nout = tf.math.reciprocal(input)\r\n# tf.Tensor(0j, shape=(), dtype=complex128)\r\nprint(out)\r\n\r\ninput = tf.constant([np.inf, np.inf], dtype=tf.complex128)\r\nout = tf.math.reciprocal(input)\r\n# On Linux: tf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)\r\n# On macOS: tf.Tensor([0.+0.j 0.+0.j], shape=(2,), dtype=complex128)\r\nprint(out)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nAttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'\r\ntf.Tensor(0.0, shape=(), dtype=float64)\r\ntf.Tensor(0j, shape=(), dtype=complex128)\r\ntf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-19T09:41:36Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78298"},{"issue_number":69,"repository":"tensorflow\/tensorflow","title":"tf.linalg.expm fails to support half\/float16 data type, which is inconsistent with doc","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nAccording to the documentation: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/linalg\/expm\r\ntf.linalg.expm is expected to accept float16 input, but it fails on float16 when actually running the following code.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\ninput = tf.constant(np.random.randn(1,1), dtype='float16')\r\nout = tf.linalg.expm(input)\r\n```\n```\n\n\n### Relevant log output\n\n```shell\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node MatrixSolve}} = MatrixSolve[T=DT_HALF, adjoint=false]\r\nAll kernels registered for op MatrixSolve:\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n [Op:MatrixSolve] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-17T13:09:08Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78107"},{"issue_number":70,"repository":"tensorflow\/tensorflow","title":"DLPack with Int32 tensor on the GPU: inconsistent eager mode \/ graph mode \/ XLA","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv1.12.1-117097-gecf05620570 2.19.0-dev20241016\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nHello,\r\n\r\nI realize that `int32` is a special dtype in TensorFlow for historical reasons. It seems that the handling of GPU int32-typed tensors has evolved over time.\r\n\r\nCurrently, the `device` field of a tensor created with:\r\n```py\r\nwith tf.device('gpu'):\r\n    x = tf.constant([0,1,2], tf.int32)\r\n```\r\n*does* indicate it's a GPU tensor: `\/job:localhost\/replica:0\/task:0\/device:GPU:0`.\r\n\r\nHowever, when exporting and re-importing it via DLPack, it comes back as a CPU tensor.\r\nThere even seems to be a unit test validating this:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/d3de971a7348ecaefdbb920e580c37ebde10d780\/tensorflow\/python\/dlpack\/dlpack_test.py#L75-L78\r\n\r\n\r\nHowever, @jhoydis found that this is *not* consistent between modes. In particular, if the tensor goes through an XLA-compiled function, it will correctly live on the GPU even after a round-trip through DLPack. (See reproducer below).\r\n\r\nWould it please be possible to revisit this behavior, so that **exporting an int32 GPU tensor via DLPack does result in a GPU DLPack capsule in all modes, not just XLA?**\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n\r\ndef f_eager(x):\r\n    return x\r\nf_graph = tf.function()(f_eager)\r\nf_xla = tf.function(jit_compile=True)(f_eager)\r\n\r\n\r\nwith tf.device('gpu'):\r\n    x = tf.constant([0,1,2], tf.int32)\r\n    print(\"Original tensor:\", x.device)\r\n\r\n    dlcapsule = tf.experimental.dlpack.to_dlpack(x)\r\n    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)\r\n    print(\"Default:\", x_.device)\r\n\r\n    dlcapsule = tf.experimental.dlpack.to_dlpack(f_eager(x))\r\n    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)\r\n    print(\"Eager:\", x_.device)\r\n\r\n    dlcapsule = tf.experimental.dlpack.to_dlpack(f_graph(x))\r\n    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)\r\n    print(\"Graph:\", x_.device)\r\n\r\n    dlcapsule = tf.experimental.dlpack.to_dlpack(f_xla(x))\r\n    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)\r\n    print(\"XLA:\", x_.device)\n```\n\n\n### Relevant log output\n\n```shell\nOriginal tensor: \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nDefault: \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nEager: \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nGraph: \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nXLA: \/job:localhost\/replica:0\/task:0\/device:GPU:0\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","comp:gpu"],"created_at":"2024-10-17T08:59:51Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78091"},{"issue_number":71,"repository":"tensorflow\/tensorflow","title":"tf.math.special.bessel_* has inconsistent result with scipy","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nBased on the documentation, special function such as bessel_y0 should have consistent result with scipy. However, when receiving `-inf`, it has inconsistent results with scipy. Please check the reproducible for details.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport scipy\r\nimport numpy as np\r\nimport tensorflow as tf\r\nx = tf.constant(-np.inf, dtype='float64')\r\nprint(\"TF:\", tf.math.special.bessel_y1(x))\r\nprint(\"Scipy: \", scipy.special.y1(x))\r\nprint(\"TF:\", tf.math.special.bessel_y0(x))\r\nprint(\"Scipy: \", scipy.special.y0(x))\r\nprint(\"TF:\", tf.math.special.bessel_k0(x))\r\nprint(\"Scipy: \", scipy.special.k0(x))\r\nprint(\"TF:\", tf.math.special.bessel_k1(x))\r\nprint(\"Scipy: \", scipy.special.k1(x))\n```\n\n\n### Relevant log output\n\n```shell\nTF: tf.Tensor(-inf, shape=(), dtype=float64)\r\nScipy:  nan\r\nTF: tf.Tensor(-inf, shape=(), dtype=float64)\r\nScipy:  nan\r\nTF: tf.Tensor(inf, shape=(), dtype=float64)\r\nScipy:  nan\r\nTF: tf.Tensor(inf, shape=(), dtype=float64)\r\nScipy:  nan\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-14T15:45:19Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77864"},{"issue_number":72,"repository":"tensorflow\/tensorflow","title":"tf.math.is_strictly_increasing's behavior is not clear on a (2,2) matrix","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen receiving this input:\r\n```\r\nx = tf.constant([[1,2],[2,3]])\r\n```\r\n`tf.math.is_strictly_increasing` outputs `False` instead of `True`.\r\nAlso, for a tensor with shape (1,3,3):\r\n```\r\nx = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],\r\n  [-0.5704009,  -0.2167283,   0.2548743 ],\r\n  [-0.14944994,  2.0107825,  -0.09678416]]])\r\n```\r\nIt's output is still `False` instead of `True` even when x's first dimension only has one element.\r\nBased on the description \"Elements of x are compared in row-major order.\", it seems that elements in x are compared along row (i.e., the first dimension).\r\nTherefore, to my understanding, if the first dimension contains only one element (such as 1x3x3 shape tensor), the output should be True. If the input is [[1,2],[3,4]], the output should also be `True` since the value in the first dimension is increasing (from `[1,2]` to `[3,4]`)\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nx = tf.constant([[1,2],[2,3]])\r\nprint(tf.math.is_strictly_increasing(x))  # False\r\nx = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],\r\n  [-0.5704009,  -0.2167283,   0.2548743 ],\r\n  [-0.14944994,  2.0107825,  -0.09678416]]])\r\nprint(tf.math.is_strictly_increasing(x))  # False\r\n```\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-14T15:28:21Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77863"},{"issue_number":73,"repository":"tensorflow\/tensorflow","title":"argmax returns incorrect result for input containing Minimum number (TensorFlow 2.x)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nCurrent Behavior:\r\nWhen using tf.math.argmax on an input array that contains -0.0, the result is incorrect. Specifically, the function returns 1 (the index of -0.0) as the position of the maximum value, while the actual maximum value is 1.401298464324817e-45 at index 2.\r\n\r\nThe same behavior is observed in Keras and JAX, as both use TensorFlow internally for the argmax function.\r\n\r\nExpected Behavior:\r\ntf.math.argmax should return 2, as the value at index 2 (1.401298464324817e-45) is greater than both -1.0 and -0.0.\r\n```\r\nimport numpy as np\r\nimport torch\r\nimport tensorflow as tf\r\nimport jax.numpy as jnp\r\nfrom tensorflow import keras\r\n\r\ndef test_argmax():\r\n    # Input data\r\n    input_data = np.array([-1.0, -0.0, 1.401298464324817e-45], dtype=np.float32)\r\n\r\n    # PyTorch argmax\r\n    pytorch_result = torch.argmax(torch.tensor(input_data, dtype=torch.float32)).item()\r\n    print(f\"PyTorch argmax result: {pytorch_result}\")\r\n\r\n    # TensorFlow argmax\r\n    tensorflow_result = tf.math.argmax(input_data).numpy()\r\n    print(f\"TensorFlow argmax result: {tensorflow_result}\")\r\n\r\n    # Keras argmax (Keras internally uses TensorFlow, so should be the same)\r\n    keras_result = keras.backend.argmax(input_data).numpy()\r\n    print(f\"Keras argmax result: {keras_result}\")\r\n\r\n    # JAX argmax\r\n    jax_result = jnp.argmax(input_data)\r\n    print(f\"JAX argmax result: {jax_result}\")\r\n\r\nif __name__ == \"__main__\":\r\n    test_argmax()\r\n\r\n```\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nPyTorch argmax result: 2\r\nTensorFlow argmax result: 1\r\nKeras argmax result: 1\r\nJAX argmax result: 1\r\n\r\n```\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","comp:ops","TF 2.16"],"created_at":"2024-10-14T10:07:59Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77853"},{"issue_number":74,"repository":"tensorflow\/tensorflow","title":"argsort incorrectly handles very small floating-point numbers and -0.0 compared to other libraries (PyTorch and JAX)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nwindows\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen using TensorFlow's argsort function on an array containing small floating-point numbers and both 0.0 and -0.0, the sort order is incorrect compared to other deep learning libraries such as PyTorch and JAX. TensorFlow incorrectly places 1.401298464324817e-45 (a very small positive number) before 0.0 and -0.0.\r\n\r\nExpected behavior is that both 0.0 and -0.0 should be treated as equivalent and placed before any positive number, including very small ones like 1.401298464324817e-45. However, TensorFlow does not follow this behavior, whereas PyTorch correctly handles this.\r\n```\r\nimport numpy as np\r\nimport torch\r\nimport tensorflow as tf\r\nimport jax.numpy as jnp\r\n\r\ndef test_argsort():\r\n    # Input data, hardcoded as float32\r\n    input_data = np.array([\r\n        -0.0, 1.401298464324817e-45, 1.100000023841858, -0.0,\r\n        5.960464477539063e-08, -2.0000100135803223, 1000000.0,\r\n        722801.375, 0.0, -1.100000023841858\r\n    ], dtype=np.float32)\r\n\r\n    # PyTorch argsort\r\n    pytorch_result = torch.argsort(torch.tensor(input_data, dtype=torch.float32)).numpy()\r\n    print(f\"PyTorch argsort result: {pytorch_result}\")\r\n\r\n    # TensorFlow argsort\r\n    tensorflow_result = tf.argsort(input_data).numpy().astype(np.int32)\r\n    print(f\"TensorFlow argsort result: {tensorflow_result}\")\r\n\r\n    # JAX argsort\r\n    jax_result = jnp.argsort(input_data).astype(np.int32)\r\n    print(f\"JAX argsort result: {jax_result}\")\r\n\r\nif __name__ == \"__main__\":\r\n    test_argsort()\r\n\r\n```\n\n### Standalone code to reproduce the issue\n\n```shell\nPyTorch argsort result: [5 9 0 3 8 1 4 2 7 6]\r\nTensorFlow argsort result: [5 9 0 1 3 8 4 2 7 6]\r\nJAX argsort result: [5 9 0 1 3 8 4 2 7 6]\r\n```\r\nExpected Behavior:\r\nTensorFlow's argsort should place 0.0 and -0.0 before any positive number, including very small values like 1.401298464324817e-45. PyTorch demonstrates the correct behavior by treating 0.0 and -0.0 as equal and placing them in the correct order relative to other values.\r\n\r\nStandalone Code to Reproduce the Issue:\r\nThe above Python code demonstrates the issue. It uses the same input data for PyTorch, TensorFlow, and JAX to show the difference in behavior. TensorFlow and JAX produce incorrect results by misplacing the small positive value before 0.0, while PyTorch produces the correct order.\r\n\r\nRelevant Log Output:\r\nNo error logs are generated, but the incorrect behavior is clearly shown in the sorting results.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","TF 2.16"],"created_at":"2024-10-14T09:02:22Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77849"},{"issue_number":75,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT3D`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIf the value contained in fft_length is the maximum value, it will cause an abort\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput = tf.constant(0, shape=[2,0,0,0] ,dtype=tf.complex64)\r\nfft_length = tf.constant(1879048192, shape=[3], dtype=tf.int32)\r\n\r\ntf.raw_ops.IRFFT3D(input=input, fft_length=fft_length)\n```\n\n\n### Relevant log output\n\n```shell\n2024-10-13 13:04:53.308156: F tensorflow\/core\/framework\/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()\r\nStatus: INVALID_ARGUMENT: Shape [2,1879048192,1879048192,1879048192] results in overflow when computing number of elements\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-13T13:07:08Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77824"},{"issue_number":76,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT2D`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSince the value in fft_length is a maximum value, it will cause abort\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput = tf.constant(0, shape=[1,4,10,0,0] ,dtype=tf.complex64)\r\nfft_length = tf.constant(2147483647, shape=[2], dtype=tf.int32)\r\n\r\ntf.raw_ops.IRFFT2D(input=input, fft_length=fft_length)\n```\n\n\n### Relevant log output\n\n```shell\n2024-10-13 12:59:11.295197: F tensorflow\/core\/framework\/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()\r\nStatus: INVALID_ARGUMENT: Shape [1,4,10,2147483647,2147483647] results in overflow when computing number of elements\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-13T13:04:05Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77823"},{"issue_number":77,"repository":"tensorflow\/tensorflow","title":"Gradients of tf.linalg.expm not supported with JIT compilation","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntested 2.17 and 2.10, both have the issue\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu\n\n### Mobile device\n\n_No response_\n\n### Python version\n\ntested 3.9 and 3.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nGradients of `tf.linalg.expm` can not be computed with JIT compilation. \r\n\r\nThis is an issue, because tf 2.17 seems to have activated jit compilation for compiled models per default whereas earlier versions did not, breaking existing code.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nA = tf.Variable([[.4, 1.5], [.6, .1]], dtype=tf.float32)\r\n\r\n@tf.function(jit_compile=True) #set jit_compile=False to make it work\r\ndef f(A):\r\n    with tf.GradientTape() as tape:\r\n        B = tf.linalg.expm(A)\r\n    return tape.gradient(B, A)\r\n\r\nf(A)\n```\n\n\n### Relevant log output\n\n```shell\n2024-10-11 11:17:27.281304: W tensorflow\/core\/framework\/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: XLA compilation requires a fixed tensor list size. Set the max number of elements. This could also happen if you're using a TensorArray in a while loop that does not have its maximum_iteration set, you can fix this by setting maximum_iteration to a suitable value.\r\n\r\nStack trace for op definition: \r\nFile \"<frozen runpy>\", line 198, in _run_module_as_main\r\nFile \"<frozen runpy>\", line 88, in _run_code\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel_launcher.py\", line 18, in <module>\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/traitlets\/config\/application.py\", line 1075, in launch_instance\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/kernelapp.py\", line 739, in start\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/tornado\/platform\/asyncio.py\", line 205, in start\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/asyncio\/base_events.py\", line 641, in run_forever\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/asyncio\/base_events.py\", line 1986, in _run_once\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/asyncio\/events.py\", line 88, in _run\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/kernelbase.py\", line 545, in dispatch_queue\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/kernelbase.py\", line 534, in process_one\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/kernelbase.py\", line 437, in dispatch_shell\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/ipkernel.py\", line 362, in execute_request\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/kernelbase.py\",\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-11T12:31:35Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77693"},{"issue_number":78,"repository":"tensorflow\/tensorflow","title":"tf.custom_gradient for function with kwarg shows unexpected behavior","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 22.04.5 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.7\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.3\/8\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have a function that takes two tensors as inputs, one as argument and one as keyword argument.\r\nThe function has a custom gradient.\r\n\r\nWhen ``tape.gradient`` for both input tensors with respect to the output of the function is called, TensorFlow throws an error, saying that only one gradient is expect and not two.\r\n\r\nWhen the function is called with both inputs as arguments (and not one of them as kwarg), no error is thrown.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\n@tf.custom_gradient\r\ndef func(x, y=0):\r\n    z = 2*x + y\r\n    def grad(dz):\r\n        dx = 2*dz\r\n        dy = dz\r\n        return dx, dy\r\n    return z, grad\r\nx = tf.constant(2.)\r\ny = tf.constant(3.)\r\nwith tf.GradientTape() as tape:\r\n    tape.watch([x, y])\r\n    z = func(x, y=y) #func(x, y) does not generate the error\r\ngrads = tape.gradient(z, [x, y])\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[129], line 14\r\n     12     tape.watch([x, y])\r\n     13     z = func(x, y=y)\r\n---> 14 grads = tape.gradient(z, [x, y])\r\n\r\nFile ~\/.local\/lib\/python3.12\/site-packages\/tensorflow\/python\/eager\/backprop.py:1066, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n   1060   output_gradients = (\r\n   1061       composite_tensor_gradient.get_flat_tensors_for_gradients(\r\n   1062           output_gradients))\r\n   1063   output_gradients = [None if x is None else ops.convert_to_tensor(x)\r\n   1064                       for x in output_gradients]\r\n-> 1066 flat_grad = imperative_grad.imperative_grad(\r\n   1067     self._tape,\r\n   1068     flat_targets,\r\n   1069     flat_sources,\r\n   1070     output_gradients=output_gradients,\r\n   1071     sources_raw=flat_sources_raw,\r\n   1072     unconnected_gradients=unconnected_gradients)\r\n   1074 if not self._persistent:\r\n   1075   # Keep track of watched variables before setting tape to None\r\n   1076   self._watched_variables = self._tape.watched_variables()\r\n\r\nFile ~\/.local\/lib\/python3.12\/site-packages\/tensorflow\/python\/eager\/imperative_grad.py:67, in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\r\n     63 except ValueError:\r\n     64   raise ValueError(\r\n     65       \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\r\n---> 67 return pywrap_tfe.TFE_Py_TapeGradient(\r\n     68     tape._tape,  # pylint: disable=protected-access\r\n     69     target,\r\n     70     sources,\r\n     71     output_gradients,\r\n     72     sources_raw,\r\n     73     compat.as_str(unconnected_gradients.value))\r\n\r\nFile ~\/.local\/lib\/python3.12\/site-packages\/tensorflow\/python\/ops\/custom_gradient.py:588, in _eager_mode_decorator.<locals>.actual_grad_fn(*result_grad_components)\r\n    585 flat_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(\r\n    586     nest.flatten(input_grads))\r\n    587 if len(flat_grads) != arg_count:\r\n--> 588   raise ValueError(\r\n    589       f\"custom_gradient function expected to return {arg_count} \"\r\n    590       f\"gradients, but returned {len(flat_grads)} instead.\")\r\n    591 return flat_grads + variable_grads\r\n\r\nValueError: custom_gradient function expected to return 1 gradients, but returned 2 instead.\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-10T10:18:54Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77559"},{"issue_number":79,"repository":"tensorflow\/tensorflow","title":"Backward compatibility issue: failure to load models saved in TensorFlow format (Keras 2) in TensorFlow 2.17","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.9.1 (model saved), 2.17.0 (model loaded)\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n(Official Docker Image) Ubuntu 22.04.4 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.8.10 (model saved),  3.11.0rc1 (model loaded)\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n## Description\r\n\r\nI have encountered a backward compatibility issue when loading models saved with Keras 2 in TensorFlow 2.9 into TensorFlow 2.17, which now uses Keras 3 API. This issue impacts various loading methods, and there does not appear to be a straightforward solution to resolve the errors.\r\n\r\n## Steps to reproduce\r\n\r\n1. **Train and export a model in TensorFlow 2.9 with Keras 2 API**:\r\n   - A simple Keras sequential model is created and trained on random data.\r\n   - The model is saved using both `tf.saved_model.save` and `tf.keras.models.save_model` with `tf` save format (which is unsupported in Keras 3).\r\n   \r\n2. **Attempt to load the models in TensorFlow 2.17 with Keras 3 API**:\r\n   - The models are loaded using TensorFlow\u2019s `tf.saved_model.load`, `keras.layers.TFSMLayer`, and `tf.keras.models.load_model`.\r\n\r\n3. **Observe the errors**:\r\n   - When loading using `tf.saved_model.load`, the error `'_UserObject' object has no attribute 'add_slot'` occurs.\r\n   - When loading using `keras.layers.TFSMLayer`, the same `'_UserObject' object has no attribute 'add_slot'` error is triggered.\r\n   - When loading using `tf.keras.models.load_model`, a different error appears: `File format not supported.` Because Keras 3 has dropped support for the default `tf` save format in version 2!\r\n\r\n## Expected behavior\r\n\r\nWhile I understand that issues related to loading legacy Keras models saved with the `tf` save format using Keras are out of scope for TensorFlow and should be addressed by the Keras team, the functionality surrounding TensorFlow's `tf.saved_model`, which uses the `SavedModel` bundle, is part of TensorFlow Core. Since this format is shared across different runtimes, it should remain backward compatible. Therefore, models saved in earlier versions of TensorFlow using the `SavedModel` format should load seamlessly in newer TensorFlow versions, without requiring users to rebuild their models or encountering compatibility errors.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n## Minimal example to reproduce the issue\r\n\r\nA minimal code example to reproduce the issue is available in this repository: [Reproduce TF Model Compatibility Issue.](https:\/\/github.com\/arianmaghsoudnia\/reproduce-tf-model-compat-issue)\r\n\r\nPlease follow the steps in the README file.\r\n\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["type:bug","comp:keras","2.17"],"created_at":"2024-10-09T10:50:39Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77356"},{"issue_number":80,"repository":"tensorflow\/tensorflow","title":"tf.nn.conv2d terminates process with invalid input shape instead of raising an exception","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow terminates the process when passing an invalid input shape to `tf.nn.conv2d`. Instead of raising a Python exception that can be caught with a try-except block.\r\n\r\nI expected TensorFlow to raise a catchable Python exception indicating that the input tensor shape is invalid. This would allow the error to be handled in a try-except block, instead of terminating the process. The error message should clearly explain the shape mismatch issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n# Define invalid input tensor and kernel\r\ninput_tensor = [[1.0, 2.0, 3.0]]\r\nkernel = [[0.5, 0.5], [0.5, 0.5]]\r\n\r\ntry:\r\n    # Create TensorFlow constants\r\n    input_tf = tf.constant(input_tensor, dtype=tf.float32)\r\n    kernel_tf = tf.constant(kernel, dtype=tf.float32)\r\n    \r\n    # Attempt to perform convolution, expecting an error\r\n    output_tf = tf.nn.conv2d(\r\n        tf.expand_dims(input_tf, axis=0), \r\n        tf.expand_dims(kernel_tf, axis=0), \r\n        strides=[1, 1, 1, 1], \r\n        padding='VALID'\r\n    )\r\n    \r\n    print(\"TensorFlow Output:\", output_tf.numpy())\r\nexcept Exception as e:\r\n    print(\"TensorFlow Error:\", e)\n```\n\n\n### Relevant log output\n\n```shell\n2024-10-09 14:53:31.118589: F .\/tensorflow\/core\/util\/tensor_format.h:427] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-09T07:52:38Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77336"},{"issue_number":81,"repository":"tensorflow\/tensorflow","title":"RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.","description":"### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nGoogle Colab with Python 3.10.12\r\n- TensorFlow installation (pip package or built from source):\r\npip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):\r\nv2.17.0\r\n\r\n### 2. Code\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nsaved_model_dir = '\/content\/saved_model'\r\n\r\nnum_calibration_steps = 100\r\n\r\ninput = tf.cast(tf.random.normal((1, 640, 640, 3)), tf.float32)\r\ndummy_input = tf.cast(tf.random.normal((1, 2)), tf.int64)\r\n\r\ndef representative_dataset_gen():\r\n    for _ in range(num_calibration_steps):\r\n        yield [dummy_input, input] #model has 2 input tensors\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\r\n  tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n\r\ntflite_quant_model = converter.convert()\r\n\r\n# Save the quantized model to a local file\r\nwith open('quantized_model.tflite', 'wb') as f:\r\n    f.write(tflite_quant_model)\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\nAfter converting the model from ONNX using onnx2tf, I got saved_model, which I tried to convert to int8 quantized model using the code above. When trying to inference the model, after reading the model by the interpreter and calling the function `allocate_tensors()` \r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=\"\/content\/quantized_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n```\r\nI get the following error:\r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n[<ipython-input-7-b6b80a3bdf94>](https:\/\/localhost:8080\/#) in <cell line: 6>()\r\n      4 interpreter = tf.lite.Interpreter(model_path=\"\/content\/quantized_model.tflite\")\r\n      5 print(interpreter.get_input_details())\r\n----> 6 interpreter.allocate_tensors()\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/lite\/python\/interpreter.py](https:\/\/localhost:8080\/#) in allocate_tensors(self)\r\n    535   def allocate_tensors(self):\r\n    536     self._ensure_safe()\r\n--> 537     return self._interpreter.AllocateTensors()\r\n    538 \r\n    539   def _safe_to_run(self):\r\n\r\nRuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.\r\n```\r\n\r\nCould someone give me some advice, suggestions on how to solve this error? I couldn't even find that anyone has solved the same problem. \r\nThe closest to this error is this [issue](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61395), but the workaround is to convert the ONNX model to Keras and for my complex model it is not possible to fix.\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:lite","TFLiteConverter","2.17"],"created_at":"2024-10-08T22:02:42Z","comments":12,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77293"},{"issue_number":82,"repository":"tensorflow\/tensorflow","title":"tensorflow.python.ops.signal.dct_ops.dct aborts with \"Assertion failure no zero-sized FFTs\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20241007\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)\r\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1oBjZoqp6WZn_VU-CTxZ3bspUc6v9D51s?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\r\nfrom tensorflow.python.eager import def_function\r\nfrom tensorflow.python.framework import tensor_spec\r\nfrom tensorflow.python.ops.signal import dct_ops\r\n\r\ndef test_with_dynamic_dimensions(dct_type, norm, shape, dtype):\r\n    @def_function.function\r\n    def func(signals):\r\n        return dct_ops.dct(signals, n=norm, type=dct_type, norm=None)\r\n    signals_spec = tensor_spec.TensorSpec([None] * len(shape), dtype)\r\n    f = func.get_concrete_function(signals_spec)\r\n    f(np.zeros([0], dtype=dtype))\r\ntest_with_dynamic_dimensions(3, None, [3], np.float32)\n```\n\n\n### Relevant log output\n\n```shell\nDUCC FFT c2r failed: \r\nbazel-out\/k8-opt\/bin\/external\/ducc\/_virtual_includes\/fft\/ducc\/src\/ducc0\/fft\/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):\r\n\r\nAssertion failure\r\nno zero-sized FFTs\r\n\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-08T06:57:49Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77211"},{"issue_number":83,"repository":"tensorflow\/tensorflow","title":"tensorflow.python.ops.parsing_ops.parse_single_sequence_example can cause a crash","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20241007\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\nLinux Ubuntu 20.04.3 LTS\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)\r\n\r\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/17PzKxkDEr3N8E9D9Kk_mT2A1LyPpoZZe?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.core.example import example_pb2\r\nfrom tensorflow.core.example import feature_pb2\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import parsing_ops\r\nexample = example_pb2.Example\r\nfeature = feature_pb2.Feature\r\nfeatures = lambda d: feature_pb2.Features(feature=d)\r\nbytes_feature = lambda v: feature(bytes_list=feature_pb2.BytesList(value=v))\r\nint64_feature = lambda v: feature(int64_list=feature_pb2.Int64List(value=v))\r\nfloat_feature = lambda v: feature(float_list=feature_pb2.FloatList(value=v))\r\nfeature_list = lambda l: feature_pb2.FeatureList(feature=l)\r\nfeature_lists = lambda d: feature_pb2.FeatureLists(feature_list=d)\r\nsequence_example = example_pb2.SequenceExample\r\n\r\ndef testSequenceExampleListWithWrongShapeFails():\r\n    original = sequence_example(feature_lists=feature_lists({'a': feature_list([int64_feature([2, 3]), int64_feature([2, 3, 4])])}))\r\n    serialized = original.SerializeToString()\r\n    parsing_ops.parse_single_sequence_example(**\r\n        ({\r\n            'example_name': 'in1',\r\n            'serialized': ops.convert_to_tensor(serialized),\r\n            'sequence_features': {'a': parsing_ops.FixedLenSequenceFeature((0, 0), dtypes.int64)}\r\n        }))\r\ntestSequenceExampleListWithWrongShapeFails()\n```\n\n\n### Relevant log output\n\n```shell\nFloating point exception (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops"],"created_at":"2024-10-08T06:52:07Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77210"},{"issue_number":84,"repository":"tensorflow\/tensorflow","title":"`SimpleDynamicBuffer::AddString` is calling `memcpy` with null data","description":"I've noticed this hitting on our ubsan builds recently:\r\n\r\n```\r\n..\/..\/third_party\/tflite\/src\/tensorflow\/compiler\/mlir\/lite\/utils\/string_utils.cc:32:10: runtime error: null pointer passed as argument 1, which is declared to never be null\r\n..\/..\/build\/linux\/debian_bullseye_amd64-sysroot\/usr\/include\/string.h:44:28: note: nonnull attribute specified here\r\n    #0 0x5a36de826450 in mlir::TFL::SimpleDynamicBuffer::AddString(char const*, unsigned long) third_party\/tflite\/src\/tensorflow\/compiler\/mlir\/lite\/utils\/string_utils.cc:32:3\r\n    #1 0x5a36de825d3e in tflite::DynamicBuffer::AddString(char const*, unsigned long) third_party\/tflite\/src\/tensorflow\/lite\/string_util.cc:37:28\r\n    #2 0x5a36de82924d in PopulateTensor<std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char> > > third_party\/tflite_support\/src\/tensorflow_lite_support\/cc\/task\/core\/task_utils.h:125:13\r\n    #3 0x5a36de82924d in tflite::task::processor::UniversalSentenceEncoderPreprocessor::Preprocess(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party\/tflite_support\/src\/tensorflow_lite_support\/cc\/task\/processor\/universal_sentence_encoder_preprocessor.cc:58:3\r\n    #4 0x5a36de81d3f7 in tflite::task::text::TextEmbedder::Preprocess(std::__Cr::vector<TfLiteTensor*, std::__Cr::allocator<TfLiteTensor*>> const&, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party\/tflite_support\/src\/tensorflow_lite_support\/cc\/task\/text\/text_embedder.cc:174:25\r\n    #5 0x5a36de81cd8c in tflite::task::core::BaseTaskApi<tflite::task::processor::EmbeddingResult, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&>::InferWithFallback(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party\/tflite_support\/src\/tensorflow_lite_support\/cc\/task\/core\/base_task_api.h:146:5\r\n    #6 0x5a36de81cc40 in tflite::task::text::TextEmbedder::Embed(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party\/tflite_support\/src\/tensorflow_lite_support\/cc\/task\/text\/text_embedder.cc:169:10\r\n    #7 0x5a36d2728c24 in ai_chat::TextEmbedder::EmbedText(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&, tflite::task::processor::EmbeddingResult&) brave\/components\/ai_chat\/core\/browser\/text_embedder.cc:271:49\r\n    #8 0x5a36d2728073 in ai_chat::TextEmbedder::EmbedSegments() brave\/components\/ai_chat\/core\/browser\/text_embedder.cc:287:19\r\n    #9 0x5a36c8c67059 in ai_chat::TextEmbedderUnitTest::EmbedSegments(ai_chat::TextEmbedder*)::'lambda'()::operator()() const brave\/components\/ai_chat\/core\/browser\/text_embedder_unittest.cc:67:58\r\n    #10 0x5a36c6762969 in base::OnceCallback<void ()>::Run() && base\/functional\/callback.h:156:12\r\n    #11 0x5a36d4977df2 in base::TaskAnnotator::RunTaskImpl(base::PendingTask&) base\/task\/common\/task_annotator.cc:202:34\r\n    #12 0x5a36d49dcfe9 in RunTask<(lambda at ..\/..\/base\/task\/thread_pool\/task_tracker.cc:678:35)> base\/task\/common\/task_annotator.h:90:5\r\n    #13 0x5a36d49dcfe9 in base::internal::TaskTracker::RunTaskImpl(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base\/task\/thread_pool\/task_tracker.cc:677:19\r\n    #14 0x5a36d49dd0f1 in base::internal::TaskTracker::RunSkipOnShutdown(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base\/task\/thread_pool\/task_tracker.cc:662:3\r\n    #15 0x5a36d49dc1f5 in base::internal::TaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base\/task\/thread_pool\/task_tracker.cc:520:5\r\n    #16 0x5a36d4af81fb in base::test::TaskEnvironment::TestTaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base\/test\/task_environment.cc:1028:46\r\n    #17 0x5a36d49db5a5 in base::internal::TaskTracker::RunAndPopNextTask(base::internal::RegisteredTaskSource) base\/task\/thread_pool\/task_tracker.cc:415:5\r\n    #18 0x5a36d4a0cabd in base::internal::WorkerThread::RunWorker() base\/task\/thread_pool\/worker_thread.cc:493:36\r\n    #19 0x5a36d4a0c100 in base::internal::WorkerThread::RunPooledWorker() base\/task\/thread_pool\/worker_thread.cc:379:3\r\n    #20 0x5a36d4a0bc86 in base::internal::WorkerThread::ThreadMain() base\/task\/thread_pool\/worker_thread.cc:359:7\r\n    #21 0x5a36d4a3e1ec in base::(anonymous namespace)::ThreadFunc(void*) base\/threading\/platform_thread_posix.cc:101:13\r\n    #22 0x7695b109ca93 in start_thread nptl\/pthread_create.c:447:8\r\n    #23 0x7695b1129c3b in clone3 misc\/..\/sysdeps\/unix\/sysv\/linux\/x86_64\/clone3.S:78\r\n```","labels":["type:bug","comp:lite","TFLiteConverter","awaiting PR merge"],"created_at":"2024-10-07T18:57:35Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77168"},{"issue_number":85,"repository":"tensorflow\/tensorflow","title":"NotImplementedError from tf.constant in trivial case","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.16.1\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.12\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nTrying to make a tensor that has the same value for all items in the batch, see the following bare minimum code. \r\nI get `NotImplementedError: cannot convert a symbolic tf.Tensor (custom_model_5_1\/strided_slice:0) to a numpy array.`\r\nI am not trying to use numpy, this is an internal error.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\nimport keras\r\nimport numpy as np\r\n\r\nclass CustomModel(keras.models.Model):\r\n    def call(self, inputs):\r\n        inputs_shape = tf.shape(inputs)\r\n        return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # NotImplementedError\r\n        #return 3.0 * tf.ones(shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # OK\r\n\r\nmodel = CustomModel()\r\nmodel.compile(run_eagerly=False, loss=\"mse\")  # OK if run_eagerly=True\r\nmodel.fit(np.array([[0.0]]), np.array([[0.0]]))\r\n```\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n{\r\n\t\"name\": \"NotImplementedError\",\r\n\t\"message\": \"Exception encountered when calling CustomModel.call().\r\n\r\nCannot convert a symbolic tf.Tensor (custom_model_5_1\/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\r\n\r\nArguments received by CustomModel.call():\r\n  \u2022 inputs=tf.Tensor(shape=(None, 1), dtype=float32)\",\r\n\t\"stack\": \"---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\nCell In[6], line 13\r\n     11 model = CustomModel()\r\n     12 model.compile(run_eagerly=False, loss=\\\"mse\\\")  # OK if run_eagerly=True\r\n---> 13 model.fit(np.array([[0.0]]), np.array([[0.0]]))\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n    119     filtered_tb = _process_traceback_frames(e.__traceback__)\r\n    120     # To get the full stack trace, call:\r\n    121     # `keras.config.disable_traceback_filtering()`\r\n--> 122     raise e.with_traceback(filtered_tb) from None\r\n    123 finally:\r\n    124     del filtered_tb\r\n\r\nCell In[6], line 8, in CustomModel.call(self, inputs)\r\n      6 def call(self, inputs):\r\n      7     inputs_shape = tf.shape(inputs)\r\n----> 8     return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/numpy\/core\/fromnumeric.py:3100, in prod(a, axis, dtype, out, keepdims, initial, where)\r\n   2979 @array_function_dispatch(_prod_dispatcher)\r\n   2980 def prod(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\r\n   2981          initial=np._NoValue, where=np._NoValue):\r\n   2982     \\\"\\\"\\\"\r\n   2983     Return the product of array elements over a given axis.\r\n   2984 \r\n   (...)\r\n   3098     10\r\n   3099     \\\"\\\"\\\"\r\n-> 3100     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\r\n   3101                           keepdims=keepdims, initial=initial, where=where)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/numpy\/core\/fromnumeric.py:88, in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)\r\n     85         else:\r\n     86             return reduction(axis=axis, out=out, **passkwargs)\r\n---> 88 return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n\r\nNotImplementedError: Exception encountered when calling CustomModel.call().\r\n\r\nCannot convert a symbolic tf.Tensor (custom_model_5_1\/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\r\n\r\nArguments received by CustomModel.call():\r\n  \u2022 inputs=tf.Tensor(shape=(None, 1), dtype=float32)\"\r\n}\r\n```\r\n","labels":["type:bug","TF 2.16"],"created_at":"2024-10-04T13:38:25Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77045"},{"issue_number":86,"repository":"tensorflow\/tensorflow","title":"TFlite compilation crashes on MacOS (error: _Float16 is not supported on this target)","description":"### Issue type\r\n\r\nBuild\/Install\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18.0-rc0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nMacOS 15.0\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.12\r\n\r\n### Bazel version\r\n\r\n6.5\r\n\r\n### GCC\/compiler version\r\n\r\nApple clang version 16.0.0 (clang-1600.0.26.3)\r\nXCode 16.0\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nCompiling TF_lite v2.18.0-rc0 from source, using the command suggested in the documentation leads to crash (log below):\r\n\r\n```PYTHON=python3 tensorflow\/lite\/tools\/pip_package\/build_pip_package_with_bazel.sh native```\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nPYTHON=python3 tensorflow\/lite\/tools\/pip_package\/build_pip_package_with_bazel.sh native\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nTF2_BEHAVIOR=1 \\\r\n    XCODE_VERSION_OVERRIDE=16.0.0.16A242d \\\r\n    ZERO_AR_DATE=1 \\\r\n  external\/local_config_cc\/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' 'DEBUG_PREFIX_MAP_PWD=.' -iquote external\/XNNPACK -iquote bazel-out\/darwin-opt\/bin\/external\/XNNPACK -iquote external\/pthreadpool -iquote bazel-out\/darwin-opt\/bin\/external\/pthreadpool -iquote external\/FXdiv -iquote bazel-out\/darwin-opt\/bin\/external\/FXdiv -iquote external\/cpuinfo -iquote bazel-out\/darwin-opt\/bin\/external\/cpuinfo -iquote external\/FP16 -iquote bazel-out\/darwin-opt\/bin\/external\/FP16 -Ibazel-out\/darwin-opt\/bin\/external\/pthreadpool\/_virtual_includes\/pthreadpool -Ibazel-out\/darwin-opt\/bin\/external\/FXdiv\/_virtual_includes\/FXdiv -Ibazel-out\/darwin-opt\/bin\/external\/cpuinfo\/_virtual_includes\/cpuinfo -Ibazel-out\/darwin-opt\/bin\/external\/FP16\/_virtual_includes\/FP16 -isystem external\/XNNPACK\/include -isystem bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/include -isystem external\/XNNPACK\/src -isystem bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/src -isystem external\/pthreadpool\/include -isystem bazel-out\/darwin-opt\/bin\/external\/pthreadpool\/include -isystem external\/FXdiv\/include -isystem bazel-out\/darwin-opt\/bin\/external\/FXdiv\/include -isystem external\/cpuinfo\/include -isystem bazel-out\/darwin-opt\/bin\/external\/cpuinfo\/include -isystem external\/cpuinfo\/src -isystem bazel-out\/darwin-opt\/bin\/external\/cpuinfo\/src -isystem external\/FP16\/include -isystem bazel-out\/darwin-opt\/bin\/external\/FP16\/include -MD -MF bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/_objs\/microkernel_configs\/cmul-config.d -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_LOG_LEVEL=0' '-DXNN_ENABLE_CPUINFO=1' '-DXNN_ENABLE_MEMOPT=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=1' '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_SPARSE=1' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_ARM_I8MM=0' '-DXNN_ENABLE_RISCV_FP16_VECTOR=0' '-DXNN_ENABLE_AVX512VNNIGFNI=1' '-DXNN_ENABLE_AVX512AMX=1' '-DXNN_ENABLE_AVX512FP16=1' '-DXNN_ENABLE_AVXVNNI=1' '-DXNN_ENABLE_AVXVNNIINT8=1' '-DXNN_ENABLE_AVX256SKX=1' '-DXNN_ENABLE_AVX256VNNI=1' '-DXNN_ENABLE_AVX256VNNIGFNI=1' '-DXNN_ENABLE_HVX=0' '-DXNN_ENABLE_KLEIDIAI=0' '-DBAZEL_CURRENT_REPOSITORY=\"XNNPACK\"' '-frandom-seed=bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/_objs\/microkernel_configs\/cmul-config.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__\/System\/Library\/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__\/Platforms\/MacOSX.platform\/Developer\/Library\/Frameworks -no-canonical-prefixes -pthread -DGRPC_BAZEL_BUILD -w -O3 '-march=native' -Iinclude -Isrc '-DXNN_ENABLE_CPUINFO=1' '-std=c99' -O2 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -target x86_64-apple-macosx15.0 -c external\/XNNPACK\/src\/configs\/cmul-config.c -o bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/_objs\/microkernel_configs\/cmul-config.o)\r\n# Configuration: 7ffafbfebd31d6f3229fc3b4603178937ffcc0387347731c7b47fcb97e2cd76d\r\n# Execution platform: @local_execution_config_platform\/\/:platform\r\nERROR: \/private\/var\/tmp\/_bazel_feranick\/50b852099a3bf3aaa184abce166f8e34\/external\/XNNPACK\/BUILD.bazel:803:36: Compiling external\/XNNPACK\/sse_prod_microkernels.c failed: (Exit 1): wrapped_clang failed: error executing command (from target @XNNPACK\/\/:sse_prod_microkernels) external\/local_config_cc\/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' ... (remaining 109 arguments skipped)\r\nIn file included from bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/sse_prod_microkernels.c:1:\r\nIn file included from external\/XNNPACK\/src\/xnnpack\/avgpool.h:15:\r\nIn file included from external\/XNNPACK\/src\/xnnpack\/microparams.h:12:\r\nIn file included from external\/XNNPACK\/src\/xnnpack\/math.h:21:\r\nIn file included from bazel-out\/darwin-opt\/bin\/external\/FP16\/_virtual_includes\/FP16\/fp16\/fp16.h:10:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:614:27: error: _Float16 is not supported on this target\r\n  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:614:8: error: _Float16 is not supported on this target\r\n  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:615:28: error: _Float16 is not supported on this target\r\n  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:615:38: error: _Float16 is not supported on this target\r\n  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                                      ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:615:8: error: _Float16 is not supported on this target\r\n  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:616:27: error: _Float16 is not supported on this target\r\n  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:616:8: error: _Float16 is not supported on this target\r\n  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:617:27: error: _Float16 is not supported on this target\r\n  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:617:8: error: _Float16 is not supported on this target\r\n  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:618:28: error: _Float16 is not supported on this target\r\n  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:618:8: error: _Float16 is not supported on this target\r\n  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:619:27: error: _Float16 is not supported on this target\r\n  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:619:8: error: _Float16 is not supported on this target\r\n  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:620:28: error: _Float16 is not supported on this target\r\n  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:620:8: error: _Float16 is not supported on this target\r\n  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:621:28: error: _Float16 is not supported on this target\r\n  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:621:8: error: _Float16 is not supported on this target\r\n  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:622:31: error: _Float16 is not supported on this target\r\n  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                               ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:622:41: error: _Float16 is not supported on this target\r\n  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                                         ^\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated.\r\nError in child process '\/usr\/bin\/xcrun'. 1\r\nTarget \/\/tensorflow\/lite\/python\/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 119.893s, Critical Path: 3.17s\r\nINFO: 342 processes: 313 internal, 29 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","comp:lite","2.17"],"created_at":"2024-10-02T18:20:47Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76976"},{"issue_number":87,"repository":"tensorflow\/tensorflow","title":"Jit-compiling `tf.while_loop` inside `tf.vectorized_map` raises `InvalidArgumentError`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nColab\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nMRE\r\n-------\r\nThe following mock-up of `cumsum` attempts to JIT compile a `tf.vectorized_map`ped function containing a `tf.scan`:\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef cumsum(xs):\r\n    return tf.scan(\r\n        lambda a, x: a + x, elems=xs\r\n    )\r\n\r\n@tf.function(jit_compile=True)\r\ndef vec_cumsum(xs):\r\n    return tf.vectorized_map(cumsum, elems=xs)\r\n\r\nxs_batched = tf.reshape(tf.range(30), (3, 10))\r\nvec_cumsum(xs_batched)\r\n```\r\n\r\n__Expected behaviour__: `vec_cumsum(xs_batched)` returns a batch of cumulative sums.\r\n\r\n__Actual behaviour__: Even though all data structures are known statically at JIT compile time, an InvalidArgumentError is raised with \"No registered 'TensorListReserve'\".  The fault is clearly related to `tf.scan`'s use of `tf.while_loop`, as a (longer) example using naked `tf.while_loop(..., max_iterations=n)` will confirm.\r\n\r\nIn JAX, it is possible to jit-compile a `vmap`ped function containing a `lax.while_loop` indicating that this is possible in HLO.  It seems the `tf.function(jit_compile=True)` machinery may be mis-transpiling to HLO somehow.\r\n\r\nMay be related to #73367 also involving `tf.vectorized_map` and `tf.while_loop` (albeit with reversed scope)?\n\n### Standalone code to reproduce the issue\n\n```shell\nColab MRE: https:\/\/colab.research.google.com\/drive\/1bmq1t3PdtebCSlNd0t-iEFrXX7Q0qqZp?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-26ea491cd046> in <cell line: 13>()\r\n     11 \r\n     12 # Fails with \"No registered 'TensorListReserve'\"\"\r\n---> 13 vec_cumsum(xs_batched)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51   try:\r\n     52     ctx.ensure_initialized()\r\n---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                         inputs, attrs, num_outputs)\r\n     55   except core._NotOkStatusException as e:\r\n\r\nInvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_vec_cumsum_462[_XlaMustCompile=true,config_proto=13561319589895757934,executor_type=11160318154034397263] on XLA_CPU_JIT: TensorListReserve (No registered 'TensorListReserve' OpKernel for XLA_CPU_JIT devices compatible with node {{function_node __inference_while_fn_428}}{{node while_init\/TensorArrayV2_4}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: element_dtype=DT_VARIANT, shape_type=DT_INT32){{function_node __inference_while_fn_428}}{{node while_init\/TensorArrayV2_4}}\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","comp:ops","2.17"],"created_at":"2024-10-01T17:01:36Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76891"},{"issue_number":88,"repository":"tensorflow\/tensorflow","title":"Multithreading is not working with teansorflow","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntensorflow==2.15.0.post1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\npython:3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI am using bert model for classification and serving the model with gunicorn worker_class=gthreads, tf.config.threading.set_intra_op_parallelism_threads(1)\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\n\r\nwhen I using above two line of code the code is working fine as expected and if increase the number to more than 1, the code getting blocked at the below line of code\r\n\r\n# Make predictions\r\noutputs = model_obj(inputs)\r\n\r\nand also inorder to reduce the docker image size I am using \r\nRUN pip3 install torch==2.0.0+cpu -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\r\n\r\n\r\nbefore installing all the dependencies\r\nFlask==2.2.5\r\ng2p-en==2.1.0\r\ngunicorn==21.2.0\r\njellyfish==1.0.3\r\nkenlm==0.2.0\r\nnltk==3.8.1\r\nnumpy==1.26.3\r\npandas==2.2.0\r\npython-dotenv==1.0.1\r\nrequests==2.31.0\r\nscikit-learn==1.4.0\r\nsemantic-router==0.0.17\r\nsemantic-router[fastembed]\r\nsentence-transformers==2.3.1\r\ntensorflow==2.15.0.post1\r\ntensorflow-hub==0.16.0\r\ntheano==1.0.5\r\ntransformers==4.37.2\r\nWerkzeug==2.2.2\r\n\r\n\r\nplease tell me why is my code is getting blocked if I use more than 1 thread.\n\n### Standalone code to reproduce the issue\n\n```shell\ndef intent_prediction(self, sentence, thresold_score):\r\n        logger.info(f\"Threshold Score: {thresold_score}\")\r\n        try:\r\n            model_obj = intent_object_dict[self.model]\r\n        except KeyError:\r\n            logger.error(\"Model not found in intent_object_dict\")\r\n            model_obj = self.load_model()\r\n\r\n        if INTENT_MODEL == \"cohere\":\r\n            score, intent = self.cohere_intent_prediction(sentence, model_obj)\r\n        else:\r\n            score, intent = self.Bert_intent_prediction(\r\n                sentence, model_obj, thresold_score\r\n            )\r\n        return score, intent\r\n\r\ndef Bert_intent_prediction(self, sentence, model_obj, thresold_score):\r\n        inputs = self.load_BERT_tokenizer(sentence)\r\n        # Make predictions\r\n        outputs = model_obj(inputs)\r\n        # Get predicted class\r\n        probabilities = tf.nn.softmax(outputs.logits, axis=1)\r\n        predicted_class = tf.argmax(probabilities, axis=1).numpy()[0]\r\n        matching_score = probabilities[0][predicted_class].numpy()\r\n        try:\r\n            intent_data = intent_label_dict[self.model]\r\n        except Exception as e:\r\n            logger.error(f\"error while getting label: {e}\")\r\n            intent_data = self.get_intent_labels()\r\n\r\n        if matching_score >= thresold_score:\r\n            logger.info(\r\n                f\"Matched Main Intent:\\\r\n    {intent_data[predicted_class]},\\\r\n    SCORE :{matching_score}\"\r\n            )\r\n            logger.info(f\"Matched Sentence: {intent_data[predicted_class]}\")\r\n        else:\r\n            logger.info(f\"Intent not matched, score is {matching_score}\")\r\n\r\n        intent = intent_data[predicted_class]\r\n\r\n        return str(matching_score), intent\n```\n\n\n### Relevant log output\n\n```shell\n30-Sep-2024 15:50:42.761|INFO    |__init__|I want my sofa get cleaned|\r\n    __init__.py:171|Enter into PUNC for intent...\r\n30-Sep-2024 15:50:42.761|INFO    |phrase_sim|I want my sofa get cleaned|\r\n    phrase_sim.py:72|Threshold Score: 0.7\r\n\r\n\r\nafter this the code is blocked\n```\n","labels":["stat:awaiting tensorflower","type:bug","TF 2.15"],"created_at":"2024-09-30T10:37:06Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76794"},{"issue_number":89,"repository":"tensorflow\/tensorflow","title":"Segmentation fault (core dumped) in `tf.data.experimental.SqlDataset`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0-dev20240925\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the illegal input to tf.data.experimental.SqlDataset triggered when a crash, and will only come when iteration data.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ndata_source_name = \"sqlite:\/\/\/path\/to\/correct_database.db\"\r\n\r\nquery = \"SELECT id, name FROM my_table\"\r\noutput_types = (tf.int64, tf.string)\r\ndataset = tf.data.experimental.SqlDataset(\r\n    'sqlite', data_source_name, query, output_types)\r\n\r\nfor element in dataset:\r\n    print(element)\n```\n\n\n### Relevant log output\n\n```shell\n2024-09-28 21:18:33.844482: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 21:18:33.907260: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 21:18:33.986019: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 21:18:34.009755: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 21:18:34.068897: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-28 21:18:38.768599: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-09-28 21:18:38.769172: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-09-28 21:18:39.132534: W tensorflow\/core\/kernels\/data\/experimental\/sql_dataset_op.cc:209] Failed to connect to database: INVALID_ARGUMENT: Sqlite::Open(sqlite:\/\/\/path\/to\/correct_database.db) failed: unable to open database file\r\nSegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T13:20:59Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76731"},{"issue_number":90,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.linalg.det\/slogdet\/logdet\/cholesky\/inv`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18.0-dev20240925\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04.3 LTS (x86_64)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\ntf.linalg.det\/slogdet\/logdet\/cholesky\/inv triggered a crash when the input is empty. Note that this will only be triggered if the gpu is available.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\ninvalid_input = tf.zeros([])\r\ntf.linalg.det(invalid_input)    # crash\r\ntf.linalg.slogdet(invalid_input)  # crash\r\ntf.linalg.cholesky(invalid_input)  # crash\r\ntf.linalg.logdet(invalid_input)  # crash\r\ntf.linalg.inv(invalid_input)  # crash\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-09-28 21:11:10.188752: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 21:11:10.199880: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 21:11:10.213635: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 21:11:10.221654: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 21:11:10.279720: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-28 21:11:17.015480: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-09-28 21:11:17.015957: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-09-28 21:11:17.154391: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T13:15:48Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76730"},{"issue_number":91,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.ResourceScatterNdop`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0-dev20240925\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the type of resource_handle is inconsistent with that of updates,tf.raw_ops.ResourceScatterNdop triggers the crash. As follows:\r\ntf.raw_ops.ResourceScatterNdUpdate\r\ntf.raw_ops.ResourceScatterNdAdd\r\ntf.raw_ops.ResourceScatterNdSub\r\ntf.raw_ops.ResourceScatterNdMax\r\ntf.raw_ops.ResourceScatterNdMin\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nresource_var = tf.Variable(initial_value=tf.zeros([2, 2], dtype=tf.int32), trainable=False)\r\nresource_handle = resource_var.handle\r\n\r\nindices = np.array([[2, 1], [1, 2]], dtype=np.int32)\r\nupdates = np.array([10, 20], dtype=np.float32)\r\ntf.raw_ops.ResourceScatterNdUpdate(  # crash\r\n    ref=resource_handle,\r\n    indices=indices,\r\n    updates=updates,\r\n    use_locking=True\r\n)\r\n\r\ntf.raw_ops.ResourceScatterNdAdd(  # crash\r\n    ref=resource_handle,\r\n    indices=indices,\r\n    updates=updates,\r\n    use_locking=True\r\n)\r\ntf.raw_ops.ResourceScatterNdSub(  # crash\r\n    ref=resource_handle,\r\n    indices=indices,\r\n    updates=updates,\r\n    use_locking=True\r\n)\r\ntf.raw_ops.ResourceScatterNdMax(  # crash\r\n    ref=resource_handle,\r\n    indices=indices,\r\n    updates=updates,\r\n    use_locking=True\r\n)\r\ntf.raw_ops.ResourceScatterNdMin(  # crash\r\n    ref=resource_handle,\r\n    indices=indices,\r\n    updates=updates,\r\n    use_locking=True\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-09-28 21:06:23.445185: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 21:06:23.508056: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 21:06:23.583640: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 21:06:23.607538: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 21:06:23.664877: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-28 21:06:31.527466: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-09-28 21:06:31.527985: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-09-28 21:06:31.782114: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (3 vs. 1) float expected, got int32\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T13:09:06Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76729"},{"issue_number":92,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.io.encode_png`\/`tf.compat.v1.image.encode_png`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18.0-dev20240925\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nThe crash was triggered when an illegal image was passed to tf.io.encode_png\/tf.compat.v1.image.encode_png\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\nimage = tf.cast(tf.tile([[[0, 0, 0, 1]], [[0, 0, 1, 0]]], [0, 0, 1]), tf.uint8)\r\n\r\nencoded_image = tf.compat.v1.image.encode_png(image) # crash\r\ntf.io.encode_png(image, compression=-1, name=None) #crash\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-09-28 20:48:36.270008: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 20:48:36.332972: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 20:48:36.411391: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 20:48:36.428306: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 20:48:36.438336: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-09-28 20:48:41.296886: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.92024-09-28 20:48:41.297450: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-09-28 20:48:41.475588: F tensorflow\/core\/lib\/png\/png_io.cc:350] 'image' Must be non NULL\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T12:49:42Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76726"},{"issue_number":93,"repository":"tensorflow\/tensorflow","title":"Floating point exception (core dumped) in `tf.nn.depth_to_space`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0-dev20240925\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific inputs, tf.nn.depth_to_space triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ntry:\r\n    # Create an empty tensor\r\n    arg_0_tensor = tf.zeros([0, 2, 3, 12], dtype=tf.float32)\r\n    # arg_0 = tf.identity(arg_0_tensor)\r\n    arg_1 = 536870912\r\n    out = tf.nn.depth_to_space(arg_0_tensor, arg_1)\r\nexcept Exception as e:\r\n    print(\"Error:\", str(e))\n```\n\n\n### Relevant log output\n\n```shell\n2024-09-28 20:41:05.888017: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 20:41:05.950498: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 20:41:06.028236: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 20:41:06.052072: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 20:41:06.111011: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-28 20:41:11.970896: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 2704 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-09-28 20:41:11.973176: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\nFloating point exception (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T12:41:49Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76724"},{"issue_number":94,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.nn.max_pool\/tf.nn.max_pool1d`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04.3 LTS (x86_64)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nUnder specific inputs, tf.nn.max_pool triggered a crash.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\ninvalid_kernel_size = -1\r\ninvalid_operation = tf.nn.max_pool(\r\n    tf.random.normal([1, 32, 32, 3]),\r\n    ksize=[1, invalid_kernel_size, invalid_kernel_size, 1],\r\n    strides=[1, 2, 2, 1],\r\n    padding='SAME'\r\n)\r\n```\r\n\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\n\r\nksize = sys.maxsize + 100  # Set to a value larger than sys.maxsize\r\ninput_tensor = tf.random.normal(shape=(2, 10, 4))\r\nresult = tf.nn.max_pool1d(input=input_tensor, ksize=ksize, strides=1, padding='SAME')\r\n\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-09-28 20:26:47.491907: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 20:26:47.554171: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 20:26:47.606570: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 20:26:47.610539: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 20:26:47.639739: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-28 20:26:54.579839: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 21471 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-09-28 20:26:54.582099: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-09-28 20:26:55.563477: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:531] Loaded cuDNN version 8907\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nF0000 00:00:1727526415.563805  147227 cuda_dnn.cc:1107] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0) \r\n*** Check failure stack trace: ***\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T12:28:21Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76722"},{"issue_number":95,"repository":"tensorflow\/tensorflow","title":"Segmentation fault (core dumped) in `tf.profiler.experimental.Profile`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0-dev20240925\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific inputs, tf.profiler.experimental.Profile triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nprofiler_options = tf.profiler.experimental.ProfilerOptions(\r\n    host_tracer_level=999,\r\n    python_tracer_level=-1,\r\n    device_tracer_level=10,\r\n    delay_ms=None\r\n)\r\n\r\nwith tf.profiler.experimental.Profile(None, options=profiler_options):\r\n    a = tf.constant(1)\r\n    b = tf.constant(2)\r\n    c = a + b\r\n    print(c.numpy())\n```\n\n\n### Relevant log output\n\n```shell\n2024-09-28 20:07:36.902909: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-09-28 20:07:36.966049: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 20:07:36.998027: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 20:07:37.002984: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered2024-09-28 20:07:37.055864: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nSegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T12:11:00Z","comments":2,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76718"},{"issue_number":96,"repository":"tensorflow\/tensorflow","title":"TensorFlow keeps creating threads when multi-GPU training \uff08thread leak\uff09","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.11.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n11.4\r\n\r\n### GPU model and memory\r\n\r\nNvidia A800 \r\n\r\n### Current behavior?\r\n\r\nI was using this machine to train a GPT-2 example from (the data I used can also be found in this link, also here https:\/\/github.com\/chinese-poetry\/chinese-poetry.git) https:\/\/keras.io\/examples\/generative\/gpt2_text_generation_with_kerasnlp\/\r\n\r\nBefore we start, I would give a baseline amount of this machine's threads :\r\n![10](https:\/\/github.com\/user-attachments\/assets\/53e16381-e50f-4054-8c2a-b790fe2e077b)\r\n\r\nWhen starting with the multi-GPU training of tensorflow by calling the tf.distribute.MirroredStrategy, the training process worked fine as usual. \r\n\r\nBut with the time went by, I found the amount of threads increased with the training process going, here is the evidence of thread increasing when processing 3354th batch (I used cat \/proc\/\"this programs' pid\"\/status to check the number of threads):\r\n![3](https:\/\/github.com\/user-attachments\/assets\/56bbfaf0-f59c-48f1-ae19-a98d563ad000)\r\n![tf](https:\/\/github.com\/user-attachments\/assets\/c546ea4b-855a-44e5-84c1-95d6d3f3aba4)\r\n![2](https:\/\/github.com\/user-attachments\/assets\/0d07b24d-e93e-451e-9e1e-51a26f7db60d)\r\n\r\nThen, the evidence of thread increasing when processing 3791st batch (the amount of threads reached 22178):\r\n![6](https:\/\/github.com\/user-attachments\/assets\/508fed5c-8aec-4deb-8e68-74dbf6aa5613)\r\n![4](https:\/\/github.com\/user-attachments\/assets\/77507790-70fd-4abf-b72c-e9d831565879)\r\n![5](https:\/\/github.com\/user-attachments\/assets\/d8945e53-23d9-4fb7-b50d-37962b93a692)\r\n\r\nWhen calculating  the 5054th batch, the training program got an error, and I captured a count of threads before the error (achieved around 31120 threads):\r\n![8](https:\/\/github.com\/user-attachments\/assets\/15b433cc-eefb-451f-a31c-2a286e17afec)\r\n![8](https:\/\/github.com\/user-attachments\/assets\/813835d2-3834-400a-b8fe-ec27f15d89ad)\r\n\r\nI checked an similar issue in https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62466, but I cannot find a solution, moreover, I have run other examples like diffusion model using this machine and the same tensorflow env with multi-GPU training, which worked fine and no any problems. So, could you please give me a help for this problem, very appreciated.\r\n\r\n\r\n\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nHere is my code\r\n\r\nimport os\r\n\r\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\r\n\r\nimport keras_nlp\r\nimport keras\r\nimport tensorflow as tf\r\nimport time\r\n\r\nkeras.mixed_precision.set_global_policy(\"mixed_float16\")\r\n\r\nimport os\r\nimport json\r\nimport datetime\r\n\r\ntrain_ds = (\r\n    tf.data.Dataset.from_tensor_slices(paragraphs)\r\n    .batch(36)\r\n    .cache()\r\n    .prefetch(tf.data.AUTOTUNE)\r\n)\r\n\r\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\", \"GPU:4\", \"GPU:5\"])\r\nprint(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\r\n\r\npreprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\r\n    \"gpt2_base_en\",\r\n    sequence_length=128,\r\n)\r\n\r\n# Open a strategy scope.\r\nwith strategy.scope():\r\n# To speed up training and generation, we use preprocessor of length 128\r\n# instead of full length 1024.\r\n    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\r\n        \"gpt2_base_en\", preprocessor=preprocessor\r\n    )\r\n    num_epochs = 5\r\n\r\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n        filepath=checkpoint_path,\r\n        save_weights_only=True,\r\n        monitor=\"accuracy\",\r\n        # monitor=\"i_loss\",\r\n        mode=\"min\",\r\n        save_best_only=True,\r\n        save_freq=\"epoch\"\r\n    )\r\n    learning_rate = 5e-4\r\n    \r\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n    gpt2_lm.compile(\r\n        optimizer=keras.optimizers.Adam(learning_rate),\r\n        loss=loss,\r\n        weighted_metrics=[\"accuracy\"],\r\n    )\r\n\r\n    gpt2_lm.fit(train_ds, epochs=num_epochs, callbacks=[\r\n        checkpoint_callback,\r\n        tensorboard_callback,\r\n    ],\r\n                )\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-09-21 00:24:25.840848: W tensorflow\/compiler\/tf2xla\/kernels\/random_ops.cc:57] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. gpt2_causal_lm\/gpt2_backbone\/embeddings_dropout\/dropout\/random_uniform\/RandomUniform\r\n2024-09-21 00:24:25.846135: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2024-09-21 00:24:26.559075: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n2024-09-21 00:24:26.572007: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n2024-09-21 00:24:26.573183: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n2024-09-21 00:24:26.581128: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n2024-09-21 00:24:26.581197: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n2024-09-21 00:24:26.588190: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n12024-09-21 00:25:01.986918: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n2024-09-21 00:25:02.215951: I tensorflow\/compiler\/jit\/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n2024-09-21 00:25:02.420703: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n2024-09-21 00:25:02.542130: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n2024-09-21 00:25:03.595774: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n12024-09-21 00:25:04.071526: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n2024-09-21 00:25:04.130504: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n2024-09-21 00:25:05.333297: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n5054\/8663 [================>.............] - ETA: 9:14 - loss: 11.8715 - accuracy: 2.2702terminate called after throwing an instance of 'std::system_error'\r\nterminate called recursively\r\n  what():  Resource temporarily unavailable\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:dist-strat","TF 2.11"],"created_at":"2024-09-20T17:14:03Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76157"},{"issue_number":97,"repository":"tensorflow\/tensorflow","title":"tf.python.ops.array_ops.transpose aborts with \"Check failed: d >= 0 (0 vs. -1)\"","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf-nightly 2.18.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04.3 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.14\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI encountered an `aborted issue` in TensorFlow when I used API `array_ops.transpose`\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nfrom tensorflow.python.ops import array_ops\r\n\r\nx =np.arange(0, 8).reshape([2, 4]).astype(np.float32)\r\ny = np.array([-1, 0]).astype(np.int32)\r\narray_ops.transpose(x, y,conjugate = False)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-09-19 16:16:30.137164: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-19T08:31:38Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76036"},{"issue_number":98,"repository":"tensorflow\/tensorflow","title":"Code error when feature name has multiple `_`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n5.15.149-99.162.amzn2.x86_64\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\nPython 3.10.14\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nExpect there shouldn't be errors just by changing feature name. \r\n\r\n### Standalone code to reproduce the issue\r\nThis is a code sample that will work normally\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Dense, Concatenate\r\nfrom tensorflow.keras import Model\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame()\r\nnumeric_feature_name = 'a' * 27\r\ncategorical_feature_name = 'b' * 11\r\ndf[numeric_feature_name] = range(1000)\r\ndf[categorical_feature_name] = 'a'\r\ndf['label'] = 1\r\n\r\nnumeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')\r\ncategorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=\"string\")\r\nencoding_layer = get_category_encoding_layer(vocab=['a'])\r\nencoded_categorical_feature = encoding_layer(categorical_feature_layer)\r\n\r\nall_inputs = [numeric_feature_layer, categorical_feature_layer]\r\nencoded_features = [numeric_feature_layer, encoded_categorical_feature]\r\nconcat_features = Concatenate()(encoded_features)\r\noutput = Dense(units=1, activation='sigmoid')(concat_features)\r\nmodel = Model(inputs=all_inputs, outputs=output)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\ndataframe_x = df[[numeric_feature_name, categorical_feature_name]]\r\ndataframe_y = df['label']\r\ndf2 = ((dict(dataframe_x), dataframe_y))\r\nds = tf.data.Dataset.from_tensor_slices(df2)\r\nds = ds.batch(32)\r\nds_train = ds\r\n\r\nmodel.fit(\r\n    ds_train,\r\n    epochs=10,\r\n    batch_size=300,\r\n    verbose=1\r\n)\r\n```\r\n\r\nHowever, if I change the feature name, the same code will throw error\r\n\r\n```\r\ndf = pd.DataFrame()\r\n## Just change the feature name here\r\nnumeric_feature_name = 'a_b_c_d_e_f_g' \r\ncategorical_feature_name = 'a_b_c_d_e_f'\r\ndf[numeric_feature_name] = range(1000)\r\ndf[categorical_feature_name] = 'a'\r\ndf['label'] = 1\r\n\r\nnumeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')\r\ncategorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=\"string\")\r\nencoding_layer = get_category_encoding_layer(vocab=['a'])\r\nencoded_categorical_feature = encoding_layer(categorical_feature_layer)\r\n\r\nall_inputs = [numeric_feature_layer, categorical_feature_layer]\r\nencoded_features = [numeric_feature_layer, encoded_categorical_feature]\r\nconcat_features = Concatenate()(encoded_features)\r\noutput = Dense(units=1, activation='sigmoid')(concat_features)\r\nmodel = Model(inputs=all_inputs, outputs=output)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\ndataframe_x = df[[numeric_feature_name, categorical_feature_name]]\r\ndataframe_y = df['label']\r\ndf2 = ((dict(dataframe_x), dataframe_y))\r\nds = tf.data.Dataset.from_tensor_slices(df2)\r\nds = ds.batch(32)\r\nds_train = ds\r\n\r\nmodel.fit(\r\n    ds_train,\r\n    epochs=10,\r\n    batch_size=300,\r\n    verbose=1\r\n)\r\n```\r\n\r\nWe have tested that this error is on 2.17.0 and if we are using 2.15 tensorflow, both codes will run smoothly.\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nEpoch 1\/10\r\n2024-09-18 05:28:05.240962: W tensorflow\/core\/framework\/op_kernel.cc:1817] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\nCell In[14], line 8\r\n      5 ds = ds.batch(32)\r\n      6 ds_train = ds\r\n----> 8 model.fit(\r\n      9     ds_train,\r\n     10     epochs=10,\r\n     11     batch_size=300,\r\n     12     verbose=1\r\n     13 )\r\n\r\nFile ~\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n    119     filtered_tb = _process_traceback_frames(e.__traceback__)\r\n    120     # To get the full stack trace, call:\r\n    121     # `keras.config.disable_traceback_filtering()`\r\n--> 122     raise e.with_traceback(filtered_tb) from None\r\n    123 finally:\r\n    124     del filtered_tb\r\n\r\nFile ~\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/tensorflow\/python\/eager\/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51 try:\r\n     52   ctx.ensure_initialized()\r\n---> 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                       inputs, attrs, num_outputs)\r\n     55 except core._NotOkStatusException as e:\r\n     56   if name is not None:\r\n\r\nUnimplementedError: Graph execution error:\r\n\r\nDetected at node functional_5_1\/Cast defined at (most recent call last):\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel_launcher.py\", line 18, in <module>\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/traitlets\/config\/application.py\", line 1075, in launch_instance\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/kernelapp.py\", line 739, in start\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/tornado\/platform\/asyncio.py\", line 205, in start\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 545, in dispatch_queue\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 534, in process_one\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 437, in dispatch_shell\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/ipkernel.py\", line 362, in execute_request\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 778, in execute_request\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/ipkernel.py\", line 449, in do_execute\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/zmqshell.py\", line 549, in run_cell\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3075, in run_cell\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3130, in _run_cell\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/async_helpers.py\", line 128, in _pseudo_sync_runner\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3334, in run_cell_async\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3517, in run_ast_nodes\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3577, in run_code\r\n\r\n  File \"\/tmp\/ipykernel_37779\/4021243845.py\", line 8, in <module>\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 320, in fit\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 121, in one_step_on_iterator\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 108, in one_step_on_data\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 51, in train_step\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/layers\/layer.py\", line 901, in __call__\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/ops\/operation.py\", line 46, in __call__\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 156, in error_handler\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\", line 167, in call\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\", line 258, in _standardize_inputs\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\", line 218, in _convert_inputs_to_tensors\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/ops\/core.py\", line 822, in convert_to_tensor\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/core.py\", line 132, in convert_to_tensor\r\n\r\nCast string to float is not supported\r\n\t [[{{node functional_5_1\/Cast}}]] [Op:__inference_one_step_on_iterator_6125]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:keras","2.17"],"created_at":"2024-09-18T17:06:56Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75996"},{"issue_number":99,"repository":"tensorflow\/tensorflow","title":"`resource_create_op` operation can cause TensorFlow to crash.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf-nightly 2.18.0.dev20240817\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 20.04.3 LTS (x86_64)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n A `Segmentation fault`  could be raised in TensorFlow when using `test_ops.resource_create_op` . The code is as follows:\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import test_ops\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import array_ops_stack\r\n\r\n\r\nsess = tf.compat.v1.Session()\r\n\r\n@tf.function\r\ndef func():\r\n    r1 = test_ops.stub_resource_handle_op(container='a', shared_name='b')\r\n    r2 = test_ops.stub_resource_handle_op(container='a', shared_name='c')\r\n    c = array_ops_stack.stack([r1, r2])\r\n    s = array_ops.strided_slice(c, [1], [2 ** 32])\r\n    with sess.as_default():\r\n        test_ops.resource_create_op(s)\r\n\r\nfunc()\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n> Segmentation fault (core dumped)\r\n\r\nThe above code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-16T01:45:51Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75825"},{"issue_number":100,"repository":"tensorflow\/tensorflow","title":"Encountering a `Segmentation fault` when using `data_flow_ops.FIFOQueue` in TensorFlow","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf-nightly 2.18.0.dev20240817\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n A `Segmentation fault`  could be raised in TensorFlow when using `data_flow_ops.FIFOQueue` . The following code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes as dtypes_lib\r\nfrom tensorflow.python.ops import data_flow_ops\r\nimport tensorflow as tf\r\n\r\nq = data_flow_ops.FIFOQueue(10, dtypes_lib.float32, ())\r\nelems = [10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\r\ntmp_var64 = elems[4:8]\r\n\r\nsess = tf.compat.v1.Session()\r\nwith sess.as_default():\r\n    q.dequeue_up_to([])\n```\n\n\n### Relevant log output\n\n```shell\n> Segmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-15T04:17:25Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75814"},{"issue_number":101,"repository":"tensorflow\/tensorflow","title":"`tf_cond.cond` and `tf.function` could cause an aborted issue","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf-nightly 2.18.0.dev20240817\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n An `aborted issue`  could be raised in TensorFlow when using `tf_cond.cond` and `tf.function`. The code is as follows:\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import constant_op\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import cond as tf_cond\r\nfrom tensorflow.python.ops import control_flow_assert\r\nfrom tensorflow.python.ops import control_flow_ops\r\nfrom tensorflow.python.platform import test\r\n\r\nimport tensorflow as tf\r\nsess = tf.compat.v1.Session()\r\n\r\n@tf.function\r\ndef func1():\r\n    with sess.as_default():\r\n        with ops.device(test.gpu_device_name()):\r\n            pred = constant_op.constant([True, False])\r\n\r\n        def fn1():\r\n            return control_flow_ops.no_op()\r\n\r\n        def fn2():\r\n            with ops.device('\/cpu:0'):\r\n                return control_flow_assert.Assert(False, ['Wrong!'])\r\n\r\n        r = tf_cond.cond(pred, fn1, fn2)\r\n\r\nfunc1()\n```\n\n\n### Relevant log output\n\n```shell\n> 2024-09-12 16:43:14.634438: F tensorflow\/core\/framework\/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor\r\n> Aborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-12T08:31:32Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75625"},{"issue_number":102,"repository":"tensorflow\/tensorflow","title":"Using `fft_ops`  would cause an `aborted issue`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf-nightly 2.18.0.dev20240817\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n An `aborted issue`  could be raised in TensorFlow when using `fft_ops`. The code is as follows:\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\r\nfrom tensorflow.python.ops.signal import fft_ops\r\nimport tensorflow as tf\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n\r\ndef _tf_ifft(x, rank, fft_length=None, feed_dict=None):\r\n    with tf.compat.v1.Session() as sess:\r\n        return sess.run(_tf_ifft_for_rank(rank)(x, fft_length), feed_dict=feed_dict)\r\n\r\ndef _tf_ifft_for_rank(rank):\r\n    if rank == 1:\r\n        return fft_ops.irfft\r\n    elif rank == 2:\r\n        return fft_ops.irfft2d\r\n    elif rank == 3:\r\n        return fft_ops.irfft3d\r\n    else:\r\n        raise ValueError('invalid rank')\r\n\r\n\r\nrank = 1\r\nextra_dims = 0\r\nnp_rtype = np.float32\r\nnp_ctype = np.complex64\r\ndims = rank + extra_dims\r\nx = np.zeros((1,) * dims).astype(np_ctype)\r\ntmp_var22 = _tf_ifft(x, rank).shape\n```\n\n\n### Relevant log output\n\n```shell\nDUCC FFT c2r failed:\r\nbazel-out\/k8-opt\/bin\/external\/ducc\/_virtual_includes\/fft\/ducc\/src\/ducc0\/fft\/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):\r\n\r\nAssertion failure\r\nno zero-sized FFTs\r\n\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-12T08:06:13Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75624"},{"issue_number":103,"repository":"tensorflow\/tensorflow","title":"Using `gen_random_index_shuffle_ops.random_index_shuffle` with `rounds=-2` can cause a crash","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf-nightly 2.18.0.dev20240817\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n A `Segmentation fault`  could be raised in TensorFlow when I used API `gen_random_index_shuffle_ops.random_index_shuffle` with `rounds=-2`. The code is as follows:\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import gen_random_index_shuffle_ops\r\nfrom tensorflow.python.ops import math_ops\r\n\r\nseed = (74, 117)\r\nseed_dtype = dtypes.int32\r\nmax_index = 129\r\nindex_dtype = dtypes.int32\r\nrounds = 4\r\n\r\nseen = (max_index + 1) * [False]\r\nseed = math_ops.cast([seed[0], seed[1], 42], seed_dtype)\r\nfor index in range(max_index + 1):\r\n    new_index = gen_random_index_shuffle_ops.random_index_shuffle(math_ops.cast(index, index_dtype), seed, max_index=math_ops.cast(max_index, index_dtype), rounds=rounds)\r\n    # rounds = -2 causes the segmentfault\r\n    new_index = gen_random_index_shuffle_ops.random_index_shuffle(math_ops.cast(index, index_dtype), seed, max_index=math_ops.cast(max_index, index_dtype), rounds=-2)\n```\n\n\n### Relevant log output\n\n```shell\n> Segmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-12T07:51:54Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75623"},{"issue_number":104,"repository":"tensorflow\/tensorflow","title":"Got one aborted issue when using `data_flow_ops.MapStagingArea` ","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf-nightly 2.18.0.dev20240817\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n An `aborted issue`  could be raised in TensorFlow when I used API `data_flow_ops.MapStagingArea`. The code is as follows:\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import data_flow_ops\r\nfrom tensorflow.python.ops import math_ops\r\nfrom tensorflow.python.platform import test\r\nimport tensorflow as tf\r\n\r\n\r\nwith ops.Graph().as_default() as g:\r\n    with ops.device('\/cpu:0'):\r\n        x = array_ops.placeholder(dtypes.float32)\r\n        pi = array_ops.placeholder(dtypes.int64)\r\n        gi = array_ops.placeholder(dtypes.int64)\r\n        v = 2.0 * (array_ops.zeros([]) + x)\r\n    with ops.device(test.gpu_device_name()):\r\n        stager = data_flow_ops.MapStagingArea([dtypes.float32])\r\n        stage = stager.put(pi, [v], [0])\r\n        k, y = stager.get([])\r\n        y = math_ops.reduce_max(math_ops.matmul(y, y))\r\ng.finalize()\r\nwith tf.compat.v1.Session(graph=g) as sess:\r\n    sess.run(stage, feed_dict={x: -1, pi: 0})\r\n    for i in range(10):\r\n        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})\n```\n\n\n### Relevant log output\n\n```shell\n2024-09-12 15:57:44.445189: F tensorflow\/core\/framework\/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 0)Must have a one element tensor\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-12T07:47:04Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75622"},{"issue_number":105,"repository":"tensorflow\/tensorflow","title":"An `aborted issue` could be raised in TensorFlow when I used API `math_ops.cast` and `array_ops.split`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf-nightly 2.18.0.dev20240817\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n An `aborted issue`  could be raised in TensorFlow when I used API `math_ops.cast` and `array_ops.split` . \n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\r\n\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import math_ops\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\nsess = tf.compat.v1.Session()\r\nwith sess.as_default():\r\n    a = math_ops.cast([2], dtypes.int32)\r\n    b = math_ops.cast([1], dtypes.int32)\r\n    value = np.random.rand(11, 11)\r\n    array_ops.split(value, [a, b])\n```\n\n\n### Relevant log output\n\n```shell\n2024-09-12 15:49:03.711972: F tensorflow\/core\/framework\/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-12T07:42:00Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75621"},{"issue_number":106,"repository":"tensorflow\/tensorflow","title":"check failed: !PyErr_Occurred() when constructing two uint64 tensors","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen running the following code, tensorflow will directly raise program abort with the error message: `.\/tensorflow\/python\/eager\/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred()`\r\n\r\n```\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\nimport tensorflow as tf\r\n\r\nlower1 = -1\r\ntry:\r\n    lower1 = tf.constant(lower1, dtype='uint64')\r\nexcept:\r\n    ...\r\nlower2 = -2\r\nlower2 = tf.constant(lower2, dtype='uint64')\r\n```\r\n\r\nIt seems the problem occurs when TensorFlow tries to construct **two uint64 tensors**. Although it is invalid to convert negative int to unsigned, an exception is more proper as program abort will directly kill the process.\r\n\r\nIndeed, only constructing one uint64 tensor will properly raises an OverFlow exception. \r\nThis issue only occurs when repeatedly constructing two uint64 tensors.\r\nAnother weird thing is that, **if I change the value of `lower2` to either `-1` or `-3` instead of `-2`**, this issue does not occur.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\r\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\nimport tensorflow as tf\r\n\r\nlower1 = -1\r\ntry:\r\n    lower1 = tf.constant(lower1, dtype='uint64')\r\nexcept:\r\n    ...\r\nlower2 = -2\r\nlower2 = tf.constant(lower2, dtype='uint64')\r\n```\n```\n\n\n### Relevant log output\n\n```shell\nF .\/tensorflow\/python\/eager\/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred() \r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:eager","2.17"],"created_at":"2024-09-09T11:18:01Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75400"},{"issue_number":107,"repository":"tensorflow\/tensorflow","title":"Gradients can't be computed for keras embeddings","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.15.1\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nWindows 11, Ubuntu 22.04LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11.6\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWe have a problem in shap for quite some while now with embeddings (see [here](https:\/\/github.com\/shap\/shap\/issues\/3440)). Since we manipulate the graph to adjust the gradient calculation in order to produce shap values we need the layers to be backpropagatable. This does not seem the case for `tensorflow.keras.layers.Embedding` and we do not know a way around this.\r\n\r\n(In a previous version there was the possibility to [manipulate](https:\/\/github.com\/shap\/shap\/blob\/master\/shap\/explainers\/_deep\/deep_tf.py#L412-L416) the [`_IsBackpropagatable` function](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v1.10.0\/tensorflow\/python\/ops\/gradients_impl.py#L293) but this is no longer possible)\r\n\r\nIn the example below one can see that the gradients just become `None` if the model contains an embedding layer. \r\nIs there a way around this, so that we can calculate gradients for embeddings again?\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers, models\r\nfrom tensorflow.keras.datasets import imdb\r\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\r\n\r\n# Load the IMDb dataset\r\nmax_features = 10000  # Only consider the top 10,000 words\r\nmaxlen = 100  # Only consider the first 100 words of each movie review\r\n\r\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\r\nX_train = pad_sequences(X_train, maxlen=maxlen)\r\nX_test = pad_sequences(X_test, maxlen=maxlen)\r\n\r\n# Build the model\r\nmodel = models.Sequential()\r\nembedding_layer = layers.Embedding(input_dim=max_features, output_dim=128, input_length=maxlen)\r\nmodel.add(embedding_layer)\r\nflat_layer = layers.Flatten()\r\nmodel.add(flat_layer)\r\ndense_layer = layers.Dense(1, activation='sigmoid')\r\nmodel.add(dense_layer)\r\n\r\n# Build the same model except for the embedding layer\r\nnew_model = models.Sequential()\r\nnew_model.add(flat_layer)\r\nnew_model.add(layers.Dense(1, activation=\"sigmoid\"))\r\n\r\n# Forward pass and gradient extraction\r\n@tf.function\r\ndef get_gradients_model(inputs):\r\n    inputs = tf.cast(inputs, tf.float32)  # Convert inputs to float32\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(inputs)  # Watch the input tensor to compute gradients w.r.t. it\r\n        predictions = model(inputs)\r\n    \r\n    gradients = tape.gradient(predictions, inputs)\r\n    \r\n    return predictions, gradients\r\n\r\n@tf.function\r\ndef get_gradients_new_model(inputs):\r\n    inputs = tf.cast(inputs, tf.float32)  # Convert inputs to float32\r\n    with tf.GradientTape() as tape:\r\n        tape.watch(inputs)  # Watch the input tensor to compute gradients w.r.t. it\r\n        predictions = new_model(inputs)\r\n    \r\n    gradients = tape.gradient(predictions, inputs)\r\n    \r\n    return predictions, gradients\r\n\r\n# Example usage\r\nsample_input = X_train[:1]  # Select a sample from the training set\r\nsample_label = y_train[:1]  # Corresponding label\r\n\r\npredictions, gradients = get_gradients_model(sample_input)\r\npredictions2, gradients2 = get_gradients_new_model(sample_input)\r\nprint(\"Gradients for model:\", gradients)\r\nprint(\"Gradients for new_model:\", gradients2)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nGradients for model: None\r\nGradients for new_model: tf.Tensor(\r\n[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\r\n  0. 0. 0. 0.]], shape=(1, 100), dtype=float32)","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-09-08T11:00:16Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75351"},{"issue_number":108,"repository":"tensorflow\/tensorflow","title":"Calibrator segfaults trying to log the \"while\" operation","description":"### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.17.0\r\n\r\n### 2. Code\r\n\r\n[reproducer.zip](https:\/\/github.com\/user-attachments\/files\/16894947\/reproducer.zip)\r\n\r\n### 3. Failure after conversion\r\n\r\nSegmentation fault (signal 11) during conversion\r\n\r\n### 5. (optional) Any other info \/ logs\r\n\r\nThe \"while\" operation does a check if an output tensor of the body subgraph is the same as the corresponding input tensor. If it's the case, it deallocates its own output tensor. The check is done at the prepare stage, so the affected tensor is already included in the \"loggable_outputs\" list by the calibrator. Then the calibrator tries to read the data from the deallocated tensor and segfaults. I've debugged it up to https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/lite\/tools\/optimize\/calibration\/calibrator.cc#L267 and found that the `tensor.data.f == nullptr`.\r\nThe check in question was introduced between 2.13 and 2.14, so it might be considered a regression: https:\/\/github.com\/tensorflow\/tensorflow\/commit\/7d49fd431ee5cebbb76eda88bc17e48921e10c85\r\n","labels":["awaiting review","stat:awaiting tensorflower","type:bug","comp:lite","TFLiteConverter","2.17"],"created_at":"2024-09-05T01:23:29Z","comments":19,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75140"},{"issue_number":109,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel raises a program abort","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen setting the `num_bits` in a large integer, this API raises the program abort.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ninputs = tf.constant(0.57681304)\r\nmin = tf.constant(2.1311088)\r\nmax = tf.constant(2.4402196)\r\nnum_bits = 10\r\nnarrow_range = False\r\ntf.raw_ops.FakeQuantWithMinMaxVarsPerChannel(inputs=inputs,min=min,max=max,num_bits=num_bits,narrow_range=narrow_range)\r\n```\n```\n\n\n### Relevant log output\n\n```shell\nF tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-08-31T17:52:07Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/74932"},{"issue_number":110,"repository":"tensorflow\/tensorflow","title":"tensorflow.python.ops.gen_math_ops.sparse_bincount can cause a crash","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf-nightly 2.18.0.dev20240817\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI encountered a `Segmentation fault ` issue in TensorFlow when using the `gen_math_ops.sparse_bincount` API. I have confirmed that the code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nfrom tensorflow.python.ops import gen_math_ops\r\n\r\nvalues = [0, 1, 2, 2]\r\nbinary = False\r\nindices = [[], [], [990, 2], [2, 349]]\r\ndense_shape = []\r\ngen_math_ops.sparse_bincount(indices=indices, values=values, dense_shape=dense_shape, size=3, weights=[],binary_output=binary)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nSegmentation fault (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-08-31T06:18:00Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/74918"},{"issue_number":111,"repository":"tensorflow\/tensorflow","title":" Aborted (core dumped): Check failed: d < dims() (1 vs. 1)","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf-nightly 2.18.0.dev20240817\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI encountered an `aborted issue` in TensorFlow when I used API `array_ops.scatter_nd` . The code is as follows:\r\n\r\n```python\r\nimport numpy as np\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import array_ops\r\n\r\nGRADIENT_TESTS_DTYPES = (dtypes.bfloat16, dtypes.float16, dtypes.float32, dtypes.float64)\r\n\r\ndef scatter_nd(indices, updates, shape):\r\n    return array_ops.scatter_nd(indices, updates, shape)\r\n\r\ndef testExtraIndicesDimensions():\r\n    indices = array_ops.zeros((1, 1, 2), dtypes.int32)\r\n    updates = array_ops.zeros([1], dtypes.int32)\r\n    shape = np.array((2, 2))\r\n    scatter = scatter_nd(indices, updates, shape)\r\n\r\ntestExtraIndicesDimensions()\r\n```\r\n\r\n> 2024-08-31 12:17:41.010855: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)\r\n> Aborted (core dumped)\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import array_ops\r\n\r\nGRADIENT_TESTS_DTYPES = (dtypes.bfloat16, dtypes.float16, dtypes.float32, dtypes.float64)\r\n\r\ndef scatter_nd(indices, updates, shape):\r\n    return array_ops.scatter_nd(indices, updates, shape)\r\n\r\ndef testExtraIndicesDimensions():\r\n    indices = array_ops.zeros((1, 1, 2), dtypes.int32)\r\n    updates = array_ops.zeros([1], dtypes.int32)\r\n    shape = np.array((2, 2))\r\n    scatter = scatter_nd(indices, updates, shape)\r\n\r\ntestExtraIndicesDimensions()\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-08-31 12:17:41.010855: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)\r\nAborted (core dumped)\r\n```\r\nI have confirmed that above code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-08-31T04:18:19Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/74912"},{"issue_number":112,"repository":"tensorflow\/tensorflow","title":"tf.math.floordiv produces incorrect result when the denominator is `-inf`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nBased on the documentation https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/floordiv, `tf.math.floordiv` should be equivalent to python's `\/\/` operator. However, when the `x=1.4` and `y=-np.inf`, `tf.math.floordiv` outputs `-0.0` while `\/\/` outputs `-1.0`.\r\n\r\nI also checked Numpy and PyTorch's APIs, both output `-1.0`. \r\n\r\nIt seems that the implementation of `tf.math.floordiv` is different from others, it would be nice if you can fix the implementation inconsistency, or make this inconsistency in the documentation.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport torch\r\nimport numpy as np\r\na = 1.4\r\nb = -np.inf\r\nprint(\"Numpy's result: \", np.floor_divide(a, b))\r\nprint(\"Python's \/\/ result: \", a \/\/ b)\r\nprint(f\"TF's result: {tf.math.floordiv(a, b)}\")\r\nprint(f\"PyTorch's result: {torch.floor_divide(torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32))}\")\r\n```\n```\n\n\n### Relevant log output\n\n```shell\nNumpy's result:  -1.0\r\nPython's \/\/ result:  -1.0\r\nTF's result: -0.0\r\nPyTorch's result: -1.0\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-08-30T18:05:02Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/74893"},{"issue_number":113,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.Round outputs zeros for any integer tensor","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nExpects the same tensor as the input according to the specification https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/Round\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nx = tf.constant([-2, -1, 1, 2, 3])\r\ntf.raw_ops.Round(x=x)\n```\n\n\n### Relevant log output\n\n```shell\n>>> import tensorflow as tf\r\n>>> x = tf.constant([-2, -1, 1, 2, 3], dtype=tf.int32)\r\n>>> tf.raw_ops.Round(x=x)\r\n<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 0, 0, 0])>\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","regression issue","2.17"],"created_at":"2024-08-29T12:24:07Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/74789"},{"issue_number":114,"repository":"tensorflow\/tensorflow","title":"With the same input and parameter settings, there is a large difference in the output of Dense layer on GPU and CPU.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.12.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWe found that when training the model, there is a large difference in the output of the DENSE layer on the cpu and gpu when using the same input tensor and parameter settings.\r\n```\r\nCPU output: [[3.4838054e+34]]\r\nGPU output: [[3.4838057e+34]]\r\n2.4758800785707605e+27\r\n```\r\nWe have tried some similar inputs, but no such problems have occurred.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport pickle\r\nimport h5py\r\n\r\ntf.random.set_seed(42)\r\ntf.config.experimental.enable_op_determinism()\r\n\r\ndef chebyshev_distance(A: np.ndarray, B: np.ndarray):\r\n    if A is None or B is None:\r\n        return 0.0\r\n    if A.shape != B.shape:\r\n        return 9999999\r\n    else:\r\n        return float(np.max(np.abs(A - B)))\r\n\r\n\r\ntf.random.set_seed(42)\r\n\r\nx_input = np.array([[37.63115]])\r\n# x_input = np.array([[38.63115]])\r\n# x_input = np.array([[3.763115]])\r\nprint(x_input)\r\n\r\ndense_layer = tf.keras.layers.Dense(units=1, activation='exponential', use_bias=True, activity_regularizer=None, bias_constraint=None, bias_initializer='random_normal', kernel_initializer='he_normal', bias_regularizer=None, kernel_regularizer=None, kernel_constraint=None)\r\n\r\nweights = [np.array([[2.112561]], dtype=np.float32), np.array([0.03791478], dtype=np.float32)]\r\n\r\ndense_layer.build((1,))  \r\ndense_layer.set_weights(weights)\r\n\r\nwith tf.device('\/CPU:0'):\r\n    x_cpu = tf.constant(x_input, dtype=tf.float32)\r\n    output_cpu = dense_layer(x_cpu)\r\n    print(\"CPU output:\", output_cpu.numpy())\r\n\r\n\r\nif tf.config.list_physical_devices('GPU'):\r\n    with tf.device('\/GPU:0'):\r\n        x_gpu = tf.constant(x_input, dtype=tf.float32)\r\n        output_gpu = dense_layer(x_gpu)\r\n        print(\"GPU output:\", output_gpu.numpy())\r\nelse:\r\n    print(\"GPU not available.\")\r\n    \r\noutput_diff = chebyshev_distance(output_cpu.numpy(), output_gpu.numpy())\r\nprint(output_diff)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.12"],"created_at":"2024-08-29T09:59:58Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/74783"},{"issue_number":115,"repository":"tensorflow\/tensorflow","title":"tf.sparse.reduce_sum error in JIT","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.17.0-rc1-2-gad6d8cc177d\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu Mate 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython 3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nCUDA 12.3\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nError when using tf.sparse.reduce_sum and JIT compilation.\r\n\r\nI have written a layer that passes all my unit tests except when I use in a model with predict or train. \r\nWould that make sense with the JIT compilation on? I am not fully certain how and when this works.\r\n\r\nAnyway, I am not exactly sure why my layer code fails, but I think that this minimum reproducible example captures the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nbatch_size = 4\r\ninput_shape = (3, 3)\r\n\r\nindices = np.array([[0, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 1], [2, 0, 0], [2, 0, 1], [3, 0, 0], [3, 0, 1]])\r\ninputs = tf.sparse.SparseTensor(dense_shape=(batch_size, *input_shape),\r\n                                indices=indices,\r\n                                values=[9, 1, 9, 1, 9, 1, 9, 1])\r\n\r\n\r\n@tf.function(input_signature=[tf.SparseTensorSpec(\r\n    shape=(4, 3, 3),\r\n    dtype=tf.dtypes.int32)], jit_compile=True)\r\ndef get_batch_sum(inputs):\r\n    # same problem with tf.sparse.reduce_max\r\n    return tf.sparse.reduce_sum(tf.sparse.reduce_sum(inputs, axis=1, output_is_sparse=True), axis=1)\r\n\r\n\r\nsum_out = get_batch_sum(inputs)\r\nprint(sum_out)\n```\n\n\n### Relevant log output\n\n```shell\n2024-08-20 16:31:01.690779: W tensorflow\/core\/framework\/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: SparseReduceSumSparse (No registered 'SparseReduceSumSparse' OpKernel for XLA_GPU_JIT devices compatible with node {{node SparseReduceSumSparse}}){{node SparseReduceSumSparse}}\r\nThe op is created at: \r\nFile \".config\/JetBrains\/PyCharmCE2023.3\/scratches\/scratch_30.py\", line 21, in <module>\r\nFile \".config\/JetBrains\/PyCharmCE2023.3\/scratches\/scratch_30.py\", line 18, in get_batch_sum\r\n\ttf2xla conversion failed while converting __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=\/path\/to\/dump\/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\r\nTraceback (most recent call last):\r\n(...)  \r\n\r\nFile \".config\/JetBrains\/PyCharmCE2023.3\/scratches\/scratch_30.py\", line 18, in get_batch_sum\r\n\ttf2xla conversion failed while converting __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=\/path\/to\/dump\/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_get_batch_sum_15]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","comp:xla","2.17"],"created_at":"2024-08-20T14:39:42Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/74131"},{"issue_number":116,"repository":"tensorflow\/tensorflow","title":"HLO for TopK oddly casts uint8 input to uint32 before passing to radix sort. ","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nTF 2 (HEAD of internal repo)\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nGoogle-Internal Environment\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nV100 (also reproducible on other GPUs)\n\n### Current behavior?\n\nAn Alphabet model invokes `tf.math.top_k` with a tensor of dtype uint8 and shape (1, 1,32768). \r\n![strange_hlo_text_with_uint32_radix_sort](https:\/\/github.com\/user-attachments\/assets\/945af324-b00b-405b-8a68-06b7c33cb1e1)\r\n\r\nFor this call, XLA ends up calling radix sort. However, the radix sort is sub-optimal because TensorFlow casts the inputs to uint32 (instead of using original dtype uint8). Of course, radix sort is faster across smaller dtypes (with fewer bytes).\r\n\r\n> @@(u32[1,32768]{1,0}, s32[1,32768]{1,0}, u8[271871]{0}) custom-call(u32[1,32768]{1,0}, s32[1,32768]{1,0}), custom_call_target=\"__cub$DeviceRadixSort\r\n\r\nI would expect HLO text more like this, where the uint8 inputs are passed directly:\r\n\r\n> (u8[1,32768]{1,0}, s32[1,32768]{1,0}, u8[310527]{0}) custom-call(u8[1,32768]{1,0}, s32[1,32768]{1,0}), custom_call_target=\"__cub$DeviceRadixSort\"\r\n\r\nWe actually use TF indirectly via the jax2tf bridge, and I see this comment in the code hinting that uint8 is incompatible with `tf.math.top_k`:\r\nhttps:\/\/github.com\/google\/jax\/blob\/0b87bf48f97ace10c7aee19c8f980788891a2df7\/jax\/experimental\/jax2tf\/jax2tf.py#L3167\r\n\r\nHowever, recently, @dimitar-asenov (on XLA GPU) has made some changes to XLA sorting logic that provides support for radix-sorting uint8.\r\n\r\nCould `tf.math.top_k` lower to HLO that avoids this up-cast to uint32 before radix sort? I believe this would halve the latency of radix sort for uint8.\n\n### Standalone code to reproduce the issue\n\n```shell\nTo repro, collect an XProf trace for the following snippet. See attached screenshot for sample trace.\r\n\r\n\r\nimport tensorflow as tf\r\n\r\ndata = tf.random.uniform(\r\n          shape=(1, 32768),\r\n          minval=0,\r\n          maxval=256,\r\n          dtype=tf.int32,\r\n      )\r\ndata = tf.cast(data, tf.uint8)\r\n_, box_indices = tf.math.top_k(data, k=1024)\r\n\r\n\r\nOr feel free to just find and run my internal experimental `benchmark_tf_top_k_uint8` binary on a machine with a GPU.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-08-18T21:52:59Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/74035"},{"issue_number":117,"repository":"tensorflow\/tensorflow","title":"Cannot dlopen some GPU libraries [can't find cuda driver] in rhel9","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.17.0-rc1-2-gad6d8cc177d 2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nRedhat enterprise 9.4 base image\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.2\n\n### GPU model and memory\n\nTesla T4, 15360MiB\n\n### Current behavior?\n\nWhat is your question?\r\n\r\nDescribe the bug\r\n\r\nError when I run GPU test, wondering if my docker linux kernel version is too low? The reason I am asking is that, my ubuntu22 is working fine.\r\n\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1723846435.253301 203 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-08-16 22:13:55.253747: W tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https:\/\/www.tensorflow.org\/install\/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n\"\r\nMy Kernel version,\r\n\r\nbash-5.1# uname -a\r\nLinux mlops-test-failed 5.10.220-209.869.amzn2.x86_64 https:\/\/github.com\/rapidsai\/cudf\/issues\/1 SMP Wed Jul 17 15:10:20 UTC 2024 x86_64 x86_64 x86_64 GNU\/Linux\r\n\r\nMy OS\/Docker image distro,\r\n\"\r\nbash-5.1# uname -m && cat \/etc\/*release\r\nx86_64\r\nNAME=\"Red Hat Enterprise Linux\"\r\nVERSION=\"9.4 (Plow)\"\r\nID=\"rhel\"\r\nID_LIKE=\"fedora\"\r\nVERSION_ID=\"9.4\"\r\nPLATFORM_ID=\"platform:el9\"\r\nPRETTY_NAME=\"Red Hat Enterprise Linux 9.4 (Plow)\"\r\nANSI_COLOR=\"0;31\"\r\nLOGO=\"fedora-logo-icon\"\r\nCPE_NAME=\"cpe:\/o:redhat:enterprise_linux:9::baseos\"\r\nHOME_URL=\"https:\/\/www.redhat.com\/\"\r\nDOCUMENTATION_URL=\"https:\/\/access.redhat.com\/documentation\/en-us\/red_hat_enterprise_linux\/9\"\r\nBUG_REPORT_URL=\"https:\/\/bugzilla.redhat.com\/\"\r\n\r\nREDHAT_BUGZILLA_PRODUCT=\"Red Hat Enterprise Linux 9\"\r\nREDHAT_BUGZILLA_PRODUCT_VERSION=9.4\r\nREDHAT_SUPPORT_PRODUCT=\"Red Hat Enterprise Linux\"\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"9.4\"\r\nRed Hat Enterprise Linux release 9.4 (Plow)\r\nRed Hat Enterprise Linux release 9.4 (Plow)\r\n\"\r\nBoth my ubuntu 22 and rhel9 were showing the nvidia-smi ok like the following.\r\n\r\nbash-5.1# nvidia-smi\r\nFri Aug 16 22:29:43 2024\r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.183.06 Driver Version: 535.183.06 CUDA Version: 12.2 |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |\r\n| Fan Temp Perf Pwr:Usage\/Cap | Memory-Usage | GPU-Util Compute M. |\r\n| | | MIG M. |\r\n|=========================================+======================+======================|\r\n| 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 |\r\n| N\/A 33C P8 11W \/ 70W | 0MiB \/ 15360MiB | 0% Default |\r\n| | | N\/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n\r\n+---------------------------------------------------------------------------------------+\r\n| Processes: |\r\n| GPU GI CI PID Type Process name GPU Memory |\r\n| ID ID Usage |\r\n|=======================================================================================|\r\n| No running processes found |\r\n+---------------------------------------------------------------------------------------+\r\n\r\nI am checking the following url,\r\nhttps:\/\/docs.nvidia.com\/cuda\/cuda-installation-guide-linux\/index.html\r\n\r\nand it says the rhel9 kernel version has to be 5.14.0-427.\r\n\r\nYour input is appreciated!\n\n### Standalone code to reproduce the issue\n\n```shell\ndef run_gpu_test():\r\n    gpus = tf.config.list_logical_devices('GPU')\r\n    print(\"Num GPUs Available: \", len(gpus))\r\n\r\nrun_gpu_test()\r\n\r\nIn Ubuntu22, it prints gpu number 1 with all the gpu information and in rhel9 it does not.\r\n\r\nThis is a Pod we created in eks, and by exec to the pod, we pasted the debugging information in the above section. I did nvidia-smi and both ubuntu22 and rhel9 shows GPU fine. Ubuntu22 works fine but not rhel9. The node we created is a AWS G4 instance, so it has tensorflow 12.7 and cuda 12.2 and we installed nvidia-plugin. I think this should be very easy to reproduce not sure if the aws kernel version matters.\n```\n\n\n### Relevant log output\n\n```shell\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1723846435.253301 203 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-08-16 22:13:55.253747: W tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https:\/\/www.tensorflow.org\/install\/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n\"\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","2.17"],"created_at":"2024-08-16T23:39:02Z","comments":3,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/73978"},{"issue_number":118,"repository":"tensorflow\/tensorflow","title":"tritonserver preload trt plugin got warning message and many core files : Failed to compile generated PTX with ptxas. Falling back to compilation by driver.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.16.2\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nlinux Ubuntu 22.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\ntritonserver preload trt plugin got warning message and many core  dump files\r\n`\r\n2024-08-16 10:09:14.975649: W tensorflow\/compiler\/mlir\/tools\/kernel_gen\/transforms\/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\r\n2024-08-16 10:09:16.033970: W tensorflow\/compiler\/mlir\/tools\/kernel_gen\/transforms\/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\r\n2024-08-16 10:09:16.701031: W tensorflow\/compiler\/mlir\/tools\/kernel_gen\/transforms\/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\r\n2024-08-16 10:09:17.498157: W tensorflow\/compiler\/mlir\/tools\/kernel_gen\/transforms\/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\r\n2024-08-16 10:09:18.328719: W tensorflow\/compiler\/mlir\/tools\/kernel_gen\/transforms\/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\r\n`\r\n\r\n![image](https:\/\/github.com\/user-attachments\/assets\/c9cf824b-f27e-456d-aa43-af501aae0694)\r\n\r\nI have an ensmble model, the first part is the tf model, the second part is the trt model. I have a trt plugin, and I load it as LD_PRELOAD. It won't be a problem if I load the two models separately. But when I load them at the same time this warning comes up and produces a lot of coredump files. Why is that? I don't understand how the trt plugin will affect tf\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nLD_PRELOAD=\/app\/lib\/ops\/libtrtplugin.so --model-repository=\/opt\/model-repo-copy --model-control-mode=explicit --load-model=first_model --load-model=second_model  --load-model=ensmble_model  --log-verbose=0 --http-port=xxx--grpc-port=xxx --metrics-port=xxx --backend-config=tensorflow,version=2 --backend-config=tensorrt,version-compatible=true --disable-auto-complete-config \r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","TF 2.16"],"created_at":"2024-08-16T10:23:45Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/73922"},{"issue_number":119,"repository":"tensorflow\/tensorflow","title":"The average should not be computed in L2Pool2d","description":"The [ONNX L2Pool2d](https:\/\/github.com\/onnx\/onnx\/blob\/main\/docs\/Operators.md#lppool), [DirectML L2 Pooling Desc](https:\/\/docs.microsoft.com\/en-us\/windows\/win32\/api\/directml\/ns-directml-dml_lp_pooling_operator_desc) and [CoreML's l2_pool2d](https:\/\/apple.github.io\/coremltools\/source\/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.iOS15.pool.l2_pool) calculate the l2 pooling by the expression `Y = (X1^2 + X2^2 + ... + Xn^2) ^ (1\/2)`,  but [TFLite L2_PooL2d kernel implementation](https:\/\/source.chromium.org\/chromium\/chromium\/src\/+\/main:third_party\/tflite\/src\/tensorflow\/lite\/kernels\/internal\/optimized\/optimized_ops.h;l=3341?q=src%2Ftensorflow%2Flite%2Fkernels%2Finternal%2Foptimized%2Foptimized_ops.h) has the average with the count of sum elements  `Y=((X1^2 + X2^2 + ... + Xn^2)\/n) ^ (1\/2)`,  is it an issue of the kernel implementation?\r\n\r\nBTW, [the kernel of l2_norm](https:\/\/source.chromium.org\/chromium\/chromium\/src\/+\/main:third_party\/tflite\/src\/tensorflow\/lite\/kernels\/internal\/optimized\/optimized_ops.h;l=1424?q=L2Normalization&ss=chromium%2Fchromium%2Fsrc) also has no the average.\r\n\r\n","labels":["type:bug","comp:lite"],"created_at":"2024-08-14T00:51:39Z","comments":3,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/73742"},{"issue_number":120,"repository":"tensorflow\/tensorflow","title":"Unable to train\/take gradient of integer variable under any condition","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\nv2.16.1-19-g810f233968c\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nDebian 11\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.12\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n12.6\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nModels with integer variables cannot have gradients computed (or be trained because of this). They fail with `ValueError: No gradients provided for any variable.` Please note that I'm referring to building\/training models, _not_ post-training model quantization.\r\n\r\nFor context, I'm working on a somewhat novel approach to solving a type of engineering problem that includes both discrete and continuous values. In some cases, variables must be one of a set of numeric values (e.g. `1`, `2`, `3000`, etc). This isn't something that can really be split out into separate models and trained independently, as both these variables are used throughout a complex, multiple-input multiple-output layer based model. An approximation cannot be used either, as even a highly accurate approximation can feasibly result in an incorrect result under certain conditions.\r\n\r\nIntuitively, a derivative cannot be calculated on integer values because they're discrete, and differentiation requires a continuous function over the domain of differentiation. However, floats (and any limited-precision data type) also suffer from this - but differentiation is generally considered \"viable\" for floats. Under any argument made for floats, integers should also be considered differentiable. There are certainly some issues with this for small number, but they have much less of an impact for larger ones (just like floats very close to 0 vs \"single digit\" floats i.e. `1`, `2`, etc).\r\n\r\nIf maintainers\/triagers\/project folks don't want to go in this direction, users like myself should be able to implement this via a `tf.custom_gradient` function. However, even when implementing this function, gradients\/training fail in the exact same manner in the exact same place. When using a custom gradient function with an integer variable, TF fails with the aforementioned error message without even calling the custom gradient function. IMO this is pretty clearly a bug. See below for specific code to reproduce this.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nThis fails (no custom gradient function, int32 type):\r\n\r\n```python\r\nimport keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nvariable_dtype = tf.int32\r\n# variable_dtype = tf.float32   # Uncommenting this fixes the issue\r\n\r\n\r\nclass BugTestLayer(keras.layers.Layer):\r\n    # Layer is just y = self.var * x\r\n    def build(self, input_shape):\r\n        self.var = self.add_variable(\r\n            (1,), initializer=\"zeros\", dtype=variable_dtype)\r\n\r\n    def call(self, input):\r\n        return input * self.var\r\n\r\n\r\ninput_layer = keras.Input((1,), dtype=tf.int32)\r\ntest_layer = BugTestLayer()\r\n\r\noutput = test_layer(input_layer)\r\nmodel = keras.Model(inputs=[input_layer], outputs=[output])\r\n\r\nvalues = np.array([[i] for i in range(1000)])\r\n\r\nmodel.compile(\r\n    loss=[keras.losses.MeanSquaredError()],\r\n    optimizer=keras.optimizers.RMSprop(),\r\n    metrics=[keras.metrics.MeanSquaredError()],\r\n)\r\n\r\n# This will always raise a `ValueError: No gradients provided for any variable.`\r\n# when using an integer type\r\nhistory = model.fit(values, values, batch_size=1, epochs=2)\r\n```\r\n\r\nThis works (no custom gradient function, float32 type):\r\n```python\r\nimport keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# variable_dtype = tf.int32\r\nvariable_dtype = tf.float32   # Uncommenting this fixes the issue\r\n\r\n\r\nclass BugTestLayer(keras.layers.Layer):\r\n    # Layer is just y = self.var * x\r\n    def build(self, input_shape):\r\n        self.var = self.add_variable(\r\n            (1,), initializer=\"zeros\", dtype=variable_dtype)\r\n\r\n    def call(self, input):\r\n        return input * self.var\r\n\r\n\r\ninput_layer = keras.Input((1,), dtype=tf.int32)\r\ntest_layer = BugTestLayer()\r\n\r\noutput = test_layer(input_layer)\r\nmodel = keras.Model(inputs=[input_layer], outputs=[output])\r\n\r\nvalues = np.array([[i] for i in range(1000)])\r\n\r\nmodel.compile(\r\n    loss=[keras.losses.MeanSquaredError()],\r\n    optimizer=keras.optimizers.RMSprop(),\r\n    metrics=[keras.metrics.MeanSquaredError()],\r\n)\r\n\r\n# This will always raise a `ValueError: No gradients provided for any variable.`\r\n# when using an integer type\r\nhistory = model.fit(values, values, batch_size=1, epochs=2)\r\n```\r\n\r\nThis fails (custom gradient function, int32 type):\r\n```python\r\n# %%\r\n\r\nimport keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# %%\r\n\r\n\r\nclass TestLayer(keras.layers.Layer):\r\n    def build(self, input_shape):\r\n        # dtype is the problem.\r\n        # integer type results in \"No gradients provided for any variable\", while\r\n        # floats work just fine.\r\n        self.var = self.add_variable(\r\n            (1,), initializer=\"zeros\", dtype=tf.int32)\r\n\r\n    @tf.custom_gradient\r\n    def op(input, var):\r\n        def grad(upstream, variables=None):\r\n            return tf.squeeze(upstream * tf.cast(var, tf.float32), axis=[1]), \\\r\n                tf.squeeze(upstream * tf.cast(input, tf.float32), axis=[1])\r\n\r\n        return tf.cast(input, tf.float32) * tf.cast(var, tf.float32), grad\r\n\r\n    def call(self, input):\r\n        return TestLayer.op(input, self.var)\r\n\r\n# %%\r\n\r\n\r\ninput_layer = keras.Input((1,), name=\"input_layer\", dtype=tf.int32)\r\ntest_layer = TestLayer()\r\n\r\noutput = test_layer(input_layer)\r\nmodel = keras.Model(inputs=[input_layer], outputs=[output], name=\"model\")\r\n\r\n# %%\r\n\r\nmodel.summary()\r\n# Example evaluation (untrained)\r\nmodel(tf.constant([[100]]))\r\n\r\nkeras.utils.plot_model(model, \"my_first_model.png\", show_dtype=True,\r\n                       show_layer_names=True, show_shapes=True, show_trainable=True)\r\n\r\n\r\n# %%\r\n\r\nvalues = np.array([\r\n    [i]\r\n    for i in range(1000)\r\n])\r\n\r\nmodel.compile(\r\n    loss=[keras.losses.MeanSquaredError()],\r\n    optimizer=keras.optimizers.RMSprop(),\r\n    metrics=[keras.metrics.MeanSquaredError()],\r\n)\r\n\r\nhistory = model.fit(values, values, batch_size=1, epochs=10)\r\n```\r\n\r\nThis works (custom gradient function, float32 type):\r\n```python\r\n# %%\r\n\r\nimport keras\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n# %%\r\n\r\n\r\nclass TestLayer(keras.layers.Layer):\r\n    def build(self, input_shape):\r\n        # dtype is the problem.\r\n        # integer type results in \"No gradients provided for any variable\", while\r\n        # floats work just fine.\r\n        self.var = self.add_variable(\r\n            (1,), initializer=\"zeros\", dtype=tf.int32)\r\n\r\n    @tf.custom_gradient\r\n    def op(input, var):\r\n        def grad(upstream, variables=None):\r\n            return tf.squeeze(upstream * tf.cast(var, tf.float32), axis=[1]), \\\r\n                tf.squeeze(upstream * tf.cast(input, tf.float32), axis=[1])\r\n\r\n        return tf.cast(input, tf.float32) * tf.cast(var, tf.float32), grad\r\n\r\n    def call(self, input):\r\n        return TestLayer.op(input, self.var)\r\n\r\n# %%\r\n\r\n\r\ninput_layer = keras.Input((1,), name=\"input_layer\", dtype=tf.float32)\r\ntest_layer = TestLayer()\r\n\r\noutput = test_layer(input_layer)\r\nmodel = keras.Model(inputs=[input_layer], outputs=[output], name=\"model\")\r\n\r\n# %%\r\n\r\nmodel.summary()\r\n# Example evaluation (untrained)\r\nmodel(tf.constant([[100]]))\r\n\r\nkeras.utils.plot_model(model, \"my_first_model.png\", show_dtype=True,\r\n                       show_layer_names=True, show_shapes=True, show_trainable=True)\r\n\r\n\r\n# %%\r\n\r\nvalues = np.array([\r\n    [i]\r\n    for i in range(1000)\r\n])\r\n\r\nmodel.compile(\r\n    loss=[keras.losses.MeanSquaredError()],\r\n    optimizer=keras.optimizers.RMSprop(),\r\n    metrics=[keras.metrics.MeanSquaredError()],\r\n)\r\n\r\nhistory = model.fit(values, values, batch_size=1, epochs=10)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nLogs for the last failure:\r\n\r\n```shell\r\nEpoch 1\/10\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nFile \/workspaces\/power sim 2\/test_docs.py:14\r\n      3 values = np.array([\r\n      4     [i]\r\n      5     for i in range(1000)\r\n      6 ])\r\n      8 model.compile(\r\n      9     loss=[keras.losses.MeanSquaredError()],\r\n     10     optimizer=keras.optimizers.RMSprop(),\r\n     11     metrics=[keras.metrics.MeanSquaredError()],\r\n     12 )\r\n---> 14 history = model.fit(values, values, batch_size=1, epochs=10)\r\n\r\nFile ~\/.local\/lib\/python3.12\/site-packages\/keras\/src\/utils\/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n    119     filtered_tb = _process_traceback_frames(e.__traceback__)\r\n    120     # To get the full stack trace, call:\r\n    121     # `keras.config.disable_traceback_filtering()`\r\n--> 122     raise e.with_traceback(filtered_tb) from None\r\n    123 finally:\r\n    124     del filtered_tb\r\n\r\nFile ~\/.local\/lib\/python3.12\/site-packages\/keras\/src\/optimizers\/base_optimizer.py:662, in BaseOptimizer._filter_empty_gradients(self, grads, vars)\r\n    659         missing_grad_vars.append(v.name)\r\n    661 if not filtered_grads:\r\n--> 662     raise ValueError(\"No gradients provided for any variable.\")\r\n    663 if missing_grad_vars:\r\n    664     warnings.warn(\r\n    665         \"Gradients do not exist for variables \"\r\n    666         f\"{list(reversed(missing_grad_vars))} when minimizing the loss.\"\r\n    667         \" If using `model.compile()`, did you forget to provide a \"\r\n    668         \"`loss` argument?\"\r\n    669     )\r\n\r\nValueError: No gradients provided for any variable.\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-08-12T21:23:47Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/73631"},{"issue_number":121,"repository":"tensorflow\/tensorflow","title":"Segmentation fault (core dumped) in `tf.config.threading.set_inter_op_parallelism_threads`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nCrash triggered when input boundary values into tf.config.threading.set_intra_op_parallelism_threads\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/12s6D2GuBFEWAdvFdCvjbs4nK888vLGvm?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\n2024-08-10 21:36:47.636016: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-08-10 21:36:47.703371: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-08-10 21:36:47.791020: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-08-10 21:36:47.817978: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-08-10 21:36:47.883418: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-08-10 21:36:56.611712: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 22279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-08-10 21:36:56.617085: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1717 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\nSegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-08-10T13:39:20Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/73519"},{"issue_number":122,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.config.threading.set_intra_op_parallelism_threads`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nCrash triggered when input negative numbers into tf.config.threading.set_intra_op_parallelism_threads.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/17JV6ppGU1XtQg25PKa8itDhihAspgHIz?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\n2024-08-10 21:29:55.538868: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-08-10 21:29:55.869155: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-08-10 21:29:55.967845: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-08-10 21:29:56.002644: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-08-10 21:29:56.222202: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-08-10 21:30:05.961788: F external\/local_tsl\/tsl\/platform\/threadpool.cc:112] Check failed: num_threads >= 1 (1 vs. -1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-08-10T13:33:57Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/73518"},{"issue_number":123,"repository":"tensorflow\/tensorflow","title":"Issue with softmax warning appearing in Tensorflow 2.17.0","description":"### TensorFlow version\r\n\r\n2.17.0\r\n\r\n### OS platform and distribution\r\n\r\nGoogle Colab\r\n\r\n### Current behavior?\r\n\r\nHi, everyone.\r\n\r\nI am practicing implementing a Transformer model that machine translates English into Korean by reading TensorFlow guides and books. However, I am having trouble because an unknown UserWarning appears during the final translation process. In that issue, I've never used softmax before, but warning me about using it. This problem appears after the model has finished training and when making inferences.\r\n\r\nI searched to see if there were any cases similar to mine, but it seems that no solution was found in any of them. \r\n\r\n\r\n\r\nsame issue: https:\/\/github.com\/tensorflow\/tensorflow\/issues\/67758\r\n\r\n\r\n\r\nThis problem did not exist in tensorflow 2.15 but appeared in 2.17.0. I can't even guess what could be causing it. For those of you who are curious about the full code, I am leaving a Colab link. You can easily reproduce it by running it with Ctrl + F9 in Google Colab. The execution time of the entire code is approximately 5 minutes ~ 5 minutes and 30 seconds on a T4 GPU. That issue is at the bottom.\r\n\r\nColab Link: https:\/\/colab.research.google.com\/drive\/1IMFWoJ1s5ReKU9LYENROpAsZ47D6cG8T?usp=sharing\r\n\r\nThe data I used is 'kor-eng.zip' located at \"https:\/\/www.manythings.org\/anki\/\".\r\n\r\nI'm really sorry for not writing the comments in English.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nclass Transformer(keras.Model):\r\n    def __init__(self, *, num_layers, encoder_sequence_length, decoder_sequence_length, source_vocab_size, target_vocab_size, embed_dim,\r\n                 dense_dim, num_heads, dropout_rate):\r\n        super().__init__()\r\n        self.encoder = Encoder(num_layers=num_layers, sequence_length=encoder_sequence_length, input_dim=source_vocab_size, embed_dim=embed_dim,\r\n                              dense_dim=dense_dim, num_heads=num_heads, dropout_rate=dropout_rate)\r\n        self.decoder = Decoder(num_layers=num_layers, sequence_length=decoder_sequence_length, input_dim=target_vocab_size, embed_dim=embed_dim,\r\n                              dense_dim=dense_dim, num_heads=num_heads, dropout_rate=dropout_rate)\r\n        self.final_layer = tf.keras.layers.Dense(units=target_vocab_size)\r\n\r\n    def call(self, inputs):\r\n        encoder_inputs, decoder_inputs = inputs\r\n        encoder_pad_mask = tf.math.not_equal(encoder_inputs, 0)[:, tf.newaxis]\r\n        decoder_pad_mask = tf.math.not_equal(decoder_inputs, 0)[:, tf.newaxis]\r\n\r\n        decoder_sequence_length = tf.shape(decoder_inputs)[1]\r\n        causal_mask = tf.linalg.band_part(tf.ones((decoder_sequence_length, decoder_sequence_length), tf.bool), -1, 0)\r\n\r\n        encoder_inputs = self.encoder(inputs=encoder_inputs, encoder_pad_mask=encoder_pad_mask)  # Shape: (batch_size, encoder_sequence_length, embed_dim)\r\n        decoder_inputs = self.decoder(inputs=decoder_inputs, encoder_outputs=encoder_inputs,\r\n                                      encoder_pad_mask=encoder_pad_mask, decoder_pad_mask=decoder_pad_mask,\r\n                                      causal_mask=causal_mask)  # Shape: (batch_size, decoder_sequence_length, embed_dim)\r\n\r\n        logits = self.final_layer(decoder_inputs)  # Shape: (batch_size, decoder_sequence_length, target_vocab_size)\r\n\r\n        try:\r\n            # losses\/metrics\uac00 \ucee4\uc9c0\uc9c0 \uc54a\uac8c keras_mask\ub97c \uc81c\uac70\r\n            del logits._keras_mask\r\n        except AttributeError:\r\n            pass\r\n\r\n        return logits\r\n\r\n\r\nclass Translator(tf.Module):\r\n    # \ub370\uc774\ud130 \uc804\ucc98\ub9ac \ud568\uc218\r\n    @staticmethod\r\n    def preprocess_text(text_: str, max_repeat: int=2) -> str:\r\n        \"\"\"\ud14d\uc2a4\ud2b8 \ubb38\uc790\uc5f4 \uc911 \uc77c\ubd80 \uaddc\uce59\uc801\uc778 \ubd80\ubd84\uc744 \uc804\ucc98\ub9ac \ud558\ub294 \ud568\uc218\r\n\r\n        Args:\r\n            text_: \ud14d\uc2a4\ud2b8 \ubb38\uc790\uc5f4 -> str\r\n            max_repeat: \ub611\uac19\uc740 \ubb38\uc790\uc5f4\uc774 \uc5f0\uc18d\ud574\uc11c \ubc18\ubcf5\ud560 \uc218 \uc788\ub294 \ucd5c\ub300 \ud69f\uc218 -> int\r\n\r\n        Returns:\r\n            text_: \uc804\ucc98\ub9ac \ub41c \ud14d\uc2a4\ud2b8 \ubb38\uc790\uc5f4 -> str\r\n        \"\"\"\r\n\r\n        text_ = text_.lower()\r\n        text_ = re.sub(pattern=rf\"[^\\w\\s{string.punctuation}]\", repl=r\"\", string=text_)\r\n        text_ = re.sub(pattern=r\"\\?+\", repl=r\"?\", string=text_) # ?\uac00 2\ubc88 \uc774\uc0c1 \uc5f0\uc18d\ub418\uba74 ?\ub85c \uc218\uc815\r\n        text_ = re.sub(pattern=r\"\\!+\", repl=r\"!\", string=text_) # !\uac00 2\ubc88 \uc774\uc0c1 \uc5f0\uc18d\ub418\uba74 !\ub85c \uc218\uc815\r\n        text_ = re.sub(pattern=r\"(?P<char>\\D)(?P=char){\" + str(max_repeat - 1) + r\",}\", repl=r\"\\g<char>\" * max_repeat, string=text_) # \uc22b\uc790\uac00 \uc544\ub2cc \ub611\uac19\uc740 \ubb38\uc790\uc5f4\uc774 repeat\ubc88 \uc774\uc0c1 \uc5f0\uc18d\ub418\uba74 repeat\ub9cc\ud07c\uc73c\ub85c \uc218\uc815\r\n        text_ = re.sub(pattern=r\"\\.{2,}\", repl=r\"...\", string=text_) # ..\uc744 ...\uc73c\ub85c \ubcc0\uacbd\r\n        text_ = re.sub(pattern=r\"\\.\\.\\.(?P<s>\\w)\", repl=r\"... \\g<s>\", string=text_) # \ub744\uc5b4\uc4f0\uae30 \uad50\uc815\r\n        text_ = re.sub(pattern=r\"\\s+\", repl=\" \", string=text_)\r\n\r\n        # \uc601\uc5b4 \ucd95\uc57d\ud615\uc744 \ud480\uae30\r\n        # 's \ucd95\uc57d\uc740 is \/ has \ub610\ub294 \uc544\uc608 \uc18c\uc720\uaca9\uc73c\ub85c \uac00\ub2a5\ud574\uc11c \uc81c\uc678\r\n        # 'd \ucd95\uc57d\uc740 had \/ would \/ could \ub4f1\uc73c\ub85c \uac00\ub2a5\ud574\uc11c \uc81c\uc678\r\n        # 'll \ucd95\uc57d\uc740 shall \/ will\ub85c \uac00\ub2a5\ud574\uc11c \uc81c\uc678\r\n        text_ = re.sub(pattern=r\"\\bi'm\\b\", repl=r\"i am\", string=text_)\r\n        text_ = re.sub(pattern=r\"\\b(?P<subj>you|we|they|there|who|when|where|what|how|why)'re\\b\", repl=r\"\\g<subj> are\", string=text_)\r\n        text_ = re.sub(pattern=r\"\\b(?P<verb>is|are|was|were|do|does|did|have|has|had|must|should|may|might|could|would|ought|dare|need)n't\\b\", repl=r\"\\g<verb> not\", string=text_)\r\n        text_ = re.sub(pattern=r\"\\bwon't\\b\", repl=r\"will not\", string=text_)\r\n        text_ = re.sub(pattern=r\"\\bcan't\\b\", repl=r\"can not\", string=text_)\r\n        text_ = re.sub(pattern=r\"\\bshan't\\b\", repl=r\"shall not\", string=text_)\r\n        text_ = re.sub(pattern=r\"\\b(?P<subj>i|you|they|we|should|could|would|must|not)'ve\\b\", repl=r\"\\g<subj> have\", string=text_)\r\n\r\n        text_ = text_.strip()\r\n        return text_\r\n\r\n    # tensorflow\uc758 \ubc14\uc774\ud2b8 \ubb38\uc790\uc5f4 \ub370\uc774\ud130 \uc804\ucc98\ub9ac \ud568\uc218\r\n    @tf.function\r\n    def tf_preprocess_text(text_: tf.string, max_repeat: int=2) -> tf.string:\r\n        \"\"\"\ud14d\uc2a4\ud2b8 \ubb38\uc790\uc5f4 \uc911 \uc77c\ubd80 \uaddc\uce59\uc801\uc778 \ubd80\ubd84\uc744 \uc804\ucc98\ub9ac \ud558\ub294 \ud568\uc218\r\n\r\n        Args:\r\n            text_: \ubc14\uc774\ud2b8 \ubb38\uc790\uc5f4 -> tf.string\r\n            max_repeat: \ub611\uac19\uc740 \ubb38\uc790\uc5f4\uc774 \uc5f0\uc18d\ud574\uc11c \ubc18\ubcf5\ud560 \uc218 \uc788\ub294 \ucd5c\ub300 \ud69f\uc218 -> int\r\n\r\n        Returns:\r\n            text_: \uc804\ucc98\ub9ac \ub41c \ubc14\uc774\ud2b8 \ubb38\uc790\uc5f4 -> tf.string\r\n        \"\"\"\r\n\r\n        text_ = tf.strings.lower(input=text_)\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=rf\"[^\\w\\s{string.punctuation}]\", rewrite=r\"\")\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\?+\", rewrite=r\"?\") # ?\uac00 2\ubc88 \uc774\uc0c1 \uc5f0\uc18d\ub418\uba74 ?\ub85c \uc218\uc815\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\!+\", rewrite=r\"!\") # !\uac00 2\ubc88 \uc774\uc0c1 \uc5f0\uc18d\ub418\uba74 !\ub85c \uc218\uc815\r\n\r\n        # \uc22b\uc790\uac00 \uc544\ub2cc \ub611\uac19\uc740 \ubb38\uc790\uc5f4\uc774 repeat\ubc88 \uc774\uc0c1 \uc5f0\uc18d\ub418\uba74 repeat\ub9cc\ud07c\uc73c\ub85c \uc218\uc815\r\n        stacks = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)\r\n        for s in tf.strings.bytes_split(text_):\r\n            if stacks.size() >= max_repeat:\r\n                if tf.strings.regex_full_match(input=s, pattern=r\"\\D\"):\r\n                    back_s = stacks.gather(indices=tf.range(start=stacks.size() - max_repeat, limit=stacks.size(), delta=1))\r\n                    if tf.math.reduce_all(back_s == s):\r\n                        continue\r\n\r\n            stacks = stacks.write(stacks.size(), s)\r\n\r\n        text_ = tf.strings.reduce_join(inputs=stacks.stack())\r\n\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\.{2,}\", rewrite=r\"...\") # ..\uc744 ...\uc73c\ub85c \ubcc0\uacbd\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\.\\.\\.(\\w)\", rewrite=r\"... \\1\") # \ub744\uc5b4\uc4f0\uae30 \uad50\uc815\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\s+\", rewrite=r\" \")\r\n\r\n        # \uc601\uc5b4 \ucd95\uc57d\ud615\uc744 \ud480\uae30\r\n        # 's \ucd95\uc57d\uc740 is \/ has \ub610\ub294 \uc544\uc608 \uc18c\uc720\uaca9\uc73c\ub85c \uac00\ub2a5\ud574\uc11c \uc81c\uc678\r\n        # 'd \ucd95\uc57d\uc740 had \/ would \/ could \ub4f1\uc73c\ub85c \uac00\ub2a5\ud574\uc11c \uc81c\uc678\r\n        # 'll \ucd95\uc57d\uc740 shall \/ will\ub85c \uac00\ub2a5\ud574\uc11c \uc81c\uc678\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\bi'm\\b\", rewrite=r\"i am\")\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\b(you|we|they|there|who|when|where|what|how|why)'re\\b\", rewrite=r\"\\1 are\")\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\b(is|are|was|were|do|does|did|have|has|had|must|should|may|might|could|would|ought|dare|need)n't\\b\", rewrite=r\"\\1 not\")\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\bwon't\\b\", rewrite=r\"will not\")\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\bcan't\\b\", rewrite=r\"can not\")\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\bsha't\\b\", rewrite=r\"shall not\")\r\n        text_ = tf.strings.regex_replace(input=text_, pattern=r\"\\b(i|you|they|we|should|could|would|must|not)'ve\\b\", rewrite=r\"\\1 have\")\r\n\r\n        text_ = tf.strings.strip(input=text_)\r\n        return text_\r\n\r\n    def __init__(self, source_tokenizer, target_tokenizer, target_length, model):\r\n        super().__init__()\r\n        self.source_tokenizer = source_tokenizer\r\n        self.target_tokenizer = target_tokenizer\r\n        self.target_length = target_length\r\n        self.model = model\r\n\r\n    def __call__(self, sentence):\r\n        sentence = Translator.tf_preprocess_text(text_=sentence)\r\n        sentence_token = self.source_tokenizer.tokenize(sentence)[tf.newaxis]\r\n\r\n        encoder_input = sentence_token\r\n\r\n        starts = tf.constant(2, dtype=tf.int32)[tf.newaxis]\r\n        ends = tf.constant(3, dtype=tf.int32)[tf.newaxis]\r\n        decoder_token = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\r\n        decoder_token = decoder_token.write(0, starts)\r\n\r\n        for i in tf.range(self.target_length):\r\n            decoder_input = tf.transpose(decoder_token.stack())\r\n            predictions = self.model([encoder_input, decoder_input], training=False)\r\n            predictions = predictions[0, -1, :] # \ub9c8\uc9c0\ub9c9 \ud1a0\ud070\uc73c\ub85c\ubd80\ud130 \uc608\uce21\r\n            predicted_id = tf.argmax(input=predictions, output_type=tf.int32)[tf.newaxis]\r\n\r\n            if predicted_id == ends:\r\n                break\r\n\r\n            decoder_token = decoder_token.write(i + 1, predicted_id)\r\n\r\n        decoder_token = tf.transpose(decoder_token.stack())[0]\r\n        decoder_token = decoder_token[1:]\r\n\r\n        return self.target_tokenizer.detokenize(decoder_token)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/ops\/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\r\n  warnings.warn(\r\nEnglish: tom came here to learn french.\r\nTranslated Korean: \ud1b0\uc740 \ud504\ub791\uc2a4\uc5b4\ub97c \ubc30\uc6b0\ub7ec \uc5ec\uae30\uc5d0 \uc654\uc5b4. \r\nReal Korean: <s> \ud1b0\uc740 \ud504\ub791\uc2a4\uc5b4\ub97c \ubc30\uc6b0\ub824\uace0 \uc5ec\uae30\uc5d0 \uc654\uc5b4. <\/s>\r\n```\r\n","labels":["type:bug","2.17"],"created_at":"2024-08-10T07:12:43Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/73516"},{"issue_number":124,"repository":"tensorflow\/tensorflow","title":"Memory leak in training","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.16.1 and 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nDebian 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython 3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\ncuda\/12.0.0_gcc-10.4.0 and cudnn\/8.9.7.29-12_gcc-10.4.0\n\n### GPU model and memory\n\ndifferent GPU, among which Tesla V100-SXM2-32GB\n\n### Current behavior?\n\nI have a memory leak (GPU memory and RAM constantly increase) during my training. \r\n\r\n**This does not happen with Tensorflow 2.11.1**. \r\n\r\n[Here is the project](https:\/\/github.com\/deep-finder\/tirfm-deepfinder).\r\nUnfortunately, this is not my code and I do not have time to create a minimal standalone code.\n\n### Standalone code to reproduce the issue\n\n```shell\ndef printMemoryUsage(self):\r\n        gpus = tf.config.list_physical_devices('GPU')             \r\n        for gpu in gpus:\r\n            gpuNameRoot = gpu.name.split(':')[0] + ':'\r\n            memory_info = tf.config.experimental.get_memory_info(gpu.name.replace(gpuNameRoot, ''))\r\n            self.display(f'Memory info of GPU {gpu.name}: current: {memory_info[\"current\"]\/1e9:.2f}, peak: {memory_info[\"peak\"]\/1e9:.2f}')\r\n            try:\r\n                import psutil\r\n                virtual_memory = psutil.virtual_memory()\r\n                print(f'Memory info of CPU: total:{virtual_memory[0]\/1e9:.2f}Gb, available: {virtual_memory[1]\/1e9:.2f}Gb, percent: {virtual_memory[2]}%')\r\n            except:\r\n                pass\r\n\r\n[...]\r\n        # Training loop:\r\n        for e in range(self.epochs):\r\n            # TRAINING:\r\n            start = time.time()\r\n            list_loss_train = []\r\n            list_acc_train = []\r\n            for it in range(self.steps_per_epoch):\r\n                if self.flag_direct_read:\r\n                    batch_data, batch_target = self.generate_batch_direct_read(path_data, path_target, self.batch_size, objlist_train)\r\n                else:\r\n                    batch_data, batch_target, idx_list = self.generate_batch_from_array(data_list, target_list, self.batch_size, objlist_train)\r\n\r\n                if self.sample_weights is not None:\r\n                    sample_weight = self.sample_weights[idx_list]\r\n                else:\r\n                    sample_weight = None\r\n\r\n                loss_train = self.net.train_on_batch(batch_data, batch_target,\r\n                                                     class_weight=self.class_weight,\r\n                                                     sample_weight=sample_weight)\r\n\r\n                self.display('epoch %d\/%d - it %d\/%d - loss: %0.3f - acc: %0.3f' % (e + 1, self.epochs, it + 1, self.steps_per_epoch, loss_train[0], loss_train[1]))\r\n\r\n                self.printMemoryUsage()\r\n\r\n                list_loss_train.append(loss_train[0])\r\n                list_acc_train.append(loss_train[1])\r\n                del batch_data\r\n                del batch_target\r\n                if idx_list is not None:\r\n                    del idx_list\r\n                gc.collect()\n```\n\n\n### Relevant log output\n\n```shell\nWith Tensorflow 2.11.1:\r\n\r\n\r\n=============================================================\r\nepoch 3\/50 - it 1\/100 - loss: 2.012 - acc: 0.976\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.02, peak: 2.90\r\nMemory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%\r\nepoch 3\/50 - it 2\/100 - loss: 2.008 - acc: 0.985\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.02, peak: 2.90\r\nMemory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%\r\nepoch 3\/50 - it 3\/100 - loss: 2.004 - acc: 0.992\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.02, peak: 2.90\r\nMemory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%\r\nepoch 3\/50 - it 4\/100 - loss: 2.006 - acc: 0.987\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.02, peak: 2.90\r\nMemory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%\r\nepoch 3\/50 - it 5\/100 - loss: 2.006 - acc: 0.989\r\n```\r\n\r\n\r\nWith Tensorflow 2.16.1 and 2.17:\r\n\r\n```\r\nepoch 1\/50 - it 6\/100 - loss: 2.473 - acc: 0.305\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.18, peak: 2.55\r\nMemory info of CPU: total:201.37Gb, available: 190.78Gb, percent: 5.3%\r\nepoch 1\/50 - it 7\/100 - loss: 2.470 - acc: 0.394\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.21, peak: 2.58\r\nMemory info of CPU: total:201.37Gb, available: 190.58Gb, percent: 5.4%\r\nepoch 1\/50 - it 8\/100 - loss: 2.466 - acc: 0.463\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.24, peak: 2.61\r\nMemory info of CPU: total:201.37Gb, available: 190.38Gb, percent: 5.5%\r\nepoch 1\/50 - it 9\/100 - loss: 2.461 - acc: 0.519\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.27, peak: 2.64\r\nMemory info of CPU: total:201.37Gb, available: 190.11Gb, percent: 5.6%\r\nepoch 1\/50 - it 10\/100 - loss: 2.455 - acc: 0.564\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.29, peak: 2.67\r\nMemory info of CPU: total:201.37Gb, available: 189.94Gb, percent: 5.7%\r\nepoch 1\/50 - it 11\/100 - loss: 2.448 - acc: 0.603\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.32, peak: 2.70\r\nMemory info of CPU: total:201.37Gb, available: 189.67Gb, percent: 5.8%\r\nepoch 1\/50 - it 12\/100 - loss: 2.437 - acc: 0.634\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.35, peak: 2.72\r\nMemory info of CPU: total:201.37Gb, available: 189.48Gb, percent: 5.9%\r\nepoch 1\/50 - it 13\/100 - loss: 2.425 - acc: 0.662\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.38, peak: 2.75\r\nMemory info of CPU: total:201.37Gb, available: 189.21Gb, percent: 6.0%\r\nepoch 1\/50 - it 14\/100 - loss: 2.408 - acc: 0.685\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.41, peak: 2.78\r\nMemory info of CPU: total:201.37Gb, available: 189.03Gb, percent: 6.1%\r\nepoch 1\/50 - it 15\/100 - loss: 2.389 - acc: 0.705\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.44, peak: 2.81\r\nMemory info of CPU: total:201.37Gb, available: 188.79Gb, percent: 6.2%\r\nepoch 1\/50 - it 16\/100 - loss: 2.369 - acc: 0.723\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.46, peak: 2.84\r\nMemory info of CPU: total:201.37Gb, available: 188.54Gb, percent: 6.4%\r\nepoch 1\/50 - it 17\/100 - loss: 2.350 - acc: 0.739\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.49, peak: 2.87\r\nMemory info of CPU: total:201.37Gb, available: 188.37Gb, percent: 6.5%\r\nepoch 1\/50 - it 18\/100 - loss: 2.332 - acc: 0.753\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.52, peak: 2.89\r\nMemory info of CPU: total:201.37Gb, available: 188.10Gb, percent: 6.6%\r\nepoch 1\/50 - it 19\/100 - loss: 2.315 - acc: 0.765\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.55, peak: 2.92\r\nMemory info of CPU: total:201.37Gb, available: 187.87Gb, percent: 6.7%\r\nepoch 1\/50 - it 20\/100 - loss: 2.300 - acc: 0.776\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.58, peak: 2.95\r\nMemory info of CPU: total:201.37Gb, available: 187.64Gb, percent: 6.8%\r\nepoch 1\/50 - it 21\/100 - loss: 2.286 - acc: 0.786\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.61, peak: 2.98\r\nMemory info of CPU: total:201.37Gb, available: 187.49Gb, percent: 6.9%\r\nepoch 1\/50 - it 22\/100 - loss: 2.274 - acc: 0.795\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.63, peak: 3.01\r\nMemory info of CPU: total:201.37Gb, available: 187.25Gb, percent: 7.0%\r\nepoch 1\/50 - it 23\/100 - loss: 2.262 - acc: 0.804\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.66, peak: 3.04\r\nMemory info of CPU: total:201.37Gb, available: 186.97Gb, percent: 7.1%\r\nepoch 1\/50 - it 24\/100 - loss: 2.251 - acc: 0.811\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.69, peak: 3.06\r\nMemory info of CPU: total:201.37Gb, available: 186.81Gb, percent: 7.2%\r\nepoch 1\/50 - it 25\/100 - loss: 2.241 - acc: 0.818\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.72, peak: 3.09\r\nMemory info of CPU: total:201.37Gb, available: 186.59Gb, percent: 7.3%\r\nepoch 1\/50 - it 26\/100 - loss: 2.232 - acc: 0.825\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.75, peak: 3.12\r\nMemory info of CPU: total:201.37Gb, available: 186.36Gb, percent: 7.5%\r\nepoch 1\/50 - it 27\/100 - loss: 2.224 - acc: 0.831\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.78, peak: 3.15\r\nMemory info of CPU: total:201.37Gb, available: 186.09Gb, percent: 7.6%\r\nepoch 1\/50 - it 28\/100 - loss: 2.216 - acc: 0.836\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.80, peak: 3.18\r\nMemory info of CPU: total:201.37Gb, available: 185.90Gb, percent: 7.7%\r\nepoch 1\/50 - it 29\/100 - loss: 2.209 - acc: 0.841\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.83, peak: 3.21\r\nMemory info of CPU: total:201.37Gb, available: 185.65Gb, percent: 7.8%\r\nepoch 1\/50 - it 30\/100 - loss: 2.202 - acc: 0.846\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.86, peak: 3.23\r\nMemory info of CPU: total:201.37Gb, available: 185.46Gb, percent: 7.9%\r\nepoch 1\/50 - it 31\/100 - loss: 2.196 - acc: 0.851\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.89, peak: 3.26\r\nMemory info of CPU: total:201.37Gb, available: 185.24Gb, percent: 8.0%\r\nepoch 1\/50 - it 32\/100 - loss: 2.190 - acc: 0.855\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.92, peak: 3.29\r\nMemory info of CPU: total:201.37Gb, available: 184.99Gb, percent: 8.1%\r\nepoch 1\/50 - it 33\/100 - loss: 2.185 - acc: 0.859\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.95, peak: 3.32\r\nMemory info of CPU: total:201.37Gb, available: 184.75Gb, percent: 8.3%\r\nepoch 1\/50 - it 34\/100 - loss: 2.180 - acc: 0.863\r\nMemory info of GPU \/physical_device:GPU:0: current: 0.97, peak: 3.35\r\nMemory info of CPU: total:201.37Gb, available: 184.51Gb, percent: 8.4%\r\nepoch 1\/50 - it 35\/100 - loss: 2.174 - acc: 0.866\r\nMemory info of GPU \/physical_device:GPU:0: current: 1.00, peak: 3.38\r\nMemory info of CPU: total:201.37Gb, available: 184.36Gb, percent: 8.4%\r\nepoch 1\/50 - it 36\/100 - loss: 2.170 - acc: 0.870\r\nMemory info of GPU \/physical_device:GPU:0: current: 1.03, peak: 3.40\r\nMemory info of CPU: total:201.37Gb, available: 184.06Gb, percent: 8.6%\r\nepoch 1\/50 - it 37\/100 - loss: 2.165 - acc: 0.873\r\nMemory info of GPU \/physical_device:GPU:0: current: 1.06, peak: 3.43\r\nMemory info of CPU: total:201.37Gb, available: 183.89Gb, percent: 8.7%\r\nepoch 1\/50 - it 38\/100 - loss: 2.161 - acc: 0.876\r\nMemory info of GPU \/physical_device:GPU:0: current: 1.09, peak: 3.46\r\nMemory info of CPU: total:201.37Gb, available: 183.66Gb, percent: 8.8%\r\nepoch 1\/50 - it 39\/100 - loss: 2.157 - acc: 0.879\r\nMemory info of GPU \/physical_device:GPU:0: current: 1.11, peak: 3.49\r\nMemory info of CPU: total:201.37Gb, available: 183.42Gb, percent: 8.9%\r\nepoch 1\/50 - it 40\/100 - loss: 2.153 - acc: 0.881\r\nMemory info of GPU \/physical_device:GPU:0: current: 1.14, peak: 3.52\r\nMemory info of CPU: total:201.37Gb, available: 183.26Gb, percent: 9.0%\r\nepoch 1\/50 - it 41\/100 - loss: 2.150 - acc: 0.884\r\nMemory info of GPU \/physical_device:GPU:0: current: 1.17, peak: 3.54\r\nMemory info of CPU: total:201.37Gb, available: 183.02Gb, percent: 9.1%\r\n```\n```\n","labels":["stat:awaiting tensorflower","type:bug","2.17"],"created_at":"2024-08-09T10:51:49Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/73457"},{"issue_number":125,"repository":"tensorflow\/tensorflow","title":"Gradient computation for `vectorized_map` nested inside `while_loop`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Mint 21.3\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI am trying to compute gradients for a `tf.vectorized_map`ped function nested within a call to `tf.while_loop` (and hence also `tf.map_fn`) as in [this (trivial!) Colab example](https:\/\/colab.research.google.com\/drive\/1IEsQAM_AU2H0bfiOfxdphk5dyi9kmS8g?usp=sharing).  The top level function can compute its return value in all three execution modes (eager, graph, XLA).  \r\n\r\nI expected to also be able to compute the gradients of the function with respect to its inputs.  This works under eager and graph mode, but not XLA mode where an InvalidArgument exception occurs:\r\n\r\n> InvalidArgumentError: Input 1 to node `gradient_tape\/while\/gradients\/while\/PartitionedCall_grad\/PartitionedCall\/gradients\/pfor\/Tile_grad\/Reshape_1` with op Reshape must be a compile-time constant.\r\n\r\nIn the example, all shapes are well-specified (by hard coding in this trivial example), so it feels like some shape information is getting lost somewhere.  Is this a bug, or a \"feature\" for which there is a workaround, I wonder? \r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1IEsQAM_AU2H0bfiOfxdphk5dyi9kmS8g?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-151283a3b91b> in <cell line: 36>()\r\n     34 print(\"Eager mode:\", value_and_grads(0.1, 0.4))  # Eager mode runs\r\n     35 print(\"Graph mode:\", tf.function(lambda: value_and_grads(0.1, 0.4))()) # Graph mode runs\r\n---> 36 print(\"XLA mode:\", tf.function(lambda: value_and_grads(0.1, 0.4), jit_compile=True)()) # XLA fails\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51   try:\r\n     52     ctx.ensure_initialized()\r\n---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                         inputs, attrs, num_outputs)\r\n     55   except core._NotOkStatusException as e:\r\n\r\nInvalidArgumentError: Input 1 to node `gradient_tape\/while\/gradients\/while\/PartitionedCall_grad\/PartitionedCall\/gradients\/pfor\/Tile_grad\/Reshape_1` with op Reshape must be a compile-time constant.\r\n\r\nXLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.\r\n\r\nStack trace for op definition: \r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/colab_kernel_launcher.py\", line 37, in <module>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/traitlets\/config\/application.py\", line 992, in launch_instance\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelapp.py\", line 619, in start\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/platform\/asyncio.py\", line 195, in start\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 685, in <lambda>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 738, in _run_callback\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 825, in inner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 361, in process_one\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 261, in dispatch_shell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 539, in execute_request\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 2975, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3030, in _run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/async_helpers.py\", line 78, in _pseudo_sync_runner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3257, in run_cell_async\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3473, in run_ast_nodes\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3553, in run_code\r\nFile \"<ipython-input-1-151283a3b91b>\", line 36, in <cell line: 36>\r\nFile \"<ipython-input-1-151283a3b91b>\", line 36, in \r\nFile \"<ipython-input-1-151283a3b91b>\", line 36, in \r\nFile \"<ipython-input-1-151283a3b91b>\", line 30, in value_and_grads\r\n\r\n\t [[{{function_node __inference___backward_f_460_482}}{{node gradients\/pfor\/Tile_grad\/Reshape_1}}]]\r\n\ttf2xla conversion failed while converting while_body_347_grad_431_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=\/path\/to\/dump\/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\r\n\r\nStack trace for op definition: \r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/colab_kernel_launcher.py\", line 37, in <module>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/traitlets\/config\/application.py\", line 992, in launch_instance\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelapp.py\", line 619, in start\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/platform\/asyncio.py\", line 195, in start\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 685, in <lambda>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 738, in _run_callback\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 825, in inner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 361, in process_one\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 261, in dispatch_shell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 539, in execute_request\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 2975, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3030, in _run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/async_helpers.py\", line 78, in _pseudo_sync_runner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3257, in run_cell_async\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3473, in run_ast_nodes\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3553, in run_code\r\nFile \"<ipython-input-1-151283a3b91b>\", line 36, in <cell line: 36>\r\nFile \"<ipython-input-1-151283a3b91b>\", line 36, in \r\nFile \"<ipython-input-1-151283a3b91b>\", line 36, in \r\nFile \"<ipython-input-1-151283a3b91b>\", line 30, in value_and_grads\r\n\r\n\t [[gradient_tape\/while\/while_grad]]\r\n\ttf2xla conversion failed while converting __inference_<lambda>_538[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=\/path\/to\/dump\/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_<lambda>_538]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","2.17"],"created_at":"2024-08-08T09:36:08Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/73367"},{"issue_number":126,"repository":"tensorflow\/tensorflow","title":"[BUG] Optimizer crash on TPU: `AttributeError: 'NoneType' object has no attribute 'extended'`","description":"### Issue type: Bug\r\n\r\nSystem info:\r\n\r\n- Kaggle TPU VM v3-8\r\n- Python 3.10\r\n- TensorFlow 2.16.1\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\ntry:\r\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\r\n    tf.config.experimental_connect_to_cluster(tpu)\r\n    tf.tpu.experimental.initialize_tpu_system(tpu)\r\n    tpu_strategy = tf.distribute.TPUStrategy(tpu)\r\n    print(\"TPU setup successful\")\r\nexcept (ValueError, ImportError) as e:\r\n    tpu_strategy = tf.distribute.get_strategy()\r\n\r\nclass BertSLPModel(tf.keras.Model):\r\n    def __init__(self):\r\n        super(BertSLPModel, self).__init__()\r\n        self.bert = bert\r\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\r\n        self.classifier = tf.keras.layers.Dense(num_classes)\r\n\r\n    def call(self, inputs):\r\n        input_ids, attention_mask = inputs\r\n        bert_output = self.bert(input_ids, attention_mask=attention_mask)\r\n        pooled_output = bert_output.pooler_output\r\n        dropout_output = self.dropout(pooled_output)\r\n        logit = self.classifier(dropout_output)\r\n        return logit\r\n\r\nwith tpu_strategy.scope():\r\n    model = BertSLPModel()\r\n    model.compile(\r\n        optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-5),\r\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\r\n    )\r\n    model.fit(\r\n        train_dataset,\r\n        validation_data=test_dataset,\r\n        epochs=epochs,\r\n        batch_size=batch_size\r\n    )\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nAttributeError: in user code:\r\n\r\n    File \"\/tf_keras\/src\/engine\/training.py\", line 1398, in train_function  *\r\n        return step_function(self, iterator)\r\n    File \"\/tf_keras\/src\/engine\/training.py\", line 1370, in run_step  *\r\n        outputs = model.train_step(data)\r\n    File \"\/tf_keras\/src\/engine\/training.py\", line 1151, in train_step  *\r\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n    File \"\/tf_keras\/src\/optimizers\/optimizer.py\", line 621, in minimize  *\r\n        self.apply_gradients(grads_and_vars)\r\n    File \"\/tf_keras\/src\/optimizers\/optimizer.py\", line 1300, in apply_gradients  *\r\n        return super().apply_gradients(grads_and_vars, name=name)\r\n    File \"\/tf_keras\/src\/optimizers\/optimizer.py\", line 715, in apply_gradients  *\r\n        self.build(trainable_variables)\r\n    File \"\/tf_keras\/src\/optimizers\/rmsprop.py\", line 121, in build  *\r\n        self._velocities.append(\r\n    File \"\/tf_keras\/src\/optimizers\/optimizer.py\", line 1201, in add_variable_from_reference  *\r\n        with strategy.extended.colocate_vars_with(model_variable):\r\n\r\n    AttributeError: 'NoneType' object has no attribute 'extended'\r\n```","labels":["stat:awaiting tensorflower","type:bug","comp:tpus","TF 2.16"],"created_at":"2024-08-07T12:09:34Z","comments":9,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/73288"},{"issue_number":127,"repository":"tensorflow\/tensorflow","title":"TF strings do not work on the GPU as indices for  `tf.gather` \/ `tf.nn.embedding_lookup`","description":"### Issue type\n\nDocumentation Feature Request\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nAs the [`tf.gather`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/gather) documentation suggests, the following code will indeed break on the CPU, but work on the GPU\r\n\r\n![image](https:\/\/github.com\/user-attachments\/assets\/4c3180fb-92ba-4d34-9518-8af5968f490e)\r\n\r\n```py\r\nindices= tf.constant([ 31., 117., 180., 255., 127.,  14.], dtype=tf.float32)\r\n\r\nprint(indices.shape)\r\n\r\nlabels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1]], dtype=tf.int32)\r\n\r\nprint(labels.shape)\r\n\r\ntf.nn.embedding_lookup(indices, labels)\r\n```\r\n\r\nBut this does not to seem to be the case when indices are of type `tf.string` instead:\r\n\r\n```py\r\nindices= tf.constant([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], dtype=tf.string)\r\n\r\nprint(indices.shape)\r\n\r\nlabels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1]], dtype=tf.int32)\r\n\r\nprint(labels.shape)\r\n\r\ntf.nn.embedding_lookup(indices, labels)\r\n```\r\n\r\nAs the op indeed runs on the CPU (see log output)\r\n\r\nA better approach is to indeed use a [`StaticHashTable`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/lookup\/StaticHashTable), and then lookup the attribute labels:\r\n\r\n```py\r\ntext_values= tf.constant([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], dtype=tf.string)\r\ntext_indices = tf.range(tf.size(text_values), dtype=tf.int32)\r\n\r\ntext_table = tf.lookup.StaticHashTable(\r\n    initializer = tf.lookup.KeyValueTensorInitializer(\r\n        keys=text_indices,\r\n        values=text_values,\r\n        key_dtype=tf.int32,\r\n        value_dtype=tf.string\r\n    ),\r\n    default_value=\"\",\r\n)\r\n\r\nlabels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1]], dtype=tf.int32)\r\n\r\ntext_table.lookup(labels)\r\n```\r\n\r\nBut I think this should be documented somewhere in `tf.gather` \/ `tf.nn.embedding_lookup`\r\n\r\n\r\nHere is a notebook that demonstrates this behaviour:\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/15Ig6Sw39lXRkZ40TvZhs4Aq_v96t-caL?usp=sharing\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nindices= tf.constant([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], dtype=tf.string)\r\n\r\nprint(indices.shape)\r\n\r\nlabels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\r\n        -1, -1, -1, -1]], dtype=tf.int32)\r\n\r\nprint(labels.shape)\r\n\r\ntf.nn.embedding_lookup(indices, labels)\n```\n\n\n### Relevant log output\n\n```shell\n(6,)\r\n(1, 100)\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-9-e96bf33d6da0> in <cell line: 15>()\r\n     13 print(labels.shape)\r\n     14 \r\n---> 15 tf.nn.embedding_lookup(indices, labels)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/framework\/ops.py in raise_from_not_ok_status(e, name)\r\n   5881 def raise_from_not_ok_status(e, name) -> NoReturn:\r\n   5882   e.message += (\" name: \" + str(name if name is not None else \"\"))\r\n-> 5883   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n   5884 \r\n   5885 \r\n\r\nInvalidArgumentError: {{function_node __wrapped__GatherV2_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} indices[0,1] = -1 is not in [0, 6) [Op:GatherV2] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-07-31T09:35:26Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/72873"},{"issue_number":128,"repository":"tensorflow\/tensorflow","title":"Sparse segment mean\/sum gives random result on empty tensor","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.12.1\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux GPU\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen inputs to `math_ops.sparse_segment_mean` or `math_ops.sparse_segment_sum` are empty. The gradient of the OP is not 0, but some random values from previous tensors. The gradient should be zero when inputs are empty\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\nfrom tensorflow.python.ops import math_ops\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n\r\ndef construct_model():\r\n  embed_dim = 128\r\n  cache_size = 2**18 + 1\r\n  with tf.device(\"\/GPU:0\"):\r\n    params = tf.compat.v1.Variable(\r\n        shape=(cache_size, embed_dim),\r\n        dtype=tf.float32,\r\n        initial_value=tf.ones(shape=(cache_size, embed_dim), dtype=tf.float32))\r\n\r\n  ids = tf.constant([], dtype=tf.int32)\r\n  segment_ids = tf.constant([], dtype=tf.int32)\r\n\r\n  embed_pooling = math_ops.sparse_segment_sum(\r\n      params,\r\n      ids,\r\n      segment_ids,\r\n      num_segments=8192,\r\n      name=\"sss\",\r\n  )\r\n\r\n  #print(embed_pooling)\r\n  loss = tf.reduce_sum(embed_pooling)\r\n  trainable_vars = tf.compat.v1.trainable_variables()\r\n  grads = tf.gradients(loss, trainable_vars)\r\n  grads_and_vars = [(g, v) for g, v in zip(grads, trainable_vars)]\r\n\r\n  sanity_check_ops = [\r\n      #tf.print(grads[0]),\r\n      tf.debugging.check_numerics(grads[0], message=\"\"),\r\n      tf.debugging.assert_equal(tf.reduce_sum(grads[0]), tf.constant(0.)),\r\n      #CRITICAL step to reproduce\r\n      tf.random.uniform((81920, 1280), 100, 101)\r\n  ]\r\n  adam_opt = tf.compat.v1.train.AdamOptimizer(\r\n      learning_rate=0.001, beta1=0.8, beta2=0.88)\r\n  with tf.control_dependencies(sanity_check_ops):\r\n\r\n    step = adam_opt.apply_gradients(grads_and_vars)\r\n\r\n  return step\r\n\r\n\r\ntf.config.threading.set_inter_op_parallelism_threads(32)\r\ntf.config.threading.set_intra_op_parallelism_threads(32)\r\nrun_op = construct_model()\r\n\r\nprofile_options = tf.compat.v1.RunOptions(\r\n    trace_level=tf.compat.v1.RunOptions.FULL_TRACE)\r\nrun_metadata = tf.compat.v1.RunMetadata()\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n  sess.run(tf.compat.v1.global_variables_initializer())\r\n  print(sess._config)\r\n\r\n  for i in range(10):\r\n    sess.run(run_op, run_metadata=run_metadata)\n```\n\n\n### Relevant log output\n\n```shell\n2024-07-30 14:39:23.458752: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-07-30 14:39:23.505787: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-07-30 14:39:25.698665: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1635] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 79078 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:e1:00.0, compute capability: 8.0\r\n2024-07-30 14:39:25.701973: I tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\r\n2024-07-30 14:39:27.165519: I tensorflow\/core\/common_runtime\/executor.cc:1197] [\/job:localhost\/replica:0\/task:0\/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: assertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]\r\n\t [[{{function_node assert_equal_1_Assert_AssertGuard_false_41}}{{node Assert}}]]\r\ndevice_count {\r\n  key: \"CPU\"\r\n  value: 1\r\n}\r\ndevice_count {\r\n  key: \"GPU\"\r\n  value: 1\r\n}\r\nintra_op_parallelism_threads: 32\r\ninter_op_parallelism_threads: 32\r\ngpu_options {\r\n  visible_device_list: \"0\"\r\n  experimental {\r\n  }\r\n}\r\nexperimental {\r\n}\r\n\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/client\/session.py\", line 1378, in _do_call\r\n    return fn(*args)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/client\/session.py\", line 1361, in _run_fn\r\n    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/client\/session.py\", line 1454, in _call_tf_sessionrun\r\n    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]\r\n\t [[{{function_node assert_equal_1_Assert_AssertGuard_false_41}}{{node Assert}}]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/t.py\", line 65, in <module>\r\n    sess.run(run_op, run_metadata=run_metadata)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/client\/session.py\", line 968, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/client\/session.py\", line 1191, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/client\/session.py\", line 1371, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/client\/session.py\", line 1397, in _do_call\r\n    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\r\n\r\nassertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]\r\n\t [[{{node Assert}}]]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.12"],"created_at":"2024-07-30T15:03:03Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/72785"},{"issue_number":129,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.BlockLSTMV2: tensorflow\/core\/framework\/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9) Aborted (core dumped) ","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0.dev20240717\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython version: 3.10.14 \n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered a bug in TensorFlow when I used API `tf.raw_ops.BlockLSTMV2`  . The code is as follows:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nseq_len_max = tf.constant(5, shape=(), dtype=tf.int64)\r\nx = tf.constant([[[8., 1.]],[[7., 6.]]], shape=(2, 1, 2), dtype=tf.float16)\r\ncs_prev = tf.constant([[5., 3.]], shape=(1, 2), dtype=tf.float16)\r\nh_prev = tf.constant([[3., 1.]], shape=(1, 2), dtype=tf.float16)\r\nw = tf.constant([[  3.,  -1.,   7.,  -2.,  -8.,   2.,  -5.,   2.],\r\n                 [ -3.,   2.,  -6.,  -4.,   0.,   0.,   2.,  -6.],\r\n                 [-10.,   0.,   8.,   2.,  -1.,  -5.,   4.,  -7.],\r\n                 [  1., -10.,  -6.,   2.,  -2., -10.,  -3.,   5.]], shape=(4, 8), dtype=tf.float16)\r\n\r\nwci = tf.constant([ 7., -7.], shape=(2,), dtype=tf.float16)\r\nwcf = tf.constant([-10., 7.], shape=(2,), dtype=tf.float16)\r\nwco = tf.constant([-3., -5.], shape=(2,), dtype=tf.float16)\r\nb = tf.constant([  1.,   6.,  -4.,  -2.,  -4.,   8., -10.,  -2.], shape=(8,), dtype=tf.float16)\r\n\r\n\r\ntf.raw_ops.BlockLSTMV2(cell_clip=-10,use_peephole=True,seq_len_max=seq_len_max,x=x,cs_prev=cs_prev,h_prev=h_prev,w=w,wci=wci,wcf=wcf,wco=wco,b=b,)\r\n```\r\n\r\nThe error message was as follows:\r\n\r\n```shell\r\n2024-07-23 20:47:52.963893: E tensorflow\/core\/util\/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\r\n2024-07-23 20:47:52.968139: F tensorflow\/core\/framework\/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9)\r\nAborted (core dumped)\r\n```\r\n\r\nThe above code would crash on `tf-nightly 2.18.0.dev20240717` (nightly-build). To reproduce the issue, I provided that a [colab notebook](https:\/\/colab.research.google.com\/drive\/1UxYr3Fg7uKd_cZA-ekKrM9WsfJK-ox_Z?usp=sharing) to reproduce the error.\r\n\r\n \n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nseq_len_max = tf.constant(5, shape=(), dtype=tf.int64)\r\nx = tf.constant([[[8., 1.]],[[7., 6.]]], shape=(2, 1, 2), dtype=tf.float16)\r\ncs_prev = tf.constant([[5., 3.]], shape=(1, 2), dtype=tf.float16)\r\nh_prev = tf.constant([[3., 1.]], shape=(1, 2), dtype=tf.float16)\r\nw = tf.constant([[  3.,  -1.,   7.,  -2.,  -8.,   2.,  -5.,   2.],\r\n                 [ -3.,   2.,  -6.,  -4.,   0.,   0.,   2.,  -6.],\r\n                 [-10.,   0.,   8.,   2.,  -1.,  -5.,   4.,  -7.],\r\n                 [  1., -10.,  -6.,   2.,  -2., -10.,  -3.,   5.]], shape=(4, 8), dtype=tf.float16)\r\n\r\nwci = tf.constant([ 7., -7.], shape=(2,), dtype=tf.float16)\r\nwcf = tf.constant([-10., 7.], shape=(2,), dtype=tf.float16)\r\nwco = tf.constant([-3., -5.], shape=(2,), dtype=tf.float16)\r\nb = tf.constant([  1.,   6.,  -4.,  -2.,  -4.,   8., -10.,  -2.], shape=(8,), dtype=tf.float16)\r\n\r\n\r\ntf.raw_ops.BlockLSTMV2(cell_clip=-10,use_peephole=True,seq_len_max=seq_len_max,x=x,cs_prev=cs_prev,h_prev=h_prev,w=w,wci=wci,wcf=wcf,wco=wco,b=b,)\n```\n\n\n### Relevant log output\n\n```shell\n2024-07-23 20:47:52.963893: E tensorflow\/core\/util\/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\r\n2024-07-23 20:47:52.968139: F tensorflow\/core\/framework\/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-07-23T12:46:06Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/72362"},{"issue_number":130,"repository":"tensorflow\/tensorflow","title":"tf.strided_slice new_axis_mask inconsistency","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly, 2.17.0, 2.16.2, 2.16.1\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nmacOS, Linux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12, 3.12.4\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI'm seeing that when `spec_size` (as described in the docs as the length of the `begin`, `end`, `strides` arrays) is less than the rank of the `input` tensor, the behavior of `strided_slice` differs when `new_axis_mask` for the bits between `len(begin)` and `tf.rank(input)` is specified.\r\n\r\nIs this the expected behavior, and if so, is there anything else like this that differs when `spec_size != tf.rank(input)`? \r\n\r\nI'm noting that it's currently permitted for `spec_size > tf.rank(input)`, which yields the same result in this case as when `spec_size == tf.rank(input)`.\n\n### Standalone code to reproduce the issue\n\n```shell\nNotebook:\r\nhttps:\/\/colab.research.google.com\/drive\/1-LCENzCjorhzyDCqq5qfB4IFoxLbHjGI?usp=sharing\r\nCode:\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef test(t, begin, end, mask):\r\n    return tf.strided_slice(t, begin, end, new_axis_mask=mask)\r\n\r\nt = tf.constant(np.arange(0,27).reshape((3,3,3)))\r\nmask = 0b111\r\n\r\n# Noting as comments the shape of the result\r\n\r\n# shape = (1, 3, 3, 3)\r\nprint(test(t, [0], [3], mask))\r\n# shape = (1, 1, 3, 3, 3)\r\nprint(test(t, [0, 0], [3, 3], mask))\r\n# shape = (1, 1, 1, 3, 3, 3)\r\nprint(test(t, [0, 0, 0], [3, 3, 3], mask))\r\n# shape = (1, 1, 1, 3, 3, 3)\r\nprint(test(t, [0, 0, 0, 0], [3, 3, 3, 3], mask))\r\n```\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-07-23T00:44:47Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/72332"},{"issue_number":131,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.MapUnstage: Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor Aborted (core dumped)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0.dev20240717\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython version: 3.10.14 \n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered a bug in TensorFlow when I used API `tf.raw_ops.MapUnstage`  with randomly generated tensors. The code is as follows:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nkey = tf.random.uniform([15, 8], minval=-10, maxval=10, dtype=tf.int64)\r\nindices = tf.random.uniform([1], minval=-10, maxval=10, dtype=tf.int32)\r\ntf.raw_ops.MapUnstage(capacity=100,memory_limit=100,dtypes=[tf.float64],\r\n                      container=\"\",shared_name=\"\",key=key,indices=indices)\r\n```\r\n\r\nThe error message was as follows:\r\n\r\n```shell\r\n2024-07-22 22:05:52.420257: F tensorflow\/core\/framework\/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor\r\nAborted (core dumped)\r\n```\r\n\r\nI have confirmed that above code would crash on `tf-nightly 2.18.0.dev20240717` (nightly-build). Also, I provided that a [colab notebook](https:\/\/colab.research.google.com\/drive\/1PXsrbcckN5x_ooMjZr5Vds6KVKMve7ph?usp=sharing) to reproduce the error.\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nkey = tf.random.uniform([15, 8], minval=-10, maxval=10, dtype=tf.int64)\r\nindices = tf.random.uniform([1], minval=-10, maxval=10, dtype=tf.int32)\r\ntf.raw_ops.MapUnstage(capacity=100,memory_limit=100,dtypes=[tf.float64],\r\n                      container=\"\",shared_name=\"\",key=key,indices=indices)\n```\n\n\n### Relevant log output\n\n```shell\n2024-07-22 22:05:52.420257: F tensorflow\/core\/framework\/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-07-22T14:08:43Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/72295"},{"issue_number":132,"repository":"tensorflow\/tensorflow","title":"TF timeline timestamp was shifted. The resultant timeline cannot be shown correctly in chrome tracing viewer","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nTF2.12\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nCUDA 12, cuDNN 8\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen profiling on GPU. Timestamp in Chrome trace format was incorrect due to shifting. The relevant commit is https:\/\/github.com\/tensorflow\/tensorflow\/commit\/701c1e97317ace357621b7f0bd4a2e427f16ed42#r144411330\r\n\r\nThere was a issue on  https:\/\/github.com\/tensorflow\/profiler\/issues\/238, but does not fix.\r\n\r\n**Incorrect** timeline if `CollectData` was called:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/aeeeef0ba125dd2b28b59c5d144dd0a237a780c4\/tensorflow\/core\/profiler\/lib\/device_profiler_session.h#L56-L59\r\n\r\n**Correct** timelie if `CollectDataInternal` was called:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/a3e2c692c18649329c4210cf8df2487d2028e267\/tensorflow\/core\/profiler\/lib\/device_profiler_session.h#L56-L59\r\n\r\nThe shifting was called in `PostProcessSingleHostXSpace` in `CollectData`\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/aeeeef0ba125dd2b28b59c5d144dd0a237a780c4\/third_party\/xla\/third_party\/tsl\/tsl\/profiler\/lib\/profiler_session.cc#L81-L88\r\nIs shifting expected behavior? If that, how to use the profiler session correctly?\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom absl import app\r\nimport logging\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ndef test_profiler():\r\n  shape = tf.constant([1000, 1000], dtype=tf.int64)\r\n  x = tf.random.normal(shape)\r\n  y = x ** 2\r\n  z = y ** 2\r\n  with tf.compat.v1.Session() as sess:\r\n    run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)\r\n    run_metadata = tf.compat.v1.RunMetadata()\r\n    #for i in range(3):\r\n    sess.run(z, options=run_options, run_metadata=run_metadata)\r\n    #logging.info(\"run_metadata.step_stats:%s\", run_metadata.step_stats)\r\n    tl = timeline.Timeline(run_metadata.step_stats)\r\n    ctf = tl.generate_chrome_trace_format()\r\n    with open('timeline.json', 'w') as f:\r\n      f.write(ctf)\r\n\r\ndef main(argv):\r\n  test_profiler()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n  app.run(main)\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:core","TF 2.12"],"created_at":"2024-07-19T06:52:16Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/72156"},{"issue_number":133,"repository":"tensorflow\/tensorflow","title":"Build Error on aarch64 AWS Graviton3 with Ubuntu 22.04 for TensorFlow v2.17.0 with mkl_aarch64","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.17.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 22.04.2 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.9\n\n### Bazel version\n\n6.5.0\n\n### GCC\/compiler version\n\nclang version 17.0.6\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI'm encountering build errors when trying to build TensorFlow v2.17.0 from source on an AWS Graviton3 instance (aarch64 architecture) running Ubuntu 22.04. The build fails with errors related to the **MakeOneDnnStream** function.\r\n\r\nExpected behavior: The build should complete successfully without any errors.\n\n### Standalone code to reproduce the issue\n\n```shell\n1. Set up an AWS Graviton3 instance with Ubuntu 22.04.\r\n2. Clone the TensorFlow repository and checkout version 2.17.0:\r\n   \r\n   git clone https:\/\/github.com\/tensorflow\/tensorflow.git\r\n   cd tensorflow\r\n   git checkout v2.17.0\r\n   \r\n3. Install Bazel and other dependencies as per the TensorFlow build documentation.\r\n4. Run the build command:\r\n   ```bash\r\n   bazel build \/\/tensorflow\/tools\/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --features=-layering_check --config=mkl_aarch64 --config=opt --copt=-march=armv8-a+sve --copt=-msve-vector-bits=256 --copt=-O3 --copt=-Wno-gnu-offsetof-extensions --copt=-Wno-unused-but-set-variable --jobs=48 --local_cpu_resources=26 --verbose_failures\r\n   ```\r\n\r\nAny guidance or fixes to resolve this build error would be greatly appreciated.\n```\n\n\n### Relevant log output\n\n```shell\n(tf-venv) user@ip-xxx-xx-x-xxx:~\/work_dirr\/tensorflow$ taskset -c 6-31 bazel build \/\/tensorflow\/tools\/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --features=-layering_check --config=mkl_aarch64 --config=opt --copt=-march=armv8-a+sve --copt=-msve-vector-bits=256 --copt=-O3 --copt=-Wno-gnu-offsetof-extensions --copt=-Wno-unused-but-set-variable  --jobs=48 --local_cpu_resources=26 --verbose_failures\r\nINFO: Reading 'startup' options from \/home\/deepesh\/work_dirr\/tensorflow\/.bazelrc: --windows_enable_symlinks\r\nINFO: Options provided by the client:\r\n  Inherited 'common' options: --isatty=1 --terminal_columns=113\r\nINFO: Reading rc options for 'build' from \/home\/deepesh\/work_dirr\/tensorflow\/.bazelrc:\r\n  Inherited 'common' options: --experimental_repo_remote_exec\r\nINFO: Reading rc options for 'build' from \/home\/deepesh\/work_dirr\/tensorflow\/.bazelrc:\r\n  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility\r\nINFO: Reading rc options for 'build' from \/home\/deepesh\/work_dirr\/tensorflow\/.tf_configure.bazelrc:\r\n  'build' options: --action_env PYTHON_BIN_PATH=\/home\/deepesh\/work_dirr\/tf-venv\/bin\/python3 --action_env PYTHON_LIB_PATH=\/home\/deepesh\/work_dirr\/tf-venv\/lib\/python3.11\/site-packages --python_path=\/home\/deepesh\/work_dirr\/tf-venv\/bin\/python3 --action_env CLANG_COMPILER_PATH=\/usr\/lib\/llvm-17\/bin\/clang --repo_env=CC=\/usr\/lib\/llvm-17\/bin\/clang --repo_env=BAZEL_COMPILER=\/usr\/lib\/llvm-17\/bin\/clang --copt=-Wno-gnu-offsetof-extensions\r\nINFO: Found applicable config definition build:short_logs in file \/home\/deepesh\/work_dirr\/tensorflow\/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\r\nINFO: Found applicable config definition build:v2 in file \/home\/deepesh\/work_dirr\/tensorflow\/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1\r\nINFO: Found applicable config definition build:mkl_aarch64 in file \/home\/deepesh\/work_dirr\/tensorflow\/.bazelrc: --define=build_with_mkl_aarch64=true --define=build_with_openmp=true --define=build_with_acl=true -c opt\r\nINFO: Found applicable config definition build:opt in file \/home\/deepesh\/work_dirr\/tensorflow\/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare\r\nINFO: Found applicable config definition build:linux in file \/home\/deepesh\/work_dirr\/tensorflow\/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=\/usr --define=LIBDIR=$(PREFIX)\/lib --define=INCLUDEDIR=$(PREFIX)\/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)\/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes\r\nINFO: Found applicable config definition build:dynamic_kernels in file \/home\/deepesh\/work_dirr\/tensorflow\/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS\r\nINFO: Analyzed target \/\/tensorflow\/tools\/pip_package:wheel (1 packages loaded, 3758 targets configured).\r\nINFO: Found 1 target...\r\nERROR: \/home\/deepesh\/.cache\/bazel\/_bazel_deepesh\/c2d183634e6ef66bd4ea91e213a12542\/external\/local_xla\/xla\/service\/cpu\/BUILD:1665:11: Compiling xla\/service\/cpu\/onednn_matmul.cc failed: (Exit 1): clang failed: error executing command (from target @local_xla\/\/xla\/service\/cpu:onednn_matmul) \r\n  (cd \/home\/deepesh\/.cache\/bazel\/_bazel_deepesh\/c2d183634e6ef66bd4ea91e213a12542\/execroot\/org_tensorflow && \\\r\n  exec env - \\\r\n    CLANG_COMPILER_PATH=\/usr\/lib\/llvm-17\/bin\/clang \\\r\n    PATH=\/home\/deepesh\/.cache\/bazelisk\/downloads\/bazelbuild\/bazel-6.5.0-linux-arm64\/bin:\/home\/deepesh\/work_dirr\/tf-venv\/bin:\/home\/deepesh\/.vscode-server\/cli\/servers\/Stable-f1e16e1e6214d7c44d078b1f0607b2388f29d729\/server\/bin\/remote-cli:\/home\/deepesh\/.local\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin:\/usr\/games:\/usr\/local\/games:\/snap\/bin \\\r\n    PWD=\/proc\/self\/cwd \\\r\n    PYTHON_BIN_PATH=\/home\/deepesh\/work_dirr\/tf-venv\/bin\/python3 \\\r\n    PYTHON_LIB_PATH=\/home\/deepesh\/work_dirr\/tf-venv\/lib\/python3.11\/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n  \/usr\/lib\/llvm-17\/bin\/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++14' -MD -MF bazel-out\/aarch64-opt\/bin\/external\/local_xla\/xla\/service\/cpu\/_objs\/onednn_matmul\/onednn_matmul.pic.d '-frandom-seed=bazel-out\/aarch64-opt\/bin\/external\/local_xla\/xla\/service\/cpu\/_objs\/onednn_matmul\/onednn_matmul.pic.o' -fPIC '-DEIGEN_MAX_ALIGN_BYTES=64' -DEIGEN_ALLOW_UNALIGNED_SCALARS '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT=\".so\"' '-DLLVM_PLUGIN_EXT=\".so\"' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' -DHAVE_BUILTIN_THREAD_POINTER '-DLLVM_NATIVE_ARCH=\"AArch64\"' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeAArch64AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeAArch64AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeAArch64Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeAArch64Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeAArch64TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeAArch64TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeAArch64TargetMCA' '-DLLVM_HOST_TRIPLE=\"aarch64-unknown-linux-gnu\"' '-DLLVM_DEFAULT_TARGET_TRIPLE=\"aarch64-unknown-linux-gnu\"' '-DLLVM_VERSION_MAJOR=19' '-DLLVM_VERSION_MINOR=0' '-DLLVM_VERSION_PATCH=0' '-DLLVM_VERSION_STRING=\"19.0.0git\"' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS '-DBLAKE3_USE_NEON=0' -DBLAKE3_NO_AVX2 -DBLAKE3_NO_AVX512 -DBLAKE3_NO_SSE2 -DBLAKE3_NO_SSE41 -DENABLE_NEON -DARM_COMPUTE_CPU_ENABLED -DARM_COMPUTE_ENABLE_NEON -DARM_COMPUTE_ENABLE_I8MM -DENABLE_FP32_KERNELS -DENABLE_QASYMM8_KERNELS -DENABLE_QASYMM8_SIGNED_KERNELS -DENABLE_QSYMM16_KERNELS -DENABLE_INTEGER_KERNELS -DENABLE_NHWC_KERNELS -DENABLE_NCHW_KERNELS -DARM_COMPUTE_GRAPH_ENABLED -DARM_COMPUTE_ENABLE_SVEF32MM -DARM_COMPUTE_ENABLE_FIXED_FORMAT_KERNELS -D_GLIBCXX_USE_NANOSLEEP -DARM_COMPUTE_OPENMP_SCHEDULER '-DDNNL_AARCH64_USE_ACL=1' '-DBAZEL_CURRENT_REPOSITORY=\"local_xla\"' -iquote external\/local_xla -iquote bazel-out\/aarch64-opt\/bin\/external\/local_xla -iquote external\/com_google_protobuf -iquote bazel-out\/aarch64-opt\/bin\/external\/com_google_protobuf -iquote external\/com_google_absl -iquote bazel-out\/aarch64-opt\/bin\/external\/com_google_absl -iquote external\/eigen_archive -iquote bazel-out\/aarch64-opt\/bin\/external\/eigen_archive -iquote external\/local_tsl -iquote bazel-out\/aarch64-opt\/bin\/external\/local_tsl -iquote external\/ml_dtypes -iquote bazel-out\/aarch64-opt\/bin\/external\/ml_dtypes -iquote external\/nsync -iquote bazel-out\/aarch64-opt\/bin\/external\/nsync -iquote external\/double_conversion -iquote bazel-out\/aarch64-opt\/bin\/external\/double_conversion -iquote external\/snappy -iquote bazel-out\/aarch64-opt\/bin\/external\/snappy -iquote external\/com_googlesource_code_re2 -iquote bazel-out\/aarch64-opt\/bin\/external\/com_googlesource_code_re2 -iquote external\/farmhash_archive -iquote bazel-out\/aarch64-opt\/bin\/external\/farmhash_archive -iquote external\/llvm-project -iquote bazel-out\/aarch64-opt\/bin\/external\/llvm-project -iquote external\/zlib -iquote bazel-out\/aarch64-opt\/bin\/external\/zlib -iquote external\/mkl_dnn_acl_compatible -iquote bazel-out\/aarch64-opt\/bin\/external\/mkl_dnn_acl_compatible -iquote external\/compute_library -iquote bazel-out\/aarch64-opt\/bin\/external\/compute_library -Ibazel-out\/aarch64-opt\/bin\/external\/ml_dtypes\/_virtual_includes\/float8 -Ibazel-out\/aarch64-opt\/bin\/external\/ml_dtypes\/_virtual_includes\/intn -Ibazel-out\/aarch64-opt\/bin\/external\/llvm-project\/mlir\/_virtual_includes\/ArithCanonicalizationIncGen -Ibazel-out\/aarch64-opt\/bin\/external\/llvm-project\/mlir\/_virtual_includes\/AsmParserTokenKinds -Ibazel-out\/aarch64-opt\/bin\/external\/compute_library\/include\/_virtual_includes\/include -isystem external\/com_google_protobuf\/src -isystem bazel-out\/aarch64-opt\/bin\/external\/com_google_protobuf\/src -isystem external\/eigen_archive -isystem bazel-out\/aarch64-opt\/bin\/external\/eigen_archive -isystem external\/eigen_archive\/mkl_include -isystem bazel-out\/aarch64-opt\/bin\/external\/eigen_archive\/mkl_include -isystem external\/ml_dtypes -isystem bazel-out\/aarch64-opt\/bin\/external\/ml_dtypes -isystem external\/ml_dtypes\/ml_dtypes -isystem bazel-out\/aarch64-opt\/bin\/external\/ml_dtypes\/ml_dtypes -isystem external\/nsync\/public -isystem bazel-out\/aarch64-opt\/bin\/external\/nsync\/public -isystem external\/farmhash_archive\/src -isystem bazel-out\/aarch64-opt\/bin\/external\/farmhash_archive\/src -isystem external\/llvm-project\/llvm\/include -isystem bazel-out\/aarch64-opt\/bin\/external\/llvm-project\/llvm\/include -isystem external\/zlib -isystem bazel-out\/aarch64-opt\/bin\/external\/zlib -isystem external\/llvm-project\/mlir\/include -isystem bazel-out\/aarch64-opt\/bin\/external\/llvm-project\/mlir\/include -isystem external\/mkl_dnn_acl_compatible\/include -isystem bazel-out\/aarch64-opt\/bin\/external\/mkl_dnn_acl_compatible\/include -isystem external\/mkl_dnn_acl_compatible\/src -isystem bazel-out\/aarch64-opt\/bin\/external\/mkl_dnn_acl_compatible\/src -isystem external\/mkl_dnn_acl_compatible\/src\/common -isystem bazel-out\/aarch64-opt\/bin\/external\/mkl_dnn_acl_compatible\/src\/common -isystem external\/mkl_dnn_acl_compatible\/src\/cpu -isystem bazel-out\/aarch64-opt\/bin\/external\/mkl_dnn_acl_compatible\/src\/cpu -isystem external\/mkl_dnn_acl_compatible\/src\/cpu\/aarch64\/xbyak_aarch64\/src -isystem bazel-out\/aarch64-opt\/bin\/external\/mkl_dnn_acl_compatible\/src\/cpu\/aarch64\/xbyak_aarch64\/src -isystem external\/mkl_dnn_acl_compatible\/src\/cpu\/aarch64\/xbyak_aarch64\/xbyak_aarch64 -isystem bazel-out\/aarch64-opt\/bin\/external\/mkl_dnn_acl_compatible\/src\/cpu\/aarch64\/xbyak_aarch64\/xbyak_aarch64 -isystem external\/mkl_dnn_acl_compatible\/src\/cpu\/gemm -isystem bazel-out\/aarch64-opt\/bin\/external\/mkl_dnn_acl_compatible\/src\/cpu\/gemm -isystem external\/compute_library\/arm_compute\/runtime -isystem bazel-out\/aarch64-opt\/bin\/external\/compute_library\/arm_compute\/runtime -isystem external\/compute_library\/src\/core\/NEON\/kernels\/assembly -isystem bazel-out\/aarch64-opt\/bin\/external\/compute_library\/src\/core\/NEON\/kernels\/assembly -isystem external\/compute_library\/src\/core\/NEON\/kernels\/convolution\/common -isystem bazel-out\/aarch64-opt\/bin\/external\/compute_library\/src\/core\/NEON\/kernels\/convolution\/common -isystem external\/compute_library\/src\/core\/NEON\/kernels\/convolution\/winograd -isystem bazel-out\/aarch64-opt\/bin\/external\/compute_library\/src\/core\/NEON\/kernels\/convolution\/winograd -isystem external\/compute_library\/src\/core\/cpu\/kernels\/assembly -isystem bazel-out\/aarch64-opt\/bin\/external\/compute_library\/src\/core\/cpu\/kernels\/assembly -isystem external\/compute_library\/src\/cpu\/kernels\/assembly -isystem bazel-out\/aarch64-opt\/bin\/external\/compute_library\/src\/cpu\/kernels\/assembly -isystem external\/compute_library\/src\/core\/NEON\/kernels\/arm_conv -isystem bazel-out\/aarch64-opt\/bin\/external\/compute_library\/src\/core\/NEON\/kernels\/arm_conv -isystem external\/compute_library\/src\/core\/NEON\/kernels\/arm_gemm -isystem bazel-out\/aarch64-opt\/bin\/external\/compute_library\/src\/core\/NEON\/kernels\/arm_gemm -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions -Wno-sign-compare '-march=armv8-a+sve' '-msve-vector-bits=256' -O3 -Wno-gnu-offsetof-extensions -Wno-unused-but-set-variable '-std=c++17' -DEIGEN_AVOID_STL_ARRAY -DEIGEN_AVOID_STL_ARRAY -Iexternal\/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -DENABLE_ONEDNN_V3 '-DDNNL_AARCH64_USE_ACL=1' -DENABLE_ONEDNN_OPENMP '-DXLA_CPU_USE_ACL=1' -fexceptions -pthread -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -c external\/local_xla\/xla\/service\/cpu\/onednn_matmul.cc -o bazel-out\/aarch64-opt\/bin\/external\/local_xla\/xla\/service\/cpu\/_objs\/onednn_matmul\/onednn_matmul.pic.o)\r\n# Configuration: 389168362472d4b0f209a68c4f568adffc1a33297c68acb48d71fb4bd10724d2\r\n# Execution platform: @local_execution_config_platform\/\/:platform\r\nIn file included from external\/local_xla\/xla\/service\/cpu\/onednn_matmul.cc:31:\r\nIn file included from external\/local_xla\/xla\/hlo\/ir\/hlo_instructions.h:37:\r\nIn file included from external\/local_xla\/xla\/hlo\/ir\/hlo_computation.h:31:\r\nexternal\/com_google_absl\/absl\/log\/log.h:199:9: warning: 'LOG' macro redefined [-Wmacro-redefined]\r\n  199 | #define LOG(severity) ABSL_LOG_INTERNAL_LOG_IMPL(_##severity)\r\n      |         ^\r\nexternal\/local_tsl\/tsl\/platform\/default\/logging.h:165:9: note: previous definition is here\r\n  165 | #define LOG(severity) _TF_LOG_##severity\r\n      |         ^\r\nIn file included from external\/local_xla\/xla\/service\/cpu\/onednn_matmul.cc:31:\r\nIn file included from external\/local_xla\/xla\/hlo\/ir\/hlo_instructions.h:37:\r\nIn file included from external\/local_xla\/xla\/hlo\/ir\/hlo_computation.h:31:\r\nexternal\/com_google_absl\/absl\/log\/log.h:237:9: warning: 'LOG_EVERY_N' macro redefined [-Wmacro-redefined]\r\n  237 | #define LOG_EVERY_N(severity, n) \\\r\n      |         ^\r\nexternal\/local_tsl\/tsl\/platform\/default\/logging.h:278:9: note: previous definition is here\r\n  278 | #define LOG_EVERY_N(severity, n)                       \\\r\n      |         ^\r\nIn file included from external\/local_xla\/xla\/service\/cpu\/onednn_matmul.cc:31:\r\nIn file included from external\/local_xla\/xla\/hlo\/ir\/hlo_instructions.h:37:\r\nIn file included from external\/local_xla\/xla\/hlo\/ir\/hlo_computation.h:31:\r\nexternal\/com_google_absl\/absl\/log\/log.h:245:9: warning: 'LOG_FIRST_N' macro redefined [-Wmacro-redefined]\r\n  245 | #define LOG_FIRST_N(severity, n) \\\r\n      |         ^\r\nexternal\/local_tsl\/tsl\/platform\/default\/logging.h:284:9: note: previous definition is here\r\n  284 | #define LOG_FIRST_N(severity, n)                       \\\r\n      |         ^\r\nIn file included from external\/local_xla\/xla\/service\/cpu\/onednn_matmul.cc:31:\r\nIn file included from external\/local_xla\/xla\/hlo\/ir\/hlo_instructions.h:37:\r\nIn file included from external\/local_xla\/xla\/hlo\/ir\/hlo_computation.h:31:\r\nexternal\/com_google_absl\/absl\/log\/log.h:253:9: warning: 'LOG_EVERY_POW_2' macro redefined [-Wmacro-redefined]\r\n  253 | #define LOG_EVERY_POW_2(severity) \\\r\n      |         ^\r\nexternal\/local_tsl\/tsl\/platform\/default\/logging.h:290:9: note: previous definition is here\r\n  290 | #define LOG_EVERY_POW_2(severity)                         \\\r\n      |         ^\r\nIn file included from external\/local_xla\/xla\/service\/cpu\/onednn_matmul.cc:31:\r\nIn file included from external\/local_xla\/xla\/hlo\/ir\/hlo_instructions.h:37:\r\nIn file included from external\/local_xla\/xla\/hlo\/ir\/hlo_computation.h:31:\r\nexternal\/com_google_absl\/absl\/log\/log.h:265:9: warning: 'LOG_EVERY_N_SEC' macro redefined [-Wmacro-redefined]\r\n  265 | #define LOG_EVERY_N_SEC(severity, n_seconds) \\\r\n      |         ^\r\nexternal\/local_tsl\/tsl\/platform\/default\/logging.h:300:9: note: previous definition is here\r\n  300 | #define LOG_EVERY_N_SEC(severity, n_seconds)                      \\\r\n      |         ^\r\nexternal\/local_xla\/xla\/service\/cpu\/onednn_matmul.cc:270:24: error: no matching function for call to 'MakeOneDnnStream'\r\n  270 |   auto onednn_stream = MakeOneDnnStream(cpu_engine, thread_pool.get());\r\n      |                        ^~~~~~~~~~~~~~~~\r\nexternal\/local_xla\/xla\/service\/cpu\/onednn_util.h:57:14: note: candidate function not viable: no known conversion from 'pointer' (aka 'tsl::OneDnnThreadPool *') to 'dnnl::threadpool_interop::threadpool_iface *' for 2nd argument\r\n   57 | dnnl::stream MakeOneDnnStream(\r\n      |              ^\r\n   58 |     const dnnl::engine& cpu_engine,\r\n   59 |     dnnl::threadpool_interop::threadpool_iface* thread_pool);\r\n      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nexternal\/local_xla\/xla\/service\/cpu\/onednn_matmul.cc:359:24: error: no matching function for call to 'MakeOneDnnStream'\r\n  359 |   auto onednn_stream = MakeOneDnnStream(cpu_engine, thread_pool.get());\r\n      |                        ^~~~~~~~~~~~~~~~\r\nexternal\/local_xla\/xla\/service\/cpu\/onednn_util.h:57:14: note: candidate function not viable: no known conversion from 'pointer' (aka 'tsl::OneDnnThreadPool *') to 'dnnl::threadpool_interop::threadpool_iface *' for 2nd argument\r\n   57 | dnnl::stream MakeOneDnnStream(\r\n      |              ^\r\n   58 |     const dnnl::engine& cpu_engine,\r\n   59 |     dnnl::threadpool_interop::threadpool_iface* thread_pool);\r\n      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n5 warnings and 2 errors generated.\r\nTarget \/\/tensorflow\/tools\/pip_package:wheel failed to build\r\nINFO: Elapsed time: 720.177s, Critical Path: 142.22s\r\nINFO: 3322 processes: 82 internal, 3240 local.\r\nFAILED: Build did NOT complete successfully\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","comp:mkl","subtype: ubuntu\/linux","2.17"],"created_at":"2024-07-18T09:02:02Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/72081"},{"issue_number":134,"repository":"tensorflow\/tensorflow","title":"`tf.data.Dataset.prefetch()` error with basic usage","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.17.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nThe most basic usage of `tf.data.Dataset.prefetch()` raises an error. \r\n\r\nThe `buffer_size` argument is documented as requiring a int64 tensor:\r\n\r\n> `buffer_size` \t\r\n> A [tf.int64](https:\/\/www.tensorflow.org\/api_docs\/python\/tf#int64) scalar [tf.Tensor](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/Tensor), representing the maximum number of elements that will be buffered when prefetching. If the value [tf.data.AUTOTUNE](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data#AUTOTUNE) is used, then the buffer size is dynamically tuned.\r\n\r\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#prefetch\r\n\r\nProbably related to https:\/\/github.com\/tensorflow\/tensorflow\/issues\/71744, The \"eager fallback\" codepath is broken.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nx = np.arange(5)\r\ny = np.arange(5)\r\n\r\nds = tf.data.Dataset.from_tensor_slices((x, y)).batch(2)\r\n\r\nds.prefetch(tf.constant(1, dtype = 'int64'))\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/data\/ops\/dataset_ops.py\", line 1259, in prefetch\r\n    return prefetch_op._prefetch(  # pylint: disable=protected-access\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/data\/ops\/prefetch_op.py\", line 28, in _prefetch\r\n    return _PrefetchDataset(input_dataset, buffer_size, name=name)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/data\/ops\/prefetch_op.py\", line 46, in __init__\r\n    variant_tensor = gen_dataset_ops.prefetch_dataset(\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/ops\/gen_dataset_ops.py\", line 6001, in prefetch_dataset\r\n    return prefetch_dataset_eager_fallback(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/ops\/gen_dataset_ops.py\", line 6072, in prefetch_dataset_eager_fallback\r\n    legacy_autotune = _execute.make_bool(legacy_autotune, \"legacy_autotune\")\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/eager\/execute.py\", line 172, in make_bool\r\n    raise TypeError(\"Expected bool for argument '%s' not %s.\" %\r\nTypeError: Expected bool for argument 'legacy_autotune' not <tf.Tensor: shape=(), dtype=bool, numpy=False>.\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:data","regression issue","2.17"],"created_at":"2024-07-16T15:02:16Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/71937"},{"issue_number":135,"repository":"tensorflow\/tensorflow","title":"`tf.data.Dataset.from_tensor_slices` allocates GPU RAM","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.17.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 22.04.4 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nPassing a numpy array to `tf.data.Dataset.from_tensor_slices()` attempts to allocate the dataset as a tensor on the GPU device, and raises an exception if there is not enough GPU RAM available. \r\n\r\nThis only started with TF 2.17.0. In all previous TF versions, all `tf.data.Dataset` operations were always pinned to the CPU.\r\n\r\nTo reproduce, create a numpy array larger than can fit on the GPU, and attempt to create a `tf.data.Dataset` from it.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ngpu_ram_gb = 12 # adjust for size of GPU \r\n\r\ngb = gpu_ram_gb+1; dtype = \"float64\"\r\nsize = (gb * 1024**3) \/\/ tf.dtypes.as_dtype(dtype).size\r\n\r\nx = np.zeros((size,), dtype = dtype)\r\n\r\ntf.data.Dataset.from_tensor_slices(x)\n```\n\n\n### Relevant log output\n\n```shell\n2024-07-12 08:20:42.771788: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-07-12 08:20:42.784893: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-07-12 08:20:42.788889: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-07-12 08:20:42.798174: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-07-12 08:20:43.492876: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1720786850.494498   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1720786850.527964   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1720786850.531351   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1720786850.535502   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1720786850.538786   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1720786850.541878   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1720786850.710210   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1720786850.711607   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\nI0000 00:00:1720786850.712908   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-07-12 08:20:50.714170: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 34 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5\r\n2024-07-12 08:20:50.715606: W external\/local_tsl\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 13958643712 exceeds 10% of free system memory.\r\n2024-07-12 08:21:05.997032: W external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 13.00GiB (rounded to 13958643712)requested by op _EagerConst\r\nIf the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \r\nCurrent allocation summary follows.\r\nCurrent allocation summary follows.\r\n2024-07-12 08:21:05.997054: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\r\n2024-07-12 08:21:05.997064: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997072: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997079: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997087: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997093: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997100: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997107: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997114: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997121: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997128: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997135: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997142: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997148: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997155: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997162: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997169: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997176: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997183: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997190: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997197: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997203: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2024-07-12 08:21:05.997212: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1062] Bin for 13.00GiB was 256.00MiB, Chunk State: \r\n2024-07-12 08:21:05.997219: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \r\n2024-07-12 08:21:05.997225: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1107] Sum Total of in-use chunks: 0B\r\n2024-07-12 08:21:05.997232: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1109] Total bytes in pool: 0 memory_limit_: 35651584 available bytes: 35651584 curr_region_allocation_bytes_: 35651584\r\n2024-07-12 08:21:05.997241: I external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:1114] Stats: \r\nLimit:                        35651584\r\nInUse:                               0\r\nMaxInUse:                            0\r\nNumAllocs:                           0\r\nMaxAllocSize:                        0\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2024-07-12 08:21:05.997248: W external\/local_tsl\/tsl\/framework\/bfc_allocator.cc:494] <allocator contains no memory>\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/tomasz\/github\/rstudio\/keras3\/test.py\", line 16, in <module>\r\n    tf.data.Dataset.from_tensor_slices(x)\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/data\/ops\/dataset_ops.py\", line 826, in from_tensor_slices\r\n    return from_tensor_slices_op._from_tensor_slices(tensors, name)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/data\/ops\/from_tensor_slices_op.py\", line 25, in _from_tensor_slices\r\n    return _TensorSliceDataset(tensors, name=name)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/data\/ops\/from_tensor_slices_op.py\", line 33, in __init__\r\n    element = structure.normalize_element(element)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/data\/util\/structure.py\", line 134, in normalize_element\r\n    ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/profiler\/trace.py\", line 183, in wrapped\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 713, in convert_to_tensor\r\n    return tensor_conversion_registry.convert(\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/tensor_conversion_registry.py\", line 234, in convert\r\n    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/constant_tensor_conversion.py\", line 29, in _constant_tensor_conversion_function\r\n    return constant_op.constant(v, dtype=dtype, name=name)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/ops\/weak_tensor_ops.py\", line 142, in wrapper\r\n    return op(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 276, in constant\r\n    return _constant_impl(value, dtype, shape, name, verify_shape=False,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 289, in _constant_impl\r\n    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 301, in _constant_eager_impl\r\n    t = convert_to_eager_tensor(value, ctx, dtype)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/tomasz\/.virtualenvs\/r-keras\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 108, in convert_to_eager_tensor\r\n    return ops.EagerTensor(value, ctx.device_name, dtype)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from \/job:localhost\/replica:0\/task:0\/device:CPU:0 to \/job:localhost\/replica:0\/task:0\/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:data","2.17"],"created_at":"2024-07-12T12:24:35Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/71744"},{"issue_number":136,"repository":"tensorflow\/tensorflow","title":"Advisory GHSA-84mw-34w6-2q43 contains wrong POC codes","description":"### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\nN\/A\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nN\/A\n\n### Mobile device\n\nN\/A\n\n### Python version\n\nN\/A\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIt seems that description of Gtihub advisory [GHSA-84mw-34w6-2q43](https:\/\/github.com\/advisories\/GHSA-84mw-34w6-2q43) contains POC codes from another advisory [GHSA-772p-x54p-hjrv](https:\/\/github.com\/advisories\/GHSA-772p-x54p-hjrv). This causes issues in understanding the vulnerability.\n\n### Standalone code to reproduce the issue\n\n```shell\nN\/A\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:core"],"created_at":"2024-07-02T07:45:33Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/70724"},{"issue_number":137,"repository":"tensorflow\/tensorflow","title":"tf.matmul gives inconsistent results for same data","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.8.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nwindows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\ncuda 11.8\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI want to do matmul(A,transpose(A)) and matmul(B,transpose(B)). Here, B is just matrix A with additional rows. A is 8 by 512 and b is 64 by 512 with first 8 rows exactly as that of A.\r\n\r\nlet\u2019s call\r\nA_out = A matmul transpose(A), this is 8 by 8,\r\nB_out = B matmul transpose(B), this is 64 by 64.\r\n\r\nSo the above matmul just dots each row of a matrix with all the other rows of the same matrix. So if the first 8 rows in B are same as A then the top-left, 8 by 8 sub matrix of B_out should exactly be same as A_out.\r\n\r\ntf.matmul operation doesnt produce this. however numpy does.\r\n\r\nPlease look into this\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Embedding\r\n\r\n# Set up\r\nembedding = Embedding(36711,512,mask_zero=True)\r\nexample_sequence = tf.constant([36710,  5095,   466, 16678,     5,     3,  5152, 36711] + [0]*(64-8))\r\n\r\npad_embed    = example_sequence # input with padding       # has shape (64,)\r\nno_pad_embed = example_sequence[:8] # input without padding # has shape (8,)\r\n\r\nno_pad_embed = embedding(no_pad_embed) # shape (8,512)  # A\r\npad_embed    = embedding(pad_embed)    # shape (64,512) # B : has same first 8 rows as A\r\n\r\n\r\n# Real problem: I want to use matmul to do A @ A.T and B @ B.T\r\npad_out    = tf.matmul(pad_embed,pad_embed, transpose_b=True)\r\nno_pad_out = tf.matmul(no_pad_embed,no_pad_embed, transpose_b=True)\r\n#print(pad_out[:8,:8])\r\n#print(no_pad_out)\r\n\r\nprint(tf.reduce_all(pad_out[:8,:8]==no_pad_out)) # False, meaning that the upper left 8 by 8 submatrix of pad_out is not equal to no_pad_out\r\n\r\nimport numpy as np\r\n\r\n# Real problem: I want to use matmul to do A @ A.T and B @ B.T\r\npad_embed   = pad_embed.numpy()\r\nno_pad_embed= no_pad_embed.numpy()\r\n\r\npad_out    = np.matmul(pad_embed,pad_embed.T)\r\nno_pad_out = np.matmul(no_pad_embed,no_pad_embed.T)\r\n#print(pad_out[:8,:8])\r\n#print(no_pad_out)\r\n\r\nprint(tf.reduce_all(pad_out[:8,:8]==no_pad_out)) # True, meaning 8 by 8 submatrix of pad_out equals no_pad_out\n```\n\n\n### Relevant log output\n\n```shell\ntf.Tensor(False, shape=(), dtype=bool)\r\ntf.Tensor(True, shape=(), dtype=bool)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.8"],"created_at":"2024-07-01T06:09:26Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/70672"},{"issue_number":138,"repository":"tensorflow\/tensorflow","title":"Kubernetes cluster resolver fails when running from within a K8S cluster.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.16.1\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nlinux\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nIf trying to create a cluster spec from a pod running within a K8s cluster, [this](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.16.1\/tensorflow\/python\/distribute\/cluster_resolver\/kubernetes_cluster_resolver.py#L90) try block fails because it can't find the kubectl config file.\r\n\r\nThe quick is rather straightforward:\r\n\r\n```python\r\n\r\nif override_client is None:\r\n    try:\r\n      from kubernetes import config as k8sconfig  # pylint: disable=g-import-not-at-top\r\n\r\n      k8sconfig.load_kube_config()\r\n    except ImportError:\r\n      if not override_client:\r\n        raise ImportError('The Kubernetes Python client must be installed '\r\n                          'before using the Kubernetes Cluster Resolver. '\r\n                          'To install the Kubernetes Python client, run '\r\n                          '`pip install kubernetes` on your command line.')\r\n\r\n...\r\n\r\n\r\n```\r\n\r\nHappy to open a MR for this.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nmain.py\r\n```python\r\nimport os\r\nimport tensorflow as tf\r\n\r\nfrom absl import logging\r\nfrom kubernetes import client, config\r\n\r\nlogging.set_verbosity(logging.DEBUG)\r\nlogging.info(\"TF version: %s\", tf.__version__)\r\n\r\nconfig.load_incluster_config()\r\nk8s_cli = client.CoreV1Api()\r\n\r\n# Fails here despite providing an override client for talking with the k8s APIs.\r\ncluster_resolver = tf.distribute.cluster_resolver.KubernetesClusterResolver(\r\n    {\"worker\": [\"job-name=mobileye-0\", \"job-name=mobileye-1\"]}, override_client=k8s_cli\r\n)\r\ntask_index = int(os.environ.get(\"TASK_INDEX\"))\r\ncluster_resolver.task_type = \"worker\"\r\ncluster_resolver.task_id = task_index\r\n\r\nlogging.info(\"Cluster spec: %s\", cluster_resolver.cluster_spec().as_dict())\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(\r\n    cluster_resolver=cluster_resolver\r\n)\r\n```\r\n\r\njob.yaml\r\n```yaml\r\napiVersion: batch\/v1\r\nkind: Job\r\nmetadata:\r\n  name: mobileye-0\r\nspec:\r\n  template:\r\n    metadata:\r\n      name: mobileye-training\r\n    spec:\r\n      containers:\r\n      - name: tensorflow\r\n        image: europe-west4-docker.pkg.dev\/msteiner-kubeflow\/mobileye-test\/test-tf-image:latest \r\n        resources:\r\n          limits:\r\n            cpu: \"1\"\r\n            memory: 3Gi\r\n        env:\r\n          - name: TASK_INDEX\r\n            value: \"0\"\r\n      restartPolicy: Never\r\n  parallelism: 1\r\n---\r\napiVersion: batch\/v1\r\nkind: Job\r\nmetadata:\r\n  name: mobileye-1\r\nspec:\r\n  template:\r\n    metadata:\r\n      name: mobileye-training\r\n    spec:\r\n      containers:\r\n      - name: tensorflow\r\n        image: europe-west4-docker.pkg.dev\/msteiner-kubeflow\/mobileye-test\/test-tf-image:latest \r\n        resources:\r\n          limits:\r\n            cpu: \"1\"\r\n            memory: 3Gi\r\n        env:\r\n          - name: TASK_INDEX\r\n            value: \"1\"\r\n      restartPolicy: Never\r\n  parallelism: 1\r\n```\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-06-28 13:23:10.980 CEST\r\n2024-06-28 11:23:10.979832: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-06-28 13:23:10.991 CEST\r\n2024-06-28 11:23:10.991166: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-06-28 13:23:11.109 CEST\r\n2024-06-28 11:23:11.108950: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\n2024-06-28 13:23:11.109 CEST\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-28 13:23:14.057 CEST\r\n2024-06-28 11:23:14.056964: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-06-28 13:23:18.311 CEST\r\nINFO:absl:TF version: 2.16.1\r\n2024-06-28 13:23:18.312 CEST\r\nINFO:absl:PATH: \/usr\/local\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\r\n2024-06-28 13:23:18.312 CEST\r\nINFO:absl:HOSTNAME: mobileye-0-v7lkr\r\n2024-06-28 13:23:18.312 CEST\r\nINFO:absl:LANG: C.UTF-8\r\n2024-06-28 13:23:18.312 CEST\r\nINFO:absl:GPG_KEY: A035C8C19219BA821ECEA86B64E628F8D684696D\r\n2024-06-28 13:23:18.312 CEST\r\nINFO:absl:PYTHON_VERSION: 3.11.8\r\n2024-06-28 13:23:18.312 CEST\r\nINFO:absl:PYTHON_PIP_VERSION: 24.0\r\n2024-06-28 13:23:18.312 CEST\r\nINFO:absl:PYTHON_SETUPTOOLS_VERSION: 65.5.1\r\n2024-06-28 13:23:18.312 CEST\r\nINFO:absl:PYTHON_GET_PIP_URL: https:\/\/github.com\/pypa\/get-pip\/raw\/dbf0c85f76fb6e1ab42aa672ffca6f0a675d9ee4\/public\/get-pip.py\r\n2024-06-28 13:23:18.312 CEST\r\nINFO:absl:PYTHON_GET_PIP_SHA256: dfe9fd5c28dc98b5ac17979a953ea550cec37ae1b47a5116007395bfacff2ab9\r\n2024-06-28 13:23:18.312 CEST\r\nINFO:absl:TASK_INDEX: 0\r\n2024-06-28 13:23:18.313 CEST\r\nINFO:absl:KUBERNETES_SERVICE_PORT: 443\r\n2024-06-28 13:23:18.313 CEST\r\nINFO:absl:KUBERNETES_SERVICE_PORT_HTTPS: 443\r\n2024-06-28 13:23:18.313 CEST\r\nINFO:absl:KUBERNETES_PORT: tcp:\/\/34.118.224.1:443\r\n2024-06-28 13:23:18.313 CEST\r\nINFO:absl:KUBERNETES_PORT_443_TCP: tcp:\/\/34.118.224.1:443\r\n2024-06-28 13:23:18.313 CEST\r\nINFO:absl:KUBERNETES_PORT_443_TCP_PROTO: tcp\r\n2024-06-28 13:23:18.313 CEST\r\nINFO:absl:KUBERNETES_PORT_443_TCP_PORT: 443\r\n2024-06-28 13:23:18.313 CEST\r\nINFO:absl:KUBERNETES_PORT_443_TCP_ADDR: 34.118.224.1\r\n2024-06-28 13:23:18.313 CEST\r\nINFO:absl:KUBERNETES_SERVICE_HOST: 34.118.224.1\r\n2024-06-28 13:23:18.313 CEST\r\nINFO:absl:HOME: \/root\r\n2024-06-28 13:23:18.313 CEST\r\nINFO:absl:TF2_BEHAVIOR: 1\r\n2024-06-28 13:23:18.313 CEST\r\nINFO:absl:TPU_ML_PLATFORM: Tensorflow\r\n2024-06-28 13:23:18.315 CEST\r\nTraceback (most recent call last):   File \"\/\/src\/main.py\", line 20, in <module>     cluster_resolver = tf.distribute.cluster_resolver.KubernetesClusterResolver(\r\n2024-06-28 13:23:18.316 CEST\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-06-28 13:23:18.316 CEST\r\n  File \"\/usr\/local\/lib\/python3.11\/site-packages\/tensorflow\/python\/distribute\/cluster_resolver\/kubernetes_cluster_resolver.py\", line 93, in __init__\r\n2024-06-28 13:23:18.317 CEST\r\n    k8sconfig.load_kube_config()\r\n2024-06-28 13:23:18.317 CEST\r\n  File \"\/usr\/local\/lib\/python3.11\/site-packages\/kubernetes\/config\/kube_config.py\", line 819, in load_kube_config\r\n2024-06-28 13:23:18.318 CEST\r\n    loader = _get_kube_config_loader(\r\n2024-06-28 13:23:18.318 CEST\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^\r\n2024-06-28 13:23:18.318 CEST\r\n  File \"\/usr\/local\/lib\/python3.11\/site-packages\/kubernetes\/config\/kube_config.py\", line 776, in _get_kube_config_loader\r\n2024-06-28 13:23:18.319 CEST\r\n    raise ConfigException(\r\n2024-06-28 13:23:18.319 CEST\r\nkubernetes.config.config_exception.ConfigException: Invalid kube-config file. No configuration found.\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:dist-strat","TF 2.16"],"created_at":"2024-06-28T11:41:32Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/70581"},{"issue_number":139,"repository":"tensorflow\/tensorflow","title":"bucketize -function wrong results on GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18.0-dev20240624\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nBucketize function returns wrong results when executed on GPU.   \r\n\r\nTested with  2.9.3, 2.15.0 and 2.18.0-dev20240624 and observed same incorrect behavior.\r\n\r\nSee the example below.   The CPU and GPU results are different so both can't be correct. GPU result seems to be wrong.\r\n\r\nCan reproduce in [colab](https:\/\/colab.research.google.com\/drive\/1hSHgt5f31eUG-OsAy0FJ0zlcBhMgPV-c?authuser=0#scrollTo=nCDoKVOFw3ML)\r\n\r\n\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.ops.gen_math_ops import bucketize\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\ngpus = tf.config.list_physical_devices('GPU')\r\nassert gpus\r\n\r\nx = [0,1,2,3]\r\nboundaries = [0.1, 1.1]\r\nwith tf.device(\"\/CPU:0\"):\r\n    print(bucketize(x, boundaries=boundaries))\r\nwith tf.device(\"\/GPU:0\"):\r\n    print(bucketize(x, boundaries=boundaries))\n```\n\n\n### Relevant log output\n\n```shell\n2.18.0-dev20240624\r\ntf.Tensor([0 1 2 2], shape=(4,), dtype=int32)\r\ntf.Tensor([1 2 2 2], shape=(4,), dtype=int32)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-06-25T10:15:06Z","comments":5,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/70343"},{"issue_number":140,"repository":"tensorflow\/tensorflow","title":"`SIGSEGV ` (Address boundary error) in `tf.io.gfile` with `TF_USE_MODULAR_FILESYSTEM=1`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.16.1, 2.17.0rc0, 2.18.0.dev20240617\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nmacOS 14.5\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nA `SIGSEGV` (Address boundary error) fault can be caused by trying to call tensorflow gfile API when a `TF_USE_MODULAR_FILESYSTEM=1` is set. This can be reproduced in version 2.16.1 and 2.17.0rc0 and the latest nightly.\n\n### Standalone code to reproduce the issue\n\n```shell\nTF_USE_MODULAR_FILESYSTEM=1 python -c 'import tensorflow as tf;tf.io.gfile.exists(\"gs:\/\/tfds-data\/dataset_info\/mnist\/3.0.1\/dataset_info.json\")'\n```\n\n\n### Relevant log output\n\n```shell\nJob 1, 'TF_USE_MODULAR_FILESYSTEM=1 pyt\u2026' terminated by signal SIGSEGV (Address boundary error)\n```\n","labels":["stat:awaiting tensorflower","type:bug","TF 2.16"],"created_at":"2024-06-20T12:52:32Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/70101"},{"issue_number":141,"repository":"tensorflow\/tensorflow","title":"Segmentation fault in `tf.raw_ops.CollectiveAllToAllV2`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, tf.raw_ops.CollectiveGatherV2 encounters \"Segmentation fault (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntest = tf.Variable([1.0, 2.0, 3.0])\r\n\r\ninput_dict = {\r\n    'input': tf.constant([], shape=(1, 1, 0), dtype=tf.float64),\r\n    'group_size': tf.constant(1, dtype=tf.int32),\r\n    'group_key': tf.constant(30631, dtype=tf.int32), \r\n    'instance_key': tf.constant(2, dtype=tf.int32), \r\n    'ordering_token': [test.handle], \r\n}\r\n\r\n\r\ntf.raw_ops.CollectiveAllToAllV2(\r\n    input=input_dict['input'],\r\n    group_size=input_dict['group_size'],\r\n    group_key=input_dict['group_key'],\r\n    instance_key=input_dict['instance_key'],\r\n    ordering_token=input_dict['ordering_token'],\r\n    communication_hint='auto',\r\n    timeout_seconds=0,\r\n    is_stateless=False,\r\n    name=None\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-06-13 14:37:50.256604: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nSegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-13T14:38:21Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69700"},{"issue_number":142,"repository":"tensorflow\/tensorflow","title":"Segmentation fault in `tf.raw_ops.CollectiveGatherV2`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, tf.raw_ops.CollectiveGatherV2 encounters \"Segmentation fault (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntest = tf.Variable([1.0, 2.0, 3.0])\r\n\r\ninput_dict = {\r\n    'input': tf.constant([], shape=(1, 1, 0), dtype=tf.float64),\r\n    'group_size': tf.constant(1, dtype=tf.int32),\r\n    'group_key': tf.constant(30631, dtype=tf.int32), \r\n    'instance_key': tf.constant(2, dtype=tf.int32), \r\n    'ordering_token': [test.handle], \r\n}\r\n\r\n\r\nresult = tf.raw_ops.CollectiveGatherV2(\r\n    input=input_dict['input'],\r\n    group_size=input_dict['group_size'],\r\n    group_key=input_dict['group_key'],\r\n    instance_key=input_dict['instance_key'],\r\n    ordering_token=input_dict['ordering_token'],\r\n    communication_hint='auto',\r\n    timeout_seconds=0,\r\n    is_stateless=False\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-06-13 14:32:37.690567: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nSegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-13T14:35:21Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69699"},{"issue_number":143,"repository":"tensorflow\/tensorflow","title":"\"invalid static_cast\" on AVX512FP16 (e.g. Sapphire Rapids)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.1\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nRHEL 8\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n6.1.0\n\n### GCC\/compiler version\n\nGCC 13.1\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nFailing build with\r\n\r\n```\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/MathFunctions.h:429:12: error: invalid \u2018static_cast\u2019 from type \u2018const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>\u2019 to type \u2018__vector(16) float\u2019\r\n429 | return static_cast(x);\r\n```\r\n\r\nwhen compiling for CPUs supporting AVX512 FP16 extensions, e.g. Sapphire Rapids\r\n\r\nThis is the same issue as #62047 reported by @OH-AU which was closes by @mihaimaruseac because an unsupported Python version was used which is however unrelated to this failure.\r\n\r\nAfter some analysis it turns out to be an issue in Eigen: https:\/\/gitlab.com\/libeigen\/eigen\/-\/issues\/2829 which might be fixed by https:\/\/gitlab.com\/libeigen\/eigen\/-\/merge_requests\/1639\r\n\r\nSo TF can either use that MR as a patch or update Eigen once the MR is merged\n\n### Standalone code to reproduce the issue\n\n```shell\nTF_PYTHON_VERSION=3.10 CFLAGS=\"-O3 -march=native -fPIC\" CXXFLAGS=$CFLAGS LIBRARY_PATH=$LD_RUN_PATH LD_LIBRARY_PATH=$LD_RUN_PATH \\\r\nLDFLAGS=\"-fPIC  -Wl,--disable-new-dtags -Wl,--rpath -Wl,${LD_RUN_PATH}\" bazel build -j 24 --config=opt -c opt  --copt=-march=native \\\r\n--config=mkl --config=tensorrt  \/\/tensorflow\/tools\/pip_package:build_pip_package --repo_env=TF_PYTHON_VERSION=3.10\n```\n\n\n### Relevant log output\n\n```shell\nIn file included from external\/eigen_archive\/Eigen\/Core:182,\r\n                 from tensorflow\/core\/kernels\/linalg\/matrix_inverse_op.cc:22:\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/MathFunctions.h: In instantiation of 'static NewType Eigen::internal::cast_impl<OldType, NewType, EnableIf>::run(const OldType&) [with OldType = Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>; NewType = __vector(16) float; EnableIf = void]':\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/GenericPacketMath.h:268:48:   required from 'static TgtPacket Eigen::internal::pcast_generic<SrcPacket, TgtPacket, false, false>::run(const SrcPacket&) [with SrcPacket = Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>; TgtPacket = __vector(16) float]'\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/GenericPacketMath.h:289:50:   required from 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = eigen_packet_wrapper<__vector(4) long long int, 1>; TgtPacket = __vector(16) float]'\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/CoreEvaluators.h:789:52:   required from 'DstPacketType Eigen::internal::unary_evaluator<Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<SrcType, DstType>, ArgType>, Eigen::internal::IndexBased>::packet(Eigen::Index) const [with int LoadMode = 0; DstPacketType = __vector(16) float; typename std::enable_if<Eigen::internal::find_packet_by_size<SrcType, Eigen::internal::unpacket_traits<DstPacketType>::size>::value, bool>::type <anonymous> = true; SrcType = Eigen::half; DstType = float; ArgType = const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; typename Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<SrcType, DstType>, ArgType>::Scalar = float; Eigen::Index = long int]'\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/AssignEvaluator.h:706:110:   required from 'void Eigen::internal::generic_dense_assignment_kernel<DstEvaluatorTypeT, SrcEvaluatorTypeT, Functor, Version>::assignPacket(Eigen::Index) [with int StoreMode = 64; int LoadMode = 0; Packet = __vector(16) float; DstEvaluatorTypeT = Eigen::internal::evaluator<Eigen::Matrix<float, -1, -1, 1, -1, -1> >; SrcEvaluatorTypeT = Eigen::internal::evaluator<Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > > >; Functor = Eigen::internal::assign_op<float, float>; int Version = 0; Eigen::Index = long int]'\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/AssignEvaluator.h:462:75:   required from 'static constexpr void Eigen::internal::dense_assignment_loop<Kernel, 3, 0>::run(Kernel&) [with Kernel = Eigen::internal::generic_dense_assignment_kernel<Eigen::internal::evaluator<Eigen::Matrix<float, -1, -1, 1, -1, -1> >, Eigen::internal::evaluator<Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > > >, Eigen::internal::assign_op<float, float>, 0>]'\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/AssignEvaluator.h:810:37:   [ skipping 3 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/AssignEvaluator.h:883:27:   required from 'void Eigen::internal::call_assignment(Dst&, const Src&, const Func&, std::enable_if_t<(! evaluator_assume_aliasing<Src>::value), void*>) [with Dst = Eigen::Matrix<float, -1, -1, 1, -1, -1>; Src = Eigen::CwiseUnaryOp<core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Func = assign_op<float, float>; std::enable_if_t<(! evaluator_assume_aliasing<Src>::value), void*> = void*; typename evaluator_traits<SrcXprType>::Shape = Eigen::DenseShape]'\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/AssignEvaluator.h:861:18:   required from 'void Eigen::internal::call_assignment(Dst&, const Src&) [with Dst = Eigen::Matrix<float, -1, -1, 1, -1, -1>; Src = Eigen::CwiseUnaryOp<core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >]'\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/PlainObjectBase.h:771:32:   required from 'Derived& Eigen::PlainObjectBase<Derived>::_set(const Eigen::DenseBase<OtherDerived>&) [with OtherDerived = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Derived = Eigen::Matrix<float, -1, -1, 1, -1, -1>]'\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/Matrix.h:227:24:   required from 'Eigen::Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>& Eigen::Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>::operator=(const Eigen::DenseBase<OtherDerived>&) [with OtherDerived = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Scalar_ = float; int Rows_ = -1; int Cols_ = -1; int Options_ = 1; int MaxRows_ = -1; int MaxCols_ = -1]'\r\nexternal\/eigen_archive\/Eigen\/src\/LU\/PartialPivLU.h:135:12:   required from 'Eigen::PartialPivLU<MatrixType, PermutationIndex>& Eigen::PartialPivLU<MatrixType, PermutationIndex>::compute(const Eigen::EigenBase<OtherDerived>&) [with InputType = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; MatrixType_ = Eigen::Matrix<float, -1, -1, 1, -1, -1>; PermutationIndex_ = int]'\r\ntensorflow\/core\/kernels\/linalg\/matrix_inverse_op.cc:113:31:   required from here\r\nexternal\/eigen_archive\/Eigen\/src\/Core\/MathFunctions.h:429:12: error: invalid 'static_cast' from type 'const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>' to type '__vector(16) float'\r\n  429 |     return static_cast<NewType>(x);\r\n      |            ^~~~~~~~~~~~~~~~~~~~~~~\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","comp:core","TF 2.15"],"created_at":"2024-06-13T07:23:50Z","comments":7,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69674"},{"issue_number":144,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.experimental.numpy.diag\/tf.compat.v1.linalg.diag\/tf.experimental.numpy.diagflat\/tf.keras.ops.diag`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, tf.experimental.numpy.diag\/tf.compat.v1.linalg.diag\/tf.experimental.numpy.diagflat\/tf.keras.ops.diag encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput_dict = {\r\n    'diagonal': tf.constant([1,2], dtype=tf.int32),\r\n    'k': tf.constant([1, 2], dtype=tf.int32)\r\n}\r\n\r\n# crash\r\ntf.experimental.numpy.diag(\r\n    v=tf.constant([1,2], dtype=tf.int32), k=tf.constant([1, 2], dtype=tf.int32)\r\n)\r\n\r\n# crash\r\n# tf.compat.v1.linalg.diag(\r\n#     diagonal=tf.constant([1,2], dtype=tf.int32),\r\n#     name='diag',\r\n#     k= tf.constant([1, 2], dtype=tf.int32),\r\n#     num_rows=-1,\r\n#     num_cols=-1,\r\n#     padding_value=0,\r\n#     align='RIGHT_LEFT'\r\n# )\r\n\r\n# crash\r\n# tf.experimental.numpy.diagflat(\r\n#     v=tf.constant([1,2], dtype=tf.int32), k=tf.constant([1, 2], dtype=tf.int32)\r\n# )\r\n\r\n# crash\r\n# tf.keras.ops.diag(\r\n#     x=tf.constant([1,2], dtype=tf.int32), k=tf.constant([1, 2], dtype=tf.int32)\r\n# )\n```\n\n\n### Relevant log output\n\n```shell\n2024-06-10 13:23:09.021306: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-10 13:23:10.029041: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-10T13:24:51Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69471"},{"issue_number":145,"repository":"tensorflow\/tensorflow","title":"Crash in `tf.raw_ops.SparseCountSparseOutput `","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nTensorFlow Nightly\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific input, tf.raw_ops.SparseCountSparseOutput will output \"The session crashed because it took up all available RAM.\" \n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nbinary_output = True\r\nindices = tf.constant(0, shape=[3456,2], dtype=tf.int64)\r\nvalues = tf.constant(536870912, shape=[3456], dtype=tf.int32)\r\ndense_shape = tf.constant([125099989676412,125099989676412], shape=[2], dtype=tf.int64)\r\nweights = tf.constant(51, shape=[3456], dtype=tf.int32)\r\n\r\n\r\ntf.raw_ops.SparseCountSparseOutput(\r\n   indices=indices, values=values, dense_shape=dense_shape, weights=weights, binary_output=binary_output,\r\n    minlength=0,\r\n    maxlength=0,\r\n    name=None\r\n)\n```\n\n\n### Relevant log output\n\n```shell\nJun 10, 2024, 11:31:57\u202fAM\tWARNING\tWARNING:root:kernel 7c5d2f95-9ab7-4b7c-99c0-d8949477c9f1 restarted\r\nJun 10, 2024, 11:31:57\u202fAM\tINFO\tKernelRestarter: restarting kernel (1\/5), keep random ports\r\nJun 10, 2024, 11:31:29\u202fAM\tWARNING\t2024-06-10 03:31:29.145870: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nJun 10, 2024, 11:31:26\u202fAM\tWARNING\tTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nJun 10, 2024, 11:31:26\u202fAM\tWARNING\t2024-06-10 03:31:26.481147: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nJun 10, 2024, 11:31:26\u202fAM\tWARNING\t2024-06-10 03:31:26.469832: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\nJun 10, 2024, 11:31:26\u202fAM\tWARNING\t2024-06-10 03:31:26.467591: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nJun 10, 2024, 11:31:26\u202fAM\tWARNING\t2024-06-10 03:31:26.467524: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nJun 10, 2024, 11:31:18\u202fAM\tINFO\tKernel started: 7c5d2f95-9ab7-4b7c-99c0-d8949477c9f1, name: python3\r\nJun 10, 2024, 11:30:19\u202fAM\tINFO\tUse Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-10T03:32:42Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69455"},{"issue_number":146,"repository":"tensorflow\/tensorflow","title":"GPU MaxPool gradient ops do not yet have a deterministic XLA implementation","description":"### Issue type\n\nFeature Request\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.19\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.4\/8.9.7.29\n\n### GPU model and memory\n\nNVIDIA GeForce RTX 3090\n\n### Current behavior?\n\nWhen TF deterministic was set, runtime exception was thrown at MaxPooling2D().\n\n### Standalone code to reproduce the issue\n\n```shell\nWhen TF deterministic was set, runtime exception was thrown at MaxPooling2D().\n```\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\r\n  File \"\/home\/ws\/miniconda3\/envs\/tf216\/lib\/python3.9\/site-packages\/IPython\/core\/interactiveshell.py\", line 3526, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-2-3dda39ff370e>\", line 1, in <module>\r\n    runfile('\/mnt\/projects\/Projects\/Test_Classification\/train_model.py', wdir='\/mnt\/projects\/Projects\/Test_Classification')\r\n  File \"\/opt\/pycharm-community-2024.1\/plugins\/python-ce\/helpers\/pydev\/_pydev_bundle\/pydev_umd.py\", line 197, in runfile\r\n    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script\r\n  File \"\/opt\/pycharm-community-2024.1\/plugins\/python-ce\/helpers\/pydev\/_pydev_imps\/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"\/mnt\/projects\/Projects\/Test_Classification\/train_model.py\", line 956, in <module>\r\n    history = model.fit(x_train, y_train,\r\n  File \"\/home\/ws\/miniconda3\/envs\/tf216\/lib\/python3.9\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 122, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/home\/ws\/miniconda3\/envs\/tf216\/lib\/python3.9\/site-packages\/tensorflow\/python\/eager\/execute.py\", line 53, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:\r\nDetected at node gradient_tape\/functional_1_1\/max_pooling2d_4_1\/MaxPool2d\/MaxPoolGrad defined at (most recent call last):\r\n<stack traces unavailable>\r\nGPU MaxPool gradient ops do not yet have a deterministic XLA implementation.\r\n\t [[{{node gradient_tape\/functional_1_1\/max_pooling2d_4_1\/MaxPool2d\/MaxPoolGrad}}]]\r\n\ttf2xla conversion failed while converting __inference_one_step_on_data_13588[]. Run with TF_DUMP_GRAPH_PREFIX=\/path\/to\/dump\/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.\r\n\t [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_14045]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-08T06:18:41Z","comments":22,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69417"},{"issue_number":147,"repository":"tensorflow\/tensorflow","title":"Crash in `tf.raw_ops.ResizeNearestNeighbor\/ResizeNearestNeighborGrad\/ResizeArea\/ResizeBicubic\/ResizeBilinear`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nTensorFlow Nightly\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific input, these APIs will output \"The session crashed because it took up all available RAM.\" The affected APIs are listed below:\r\n\r\n1. tf.raw_ops.ResizeBilinear\r\n2. tf.raw_ops.ResizeBicubic\r\n3. tf.raw_ops.ResizeArea\r\n4. tf.raw_ops.ResizeNearestNeighbor\r\n5. tf.raw_ops.ResizeNearestNeighborGrad\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1a6B_lYMfENTbO4ifQMLYe8Hq6CHcc6hP?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nTimestamp\tLevel\tMessage\r\nJun 6, 2024, 6:57:43\u202fPM\tWARNING\tWARNING:root:kernel d0444564-3fb8-4d42-ac83-1338e6842d5a restarted\r\nJun 6, 2024, 6:57:43\u202fPM\tINFO\tKernelRestarter: restarting kernel (1\/5), keep random ports\r\nJun 6, 2024, 6:57:25\u202fPM\tWARNING\t2024-06-06 10:57:25.421101: W external\/local_tsl\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 51536461872 exceeds 10% of free system memory.\r\nJun 6, 2024, 6:57:21\u202fPM\tWARNING\t2024-06-06 10:57:21.507228: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nJun 6, 2024, 6:57:17\u202fPM\tWARNING\tTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nJun 6, 2024, 6:57:17\u202fPM\tWARNING\t2024-06-06 10:57:17.995922: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nJun 6, 2024, 6:56:13\u202fPM\tWARNING\tWARNING:root:kernel d0444564-3fb8-4d42-ac83-1338e6842d5a restarted\r\nJun 6, 2024, 6:56:13\u202fPM\tINFO\tKernelRestarter: restarting kernel (1\/5), keep random ports\r\nJun 6, 2024, 6:55:56\u202fPM\tWARNING\t2024-06-06 10:55:56.764126: W external\/local_tsl\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 103072923744 exceeds 10% of free system memory.\r\nJun 6, 2024, 6:55:52\u202fPM\tWARNING\t2024-06-06 10:55:52.277507: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-06T14:30:10Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69322"},{"issue_number":148,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.ResourceApplyCenteredRMSProp\/tf.raw_ops.ResourceSparseApplyCenteredRMSProp`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific input, tf.raw_ops.ResourceApplyCenteredRMSProp\/tf.raw_ops.ResourceSparseApplyCenteredRMSProp triggers \"Aborted (core dumped)\". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nvar = tf.Variable([1.0, 2.0, 3.0])\r\naccum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) \r\naccum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) \r\nlr = 0.01\r\ngrad = tf.constant([0.1, 0.2, 0.3])\r\nmomentum = 0.9\r\n\r\n# crash\r\ntf.raw_ops.ResourceSparseApplyCenteredRMSProp(var=var.handle, mg=accum.handle, ms=accum2.handle, mom=var.handle,\r\n                                              lr=lr, rho=lr,\r\n                                              momentum=momentum,\r\n                                              epsilon=0.9, grad=tf.constant([0.1, 0.2, 0.3]),\r\n                                              indices=tf.constant([1,2,2]),\r\n                                              use_locking=False)\r\n# crash\r\n# tf.raw_ops.ResourceApplyCenteredRMSProp(var=var.handle, mg=accum.handle, ms=accum2.handle, mom=var.handle,\r\n#                                               lr=lr, rho=lr,\r\n#                                               momentum=momentum,\r\n#                                               epsilon=0.9, grad=tf.constant([0.1, 0.2, 0.3]),\r\n#                                               use_locking=False)\n```\n\n\n### Relevant log output\n\n```shell\n2024-06-06 01:23:02.820585: I tensorflow\/core\/util\/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-06-06 01:23:02.853830: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-06 01:23:03.911976: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-06T01:24:46Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69286"},{"issue_number":149,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.ResourceApplyAdagrad\/tf.raw_ops.ResourceApplyAdagradDA\/tf.raw_ops.ResourceApplyAdagradV2`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.16\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04.3 LTS (x86_64)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nOn specific input, tf.raw_ops.ResourceApplyAdagrad\/tf.raw_ops.ResourceApplyAdagradDA\/tf.raw_ops.ResourceApplyAdagradV2 triggers \"Aborted (core dumped)\". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\nvar = tf.Variable([1.0, 2.0, 3.0])\r\naccum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) \r\naccum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) \r\nlr = 0.01\r\ngrad = tf.constant([0.1, 0.2, 0.3])\r\nmomentum = 0.9\r\n\r\n# crash\r\ntf.raw_ops.ResourceApplyAdagrad(\r\n    var=var.handle,\r\n    accum=accum.handle,\r\n    lr=lr,\r\n    grad=grad,\r\n    use_locking=False,\r\n)\r\n\r\n# crash\r\n# tf.raw_ops.ResourceApplyAdagradDA(var=var.handle,\r\n#               gradient_accumulator=accum.handle,\r\n#               gradient_squared_accumulator=accum2.handle,\r\n#               grad=grad,  lr=lr,\r\n#               l1=lr, l2=lr, global_step=1000,\r\n#               use_locking=False)\r\n\r\n# crash\r\n# tf.raw_ops.ResourceApplyAdagradV2(var=var.handle,\r\n#                 accum=accum.handle,\r\n#                 epsilon = 0.9,\r\n#                 grad=grad, lr=lr,\r\n#                 use_locking=False,update_slots=False)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-06-06 01:18:44.797518: I tensorflow\/core\/util\/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-06-06 01:18:44.829812: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-06 01:18:45.875807: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-06T01:19:37Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69285"},{"issue_number":150,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.ResourceSparseApplyAdagrad\/tf.raw_ops.ResourceSparseApplyAdagradDA\/tf.raw_ops.ResourceSparseApplyAdagradV2`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific input, tf.raw_ops.ResourceSparseApplyAdagrad\/tf.raw_ops.ResourceSparseApplyAdagradDA\/tf.raw_ops.ResourceSparseApplyAdagradV2 triggers \"Aborted (core dumped)\". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nvar = tf.Variable([1.0, 2.0, 3.0])\r\naccum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) \r\naccum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) \r\nlr = 0.01\r\ngrad = tf.constant([0.1, 0.2, 0.3])\r\nmomentum = 0.9\r\n\r\n# crash\r\ntf.raw_ops.ResourceSparseApplyAdagrad(\r\n    var=var.handle,\r\n    accum=accum.handle,\r\n    lr=lr,\r\n    grad=grad,\r\n    indices=tf.constant([1,2,3]),\r\n    use_locking=False,\r\n)\r\n\r\n\r\n# crash only when gpu is available \r\n# tf.raw_ops.ResourceSparseApplyAdagradDA(var=var.handle,\r\n#                     gradient_accumulator=accum.handle,\r\n#                     gradient_squared_accumulator=accum2.handle,\r\n#                     grad=grad, indices=tf.constant([1,2,3]), lr=lr,\r\n#                     l1=lr, l2=lr, global_step=1000,\r\n#                     use_locking=False)\r\n\r\n# crash\r\n# tf.raw_ops.ResourceSparseApplyAdagradV2(var=var.handle,\r\n#                     accum=accum.handle,\r\n#                     epsilon = 0.9,\r\n#                     grad=grad, indices=tf.constant([1,2,3]), lr=lr,\r\n#                     use_locking=False,update_slots=False)\n```\n\n\n### Relevant log output\n\n```shell\n2024-06-06 01:14:57.455749: I tensorflow\/core\/util\/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-06-06 01:14:57.489757: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-06 01:14:58.552846: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-06T01:15:35Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69284"},{"issue_number":151,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.ResourceSparseApplyAdadelta\/tf.raw_ops.ResourceApplyAdadelta`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific input, tf.raw_ops.ResourceSparseApplyAdadelta\/tf.raw_ops.ResourceApplyAdadelta triggers \"Aborted (core dumped)\". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nvar = tf.Variable([1.0, 2.0, 3.0])\r\naccum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) \r\naccum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) \r\nlr = 0.01\r\ngrad = tf.constant([0.1, 0.2, 0.3])\r\nmomentum = 0.9\r\n\r\n# crash\r\ntf.raw_ops.ResourceSparseApplyAdadelta(\r\n    var=var.handle,\r\n    accum=accum.handle,\r\n    accum_update=accum2.handle,\r\n    lr=lr,\r\n    rho=0.9,\r\n    epsilon=0.9,\r\n    grad=grad,\r\n    indices=tf.constant([1,2,2]),\r\n    use_locking=False,\r\n)\r\n# crash\r\n# tf.raw_ops.ResourceApplyAdadelta(\r\n#     var=var.handle,\r\n#     accum=accum.handle,\r\n#     accum_update=accum2.handle,\r\n#     lr=lr,\r\n#     rho=0.9,\r\n#     epsilon=0.9,\r\n#     grad=grad,\r\n#     use_locking=False,\r\n# )\n```\n\n\n### Relevant log output\n\n```shell\n2024-06-06 01:10:09.399119: I tensorflow\/core\/util\/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-06-06 01:10:09.432483: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-06 01:10:10.501032: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-06T01:10:53Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69283"},{"issue_number":152,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.ResourceApplyRMSProp\/tf.raw_ops.ResourceSparseApplyRMSProp`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific input, tf.raw_ops.ResourceApplyRMSProp\/tf.raw_ops.ResourceSparseApplyRMSProp triggers \"Aborted (core dumped)\". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nvar = tf.Variable([1.0, 2.0, 3.0])\r\naccum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) \r\naccum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) \r\nlr = 0.01\r\ngrad = tf.constant([0.1, 0.2, 0.3])\r\nmomentum = 0.9\r\n\r\n# crash\r\ntf.raw_ops.ResourceApplyRMSProp(\r\n    var=var.handle,\r\n    ms=accum.handle,\r\n    mom=accum2.handle,\r\n    lr=lr,\r\n    rho=lr,\r\n    momentum=momentum,\r\n    epsilon=0.9,\r\n    grad=grad,\r\n    use_locking=False,\r\n)\r\n\r\n# tf.raw_ops.ResourceSparseApplyRMSProp(\r\n#     var=var.handle,\r\n#     ms=accum.handle,\r\n#     mom=accum2.handle,\r\n#     lr=lr,\r\n#     rho=lr,\r\n#     momentum=momentum,\r\n#     indices=tf.constant([1,2,2]),\r\n#     epsilon=0.9,\r\n#     grad=grad,\r\n#     use_locking=False,\r\n# )\n```\n\n\n### Relevant log output\n\n```shell\n2024-06-06 01:07:00.757682: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-06 01:07:01.835541: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-06T01:07:46Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69281"},{"issue_number":153,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.ResourceApplyKerasMomentum\/tf.raw_ops.ResourceSparseApplyKerasMomentum`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific input, tf.raw_ops.ResourceApplyKerasMomentum\/tf.raw_ops.ResourceSparseApplyKerasMomentum triggers \"Aborted (core dumped)\". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. We analyzed it, and the cause of this crash is probably the grad parameter.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nvar = tf.Variable([1.0, 2.0, 3.0])\r\naccum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) \r\naccum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) \r\nlr = 0.01\r\ngrad = tf.constant([0.1, 0.2, 0.3])\r\nmomentum = 0.9\r\n\r\n# crash\r\ntf.raw_ops.ResourceApplyKerasMomentum(\r\n    var=var.handle,\r\n    accum=accum.handle,\r\n    grad=grad,\r\n    lr=lr,\r\n    momentum=0.1,\r\n    use_locking=False,\r\n    use_nesterov=False,\r\n    name=None\r\n)\r\n\r\n# crash\r\n# tf.raw_ops.ResourceSparseApplyKerasMomentum(\r\n#     var=var.handle,\r\n#     accum=accum.handle,\r\n#     grad=grad,\r\n#     lr=lr,\r\n#     indices=tf.constant([1,2,2]),\r\n#     momentum=0.1,\r\n#     use_locking=False,\r\n#     use_nesterov=False,\r\n#     name=None\r\n# )\n```\n\n\n### Relevant log output\n\n```shell\n2024-06-06 00:59:43.994008: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-06 00:59:44.992853: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-06T01:01:58Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69279"},{"issue_number":154,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.ResourceApplyFtrl\/tf.raw_ops.ResourceApplyFtrlV2\/tf.raw_ops.ResourceSparseApplyFtrl\/tf.raw_ops.ResourceSparseApplyFtrlV2`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific input, tf.raw_ops.ResourceApplyFtrl\/tf.raw_ops.ResourceApplyFtrlV2\/tf.raw_ops.ResourceSparseApplyFtrl\/tf.raw_ops.ResourceSparseApplyFtrlV2 triggers \"Aborted (core dumped)\". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. We analyzed it, and the cause of this crash is probably the grad parameter.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nvar = tf.Variable([1.0, 2.0, 3.0])\r\naccum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) \r\naccum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) \r\nlr = 0.01\r\ngrad = tf.constant([0.1, 0.2, 0.3])\r\nmomentum = 0.9\r\n\r\n# crash\r\ntf.raw_ops.ResourceApplyFtrl(\r\n    var=var.handle,\r\n    accum=accum.handle,\r\n    linear=var.handle,\r\n    grad=grad,\r\n    lr=lr,\r\n    l1=0.1,\r\n    l2=0.1,\r\n    lr_power=1,\r\n    use_locking=False,\r\n    multiply_linear_by_lr=False,\r\n    name=None\r\n)\r\n# crash\r\n# tf.raw_ops.ResourceApplyFtrlV2(\r\n#     var=var.handle,\r\n#     accum=accum.handle,\r\n#     linear=var.handle,\r\n#     grad=grad,\r\n#     lr=lr,\r\n#     l1=0.1,\r\n#     l2=0.1,\r\n#     lr_power=1,\r\n#     l2_shrinkage=1,\r\n#     use_locking=False,\r\n#     multiply_linear_by_lr=False,\r\n#     name=None\r\n# )\r\n# crash\r\n# tf.raw_ops.ResourceSparseApplyFtrl(\r\n#    var=var.handle,\r\n#     accum=accum.handle,\r\n#     linear=var.handle,\r\n#     grad=grad,\r\n#     lr=lr,\r\n#     l1=0.1,\r\n#     l2=0.1,\r\n#     lr_power=-1,\r\n#     use_locking=False,\r\n#     indices=tf.constant([1,2,2]),\r\n#     multiply_linear_by_lr=False,\r\n#     name=None\r\n# )\r\n# crash\r\n# tf.raw_ops.ResourceSparseApplyFtrlV2(\r\n#     var=var.handle,\r\n#     accum=accum.handle,\r\n#     linear=var.handle,\r\n#     grad=grad,\r\n#     lr=lr,\r\n#     l1=0.1,\r\n#     l2=0.1,\r\n#     lr_power=-1,\r\n#     use_locking=False,\r\n#     indices=tf.constant([1,2,2]),\r\n#     multiply_linear_by_lr=False,\r\n#     l2_shrinkage=1,\r\n#     name=None\r\n# )\n```\n\n\n### Relevant log output\n\n```shell\n2024-06-06 00:53:09.467458: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-06 00:53:10.440397: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-06T00:53:19Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69278"},{"issue_number":155,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.QuantizeAndDequantizeV3`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nA crash is triggered when an unintended value is passed to the input parameter of the tf.raw_ops.QuantizeAndDequantizeV3 function.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput_data = tf.random.uniform(shape=[2, 1], maxval=9.0)\r\ninput_min = tf.constant([0, 1.0], dtype=tf.dtypes.float32)\r\ninput_max = tf.constant([1.0], dtype=tf.dtypes.float32)\r\n# crash\r\ntf.raw_ops.QuantizeAndDequantizeV3(input=input_data, input_min=input_min, input_max=input_max, num_bits=8, signed_input=True, range_given=True, narrow_range=False, axis=(- 1))\n```\n\n\n### Relevant log output\n\n```shell\n2024-06-05 10:39:35.285635: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-05 10:39:36.361982: F tensorflow\/core\/framework\/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-05T10:42:09Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69220"},{"issue_number":156,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.transpose`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nA crash is triggered when some boundary values are passed to the perm parameter of these transpose functions. the affected APIs are as follows:\r\n\r\n1. tf.transpose\r\n2. tf.experimental.numpy.transpose\r\n3. tf.compat.v1.transpose\r\n4. tf.keras.ops.transpose\r\n5. tf.raw_ops.transpose\r\n6. tf.raw_ops.ConjugateTranspose\r\n\r\ntf.raw_ops.ConjugateTranspose is a Segmentation fault crash, all other API crashes are Aborted. \n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.random.normal(shape=(3, 3))\r\n# crash\r\nx = tf.transpose(x, perm=((- 1), (- 2))) \r\n \r\n# crash \r\n# tf.experimental.numpy.transpose(     \r\n#     x, axes=((- 1), (- 2))\r\n# )\r\n\r\n# crash \r\n# tf.compat.v1.transpose(  \r\n#     x, perm=((- 1), (- 2)), name='transpose', conjugate=False\r\n# )\r\n\r\n# crash\r\n# tf.keras.ops.transpose(\r\n#     x, axes=((- 1), (- 2))\r\n# )\r\n\r\n# crash\r\n# tf.raw_ops.ConjugateTranspose(\r\n#     x=x, perm=((- 1), (- 2))\r\n# )\r\n\r\n# crash\r\n# tf.raw_ops.Transpose(\r\n#     x=x, perm=((- 1), (- 2))\r\n# )\n```\n\n\n### Relevant log output\n\n```shell\n2024-06-05 09:16:55.482645: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-06-05 09:16:56.526139: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\r\n\r\n2024-06-05 09:17:47.967585: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nSegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-06-05T09:19:56Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/69213"},{"issue_number":157,"repository":"tensorflow\/tensorflow","title":"segmentation fault when tf.histogram_fixed_width receives large `value_range` and `nbins` on CPU mode ","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen setting the `nbins` to a large value and the value range to [-np.inf, np.inf], the API `tf.histogram_fixed_width` will raises a segmentation fault.\r\nInterestingly, I found this issue only occurs on CPU backend while this API works fine on GPU.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\r\nimport numpy as np\r\nimport tensorflow as tf\r\ninput = tf.constant([1.,2,3])\r\nbins = 23\r\nout = tf.histogram_fixed_width(input, [-np.inf, np.inf], nbins=bins)  # Segmentation fault (core dumped)\r\nprint(out)\r\n```\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-29T19:33:25Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/68836"},{"issue_number":158,"repository":"tensorflow\/tensorflow","title":"Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence when call some methods of `tf.data`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.3\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen calling and iterating over the results of some of tf.data's methods it outputs \"Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ndef dataset_test():\r\n \r\n    dataset = tf.data.Dataset.from_tensor_slices([\"apple\", \"banana\", \"cherry\"])\r\n    # dataset = tf.data.TextLineDataset([\"file1.txt\", \"file2.txt\"]).range(10)             # same output\r\n    # dataset = tf.data.TFRecordDataset([\"file1.tfrecords\", \"file2.tfrecords\"]).range(10) # same output\r\n    # dataset = tf.data.Dataset.from_tensors([1,2,3]).range(10)                           # same output\r\n\r\n    print(list(dataset.as_numpy_iterator()))\r\n\r\nif __name__ == \"__main__\":\r\n    dataset_test()\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-24 06:23:06.268182: I tensorflow\/core\/util\/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-05-24 06:23:06.309884: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-05-24 06:23:06.908161: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-05-24 06:23:07.558225: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1928] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 22137 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:18:00.0, compute capability: 8.6\r\n2024-05-24 06:23:07.558795: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1928] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 22453 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:af:00.0, compute capability: 8.6\r\n2024-05-24 06:23:07.917142: W tensorflow\/core\/framework\/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.16"],"created_at":"2024-05-24T11:24:11Z","comments":21,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/68593"},{"issue_number":159,"repository":"tensorflow\/tensorflow","title":"TFLite ConvTranspose3D implemented typo","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nin tflite convtranspose3d optimized kernel, pad depth seems typo error\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/30307463\/39bdf859-f236-48f9-b02a-c8231d11cb88)\r\n\r\nAnd also compared with same convtransposed3d with torch interface, tflite have mismatch\n\n### Standalone code to reproduce the issue\n\n```shell\nin tflite convtranspose3d optimized kernel, pad depth seems typo error\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/30307463\/39bdf859-f236-48f9-b02a-c8231d11cb88)\r\n\r\nhere, seems \r\nconst int spatial_dim_1_padding_after =\r\n      params.padding_values.depth + params.padding_values.depth_offset;\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","comp:lite","awaiting PR merge","TF 2.16"],"created_at":"2024-05-21T07:27:49Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/68319"},{"issue_number":160,"repository":"tensorflow\/tensorflow","title":"ValueError: as_list() is not defined on an unknown TensorShape. during training","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.0-dev20240517\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nmacOS Sonoma \n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI get the following error when training a transformer for translation:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[9], [line 6](vscode-notebook-cell:?execution_count=9&line=6)\r\n      [1](vscode-notebook-cell:?execution_count=9&line=1) model.compile(optimizer='adam', \r\n      [2](vscode-notebook-cell:?execution_count=9&line=2)     loss='sparse_categorical_crossentropy', \r\n      [3](vscode-notebook-cell:?execution_count=9&line=3)     metrics=['accuracy'],\r\n      [4](vscode-notebook-cell:?execution_count=9&line=4)     run_eagerly=False\r\n      [5](vscode-notebook-cell:?execution_count=9&line=5)     )\r\n----> [6](vscode-notebook-cell:?execution_count=9&line=6) model.fit(dataset_train, epochs=10)\r\n\r\nFile ~\/opt\/anaconda3\/envs\/tensorflow_latest\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n    [119](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/glo\/Desktop\/deep_learning_training-main\/~\/opt\/anaconda3\/envs\/tensorflow_latest\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py:119)     filtered_tb = _process_traceback_frames(e.__traceback__)\r\n    [120](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/glo\/Desktop\/deep_learning_training-main\/~\/opt\/anaconda3\/envs\/tensorflow_latest\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py:120)     # To get the full stack trace, call:\r\n    [121](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/glo\/Desktop\/deep_learning_training-main\/~\/opt\/anaconda3\/envs\/tensorflow_latest\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py:121)     # `keras.config.disable_traceback_filtering()`\r\n--> [122](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/glo\/Desktop\/deep_learning_training-main\/~\/opt\/anaconda3\/envs\/tensorflow_latest\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py:122)     raise e.with_traceback(filtered_tb) from None\r\n    [123](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/glo\/Desktop\/deep_learning_training-main\/~\/opt\/anaconda3\/envs\/tensorflow_latest\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py:123) finally:\r\n    [124](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/glo\/Desktop\/deep_learning_training-main\/~\/opt\/anaconda3\/envs\/tensorflow_latest\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py:124)     del filtered_tb\r\n\r\nFile ~\/opt\/anaconda3\/envs\/tensorflow_latest\/lib\/python3.10\/site-packages\/optree\/ops.py:594, in tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests)\r\n    [592](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/glo\/Desktop\/deep_learning_training-main\/~\/opt\/anaconda3\/envs\/tensorflow_latest\/lib\/python3.10\/site-packages\/optree\/ops.py:592) leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)\r\n    [593](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/glo\/Desktop\/deep_learning_training-main\/~\/opt\/anaconda3\/envs\/tensorflow_latest\/lib\/python3.10\/site-packages\/optree\/ops.py:593) flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]\r\n--> [594](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/glo\/Desktop\/deep_learning_training-main\/~\/opt\/anaconda3\/envs\/tensorflow_latest\/lib\/python3.10\/site-packages\/optree\/ops.py:594) return treespec.unflatten(map(func, *flat_args))\r\n\r\nValueError: as_list() is not defined on an unknown TensorShape.\r\n```\n\n### Standalone code to reproduce the issue\n\n```shell\nHere is a colab link to the code:\r\nhttps:\/\/colab.research.google.com\/drive\/1BQ4lhaZPP5XGb_IUe-rVacMiFPWCfm9Y?usp=sharing\r\n\r\nRunning eagerly works fine but an error occurs in graph mode.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-18T15:26:10Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/68217"},{"issue_number":161,"repository":"tensorflow\/tensorflow","title":"Strange finding: When the global seed and @tf.function decorator are used, the random sampling values of the two adjacent periods are equal","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWindows 10\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen I set the global seed, applied the @tf.function decorator, and performed gradient updates every two periods, I observed an unexpected phenomenon: the error values were randomly sampled from two consecutive periods with identical values. However, my expectation was that the error values should differ when sampled in each period instead of being equal in adjacent periods. Furthermore, I noticed that removing the @tf.function decorator from the model function prevented this issue of having identical error values in consecutive periods. What could be causing this phenomenon? How should one handle this situation when using @tf.function?\"\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os\r\nimport random\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# set seeds\r\nSEED = 0\r\nrandom.seed(SEED)\r\nnp.random.seed(SEED)\r\ntf.random.set_seed(SEED)\r\nos.environ['PYTHONHASHSEED'] = str(SEED)\r\ntf.keras.utils.set_random_seed(SEED)\r\ntf.config.experimental.enable_op_determinism()\r\n\r\n@tf.function\r\ndef model(x):\r\n    err = tf.random.uniform(shape=(1,))\r\n    loss = x + err\r\n    return err, loss\r\n\r\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\r\ntraining_periods = 10\r\n\r\n# initialize x\r\nx = tf.Variable(tf.random.uniform(shape=(1,)), trainable=True)\r\n\r\nfor tt in range(training_periods):\r\n    if tt % 2 == 0:\r\n        with tf.GradientTape() as tape:\r\n            err, loss = model(x)\r\n        \r\n        gradients = tape.gradient(loss, [x])  # suppose x need to be optimized\r\n        optimizer.apply_gradients(zip(gradients, [x]))  # update x\r\n        \r\n        print(f\"Period: {tt}, err (trained): {err.numpy()}\")\r\n        \r\n    else:\r\n        err, loss = model(x)\r\n        \r\n        print(f\"Period: {tt}, err (not trained): {err.numpy()}\")\r\n\r\n# outcomes\r\nPeriod: 0, err (trained): [0.01975703]\r\nPeriod: 1, err (not trained): [0.01975703]\r\nPeriod: 2, err (trained): [0.5400312]\r\nPeriod: 3, err (not trained): [0.5400312]\r\nPeriod: 4, err (trained): [0.51667833]\r\nPeriod: 5, err (not trained): [0.51667833]\r\nPeriod: 6, err (trained): [0.4683528]\r\nPeriod: 7, err (not trained): [0.4683528]\r\nPeriod: 8, err (trained): [0.14856052]\r\nPeriod: 9, err (not trained): [0.14856052]\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:apis","comp:tf.function","TF 2.16"],"created_at":"2024-05-18T14:09:09Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/68215"},{"issue_number":162,"repository":"tensorflow\/tensorflow","title":"Exit code 137 in `tf.raw_ops.ResizeNearestNeighborGrad`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the value in the input `size` is too large, program exceeds the memory limit.\n\n### Standalone code to reproduce the issue\n\n```shell\n# Signal --4;2024-05-14 00:40:39.895950: I tensorflow\/core\/util\/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-05-14 00:40:39.896316: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-14 00:40:39.900744: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-14 00:40:39.956390: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-05-14 00:40:40.842339: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n\r\n# ResizeNearestNeighborOpGrad\r\n\r\nimport tensorflow as tf\r\n\r\nalign_corners = True\r\nhalf_pixel_centers = False\r\ngrads = tf.constant(1.5e+300, shape=[1,8,16,3], dtype=tf.float64)\r\nsize = tf.constant([65534,65534], shape=[2], dtype=tf.int32)\r\ntf.raw_ops.ResizeNearestNeighborGrad(grads=grads, size=size, align_corners=align_corners, half_pixel_centers=half_pixel_centers)\n```\n\n\n### Relevant log output\n\n```shell\n(tensorflow-2.16.1-orig) root@b29bda27c601:\/mnt# python tests\/ResizeNearestNeighborGrad.py \r\n2024-05-15 08:43:01.439844: I tensorflow\/core\/util\/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-05-15 08:43:01.440211: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-05-15 08:43:01.444264: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-05-15 08:43:01.498275: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-05-15 08:43:02.744313: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nKilled\r\n```\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-15T09:47:39Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/67626"},{"issue_number":163,"repository":"tensorflow\/tensorflow","title":"Missing return in error check in mlir::TFTPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.1\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nPossible null pointer dereference may occur because of missing return in error check:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/63bb7339a2bc98af97c61e86eedbbd60c123529c\/tensorflow\/compiler\/mlir\/tensorflow\/transforms\/extract_tpu_copy_with_dynamic_shape_op.cc#L61-L65\n\n### Standalone code to reproduce the issue\n\n```shell\nBug was found with Svace static analyzer\n```\n\n\n### Relevant log output\n\n_No response_","labels":["awaiting review","type:bug","TF 2.16"],"created_at":"2024-05-14T12:28:06Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/67551"},{"issue_number":164,"repository":"tensorflow\/tensorflow","title":"Segmentation fault (core dumped) in `tf.raw_ops.FusedResizeAndPadConv2D`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIllegal `size` will trigger a segfault.\n\n### Standalone code to reproduce the issue\n\n```shell\n# Signal --4;2024-05-13 05:48:16.272992: I tensorflow\/core\/util\/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-05-13 05:48:16.273352: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-13 05:48:16.277715: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-13 05:48:16.331241: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-05-13 05:48:17.210828: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT2024-05-13 05:48:17.792175: E tensorflow\/core\/util\/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\r\n\r\n# FusedResizeConv2DUsingGemmOp\r\n\r\nimport tensorflow as tf\r\n\r\nmode = \"REFLECT\"\r\nstrides = [1, 1, 1, 1]\r\npadding = \"VALID\"\r\nresize_align_corners = False\r\ninput = tf.constant(0, shape=[1,2,3,2], dtype=tf.float16)\r\nsize = tf.constant([65534,65534], shape=[2], dtype=tf.int32)\r\npaddings = tf.constant(0, shape=[4,2], dtype=tf.int32)\r\nfilter = tf.constant(0, shape=[1,2,2,2], dtype=tf.float16)\r\ntf.raw_ops.FusedResizeAndPadConv2D(input=input, size=size, paddings=paddings, filter=filter, mode=mode, strides=strides, padding=padding, resize_align_corners=resize_align_corners)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-14 08:31:20.675971: E tensorflow\/core\/util\/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\r\nSegmentation fault (core dumped)\r\n```\r\nASAN report:\r\n```\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\n=================================================================\r\nAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\nAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\nAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\nAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\nAddressSanitizer==2060653==ERROR: AddressSanitizer: SEGV on unknown address 0x7fbb6ce8a800 (pc 0x7fc21cc305a9 bp 0x7fc1b01c05c0 sp 0x7fc1b01c03b0 T93)\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n==2060653==The signal is caused by a WRITE memory access.\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizerAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\nAddressSanitizer:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n:DEADLYSIGNAL\r\n    #0 0x7fc21cc305a9 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::pack_rhs(long, long) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x394ae5a9)\r\n    #1 0x7fc21cc31692 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::enqueue_packing_helper(long, long, long, bool) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x394af692)\r\n    #2 0x7fc24fd0b121 in Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41fd121)\r\n    #3 0x7fc24fd00326 in void absl::lts_20230802::internal_any_invocable::RemoteInvoker<false, void, tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}&>(absl::lts_20230802::internal_any_invocable::TypeErasedState*) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f2326)\r\n    #4 0x7fc24e62625c in tsl::(anonymous namespace)::PThread::ThreadFn(void*) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x2b1825c)\r\n    #5 0x7fc2f9885ac2  (\/lib\/x86_64-linux-gnu\/libc.so.6+0x94ac2)\r\n    #6 0x7fc2f9916a03 in __clone (\/lib\/x86_64-linux-gnu\/libc.so.6+0x125a03)\r\n\r\nAddressSanitizer can not provide additional info.\r\nSUMMARY: AddressSanitizer: SEGV (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x394ae5a9) in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::pack_rhs(long, long)\r\nThread T93 created by T0 here:\r\n    #0 0x7fc2f9bab685 in __interceptor_pthread_create ..\/..\/..\/..\/src\/libsanitizer\/asan\/asan_interceptors.cpp:216\r\n    #1 0x7fc24e633fbf in tsl::(anonymous namespace)::PThread::PThread(tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, absl::lts_20230802::AnyInvocable<void ()>) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x2b25fbf)\r\n    #2 0x7fc24e63450a in tsl::(anonymous namespace)::PosixEnv::StartThread(tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, absl::lts_20230802::AnyInvocable<void ()>) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x2b2650a)\r\n    #3 0x7fc24fd093a8 in Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::ThreadPoolTempl(int, bool, tsl::thread::EigenEnvironment) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41fb3a8)\r\n    #4 0x7fc24fd0d498 in tsl::thread::ThreadPool::ThreadPool(tsl::Env*, tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, Eigen::Allocator*) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41ff498)\r\n    #5 0x7fc24d7ef278 in tensorflow::LocalDevice::EigenThreadPoolInfo::EigenThreadPoolInfo(tensorflow::SessionOptions const&, int, tsl::Allocator*) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1ce1278)\r\n    #6 0x7fc24d7f05c5 in tensorflow::LocalDevice::LocalDevice(tensorflow::SessionOptions const&, tensorflow::DeviceAttributes const&) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1ce25c5)\r\n    #7 0x7fc24d7e3111 in tensorflow::ThreadPoolDevice::ThreadPoolDevice(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tsl::gtl::IntType<tensorflow::Bytes_tag_, long>, tensorflow::DeviceLocality const&, tsl::Allocator*) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1cd5111)\r\n    #8 0x7fc24d7dc1dd in tensorflow::ThreadPoolDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1cce1dd)\r\n    #9 0x7fc24d9a667c in tensorflow::DeviceFactory::AddCpuDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1e9867c)\r\n    #10 0x7fc24d9a6bbd in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1e98bbd)\r\n    #11 0x7fc1f3dd6db8 in TFE_NewContext (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x10654db8)\r\n    #12 0x7fc1df839e70 in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(TFE_ContextOptions const*)#9}, pybind11::object, TFE_ContextOptions const*, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::return_value_policy>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(TFE_ContextOptions const*)#9}&&, pybind11::object (*)(TFE_ContextOptions const*), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::return_value_policy const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0x1b6e70)\r\n    #13 0x7fc1df851899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (\/mnt\/\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0x1ce899)\r\n    #14 0x51ad66  (\/usr\/bin\/python3.11+0x51ad66)\r\n\r\n==2060653==ABORTING\r\n```\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-14T08:39:21Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/67529"},{"issue_number":165,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `QuantizedInstanceNorm`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nAbort is triggered when the input x does not meet the condition of dimension 4.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\noutput_range_given = False\r\ngiven_y_min = 0\r\ngiven_y_max = 0\r\nvariance_epsilon = 1e-05\r\nmin_separation = 0.001\r\nx = tf.constant([], shape=[0,0,0,1,1,0,0,0,11,9], dtype=tf.quint8)\r\nx_min = tf.constant(-3.5e+35, shape=[], dtype=tf.float32)\r\nx_max = tf.constant(0, shape=[], dtype=tf.float32)\r\ntf.raw_ops.QuantizedInstanceNorm(x=x, x_min=x_min, x_max=x_max, output_range_given=output_range_given, given_y_min=given_y_min, given_y_max=given_y_max, variance_epsilon=variance_epsilon, min_separation=min_separation)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-14 08:25:37.034690: F tensorflow\/core\/framework\/tensor_shape.cc:45] Check failed: NDIMS == dims() (4 vs. 10)Asking for tensor of 4 dimensions from a tensor of 10 dimensions\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-14T08:30:42Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/67528"},{"issue_number":166,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.FusedPadConv2D`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen `input` dimension is less than 4\uff0c it will cause abort.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nmode = \"REFLECT\"\r\nstrides = [1, 1, 1, 1]\r\npadding = \"VALID\"\r\ninput = tf.constant(1.5e+300, shape=[], dtype=tf.float64)\r\npaddings = tf.constant(65534, shape=[4,2], dtype=tf.int32)\r\nfilter = tf.constant([], shape=[1,1,0,1,0], dtype=tf.float64)\r\ntf.raw_ops.FusedPadConv2D(input=input, paddings=paddings, filter=filter, mode=mode, strides=strides, padding=padding)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-14 08:19:37.953832: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 0)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-14T08:23:10Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/67527"},{"issue_number":167,"repository":"tensorflow\/tensorflow","title":"Numerical precision issue of operators selu, leakyRelu, softplus and their corresponding backward operators on Bfloat16 vs float32","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI'd like to bring to attention an issue concerning the numerical precision of several operators (selu, leaky, relu) when operating on Bfloat16 versus float32 data types. I conducted comparisons using 20,000 random tensors for these operators, assessing the outputs in both Bfloat16 and float32 and computing the discrepancies. My observations indicate that differences generated by TensorFlow are generally more pronounced compared to PyTorch. Particularly noteworthy is the significant error produced by the SeluGrad operator. The results are summarized in the table below:\r\n\r\n| Operator           | TensorFlow | PyTorch |\r\n|--------------------|------------|---------|\r\n| selu               | 0.24918    | 0.12243 |\r\n| leakyrelu          | 0.01875    | 0.00094 |\r\n| softplus           | 0.05488    | 0.01554 |\r\n| seluGrad      | 10.41794   | 0.12406 |\r\n| leakyreluGrad | 0.01875    | 0.00094 |\r\n| softplusGrad | 0.13502    | 0.12484 |\r\n\r\nIn a standalone code to reproduce the issue, I provide illustrative instances for seluGrad operators, where the output discrepancy between Bfloat16 and float32 can be as high as 10.4.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfeatures = tf.convert_to_tensor(np.array([-0.00112915]), dtype=tf.float32)\r\ngradients = tf.convert_to_tensor(np.array([-14.6875]), dtype=tf.float32)\r\n\r\nx = tf.Variable(features)\r\nk = tf.constant(gradients)\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    y = tf.nn.selu(features=x)\r\n    z = k*y\r\n    fianl = tf.reduce_mean(z)\r\n    \r\nprint('float32 gradient:',tape.gradient(z, x))\r\n\r\n\r\nfeatures = tf.cast(features, dtype=tf.bfloat16)\r\ngradients = tf.cast(gradients, dtype=tf.bfloat16)\r\nx = tf.Variable(features)\r\nk = tf.constant(gradients)\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    y = tf.nn.selu(features=x)\r\n    z = k*y\r\n    fianl = tf.reduce_mean(z)\r\nprint('float16 gradient:',tape.gradient(z, x))\n```\n\n\n### Relevant log output\n\n```shell\nfloat32 gradient: tf.Tensor([-25.792944], shape=(1,), dtype=float32)\r\nbfloat16 gradient: tf.Tensor([-15.375], shape=(1,), dtype=bfloat16)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-13T12:05:08Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/67440"},{"issue_number":168,"repository":"tensorflow\/tensorflow","title":"Failed to compile on aarch64","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.16.1\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\naarch64\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\nbazelisk using the default version requested by tensorflow\r\n\r\n### GCC\/compiler version\r\n\r\nclang 17 (I also tried with clang16).\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen compiling the tensorflow source code on aarch64, I always get the following error:\r\n\r\n```\r\nWARN ERROR: \/home\/build\/tensorflow\/BUILD:1263:20: Linking tensorflow\/libtensorflow.so.2.16.1 failed: (Exit 1): clang-17 failed: error executing command (from target \/\/tensorflow:libtensorflow.so.2.16.1) \/usr\/bin\/clang-17 @bazel-out\/aarch64-opt\/bin\/tensorflow\/libtensorflow.so.2.16.1-2.params\r\n```\r\n\r\nHowever the compilation works on x86_64.\r\n\r\n``` \r\nexport PYTHON_BIN_PATH=\/usr\/bin\/python\r\n      export TF_PYTHON_VERSION=3.10\r\n      export USE_DEFAULT_PYTHON_LIB_PATH=1\r\n      export TF_NEED_JEMALLOC=1\r\n      export TF_NEED_KAFKA=1\r\n      export TF_NEED_OPENCL_SYCL=0\r\n      export TF_NEED_AWS=1\r\n      export TF_NEED_GCP=1\r\n      export TF_NEED_HDFS=1\r\n      export TF_NEED_S3=1\r\n      export TF_ENABLE_XLA=1\r\n      export TF_NEED_GDR=0\r\n      export TF_NEED_VERBS=0\r\n      export TF_NEED_OPENCL=0\r\n      export TF_NEED_MPI=0\r\n      export TF_NEED_TENSORRT=0\r\n      export TF_NEED_NGRAPH=0\r\n      export TF_NEED_IGNITE=0\r\n      export TF_NEED_ROCM=0\r\n      export TF_SYSTEM_LIBS=\"boringssl,curl,gif,icu,libjpeg_turbo,nasm,png,zlib\"\r\n      export TF_SET_ANDROID_WORKSPACE=0\r\n\r\n      .\/configure\r\n\r\n      bazel --bazelrc=.tf_configure.bazelrc build \\\r\n        --config=opt \\\r\n        --config=mkl_threadpool \\\r\n        \/\/tensorflow:libtensorflow.so \\\r\n        \/\/tensorflow:libtensorflow_cc.so \\\r\n        \/\/tensorflow:install_headers \\\r\n        \/\/tensorflow\/tools\/pip_package:build_pip_package\r\n```\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nexport PYTHON_BIN_PATH=\/usr\/bin\/python\r\n      export TF_PYTHON_VERSION=3.10\r\n      export USE_DEFAULT_PYTHON_LIB_PATH=1\r\n      export TF_NEED_JEMALLOC=1\r\n      export TF_NEED_KAFKA=1\r\n      export TF_NEED_OPENCL_SYCL=0\r\n      export TF_NEED_AWS=1\r\n      export TF_NEED_GCP=1\r\n      export TF_NEED_HDFS=1\r\n      export TF_NEED_S3=1\r\n      export TF_ENABLE_XLA=1\r\n      export TF_NEED_GDR=0\r\n      export TF_NEED_VERBS=0\r\n      export TF_NEED_OPENCL=0\r\n      export TF_NEED_MPI=0\r\n      export TF_NEED_TENSORRT=0\r\n      export TF_NEED_NGRAPH=0\r\n      export TF_NEED_IGNITE=0\r\n      export TF_NEED_ROCM=0\r\n      export TF_SYSTEM_LIBS=\"boringssl,curl,gif,icu,libjpeg_turbo,nasm,png,zlib\"\r\n      export TF_SET_ANDROID_WORKSPACE=0\r\n\r\n      .\/configure\r\n\r\n      bazel --bazelrc=.tf_configure.bazelrc build \\\r\n        --config=opt \\\r\n        --config=mkl_threadpool \\\r\n        \/\/tensorflow:libtensorflow.so \\\r\n        \/\/tensorflow:libtensorflow_cc.so \\\r\n        \/\/tensorflow:install_headers \\\r\n        \/\/tensorflow\/tools\/pip_package:build_pip_package\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024\/05\/09 13:03:51 WARN [14,447 \/ 14,578] Compiling tensorflow\/compiler\/jit\/xla_platform_info.cc; 20s local ... (16 actions running)\r\n2024\/05\/09 13:03:52 WARN [14,448 \/ 14,578] Compiling tensorflow\/compiler\/jit\/xla_platform_info.cc; 21s local ... (16 actions running)\r\n2024\/05\/09 13:03:55 WARN [14,449 \/ 14,578] Compiling tensorflow\/compiler\/jit\/xla_platform_info.cc; 24s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:03:57 WARN [14,450 \/ 14,578] Compiling tensorflow\/compiler\/jit\/xla_platform_info.cc; 26s local ... (16 actions running)\r\n2024\/05\/09 13:03:58 WARN [14,452 \/ 14,578] Compiling tensorflow\/compiler\/jit\/get_compiler_ir.cc; 26s local ... (16 actions running)\r\n2024\/05\/09 13:04:00 WARN [14,454 \/ 14,578] Compiling tensorflow\/compiler\/jit\/kernels\/xla_ops.cc; 27s local ... (16 actions running)\r\n2024\/05\/09 13:04:01 WARN [14,457 \/ 14,578] Compiling tensorflow\/compiler\/jit\/xla_cpu_device.cc; 22s local ... (16 actions running)\r\n2024\/05\/09 13:04:03 WARN [14,459 \/ 14,578] Compiling tensorflow\/compiler\/jit\/xla_cpu_device.cc; 24s local ... (16 actions running)\r\n2024\/05\/09 13:04:04 WARN [14,460 \/ 14,578] Compiling tensorflow\/compiler\/jit\/xla_cpu_device.cc; 25s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:04:06 WARN [14,461 \/ 14,578] Compiling tensorflow\/compiler\/jit\/xla_cpu_device.cc; 27s local ... (16 actions running)\r\n2024\/05\/09 13:04:07 WARN [14,464 \/ 14,578] Compiling tensorflow\/compiler\/tf2xla\/mlir_tf2xla.cc; 22s local ... (16 actions running)\r\n2024\/05\/09 13:04:11 WARN [14,465 \/ 14,578] Compiling tensorflow\/compiler\/tf2xla\/mlir_tf2xla.cc; 26s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:04:12 WARN [14,467 \/ 14,578] Compiling tensorflow\/compiler\/tf2xla\/mlir_tf2xla.cc; 27s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:04:13 WARN [14,468 \/ 14,578] Compiling tensorflow\/compiler\/tf2xla\/mlir_tf2xla.cc; 28s local ... (16 actions running)\r\n2024\/05\/09 13:04:14 WARN [14,470 \/ 14,578] Compiling tensorflow\/compiler\/tf2xla\/mlir_tf2xla.cc; 29s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:04:16 WARN [14,473 \/ 14,578] Compiling tensorflow\/compiler\/aot\/codegen.cc; 19s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:04:17 WARN [14,476 \/ 14,578] Compiling tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_plugin_init.cc; 16s local ... (16 actions running)\r\n2024\/05\/09 13:04:18 WARN [14,478 \/ 14,578] Compiling tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_plugin_init.cc; 17s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:04:20 WARN [14,479 \/ 14,578] Compiling tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_plugin_init.cc; 19s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:04:21 WARN [14,481 \/ 14,578] Compiling tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_plugin_init.cc; 20s local ... (16 actions, 14 running)\r\n2024\/05\/09 13:04:22 WARN [14,482 \/ 14,578] Compiling tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_plugin_init.cc; 21s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:04:23 WARN [14,486 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 21s local ... (16 actions running)\r\n2024\/05\/09 13:04:25 WARN [14,487 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 23s local ... (16 actions running)\r\n2024\/05\/09 13:04:26 WARN [14,489 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 24s local ... (16 actions running)\r\n2024\/05\/09 13:04:28 WARN [14,491 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 25s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:04:29 WARN [14,492 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 27s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:04:30 WARN [14,494 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 28s local ... (16 actions, 15 running)\r\n2024\/05\/09 13:04:31 WARN [14,497 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 29s local ... (14 actions running)\r\n2024\/05\/09 13:04:32 WARN [14,498 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 30s local ... (13 actions running)\r\n2024\/05\/09 13:04:34 WARN [14,500 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 32s local ... (11 actions running)\r\n2024\/05\/09 13:04:36 WARN [14,505 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 34s local ... (6 actions running)\r\n2024\/05\/09 13:04:38 WARN [14,506 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 36s local ... (5 actions running)\r\n2024\/05\/09 13:04:40 WARN [14,508 \/ 14,578] Compiling tensorflow\/compiler\/mlir\/python\/mlir.cc; 38s local ... (3 actions running)\r\n2024\/05\/09 13:04:40 WARN ERROR: \/home\/build\/tensorflow\/BUILD:1263:20: Linking tensorflow\/libtensorflow.so.2.16.1 failed: (Exit 1): clang-17 failed: error executing command (from target \/\/tensorflow:libtensorflow.so.2.16.1) \/usr\/bin\/clang-17 @bazel-out\/aarch64-opt\/bin\/tensorflow\/libtensorflow.so.2.16.1-2.params\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN \/usr\/bin\/..\/lib64\/gcc\/aarch64-unknown-linux-gnu\/13.2.0\/..\/..\/..\/..\/aarch64-unknown-linux-gnu\/bin\/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.\r\n2024\/05\/09 13:04:40 WARN clang-17: error: linker command failed with exit code 1 (use -v to see invocation)\r\n2024\/05\/09 13:04:41 WARN INFO: Elapsed time: 4130.234s, Critical Path: 216.57s\r\n2024\/05\/09 13:04:41 WARN INFO: 14511 processes: 1423 internal, 13088 local.\r\n2024\/05\/09 13:04:41 WARN FAILED: Build did NOT complete successfully\r\n```\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","TF 2.16"],"created_at":"2024-05-09T13:28:57Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/67251"},{"issue_number":169,"repository":"tensorflow\/tensorflow","title":"GlobalAveragePooling1D fails with empty inputs and a mask","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.keras.layers.GlobalAveragePooling1D` cannot be called on an empty tensor with an empty mask. This can cause issue when using a model that uses this layer under a distributed strategy, e.g. `MirroredStrategy`, which will distribute the data over multiple GPUs. For instance, for a dataset with 3 samples, a batch_size of 2 and 3 GPUs, one of the GPUs will have empty batches which will cause the error.\r\n\r\nThis is due to the casting `math_ops.cast(mask, inputs[0].dtype)` in `GlobalAveragePooling1D::call()` which implies a non-empty inputs tensor, while `math_ops.cast(mask, inputs.dtype`) should do the trick without causing the error.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = np.random.rand(0, 3, 4)\r\nmask = np.random.rand(0, 3)\r\nprint(x, mask)\r\n\r\ny = tf.keras.layers.GlobalAveragePooling1D()(x, mask=mask)\r\nprint(y)\n```\n\n\n### Relevant log output\n\n```shell\n[] []\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-20-0f7c323aaaab> in <cell line: 7>()\r\n      5 print(x, mask)\r\n      6 \r\n----> 7 y = tf.keras.layers.GlobalAveragePooling1D()(x, mask=mask)\r\n      8 print(y)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py in error_handler(*args, **kwargs)\r\n     68             # To get the full stack trace, call:\r\n     69             # `tf.debugging.disable_traceback_filtering()`\r\n---> 70             raise e.with_traceback(filtered_tb) from None\r\n     71         finally:\r\n     72             del filtered_tb\r\n\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/framework\/ops.py in raise_from_not_ok_status(e, name)\r\n   5881 def raise_from_not_ok_status(e, name) -> NoReturn:\r\n   5882   e.message += (\" name: \" + str(name if name is not None else \"\"))\r\n-> 5883   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n   5884 \r\n   5885 \r\n\r\nInvalidArgumentError: Exception encountered when calling layer 'global_average_pooling1d_8' (type GlobalAveragePooling1D).\r\n\r\n{{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: global_average_pooling1d_8\/strided_slice\/\r\n\r\nCall arguments received by layer 'global_average_pooling1d_8' (type GlobalAveragePooling1D):\r\n  \u2022 inputs=tf.Tensor(shape=(0, 3, 4), dtype=float32)\r\n  \u2022 mask=array([], shape=(0, 3), dtype=float64)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF 2.15"],"created_at":"2024-05-06T14:57:57Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/67023"},{"issue_number":170,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `TensorScatterOp`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe dimensions of `indices` and `updates` must be equal.If not equal,`tf.raw\r\n_ops.TensorScatterSub,tf.raw\r\n_ops.TensorScatterAdd,tf.raw\r\n_ops.TensorScatterMin,tf.raw\r\n_ops.TensorScatterMax,tf.raw\r\n_ops.TensorScatterUpdate ` will crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntensor = tf.constant(0, shape=[101,32], dtype=tf.float32)\r\nindices = tf.constant([], shape=[0,0,0,6], dtype=tf.int32)\r\nupdates = tf.constant([], shape=[0,0], dtype=tf.float32)\r\ntf.raw_ops.TensorScatterAdd(tensor=tensor, indices=indices, updates=updates)\r\n```\r\n```\r\nimport tensorflow as tf\r\n\r\ntensor = tf.constant(0, shape=[101,32], dtype=tf.float32)\r\nindices = tf.constant([], shape=[0,0,0,6], dtype=tf.int32)\r\nupdates = tf.constant([], shape=[0,0], dtype=tf.float32)\r\ntf.raw_ops.TensorScatterMax(tensor=tensor, indices=indices, updates=updates)\r\n```\r\n```\r\nimport tensorflow as tf\r\n\r\ntensor = tf.constant(0, shape=[101,32], dtype=tf.float32)\r\nindices = tf.constant([], shape=[0,0,0,6], dtype=tf.int32)\r\nupdates = tf.constant([], shape=[0,0], dtype=tf.float32)\r\ntf.raw_ops.TensorScatterMin(tensor=tensor, indices=indices, updates=updates)\r\n```\r\n```\r\nimport tensorflow as tf\r\n\r\ntensor = tf.constant(0, shape=[101,32], dtype=tf.float32)\r\nindices = tf.constant([], shape=[0,0,0,6], dtype=tf.int32)\r\nupdates = tf.constant([], shape=[0,0], dtype=tf.float32)\r\ntf.raw_ops.TensorScatterSub(tensor=tensor, indices=indices, updates=updates)\r\n```\r\n```\r\nimport tensorflow as tf\r\n\r\ntensor = tf.constant(0, shape=[101,32], dtype=tf.float32)\r\nindices = tf.constant([], shape=[0,0,0,6], dtype=tf.int32)\r\nupdates = tf.constant([], shape=[0,0], dtype=tf.float32)\r\ntf.raw_ops.TensorScatterUpdate(tensor=tensor, indices=indices, updates=updates)\r\n```\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-01 04:00:48.333921: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 2)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-01T04:08:30Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66768"},{"issue_number":171,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.SparseBincount`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nMultiplication of `dense_shape` and `size` causes integer overflow\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nbinary_output = True\r\nindices = tf.constant(0, shape=[3456,2], dtype=tf.int64)\r\nvalues = tf.constant(-536870912, shape=[3456], dtype=tf.int32)\r\ndense_shape = tf.constant([1250999896764,1250999896764], shape=[2], dtype=tf.int64)\r\nsize = tf.constant(305032608, shape=[], dtype=tf.int32)\r\nweights = tf.constant(51, shape=[3456], dtype=tf.int32)\r\ntf.raw_ops.SparseBincount(indices=indices, values=values, dense_shape=dense_shape, size=size, weights=weights, binary_output=binary_output)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-01 03:51:04.482001: F tensorflow\/core\/framework\/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 1250999896764 with 305032608, result: -1\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-01T03:53:11Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66766"},{"issue_number":172,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.NearestNeighbors`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nParameter k being set to a negative number will cause a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\npoints = tf.constant(0.554979503, shape=[1000,100], dtype=tf.float32)\r\ncenters = tf.constant(0.554979503, shape=[1000,100], dtype=tf.float32)\r\nk = tf.constant(-1250999896764, shape=[], dtype=tf.int64)\r\ntf.raw_ops.NearestNeighbors(points=points, centers=centers, k=k)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-01 03:49:00.796702: F tensorflow\/core\/framework\/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -1250999896764\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-01T03:50:23Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66765"},{"issue_number":173,"repository":"tensorflow\/tensorflow","title":"Check fail in `tf.raw_ops.MaxPoolGradWithArgmax`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen argmax is set to a maximum value\uff0c `tf.raw_ops.MaxPoolGradWithArgmax` encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nksize = [1, 1, 1, 1]\r\nstrides = [1, 1, 1, 1]\r\npadding = \"VALID\"\r\ninclude_batch_in_index = False\r\ninput = tf.constant(1, shape=[1,1,1,1], dtype=tf.float32)\r\ngrad = tf.constant(1, shape=[1,1,1,1], dtype=tf.float32)\r\nargmax = tf.constant(1250999896764, shape=[1,1,1,1], dtype=tf.int64)\r\ntf.raw_ops.MaxPoolGradWithArgmax(input=input, grad=grad, argmax=argmax, ksize=ksize, strides=strides, padding=padding, include_batch_in_index=include_batch_in_index)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-01 03:42:17.994627: F tensorflow\/core\/kernels\/maxpooling_op.cc:1081] Check failed: grad_out_index >= output_start && grad_out_index < output_end Invalid output gradient index: 1250999896764, 0, 1\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-01T03:46:46Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66764"},{"issue_number":174,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) with `tf.raw_ops.LSTMBlockCellGrad`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe parameter x is expected to be two-dimensional, and an error will occur when x is one-dimensional\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nuse_peephole = False\r\nx = tf.constant(0, shape=[17], dtype=tf.float32)\r\ncs_prev = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nh_prev = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nw = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nwci = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nwcf = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nwco = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nb = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\ni = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\ncs = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nf = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\no = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nci = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nco = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\ncs_grad = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nh_grad = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\ntf.raw_ops.LSTMBlockCellGrad(x=x, cs_prev=cs_prev, h_prev=h_prev, w=w, wci=wci, wcf=wcf, wco=wco, b=b, i=i, cs=cs, f=f, o=o, ci=ci, co=co, cs_grad=cs_grad, h_grad=h_grad, use_peephole=use_peephole)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-01 03:41:18.804132: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-01T03:41:25Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66763"},{"issue_number":175,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) with `tf.raw_ops.LSTMBlockCell`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, `tf.raw_ops.LSTMBlockCell` encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nforget_bias = 1\r\ncell_clip = 0\r\nuse_peephole = False\r\nx = tf.constant([], shape=[0], dtype=tf.float32)\r\ncs_prev = tf.constant([], shape=[0], dtype=tf.float32)\r\nh_prev = tf.constant([], shape=[0,0], dtype=tf.float32)\r\nw = tf.constant([], shape=[0], dtype=tf.float32)\r\nwci = tf.constant([], shape=[0,0], dtype=tf.float32)\r\nwcf = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nwco = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\nb = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)\r\ntf.raw_ops.LSTMBlockCell(x=x, cs_prev=cs_prev, h_prev=h_prev, w=w, wci=wci, wcf=wcf, wco=wco, b=b, forget_bias=forget_bias, cell_clip=cell_clip, use_peephole=use_peephole)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-01 03:36:12.005999: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-01T03:37:18Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66762"},{"issue_number":176,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) with `tf.raw_ops.LRNGrad`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIf  the dimension of `output_image` is less than `input_grads` and `input_image`, tf.raw_ops.tf.raw_ops.LoadAndRemapMatrix encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\n# LRNGradOp\r\n\r\nimport tensorflow as tf\r\n\r\ndepth_radius = 1\r\nbias = 1.59018219\r\nalpha = 0.117728651\r\nbeta = 0.404427052\r\ninput_grads = tf.constant([], shape=[0,0,0,0], dtype=tf.float32)\r\ninput_image = tf.constant([], shape=[0,0,0,0], dtype=tf.float32)\r\noutput_image = tf.constant([], shape=[0], dtype=tf.float32)\r\ntf.raw_ops.LRNGrad(input_grads=input_grads, input_image=input_image, output_image=output_image, depth_radius=depth_radius, bias=bias, alpha=alpha, beta=beta)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-01 03:28:07.055077: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-01T03:33:03Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66761"},{"issue_number":177,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) with `tf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nCheck Failed in `tf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient` when the input of inputs is scalar, which causes the program to crash.\n\n### Standalone code to reproduce the issue\n\n```shell\n# FakeQuantWithMinMaxVarsPerChannelGradientOp\r\n\r\nimport tensorflow as tf\r\n\r\nnum_bits = 8\r\nnarrow_range = False\r\ngradients = tf.constant(0, shape=[], dtype=tf.float32)\r\ninputs = tf.constant(0, shape=[], dtype=tf.float32)\r\nmin = tf.constant(124, shape=[3,1], dtype=tf.float32)\r\nmax = tf.constant(124, shape=[3,1], dtype=tf.float32)\r\ntf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient(gradients=gradients, inputs=inputs, min=min, max=max, num_bits=num_bits, narrow_range=narrow_range)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-01 03:10:46.669162: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-01T03:16:02Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66759"},{"issue_number":178,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) with `tf.raw_ops.Dilation2DBackpropFilter`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, `tf.raw_ops.Dilation2DBackpropFilter` encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nstrides = [1, 1, 1, 1]\r\nrates = [1, 1, 1, 1]\r\npadding = \"VALID\"\r\ninput = tf.constant([], shape=[0,0,0,0], dtype=tf.float32)\r\nfilter = tf.constant([], shape=[0,0,0], dtype=tf.float32)\r\nout_backprop = tf.constant([], shape=[0], dtype=tf.float32)\r\ntf.raw_ops.Dilation2DBackpropFilter(input=input, filter=filter, out_backprop=out_backprop, strides=strides, rates=rates, padding=padding)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-01 03:00:39.549898: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-01T03:05:19Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66758"},{"issue_number":179,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) with `tf.raw_ops.CombinedNonMaxSuppression`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIf `max_total_size` is too large , `tf.raw_ops.CombinedNonMaxSuppression` encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\npad_per_class = True\r\nclip_boxes = False\r\nboxes = tf.constant(3.5e+35, shape=[], dtype=tf.float32)\r\nscores = tf.constant(0, shape=[], dtype=tf.float32)\r\nmax_output_size_per_class = tf.constant(8, shape=[], dtype=tf.int32)\r\nmax_total_size = tf.constant(1879048192, shape=[], dtype=tf.int32)\r\niou_threshold = tf.constant(0.5, shape=[], dtype=tf.float32)\r\nscore_threshold = tf.constant(0.5, shape=[], dtype=tf.float32)\r\ntf.raw_ops.CombinedNonMaxSuppression(boxes=boxes, scores=scores, max_output_size_per_class=max_output_size_per_class, max_total_size=max_total_size, iou_threshold=iou_threshold, score_threshold=score_threshold, pad_per_class=pad_per_class, clip_boxes=clip_boxes)\n```\n\n\n### Relevant log output\n\n```shell\n2024-05-01 02:52:31.372751: W tensorflow\/core\/kernels\/image\/non_max_suppression_op.cc:998] Detected a large value for `max_total_size`. This may cause OOM error. (max_total_size: 1879048192)\r\n2024-05-01 02:52:31.372808: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 0)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-05-01T02:59:49Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66757"},{"issue_number":180,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.StringSplitV2 does not limit a number of splits by max_split attribute","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\ntf.raw_ops.StringSplitV2 does not limit a number of splits by max_split attribute.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntf.raw_ops.StringSplitV2(input=['one<>'], sep='<>', maxsplit=1) # outputs two splits 'one' and '' instead of just one split\r\n\r\ntf.raw_ops.StringSplitV2(input=['one<>two'], sep='<>', maxsplit=1) # outputs two splits 'one' and 'two' instead of just one split\n```\n\n\n### Relevant log output\n\n```shell\n>>> tf.raw_ops.StringSplitV2(input=['one<>'], sep='<>', maxsplit=1)\r\nStringSplitV2(indices=<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\r\narray([[0, 0],\r\n       [0, 1]])>, values=<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'one', b''], dtype=object)>, shape=<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 2])>)\r\n\r\n>>> tf.raw_ops.StringSplitV2(input=['one<>two'], sep='<>', maxsplit=1)\r\nStringSplitV2(indices=<tf.Tensor: shape=(2, 2), dtype=int64, numpy=\r\narray([[0, 0],\r\n       [0, 1]])>, values=<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'one', b'two'], dtype=object)>, shape=<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 2])>)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-28T09:40:56Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66583"},{"issue_number":181,"repository":"tensorflow\/tensorflow","title":"Profiler does not Seem to Output Timesteps in xplane.pb - \"No step marker observed and hence the step time is unknown\" from Tensorboard","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0   (cuda120py39hb94c71b_3  from conda-forge)\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu Jammy in podman Container\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.18\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.4 (cuda-cupti 12.4.127  @ h59595ed_1   from conda-forge)\n\n### GPU model and memory\n\nRTX 3090, 24GiB\n\n### Current behavior?\n\nI am in the process of writing a [custom loss function]( https:\/\/github.com\/stellarpower\/Soft-DTW\/tree\/stellapower.AddFeature.BetterParallelisation ), and trying to profile it to see where resources are currently used.\r\n\r\nI have installed newer CUDA drives, the latest release of Tensorflow (2.15), the CUDA PTI libraries, and other dependencies needed for the Tensorboard profiler plugin.\r\n\r\nI can run an example with the profiler and get what looks like reasonable data. With my own code, I get a warning back from `_pywrap_profiler.xspace_to_tools_data()` that no timesteps are contained in the file, and thus, some of the useful profiling information is absent\/unusable. I cut back the example and found that if I use the MSE loss, the profile is complete; if I change to my own loss function, the timesteps are no longer output.\r\n\r\nGiven that the message is coming back from the [core profiler library]( https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.15.0\/tensorflow\/core\/profiler\/utils\/diagnostics.cc#L62 ), and is contained within the encoded protocol buffers, I believe that this is an issue with the profiler and the main library, rather than the tensorboard utility or the profiling plugin.\r\n\r\nThe loss function is reasonably complicated, so I suspected at first the large files might be an issue. However, when reducing down to the toy example above, whilst there's a difference, the overhead in the files makes this difference much closer:\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/5004545\/6fa3ee71-39fd-4791-a9a1-8f8d6f08d091)\r\nI previously had warning s that the processor has dropped frames due to insufficient buffer space, but with this trivially small data size, those are gone.\n\n### Standalone code to reproduce the issue\n\n```shell\n#!\/usr\/bin\/env python\r\n    \r\nimport os, sys, pprint, numpy\r\nimport tensorflow as tf\r\n\r\n\r\nScriptDirectory = os.path.dirname(os.path.realpath(__file__))\r\n\r\n# https:\/\/github.com\/stellarpower\/Soft-DTW\/tree\/stellapower.AddFeature.BetterParallelisation\r\nsys.path.insert(0, f\"\/path\/to\/Soft-DTW\")\r\nfrom softdtwkeras.SDTWLoss import SDTWLoss\r\n#############################################################\r\n\r\n\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow import keras\r\n\r\nfrom softdtwkeras.SDTWLoss import SDTWLoss\r\nfrom datetime import datetime\r\n\r\n\r\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nEpochs = 128\r\n\r\n\r\nProfilingTensorboardLogsDirectory = f\"{ ScriptDirectory }\/BugLogs\/{ datetime.now().strftime('%Y-%m-%d_%H.%M.%S') }\"\r\n\r\ntboard_callback = tf.keras.callbacks.TensorBoard(\r\n    log_dir = ProfilingTensorboardLogsDirectory,\r\n    histogram_freq = 1,\r\n    \r\n    profile_batch = (32, 96) # Should be Middle 50%\r\n)\r\n\r\n\r\n\r\n## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\n# Very small toy data, in case the file size is an issue.\r\nx_train = numpy.zeros((1, 1, 1))\r\n\r\n\r\nmodel = tf.keras.models.Sequential([\r\n    tf.keras.layers.Flatten(batch_input_shape=(1, 1, 1), name='layers_flatten'),\r\n])\r\n\r\nmodel.compile(\r\n      optimizer = 'adam',\r\n    # This runs okay:\r\n      # loss      =  'mse',\r\n    # This results in no timestep info in the xplane file.\r\n        loss      = SDTWLoss(gamma=0.5, BatchSize = 1)\r\n)\r\n\r\n\r\nmodel.fit(\r\n    x         = x_train, \r\n    y         = x_train, # << trivial identity function\r\n    epochs    = Epochs,\r\n    callbacks = [tboard_callback]\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-04-24 23:32:23.290829: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-04-24 23:32:23.290868: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-04-24 23:32:23.291486: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-04-24 23:32:23.296288: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n\r\n\r\n\r\n2024-04-24 23:32:26.851127: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:104] Profiler session initializing.\r\n2024-04-24 23:32:26.851153: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:119] Profiler session started.\r\n2024-04-24 23:32:26.857074: I external\/local_xla\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:1883] Profiler found 1 GPUs\r\n2024-04-24 23:32:26.861348: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:131] Profiler session tear down.\r\n2024-04-24 23:32:26.861419: I external\/local_xla\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:2017] CUPTI activity buffer flushed\r\n\r\n\r\n2024-04-24 23:32:26.871326: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-04-24 23:32:26.903533: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-04-24 23:32:26.903741: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-04-24 23:32:26.906062: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-04-24 23:32:26.906217: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-04-24 23:32:26.906350: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-04-24 23:32:26.982483: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-04-24 23:32:26.982679: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-04-24 23:32:26.982827: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n\r\n\r\n2024-04-24 23:32:26.982929: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1929] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 22199 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\r\n\r\n\r\nEpoch 1\/128\r\n1\/1 [==============================] - 2s 2s\/step - loss: 0.0000e+00\r\n...\r\nEpoch 31\/128\r\n1\/1 [==============================] - 0s 6ms\/step - loss: 0.0000e+00\r\nEpoch 32\/128\r\n2024-04-24 23:32:30.249189: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:104] Profiler session initializing.\r\n2024-04-24 23:32:30.249218: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:119] Profiler session started.\r\n1\/1 [==============================] - 0s 15ms\/step - loss: 0.0000e+00\r\nEpoch 33\/128\r\n1\/1 [==============================] - 0s 7ms\/step - loss: 0.0000e+00\r\n...\r\nEpoch 95\/128\r\n1\/1 [==============================] - 0s 7ms\/step - loss: 0.0000e+00\r\nEpoch 96\/128\r\n\r\n\r\n2024-04-24 23:32:30.746769: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:70] Profiler session collecting data.\r\n2024-04-24 23:32:30.747796: I external\/local_xla\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:2017] CUPTI activity buffer flushed\r\n2024-04-24 23:32:30.792759: I external\/local_xla\/xla\/backends\/profiler\/gpu\/cupti_collector.cc:541]  GpuTracer has collected 4216 callback api events and 3979 activity events. \r\n2024-04-24 23:32:30.840791: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:131] Profiler session tear down.\r\n2024-04-24 23:32:30.841246: I external\/local_tsl\/tsl\/profiler\/rpc\/client\/save_profile.cc:144] Collecting XSpace to repository: \/BugLogs\/2024-04-24_23.32.26\/plugins\/profile\/2024_04_24_23_32_30\/the-alchemist.xplane.pb\r\n\r\n\r\n1\/1 [==============================] - 0s 120ms\/step - loss: 0.0000e+00\r\nEpoch 97\/128\r\n...\n```\n","labels":["comp:tensorboard","stat:awaiting tensorflower","type:bug","comp:apis","TF 2.15"],"created_at":"2024-04-24T23:48:35Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66410"},{"issue_number":182,"repository":"tensorflow\/tensorflow","title":"Denial of Service in tf.raw_ops.Unstage","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.0\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nHi,\r\nWhen using `tf.raw_ops.Unstage`, a `Denial of Service` was encountered under normal invocation, with all passed parameters meeting the requirements of the API documentation.\r\nAfter a long wait, there was still no response.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntensor = tf.constant([1.0, 2.0, 3.0])\r\nstage_op = tf.raw_ops.Stage(values=[tensor])\r\nprint(tensor.dtype)  # <dtype: 'float32'>\r\n\r\nunstage_op = tf.raw_ops.Unstage(dtypes=[tensor.dtype])  # Hang\r\n\r\nwith tf.compat.v1.Session() as sess:\r\n    print(\"here\")\r\n    sess.run(stage_op)\r\n    result = sess.run(unstage_op)\r\n\r\nprint(result)  # Expected output: [1. 2. 3.]\n```\n\n\n### Relevant log output\n\n```shell\n2024-04-22 02:23:12.166249: I tensorflow\/core\/util\/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-04-22 02:23:12.166825: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-04-22 02:23:12.169923: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-04-22 02:23:12.215234: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-04-22 02:23:13.066132: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-04-22 02:23:13.789282: W tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https:\/\/www.tensorflow.org\/install\/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n<dtype: 'float32'>\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-22T02:27:21Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66174"},{"issue_number":183,"repository":"tensorflow\/tensorflow","title":"tf.function deadlock with multiple multiprocess\/threading","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.16\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n\r\n### Current behavior?\r\n\r\nTensorFlow gets stuck when using multiprocessing\/threading more than once.\r\n\r\nI've observed it in more complicated situations with only once multithreading, but the following is a reproducible, standalone example that illustrates the point.\r\n\r\nThe code works correctly if either the `tf.function` decorator is removed _or_ if xla compilation `jit_compile` is enabled (!).\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport multiprocessing as mp  # can also be another module for multiprocess\/threading\r\nimport random\r\n\r\nimport tensorflow as tf\r\n\r\n\r\n# if we use jit_compile=True, it will work, magically\r\n# it also works if there is no decorator at all, i.e. only eager mode\r\n@tf.function(jit_compile=False)\r\ndef testjit(x):\r\n    return tf.math.reduce_sum(x)\r\n\r\n\r\ndef make_zdata(_=None):\r\n    print('Making data')\r\n    # just to make sure that we recompile the function (different shapes)\r\n    rnd = tf.random.uniform([random.randint(100, 10000)], -1, 1)\r\n    zdata = testjit(rnd)\r\n    print('Made data')\r\n    return zdata\r\n\r\nwith mp.Pool(1) as executor:\r\n    executor.map(make_zdata, [1])\r\nexecutor.terminate()\r\n\r\n# if we run this, it will fail (if jit_compile=False)\r\nwith mp.Pool(1) as executor:\r\n    executor.map(make_zdata, [1])  # here, the code will be stuck\r\nexecutor.terminate()\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nwill be, approximately:\r\n```\r\nMaking data\r\nMade data\r\nMaking data\r\n```\r\nand then it's stuck. Otherwise, if using `jit_compile=True` or not using the tf.function decorator at all, another `Made data` will be printed","labels":["stat:awaiting tensorflower","type:bug","comp:tf.function","TF 2.16"],"created_at":"2024-04-19T22:45:12Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66115"},{"issue_number":184,"repository":"tensorflow\/tensorflow","title":"Failing Tensorflow unit tests for BF16 hardware","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWe have the following unit test failures in Tensorflow github\/nightly:\r\n\r\n\/\/tensorflow\/compiler\/tests:conv3d_test_cpu\r\n\/\/tensorflow\/compiler\/tests:conv3d_test_cpu_mlir_bridge_test\r\n\/\/tensorflow\/compiler\/tests:stateful_random_ops_test_cpu\r\n\/\/tensorflow\/compiler\/tests:stateless_random_ops_test_cpu\r\n\/\/tensorflow\/compiler\/tests:stateless_random_ops_test_cpu_mlir_bridge_test\r\n\/\/tensorflow\/compiler\/tests:stateful_random_ops_test_cpu_mlir_bridge_test\r\n\/\/tensorflow\/compiler\/tests:stochastic_cast_op_test_cpu\r\n\r\nOn investigation, the first commit where this issue is present is https:\/\/github.com\/tensorflow\/tensorflow\/commit\/a4d7e978701feb9ed485b926b22676b9f7651d4e, tests pass with the commit immediately prior to this.\r\n\r\n The tests do not fail in the upstream CI because it uses N1 cores with no bf16 HW.\r\n\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nFrom the directory tensorflow\/ci\/official, to reproduce the failure for e.g. \/\/tensorflow\/compiler\/tests:conv3d_test_cpu we want to:\r\nopen any.sh and remove cd \"$(dirname \"$0\")\/..\/..\/\"  # tensorflow\/\r\nrun:\r\nTFCI=py311,linux_arm64  TF_ANY_MODE=test TF_ANY_TARGETS=\/\/tensorflow\/compiler\/tests:conv3d_test_cpu  .\/any.sh\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nAn example error from the stochastic_cast test:\r\n\r\nFAIL: \/\/tensorflow\/compiler\/tests:stochastic_cast_op_test_cpu (shard 13 of 20) (see \/root\/.cache\/bazel\/_bazel_root\/574657b8af23672198530ef061ba4201\/execroot\/org_tensorflow\/bazel-out\/aarch64-opt\/testlogs\/tensorflow\/compiler\/tests\/stochastic_cast_op_test_cpu\/shard_13_of_20\/test.log)\r\nINFO: From Testing \/\/tensorflow\/compiler\/tests:stochastic_cast_op_test_cpu (shard 13 of 20):\r\n==================== Test output for \/\/tensorflow\/compiler\/tests:stochastic_cast_op_test_cpu (shard 13 of 20):\r\n2024-04-15 10:17:41.267151: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\nRunning tests under Python 3.11.6: \/root\/.cache\/bazel\/_bazel_root\/574657b8af23672198530ef061ba4201\/execroot\/org_tensorflow\/bazel-out\/aarch64-opt\/bin\/tensorflow\/compiler\/tests\/stochastic_cast_op_test_cpu.runfiles\/python_aarch64-unknown-linux-gnu\/bin\/python3\r\nWARNING:tensorflow:From \/root\/.cache\/bazel\/_bazel_root\/574657b8af23672198530ef061ba4201\/execroot\/org_tensorflow\/bazel-out\/aarch64-opt\/bin\/tensorflow\/compiler\/tests\/stochastic_cast_op_test_cpu.runfiles\/org_tensorflow\/tensorflow\/compiler\/tests\/xla_test.py:106: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nXLA:CPU and XLA:GPU devices are deprecated\r\nW0415 10:17:44.369471 247748062629904 deprecation.py:50] From \/root\/.cache\/bazel\/_bazel_root\/574657b8af23672198530ef061ba4201\/execroot\/org_tensorflow\/bazel-out\/aarch64-opt\/bin\/tensorflow\/compiler\/tests\/stochastic_cast_op_test_cpu.runfiles\/org_tensorflow\/tensorflow\/compiler\/tests\/xla_test.py:106: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nXLA:CPU and XLA:GPU devices are deprecated\r\n[ RUN      ] StochasticCastOpTest.testStochasticCastOpResultProbability_0.125_from_bfloat16_to_int16\r\nINFO:tensorflow:Start test case: StochasticCastOpTest.testStochasticCastOpResultProbability_0.125_from_bfloat16_to_int16\r\nI0415 10:17:44.370704 247748062629904 xla_test.py:231] Start test case: StochasticCastOpTest.testStochasticCastOpResultProbability_0.125_from_bfloat16_to_int16\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1713176264.392647   68553 service.cc:145] XLA service 0xbd1261c1dc00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\nI0000 00:00:1713176264.392683   68553 service.cc:153]   StreamExecutor device (0): Host, Default Version\r\n2024-04-15 10:17:44.398999: I tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\r\nI0000 00:00:1713176264.411515   73786 xla_device.cc:462] XLA_GPU and XLA_CPU devices are deprecated and will be removed in subsequent releases. Instead, use either @tf.function(jit_compile=True) for must-compile semantics, or run with TF_XLA_FLAGS=--tf_xla_auto_jit=2 for auto-clustering best-effort compilation.\r\nI0000 00:00:1713176264.457087   73787 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\nLLVM ERROR: Cannot select: 0xe14f2452b580: v8bf16,ch = masked_load<(load unknown-size from %ir.lsr.iv217, align 2, !alias.scope !4, !noalias !7)> 0xe14f241a0ea0, 0xe14f2452bd60, undef:i64, 0xe14f244faa10, undef:v8bf16\r\n  0xe14f2452bd60: i64,ch = CopyFromReg 0xe14f241a0ea0, Register:i64 %65\r\n    0xe14f2452d220: i64 = Register %65\r\n  0xe14f2452b9e0: i64 = undef\r\n  0xe14f244faa10: v8i16 = AArch64ISD::VASHR 0xe14f24529720, Constant:i32<15>\r\n    0xe14f24529720: v8i16 = AArch64ISD::VSHL 0xe14f2454cb00, Constant:i32<15>\r\n      0xe14f2454cb00: v8i16 = any_extend 0xe14f2452b970\r\n        0xe14f2452b970: v8i8,ch = CopyFromReg 0xe14f241a0ea0, Register:v8i8 %66\r\n          0xe14f2452bdd0: v8i8 = Register %66\r\n      0xe14f245602e0: i32 = Constant<15>\r\n    0xe14f245602e0: i32 = Constant<15>\r\n  0xe14f2452be40: v8bf16 = undef\r\nIn function: parallel_fusion\r\nFatal Python error: Aborted\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","comp:ops"],"created_at":"2024-04-18T09:41:03Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65988"},{"issue_number":185,"repository":"tensorflow\/tensorflow","title":"Segmentation fault in `tf.config.experimental_connect_to_cluster` with `@tf.function`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.0\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen configuring `tf.config.experimental_connect_to_cluster` and using the `@tf.function` decorator on a function, \r\neven though the function's content is `pass`, a `Segmentation fault (core dumped)` still occurs.\r\n\r\nWhen the `@tf.function` decorator is commented out, the program executes with a return code of 0 and no errors occur.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nThis simple code repeats the problem:\r\n\r\nimport tensorflow as tf\r\n\r\ncluster = tf.train.ClusterSpec({\"worker\": [\"worker0.example.com:2222\",\r\n                                           \"worker1.example.com:2222\",\r\n                                           \"worker2.example.com:2222\"],\r\n                                \"ps\": [\"ps0.example.com:2222\",\r\n                                       \"ps1.example.com:2222\"]})\r\n\r\ntf.config.experimental_connect_to_cluster(\r\n    cluster,\r\n    job_name='worker',\r\n    task_index=0\r\n)\r\n\r\n\r\n@tf.function\r\ndef test():\r\n    pass\r\n\r\ntest()\n```\n\n\n### Relevant log output\n\n```shell\nThe error message I got:\r\n\r\n2024-04-15 13:29:11.263846: I tensorflow\/core\/util\/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-04-15 13:29:11.264202: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-04-15 13:29:11.267063: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-04-15 13:29:11.305386: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-04-15 13:29:11.992670: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-04-15 13:29:12.465636: W tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https:\/\/www.tensorflow.org\/install\/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\nE0415 13:29:12.509477560 2432185 server_chttp2.cc:40]        {\"created\":\"@1713187752.509411414\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/ext\/transport\/chttp2\/server\/chttp2_server.cc\",\"file_line\":395,\"referenced_errors\":[{\"created\":\"@1713187752.509406740\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_posix.cc\",\"file_line\":342,\"referenced_errors\":[{\"created\":\"@1713187752.509393568\",\"description\":\"Unable to configure socket\",\"fd\":15,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1713187752.509389969\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]},{\"created\":\"@1713187752.509406103\",\"description\":\"Unable to configure socket\",\"fd\":15,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1713187752.509403589\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\r\n2024-04-15 13:29:12.509541: E tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server\r\n2024-04-15 13:29:12.509869: E tensorflow\/core\/common_runtime\/eager\/context_distributed_manager.cc:450] Could not start gRPC server\r\nSegmentation fault (core dumped)\r\n```\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-16T10:53:07Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65835"},{"issue_number":186,"repository":"tensorflow\/tensorflow","title":"`Segmentation fault` in `tf.raw_ops.RaggedRange` and `tf.ragged.range` when `starts` or `limits` has large (postive\/negative) values.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`Segmentation fault` in `tf.raw_ops.RaggedRange` and `tf.ragged.range` when `starts` or `limits` has large (postive\/negative) values.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nhttps:\/\/colab.research.google.com\/drive\/1c5J_tSNwLdPmi0TXMaJNNhUb-Ha__dw8?usp=sharing\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nsegmentation fault (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-04-16T05:05:06Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65790"},{"issue_number":187,"repository":"tensorflow\/tensorflow","title":"`Segmentation fault` in `tf.raw_ops.CollectiveGather` when `input` is empty.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`Segmentation fault` in `tf.raw_ops.CollectiveGather` when `input` is empty.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1lx42HhXrkx5BPwSxLg-bGuJMMCTEIQyt?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nsegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-16T04:43:40Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65785"},{"issue_number":188,"repository":"tensorflow\/tensorflow","title":"`Segmentation fault` in `tf.raw_ops.LoadAndRemapMatrix` when the value of `num_cols` is too large while `col_remapping` is empty.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the value of `num_cols` is too large while `col_remapping` is empty, `tf.raw_ops.LoadAndRemapMatrix` triggers `Segmentation fault`.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1Eba1FTUMG_as_2FtrSUcAt44Z-LWPX4Z?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nsegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-15T16:19:28Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65732"},{"issue_number":189,"repository":"tensorflow\/tensorflow","title":"`Check failed` in `tf.raw_ops.LoadAndRemapMatrix` when the rank of `ckpt_path` is not equal to the rank of `old_tensor_name`.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the rank of `ckpt_path` is not equal to the rank of `old_tensor_name`, `tf.raw_ops.LoadAndRemapMatrix` triggers `Check fail` error.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1Eba1FTUMG_as_2FtrSUcAt44Z-LWPX4Z?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nCheck failed: NDIMS == dims() (1 vs. 0)Asking for tensor of 1 dimensions from a tensor of 0 dimensions\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-15T16:04:05Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65731"},{"issue_number":190,"repository":"tensorflow\/tensorflow","title":"`Overflow` in `tf.raw_ops.SerializeManySparse` when there are too large values in `sparse_shape`.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n When there are too large values in `sparse_shape`  while `sparse_indices` and `sparse_values` are empty, ``tf.raw_ops.SerializeManySparse`` triggers overflow error.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1itdHdvRiR28lfLLBay-2sUuKSX_IzBPC?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nNon-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 7777777777777777777 with 3, result: -1\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-15T14:50:33Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65727"},{"issue_number":191,"repository":"tensorflow\/tensorflow","title":"Some errors in `tf.raw_ops.SparseTensorDenseMatMul` when there are negative or too large values in `a_shape`.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen there are negative values in `a_shape`, `tf.raw_ops.SparseTensorDenseMatMul` triggers `INVALID_ARGUMENT` error. When there are too large values in `a_shape`, `tf.raw_ops.SparseTensorDenseMatMul` triggers `overflow` error.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nhttps:\/\/colab.research.google.com\/drive\/1_dmg1zT1TuYnmPgtlUWSDq8Gg4eeyq0b?usp=sharing\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nNon-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -1\r\n\r\nor\r\n\r\nNon-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 9223372036854775804 with 3, result: -1\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-15T14:40:25Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65724"},{"issue_number":192,"repository":"tensorflow\/tensorflow","title":"`Check failed` in `f.raw_ops.CropAndResizeGradBoxes` when `boxes` and `box_indices` are empty. ","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`Check failed` in `f.raw_ops.CropAndResizeGradBoxes` when `boxes` and `box_indices` are empty, which causes the program to crash. See the colab link below for details.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nhttps:\/\/colab.research.google.com\/drive\/1jt4YeBZkcM_o_tqxDsIKePHoeFZ-CdCS?usp=sharing\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nCheck failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-04-15T12:48:08Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65721"},{"issue_number":193,"repository":"tensorflow\/tensorflow","title":"Memory leak in tf.data when iterating over Dataset.from_generator","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv1.12.1-108954-g88310ddcbdd 2.17.0-dev20240412\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nDocker: tensorflow\/tensorflow:nightly\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.0rc1\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI discovered what appears to be a memory leak when iterating over a tf.data.Dataset created with from_generator. Process memory usage continues to grow out of hand. The effect only appears in certain combinations of Tensorflow and Python and it may have appeared in Python 3.11. Here are some examples I've tested:\r\n\r\nPython 3.10.10, tensorflow 2.13.0: yes\r\nPython 3.10.10, tensorflow 2.16.1: no\r\nPython 3.10.12, tensorflow v2.15.0-0-g6887368d6d4: no\r\nPython 3.11, tensorflow 2.16.1: yes\r\nPython: 3.11.0rc1, tensorflow v1.12.1-109002-g2c2c0a17f05: yes\r\n\r\nMaybe related to https:\/\/docs.python.org\/3\/whatsnew\/3.11.html#faster-cpython? I thought maybe Python is re-using the memory and not freeing it, but usage grows ridiculously (I noticed it because it started taking up tens of GB in one case) and it seems like it shouldn't with a generator. Odd that 2.13.0 experiences the problem with Python 3.10.10 too though.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1LmdIqWME19GLFG0E7dsCtRtscLLFF89R?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\n2024-04-14 16:12:10.319053: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nPython: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]\r\nTensorFlow: ('v1.12.1-109002-g2c2c0a17f05', '2.17.0-dev20240413')\r\nIterations:        10000 Memory use: 489308160\r\nIterations:        20000 Memory use: 496451584\r\nIterations:        30000 Memory use: 503353344\r\nIterations:        40000 Memory use: 510517248\r\nIterations:        50000 Memory use: 517464064\r\nIterations:        60000 Memory use: 524591104\r\n...\r\nIterations:       980000 Memory use: 1173454848\r\nIterations:       990000 Memory use: 1180618752\r\nIterations:      1000000 Memory use: 1187770368\r\n2024-04-14 16:19:00.851851: I tensorflow\/core\/framework\/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\r\n379\r\n\r\n\r\n\r\nOutput from running valgrind:\r\nPYTHONMALLOC=malloc valgrind --track-origins=yes --leak-check=full --show-leak-kinds=definite --trace-children=yes python .\/tfdata_test.py\r\nI stopped it at about 80,000 iterations because it's much slower running under valgrind\r\n==324== 24,434,160 (4,660,032 direct, 19,774,128 indirect) bytes in 72,813 blocks are definitely lost in loss record 188,561 of 188,562\r\n==324==    at 0x4848899: malloc (in \/usr\/libexec\/valgrind\/vgpreload_memcheck-amd64-linux.so)\r\n==324==    by 0x4D4AC1: ??? (in \/usr\/bin\/python3.11)\r\n==324==    by 0x5BA613: ??? (in \/usr\/bin\/python3.11)\r\n==324==    by 0x5BB4C4: ??? (in \/usr\/bin\/python3.11)\r\n==324==    by 0x4FE1B5: _PyEval_EvalFrameDefault (in \/usr\/bin\/python3.11)\r\n==324==    by 0x531822: _PyFunction_Vectorcall (in \/usr\/bin\/python3.11)\r\n==324==    by 0x530FB8: ??? (in \/usr\/bin\/python3.11)\r\n==324==    by 0x5DA693: _PyObject_CallMethod_SizeT (in \/usr\/bin\/python3.11)\r\n==324==    by 0x26C7748A: _descriptor_from_pep3118_format (in \/usr\/local\/lib\/python3.11\/dist-packages\/numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so)\r\n==324==    by 0x26C8CBD5: _array_from_buffer_3118.part.0 (in \/usr\/local\/lib\/python3.11\/dist-packages\/numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so)\r\n==324==    by 0x26C8DF4E: _array_from_array_like (in \/usr\/local\/lib\/python3.11\/dist-packages\/numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so)\r\n==324==    by 0x26C6FCD4: PyArray_DiscoverDTypeAndShape_Recursive (in \/usr\/local\/lib\/python3.11\/dist-packages\/numpy\/core\/_multiarray_umath.cpython-311-x86_64-linux-gnu.so)\r\n==324==\r\n==324== 26,302,320 bytes in 73,062 blocks are definitely lost in loss record 188,562 of 188,562\r\n==324==    at 0x484DA83: calloc (in \/usr\/libexec\/valgrind\/vgpreload_memcheck-amd64-linux.so)\r\n==324==    by 0x625128: ??? (in \/usr\/bin\/python3.11)\r\n==324==    by 0x6250BA: PyThreadState_New (in \/usr\/bin\/python3.11)\r\n==324==    by 0x64E44F: PyGILState_Ensure (in \/usr\/bin\/python3.11)\r\n==324==    by 0xA597C58: tensorflow::PyFuncOp::Compute(tensorflow::OpKernelContext*) (in \/usr\/local\/lib\/python3.11\/dist-packages\/tensorflow\/python\/_pywrap_tensorflow_internal.so)\r\n==324==    by 0x92F0F3F: tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (in \/usr\/local\/lib\/python3.11\/dist-packages\/tensorflow\/libtensorflow_framework.so.2)\r\n==324==    by 0x927A539: tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::Process(tensorflow::SimplePropagatorState::TaggedNode const&, long) (in \/usr\/local\/lib\/python3.11\/dist-packages\/tensorflow\/libtensorflow_framework.so.2)\r\n==324==    by 0x9A3D39D: std::_Function_handler<void (), std::_Bind<tensorflow::data::RunnerWithMaxParallelism(std::function<void (std::function<void ()>)>, int)::$_0::operator()(std::function<void (std::function<void ()>)> const&, std::function<void ()>) const::{lambda(std::function<void ()> const&)#1} (std::function<void ()>)> >::_M_invoke(std::_Any_data const&) (in \/usr\/local\/lib\/python3.11\/dist-packages\/tensorflow\/libtensorflow_framework.so.2)\r\n==324==    by 0x9C2299F: Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int) (in \/usr\/local\/lib\/python3.11\/dist-packages\/tensorflow\/libtensorflow_framework.so.2)\r\n==324==    by 0x9C223D0: void std::__invoke_impl<void, tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}&>(std::__invoke_other, tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}&) (in \/usr\/local\/lib\/python3.11\/dist-packages\/tensorflow\/libtensorflow_framework.so.2)\r\n==324==    by 0x960E55A: tsl::(anonymous namespace)::PThread::ThreadFn(void*) (in \/usr\/local\/lib\/python3.11\/dist-packages\/tensorflow\/libtensorflow_framework.so.2)\r\n==324==    by 0x4A27AC2: start_thread (pthread_create.c:442)\r\n==324==\r\n==324== LEAK SUMMARY:\r\n==324==    definitely lost: 30,984,576 bytes in 146,328 blocks\r\n==324==    indirectly lost: 19,783,920 bytes in 72,736 blocks\r\n==324==      possibly lost: 136,684,089 bytes in 1,082,632 blocks\r\n==324==    still reachable: 17,341,568 bytes in 182,582 blocks\r\n==324==                       of which reachable via heuristic:\r\n==324==                         stdstring          : 341 bytes in 8 blocks\r\n==324==                         newarray           : 104,387 bytes in 320 blocks\r\n==324==                         multipleinheritance: 7,168 bytes in 24 blocks\r\n==324==         suppressed: 0 bytes in 0 blocks\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.16"],"created_at":"2024-04-14T19:02:53Z","comments":5,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65675"},{"issue_number":194,"repository":"tensorflow\/tensorflow","title":"Some `Check Failed` errors in `tf.raw_ops.MatrixSetDiagV2`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSome `Check Failed` errors in `tf.raw_ops.MatrixSetDiagV2`, which causes the program to crash. See the colab link below for details.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1-yHRqhkbV3J77AaBD4GXYh3nSF49Inep?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\n# Check failed: d < dims() (1 vs. 1)\r\n# Check failed: d < dims() (2 vs. 2)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-14T17:41:03Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65674"},{"issue_number":195,"repository":"tensorflow\/tensorflow","title":"Some `Check Failed` errors in `tf.raw_ops.MatrixSetDiagV3`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSome `Check Failed` errors in `tf.raw_ops.MatrixSetDiagV3`, which causes the program to crash. See the colab link below for details.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/11e72xipp_lz6fHcp7bN2MQy3Kw5KqHpL?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\n# Check failed: d < dims() (1 vs. 1)\r\n# Check failed: d < dims() (2 vs. 2)\n```\n","labels":["awaiting review","type:bug","comp:ops","TF 2.15"],"created_at":"2024-04-14T17:33:52Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65673"},{"issue_number":196,"repository":"tensorflow\/tensorflow","title":"`Segmentation Fault` in `tf.raw_ops.TensorListScatter` and `tf.raw_ops.TensorListScatterV2` when the value of `indices` is too large.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the value of `indices` is too large,  `tf.raw_ops.TensorListScatter` and `tf.raw_ops.TensorListScatterV2` triggers `Segmentation Fault` due to RAM being full.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1BOVVbofLSB2x_chg_WuFIdTWZXS9hiw0?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nsegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-04-14T17:14:00Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65672"},{"issue_number":197,"repository":"tensorflow\/tensorflow","title":"`Check failed` in `tf.raw_ops.TensorScatterSub` and `tf.tensor_scatter_nd_sub` when the rank of `indices` > 2.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`Check failed` in `tf.raw_ops.TensorScatterSub` and `tf.tensor_scatter_nd_sub` when the rank of `indices` > 2, which causes the program to crash. See the colab link below for details.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1wqe_qe71oLucYiq1WS8s9GkZiVsNlaMz?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nCheck failed: d < dims() (1 vs. 1)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-14T16:41:30Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65670"},{"issue_number":198,"repository":"tensorflow\/tensorflow","title":"`Check failed` in `tf.raw_ops.TensorScatterUpdate` and `tf.tensor_scatter_nd_update` when the rank of `indices` > 2.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`Check failed` in `tf.raw_ops.TensorScatterUpdate` and `tf.tensor_scatter_nd_update` when the rank of `indices` > 2, which causes the program to crash. See the colab link below for details.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1PLVxv-jQDZ15P4lrd3AhyVU-RAesammI?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nCheck failed: d < dims() (1 vs. 1)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-04-14T16:11:54Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65667"},{"issue_number":199,"repository":"tensorflow\/tensorflow","title":"`Check failed` in `tf.raw_ops.ScatterNd` and `tf.scatter_nd` when the rank of `indices` > 2. ","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`Check failed` in `tf.raw_ops.ScatterNd` and `tf.scatter_nd` when the rank of `indices` > 2, which causes the program to crash. See the colab link below for details.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nhttps:\/\/colab.research.google.com\/drive\/1rr8w5IOzVdjs07YECnyOsnFu3ouUFkBs?usp=sharing\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nCheck failed: d < dims() (1 vs. 1)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-14T15:25:42Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65666"},{"issue_number":200,"repository":"tensorflow\/tensorflow","title":"`segmentation fault` in `tf.raw_ops.AddManySparseToTensorsMap` when `sparse_indices` and `sparse_values` are empty and `sparse_shape` is too large.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`segmentation fault` in `tf.raw_ops.AddManySparseToTensorsMap` when `sparse_indices` and `sparse_values` are empty and `sparse_shape` is too large, which causes the program to crash. See the colab link below for details.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nhttps:\/\/colab.research.google.com\/drive\/1KNuiyCMsSo32teT3lNwYyYbQkD9tEBET?usp=sharing\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nsegmentation fault (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-04-14T04:28:10Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65647"},{"issue_number":201,"repository":"tensorflow\/tensorflow","title":"`Check failed` in `tf.raw_ops.UnbatchGrad` when the input of `original_input` is empty.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen the input of `original_input` is empty,  `tf.raw_ops.UnbatchGrad` will trigger `Check failed` of the input tensor dimension.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nhttps:\/\/colab.research.google.com\/drive\/1ekseH5mJBvt3_SeK4MuAAM47U8qi5BEB?usp=sharing\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nCheck failed: d < dims() (0 vs. 0)\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-13T18:25:26Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65636"},{"issue_number":202,"repository":"tensorflow\/tensorflow","title":"Some `Check Failed` errors in `tf.raw_ops.Unbatch`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSome `Check Failed` errors in `tf.raw_ops.Unbatch`, which causes the program to crash. See the colab link below for details.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1JRw_-UwodqwPx87naMM04nnFLaLaZcXn?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nCheck failed: size >= 0 (0 vs. -1) Aborted (core dumped)\r\nCheck failed: d < dims() (1 vs. 1) Aborted (core dumped)\r\nCheck failed: new_num_elements == NumElements() (6 vs. 18) Aborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-04-13T17:50:28Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65633"},{"issue_number":203,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.Unbatch aborts with \"Check failed: d < dims()\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython 3.11.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.Unbatch` aborts with \"Check failed: d < dims()\"\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntf.raw_ops.Unbatch(batched_tensor=tf.random.uniform([], dtype=tf.dtypes.float32, maxval=100000000),batch_index=tf.random.uniform([1], dtype=tf.dtypes.int64, minval=-100000000, maxval=1000000000),id=tf.random.uniform([4], dtype=tf.dtypes.int64, minval=-100000000, maxval=1000000000),timeout_micros=30,container=\"N\/J,\",shared_name=\"&o0n7W\",)\n```\n\n\n### Relevant log output\n\n```shell\n2024-04-10 06:36:02.435578: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-10T06:56:36Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65379"},{"issue_number":204,"repository":"tensorflow\/tensorflow","title":"[DOCS] Missing complex input for Round op","description":"### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nMacOS Sonoma\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhile testing the `Round` operation, I noticed an inconsistency with the [documentation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/Round). On the official page, it's described that a complex tensor can be passed as input. However, when I try to replicate this in my environment, it doesn't work as expected. I have to apply the `Round` operation separately to the real and imaginary parts of the tensor to achieve the desired result.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nx = tf.constant([1+2j, 3+4j])\r\nrounded = tf.raw_ops.Round(x=x)\n```\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/Users\/lucatam\/Documents\/OpenSourceProject\/openvino\/opvenv\/lib\/python3.9\/site-packages\/tensorflow\/python\/util\/tf_export.py\", line 403, in wrapper\r\n    return f(**kwargs)\r\n  File \"\/Users\/lucatam\/Documents\/OpenSourceProject\/openvino\/opvenv\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/gen_math_ops.py\", line 8770, in round\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"\/Users\/lucatam\/Documents\/OpenSourceProject\/openvino\/opvenv\/lib\/python3.9\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 5883, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Round}} = Round[T=DT_COMPLEX128]\r\nAll kernels registered for op Round:\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n [Op:Round] name:\n```\n","labels":["type:docs-bug","awaiting review","type:bug","comp:ops","TF 2.16"],"created_at":"2024-04-09T10:54:25Z","comments":1,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/65317"},{"issue_number":205,"repository":"tensorflow\/tensorflow","title":"`SystemError` in `tf.ensure_shape` and `tf.compat.v1.ensure_shape` when `dtype` of `shape` is `tf.uint64` and its value is too large.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen the `dtype` of `shape` in `tf.ensure_shape` or `tf.v1.ensure_shape` is `tf.uint64` and its value is close to 2^64,`SystemError` and `OverflowError` will be triggered.\r\nTaking `shape = tf.constant([18446743219011059112, 1], dtype=tf.uint64)` as an example, in eager mode, these APIs will trigger a `SystemError` when it is called. I think it is a bug.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nhttps:\/\/colab.research.google.com\/drive\/1sy4yzSx-n6HqS9oMvPv46WasMtMsCU_R?usp=sharing\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nSystemError: <built-in function isinstance> returned a result with an exception set\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-04-02T12:46:59Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/64917"},{"issue_number":206,"repository":"tensorflow\/tensorflow","title":"Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed. Aborted (core dumped)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0.post1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nRocky Linux 8.9\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n8.9.2.26\n\n### GPU model and memory\n\n NVIDIA A100-SXM4-80GB \n\n### Current behavior?\n\nI am trying to run a simple denoising autoencoder. my training data and label data are 900 samples of healpy maps with nside 64 resolution, loaded as numpy array. After normalising the maps, I used tf.data.Dataset.from_tensor_slices, to create dataset. when I used random noise to create these maps and ran on jupyter notebook, although took ages to initiate training after doing model.fit(), but it did run and produced some result. knowing that the model works, I tried to run on GPU with real data. this is where the issue started.  it shows the following error: Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed. Aborted (core dumped), and the process stops. \n\n### Standalone code to reproduce the issue\n\n```shell\nHere is a colab link:\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/1odbf3gQT9h-DI6zdh5e1llmG9zhjY45l?usp=sharing\r\n\r\nIt runs on colab. but it doesn't run in the terminal.\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-27 12:14:07.288975: F external\/local_tsl\/tsl\/platform\/default\/env.cc:74] Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed.\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.15"],"created_at":"2024-03-28T16:15:23Z","comments":4,"reactions":3,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/64681"},{"issue_number":207,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) with `tf.raw_ops.LoadAndRemapMatrix`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, `tf.raw_ops.tf.raw_ops.LoadAndRemapMatrix` encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nnum_rows = 7\r\nnum_cols = 16\r\nmax_rows_in_memory = -1\r\nckpt_path = tf.constant(\"\/tmp\", shape=[1], dtype=tf.string)\r\nold_tensor_name = tf.constant(\"\/tmp\", shape=[1], dtype=tf.string)\r\nrow_remapping = tf.constant(-1, shape=[7], dtype=tf.int64)\r\ncol_remapping = tf.constant(0, shape=[], dtype=tf.int64)\r\ninitializing_values = tf.constant(42, shape=[112], dtype=tf.float32)\r\ntf.raw_ops.LoadAndRemapMatrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, row_remapping=row_remapping, col_remapping=col_remapping, initializing_values=initializing_values, num_rows=num_rows, num_cols=num_cols, max_rows_in_memory=max_rows_in_memory)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-28 07:29:28.849010: F tensorflow\/core\/framework\/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 0)Asking for tensor of 1 dimensions from a tensor of 0 dimensions\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-03-28T07:38:24Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/64655"},{"issue_number":208,"repository":"tensorflow\/tensorflow","title":"Conv2D computes wrongly in Windows OS","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.16\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nWindows 10\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nOn Windows OS, Conv2D generates a wrong output in some cases, while performs correctly on some others.\r\nThis error does not occur on Linux OS, even with the same code.\r\n\r\nAn wrong execution example:\r\n![\u56fe\u7247](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/97206823\/51fab71a-9bc6-42a9-ab67-19a6c9ec50d0)\r\n\r\nYou can tell that the result `l(x)` has a wrong shape.\r\n\r\nI notice that an exisiting issue #63860 points out the similar error in Conv3D. I guess Conv2D and Conv3D have similar problem since they have the same parent class BaseConv.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\n#This test case works fine on linux OS, while goes wrongly on Windows.\r\n\r\n\r\nfrom keras.layers import Conv2D\r\nimport numpy as np\r\n\r\nx=np.random.rand(1,2,2,1)\r\nprint(l(x).shape)\r\nprint(l.compute_output_shape(x.shape))\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nTensorShape([1, 2, 2, 1])\r\n(1, 0, 0, 1)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","subtype:windows","subtype:cpu-intel","TF 2.16"],"created_at":"2024-03-25T07:25:12Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/64396"},{"issue_number":209,"repository":"tensorflow\/tensorflow","title":"Improper use of PluggableDevice data pointer","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Arch\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorListFromTensor op assumes that the data pointer from a PluggableDevice tensor is to virtual address space and can add an offset to it.\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/39cb3719012c249ba40bffc4cebaad66311a4669\/tensorflow\/core\/kernels\/list_kernels.h#L805-L821\r\n\r\nThe assumption that all PluggableDevices set the data pointer of Tensor to be virtual address space is false.\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/39cb3719012c249ba40bffc4cebaad66311a4669\/tensorflow\/c\/experimental\/stream_executor\/stream_executor.h#L144-L146\r\n\r\nUnfortunately, this is not the only occurrence of a bug like this. I have seen similar issues with distributed training offsetting PluggableDevice data pointers on copy as well but I have not taken the time to look into it yet.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.debugging.set_log_device_placement(True)\r\ntf.config.run_functions_eagerly(False)\r\na = np.zeros((8, 1), dtype=np.float32)\r\n\r\nwith tf.device(\"MY_DEVICE:0\"):\r\n    b = tf.raw_ops.TensorListFromTensor(tensor=a, element_shape=[1])\r\n\r\nprint(b)\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:runtime","comp:core","TF 2.15"],"created_at":"2024-03-23T23:27:55Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/64372"},{"issue_number":210,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.ApproximateEqual\/Erfinv fail support a few data types, inconsistent with the documentation","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nHi, I want to report two issues: both tf.raw_ops.ApproximateEqual and tf.raw_ops.Erfinv can only support float32, float64, and double and fail on other datatype with the error message `NotFoundError: Could not find device for node: {{node xxx}} = xxx[T=DT_XXX]`\r\n\r\nThese behaviors are inconsistent with related documentations of these two APIs:\r\n\r\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/ApproximateEqual\r\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/Erfinv\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\ndtype_list = [\r\n    'bfloat16', 'bool', 'complex128', 'complex64',\r\n    'double', 'float16', 'float32',\r\n    'float64', 'half', 'int16', 'int32', 'int64', 'int8',\r\n    'uint16', 'uint32', 'uint64', 'uint8'\r\n]\r\nfor dtype in dtype_list:\r\n    x = tf.constant(np.random.randint(-50, 50, ()), dtype=dtype)\r\n    y = tf.constant(np.random.randint(-50, 50, ()), dtype=dtype)\r\n    try:\r\n        out = tf.raw_ops.ApproximateEqual(x=x,y=y)\r\n        print(f\"ApproximateEqual Success on dtype: {dtype}\")\r\n    except:\r\n        print(f\"ApproximateEqual Fail on dtype: {dtype}\")\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\ndtype_list = [\r\n    'bfloat16', 'bool', 'complex128', 'complex64',\r\n    'double', 'float16', 'float32',\r\n    'float64', 'half', 'int16', 'int32', 'int64', 'int8',\r\n    'uint16', 'uint32', 'uint64', 'uint8'\r\n]\r\nfor dtype in dtype_list:\r\n    x = tf.constant(np.random.randint(-50, 50, ()), dtype=dtype)\r\n    try:\r\n        out = tf.raw_ops.Erfinv(x=x)\r\n        print(f\"Erfinv Success on dtype: {dtype}\")\r\n    except:\r\n        print(f\"Erfinv Fail on dtype: {dtype}\")\n```\n\n\n### Relevant log output\n\n```shell\nApproximateEqual Fail on dtype: bfloat16\r\nApproximateEqual Fail on dtype: bool\r\nApproximateEqual Fail on dtype: complex128\r\nApproximateEqual Fail on dtype: complex64\r\nApproximateEqual Success on dtype: double\r\nApproximateEqual Fail on dtype: float16\r\nApproximateEqual Success on dtype: float32\r\nApproximateEqual Success on dtype: float64\r\nApproximateEqual Fail on dtype: half\r\nApproximateEqual Fail on dtype: int16\r\nApproximateEqual Fail on dtype: int32\r\nApproximateEqual Fail on dtype: int64\r\nApproximateEqual Fail on dtype: int8\r\nApproximateEqual Fail on dtype: uint16\r\nApproximateEqual Fail on dtype: uint32\r\nApproximateEqual Fail on dtype: uint64\r\nApproximateEqual Fail on dtype: uint8\r\n\r\nErfinv Fail on dtype: bfloat16\r\nErfinv Fail on dtype: bool\r\nErfinv Fail on dtype: complex128\r\nErfinv Fail on dtype: complex64\r\nErfinv Success on dtype: double\r\nErfinv Fail on dtype: float16\r\nErfinv Success on dtype: float32\r\nErfinv Success on dtype: float64\r\nErfinv Fail on dtype: half\r\nErfinv Fail on dtype: int16\r\nErfinv Fail on dtype: int32\r\nErfinv Fail on dtype: int64\r\nErfinv Fail on dtype: int8\r\nErfinv Fail on dtype: uint16\r\nErfinv Fail on dtype: uint32\r\nErfinv Fail on dtype: uint64\r\nErfinv Fail on dtype: uint8\n```\n","labels":["awaiting review","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-22T05:21:16Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/64256"},{"issue_number":211,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in tf.raw_ops.SparseMatrixZeros","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.13\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n7.5.0\n\n### CUDA\/cuDNN version\n\n12.0\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, `tf.raw_ops.SparseMatrixZeros` encounters \"Aborted (core dumped)\".\r\nHowever, the parameters `dense_shape` and `type` meet the requirements in the API documentation. (https:\/\/tensorflow.google.cn\/api_docs\/python\/tf\/raw_ops\/SparseMatrixZeros)\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\ndense_shape=tf.constant(value=np.random.randint(0,100,size=(1, 1)), shape=(1, 1), dtype=tf.int64)\r\ntype=tf.float64\r\nname=None\r\ntf.raw_ops.SparseMatrixZeros(dense_shape=dense_shape, type=type, name=name)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-19 02:18:57.223847: F tensorflow\/core\/framework\/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2024-03-19T02:26:44Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63937"},{"issue_number":212,"repository":"tensorflow\/tensorflow","title":"Conv3D Operation Error on Windows platform","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf-2.15.0, tf-2.16.0, tf-2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWindows 10\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI found an output shape inconsistent in Conv3D layer:\r\n```\r\nfrom tensorflow.keras.layers import Conv3D\r\nimport numpy as np\r\n\r\nx=np.random.rand(3,5,5,5,4)\r\nl=Conv3D(5,[2,2,3],[1,1,1],'valid','channels_first',[2,2,2],1)\r\nprint(l(x).shape)\r\nprint(l.compute_output_shape(x.shape))\r\n```\r\nThe output is:\r\n```\r\n(3, 5, 5, 5, 4)\r\n(3, 5, 3, 3, 0)\r\n```\r\n\r\nHowever, I failed to reproduce it on google colab. After checking the package and re-running it on a Linux machine and another Windows machine, I found this bug may come from the package 'tensorflow-intel', which exists only on Windows platform.\r\n\r\nCould you please check it?\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.keras.layers import Conv3D\r\nimport numpy as np\r\n\r\nx=np.random.rand(3,5,5,5,4)\r\nl=Conv3D(5,[2,2,3],[1,1,1],'valid','channels_first',[2,2,2],1)\r\nprint(l(x).shape)\r\nprint(l.compute_output_shape(x.shape))\r\n```\n```\n\n\n### Relevant log output\n\n```shell\n(3, 5, 5, 5, 4)\r\n(3, 5, 3, 3, 0)\n```\n","labels":["stat:awaiting tensorflower","type:bug","subtype:cpu-intel","TF 2.15"],"created_at":"2024-03-18T12:26:37Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63860"},{"issue_number":213,"repository":"tensorflow\/tensorflow","title":"tf.linalg.svd([inf, -inf]) throws internal error logs to the console, and generates inconsistent results on different dtypes.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux-5.14.0-362.18.1.el9_3.x86_64-x86_64-with-glibc2.34\n\n### Mobile device\n\nAlmaLinux 9\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nAn internal error log was thrown to the console.\r\nAlso, we found that API generates inconsistent results on different input dtypes.\r\nThe same error log has appeared in #56204, but I am not sure whether we are talking about the same problem.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninput = tf.constant([[np.inf, -np.inf]], dtype=tf.complex64)\r\nout = tf.linalg.svd(input, full_matrices=False, compute_uv=False) # 0\r\n\r\ninput = tf.constant([[np.inf, -np.inf]], dtype=tf.float64)\r\nout = tf.linalg.svd(input, full_matrices=False, compute_uv=False) # nan\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-18 02:28:14.593059: E .\/tensorflow\/core\/kernels\/linalg\/svd_op_impl.h:110] Eigen::BDCSVD failed with error code 3\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-17T18:58:12Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63852"},{"issue_number":214,"repository":"tensorflow\/tensorflow","title":"Adding TensorFlow Hub KerasLayer to Sequential Model Raises ValueError","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.1\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 22.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython 3.11.0rc1\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.3.0\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI can execute the following code without any issues in TensorFlow 2.15.0 and TensorFlow Hub 1.16.1. However, when I upgrade the TensorFlow version to 2.16.0 or above, I encounter an error stating that `KerasLayer` cannot be added to the Sequential model.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport tensorflow_hub as hub\r\n\r\nimage_size = 224\r\nURL = \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b0\/feature-vector\/1\"\r\n\r\nmodel = tf.keras.Sequential([\r\n        hub.KerasLayer(URL, input_shape=(image_size, image_size, 3))\r\n])\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[29], line 1\r\n----> 1 model = tf.keras.Sequential([\r\n      2         feature_extractor,\r\n      3         tf.keras.layers.Dense(2, activation = 'softmax')\r\n      4 ])\r\n      6 model.build([None, image_size, image_size, 3])\r\n      7 model.summary()\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/models\/sequential.py:70, in Sequential.__init__(self, layers, trainable, name)\r\n     68 if layers:\r\n     69     for layer in layers:\r\n---> 70         self.add(layer, rebuild=False)\r\n     71     self._maybe_rebuild()\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/models\/sequential.py:92, in Sequential.add(self, layer, rebuild)\r\n     90         layer = origin_layer\r\n     91 if not isinstance(layer, Layer):\r\n---> 92     raise ValueError(\r\n     93         \"Only instances of `keras.Layer` can be \"\r\n     94         f\"added to a Sequential model. Received: {layer} \"\r\n     95         f\"(of type {type(layer)})\"\r\n     96     )\r\n     97 if not self._is_layer_name_unique(layer):\r\n...\r\n    101         \"the name of a layer in this model. Update the `name` argument \"\r\n    102         \"to pass a unique name.\"\r\n    103     )\r\n\r\nValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x7a4ac7e30f40> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)\n```\n","labels":["type:bug","comp:keras","awaiting PR merge","TF 2.16"],"created_at":"2024-03-17T11:08:55Z","comments":17,"reactions":3,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63849"},{"issue_number":215,"repository":"tensorflow\/tensorflow","title":"A check fail can be triggered in tf.raw_ops.StatefulPartitionedCall","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIn the latest version of TensorFlow, the following code can trigger a crash in `tf.raw_ops.StatefulPartitionedCall` due to check-fail.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput_data = tf.constant([[1, 2, 3], [4, 5, 6]])\r\n\r\n@tf.function\r\ndef my_function(inputs):\r\n    return tf.reduce_sum(inputs, axis=1)\r\n\r\ncallable_function = tf.function(my_function).get_concrete_function(tf.TensorSpec(shape=None, dtype=tf.int32))\r\n\r\noutput = tf.raw_ops.StatefulPartitionedCall(args=[input_data], Tout=None, f=callable_function)\r\n\r\nprint(output)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-14 15:58:52.804195: F tensorflow\/core\/framework\/op_kernel.cc:989] Check failed: index < outputs_.size() (0 vs. 0)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-14T15:59:00Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63721"},{"issue_number":216,"repository":"tensorflow\/tensorflow","title":"A check fail can be triggered in tf.raw_ops.ResourceSparseApplyKerasMomentum","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIn the latest version of TensorFlow, the following code can trigger a crash in `tf.raw_ops.ResourceSparseApplyKerasMomentum` due to check-fail.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nvar = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.complex64)  \r\naccum = tf.Variable([[0.1, 0.2], [0.3, 0.4]])\r\nlr = 0.01\r\ngrad = tf.constant([[0.1, 0.2], [0.3, 0.4]])\r\nindices = tf.constant([0, 1])  \r\nmomentum = 0.9\r\n\r\nresult = tf.raw_ops.ResourceSparseApplyKerasMomentum(var=var.handle, accum=accum.handle, lr=lr, grad=grad, indices=indices, momentum=momentum)\r\n\r\nprint(result)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-14 15:57:08.452454: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64\r\nAborted (core dumped)\n```\n","labels":["awaiting review","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-14T15:57:19Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63720"},{"issue_number":217,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.ResourceApplyProximalAdagrad: Aborted (core dumped)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, `tf.raw_ops.ResourceApplyProximalAdagrad` encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nvar = tf.Variable([1.0, 2.0, 3.0])\r\naccum = tf.Variable([0.1, 0.2, 0.3])\r\nlr = 0.01\r\nl1 = 0.1\r\nl2 = 0.01\r\ngrad = tf.constant([0.1, 0.2, 0.3], dtype=tf.float64)\r\n\r\nvar_resource = tf.raw_ops.VarHandleOp(dtype=var.dtype, shape=var.shape)\r\naccum_resource = tf.raw_ops.VarHandleOp(dtype=accum.dtype, shape=accum.shape)\r\n\r\nassign_var = tf.raw_ops.AssignVariableOp(resource=var_resource, value=var)\r\nassign_accum = tf.raw_ops.AssignVariableOp(resource=accum_resource, value=accum)\r\n\r\nresult = tf.raw_ops.ResourceApplyProximalAdagrad(var=var_resource, accum=accum_resource, lr=lr, l1=l1, l2=l2, grad=grad)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-14 05:57:20.787563: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (1 vs. 2) double expected, got float\r\nAborted (core dumped)\n```\n","labels":["awaiting review","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-14T05:57:34Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63697"},{"issue_number":218,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.ResourceApplyMomentum: Aborted (core dumped)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, `tf.raw_ops.ResourceApplyMomentum` encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nvar = tf.Variable([1.0, 2.0, 3.0])\r\naccum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) \r\nlr = 0.01\r\ngrad = tf.constant([0.1, 0.2, 0.3])\r\nmomentum = 0.9\r\n\r\noutput = tf.raw_ops.ResourceApplyMomentum(var=var.handle, accum=accum.handle, lr=lr, grad=grad, momentum=momentum)\r\n\r\nprint(output)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-14 05:55:18.324852: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-14T05:55:30Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63696"},{"issue_number":219,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.QuantizeAndDequantizeV3: Aborted (core dumped)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, `tf.raw_ops.QuantizeAndDequantizeV3` encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput_data = tf.constant([1.5, 2.5, 3.5, 4.5])\r\ninput_min = tf.constant([[[0.0, 0.0], [0.0, 0.0]], [[0.0, 0.0], [0.0, 0.0]], [[0.0, 0.0], [0.0, 0.0]]])\r\n\r\ninput_max = tf.constant(5.0)\r\nnum_bits = tf.constant(8)\r\nsigned_input = True\r\nrange_given = True\r\nnarrow_range = False\r\naxis = -1\r\n\r\noutput_data = tf.raw_ops.QuantizeAndDequantizeV3(input=input_data, input_min=input_min, input_max=input_max, num_bits=num_bits, signed_input=signed_input, range_given=range_given, narrow_range=narrow_range, axis=axis)\r\n\r\nprint(output_data)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-14 05:50:03.159345: F tensorflow\/core\/framework\/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-14T05:50:58Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63693"},{"issue_number":220,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.ExperimentalDatasetToTFRecord: Aborted (core dumped)","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nUnder specific input, `tf.raw_ops.ExperimentalDatasetToTFRecord` encounters \"Aborted (core dumped)\".\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\ninput_data = tf.data.Dataset.range(10).map(lambda x: tf.strings.as_string(x))\r\ninput_data = input_data.batch(2)  \r\n\r\nfilename = tf.constant(\"output.tfrecord\")\r\ncompression_type = tf.constant(\"GZIP\")\r\ntf.raw_ops.ExperimentalDatasetToTFRecord(input_dataset=input_data._variant_tensor, filename=filename, compression_type=compression_type)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-03-14 05:47:55.415453: F tensorflow\/core\/framework\/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-03-14T05:48:39Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63691"},{"issue_number":221,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.DatasetToTFRecord: Aborted (core dumped)","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nUnder specific input, `tf.raw_ops.DatasetToTFRecord` encounters \"Aborted (core dumped)\".\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\ninput_data = [[1, 2], [3, 4], [5, 6]]\r\n\r\ninput_data_strings = [[str(d) for d in inner_list] for inner_list in input_data]\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(input_data_strings)\r\n\r\nfilename = \"output.tfrecord\"\r\n\r\ntf.raw_ops.DatasetToTFRecord(input_dataset=dataset._variant_tensor, filename=filename, compression_type=\"\")\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-03-14 05:41:32.476705: F tensorflow\/core\/framework\/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-14T05:41:47Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63689"},{"issue_number":222,"repository":"tensorflow\/tensorflow","title":"tf.transpose lead to program abortion on a normal value while the perm contains -1 ","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nHi, the tf.transpose will lead to a program abortion when receiving an empty input and perm is [-1, -2].\r\nThe traceback indicate a check fail:\r\n```\r\ntensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\n```\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput = tf.constant([[]])\r\ndim0 = -2\r\ndim1 = -1\r\ntf.transpose(input, perm=[dim1, dim0])\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-14T05:16:01Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63685"},{"issue_number":223,"repository":"tensorflow\/tensorflow","title":"_pywrap_tensorflow_internal.so library contains RUNPATH entries that look like C++ symbols","description":"### Issue type\r\n\r\nBug\r\n\r\n### TensorFlow version\r\n\r\n2.13 - 2.16\r\n\r\n### Python version\r\n\r\n3.11, 3.12\r\n\r\n### Current behavior?\r\n\r\nThe RUNPATH in the wheel shared objects is weirdly mangled:\r\n\r\n```\r\n$ wget https:\/\/files.pythonhosted.org\/packages\/0d\/58\/d302d19f06d028ad6c2e33c449ef9813058a16dc35c0ca9580740e0dc9ed\/tensorflow-2.16.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n$ unzip *.whl\r\n$ objdump -x tensorflow\/python\/_pywrap_tensorflow_internal.so | grep RUNPATH\r\n  RUNPATH              $ORIGIN\/..\/..\/_solib_local\/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so_Ucclib___Utensorflow:$ORIGIN\/_pywrap_tensorflow_internal.so.runfiles\/org_tensorflow\/_solib_local\/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so_Ucclib___Utensorflow:$ORIGIN\/..\/..\/_solib_local\/_Utensorflow:$ORIGIN\/_pywrap_tensorflow_internal.so.runfiles\/org_tensorflow\/_solib_local\/_Utensorflow:$ORIGIN\/:$ORIGIN\/..:$ORIGIN\/..\/..\/nvidia\/nccl\/lib:$ORIGIN\/..\/nvidia\/nccl\/lib:$ORIGIN\/..\/..\/tensorflow\/tsl\/python\/lib\/core\r\n```\r\n\r\nIt's not just that one, almost every SO is affected, see:\r\n```\r\nfind tensorflow\/ -name *.so -exec objdump -x {} \\; | grep RUNPATH\r\n```\r\n\r\nUnder a custom NixOS build, this mess seems to further confuse follow-on patchelf invocations, which is requiring additional hackery to correct. Possibly related to #34991; the objdump in that ticket shows the same kind of RUNPATH.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nDownload any wheel from PyPI, examine the `so`s contained within for their RUNPATH values.\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","TF 2.16"],"created_at":"2024-03-13T19:50:40Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63646"},{"issue_number":224,"repository":"tensorflow\/tensorflow","title":"Incorrect result when using tf.norm to conduct p-norm computation on float16 precision","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\ntf.norm outputs incorrect result under float16 computation when the ord is large. Here is the code to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\ninput = tf.constant(1.751, dtype=\"float16\")\r\np = 144111111\r\nprint(tf.norm(input, p))   # tf.Tensor(1.0, shape=(), dtype=float16)\r\n```\r\nInstead, if I change the dtype to 'float32', tf.norm outputs `inf`. It seems that tf.norm overflows during this computation https:\/\/github.com\/tensorflow\/tensorflow\/blob\/9ce1d5f2f7e5e8866a55633c182cc2dda1a2d6b8\/tensorflow\/python\/ops\/linalg_ops.py#L784\r\n\r\nAlthough fixing this issue by letting tf.norm also outputs `inf` in `float16` (which is consistent with the result in `float32`. I am suggesting another possible fix according to this post https:\/\/timvieira.github.io\/blog\/post\/2014\/11\/10\/numerically-stable-p-norms\/\r\n\r\nIndeed, the original implementation may not be numerical stable, a more stable one is to normalize the input before doing the actual computation. Here is the naive implementation of original tf.norm and the numerical stable one as a possible fix:\r\n\r\n```\r\nimport tensorflow as tf\r\ninput = tf.constant(1.751, dtype=\"float16\")\r\np = 144111111\r\n\r\ndef origin_norm(x, p):\r\n  result = tf.math.abs(x)\r\n  result = tf.math.pow(\r\n      tf.reduce_sum(tf.math.pow(result, p), axis=None, keepdims=True),\r\n      1.0 \/ p)\r\n  return result\r\n\r\ndef refined_norm(x, p):\r\n  a = np.abs(x).max()\r\n  return a * origin_norm(x\/a, p)\r\n\r\nprint(f\"The original tf.norm's output is: \", tf.norm(input, p))\r\nprint(f\"The naive implementation of tf.norm: {origin_norm(input, p)}, the refined norm: {refined_norm(input, p)}\")\r\n```\r\n\r\nThe `refined_norm` will output correct results (i.e., 1.7509765625) not only for `float16` computation but also for `float32` computation.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ninput = tf.constant(1.751, dtype=\"float16\")\r\np = 144111111\r\n\r\ndef origin_norm(x, p):\r\n  result = tf.math.abs(x)\r\n  result = tf.math.pow(\r\n      tf.reduce_sum(tf.math.pow(result, p), axis=None, keepdims=True),\r\n      1.0 \/ p)\r\n  return result\r\n\r\ndef refined_norm(x, p):\r\n  a = np.abs(x).max()\r\n  return a * origin_norm(x\/a, p)\r\n\r\nprint(f\"The original tf.norm's output is: \", tf.norm(input, p))\r\nprint(f\"The naive implementation of tf.norm: {origin_norm(input, p)}, the refined norm: {refined_norm(input, p)}\")\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-13T19:26:58Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63643"},{"issue_number":225,"repository":"tensorflow\/tensorflow","title":"How can I exit the XLAControlFlowContext when inside a jit_compile tf.function? Exit() function take no effect.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 22.04\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### CUDA\/cuDNN version\r\n\r\nCUDA12.3\/cuDNN9.0\r\n\r\n### GPU model and memory\r\n\r\nGTX4090 24GiB\r\n\r\n### Current behavior?\r\n\r\nI have a custom op that is a normal OpKernel unable to be compile by XLA cluster. But unfortunately, when the user use this custom op, they have to wrap it into a tf.funtion. So somehow when the use Keras jit_compile or something else, errors happened.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\n@tf.function(jit_compile=True)\r\ndef test(a):\r\n    b = a + a\r\n    ctx = tf.__internal__.get_enclosing_xla_context()\r\n    ctx.Exit()\r\n    tf.print(b)\r\n    ctx.Enter()\r\n    return b * b\r\n\r\ntest(tf.constant(1))\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-03-14 02:28:52.720666: W tensorflow\/core\/framework\/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:296 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_test_9[_XlaMustCompile=true,config_proto=9241198235816212909,executor_type=11160318154034397263] on XLA_GPU_JIT: StringFormat (No registered 'StringFormat' OpKernel for XLA_GPU_JIT devices compatible with node {{node StringFormat}}){{node StringFormat}}\r\nThe op is created at: \r\nFile \"xla_context_test.py\", line 12, in <module>\r\n  test(tf.constant(1))\r\nFile \"xla_context_test.py\", line 8, in test\r\n  tf.print(b)\r\nTraceback (most recent call last):\r\n  File \"xla_context_test.py\", line 12, in <module>\r\n    test(tf.constant(1))\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/tensorflow\/python\/eager\/execute.py\", line 54, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_test_9[_XlaMustCompile=true,config_proto=9241198235816212909,executor_type=11160318154034397263] on XLA_GPU_JIT: StringFormat (No registered 'StringFormat' OpKernel for XLA_GPU_JIT devices compatible with node {{node StringFormat}}){{node StringFormat}}\r\nThe op is created at: \r\nFile \"xla_context_test.py\", line 12, in <module>\r\n  test(tf.constant(1))\r\nFile \"xla_context_test.py\", line 8, in test\r\n  tf.print(b) [Op:__inference_test_9]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","comp:tf.function","TF 2.15"],"created_at":"2024-03-13T18:29:08Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63632"},{"issue_number":226,"repository":"tensorflow\/tensorflow","title":" TypeError: this __dict__ descriptor does not support '_DictWrapper' objects during trivial model save","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo (bug doesn't exist in tf-nightly 2.17.0.dev20240312)\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nv2.16.1\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nOSX\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.12\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen calling ` tf.saved_model.save(model, saved_model_path)`\r\n\r\nwe see:\r\n\r\n```\r\n..\/..\/..\/..\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/saved_model\/save.py:190: in list_children\r\n    for name, child in super(_AugmentedGraphView, self).list_children(\r\n..\/..\/..\/..\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/checkpoint\/graph_view.py:75: in list_children\r\n    for name, ref in super(ObjectGraphView,\r\n..\/..\/..\/..\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/checkpoint\/trackable_view.py:85: in children\r\n    ref = converter.convert_to_trackable(ref, parent=obj)\r\n..\/..\/..\/..\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/trackable\/converter.py:31: in convert_to_trackable\r\n    if (tensor_util.is_tf_type(obj) and\r\n..\/..\/..\/..\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/framework\/tensor_util.py:1156: in is_tf_type\r\n    return isinstance(x, tf_type_classes)\r\n..\/..\/..\/..\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/typing.py:1871: in __instancecheck__\r\n    val = getattr_static(instance, attr)\r\n..\/..\/..\/..\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/inspect.py:1839: in getattr_static\r\n    instance_result = _check_instance(obj, attr)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\r\n\r\nobj = DictWrapper({'input_shape': [(None, 16), (None, 32)]}), attr = 'is_tensor_like'\r\n\r\n    def _check_instance(obj, attr):\r\n        instance_dict = {}\r\n        try:\r\n>           instance_dict = object.__getattribute__(obj, \"__dict__\")\r\nE           TypeError: this __dict__ descriptor does not support '_DictWrapper' objects\r\n\r\n..\/..\/..\/..\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/inspect.py:1793: TypeError\r\n```\r\n\r\nI suspect this is related to #59869 which was supposedly fixed. However, in 2.16.1 TF removes the pin on wrapt and the issue indeed persists. I've even tried downgrading wrapt to 1.14.1 and the issue remains.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\nfrom tensorflow.keras import layers\r\nimport tempfile\r\n\r\ndef get_two_tower_models():\r\n    online_features = [layers.Input(shape=(32,)), layers.Input(shape=(16,))]\r\n    offline_features = [layers.Input(shape=(16,)), layers.Input(shape=(32,))]\r\n    all_features = []\r\n    all_features.extend(online_features)\r\n    all_features.extend(offline_features)\r\n\r\n    def get_offline_tower(offline_features):\r\n        offline_inputs = layers.concatenate(offline_features, name=\"offline_concatenated\")\r\n        offline_hidden = layers.Dense(32, activation=\"tanh\", name=\"offline_hidden_1\")(offline_inputs)\r\n        offline_hidden = layers.Dense(16, activation=\"tanh\", name=\"offline_hidden_2\")(offline_hidden)\r\n        offline_final_embed = layers.Dense(8, name=\"offline_hidden_3\")(offline_hidden)\r\n\r\n        return offline_final_embed\r\n\r\n    def get_online_tower(online_features):\r\n        online_inputs = layers.concatenate(online_features, name=\"online_concatenated\")\r\n        online_hidden = layers.Dense(32, activation=\"tanh\", name=\"online_hidden_1\")(online_inputs)\r\n        online_hidden = layers.Dense(16, activation=\"tanh\", name=\"online_hidden_2\")(online_hidden)\r\n        online_final_embed = layers.Dense(8, name=\"online_hidden_3\")(online_hidden)\r\n\r\n        return online_final_embed\r\n\r\n    offline_tower_embed = get_offline_tower(offline_features)\r\n    online_tower_embed = get_online_tower(online_features)\r\n\r\n    # We normalize vectors with L2 norm to make sure we get the cosine similarity\r\n    offline_online_dot = layers.Dot(axes=1, normalize=True)([offline_tower_embed, online_tower_embed])\r\n\r\n    offline_tower_model = tf.keras.Model(inputs=offline_features, outputs=offline_tower_embed)\r\n    online_tower_model = tf.keras.Model(inputs=online_features, outputs=online_tower_embed)\r\n\r\n    full_model = tf.keras.Model(inputs=all_features, outputs=offline_online_dot)\r\n\r\n    return (full_model, offline_tower_model, online_tower_model)\r\n\r\nfull_model, offline_tower_model, online_tower_model = get_two_tower_models()\r\n\r\nwith tempfile.TemporaryDirectory() as tmpdirname:\r\n    tf.saved_model.save(online_tower_model, tmpdirname)\r\n```\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"\/Users\/alfredo_luque\/repos\/git.musta.ch\/airbnb\/bighead-service\/packages\/ml-frameworks\/tensorflow\/tests\/minimal_repro.py\", line 44, in <module>\r\n    tf.saved_model.save(online_tower_model, tmpdirname)\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/saved_model\/save.py\", line 1392, in save\r\n    save_and_return_nodes(obj, export_dir, signatures, options)\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/saved_model\/save.py\", line 1427, in save_and_return_nodes\r\n    _build_meta_graph(obj, signatures, options, meta_graph_def))\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/saved_model\/save.py\", line 1642, in _build_meta_graph\r\n    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/saved_model\/save.py\", line 1564, in _build_meta_graph_impl\r\n    saveable_view = _SaveableView(augmented_graph_view, options)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/saved_model\/save.py\", line 285, in __init__\r\n    checkpoint_util.objects_ids_and_slot_variables_and_paths(\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/checkpoint\/util.py\", line 160, in objects_ids_and_slot_variables_and_paths\r\n    trackable_objects, node_paths = graph_view.breadth_first_traversal()\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/checkpoint\/graph_view.py\", line 124, in breadth_first_traversal\r\n    return self._breadth_first_traversal()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/saved_model\/save.py\", line 156, in _breadth_first_traversal\r\n    super(_AugmentedGraphView, self)._breadth_first_traversal())\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/checkpoint\/graph_view.py\", line 128, in _breadth_first_traversal\r\n    return super(ObjectGraphView, self)._descendants_with_paths()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/checkpoint\/trackable_view.py\", line 111, in _descendants_with_paths\r\n    for name, dependency in self.children(current_trackable).items():\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/checkpoint\/graph_view.py\", line 97, in children\r\n    for name, ref in self.list_children(obj, **kwargs):\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/saved_model\/save.py\", line 190, in list_children\r\n    for name, child in super(_AugmentedGraphView, self).list_children(\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/checkpoint\/graph_view.py\", line 75, in list_children\r\n    for name, ref in super(ObjectGraphView,\r\n                     ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/checkpoint\/trackable_view.py\", line 85, in children\r\n    ref = converter.convert_to_trackable(ref, parent=obj)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/trackable\/converter.py\", line 31, in convert_to_trackable\r\n    if (tensor_util.is_tf_type(obj) and\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/site-packages\/tensorflow\/python\/framework\/tensor_util.py\", line 1156, in is_tf_type\r\n    return isinstance(x, tf_type_classes)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/typing.py\", line 1871, in __instancecheck__\r\n    val = getattr_static(instance, attr)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/inspect.py\", line 1839, in getattr_static\r\n    instance_result = _check_instance(obj, attr)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/Users\/alfredo_luque\/miniforge3\/envs\/local--bighead--v0.0.1\/lib\/python3.12\/inspect.py\", line 1793, in _check_instance\r\n    instance_dict = object.__getattribute__(obj, \"__dict__\")\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: this __dict__ descriptor does not support '_DictWrapper' objects\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:model","TF 2.16"],"created_at":"2024-03-12T19:52:24Z","comments":17,"reactions":7,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63548"},{"issue_number":227,"repository":"tensorflow\/tensorflow","title":"tf.tensor_scatter_nd_add: Aborted (core dumped)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, `tf.tensor_scatter_nd_add` encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n# Generate input data\r\ninput_tensor = tf.zeros([15, 15, 15])\r\nindices = tf.constant([[[0, 0, 0], [1, 1, 1]], [[2, 2, 2], [3, 3, 3]], [[4, 4, 4], [5, 5, 5]], [[6, 6, 6], [7, 7, 7]], [[8, 8, 8], [9, 9, 9]], [[10, 10, 10], [11, 11, 11]], [[12, 12, 12], [13, 13, 13]], [[14, 14, 14], [0, 0, 0]], [[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]], [[7, 7, 7], [8, 8, 8]], [[9, 9, 9], [10, 10, 10]], [[11, 11, 11], [12, 12, 12]], [[13, 13, 13], [14, 14, 14]]])\r\nupdates = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0])  # Cast updates to float\r\n\r\n# Invoke tf.tensor_scatter_nd_add\r\nresult = tf.tensor_scatter_nd_add(input_tensor, indices, updates)\r\n\r\n# Print the result\r\nprint(result)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-10 14:59:51.853766: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-10T15:00:49Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63380"},{"issue_number":228,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.TensorScatterSub: Aborted (core dumped)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, `tf.raw_ops.TensorScatterSub` encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n# Generate input data\r\ntensor = tf.constant([1, 2, 3, 4, 5])\r\nindices = tf.constant([[[1], [3]], [[0], [2]]])  # Nested structure for indices\r\nupdates = tf.constant([10, 20])\r\n\r\n# Invoke tf.raw_ops.TensorScatterSub\r\nresult = tf.raw_ops.TensorScatterSub(tensor=tensor, indices=indices, updates=updates)\r\n\r\n# Print the result\r\nprint(result)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-10 14:55:41.958738: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)\r\nAborted (core dumped)\n```\n","labels":["type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-10T14:57:36Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63378"},{"issue_number":229,"repository":"tensorflow\/tensorflow","title":"tf.raw_ops.FusedPadConv2D: Aborted (core dumped)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific input, `tf.raw_ops.FusedPadConv2D` encounters \"Aborted (core dumped)\".\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n# Generate input data\r\ninput_data = tf.random.normal([3, 10, 10])\r\n\r\n# Define paddings\r\npaddings = tf.constant([[0, 0], [1, 1], [1, 1]])\r\n\r\n# Define filter\r\nfilter = tf.random.normal([3, 3, 3, 16])\r\n\r\n# Define mode\r\nmode = \"REFLECT\"  # Change mode to \"REFLECT\" or \"SYMMETRIC\"\r\n\r\n# Define strides\r\nstrides = [1, 1, 1, 1]\r\n\r\n# Define padding\r\npadding = \"VALID\"\r\n\r\n# Invoke tf.raw_ops.FusedPadConv2D\r\noutput = tf.raw_ops.FusedPadConv2D(input=input_data, paddings=paddings, filter=filter, mode=mode, strides=strides, padding=padding)\r\n\r\nprint(output)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-10 14:49:28.555826: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (3 vs. 3)\r\nAborted (core dumped)\n```\n","labels":["type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-10T14:51:07Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63376"},{"issue_number":230,"repository":"tensorflow\/tensorflow","title":"core dumped with tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\ncore dumped error with specific input parameters.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n# Generate input data\r\ninput_data = tf.constant([[1.5, 2.5, 3.5], [4.5, 5.5, 6.5]])\r\n\r\n# Define min and max values per channel\r\nmin_per_channel = tf.constant([1.0, 2.0, 3.0])\r\nmax_per_channel = tf.constant([2.0, 3.0, 4.0])\r\n\r\n# Invoke tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel with inputs as 0-dimensional tensor and max as a 1x3 tensor\r\nquantized_output = tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel(inputs=tf.constant(0.0), min=min_per_channel, max=max_per_channel, num_bits=8, narrow_range=False)\r\n\r\n# Print the quantized output\r\nprint(quantized_output)\n```\n\n\n### Relevant log output\n\n```shell\n2024-03-09 15:02:07.858055: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\n```\n","labels":["type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-09T15:03:18Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63355"},{"issue_number":231,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) with tf.raw_ops.AvgPoolGrad","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\ncore dumped error with specific input parameters.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n# Generate input data\r\ninput_data = tf.random.normal([1, 28, 28, 3])\r\ngrad = tf.random.normal([1, 14, 14, 6])  # Change the number of channels in grad tensor\r\n\r\n# Perform average pooling\r\nresult = tf.nn.avg_pool2d(input_data, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', data_format='NHWC')\r\n\r\n# Compute gradient\r\ngrad_result = tf.raw_ops.AvgPoolGrad(orig_input_shape=tf.shape(input_data), grad=grad, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', data_format='NHWC')\r\n\r\nprint(grad_result)\n```\n\n\n### Relevant log output\n\n```shell\nfree(): corrupted unsorted chunks\r\nAborted (core dumped)\n```\n","labels":["type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-09T14:54:40Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63353"},{"issue_number":232,"repository":"tensorflow\/tensorflow","title":"Segmentation fault with tf.raw_ops.AudioSpectrogram","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSegmentation fault error with specific input parameters.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n# Generate input data\r\ninput_data = tf.random.normal([1, 44100], dtype=tf.float32)\r\n\r\n# Invoke tf.raw_ops.AudioSpectrogram with a negative window_size\r\nspectrogram = tf.raw_ops.AudioSpectrogram(input=input_data, window_size=-1024, stride=64, magnitude_squared=False)\r\n\r\n# Print the spectrogram\r\nprint(spectrogram)\n```\n\n\n### Relevant log output\n\n```shell\nSegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-09T14:50:26Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63352"},{"issue_number":233,"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in tf.io.TFRecordWriter","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI have tested on tensorflow-nightly.\r\nThere are five bugs related to `tf.io.TFRecordWriter`. \r\n\r\nBUG 1: `compression_method=3` -> Aborted (core dumped)\r\nCode:\r\n```\r\nimport tensorflow as tf\r\ninput_data = b'input_data_example'\r\noptions = tf.io.TFRecordOptions(compression_type='ZLIB', compression_method=3)\r\nwith tf.io.TFRecordWriter('output.tfrecord', options=options) as writer:\r\n    writer.write(input_data)\r\n```\r\n\r\nBUG 2: `window_bits=-8` -> Aborted (core dumped)\r\nCODE:\r\n```\r\nimport tensorflow as tf\r\ninput_data = b'input_data_example'\r\n\r\noptions = tf.io.TFRecordOptions(\r\n    compression_type='ZLIB',\r\n    window_bits=-8  # negative window_bits\r\n)\r\n\r\nwith tf.io.TFRecordWriter('output.tfrecord', options=options) as writer:\r\n    writer.write(input_data)\r\n```\r\n\r\nBUG3: `compression_level=-9` -> Aborted (core dumped)\r\nCODE:\r\n```\r\nimport tensorflow as tf\r\ninput_data = b'input_data_example'\r\noptions = tf.io.TFRecordOptions(compression_type='ZLIB', compression_level=-9)\r\nwith tf.io.TFRecordWriter('output.tfrecord', options=options) as writer:\r\n    writer.write(input_data)\r\n```\r\n\r\nBUG 4: `mem_level=-10` -> Aborted (core dumped)\r\nCODE:\r\n```\r\nimport tensorflow as tf\r\ninput_data = b'input_data_example'\r\noptions = tf.io.TFRecordOptions(compression_type='ZLIB', mem_level=-10)\r\nwith tf.io.TFRecordWriter('output.tfrecord', options=options) as writer:\r\n    writer.write(input_data)\r\n```\r\n\r\nBUG 5: `compression_strategy=-2` -> Aborted (core dumped)\r\n```\r\nimport tensorflow as tf\r\n\r\ninput_data = b'input_data_example'\r\noptions = tf.io.TFRecordOptions(compression_type='ZLIB', compression_strategy=-2)\r\nwith tf.io.TFRecordWriter('output.tfrecord', options=options) as writer:\r\n    writer.write(input_data)\r\n```\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nThe reproducible test cases are provided above.\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nAll the above codes output `Aborted (core dumped)`.\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-09T03:15:46Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63337"},{"issue_number":234,"repository":"tensorflow\/tensorflow","title":"The result of tf.quantization.fake_quant_with_min_max_args is inconsistent between CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nCUDA 12.2\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nFor the same input, `tf.quantization.fake_quant_with_min_max_args ` produces inconsistent results on CPU and GPU.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput_data = tf.constant([0.0, 1.0, 2.0, 3.0, 4.0])\r\n\r\nwith tf.device('cpu:0'):\r\n    quantized_input_data = tf.quantization.fake_quant_with_min_max_args(input_data, min=-6.0, max=6.0)\r\n    print(quantized_input_data)\r\n\r\nwith tf.device('gpu:0'):\r\n    quantized_input_data = tf.quantization.fake_quant_with_min_max_args(input_data, min=-6.0, max=6.0)\r\n    print(quantized_input_data)\n```\n\n\n### Relevant log output\n\n```shell\ntf.Tensor([0.        0.9882353 2.0235295 3.0117648 4.       ], shape=(5,), dtype=float32)\r\ntf.Tensor([0.        0.9882353 1.9764706 3.0117648 4.       ], shape=(5,), dtype=float32)\n```\n","labels":["type:bug","comp:ops","TF 2.15"],"created_at":"2024-03-08T08:28:07Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63271"},{"issue_number":235,"repository":"tensorflow\/tensorflow","title":"`tf.raw_ops.ArgMax`: Heap buffer overflow","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11.7\r\n\r\n### Bazel version\r\n\r\n6.5.0\r\n\r\n### GCC\/compiler version\r\n\r\nclang 16\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`tf.raw_ops.ArgMax` can lead to heap buffer overflow.\r\n[Error location](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/774b0c3e97b5ef60bfc9c54961347dd0bc3660a8\/tensorflow\/core\/kernels\/argmax_op.cc#L59):\r\n```C++\r\n    const int32_t dim = internal::SubtleMustCopy(dimension.scalar<int32>()());\r\n```\r\nIt copies scalar value of `dimension` without checking exact type. Therefore when `dimension` is an `int16` it reads over the bound.\r\nNote that `int16` is an allowed type for `dimension` according to [opdef](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/41b93a8b310086f69aab6b6369d2af9d5178881d\/tensorflow\/core\/ops\/math_ops.cc#L1153-L1160):\r\n```C++\r\nREGISTER_OP(\"ArgMax\")\r\n    .Input(\"input: T\")\r\n    .Input(\"dimension: Tidx\")\r\n    .Output(\"output: output_type\")\r\n    .Attr(\"T: {realnumbertype, quantizedtype, bool}\")\r\n    .Attr(\"Tidx: {int16, int32, int64} = DT_INT32\")\r\n    .Attr(\"output_type: {int16, uint16, int32, int64} = DT_INT64\")\r\n    .SetShapeFn(ArgOpShape);\r\n```\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.raw_ops.ArgMax(\r\n    input=tf.random.normal([1,1,1,1]),\r\n    dimension=tf.constant(1,shape=[],dtype=tf.int16),\r\n    output_type=tf.dtypes.int64,\r\n    name=None\r\n)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nThe below log needs ASAN build.\r\n```shell\r\n=================================================================\r\n==4008222==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x609000000400 at pc 0x7fa6a0dca809 bp 0x7ffe63b29d90 sp 0x7ffe63b29d88\r\nREAD of size 4 at 0x609000000400 thread T0\r\n    #0 0x7fa6a0dca808 in int const tensorflow::internal::SubtleMustCopy<int>(int const&) \/proc\/self\/cwd\/.\/tensorflow\/core\/framework\/bounds_check.h:49:10\r\n    #1 0x7fa6a0dca808 in tensorflow::ArgOp<Eigen::ThreadPoolDevice, float, long, tensorflow::functor::ArgMax<Eigen::ThreadPoolDevice, float, long>>::Compute(tensorflow::OpKernelContext*) \/proc\/self\/cwd\/tensorflow\/core\/kernels\/argmax_op.cc:59:25\r\n    #2 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/threadpool_device.cc:185:14\r\n    #3 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/single_threaded_executor.cc:445:15\r\n    #4 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/function.cc:1339:3\r\n    #5 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/process_function_library_runtime.cc:931:16\r\n    #6 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/process_function_library_runtime.cc:1526:21\r\n    #7 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/kernel_and_device.cc:464:21\r\n    #8 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/execute.cc:2167:3\r\n    #9 0x7fa69da86630 in tensorflow::ExecuteNode::Run() \/proc\/self\/cwd\/.\/tensorflow\/core\/common_runtime\/eager\/execute_node.h:127:12\r\n    #10 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/eager_executor.cc:128:13\r\n    #11 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/execute.cc:1685:25\r\n    #12 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/execute.cc:1760:14\r\n    #13 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/execute.cc:2127:12\r\n    #14 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/execute.cc:2207:10\r\n    #15 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/core.cc:187:10\r\n    #16 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/custom_device_op_handler.cc\r\n    #17 0x7fa6851e18f1 in TFE_Execute \/proc\/self\/cwd\/tensorflow\/c\/eager\/c_api.cc:907:62\r\n    #18 0x7fa6b989418d in TFE_Py_FastPathExecute_C(_object*) \/proc\/self\/cwd\/tensorflow\/python\/eager\/pywrap_tfe_src.cc:3979:3\r\n    #19 0x7fa667d2683e in pybind11_init__pywrap_tfe(pybind11::module_&)::$_60::operator()(pybind11::args) const \/proc\/self\/cwd\/tensorflow\/python\/tfe_wrapper.cc:1276:35\r\n    #20 0x7fa667d2683e in pybind11::object pybind11::detail::argument_loader<pybind11::args>::call_impl<pybind11::object, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, 0ul, pybind11::detail::void_type>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, std::integer_sequence<unsigned long, 0ul>, pybind11::detail::void_type&&) && \/proc\/self\/cwd\/bazel-out\/k8-dbg\/bin\/external\/pybind11\/_virtual_includes\/pybind11\/pybind11\/cast.h:1443:16\r\n    #21 0x7fa667d2683e in std::enable_if<!std::is_void<pybind11::object>::value, pybind11::object>::type pybind11::detail::argument_loader<pybind11::args>::call<pybind11::object, pybind11::detail::void_type, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&) && \/proc\/self\/cwd\/bazel-out\/k8-dbg\/bin\/external\/pybind11\/_virtual_includes\/pybind11\/pybind11\/cast.h:1411:42\r\n    #22 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const \/proc\/self\/cwd\/bazel-out\/k8-dbg\/bin\/external\/pybind11\/_virtual_includes\/pybind11\/pybind11\/pybind11.h:248:69\r\n    #23 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) \/proc\/self\/cwd\/bazel-out\/k8-dbg\/bin\/external\/pybind11\/_virtual_includes\/pybind11\/pybind11\/pybind11.h:223:21\r\n    #24 0x7fa667d67a59 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) \/proc\/self\/cwd\/bazel-out\/k8-dbg\/bin\/external\/pybind11\/_virtual_includes\/pybind11\/pybind11\/pybind11.h:939:30\r\n    #25 0x528186 in cfunction_call \/usr\/local\/src\/conda\/python-3.11.7\/Objects\/methodobject.c:542:18\r\n    #26 0x503a0b in _PyObject_MakeTpCall \/usr\/local\/src\/conda\/python-3.11.7\/Objects\/call.c:214:18\r\n    #27 0x510f32 in _PyEval_EvalFrameDefault \/usr\/local\/src\/conda\/python-3.11.7\/Python\/ceval.c:4769:23\r\n    #28 0x538732 in _PyEval_EvalFrame \/usr\/local\/src\/conda\/python-3.11.7\/Include\/internal\/pycore_ceval.h:73:16\r\n    #29 0x538732 in _PyEval_Vector \/usr\/local\/src\/conda\/python-3.11.7\/Python\/ceval.c:6434:24\r\n    #30 0x538732 in _PyFunction_Vectorcall \/usr\/local\/src\/conda\/python-3.11.7\/Objects\/call.c:393:16\r\n    #31 0x5426bb in _PyVectorcall_Call \/usr\/local\/src\/conda\/python-3.11.7\/Objects\/call.c:257:24\r\n    #32 0x5426bb in _PyObject_Call \/usr\/local\/src\/conda\/python-3.11.7\/Objects\/call.c:328:16\r\n    #33 0x5426bb in PyObject_Call \/usr\/local\/src\/conda\/python-3.11.7\/Objects\/call.c:355:12\r\n    #34 0x514ff0 in do_call_core \/usr\/local\/src\/conda\/python-3.11.7\/Python\/ceval.c:7352:12\r\n    #35 0x514ff0 in _PyEval_EvalFrameDefault \/usr\/local\/src\/conda\/python-3.11.7\/Python\/ceval.c:5376:22\r\n    #36 0x5cb559 in _PyEval_EvalFrame \/usr\/local\/src\/conda\/python-3.11.7\/Include\/internal\/pycore_ceval.h:73:16\r\n    #37 0x5cb559 in _PyEval_Vector \/usr\/local\/src\/conda\/python-3.11.7\/Python\/ceval.c:6434:24\r\n    #38 0x5cac2e in PyEval_EvalCode \/usr\/local\/src\/conda\/python-3.11.7\/Python\/ceval.c:1148:21\r\n    #39 0x5ebcf6 in run_eval_code_obj \/usr\/local\/src\/conda\/python-3.11.7\/Python\/pythonrun.c:1710:9\r\n    #40 0x5e788f in run_mod \/usr\/local\/src\/conda\/python-3.11.7\/Python\/pythonrun.c:1731:19\r\n    #41 0x5fc831 in pyrun_file \/usr\/local\/src\/conda\/python-3.11.7\/Python\/pythonrun.c:1626:15\r\n    #42 0x5fbbfe in _PyRun_SimpleFileObject \/usr\/local\/src\/conda\/python-3.11.7\/Python\/pythonrun.c:440:13\r\n    #43 0x5fb922 in _PyRun_AnyFileObject \/usr\/local\/src\/conda\/python-3.11.7\/Python\/pythonrun.c:79:15\r\n    #44 0x5f65cd in pymain_run_file_obj \/usr\/local\/src\/conda\/python-3.11.7\/Modules\/main.c:360:15\r\n    #45 0x5f65cd in pymain_run_file \/usr\/local\/src\/conda\/python-3.11.7\/Modules\/main.c:379:15\r\n    #46 0x5f65cd in pymain_run_python \/usr\/local\/src\/conda\/python-3.11.7\/Modules\/main.c:601:21\r\n    #47 0x5f65cd in Py_RunMain \/usr\/local\/src\/conda\/python-3.11.7\/Modules\/main.c:680:5\r\n    #48 0x5bb3d8 in Py_BytesMain \/usr\/local\/src\/conda\/python-3.11.7\/Modules\/main.c:734:12\r\n    #49 0x7fa791408d8f in __libc_start_call_main csu\/..\/sysdeps\/nptl\/libc_start_call_main.h:58:16\r\n    #50 0x7fa791408e3f in __libc_start_main csu\/..\/csu\/libc-start.c:392:3\r\n    #51 0x5bb222 in _start (\/home\/loft\/anaconda3\/envs\/tf-latest-asan\/bin\/python3.11+0x5bb222)\r\n\r\n0x609000000402 is located 0 bytes after 2-byte region [0x609000000400,0x609000000402)\r\nallocated by thread T0 here:\r\n    #0 0x7fa7917ed617 in __interceptor_posix_memalign \/home\/runner\/work\/llvm-project\/llvm-project\/final\/llvm-project\/compiler-rt\/lib\/asan\/asan_malloc_linux.cpp:145:3\r\n    #1 0x7fa6be4f6902 in tsl::port::AlignedMalloc(unsigned long, int) (\/home\/loft\/anaconda3\/envs\/tf-latest-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x4a18902)\r\n    #2 0x7fa6bc2d3bcc in tsl::(anonymous namespace)::CPUAllocator::AllocateRaw(unsigned long, unsigned long) cpu_allocator_impl.cc\r\n    #3 0x7fa6bc4883bc in short* tensorflow::TypedAllocator::Allocate<short>(tsl::Allocator*, unsigned long, tsl::AllocationAttributes const&) \/proc\/self\/cwd\/.\/tensorflow\/core\/framework\/typed_allocator.h:47:24\r\n    #4 0x7fa6bc4883bc in tensorflow::(anonymous namespace)::Buffer<short>::Buffer(tsl::Allocator*, long, tsl::AllocationAttributes const&) \/proc\/self\/cwd\/tensorflow\/core\/framework\/tensor.cc:574:21\r\n    #5 0x7fa6bc4883bc in tensorflow::Tensor::Tensor(tsl::Allocator*, tensorflow::DataType, tensorflow::TensorShape const&, tsl::AllocationAttributes const&) \/proc\/self\/cwd\/tensorflow\/core\/framework\/tensor.cc:986:5\r\n    #6 0x7fa6bbef871d in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes, tsl::AllocationAttributes const&) \/proc\/self\/cwd\/tensorflow\/core\/framework\/op_kernel.cc:764:10\r\n    #7 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes) \/proc\/self\/cwd\/.\/tensorflow\/core\/framework\/op_kernel.h:1270:12\r\n    #8 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**, tsl::AllocatorAttributes) \/proc\/self\/cwd\/tensorflow\/core\/framework\/op_kernel.cc:822:14\r\n    #9 0x7fa6bbef4368 in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**) \/proc\/self\/cwd\/tensorflow\/core\/framework\/op_kernel.cc:728:10\r\n    #10 0x7fa6a9bf0207 in tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) (\/home\/loft\/anaconda3\/envs\/tf-latest-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e9fe207)\r\n    #11 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/threadpool_device.cc:185:14\r\n    #12 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/single_threaded_executor.cc:445:15\r\n    #13 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/function.cc:1339:3\r\n    #14 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/process_function_library_runtime.cc:931:16\r\n    #15 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/process_function_library_runtime.cc:1526:21\r\n    #16 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/kernel_and_device.cc:464:21\r\n    #17 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/execute.cc:2167:3\r\n    #18 0x7fa69da86630 in tensorflow::ExecuteNode::Run() \/proc\/self\/cwd\/.\/tensorflow\/core\/common_runtime\/eager\/execute_node.h:127:12\r\n    #19 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/eager_executor.cc:128:13\r\n    #20 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/execute.cc:1685:25\r\n    #21 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/execute.cc:1760:14\r\n    #22 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/execute.cc:2127:12\r\n    #23 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/execute.cc:2207:10\r\n    #24 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/core.cc:187:10\r\n    #25 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) \/proc\/self\/cwd\/tensorflow\/core\/common_runtime\/eager\/custom_device_op_handler.cc\r\n    #26 0x7fa6851e18f1 in TFE_Execute \/proc\/self\/cwd\/tensorflow\/c\/eager\/c_api.cc:907:62\r\n    #27 0x7fa6b985dcbf in tensorflow::EagerCast(TFE_Context*, TFE_TensorHandle*, TF_DataType, TF_DataType, TSL_Status*) \/proc\/self\/cwd\/tensorflow\/python\/eager\/pywrap_tensor.cc:259:3\r\n    #28 0x7fa6b985e67b in tensorflow::ConvertToEagerTensorUncached(TFE_Context*, _object*, tensorflow::DataType, char const*) \/proc\/self\/cwd\/tensorflow\/python\/eager\/pywrap_tensor.cc:317:11\r\n    #29 0x7fa6b985f8a1 in tensorflow::ConvertToEagerTensor(TFE_Context*, _object*, tensorflow::DataType, char const*) \/proc\/self\/cwd\/tensorflow\/python\/eager\/pywrap_tensor.cc:405:14\r\n    #30 0x7fa6b9860088 in EagerTensor_init \/proc\/self\/cwd\/tensorflow\/python\/eager\/pywrap_tensor.cc:529:18\r\n    #31 0x5039d2 in type_call \/usr\/local\/src\/conda\/python-3.11.7\/Objects\/typeobject.c:1103:19\r\n    #32 0x5039d2 in _PyObject_MakeTpCall \/usr\/local\/src\/conda\/python-3.11.7\/Objects\/call.c:214:18\r\n\r\nSUMMARY: AddressSanitizer: heap-buffer-overflow \/proc\/self\/cwd\/.\/tensorflow\/core\/framework\/bounds_check.h:49:10 in int const tensorflow::internal::SubtleMustCopy<int>(int const&)\r\nShadow bytes around the buggy address:\r\n  0x609000000180: fa fa fa fa fa fa fa fa fd fa fa fa fa fa fa fa\r\n  0x609000000200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x609000000280: fd fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x609000000300: 04 fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x609000000380: fd fd fd fd fa fa fa fa fa fa fa fa fa fa fa fa\r\n=>0x609000000400:[02]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x609000000480: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x609000000500: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x609000000580: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x609000000600: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x609000000680: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\nShadow byte legend (one shadow byte represents 8 application bytes):\r\n  Addressable:           00\r\n  Partially addressable: 01 02 03 04 05 06 07 \r\n  Heap left redzone:       fa\r\n  Freed heap region:       fd\r\n  Stack left redzone:      f1\r\n  Stack mid redzone:       f2\r\n  Stack right redzone:     f3\r\n  Stack after return:      f5\r\n  Stack use after scope:   f8\r\n  Global redzone:          f9\r\n  Global init order:       f6\r\n  Poisoned by user:        f7\r\n  Container overflow:      fc\r\n  Array cookie:            ac\r\n  Intra object redzone:    bb\r\n  ASan internal:           fe\r\n  Left alloca redzone:     ca\r\n  Right alloca redzone:    cb\r\n==4008222==ABORTING\r\n```\r\n","labels":["awaiting review","type:bug","comp:ops","TF 2.15"],"created_at":"2024-02-27T13:06:46Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63070"},{"issue_number":236,"repository":"tensorflow\/tensorflow","title":"`tf.raw_ops.ExtractImagePatches`: Assertion failure in shape inference step","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11.7\r\n\r\n### Bazel version\r\n\r\n6.5.0\r\n\r\n### GCC\/compiler version\r\n\r\nclang 16\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`tf.raw_ops.ExtractImagePatches` can lead to assertion failure in shape inference step.\r\nError location is [here](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/f5daeae21404e8d672c785cc0c4c469ddc1b1a8a\/tensorflow\/core\/ops\/array_ops.cc#L2686-L2687):\r\n```C++\r\n      TF_RETURN_IF_ERROR(c->Multiply(\r\n          c->Dim(input_shape, 3), ksize_rows * ksize_cols, &output_depth_dim));\r\n```\r\nBecause it does not check validity of `ksize_rows * ksize_cols`, the negative value of it is fed to [`Multiply`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/83f1804f3427ae888e62b26b5bcba8afc9e24ef7\/tensorflow\/core\/framework\/shape_inference.cc#L1096C1-L1098C56):\r\n```C++\r\nStatus InferenceContext::Multiply(DimensionHandle first,\r\n                                  DimensionOrConstant second,\r\n                                  DimensionHandle* out)\r\n```\r\nAnd it ends up with assertion failure at [a constructor of `DimensionOrConstant`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/daea84aad67d1fcc8d58c648e1da58ee576445f9\/tensorflow\/core\/framework\/shape_inference.h#L891-L896).\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```Python\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ntf.raw_ops.ExtractImagePatches(\r\n    images=tf.random.normal([1,1,1,1]),\r\n    ksizes=[1,-1,2,1],\r\n    strides=[1,1,1,1],\r\n    rates=[1,1,1,1],\r\n    padding=\"VALID\",\r\n    name=None\r\n)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nRelease build: Outputs nothing\r\n\r\nDebug build:\r\n```shell\r\n2024-02-27 20:05:58.701202: F .\/tensorflow\/core\/framework\/shape_inference.h:891] Check failed: val >= 0 || val == InferenceContext::kUnknownDim Dimension must be non-negative or equal to InferenceContext::kUnknownDim but got -2\r\nAborted (core dumped)\r\n```\r\n","labels":["awaiting review","type:bug","comp:ops","comp:core","TF 2.15"],"created_at":"2024-02-27T11:12:14Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63067"},{"issue_number":237,"repository":"tensorflow\/tensorflow","title":"C++ API `SparseFillEmptyRows` can lead to `std::length_error`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n6.5.0\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nC++ API `SparseFillEmptyRows` can lead to `std::length_error`.\r\nError location is [here](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/9da417e3f63215efe995b83e7b9f9b34115a424e\/tensorflow\/core\/kernels\/fill_empty_rows_functor.h#L114C36-L114C46):\r\n```C++\r\n    std::vector<Tindex> csr_offset(dense_rows, 0);\r\n```\r\nBecause lack of checking negativity, negative value of `dense_rows` is fed to the constructor of `std::vector`. By integer underflow, the value is converted to extremely large number and it ends up with `std::length_error`.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```C++\r\n#include \"tensorflow\/cc\/framework\/scope.h\"\r\n#include \"tensorflow\/core\/graph\/graph.h\"\r\n#include \"tensorflow\/core\/public\/session.h\"\r\n#include \"tensorflow\/cc\/ops\/array_ops.h\"\r\n#include \"tensorflow\/cc\/ops\/standard_ops.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nint main() {\r\n  SessionOptions options;\r\n  std::unique_ptr<tensorflow::Session>\r\n    session(tensorflow::NewSession(options));\r\n  Scope scope = Scope::NewRootScope();\r\n\r\n  Input indices = {{1l}};\r\n  Input values = {1};\r\n  Input dense_shape = {-1l};\r\n  Input default_value = 1;\r\n  auto target = ops::SparseFillEmptyRows(scope.WithOpName(\"target\"), indices, values, dense_shape, default_value);\r\n\r\n  Status status;\r\n  GraphDef graph_def;\r\n  status = scope.ToGraphDef(&graph_def);\r\n  if (!status.ok()) {\r\n    LOG(WARNING) << \"Could not build graph: \" << status.message();\r\n  }\r\n\r\n  status = session->Create(graph_def);\r\n  if (!status.ok()) {\r\n    LOG(WARNING) << \"Could not create session: \" << status.message();\r\n  }\r\n\r\n  std::vector<Tensor> outputs;\r\n  status = session->Run({}, {\"target\"}, {\"target\"}, &outputs);\r\n  if (!status.ok()) {\r\n    LOG(WARNING) << \"Could not run session: \" << status.message();\r\n  }\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nterminate called after throwing an instance of 'std::length_error'\r\n  what():  cannot create std::vector larger than max_size()\r\nAborted (core dumped)\r\n```\r\n","labels":["awaiting review","type:bug","comp:ops","TF 2.15"],"created_at":"2024-02-27T10:56:31Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63066"},{"issue_number":238,"repository":"tensorflow\/tensorflow","title":"`tf.raw_ops.ExtractGlimpse`: Assertion failure in shape inference step","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11.7\r\n\r\n### Bazel version\r\n\r\n6.5.0\r\n\r\n### GCC\/compiler version\r\n\r\nclang 16\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`tf.raw_ops.ExtractGlimpse` can lead to assertion failure in shape inference step.\r\nError location is [here](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/daea84aad67d1fcc8d58c648e1da58ee576445f9\/tensorflow\/core\/ops\/image_ops.cc#L57):\r\n```C++\r\n    height = c->MakeDim(vec(0));\r\n```\r\nIt makes a dim from input argument without checking negativity.\r\nIn debug build, it ends up with assertion failure in [a constructor of `DimensionOrConstant`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/daea84aad67d1fcc8d58c648e1da58ee576445f9\/tensorflow\/core\/framework\/shape_inference.h#L891-L896).\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```Python\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ntf.raw_ops.ExtractGlimpse(\r\n    input=tf.random.normal([1,1,1,1]),\r\n    size=[-2,-2],\r\n    offsets=tf.random.normal([1,2]),\r\n    centered=True,\r\n    normalized=True,\r\n    uniform_noise=True,\r\n    noise='uniform',\r\n    name=None\r\n)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nRelease build: Outputs nothing\r\n\r\nDebug build:\r\n```shell\r\n2024-02-27 19:02:16.866010: F .\/tensorflow\/core\/framework\/shape_inference.h:891] Check failed: val >= 0 || val == InferenceContext::kUnknownDim Dimension must be non-negative or equal to InferenceContext::kUnknownDim but got -2\r\nAborted (core dumped)\r\n```\r\n","labels":["awaiting review","type:bug","comp:ops","comp:core"],"created_at":"2024-02-27T10:11:25Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63063"},{"issue_number":239,"repository":"tensorflow\/tensorflow","title":"`tf.raw_ops.DecodeAndCropJpeg`: Assertion failure in shape inference step","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11.7\r\n\r\n### Bazel version\r\n\r\n6.5.0\r\n\r\n### GCC\/compiler version\r\n\r\nclang 16\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`tf.raw_ops.DecodeAndCropJpeg` can lead to assertion failure in shape inference step.\r\nError location is [here](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/daea84aad67d1fcc8d58c648e1da58ee576445f9\/tensorflow\/core\/ops\/image_ops.cc#L523):\r\n```C++\r\n       w = c->MakeDim(crop_window_vec(3));\r\n```\r\nIt makes a dim from an input argument without checking negativity.\r\nIn debug build, it ends up with assertion failure in [a constructor of`DimensionOrConstant`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/daea84aad67d1fcc8d58c648e1da58ee576445f9\/tensorflow\/core\/framework\/shape_inference.h#L891-L896).\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```Python\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\ntf.raw_ops.DecodeAndCropJpeg(\r\n    contents=\"abc\",\r\n    crop_window=[0,0,0,-2],\r\n    channels=0,\r\n    ratio=1,\r\n    fancy_upscaling=True,\r\n    try_recover_truncated=False,\r\n    acceptable_fraction=1,\r\n    dct_method='',\r\n    name=None\r\n)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nRelease build: Outputs nothing\r\n\r\nDebug build:\r\n```shell\r\n2024-02-27 18:41:59.133420: F .\/tensorflow\/core\/framework\/shape_inference.h:891] Check failed: val >= 0 || val == InferenceContext::kUnknownDim Dimension must be non-negative or equal to InferenceContext::kUnknownDim but got -2\r\nAborted (core dumped)\r\n```\r\n","labels":["awaiting review","type:bug","comp:ops","comp:core"],"created_at":"2024-02-27T09:52:15Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63062"},{"issue_number":240,"repository":"tensorflow\/tensorflow","title":"`tf.raw_ops.AvgPool`: negative kernel size is not checked at shape inference step","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11.7\r\n\r\n### Bazel version\r\n\r\n6.5.0\r\n\r\n### GCC\/compiler version\r\n\r\nclang 16\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nCurrently shape inference step of `tf.raw_ops.AvgPool` allows negative kernel size.\r\nNote that debug build rejects it [here](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/7ba14e559de0112cbf59dc6db6f8c6a18283642a\/tensorflow\/core\/framework\/common_shape_fns.cc#L1393-L1394):\r\n```C++\r\n  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(\r\n      c, in_rows_dim, kernel_rows, stride_rows, padding, &output_rows));\r\n```\r\n, where `kernel_rows` is converted to `DimensionOrConstant` and ends up with assertion failure [here](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/7ba14e559de0112cbf59dc6db6f8c6a18283642a\/tensorflow\/core\/framework\/shape_inference.h#L890-L895):\r\n```C++\r\ninline DimensionOrConstant::DimensionOrConstant(int64_t val) : val(val) {\r\n  DCHECK(val >= 0 || val == InferenceContext::kUnknownDim)\r\n      << \"Dimension must be non-negative or equal to \"\r\n         \"InferenceContext::kUnknownDim but got \"\r\n      << val;\r\n}\r\n```\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```Python\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nx = tf.raw_ops.AvgPool(\r\n    value=tf.random.normal([1,1,1,1]),\r\n    ksize=[1,-2,1,1],\r\n    strides=[1,1,1,1],\r\n    padding=\"SAME\",\r\n    data_format='NHWC',\r\n    name=None\r\n)\r\n\r\nprint(x)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nRelease Build:\r\n```shell\r\nTensor(\"AvgPool:0\", shape=(1, 1, 1, 1), dtype=float32)\r\n```\r\n\r\nDebug Build:\r\n```shell\r\n2024-02-23 22:18:43.783609: F .\/tensorflow\/core\/framework\/shape_inference.h:891] Check failed: val >= 0 || val == InferenceContext::kUnknownDim Dimension must be non-negative or equal to InferenceContext::kUnknownDim but got -2\r\nAborted (core dumped)\r\n```\r\n","labels":["awaiting review","type:bug","comp:ops","TF 2.15"],"created_at":"2024-02-23T13:29:12Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63034"},{"issue_number":241,"repository":"tensorflow\/tensorflow","title":"`tf.raw_ops.ConjugateTranspose`: negative value of `perm` can lead to out-of-bounds read","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11.7\r\n\r\n### Bazel version\r\n\r\n6.5.0\r\n\r\n### GCC\/compiler version\r\n\r\nclang 16\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nIn `tf.raw_ops.ConjugateTranspose`, negative value of `perm` can lead to out-of-bounds read.\r\n[Here](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/617b5e97d6a9a71e1972dfe6fead5bf460094658\/tensorflow\/core\/kernels\/transpose_functor_cpu.cc#L52),\r\n```C++\r\n        i_idx += ratio * in_strides[perm[i]];\r\n```\r\nas there is no guard which checks validity of `perm`, when its value is -1 `in_strides[perm[i]]` can be an out-of-bounds reading(I guess -1 would be interpreted as an `SIZE_T_MAX` or something).\r\nNote that the below code ends up with absl assertion failure in debug build.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```Python\r\nimport tensorflow as tf\r\n\r\ntf.raw_ops.ConjugateTranspose(\r\n    x=tf.random.normal([2]),\r\n    perm=[-1])\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nRelease Build:\r\n```shell\r\n    Outputs nothing\r\n```\r\n\r\nDebug Build:\r\n```shell\r\npython: external\/com_google_absl\/absl\/container\/inlined_vector.h:363: auto absl::InlinedVector<long, 8>::operator[](size_type)::(anonymous class)::operator()() const [T = long, N = 8, A = std::allocator<long>]: Assertion `false && \"i < size()\"' failed.\r\nAborted (core dumped)\r\n```\r\n","labels":["awaiting review","type:bug","comp:core","TF 2.15"],"created_at":"2024-02-23T13:09:32Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/63033"},{"issue_number":242,"repository":"tensorflow\/tensorflow","title":"`tf.raw_ops.Transpose` aborts with negative `perm` value","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.7\n\n### Bazel version\n\n6.5.0\n\n### GCC\/compiler version\n\nclang 16\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.Transpose` aborts with negative `perm` value. [gist](https:\/\/colab.research.google.com\/drive\/1r0oUxDcu-uWHgjQGOU8qvcSzD1z2fv-a?usp=sharing)\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntf.raw_ops.Transpose(\r\n    x=tf.random.normal([2,1]),\r\n    perm=[-1,0],\r\n    name=None\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-02-20 19:35:25.981306: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\n```\n","labels":["awaiting review","type:bug","comp:ops","TF 2.15"],"created_at":"2024-02-20T10:39:53Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62995"},{"issue_number":243,"repository":"tensorflow\/tensorflow","title":"v2.15.0 docker image print error messages like \"Unable to register cuDNN\/cuFFT... factory \"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nubuntu 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n```\r\n$ docker pull tensorflow\/tensorflow:2.15.0-gpu\r\n$ docker run -it --gpus all tensorflow\/tensorflow:2.15.0-gpu\r\n$import tensorflow as tf\r\n2024-02-18 02:43:26.283147: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-02-18 02:43:26.283204: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-02-18 02:43:26.283926: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-02-18 02:43:26.289011: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n>>> print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\r\nNum GPUs Available:  8\r\n```\r\n\r\nI tried some examples and it worked. But why this error messages show?\n\n### Standalone code to reproduce the issue\n\n```shell\nUbuntu 18.04 nvidia-driver 535.104.12\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux","TF 2.15"],"created_at":"2024-02-18T02:51:35Z","comments":8,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62987"},{"issue_number":244,"repository":"tensorflow\/tensorflow","title":"C++ API `GatherV2` aborts with inappropriate input","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11.7\r\n\r\n### Bazel version\r\n\r\n6.5.0\r\n\r\n### GCC\/compiler version\r\n\r\nclang 16\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nC++ API `GatherV2` aborts with inappropriate input.\r\nSuspected code location is [here](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/e193d8ea7776ef5c6f5d769b6fb9c070213e737a\/tensorflow\/core\/ops\/array_ops.cc#L1240-L1242),\r\n```C++\r\n          c->set_output(0, c->UnknownShapeOfRank(c->Rank(params_shape) +\r\n                                                 c->Rank(indices_shape) - 1 -\r\n                                                 batch_dims));\r\n```\r\nwhich calls `UnknownShapeOfRank` with negative value due to lack of validity check.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```C++\r\n#include \"tensorflow\/cc\/framework\/scope.h\"\r\n#include \"tensorflow\/core\/graph\/graph.h\"\r\n#include \"tensorflow\/core\/public\/session.h\"\r\n#include \"tensorflow\/cc\/ops\/array_ops.h\"\r\n#include \"tensorflow\/cc\/ops\/standard_ops.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nint main() {\r\n  SessionOptions options;\r\n  ConfigProto & config = options.config;\r\n  config.set_inter_op_parallelism_threads(1);\r\n  config.set_intra_op_parallelism_threads(1);\r\n  config.set_use_per_session_threads(false);\r\n  std::unique_ptr<tensorflow::Session>\r\n    session(tensorflow::NewSession(options));\r\n  Scope scope = Scope::NewRootScope();\r\n\r\n  auto params = ops::RandomUniformInt(scope, {1}, 0, 15);\r\n  auto attrs = \r\n    ops::GatherV2::Attrs()\r\n      .BatchDims(2);\r\n  auto target = ops::GatherV2(scope.WithOpName(\"target\"), params, 1, 1, attrs);\r\n\r\n  GraphDef graph_def;\r\n  TF_CHECK_OK(scope.ToGraphDef(&graph_def));\r\n\r\n  Status status = session->Create(graph_def);\r\n  if (!status.ok()) {\r\n    LOG(WARNING) << \"Could not create session: \" << status.message();\r\n  }\r\n\r\n  std::vector<Tensor> outputs;\r\n  status = session->Run({}, {\"target\"}, {\"\"}, &outputs);\r\n  if (!status.ok()) {\r\n    LOG(WARNING) << \"Could not run session: \" << status.message();\r\n  }\r\n\r\n  return 0;\r\n}\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-02-18 03:14:31.909510: F tensorflow\/core\/framework\/shape_inference.cc:705] Check failed: rank >= 0 (0 vs. -2)rank must not be negative\r\nAborted (core dumped)\r\n```\r\n","labels":["awaiting review","type:bug","comp:runtime","comp:ops"],"created_at":"2024-02-17T18:19:19Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62985"},{"issue_number":245,"repository":"tensorflow\/tensorflow","title":"`tf.raw_ops.FakeQuantWithMinMaxArgs` aborts when `min>0` in Debug build","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.7\n\n### Bazel version\n\n6.5.0\n\n### GCC\/compiler version\n\nclang 16\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.FakeQuantWithMinMaxArgs` aborts when `min>0` in Debug build(compiled with --config=dbg).\r\nIt crashes [here](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/e193d8ea7776ef5c6f5d769b6fb9c070213e737a\/tensorflow\/core\/kernels\/fake_quant_ops_functor.h#L86), \r\n```C++\r\neigen_assert(min <= 0.0f && \"min should be <= 0.0\");\r\n```\r\nand it doesn't seem to be consistent with [document](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/FakeQuantWithMinMaxArgs).\r\nNote that `tf.raw_ops.FakeQuantWithMinMaxArgsGradient` has the same situation.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n# it aborts only when compiled with `--config=dbg`\r\ntf.raw_ops.FakeQuantWithMinMaxArgs(\r\n    inputs=tf.random.normal([1,1,1]),\r\n    min=1,\r\n    max=6,\r\n    num_bits=8,\r\n    narrow_range=False)\n```\n\n\n### Relevant log output\n\n```shell\npython: .\/tensorflow\/core\/kernels\/fake_quant_ops_functor.h:86: void tensorflow::FakeQuantWithMinMaxArgsFunctor<Eigen::ThreadPoolDevice>::operator()(const Device &, ConstFlat<float>, const float, const float, const int, const int, Flat<float>) [Device = Eigen::ThreadPoolDevice]: Assertion `min <= 0.0f && \"min should be <= 0.0\"' failed.\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","comp:core"],"created_at":"2024-02-17T17:48:24Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62983"},{"issue_number":246,"repository":"tensorflow\/tensorflow","title":"`tf.raw_ops.LoopCond` aborts in Debug build","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.7\n\n### Bazel version\n\n6.5.0\n\n### GCC\/compiler version\n\nclang 16\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.raw_ops.LoopCond` aborts in Debug build(compiled with `--config=dbg`).\r\nIn release build, it does not crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n# it aborts only when compiled with `--config=dbg`\r\ntf.raw_ops.LoopCond(input=True)\n```\n\n\n### Relevant log output\n\n```shell\n2024-02-18 02:37:24.039192: F tensorflow\/core\/graph\/graph_partition.cc:644] Check failed: !frame_name.empty() \r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux"],"created_at":"2024-02-17T17:39:07Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62982"},{"issue_number":247,"repository":"tensorflow\/tensorflow","title":"Could not find device for node GenerateBoundingBoxProposals","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen using tf.image.generate_bounding_box_proposals in CPU, it raises the following error: \r\n\r\n```\r\nNotFoundError: Could not find device for node: {{node GenerateBoundingBoxProposals}} = GenerateBoundingBoxProposals[post_nms_topn=300]\r\nAll kernels registered for op GenerateBoundingBoxProposals:\r\n  device='GPU'\r\n [Op:GenerateBoundingBoxProposals] name: \r\n```\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nscores = tf.constant([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4]])\r\nbbox_deltas = tf.constant([[1, 1, 1, 1], [2, 2, 2, 2]])\r\nimage_info = tf.constant([100, 100, 1])\r\nanchors = tf.constant([[10, 10, 20, 20], [30, 30, 40, 40], [50, 50, 60, 60]])\r\n\r\nresult = tf.image.generate_bounding_box_proposals(scores, bbox_deltas, image_info, anchors)\r\n\r\nprint(result)\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","comp:ops","TF 2.15"],"created_at":"2024-02-15T07:24:02Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62969"},{"issue_number":248,"repository":"tensorflow\/tensorflow","title":"Error with Custom Keras Model","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.13.0-17-gf841394b1b7 2.13.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.6 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.18\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nExpected output (ran using v2.8.3-90-g1b8f5c396f0 2.8.4):\r\n\r\n```\r\n{'name': 'custom_model', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 96, 96, 3), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_1'}, 'name': 'input_1', 'inbound_nodes': []}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 8, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'conv2d', 'inbound_nodes': [[['input_1', 0, 0, {}]]]}], 'input_layers': [['input_1', 0, 0]], 'output_layers': [['conv2d', 0, 0]]}\r\n{'name': 'model', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 96, 96, 3), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_1'}, 'name': 'input_1', 'inbound_nodes': []}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 8, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'conv2d', 'inbound_nodes': [[['input_1', 0, 0, {}]]]}], 'input_layers': [['input_1', 0, 0]], 'output_layers': [['conv2d', 0, 0]]}\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\nWARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\r\nWARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\r\nModel: \"custom_model\"\r\n_________________________________________________________________\r\n Layer (type)                Output Shape              Param #   \r\n=================================================================\r\n input_1 (InputLayer)        [(None, 96, 96, 3)]       0         \r\n                                                                 \r\n conv2d (Conv2D)             (None, 94, 94, 8)         224       \r\n                                                                 \r\n=================================================================\r\nTotal params: 224\r\nTrainable params: 224\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\nModel: \"model\"\r\n_________________________________________________________________\r\n Layer (type)                Output Shape              Param #   \r\n=================================================================\r\n input_1 (InputLayer)        [(None, 96, 96, 3)]       0         \r\n                                                                 \r\n conv2d (Conv2D)             (None, 94, 94, 8)         224       \r\n                                                                 \r\n=================================================================\r\nTotal params: 224\r\nTrainable params: 224\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\nimport tensorflow as tf  # noqa: E402\r\n\r\n\r\nclass CustomModel(tf.keras.Model):\r\n    def __init__(self, **kwargs):\r\n        super(CustomModel, self).__init__(**kwargs)\r\n\r\n\r\ninput_shape = (96, 96, 3)\r\ninputs = tf.keras.layers.Input(shape=input_shape)\r\noutputs = tf.keras.layers.Conv2D(\r\n    filters=8, kernel_size=3, strides=1)(inputs)\r\n\r\ncust_model = CustomModel(inputs=inputs, outputs=outputs)\r\nprint(cust_model.get_config())\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\r\nprint(model.get_config())\r\n\r\ncust_model_path = '\/home\/ubuntu\/automltraining\/my_custom_model.h5'\r\ncust_model.save(cust_model_path)\r\n\r\nmodel_path = '\/home\/ubuntu\/automltraining\/my_model.h5'\r\nmodel.save(model_path)\r\n\r\ncust_model2 = tf.keras.models.load_model(\r\n    cust_model_path, custom_objects={\"CustomModel\": CustomModel})\r\nmodel2 = tf.keras.models.load_model(model_path)\r\n\r\ncust_model2.summary()\r\nmodel2.summary()\n```\n\n\n### Relevant log output\n\n```shell\n{'name': 'custom_model', 'trainable': True}\r\n{'name': 'model', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 96, 96, 3), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_1'}, 'registered_name': None, 'name': 'input_1', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 8, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 96, 96, 3)}, 'name': 'conv2d', 'inbound_nodes': [[['input_1', 0, 0, {}]]]}], 'input_layers': [['input_1', 0, 0]], 'output_layers': [['conv2d', 0, 0]]}\r\n\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/keras\/src\/engine\/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\r\n  saving_api.save_model(\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\r\nTraceback (most recent call last):\r\n  File \"\/home\/ubuntu\/automltraining\/get_config_debug.py\", line 28, in <module>\r\n    cust_model2 = tf.keras.models.load_model(\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/keras\/src\/saving\/saving_api.py\", line 238, in load_model\r\n    return legacy_sm_saving_lib.load_model(\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 70, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/keras\/src\/engine\/training.py\", line 3246, in from_config\r\n    raise TypeError(\r\nTypeError: Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the  constructor to <class '__main__.CustomModel'>, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of CustomModel from its config.\r\n\r\nReceived config={'name': 'custom_model', 'trainable': True}\r\n\r\nError encountered during deserialization: __init__() missing 2 required positional arguments: 'inputs' and 'outputs'\n```\n","labels":["type:bug","comp:keras","TF 2.13"],"created_at":"2024-02-09T05:34:22Z","comments":5,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62927"},{"issue_number":249,"repository":"tensorflow\/tensorflow","title":"MLP subsequence is inconsistent with full sequence","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv1.12.1-105526-gf9a553f735c 2.16.0-dev20240206\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n**Issue:** Given a two layer MLP with a reshape, I notice that `model(data)[:, :end_idx])` does not equal `model(data[:, :end_idx])`.\r\nThat is, comparing the output of the model on a subsequence of the data is _not_ the same as the corresponding subsequence of the output, even when the MLP should not have any interactions across the time-dimension.\r\n\r\nCan you confirm this is expected behaviour? I notice that GPUs (T4 on colab) has more inconsistencies than CPU, so I assume this is some sort of computional optimizations. \r\nIs there anyway to guarantee that the outputs match? I have tried  `tf.config.experimental.enable_op_determinism()`.\r\n\r\nThanks!\r\nErik\n\n### Standalone code to reproduce the issue\n\n```shell\nColab: https:\/\/colab.research.google.com\/drive\/1ffw4XhJKocu71ORANA_70eG7Tl20-cLh?usp=sharing\r\n\r\n\r\nCode from colab:\r\n\r\nimport tensorflow as tf\r\n\r\ntf.keras.utils.set_random_seed(1)\r\ntf.config.experimental.enable_op_determinism()\r\n\r\nclass BrokenModel(tf.keras.Model):\r\n\r\n    def __init__(self,):\r\n        super().__init__()\r\n\r\n        self.dense1 = tf.keras.layers.Dense(\r\n            32, activation=None,\r\n        )\r\n        self.dense2 = tf.keras.layers.Dense(\r\n            256 , activation=None,\r\n        )\r\n\r\n    def call(self, x: tf.Tensor):\r\n        dims = tf.shape(x)\r\n\r\n        # First dense\r\n        x = self.dense1(x)\r\n\r\n        # Flatten last two dimensions\r\n        x = tf.reshape(\r\n            x, (dims[0], dims[1],  -1)\r\n        )\r\n\r\n        # Second dense\r\n        x = self.dense2(x)\r\n        return x\r\n\r\ndef evaluate_subsequence_consistency(\r\n    model,\r\n    data,\r\n):\r\n\r\n    orig_output = model(data)\r\n\r\n    for end_idx in range(0, data.shape[1]):\r\n        # Slice subsequence\r\n        output = model(data[:, :end_idx])\r\n\r\n        # Check if output is the same\r\n        res_str = \"\"\r\n        for j in range(0, end_idx):\r\n            # Assumes output has shape [B, T, ...]\r\n            # Check if each time step is identical between subsequence and full sequence\r\n            orig_data = orig_output[:, j].numpy()\r\n            new_data = output[:, j].numpy()\r\n\r\n            if tf.experimental.numpy.allclose(orig_data, new_data):\r\n                res_str += \".\"\r\n            else:\r\n                res_str += \"X\"\r\n\r\n        print(f\"model(data)[:, :{end_idx}] vs model(data[:, :{end_idx}]) => {res_str}\")\r\n\r\nmodel = BrokenModel()\r\ndata = tf.ones((1, 313, 33, 3))\r\nevaluate_subsequence_consistency(model, data)\n```\n\n\n### Relevant log output\n\n```shell\nX = output does not match, . = output is matching\r\n\r\nmodel(data)[:, :1] vs model(data[:, :1]) => X\r\nmodel(data)[:, :2] vs model(data[:, :2]) => XX\r\nmodel(data)[:, :3] vs model(data[:, :3]) => XXX\r\nmodel(data)[:, :4] vs model(data[:, :4]) => XXXX\r\nmodel(data)[:, :5] vs model(data[:, :5]) => XXXXX\r\nmodel(data)[:, :6] vs model(data[:, :6]) => XXXXXX\r\nmodel(data)[:, :7] vs model(data[:, :7]) => XXXXXXX\r\nmodel(data)[:, :8] vs model(data[:, :8]) => XXXXXXXX\r\nmodel(data)[:, :9] vs model(data[:, :9]) => XXXXXXXXX\r\nmodel(data)[:, :10] vs model(data[:, :10]) => XXXXXXXXXX\r\nmodel(data)[:, :11] vs model(data[:, :11]) => XXXXXXXXXXX\r\nmodel(data)[:, :12] vs model(data[:, :12]) => XXXXXXXXXXXX\r\nmodel(data)[:, :13] vs model(data[:, :13]) => XXXXXXXXXXXXX\r\nmodel(data)[:, :14] vs model(data[:, :14]) => XXXXXXXXXXXXXX\r\nmodel(data)[:, :15] vs model(data[:, :15]) => XXXXXXXXXXXXXXX\r\nmodel(data)[:, :16] vs model(data[:, :16]) => XXXXXXXXXXXXXXXX\r\nmodel(data)[:, :17] vs model(data[:, :17]) => XXXXXXXXXXXXXXXXX\r\nmodel(data)[:, :18] vs model(data[:, :18]) => XXXXXXXXXXXXXXXXXX\r\nmodel(data)[:, :19] vs model(data[:, :19]) => XXXXXXXXXXXXXXXXXXX\r\nmodel(data)[:, :20] vs model(data[:, :20]) => XXXXXXXXXXXXXXXXXXXX\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2024-02-07T15:08:07Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62917"},{"issue_number":250,"repository":"tensorflow\/tensorflow","title":"Cannot disable XLA and\/or JIT","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.15.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04.3\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.12 (Unrelated)\r\n\r\n### Bazel version\r\n\r\n6.3.2\r\n\r\n### GCC\/compiler version\r\n\r\nClang 18\r\n\r\n### CUDA\/cuDNN version\r\n\r\n12.2\r\n\r\n### GPU model and memory\r\n\r\nA100 and A10\r\n\r\n### Current behavior?\r\n\r\nAfter upgrading from `2.12.1` to `2.15.0` we observed a lot **new** logs when starting a **C++** service using Tensorflow, that it calls `ptxas` to compile some generated PTX, a snippet is attached below.\r\n\r\nWe tried a bunch of options in TF_XLA_FLAGS such as `--tf_xla_auto_jit=-1`, `--tf_mlir_enable_mlir_bridge=0`, `--tf_xla_cpu_global_jit=0`, `--tf_xla_clustering_fuel=0`, etc. but it still compiles those ops in the pass of `CreateGpuKernelToBlobPass` anyway.\r\n\r\nI wonder if anything related to XLA \/ JIT changed between `2.12.1` and `2.15.0`? And is there a way to simply disable all XLA and JIT?\r\n\r\nBTW: Both Tensorflow versions were built with XLA and CUDA supports, with `TF_CUDA_COMPUTE_CAPABILITIES=\"7.0,7.5,8.0,8.6,9.0\"`.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nIt is a C++ service which loads model and do inference, we do not currently have a minimal example at this time. However, we can try providing detailed context as much as possible if we can narrow the scenario down to a considerable small portion of the service.\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-01-29 15:14:43.203921: I external\/local_xla\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:565] Compile module main_kernel_0 time: 7.33 ms (cumulative: 82.4 ms, max: 9.23 ms, #called: 15)2024-01-29 15:14:43.204021: I external\/local_xla\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:565] Compile module main_kernel time: 7.43 ms (cumulative: 89.9 ms, max: 9.23 ms, #called: 16)2024-01-29 15:14:43.204084: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:263] ptx written to: \/tmp\/tempfile-jscs02-ai-deep-dev-4a10-01-d460ae06-1673-61010665dbba92024-01-29 15:14:43.204112: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:295] \/usr\/local\/cuda-12.2\/bin\/ptxas \/tmp\/tempfile-jscs02-ai-deep-dev-4a10-01-d460ae06-1673-61010665dbba9 -o \/tmp\/tempfile-jscs02-ai-deep-dev-4a10-01-d460ae06-1673-61010665dbc05 -arch=sm_86 --warn-on-spills -v 2024-01-29 15:14:43.204182: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:263] ptx written to: \/tmp\/tempfile-jscs02-ai-deep-dev-4a10-01-b79066f7-1673-61010665dbc0d2024-01-29 15:14:43.204209: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:295] \/usr\/local\/cuda-12.2\/bin\/ptxas \/tmp\/tempfile-jscs02-ai-deep-dev-4a10-01-b79066f7-1673-61010665dbc0d -o \/tmp\/tempfile-jscs02-ai-deep-dev-4a10-01-b79066f7-1673-61010665dbc66 -arch=sm_86 --warn-on-spills -v2024-01-29 15:14:43.204334: I external\/local_xla\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:565] Compile module main_kernel_1 time: 7.71 ms (cumulative: 97.6 ms, max: 9.23 ms, #called: 17)2024-01-29 15:14:43.204499: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:263] ptx written to: \/tmp\/tempfile-jscs02-ai-deep-dev-4a10-01-7941e65c-1673-61010665dbd48\r\n2024-01-29 15:14:43.204528: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:295] \/usr\/local\/cuda-12.2\/bin\/ptxas \/tmp\/tempfile-jscs02-ai-deep-dev-4a10-01-7941e65c-1673-61010665dbd48 -o \/tmp\/tempfile-jscs02-ai-deep-dev-4a10-01-7941e65c-1673-61010665dbda4 -arch=sm_86 --warn-on-spills -v2024-01-29 15:14:43.236003: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:333] ptxas info    : 0 bytes gmem\r\nptxas info    : Compiling entry function 'main_kernel' for 'sm_86'ptxas info    : Function properties for main_kernel    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loadsptxas info    : Used 12 registers, 452 bytes cmem[0]\r\n2024-01-29 15:14:43.236589: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:333] ptxas info    : 0 bytes gmem\r\nptxas info    : Compiling entry function 'main_kernel' for 'sm_86'ptxas info    : Function properties for main_kernel    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 14 registers, 476 bytes cmem[0]\r\n\r\n2024-01-29 15:14:43.237263: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:333] ptxas info    : 0 bytes gmem\r\nptxas info    : Compiling entry function 'main_kernel' for 'sm_86'\r\nptxas info    : Function properties for main_kernel\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 14 registers, 508 bytes cmem[0]\r\n\r\n2024-01-29 15:14:43.238685: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:333] ptxas info    : 0 bytes gmem\r\nptxas info    : Compiling entry function 'main_kernel' for 'sm_86'\r\nptxas info    : Function properties for main_kernel\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 14 registers, 532 bytes cmem[0]\r\n\r\n2024-01-29 15:14:43.240855: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:333] ptxas info    : 0 bytes gmem\r\nptxas info    : Compiling entry function 'main_kernel' for 'sm_86'\r\nptxas info    : Function properties for main_kernel\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 15 registers, 564 bytes cmem[0]\r\n\r\n2024-01-29 15:14:43.241720: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:333] ptxas info    : 0 bytes gmem\r\nptxas info    : Compiling entry function 'main_kernel' for 'sm_86'\r\nptxas info    : Function properties for main_kernel\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 18 registers, 444 bytes cmem[0]\r\n\r\n2024-01-29 15:14:43.241731: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:333] ptxas info    : 0 bytes gmem\r\nptxas info    : Compiling entry function 'main_kernel' for 'sm_86'\r\nptxas info    : Function properties for main_kernel\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 16 registers, 444 bytes cmem[0]\r\n\r\n2024-01-29 15:14:43.242423: I external\/local_xla\/xla\/stream_executor\/gpu\/asm_compiler.cc:333] ptxas info    : 0 bytes gmem\r\nptxas info    : Compiling entry function 'main_kernel' for 'sm_86'\r\nptxas info    : Function properties for main_kernel\r\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\r\nptxas info    : Used 18 registers, 452 bytes cmem[0]\r\n\r\nlibunwind: __unw_add_dynamic_fde: bad fde: FDE is really a CIE\r\n2024-01-29 15:14:43.416923: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:812] GpuDevice::ComputeHelper scheduled dense\/clip_by_value_65 op Maximum on GPU 0 stream[0]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.15"],"created_at":"2024-01-29T16:09:44Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62864"},{"issue_number":251,"repository":"tensorflow\/tensorflow","title":"tf.concat (and tf.transpose) inside a for loop with tf.range in the context of a GradientTape while using XLA dosn't work","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.15.0.post1, 2.15.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nWSL Ubuntu 22.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.12\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n12.2\r\n\r\n### GPU model and memory\r\n\r\nRTX 3080 Ti\r\n\r\n### Current behavior?\r\n\r\n### Current behavior\r\nEven tho the shapes of the elements which are concatenated are well defined to compile time, when running the code provided, the execution fails with the error log provided. This is also the case, if you enforce the shapes with using tf.reshape(). \r\nIf you remove either \r\n- the GradientTape context\r\n- or the for loop with tf.range\r\n- or the XLA compilation \r\n\r\nthe code will work as expected.\r\n\r\nUnrolling the for loop with range() is not desired since the number of iterations will be >50000 in the final project. Also converting the for loop in a tf.while loop with maximum_iterations specified will result in the same error. Specifying the batchsize to a constant value (also in the input_signature) won't resolve the issue either. Also changing the tf.device between GPU \/ CPU won't resolve the issue.\r\nThe same error arises if you try to use tf.transpose()\r\n\r\n\r\n### Expected behavior\r\nThe arrays with well defined shapes at compile time will be concatenated \/ transposed when using a for loop with tf.range in the context of a GradientTape while using XLA (jit_compile=True)\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nclass Model():\r\n    def __init__(self, batchsize):\r\n        self.batchsize = batchsize\r\n        self.g = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=\"g\")\r\n        self.m = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=\"m\")\r\n        self.d = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=\"d\")\r\n        self.k = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=\"k\")\r\n        self.y0 = tf.ones([self.batchsize, 2, 1])\r\n\r\n\r\n    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 2, 1), dtype=tf.float32)],\r\n                 jit_compile=False)\r\n    def compute(self, yi):\r\n        A1 = tf.concat([tf.zeros_like(self.m), tf.ones_like(self.m)], 2)\r\n        A2 = tf.concat([-self.k\/self.m, -self.d\/self.m], 2)\r\n        A = tf.concat([A1, A2], 1)\r\n        #A = tf.transpose(A, perm=[0, 2, 1])\r\n        B = tf.concat([tf.zeros_like(self.g), self.g], 1)\r\n\r\n        dy = tf.linalg.matmul(A, yi) + B\r\n        return dy\r\n    \r\n\r\nclass Estimator():\r\n    def __init__(self, model):\r\n        self.model = model\r\n        self.dt = tf.constant(0.001)\r\n\r\n    @tf.function(jit_compile=True, reduce_retracing=False)\r\n    def estimate(self):\r\n        with tf.GradientTape() as tape:\r\n            yi = self.model.y0\r\n            for i in tf.range(10):\r\n                dyi = self.model.compute(yi)\r\n                yi = yi + dyi*self.dt\r\n        grads = tape.gradient(yi, [self.model.g, self.model.m, self.model.d, self.model.k])\r\n        return grads\r\n\r\n\r\n\r\ndevice = \"CPU:0\"\r\n#device = \"GPU:0\"\r\n\r\nwith tf.device(device):    \r\n    batchsize = 5\r\n    model = Model(batchsize)\r\n    estimator = Estimator(model)\r\n    grads = estimator.estimate()\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nOP_REQUIRES failed at concat_op.cc:168 : INVALID_ARGUMENT: Input 0 to node `gradient_tape\/while\/gradients\/while\/StatefulPartitionedCall_grad\/PartitionedCall\/gradients\/concat_3_grad\/ConcatOffset` with op ConcatOffset must be a compile-time constant.\r\n\r\nXLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.15"],"created_at":"2024-01-29T09:22:38Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62861"},{"issue_number":252,"repository":"tensorflow\/tensorflow","title":"Tensorflow distributes training throws exception on mac m2","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMac OS 14.2.1\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.6\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI am trying to run distributed training using `tf.distribute.experimental.MultiWorkerMirroredStrategy()` on two Mac M2 machines. However, training does not start on the GPU, and the code throws the attached exception.\r\nThe distributed training works fine if I use CPU only.\r\nI have installed the latest `tensorflow-metal 1.1.0`.\r\n\r\nIs `MultiWorkerMirroredStrategy` supported on Mac M2?\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom pandas import read_csv\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import LabelEncoder\r\nfrom tensorflow.keras import Sequential\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense\r\nimport datetime\r\nimport os\r\nimport keras\r\nimport json\r\nimport glob\r\n\r\n\r\nprint(\"Tensforflow version: \", tf.__version__)\r\nprint(\"Availabe devices: \", devices)\r\nif len(devices) == 0:\r\n    print(\"No devices for mac found\")\r\n    exit(1)\r\n\r\n    \r\n\r\ndirectory = os.environ['TF_FOLDER']\r\n\r\ncheckpoint_dir = os.path.join(directory, \"ckpt\")\r\nif not os.path.exists(checkpoint_dir):\r\n    os.makedirs(checkpoint_dir)\r\n\r\npath = 'https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/ionosphere.csv'\r\ndf = read_csv(path, header=None)\r\n\r\nX, y = df.values[:, :-1], df.values[:, -1]\r\n\r\nX = X.astype('float32')\r\n\r\ny = LabelEncoder().fit_transform(y)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\r\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\r\n\r\nn_features = X_train.shape[1]\r\n\r\n\r\ndef get_compiled_model():\r\n       model = tf.keras.Sequential([\r\n            tf.keras.layers.Flatten(input_shape=(n_features,)),\r\n            tf.keras.layers.Dense(128, activation='relu'),\r\n            tf.keras.layers.Dense(10)\r\n        ])\r\n       model.summary()\r\n       model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\r\n       return model\r\n\r\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\nprint(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\r\n\r\nwith strategy.scope():\r\n    model = get_compiled_model()\r\n\r\nlog_dir = os.path.join(directory, \"logs\/fit\/\") + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\r\n\r\n\r\nclass CustomModelCheckpoint(keras.callbacks.ModelCheckpoint):\r\n    def __init__(self, filepath, max_to_keep=100, **kwargs):\r\n        super().__init__(filepath, **kwargs)\r\n        self.filepath = filepath\r\n        self.max_to_keep = max_to_keep\r\n\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        super().on_epoch_end(epoch, logs)\r\n        files = sorted(glob.glob(self.filepath.format(epoch='*')))\r\n        if len(files) > self.max_to_keep:\r\n            for f in files[:-self.max_to_keep]:\r\n                os.remove(f)\r\n\r\n\r\ncallbacks = [\r\n        CustomModelCheckpoint(\r\n            filepath=checkpoint_dir + \"\/ckpt-{epoch}\", save_freq=\"epoch\", max_to_keep=10, save_weights_only=True\r\n        ),\r\n       keras.callbacks.TensorBoard('tensorboard_logs')\r\n    ]\r\n\r\nlatest = tf.train.latest_checkpoint(checkpoint_dir)\r\nif latest:\r\n    print(\"Loading model checkpoint {} ...\\n\".format(latest))\r\n    model.load_weights(latest)\r\n\r\nmodel.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1,callbacks=callbacks)\r\n\r\nloss, acc = model.evaluate(X_test, y_test, verbose=0)\r\nprint(f'Test Accuracy: {acc:.3f}')\n```\n\n\n### Relevant log output\n\n```shell\n2024-01-28 14:46:40.395499: I tensorflow\/core\/grappler\/optimizers\/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\r\nEpoch 1\/100\r\n2024-01-28 14:46:40.778281: W tensorflow\/core\/framework\/op_kernel.cc:1803] INTERNAL: Failed to build OpKernel for Add : No registered 'Add' OpKernel for 'GPU' devices compatible with node {{node Add}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_INT64, _device=\"\/job:worker\/replica:0\/task:0\/device:GPU:0\"\r\n\t.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_BFLOAT16]\r\n  device='DEFAULT'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_STRING]\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/inet11\/git\/tensorflow\/model.py\", line 129, in <module>\r\n    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1,callbacks=callbacks)\r\n  File \"\/Users\/inet11\/git\/tensorflow\/email\/tf\/lib\/python3.11\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 70, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/Users\/inet11\/git\/tensorflow\/email\/tf\/lib\/python3.11\/site-packages\/tensorflow\/python\/eager\/execute.py\", line 53, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntensorflow.python.framework.errors_impl.InternalError: Graph execution error:\r\n\r\nDetected at node Add defined at (most recent call last):\r\n<stack traces unavailable>\r\n2 root error(s) found.\r\n  (0) INTERNAL:  Failed to build OpKernel for Add : No registered 'Add' OpKernel for 'GPU' devices compatible with node {{node Add}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_INT64, _device=\"\/job:worker\/replica:0\/task:0\/device:GPU:0\"\r\n\t.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_BFLOAT16]\r\n  device='DEFAULT'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_HALF]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]\r\n  device='CPU'; T in [DT_INT8]\r\n  device='CPU'; T in [DT_INT16]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_UINT8]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_STRING]\r\n\r\n\t [[CollectiveReduceV2]]\r\n  (1) CANCELLED:  Function was cancelled before it was started\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_1260]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:dist-strat","TF 2.15"],"created_at":"2024-01-28T23:20:51Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62859"},{"issue_number":253,"repository":"tensorflow\/tensorflow","title":"RAM memory leak with tf.function when training multiple models in a loop","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 22.04.3 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\nn\/a\r\n\r\n### GPU model and memory\r\n\r\nn\/a\r\n\r\n### Current behavior?\r\n\r\nWhen I train multiple models in a loop, if I decorate the `train()` function with `@tf.function`, then the memory usage keeps on increasing after each loop iteration, even when I delete the model at the end of each loop and clear the TensorFlow graph\/session.\r\n\r\nThe memory leak does not occur when `@tf.function` is removed. However, model training performance is significantly slower.\r\n\r\nColab notebook to reproduce issue:\r\nhttps:\/\/colab.research.google.com\/drive\/1sJsGmcFeZVx6ImNbqnBsgF_LzIyPxXPW?usp=sharing\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport gc\r\nimport os\r\n\r\nimport psutil\r\nimport tensorflow as tf\r\n\r\n\r\nclass MyModel:\r\n    def __init__(self):\r\n        self.dnn = tf.keras.Sequential([\r\n            tf.keras.layers.Dense(256),\r\n            tf.keras.layers.Dense(256),\r\n            tf.keras.layers.Dense(256),\r\n            tf.keras.layers.Dense(1),\r\n        ])\r\n        self.optimizer = tf.optimizers.Adam()\r\n    \r\n    @tf.function   # if we remove this @tf.function decorator, then there is no memory leak\r\n    def train(self, X):\r\n        with tf.GradientTape() as tape:\r\n            loss = tf.reduce_sum(self.dnn(X))\r\n        grads = tape.gradient(loss, self.dnn.trainable_variables)\r\n        self.optimizer.apply_gradients(zip(grads, self.dnn.trainable_variables))\r\n\r\nprocess = psutil.Process(os.getpid())\r\nrss = int(process.memory_info().rss \/ 1024 \/ 1024)  # in MB\r\nprint(f'rss: {rss} MB')\r\n\r\nX = tf.ones((50, 80))\r\nfor i in range(50):\r\n    model = MyModel()\r\n    for _ in range(20):\r\n        model.train(X)\r\n\r\n    del model\r\n    tf.keras.backend.clear_session()\r\n    tf.compat.v1.reset_default_graph()\r\n    gc.collect()\r\n\r\n    new_rss = int(process.memory_info().rss \/ 1024 \/ 1024)\r\n    if new_rss > rss:\r\n        rss_increase = new_rss - rss\r\n        rss = new_rss\r\n        print(f'Iter {i:05d}, rss increase: {rss_increase} MB, rss: {rss} MB')\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nrss: 593 MB\r\nIter 00000, rss increase: 31 MB, rss: 624 MB\r\nIter 00001, rss increase: 6 MB, rss: 630 MB\r\nIter 00002, rss increase: 5 MB, rss: 635 MB\r\nIter 00003, rss increase: 5 MB, rss: 640 MB\r\nIter 00004, rss increase: 5 MB, rss: 645 MB\r\nIter 00005, rss increase: 5 MB, rss: 650 MB\r\nIter 00006, rss increase: 5 MB, rss: 655 MB\r\nIter 00007, rss increase: 5 MB, rss: 660 MB\r\nIter 00008, rss increase: 5 MB, rss: 665 MB\r\nIter 00009, rss increase: 5 MB, rss: 670 MB\r\nIter 00010, rss increase: 4 MB, rss: 674 MB\r\n---  <omitting some rows for brevity>  ---\r\nIter 00040, rss increase: 5 MB, rss: 822 MB\r\nIter 00041, rss increase: 5 MB, rss: 827 MB\r\nIter 00042, rss increase: 5 MB, rss: 832 MB\r\nIter 00043, rss increase: 5 MB, rss: 837 MB\r\nIter 00044, rss increase: 5 MB, rss: 842 MB\r\nIter 00045, rss increase: 5 MB, rss: 847 MB\r\nIter 00046, rss increase: 5 MB, rss: 852 MB\r\nIter 00047, rss increase: 5 MB, rss: 857 MB\r\nIter 00048, rss increase: 5 MB, rss: 862 MB\r\nIter 00049, rss increase: 5 MB, rss: 867 MB\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:tf.function","TF 2.15"],"created_at":"2024-01-27T05:06:09Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62854"},{"issue_number":254,"repository":"tensorflow\/tensorflow","title":"[ tf-opt ] Keras Official Implementation of Stable Diffusion fails to generate stablehlo mlir","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.14.0-rc1-21-g4dacf3f368e 2.14.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nAfter saving the diffusion model in saved_model format using the code given below, I ran the following command to convert the saved_model into an input mlir. \r\n\r\n```\r\ntf-mlir-translate --savedmodel-objectgraph-to-mlir --tf-savedmodel-exported-names=serving_fn .\/signed-model\/ -o tf_executor.mlir\r\n```\r\n\r\nThis actually produces a large mlir file of 6.5 GB . Then I ran this: \r\n``` tf-opt --tf-executor-graph-pruning --tf-executor-to-functional-conversion --canonicalize --tf-lower-to-mlprogram-and-hlo .\/tf_executor.mlir -o stablehlo.mlir```\r\n\r\nThis process runs for a couple of minutes and ends without any error. However, in the ```stablehlo.mlir``` does not have any mlir in it. Its an empty file with the following in it: \"module{}\". It does not provide any error message at all. \r\n\r\nI was expecting the second command to generate a stablehlo mlir file corresponding to the diffusion model which I could later pass on to the iree-importer for importing into iree. \r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom keras_cv import models\r\nfrom keras_cv.src.models.stable_diffusion.constants import _ALPHAS_CUMPROD\r\nimport tensorflow as tf\r\nfrom keras_cv.src.backend import ops\r\nimport math\r\nfrom keras_cv.models.stable_diffusion import DiffusionModel\r\n\r\nIMG_HEIGHT = 512\r\nIMG_WIDTH = 512\r\nMAX_PROMPT_LENGTH = 77\r\nALPHAS_CUMPROD_TF = tf.constant(_ALPHAS_CUMPROD)\r\nUNCONDITIONAL_GUIDANCE_SCALE = 7.5\r\nHIDDEN_DIM = 768\r\nSEED = None\r\n\r\n\r\nsignature_dict = {\r\n    \"context\": tf.TensorSpec(shape=[None, MAX_PROMPT_LENGTH, HIDDEN_DIM], dtype=tf.float32, name=\"context\"),\r\n    \"unconditional_context\": tf.TensorSpec(\r\n        shape=[None, MAX_PROMPT_LENGTH, HIDDEN_DIM], dtype=tf.float32, name=\"unconditional_context\"\r\n    ),\r\n    \"num_steps\": tf.TensorSpec(shape=[], dtype=tf.int32, name=\"num_steps\"),\r\n    \"batch_size\": tf.TensorSpec(shape=[], dtype=tf.int32, name=\"batch_size\"),\r\n}\r\n\r\ndef diffusion_model_exporter(model: tf.keras.Model):\r\n    @tf.function\r\n    def get_timestep_embedding(timestep, batch_size, dim=320, max_period=10000):\r\n        half = dim \/\/ 2\r\n        range = ops.cast(ops.arange(0, half), \"float32\")\r\n        freqs = ops.exp(-math.log(max_period) * range \/ half)\r\n        args = ops.convert_to_tensor([timestep], dtype=\"float32\") * freqs\r\n        embedding = ops.concatenate([ops.cos(args), ops.sin(args)], 0)\r\n        embedding = ops.reshape(embedding, [1, -1])\r\n        return ops.repeat(embedding, batch_size, axis=0)\r\n    \r\n    @tf.function(input_signature=[signature_dict])\r\n    def serving_fn(inputs):\r\n        img_height = tf.cast(tf.math.round(IMG_HEIGHT \/ 128) * 128, tf.int32)\r\n        img_width = tf.cast(tf.math.round(IMG_WIDTH \/ 128) * 128, tf.int32)\r\n\r\n        batch_size = inputs[\"batch_size\"]\r\n        num_steps = inputs[\"num_steps\"]\r\n\r\n        context = inputs[\"context\"]\r\n        unconditional_context = inputs[\"unconditional_context\"]\r\n\r\n        latent = tf.random.normal((batch_size, img_height \/\/ 8, img_width \/\/ 8, 4))\r\n\r\n        timesteps = tf.range(1, 1000, 1000 \/\/ num_steps)\r\n        alphas = tf.map_fn(lambda t: ALPHAS_CUMPROD_TF[t], timesteps, dtype=tf.float32)\r\n        alphas_prev = tf.concat([[1.0], alphas[:-1]], 0)\r\n\r\n        index = num_steps - 1\r\n        latent_prev = None\r\n        for timestep in timesteps[::-1]:\r\n            latent_prev = latent\r\n            t_emb = get_timestep_embedding(timestep, batch_size)\r\n            unconditional_latent = model(\r\n                [latent, t_emb, unconditional_context], training=False\r\n            )\r\n            latent = model([latent, t_emb, context], training=False)\r\n            latent = unconditional_latent + UNCONDITIONAL_GUIDANCE_SCALE * (\r\n                latent - unconditional_latent\r\n            )\r\n            a_t, a_prev = alphas[index], alphas_prev[index]\r\n            pred_x0 = (latent_prev - tf.math.sqrt(1 - a_t) * latent) \/ tf.math.sqrt(a_t)\r\n            latent = (\r\n                latent * tf.math.sqrt(1.0 - a_prev) + tf.math.sqrt(a_prev) * pred_x0\r\n            )\r\n            index = index - 1\r\n\r\n        return {\"latent\": latent}\r\n\r\n    return serving_fn\r\n\r\ndiffuser = DiffusionModel(IMG_HEIGHT, IMG_WIDTH, MAX_PROMPT_LENGTH)\r\n\r\ntf.saved_model.save(diffuser, '.\/stable-diffusion\/signed-model', signatures={\"serving_default\": diffusion_model_exporter(diffuser)} )\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF2.14"],"created_at":"2024-01-25T20:19:45Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62847"},{"issue_number":255,"repository":"tensorflow\/tensorflow","title":"Multi-GPU training with gradient propagation in FP16 with XLA fails.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.14\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n11.8\/8.7\r\n\r\n### GPU model and memory\r\n\r\nA100 40GB (20GB MiG)\r\n\r\n### Current behavior?\r\n\r\nFollowing advice given [here](https:\/\/www.youtube.com\/watch?v=6ovfZW8pepo), I implemented gradient propagation in FP16 and concurrent backpropagation with gradient propagation. I am using mixed precision with XLA enabled.\r\n\r\nThe custom training step works fine when XLA is not enabled. However, when I enable it, I get the error that you can see in the log.\r\n\r\nThis is run on 2 GPUs using MultiWorkerMirroredStrategy.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport json\r\nimport os\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef get_replica_hostnames():\r\n    ...\r\n\r\n\r\ndef get_replica_id():\r\n    ...\r\n\r\n\r\ndef set_multiworker_env_config():\r\n    hostnames = get_replica_hostnames()\r\n    replica_index = get_replica_id()\r\n\r\n    os.environ[\"TF_CONFIG\"] = json.dumps(\r\n        {\r\n            \"cluster\": {\r\n                \"worker\": hostnames,\r\n            },\r\n            \"task\": {\"type\": \"worker\", \"index\": replica_index},\r\n        }\r\n    )\r\n\r\n\r\nclass Model(tf.keras.models.Model):\r\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(*args, **kwargs)\r\n\r\n        self._embedder = tf.keras.Sequential(\r\n            [\r\n                tf.keras.layers.Conv2D(\r\n                    filters=8,\r\n                    kernel_size=3,\r\n                    padding=\"same\",\r\n                    activation=tf.keras.activations.relu,\r\n                    use_bias=False,\r\n                ),\r\n                tf.keras.layers.BatchNormalization(),\r\n                tf.keras.layers.Conv2D(\r\n                    filters=8,\r\n                    kernel_size=3,\r\n                    padding=\"same\",\r\n                    activation=tf.keras.activations.relu,\r\n                    use_bias=False,\r\n                ),\r\n                tf.keras.layers.BatchNormalization(),\r\n                tf.keras.layers.MaxPool2D(),\r\n                tf.keras.layers.GlobalAveragePooling2D(),\r\n            ]\r\n        )\r\n\r\n        self._classifier = tf.keras.layers.Dense(550)\r\n\r\n    def call(self, x: tf.Tensor) -> tf.Tensor:\r\n        x = self._embedder(x)\r\n        x = self._classifier(x)\r\n        x = tf.keras.layers.Activation(\"linear\", dtype=\"float32\")(x)\r\n        return x\r\n\r\n    def train_step(self, data):\r\n        x, y, sample_weight = tf.keras.utils.unpack_x_y_sample_weight(data)\r\n\r\n        # Run forward pass.\r\n        with tf.GradientTape() as tape:\r\n            y_pred = self(x, training=True)\r\n            loss = self.compute_loss(x, y, y_pred, sample_weight)\r\n            loss = self.optimizer.get_scaled_loss(loss)\r\n\r\n        self._validate_target_and_loss(y, loss)\r\n\r\n        # Run backward pass.\r\n        grads = tape.gradient(loss, self.trainable_variables)\r\n        grads = [tf.cast(grad, \"float16\") for grad in grads]\r\n        all_reduced_grads = tf.distribute.get_replica_context().all_reduce(\r\n            tf.distribute.ReduceOp.SUM,\r\n            grads,\r\n            tf.distribute.experimental.CommunicationOptions(\r\n                bytes_per_pack=50 * 1024 * 1024,\r\n            ),\r\n        )\r\n        all_reduced_grads = [tf.cast(grad, \"float32\") for grad in all_reduced_grads]\r\n        all_reduced_grads = self.optimizer.get_unscaled_gradients(all_reduced_grads)\r\n\r\n        self.optimizer.apply_gradients(\r\n            zip(all_reduced_grads, self.trainable_variables),\r\n            skip_gradients_aggregations=True,\r\n        )\r\n\r\n        return self.compute_metrics(x, y, y_pred, sample_weight)\r\n\r\n\r\ndef create_dummy_dataset(batch_size: int) -> tf.data.Dataset:\r\n    X = np.random.rand(batch_size, 384, 640, 1)\r\n    y = np.random.randint(550, size=batch_size)\r\n    return tf.data.Dataset.from_tensor_slices((X, y)).batch(batch_size, True).repeat()\r\n\r\n\r\ndef train():\r\n    tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\r\n\r\n    set_multiworker_env_config()\r\n    strategy = tf.distribute.MultiWorkerMirroredStrategy()\r\n    num_replicas = strategy.num_replicas_in_sync\r\n\r\n    batch_size = 384 * num_replicas\r\n    dataset = create_dummy_dataset(batch_size)\r\n    dataset = strategy.experimental_distribute_dataset(dataset)\r\n\r\n    with strategy.scope():\r\n        model = Model()\r\n        model.compile(\r\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(\r\n                from_logits=True,\r\n            ),\r\n            optimizer=tf.keras.optimizers.Adam(\r\n                learning_rate=1e-3,\r\n                weight_decay=1e-5,\r\n            ),\r\n            metrics=[\r\n                \"accuracy\",\r\n            ],\r\n            jit_compile=True,\r\n        )\r\n\r\n    model.fit(\r\n        dataset,\r\n        epochs=10,\r\n        steps_per_epoch=100,\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    train()\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-01-15 14:06:48.704206: E tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-01-15 14:06:48.704294: E tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-01-15 14:06:48.704350: E tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-01-15 14:06:48.712047: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-01-15 14:06:49.711421: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-01-15 14:06:51.459926: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1886] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 18184 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:01:00.0, compute capability: 8.0\r\n2024-01-15 14:06:51.469378: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1886] Created device \/job:worker\/replica:0\/task:0\/device:GPU:0 with 18184 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:01:00.0, compute capability: 8.0\r\n2024-01-15 14:06:51.490217: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:457] Started server with target: grpc:\/\/gen-svc-82ee2ab7-aae8-44b2-a5ab-1f9fd0401075:80\r\n2024-01-15 14:06:51.495668: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:551] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 3072789976930509670\r\n2024-01-15 14:06:51.495948: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service_agent.cc:299] Coordination agent has successfully connected.\r\n2024-01-15 14:06:52.263966: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:551] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 14088291762770279744\r\n2024-01-15 14:07:16.278224: W tensorflow\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.\r\n2024-01-15 14:07:17.178980: W tensorflow\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.\r\n2024-01-15 14:07:17.905980: W tensorflow\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.\r\n2024-01-15 14:07:22.444540: W tensorflow\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.\r\nEpoch 1\/10\r\n2024-01-15 14:07:24.533902: W tensorflow\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.\r\n2024-01-15 14:07:27.793986: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x7f080000c190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2024-01-15 14:07:27.794120: I tensorflow\/compiler\/xla\/service\/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB MIG 3g.20gb, Compute Capability 8.0\r\n2024-01-15 14:07:28.936911: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2024-01-15 14:07:30.163125: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:442] Loaded cuDNN version 8906\r\n2024-01-15 14:08:24.307063: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:328] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_232', 12 bytes spill stores, 12 bytes spill loads\r\n\r\n2024-01-15 14:08:24.726556: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:328] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_232', 12 bytes spill stores, 12 bytes spill loads\r\n\r\n2024-01-15 14:08:24.789581: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:328] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_232', 8 bytes spill stores, 8 bytes spill loads\r\n\r\n2024-01-15 14:08:26.317751: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:328] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_392', 92 bytes spill stores, 92 bytes spill loads\r\n\r\n2024-01-15 14:08:26.687122: W tensorflow\/core\/framework\/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:624 : UNKNOWN: <unknown>:0: error: loc(\"all-reduce-start.1\"): 'lmhlo_gpu.all_reduce_start' op requires the same element type for all operands\r\n<unknown>:0: note: loc(\"all-reduce-start.1\"): see current operation: \r\n%225 = \"lmhlo_gpu.all_reduce_start\"(%222, %8, %224, %18, %220, %222, %8, %224, %18, %220) ({\r\n^bb0(%arg58: tensor<f32>, %arg59: tensor<f32>):\r\n  %251 = \"mhlo.add\"(%arg58, %arg59) : (tensor<f32>, tensor<f32>) -> tensor<f32>\r\n  \"mhlo.return\"(%251) : (tensor<f32>) -> ()\r\n}) {channel_id = #mhlo.channel_handle<handle = 4294967300, type = 0>, constrain_layout = false, is_sync = true, replica_groups = dense<> : tensor<0x0xi64>, use_global_device_ids = false} : (memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>) -> !mhlo.token\r\n\r\n2024-01-15 14:08:26.687331: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10204104846648882243\r\n2024-01-15 14:08:26.687358: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5394287870414942276\r\n2024-01-15 14:08:26.687399: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8969947325169437454\r\nTraceback (most recent call last):\r\n  File \"\/kirax_source\/train.py\", line 133, in <module>\r\n    train()\r\n  File \"\/kirax_source\/train.py\", line 125, in train\r\n    model.fit(\r\n  File \"\/usr\/local\/lib\/python3.11\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 70, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/usr\/local\/lib\/python3.11\/dist-packages\/tensorflow\/python\/eager\/execute.py\", line 60, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntensorflow.python.framework.errors_impl.UnknownError: Graph execution error:\r\n\r\nDetected at node StatefulPartitionedCall defined at (most recent call last):\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1002, in _bootstrap\r\n\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1045, in _bootstrap_inner\r\n\r\nDetected at node StatefulPartitionedCall defined at (most recent call last):\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1002, in _bootstrap\r\n\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1045, in _bootstrap_inner\r\n\r\n2 root error(s) found.\r\n  (0) UNKNOWN:  <unknown>:0: error: loc(\"all-reduce-start.1\"): 'lmhlo_gpu.all_reduce_start' op requires the same element type for all operands\r\n<unknown>:0: note: loc(\"all-reduce-start.1\"): see current operation: \r\n%225 = \"lmhlo_gpu.all_reduce_start\"(%222, %8, %224, %18, %220, %222, %8, %224, %18, %220) ({\r\n^bb0(%arg58: tensor<f32>, %arg59: tensor<f32>):\r\n  %251 = \"mhlo.add\"(%arg58, %arg59) : (tensor<f32>, tensor<f32>) -> tensor<f32>\r\n  \"mhlo.return\"(%251) : (tensor<f32>) -> ()\r\n}) {channel_id = #mhlo.channel_handle<handle = 4294967300, type = 0>, constrain_layout = false, is_sync = true, replica_groups = dense<> : tensor<0x0xi64>, use_global_device_ids = false} : (memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>) -> !mhlo.token\r\n\r\n         [[{{node StatefulPartitionedCall}}]]\r\n         [[Reshape_3\/_24]]\r\n  (1) UNKNOWN:  <unknown>:0: error: loc(\"all-reduce-start.1\"): 'lmhlo_gpu.all_reduce_start' op requires the same element type for all operands\r\n<unknown>:0: note: loc(\"all-reduce-start.1\"): see current operation: \r\n%225 = \"lmhlo_gpu.all_reduce_start\"(%222, %8, %224, %18, %220, %222, %8, %224, %18, %220) ({\r\n^bb0(%arg58: tensor<f32>, %arg59: tensor<f32>):\r\n  %251 = \"mhlo.add\"(%arg58, %arg59) : (tensor<f32>, tensor<f32>) -> tensor<f32>\r\n  \"mhlo.return\"(%251) : (tensor<f32>) -> ()\r\n}) {channel_id = #mhlo.channel_handle<handle = 4294967300, type = 0>, constrain_layout = false, is_sync = true, replica_groups = dense<> : tensor<0x0xi64>, use_global_device_ids = false} : (memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>) -> !mhlo.token\r\n\r\n         [[{{node StatefulPartitionedCall}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_2528]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:dist-strat","TF2.14"],"created_at":"2024-01-15T14:56:06Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62796"},{"issue_number":256,"repository":"tensorflow\/tensorflow","title":"tf.data.Dataset.map() makes unnecessary memory allocations","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.15.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen `tf.data.Dataset.map` is called with a function which references tensor(s) (or a nested object with tensors), it seems to be making a copy of these tensors. If these tensors are large, this causes large memory allocations which can cause the process to OOM.\r\n\r\nPlease see the below code snippet which reproduces the issue ([here](https:\/\/colab.research.google.com\/drive\/1UykDpf0FefrcNyCgPW9-KW-7zCQFbzTg#scrollTo=OtY6ihLwfvNJ) is a reference to the colab). I am allocating a tensor which takes 2GB of memory and then referencing it in the `get_data` function. This function is used in `tf.data.Dataset.map` to construct the dataset. I am chaining multiple `map` calls to exacerbate the bug to cause OOM in colab. Each `map` call allocates a new copy of the original tensor referenced by the passed function.\r\n\r\nPlease note that this is not a memory leak as these copies are subsequently freed and the memory is released back to the mem allocator. However, depending on the allocator and it's settings, the allocator may hold on to the memory for a long time and not release back to the OS, causing a memory bloat for the process in the best case, and an OOM in the worse case.\r\n\r\nIt is expected that these tensor copies do not happen as there is no functional need.\r\n\r\nIt is possible that the root cause of this issue is the same as #61344 , in which case feel free to close this issue and track the underlying bug over there.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nprint(tf.version.VERSION)\r\n\r\n# Depending on the underlying RAM resources,\r\n# increase this value to see OOM. With 10,\r\n# this code should OOM with total RAM resources of 20GB or less.\r\nNUM_MAP_CALLS=10\r\n\r\n# Allocate a large tensor. This will take 2GB of RAM.\r\nt = tf.random.uniform((2048, 1024*256))\r\n    \r\ndef get_data(_idx):\r\n  return t[0, 0]\r\n    \r\nds = tf.data.Dataset.range(1)\r\nfor _ in range(NUM_MAP_CALLS):\r\n  ds = ds.map(get_data)\r\n\r\nnext(iter(ds))\n```\n\n\n### Relevant log output\n\n```shell\nJan 13, 2024, 12:04:55\u202fPM\tWARNING\tWARNING:root:kernel 0585280c-54b4-4ab5-8a4d-e5502242c92a restarted\r\nJan 13, 2024, 12:04:55\u202fPM\tINFO\tKernelRestarter: restarting kernel (1\/5), keep random ports\r\nJan 13, 2024, 12:04:49\u202fPM\tWARNING\t2024-01-13 17:04:49.124663: W external\/local_tsl\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 2147483648 exceeds 10% of free system memory.\r\nJan 13, 2024, 12:04:44\u202fPM\tWARNING\t2024-01-13 17:04:44.817746: W external\/local_tsl\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 2147483648 exceeds 10% of free system memory.\r\nJan 13, 2024, 12:04:39\u202fPM\tWARNING\t2024-01-13 17:04:39.468439: W external\/local_tsl\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 2147483648 exceeds 10% of free system memory.\r\nJan 13, 2024, 12:04:32\u202fPM\tWARNING\t2024-01-13 17:04:32.994518: W external\/local_tsl\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 2147483648 exceeds 10% of free system memory.\r\nJan 13, 2024, 12:04:28\u202fPM\tWARNING\t2024-01-13 17:04:28.102305: W external\/local_tsl\/tsl\/framework\/cpu_allocator_impl.cc:83] Allocation of 2147483648 exceeds 10% of free system memory.\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:data","type:performance","TF 2.15"],"created_at":"2024-01-13T17:07:04Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62788"},{"issue_number":257,"repository":"tensorflow\/tensorflow","title":"Potential memory leak with SymbolicTensor","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.15\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nManjaro Linux\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11.5\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n12.3.52\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nHello,\r\n\r\nI am working on a TinyML NAS framework, i.e. throughout the execution of my code, hundreds if not thousands of models are created and trained. I have come across a problem that has been starving my system of memory after a couple of days of execution. By using `tracemalloc` I have been able to see that the main contributor appears to be the symbolic tensor created when creating a Conv2D layer. Maybe I am missing something basic in terms of garbage collection in my code but over time the demo code will eventually consume all system memory.\r\n\r\nI have also tried `tf.keras.backend.clear_session()` and `gc.collect()` but neither help. \r\n\r\nAny help would be appreciated.\r\n\r\nCheers\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport gc\r\nimport tracemalloc, sys, linecache, os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\nEPOCHS = 5\r\nBS = 512\r\nTEST_LOOPS = 10000\r\n\r\n\r\ndef start_tracemalloc():\r\n    tracemalloc.start()\r\n\r\n\r\ndef display_top(snapshot, key_type=\"lineno\", limit=5):\r\n    snapshot = snapshot.filter_traces(\r\n        (\r\n            tracemalloc.Filter(False, \"<frozen importlib._bootstrap>\"),\r\n            tracemalloc.Filter(False, \"<unknown>\"),\r\n        )\r\n    )\r\n    top_stats = snapshot.statistics(key_type)\r\n\r\n    print(\"Top %s lines\" % limit)\r\n    for index, stat in enumerate(top_stats[:limit], 1):\r\n        frame = stat.traceback[0]\r\n        # replace \"\/path\/to\/module\/file.py\" with \"module\/file.py\"\r\n        filename = os.sep.join(frame.filename.split(os.sep)[-2:])\r\n        print(\r\n            \"#%s: %s:%s: %.1f KiB\" % (index, filename, frame.lineno, stat.size \/ 1024)\r\n        )\r\n        line = linecache.getline(frame.filename, frame.lineno).strip()\r\n        if line:\r\n            print(\"    %s\" % line)\r\n\r\n    other = top_stats[limit:]\r\n    if other:\r\n        size = sum(stat.size for stat in other)\r\n        print(\"%s other: %.1f KiB\" % (len(other), size \/ 1024))\r\n    total = sum(stat.size for stat in top_stats)\r\n    print(\"Total allocated size: %.1f KiB\" % (total \/ 1024))\r\n\r\n\r\ndef display_snapshot():\r\n    snapshot = tracemalloc.take_snapshot()\r\n    display_top(snapshot)\r\n\r\n\r\ndef create_model() -> keras.models.Model:\r\n    inputs = keras.layers.Input(shape=(28, 28, 1))\r\n    x = keras.layers.Conv2D(32, kernel_size=(3, 3), padding=\"valid\")(inputs)\r\n    x = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None)(x)\r\n    x = keras.layers.Flatten()(x)\r\n    x = keras.layers.Dense(128, activation=tf.nn.relu)(x)\r\n    x = keras.layers.Dropout(0.2)(x)\r\n    outputs = keras.layers.Dense(10, activation=tf.nn.softmax)(x)\r\n\r\n    model = keras.models.Model(inputs=inputs, outputs=outputs)\r\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\r\n\r\n    return model\r\n\r\n\r\ndef train_model(model, train_images, train_labels):\r\n    model.fit(train_images, train_labels, epochs=EPOCHS, batch_size=BS)\r\n\r\n\r\ndef main() -> int:\r\n    mnist = keras.datasets.mnist\r\n    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\r\n\r\n    train_images = train_images.reshape(train_images.shape[0], train_images.shape[1], train_images.shape[2], 1)\r\n    train_images = train_images.astype(np.float32) \/ 255.0\r\n\r\n    start_tracemalloc()\r\n\r\n    for i in range(TEST_LOOPS):\r\n        tf.keras.backend.clear_session()\r\n        gc.collect()\r\n\r\n        model = create_model()\r\n        train_model(model, train_images, train_labels)\r\n        display_snapshot()\r\n        del model\r\n\r\n\r\nif __name__ == '__main__':\r\n    sys.exit(main())\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nEpoch 1\/5\r\n...\r\nTop 5 lines\r\n#1: <frozen abc>:123: 1856.2 KiB\r\n#2: python3.11\/linecache.py:137: 606.4 KiB\r\n    lines = fp.readlines()\r\n#3: framework\/ops.py:245: 197.9 KiB\r\n    return pywrap_tf_session.PyTensor.__new__(\r\n#4: <frozen importlib._bootstrap_external>:729: 158.6 KiB\r\n#5: framework\/ops.py:1211: 137.3 KiB\r\n    self._gradient_function = None\r\n952 other: 1407.6 KiB\r\nTotal allocated size: 4364.1 KiB\r\nEpoch 1\/5\r\n...\r\nTop 5 lines\r\n#1: <frozen abc>:123: 1846.9 KiB\r\n#2: python3.11\/linecache.py:137: 620.8 KiB\r\n    lines = fp.readlines()\r\n#3: framework\/ops.py:245: 382.3 KiB\r\n    return pywrap_tf_session.PyTensor.__new__(\r\n#4: framework\/ops.py:1211: 264.9 KiB\r\n    self._gradient_function = None\r\n#5: framework\/ops.py:1161: 254.7 KiB\r\n    self = Operation(c_op, SymbolicTensor)\r\n947 other: 2160.5 KiB\r\nTotal allocated size: 5530.0 KiB\r\nEpoch 1\/5 \r\n...\r\nTop 5 lines\r\n#1: <frozen abc>:123: 1843.9 KiB\r\n#2: python3.11\/linecache.py:137: 620.8 KiB\r\n    lines = fp.readlines()\r\n#3: framework\/ops.py:245: 568.5 KiB\r\n    return pywrap_tf_session.PyTensor.__new__(\r\n#4: framework\/ops.py:1211: 394.1 KiB\r\n    self._gradient_function = None\r\n#5: framework\/ops.py:1161: 378.9 KiB\r\n    self = Operation(c_op, SymbolicTensor)\r\n949 other: 2702.3 KiB\r\nTotal allocated size: 6508.5 KiB\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","TF 2.15"],"created_at":"2024-01-11T12:16:19Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62783"},{"issue_number":258,"repository":"tensorflow\/tensorflow","title":"Tensorflow numpy_function causes errors when using tf.shape","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.6 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.18\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nRaises an error. \r\n\r\nExpect: No errors.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf  # v2.13.0-17-gf841394b1b7 2.13.1\r\nimport tensorflow_datasets as tfds  # 4.9.3\r\nfrom functools import partial\r\n\r\nAUTO = tf.data.experimental.AUTOTUNE\r\n\r\ndataset_name = \"mnist\"\r\ndataset = tfds.load(\r\n    dataset_name,\r\n    split=\"train[:1%]\"\r\n)\r\n\r\ndataset_len = dataset.reduce(0, lambda x, _: x + 1).numpy()\r\nprint(\"dataset_len\", dataset_len)\r\n\r\nimage_size = [28, 28, 1]\r\nnew_image_size = [14, 2, 28, 1]\r\n\r\n\r\ndef prepare_input(\r\n    image, labels,\r\n    image_size\r\n):\r\n\r\n    image = tf.image.resize(image, image_size)\r\n\r\n    split_shape = tf.shape(image)[1] \/\/ 2\r\n    image_reshape = tf.reshape(\r\n        image, [split_shape, 2, image_size[1], 1]\r\n    )\r\n\r\n    return image_reshape, labels\r\n\r\n\r\npartial_prepare_input = partial(\r\n    prepare_input,\r\n    image_size=tf.constant(image_size[:2], dtype=tf.int32)\r\n)\r\n\r\n\r\ndef prepare_input_wrapper(sample):\r\n    image = sample[\"image\"]\r\n    labels = sample[\"label\"]\r\n\r\n    labels_shape = labels.shape\r\n\r\n    image, label = tf.numpy_function(\r\n        func=partial_prepare_input,\r\n        inp=[image, labels],\r\n        Tout=[tf.float32, tf.int64],\r\n        name=\"numpy_function_1\"\r\n    )\r\n\r\n    image.set_shape(new_image_size)\r\n    label.set_shape(labels_shape)\r\n\r\n    return {\r\n        \"image\": image,\r\n        \"label\": label\r\n    }\r\n\r\n\r\ndataset = dataset.map(\r\n    lambda sample: prepare_input_wrapper(sample),\r\n    num_parallel_calls=AUTO\r\n)\r\n\r\n\r\nfor x in dataset:\r\n    print(x['image'].shape, x['label'].shape)\n```\n\n\n### Relevant log output\n\n```shell\n2024-01-11 02:21:59.538118: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-01-11 02:21:59.575984: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-01-11 02:22:00.346338: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-01-11 02:22:02.022172: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-01-11 02:22:02.058030: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-01-11 02:22:02.061266: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-01-11 02:22:02.064864: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-01-11 02:22:02.067959: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-01-11 02:22:02.070999: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-01-11 02:22:02.820643: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-01-11 02:22:02.821876: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-01-11 02:22:02.822898: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https:\/\/github.com\/torvalds\/linux\/blob\/v6.0\/Documentation\/ABI\/testing\/sysfs-bus-pci#L344-L355\r\n2024-01-11 02:22:02.823877: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1639] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 13623 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\r\ndataset_len 600\r\n2024-01-11 02:22:03.323582: W tensorflow\/core\/framework\/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: slice index 1 of dimension 0 out of bounds.\r\n2024-01-11 02:22:03.323637: W tensorflow\/core\/framework\/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead.\r\n2024-01-11 02:22:03.324399: W tensorflow\/core\/framework\/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: slice index 1 of dimension 0 out of bounds.\r\n2024-01-11 02:22:03.324531: W tensorflow\/core\/framework\/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead.\r\n2024-01-11 02:22:03.329642: W tensorflow\/core\/framework\/op_kernel.cc:1816] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice\/\r\nTraceback (most recent call last):\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/script_ops.py\", line 268, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 643, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"\/home\/ubuntu\/automltraining\/replicate_error.py\", line 27, in prepare_input\r\n    split_shape = tf.shape(image)[1] \/\/ 2\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 6656, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice\/\r\n\r\n\r\n2024-01-11 02:22:03.330620: W tensorflow\/core\/framework\/op_kernel.cc:1816] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice\/\r\nTraceback (most recent call last):\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/script_ops.py\", line 268, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 643, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"\/home\/ubuntu\/automltraining\/replicate_error.py\", line 27, in prepare_input\r\n    split_shape = tf.shape(image)[1] \/\/ 2\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 6656, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice\/\r\n\r\n\r\n2024-01-11 02:22:03.333474: W tensorflow\/core\/framework\/op_kernel.cc:1816] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice\/\r\nTraceback (most recent call last):\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/script_ops.py\", line 268, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 643, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"\/home\/ubuntu\/automltraining\/replicate_error.py\", line 27, in prepare_input\r\n    split_shape = tf.shape(image)[1] \/\/ 2\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 6656, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice\/\r\n\r\n\r\n2024-01-11 02:22:03.337583: W tensorflow\/core\/framework\/op_kernel.cc:1816] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice\/\r\nTraceback (most recent call last):\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/script_ops.py\", line 268, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 643, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"\/home\/ubuntu\/automltraining\/replicate_error.py\", line 27, in prepare_input\r\n    split_shape = tf.shape(image)[1] \/\/ 2\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 6656, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice\/\r\n\r\n\r\n2024-01-11 02:22:03.337628: W tensorflow\/core\/framework\/op_kernel.cc:1816] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice\/\r\nTraceback (most recent call last):\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/script_ops.py\", line 268, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 643, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"\/home\/ubuntu\/automltraining\/replicate_error.py\", line 27, in prepare_input\r\n    split_shape = tf.shape(image)[1] \/\/ 2\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 6656, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice\/\r\n\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/ubuntu\/automltraining\/replicate_error.py\", line 69, in <module>\r\n    for x in dataset:\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py\", line 814, in __next__\r\n    return self._next_internal()\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py\", line 777, in _next_internal\r\n    ret = gen_dataset_ops.iterator_get_next(\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/gen_dataset_ops.py\", line 3028, in iterator_get_next\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 6656, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.UnknownError: {{function_node __wrapped__IteratorGetNext_output_types_2_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice\/\r\nTraceback (most recent call last):\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/script_ops.py\", line 268, in __call__\r\n    ret = func(*args)\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 643, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"\/home\/ubuntu\/automltraining\/replicate_error.py\", line 27, in prepare_input\r\n    split_shape = tf.shape(image)[1] \/\/ 2\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n\r\n  File \"\/home\/ubuntu\/miniconda3\/envs\/automl\/lib\/python3.9\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 6656, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice\/\r\n\r\n\r\n         [[{{node numpy_function_1}}]] [Op:IteratorGetNext] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2024-01-11T02:23:23Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62778"},{"issue_number":259,"repository":"tensorflow\/tensorflow","title":"NCCL + XLA fails for multi-GPU training.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.15\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n12.2\/8.9.4\r\n\r\n### GPU model and memory\r\n\r\nA100 40GB (20GB MiG)\r\n\r\n### Current behavior?\r\n\r\nI am trying to run multi-GPU training with an XLA compiled model (simple CNN with a classification head). Without XLA, everything runs fine. With XLA enabled, I get one of two errors in the log, depending on whether I am using 4 GPUs or 2 GPUs. The GPUs are split into 2 MiGs.\r\n\r\nI also tried on previous TF\/CUDA versions and I get the same result.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport argparse\r\nimport json\r\nimport os\r\nfrom typing import Any\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef get_replica_hostnames():\r\n    ...\r\n\r\n\r\ndef get_replica_id():\r\n    ...\r\n\r\n\r\ndef set_multiworker_env_config():\r\n    hostnames = get_replica_hostnames()\r\n    replica_index = get_replica_id()\r\n\r\n    os.environ[\"TF_CONFIG\"] = json.dumps(\r\n        {\r\n            \"cluster\": {\r\n                \"worker\": hostnames,\r\n            },\r\n            \"task\": {\"type\": \"worker\", \"index\": replica_index},\r\n        }\r\n    )\r\n\r\n\r\nclass Model(tf.keras.models.Model):\r\n    def __init__(self, *args: Any, **kwargs: Any) -> None:\r\n        super().__init__(*args, **kwargs)\r\n\r\n        self._embedder = tf.keras.Sequential(\r\n            [\r\n                tf.keras.layers.Conv2D(\r\n                    filters=8,\r\n                    kernel_size=3,\r\n                    padding=\"same\",\r\n                    activation=tf.keras.activations.relu,\r\n                    use_bias=False,\r\n                ),\r\n                tf.keras.layers.BatchNormalization(),\r\n                tf.keras.layers.Conv2D(\r\n                    filters=8,\r\n                    kernel_size=3,\r\n                    padding=\"same\",\r\n                    activation=tf.keras.activations.relu,\r\n                    use_bias=False,\r\n                ),\r\n                tf.keras.layers.BatchNormalization(),\r\n                tf.keras.layers.MaxPool2D(),\r\n                tf.keras.layers.GlobalAveragePooling2D(),\r\n            ]\r\n        )\r\n\r\n        self._classifier = tf.keras.layers.Dense(550)\r\n\r\n    def call(self, x: tf.Tensor) -> tf.Tensor:\r\n        x = self._embedder(x)\r\n        x = self._classifier(x)\r\n        x = tf.keras.layers.Activation(\"linear\", dtype=\"float32\")(x)\r\n        return x\r\n\r\n\r\ndef create_dummy_dataset(batch_size: int) -> tf.data.Dataset:\r\n    X = np.random.rand(batch_size, 384, 640, 1)\r\n    y = np.random.randint(550, size=batch_size)\r\n    return tf.data.Dataset.from_tensor_slices((X, y)).batch(batch_size, True).repeat()\r\n\r\n\r\ndef train():\r\n    set_multiworker_env_config()\r\n    strategy = tf.distribute.MultiWorkerMirroredStrategy()\r\n    num_replicas = strategy.num_replicas_in_sync\r\n\r\n    batch_size = 16 * num_replicas\r\n    dataset = create_dummy_dataset(batch_size)\r\n    dataset = strategy.experimental_distribute_dataset(dataset)\r\n\r\n    with strategy.scope():\r\n        model = Model()\r\n        model.compile(\r\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(\r\n                from_logits=True,\r\n            ),\r\n            optimizer=tf.keras.optimizers.Adam(\r\n                learning_rate=1e-3,\r\n                weight_decay=1e-5,\r\n            ),\r\n            metrics=[\r\n                \"accuracy\",\r\n            ],\r\n            jit_compile=True,\r\n        )\r\n\r\n    model.fit(\r\n        dataset,\r\n        epochs=10,\r\n        steps_per_epoch=100,\r\n    )\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    train()\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nLog output for 4 GPUs:\r\n\r\n2024-01-08 12:48:24.103746: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-01-08 12:48:24.103807: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-01-08 12:48:24.104896: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-01-08 12:48:24.111875: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-01-08 12:48:24.978525: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-01-08 12:48:27.492019: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1929] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 18370 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:01:00.0, compute capability: 8.0\r\n2024-01-08 12:48:27.503592: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1929] Created device \/job:worker\/replica:0\/task:0\/device:GPU:0 with 18370 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:01:00.0, compute capability: 8.0\r\n2024-01-08 12:48:27.528772: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:457] Started server with target: grpc:\/\/gen-svc-4e03d749-0565-47d7-b7ff-63437f0ab5b3:80\r\n2024-01-08 12:48:27.535501: I external\/local_tsl\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:553] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 9493928235207696637\r\n2024-01-08 12:48:27.535846: I external\/local_tsl\/tsl\/distributed_runtime\/coordination\/coordination_service_agent.cc:304] Coordination agent has successfully connected.\r\n2024-01-08 12:48:28.425710: I external\/local_tsl\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:553] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 17537869834892823189\r\n2024-01-08 12:48:29.509519: I external\/local_tsl\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:553] \/job:worker\/replica:0\/task:2 has connected to coordination service. Incarnation: 11924625857490357420\r\n2024-01-08 12:48:29.766944: I external\/local_tsl\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:553] \/job:worker\/replica:0\/task:3 has connected to coordination service. Incarnation: 8010175117178506894\r\nWARNING:absl:You use TensorFlow DType <dtype: 'string'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to object.\r\nWARNING:absl:You use TensorFlow DType <dtype: 'int64'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int64.\r\nml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:563 [0] NCCL INFO Bootstrap : Using eth0:10.233.118.112<0>\r\nml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:563 [0] NCCL INFO NET\/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\n2024-01-08 12:48:32.756935: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:104] Profiler session initializing.\r\n2024-01-08 12:48:32.756993: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:119] Profiler session started.\r\n2024-01-08 12:48:32.757173: I external\/local_xla\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:1883] Profiler found 1 GPUs\r\n2024-01-08 12:48:32.790801: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:131] Profiler session tear down.\r\n2024-01-08 12:48:32.790934: I external\/local_xla\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:2017] CUPTI activity buffer flushed\r\nEpoch 1\/10\r\n2024-01-08 12:48:37.520453: I external\/local_xla\/xla\/service\/service.cc:168] XLA service 0x7fe284006a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2024-01-08 12:48:37.520583: I external\/local_xla\/xla\/service\/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB MIG 3g.20gb, Compute Capability 8.0\r\n2024-01-08 12:48:37.680983: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2024-01-08 12:48:38.693930: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:454] Loaded cuDNN version 8904\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1704718137.764316     430 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\nml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO cudaDriverVersion 12020\r\nNCCL version 2.16.5+cudaCUDA_MAJOR.CUDA_MINOR\r\nml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO Failed to open libibverbs.so[.1]\r\nml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO NET\/Socket : Using [0]eth0:10.233.118.112<0>\r\nml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO Using network Socket\r\n\r\nml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] external\/nccl_archive\/src\/init.cc:642 NCCL WARN Duplicate GPU detected : rank 0 and rank 2 both on CUDA device 1000\r\nml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO external\/nccl_archive\/src\/init.cc:1100 -> 5\r\nml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO external\/nccl_archive\/src\/init.cc:1173 -> 5\r\nml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO external\/nccl_archive\/src\/init.cc:1209 -> 5\r\n2024-01-08 12:48:58.577448: W external\/local_xla\/xla\/service\/gpu\/runtime\/support.cc:58] Intercepted XLA runtime error:\r\nINTERNAL: external\/local_xla\/xla\/service\/gpu\/nccl_utils.cc:297: NCCL operation ncclCommInitRank(&comm, nranks, id, rank) failed: invalid usage\r\n2024-01-08 12:48:58.577775: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16898057275935290807\r\n2024-01-08 12:48:58.577798: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7386102362502530449\r\n2024-01-08 12:48:58.577844: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11220456033729140565\r\nTraceback (most recent call last):\r\n  File \"\/kirax_source\/train.py\", line 346, in <module>\r\n    train_tf(args=args, jit_compile=XLA)\r\n  File \"\/kirax_source\/train.py\", line 246, in train_tf\r\n    model.fit(\r\n  File \"\/usr\/local\/lib\/python3.11\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 70, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/usr\/local\/lib\/python3.11\/dist-packages\/tensorflow\/python\/eager\/execute.py\", line 53, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntensorflow.python.framework.errors_impl.InternalError: Graph execution error:\r\n\r\nDetected at node StatefulPartitionedCall defined at (most recent call last):\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1002, in _bootstrap\r\n\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1045, in _bootstrap_inner\r\n\r\nDetected at node StatefulPartitionedCall defined at (most recent call last):\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1002, in _bootstrap\r\n\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1045, in _bootstrap_inner\r\n\r\n2 root error(s) found.\r\n  (0) INTERNAL:  Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.all_reduce' failed: external\/local_xla\/xla\/service\/gpu\/nccl_utils.cc:297: NCCL operation ncclCommInitRank(&comm, nranks, id, rank) failed: invalid usage; current tracing scope: all-reduce-start.4; current profiling annotation: XlaModule:#hlo_module=a_inference_run_step_7562__.4006,program_id=447#.\r\n         [[{{node StatefulPartitionedCall}}]]\r\n         [[Reshape_3\/_22]]\r\n  (1) INTERNAL:  Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.all_reduce' failed: external\/local_xla\/xla\/service\/gpu\/nccl_utils.cc:297: NCCL operation ncclCommInitRank(&comm, nranks, id, rank) failed: invalid usage; current tracing scope: all-reduce-start.4; current profiling annotation: XlaModule:#hlo_module=a_inference_run_step_7562__.4006,program_id=447#.\r\n         [[{{node StatefulPartitionedCall}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_7900]\r\n\r\nLog output for 2 GPUs:\r\n\r\n2024-01-08 13:32:56.555585: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-01-08 13:32:56.555666: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-01-08 13:32:56.557055: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-01-08 13:32:56.563733: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-01-08 13:32:57.475551: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-01-08 13:32:59.868467: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1929] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 18370 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:41:00.0, compute capability: 8.0\r\n2024-01-08 13:32:59.880625: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1929] Created device \/job:worker\/replica:0\/task:0\/device:GPU:0 with 18370 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:41:00.0, compute capability: 8.0\r\n2024-01-08 13:32:59.903354: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:457] Started server with target: grpc:\/\/gen-svc-d871b43e-6f9e-4d8f-9faf-d98a734319f3:80\r\n2024-01-08 13:32:59.912608: I external\/local_tsl\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:553] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 12924650074147221766\r\n2024-01-08 13:32:59.913045: I external\/local_tsl\/tsl\/distributed_runtime\/coordination\/coordination_service_agent.cc:304] Coordination agent has successfully connected.\r\n2024-01-08 13:33:00.820050: I external\/local_tsl\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:553] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 4023732460757352804\r\nWARNING:absl:You use TensorFlow DType <dtype: 'string'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to object.\r\nWARNING:absl:You use TensorFlow DType <dtype: 'int64'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int64.\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:784 [0] NCCL INFO Bootstrap : Using eth0:10.233.118.63<0>\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:784 [0] NCCL INFO NET\/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\n2024-01-08 13:33:03.552619: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:104] Profiler session initializing.\r\n2024-01-08 13:33:03.552667: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:119] Profiler session started.\r\n2024-01-08 13:33:03.552814: I external\/local_xla\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:1883] Profiler found 1 GPUs\r\n2024-01-08 13:33:03.587433: I external\/local_tsl\/tsl\/profiler\/lib\/profiler_session.cc:131] Profiler session tear down.\r\n2024-01-08 13:33:03.587611: I external\/local_xla\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:2017] CUPTI activity buffer flushed\r\nEpoch 1\/10\r\n2024-01-08 13:33:08.149597: I external\/local_xla\/xla\/service\/service.cc:168] XLA service 0x7ff338008540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2024-01-08 13:33:08.149730: I external\/local_xla\/xla\/service\/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB MIG 3g.20gb, Compute Capability 8.0\r\n2024-01-08 13:33:08.732338: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2024-01-08 13:33:09.871101: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:454] Loaded cuDNN version 8904\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nI0000 00:00:1704720809.106117     425 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO cudaDriverVersion 12020\r\nNCCL version 2.16.5+cudaCUDA_MAJOR.CUDA_MINOR\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Failed to open libibverbs.so[.1]\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO NET\/Socket : Using [0]eth0:10.233.118.63<0>\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Using network Socket\r\n\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] external\/nccl_archive\/src\/misc\/nvmlwrap.cc:183 NCCL WARN nvmlDeviceGetHandleByPciBusId() failed: Not Found\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 00\/02 :    0   1\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 01\/02 :    0   1\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Trees [0] 1\/-1\/-1->0->-1 [1] -1\/-1\/-1->0->1\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO P2P Chunksize set to 131072\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1284 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1284 [0] NCCL INFO NET\/Socket: Using 8 threads and 1 sockets per thread\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 00\/0 : 1[1000] -> 0[41000] [receive] via NET\/Socket\/0\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1284 [0] NCCL INFO NET\/Socket: Using 8 threads and 1 sockets per thread\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 01\/0 : 1[1000] -> 0[41000] [receive] via NET\/Socket\/0\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 00\/0 : 0[41000] -> 1[1000] [send] via NET\/Socket\/0\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 01\/0 : 0[41000] -> 1[1000] [send] via NET\/Socket\/0\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Connected all rings\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Connected all trees\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO threadThresholds 8\/8\/64 | 16\/8\/64 | 512 | 512\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer\r\nml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO comm 0x7fe61cffcb20 rank 0 nranks 2 cudaDev 0 busId 41000 commId 0xe105507e5746b5a2 - Init COMPLETE\r\n2024-01-08 13:33:29.780117: W external\/local_xla\/xla\/service\/gpu\/runtime\/support.cc:58] Intercepted XLA runtime error:\r\nINTERNAL: There was an error before calling cuModuleGetFunction (101): cudaErrorInvalidDevice : invalid device ordinal\r\n2024-01-08 13:33:29.780331: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14229809023376429067\r\n2024-01-08 13:33:29.780349: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10883816187180422187\r\n2024-01-08 13:33:29.780389: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5243788738829094043\r\nTraceback (most recent call last):\r\n  File \"\/kirax_source\/train.py\", line 346, in <module>\r\n    train_tf(args=args, jit_compile=XLA)\r\n  File \"\/kirax_source\/train.py\", line 246, in train_tf\r\n    model.fit(\r\n  File \"\/usr\/local\/lib\/python3.11\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 70, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/usr\/local\/lib\/python3.11\/dist-packages\/tensorflow\/python\/eager\/execute.py\", line 53, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntensorflow.python.framework.errors_impl.InternalError: Graph execution error:\r\n\r\nDetected at node StatefulPartitionedCall defined at (most recent call last):\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1002, in _bootstrap\r\n\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1045, in _bootstrap_inner\r\n\r\nDetected at node StatefulPartitionedCall defined at (most recent call last):\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1002, in _bootstrap\r\n\r\n  File \"\/usr\/lib\/python3.11\/threading.py\", line 1045, in _bootstrap_inner\r\n\r\n2 root error(s) found.\r\n  (0) INTERNAL:  Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.func.launch' failed: There was an error before calling cuModuleGetFunction (101): cudaErrorInvalidDevice : invalid device ordinal; current tracing scope: fusion.274; current profiling annotation: XlaModule:#hlo_module=a_inference_run_step_7562__.4006,program_id=447#.\r\n         [[{{node StatefulPartitionedCall}}]]\r\n         [[CollectiveReduceV2_1\/_17]]\r\n  (1) INTERNAL:  Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.func.launch' failed: There was an error before calling cuModuleGetFunction (101): cudaErrorInvalidDevice : invalid device ordinal; current tracing scope: fusion.274; current profiling annotation: XlaModule:#hlo_module=a_inference_run_step_7562__.4006,program_id=447#.\r\n         [[{{node StatefulPartitionedCall}}]]\r\n0 successful operations.\r\n0 derived errors ignored. [Op:__inference_train_function_7900]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.15"],"created_at":"2024-01-08T13:54:59Z","comments":8,"reactions":2,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62757"},{"issue_number":260,"repository":"tensorflow\/tensorflow","title":"Can no longer run XLA lit tests","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nTF 2.15\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### Current behavior?\r\n\r\nPreviously, I could run the XLA unit tests via `bazel test \/\/tensorflow\/compiler\/xla\/...:all`.\r\nHowever, in TF 2.15 after xla was moved to third_party\/xla I am encountering issues.\r\nI updated my command to `bazel test @local_xla\/\/xla\/...:all`. While most tests run successfully, it seems there are some hardcoded paths which are preventing the llvm lit tests from running correctly. See the logs below. Probably the lit configs need to be updated?\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nCheckout tensorflow. Configure. Run `bazel test @local_xla\/\/xla\/...:all`\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n================================================================================\r\nFAIL: @local_xla\/\/xla\/mlir\/backends\/gpu\/transforms\/tests:gpu_memcpy.mlir.test (see \/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/testlogs\/external\/local_xla\/xla\/mlir\/backends\/gpu\/transforms\/tests\/gpu_memcpy.mlir.test\/test.log)\r\n[27,548 \/ 27,604] 348 \/ 447 tests, 216 failed; [Sched] Testing @local_xla\/\/xla\/mlir_hlo\/tests:Dialect\/mhlo\/hlo-collapse-elementwise-map.mlir.test; 45s ... (55 actions, 2 running)\r\nINFO: From Testing @local_xla\/\/xla\/mlir\/backends\/gpu\/transforms\/tests:gpu_memcpy.mlir.test:\r\n==================== Test output for @local_xla\/\/xla\/mlir\/backends\/gpu\/transforms\/tests:gpu_memcpy.mlir.test:\r\nRunning test \/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/external\/local_xla\/xla\/mlir\/backends\/gpu\/transforms\/tests\/gpu_memcpy.mlir.test.runfiles\/org_tensorflow\/..\/local_xla\/xla\/mlir\/backends\/gpu\/transforms\/tests\/gpu_memcpy.mlir.test xla\/gpu_memcpy.mlir --config-prefix=runlit -v on GPU 0\r\nlit.py: \/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/external\/llvm-raw\/llvm\/utils\/lit\/lit\/discovery.py:137: warning: unable to find test suite for 'xla\/gpu_memcpy.mlir'\r\nlit.py: \/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/external\/llvm-raw\/llvm\/utils\/lit\/lit\/discovery.py:276: warning: input 'xla\/gpu_memcpy.mlir' contained no tests\r\nerror: did not discover any tests for provided path(s)\r\n================================================================================\r\nFAIL: @local_xla\/\/xla\/mlir_hlo\/tests:Dialect\/mhlo\/hlo-collapse-elementwise-map.mlir.test (see \/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/testlogs\/external\/local_xla\/xla\/mlir_hlo\/tests\/Dialect\/mhlo\/hlo-collapse-elementwise-map.mlir.test\/test.log)\r\nINFO: From Testing @local_xla\/\/xla\/mlir_hlo\/tests:Dialect\/mhlo\/hlo-collapse-elementwise-map.mlir.test:\r\n==================== Test output for @local_xla\/\/xla\/mlir_hlo\/tests:Dialect\/mhlo\/hlo-collapse-elementwise-map.mlir.test:\r\nRunning test \/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/external\/local_xla\/xla\/mlir_hlo\/tests\/Dialect\/mhlo\/hlo-collapse-elementwise-map.mlir.test.runfiles\/org_tensorflow\/..\/local_xla\/xla\/mlir_hlo\/tests\/Dialect\/mhlo\/hlo-collapse-elementwise-map.mlir.test -v external\/local_xla\/xla\/mlir_hlo\/tests\/Dialect\/mhlo\/hlo-collapse-elementwise-map.mlir on GPU 0\r\nlit.py: \/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/external\/llvm-raw\/llvm\/utils\/lit\/lit\/TestingConfig.py:151: fatal: unable to parse config file '\/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/external\/local_xla\/xla\/mlir_hlo\/tests\/Dialect\/mhlo\/hlo-collapse-elementwise-map.mlir.test.runfiles\/org_tensorflow\/external\/local_xla\/xla\/mlir_hlo\/tests\/lit.site.cfg.py', traceback: Traceback (most recent call last):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/external\/llvm-raw\/llvm\/utils\/lit\/lit\/TestingConfig.py\", line 139, in load_from_path\r\n    exec(compile(data, path, \"exec\"), cfg_globals, None)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/external\/local_xla\/xla\/mlir_hlo\/tests\/Dialect\/mhlo\/hlo-collapse-elementwise-map.mlir.test.runfiles\/org_tensorflow\/external\/local_xla\/xla\/mlir_hlo\/tests\/lit.site.cfg.py\", line 44, in <module>\r\n    lit_config.load_config(config, \"xla\/mlir_hlo\/tests\/lit.cfg.py\")\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/external\/llvm-raw\/llvm\/utils\/lit\/lit\/LitConfig.py\", line 152, in load_config\r\n    config.load_from_path(path, self)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/a8fc6d0749b4f3c43761726a36e8ec4c\/external\/llvm-raw\/llvm\/utils\/lit\/lit\/TestingConfig.py\", line 126, in load_from_path\r\n    f = open(path)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'xla\/mlir_hlo\/tests\/lit.cfg.py'\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install"],"created_at":"2024-01-03T22:23:21Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62732"},{"issue_number":261,"repository":"tensorflow\/tensorflow","title":"Textfile initializer sharing bug","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.15\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe logic here for TextFileInitializer [sharing](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/059b23d0dfd34e0d6cdf1f4a65dbc8ed1dfdf54a\/tensorflow\/python\/ops\/lookup_ops.py#L790) does not consider key\/value dtype. If two text file initializers are made for same vocab file but with different dtypes (tf.int64\/tf.string), then sharing will cause one of them to fail and crash.\r\n\r\nThe fix should be small change to add dtypes to shared_name.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os\r\nimport tempfile\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.lookup import TextFileIndex\r\n\r\nwith tempfile.TemporaryDirectory(\"w\") as tmp_dir:\r\n    vocab_file = os.path.join(tmp_dir, \"vocab.txt\")\r\n    with open(vocab_file, \"w\") as f:\r\n        f.write(\"\\n\".join([\"1\", \"2\", \"3\"]))\r\n\r\n    initializer1 = tf.lookup.TextFileInitializer(\r\n        vocab_file, tf.string, TextFileIndex.WHOLE_LINE, tf.int64, TextFileIndex.LINE_NUMBER\r\n    )\r\n    table1 = tf.lookup.StaticHashTable(initializer1, -1)\r\n\r\n    initializer2 = tf.lookup.TextFileInitializer(\r\n        vocab_file, tf.int64, TextFileIndex.WHOLE_LINE, tf.int64, TextFileIndex.LINE_NUMBER\r\n    )\r\n    table2 = tf.lookup.StaticHashTable(initializer2, -1)\r\n\r\n    _ = table1.lookup(tf.constant([\"1\", \"2\", \"3\"], dtype=tf.string))\r\n    _ = table2.lookup(tf.constant([1, 2, 3], dtype=tf.int64))\n```\n\n\n### Relevant log output\n\n```shell\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-12-13 13:50:41.113925: W tensorflow\/core\/framework\/op_kernel.cc:1745] OP_REQUIRES failed at lookup_table_op.h:94 : INVALID_ARGUMENT: Conflicting key\/value dtypes int64->int64 with string-int64 for table hash_table_\/var\/folders\/3g\/0v44vhyd05q6n_77rl_jnh6r0000gn\/T\/tmpiimi4gc8w\/vocab.txt_-2_-1\r\nTraceback (most recent call last):\r\n  File \"\/Users\/pa-loaner\/.pyenv\/versions\/3.9.16\/lib\/python3.9\/pdb.py\", line 1726, in main\r\n    pdb._runscript(mainpyfile)\r\n  File \"\/Users\/pa-loaner\/.pyenv\/versions\/3.9.16\/lib\/python3.9\/pdb.py\", line 1586, in _runscript\r\n    self.run(statement)\r\n  File \"\/Users\/pa-loaner\/.pyenv\/versions\/3.9.16\/lib\/python3.9\/bdb.py\", line 580, in run\r\n    exec(cmd, globals, locals)\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/Users\/pa-loaner\/Snapchat\/Dev\/training-platform\/scratch\/text_file_bug.py\", line 1, in <module>\r\n    import os\r\n  File \"\/Users\/pa-loaner\/Snapchat\/Dev\/.venvs\/bento\/lib\/python3.9\/site-packages\/tensorflow\/python\/training\/tracking\/resource.py\", line 104, in __call__\r\n    return previous_getter(*args, **kwargs)\r\n  File \"\/Users\/pa-loaner\/Snapchat\/Dev\/.venvs\/bento\/lib\/python3.9\/site-packages\/tensorflow\/python\/training\/tracking\/resource.py\", line 99, in <lambda>\r\n    previous_getter = lambda *a, **kw: default_resource_creator(None, *a, **kw)\r\n  File \"\/Users\/pa-loaner\/Snapchat\/Dev\/.venvs\/bento\/lib\/python3.9\/site-packages\/tensorflow\/python\/training\/tracking\/resource.py\", line 96, in default_resource_creator\r\n    obj.__init__(*a, **kw)\r\n  File \"\/Users\/pa-loaner\/Snapchat\/Dev\/.venvs\/bento\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/lookup_ops.py\", line 347, in __init__\r\n    super(StaticHashTable, self).__init__(default_value, initializer)\r\n  File \"\/Users\/pa-loaner\/Snapchat\/Dev\/.venvs\/bento\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/lookup_ops.py\", line 198, in __init__\r\n    self._resource_handle = self._create_resource()\r\n  File \"\/Users\/pa-loaner\/Snapchat\/Dev\/.venvs\/bento\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/lookup_ops.py\", line 360, in _create_resource\r\n    table_ref = gen_lookup_ops.hash_table_v2(\r\n  File \"\/Users\/pa-loaner\/Snapchat\/Dev\/.venvs\/bento\/lib\/python3.9\/site-packages\/tensorflow\/python\/ops\/gen_lookup_ops.py\", line 466, in hash_table_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"\/Users\/pa-loaner\/Snapchat\/Dev\/.venvs\/bento\/lib\/python3.9\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 7164, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Conflicting key\/value dtypes int64->int64 with string-int64 for table hash_table_\/var\/folders\/3g\/0v44vhyd05q6n_77rl_jnh6r0000gn\/T\/tmpiimi4gc8w\/vocab.txt_-2_-1 [Op:HashTableV2] name: hash_table\r\nUncaught exception. Entering post mortem debugging\r\nRunning 'cont' or 'step' will restart the program\r\n> \/Users\/pa-loaner\/Snapchat\/Dev\/.venvs\/bento\/lib\/python3.9\/site-packages\/tensorflow\/python\/framework\/ops.py(7164)raise_from_not_ok_status()\r\n-> raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2023-12-13T19:51:16Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62631"},{"issue_number":262,"repository":"tensorflow\/tensorflow","title":"TFlite model signature lost after populating with metadata","description":"### 1. System information\r\n\r\n- Mac OS 14.1.2\r\n- TensorFlow mac 2.13.0\r\n- Tflite support version '0.1.0a1'\r\n\r\n### 2. Code\r\n\r\n```\r\ndef write_metadata(model_path, run_name):\r\n    model_meta = _metadata_fb.ModelMetadataT()\r\n    model_meta.name = \"test model\"\r\n    model_meta.description = run_name\r\n    model_meta.version = datetime.now().strftime(\"%Y.%m.%d\")\r\n    model_meta.author = \"Test\"\r\n    model_meta.license = f\"test\"\\\r\n        \"All rights reserved - test\"\r\n\r\n    input_meta_image = _metadata_fb.TensorMetadataT()\r\n    input_meta_image.description = \"Input image for which to score doneness.\"\r\n    input_meta_image.content = _metadata_fb.ContentT()\r\n    input_meta_image.content.contentProperties = _metadata_fb.ImagePropertiesT()\r\n    input_meta_image.content.contentProperties.colorSpace = _metadata_fb.ColorSpaceType.RGB\r\n    input_meta_image.content.contentPropertiesType = _metadata_fb.ContentProperties.ImageProperties\r\n\r\n    input_meta_state = _metadata_fb.TensorMetadataT()\r\n    input_meta_state.description = \"Input state.\"\r\n    input_meta_state.name = \"Input state.\"\r\n\r\n    # Creates output info.\r\n    output_meta_doneness = _metadata_fb.TensorMetadataT()\r\n    output_meta_doneness.description = \"Output doneness score between 0 and 1.\"\r\n    output_meta_doneness.name = \"Output Doneness\"\r\n\r\n    output_meta_state = _metadata_fb.TensorMetadataT()\r\n    output_meta_state.description = \"Output state.\"\r\n    output_meta_state.name = \"Output state.\"\r\n\r\n    subgraph = _metadata_fb.SubGraphMetadataT()\r\n\r\n    interpreter = tf.lite.Interpreter(model_path)\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n    state_first_input = input_details[0]['shape'][-1] != 3\r\n    state_first_output = output_details[0]['shape'][-1] != 1\r\n\r\n    input_meta = [input_meta_state, input_meta_image]\r\n    output_meta = [output_meta_state, output_meta_doneness]\r\n\r\n    subgraph.inputTensorMetadata = input_meta if state_first_input else input_meta[::-1]\r\n    subgraph.outputTensorMetadata = output_meta if state_first_output else output_meta[::-1]\r\n\r\n    model_meta.subgraphMetadata = [subgraph]\r\n    b = flatbuffers.Builder(0)\r\n    b.Finish(\r\n        model_meta.Pack(b),\r\n        _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\r\n    metadata_buf = b.Output()\r\n\r\n    populator = _metadata.MetadataPopulator.with_model_file(model_path)\r\n    populator.load_metadata_buffer(metadata_buf)\r\n    populator.populate()\r\n\r\n```\r\n\r\nI am using above function to populate the metadata. without running above function getting signature output like \r\n\r\n```\r\ninterpreter.get_signature_list()\r\n{'serving_default': {'inputs': ['input_image', 'input_state'], 'outputs': ['output_1', 'output_2']}}\r\n```\r\n\r\nafter running metadata function \r\n```\r\ninterpreter.get_signature_list()\r\n{}\r\n```\r\n\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:lite","TFLiteConverter","TF 2.13"],"created_at":"2023-12-11T21:30:02Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62620"},{"issue_number":263,"repository":"tensorflow\/tensorflow","title":"Process is aborted (core dumped) when axis is a large negative integer when calling tf.gather ","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI am actually using 2.10.0 but I find this issue insists in 2.16.0-dev20231209 (tf-nightly). Here is the code to reproduce:\r\n\r\n```\r\nimport tensorflow as tf\r\nparams = tf.constant([[0.69]])\r\nindices = tf.constant([16])\r\naxis = tf.constant(-9223372036854775808, dtype='int64')\r\ntf.gather(params,indices,axis=axis)\r\n```\r\n\r\nThe process will directly be killed by the system when running above code.\r\n\r\nHere is the related output:\r\n\r\n```\r\n2023-12-09 22:48:16.035701: F .\/tensorflow\/core\/framework\/tensor.h:832] Check failed: new_num_elements == NumElements() (0 vs. 1)\r\nAborted (core dumped)\r\n```\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nparams = tf.constant([[0.69]])\r\nindices = tf.constant([16])\r\naxis = tf.constant(-9223372036854775808, dtype='int64')\r\ntf.gather(params,indices,axis=axis)\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2023-12-09T14:54:43Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62603"},{"issue_number":264,"repository":"tensorflow\/tensorflow","title":"[BUG] race condition in local rendezvous","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### TensorFlow version\r\n\r\n2.10, 2.12, doesn't really matter\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nubuntu 20.04\r\n\r\n### Python version\r\n\r\n3.8, 3.9\r\n\r\n### Bazel version\r\n\r\n5.3.0\r\n\r\n### GCC\/compiler version\r\n\r\n9.4\r\n\r\n### Current behavior?\r\n\r\nThe story is kinda bit long and it took us months to debug this issue. I'll try to keep it short.\r\n\r\n### Background and what the problem is\r\nWe run the [DLRM from NV DLE](https:\/\/github.com\/NVIDIA\/DeepLearningExamples\/tree\/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb\/Tensorflow2\/Recommendation\/DLRM) in our CI daily test. \r\n\r\nVery rare, like once every few weeks, the daily test report such error.\r\n\r\n`\r\nW tensorflow\/core\/framework\/op_kernel.cc:1874] OP_REQUIRES failed at strided_slice_op.cc:112 : INVALID_ARGUMENT: Expected begin, end, and strides to be 1D equal size tensors, but got shapes [65536], [1], and [1] instead.\r\n`\r\n\r\nThe[ strided slice op validator](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/3a029b19c9c156cd68cab671b5ce95bde839f15e\/tensorflow\/core\/util\/strided_slice_op.cc#L214) isn't happy about the input parameters.\r\n\r\nThe problem is like a ghost. It appears once every 1 or 2 weeks which makes it very hard to debug. But one thing is for sure is that it's not a random dram bit flip caused by cosmic rays because the error systoms is stable: one of (begin, end, stride) tensor's shape is incorrectly set to [65535] instead of [1]\r\n\r\n### Evidence\r\n\r\nI'll skip the lengthy debug process and jump to the last step.\r\n\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/8800468\/d73ddf6d-7cc9-4a10-90f8-1e4d3885c91b)\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/8800468\/0b99c252-59b0-490d-8321-924adb70be72)\r\n\r\n\r\nWe observed a malfunctioned SEND\/RECV pair.\r\n\r\nThe item tensor in SENDOP has a shape of [1] but during the local rendezvous process the recevied tensor got a shape of [65536]. This tensor later flows to strided slice op and triggers the grumpy validator.\r\n\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/8800468\/7626ef2f-0de4-4eb0-8c25-3cfd9618b359)\r\n\r\nWe grep the key hash (red highlighted 1632...9087) in the debug log.  There are two threads: 66787 and 63997. Most of the time the rendezvous runs in a SEND-RECV pattern within the same thread. \r\n\r\nBut in the green rectangle you can see two threads interleaved and a SEND-SEND-RECV-RECV pattern is observed. Soon after this the bomb exploded.\r\n\r\n### Analysis\r\n\r\nIt seems the root cause is a bug in the rendezvous mechanism. Two unrelated operations generated the same communication Rendezvous key. \r\n\r\nAssuming there are 2 threads, when the CPU load is not heavy, in most cases the scheduling order is\r\n\r\n- THREAD A: SEND(KEY, VALUE_A)\r\n\r\n- THREAD A: RECV(KEY)\r\n\r\n- THREAD B: SEND(KEY, VALUE_B)\r\n\r\n- THREAD B: RECV(KEY)\r\n\r\nIn this way, everyone will be fine even if the keys are the same. It is equivalent to time-division multiplexing of the same KEY.\r\n\r\nWhen the CPU load becomes heavy (which is the case in our daily test scenario), thread scheduling becomes unpredictable. It is possible that such a pattern may occur\r\n\r\n- THREAD A: SEND(KEY, VALUE_A)\r\n\r\n- THREAD B: SEND(KEY, VALUE_B)\r\n\r\n- THREAD A: RECV(KEY)\r\n\r\n- THREAD B: RECV(KEY)\r\n\r\nThis will cause THREAD A to incorrectly receive the data sent by THREAD B.\r\n\r\n\r\n### Related python code\r\n\r\n1. Thread A is running the normal [training loop.](https:\/\/github.com\/NVIDIA\/DeepLearningExamples\/blob\/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb\/Tensorflow2\/Recommendation\/DLRM\/main.py#L281)\r\n2. Thread B is created for the RawBinaryDataset class. The pre-processing creates an [asynchronous thread pool.](https:\/\/github.com\/NVIDIA\/DeepLearningExamples\/blob\/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb\/Tensorflow2\/Recommendation\/DLRM\/split_binary_dataset.py#L138)\r\n3. Thread A is running[ x = x[self.begin_idx:self.end_idx]](https:\/\/github.com\/NVIDIA\/DeepLearningExamples\/blob\/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb\/Tensorflow2\/Recommendation\/DLRM\/model.py#L58). Translated into StridedSlice operation by eager runtime. begin_idx, end_idx and the implicit stride are all constant tensors on CPU, and StridedSlice is an device operator. So these three tensors need to be sent to the device side.\r\n4. Thread B is running[ tensor = tf.expand_dims(tensor, axis=1). ](https:\/\/github.com\/NVIDIA\/DeepLearningExamples\/blob\/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb\/Tensorflow2\/Recommendation\/DLRM\/split_binary_dataset.py#L204). Execute expand_dims. Similar to thread A, the expand_dims operator runs on the device side while the input is on CPU, and the tensor needs to be sent to the device. The shape of this tensor is [65536].\r\n5. Due to the defect of Rendezous::CreateKey, the keys generated by these two operators in the eager runtime are exactly the same.\r\n6. Multi-threading + HASH KEY collision + SEND-SEND-RECV-RECV, all three together, BOOOM.\r\n\r\n### FIX\r\n\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/8800468\/f50ed76a-9cef-4bf7-87b2-e4a298236dd9)\r\n\r\nThe key string is \r\n\/job:localhost\/replica:0\/task:0\/device:CPU:0;ea74dbce35f0ab7e;\/job:localhost\/replica:0\/task:0\/device:MLU:0;edge_2_input;0:0\r\n\r\nThe rendezvous key consists 5 parts.\r\n- src_device: \/job:localhost\/replica:0\/task:0\/device:CPU:0;\r\n- src_incarnation: ea74dbce35f0ab7e;\r\n- dst_device: \/job:localhost\/replica:0\/task:0\/device:MLU:0;\r\n- name: edge_2_input;\r\n- frame_inter: 0:0\r\n\r\nIn this case,  4 out of 5 (src_device, src_incarnation, dst_device_frame_iter) are naturely indistinguishable.\r\n\r\nUnless we add new field in the key, the only field we can play with is `name`.\r\n\r\nThe creation of `edge_2_input` involes another few tons of code.\r\n\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/8800468\/b2d85cd7-1e47-492b-be50-1e3d63f13d55)\r\n\r\nA quick fix for my current problem is simply add `dst_name` in the `tensor_name_attr` during graph partition. The names will become `edge_2_input_strided_slice` and `edge_2_input_expand_dims` thus my problem is solved.\r\n\r\n```diff\r\ndiff --git a\/tensorflow\/core\/graph\/graph_partition.cc b\/tensorflow\/core\/graph\/graph_partition.cc\r\nindex a4f09383c63..57a4e919526 100644\r\n--- a\/tensorflow\/core\/graph\/graph_partition.cc\r\n+++ b\/tensorflow\/core\/graph\/graph_partition.cc\r\n@@ -1147,7 +1147,8 @@ Status Partition(const PartitionOptions& opts, Graph* g,\r\n         tensor_name_attr = opts.get_tensor_name_attr(edge);\r\n       } else {\r\n         tensor_name_attr =\r\n-            strings::StrCat(\"edge_\", edge->id(), \"_\", edge->src()->name());\r\n+            strings::StrCat(\"edge_\", edge->id(), \"_\", edge->src()->name(),\r\n+            \"_\", edge->dst()->name());\r\n       }\r\n```\r\n\r\nBut a more approriate and generic fix might be to have a unique src node name. The source name will be like input_xxxxxx and input_yyyyyy. But this sounds like a fundamental change and I'm not sure if this would break too many things. And I'm not sure about the right place to make the change, like manipulating the input name a little bit [here](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/core\/common_runtime\/eager\/execute.cc#L988).\r\n\r\nI'd like to hear your opinion on this problem and I'd like to file a PR if you could point me the right place to apply the fix.\r\n\r\nCheers\r\nHengwen\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nA minimum case derived from the [DLRM from NV DLE](https:\/\/github.com\/NVIDIA\/DeepLearningExamples\/tree\/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb\/Tensorflow2\/Recommendation\/DLRM) \r\n\r\nClick run all and you can reproduce the error in a few seconds.\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/1CUiyuG2Aob-Fj8xllrmbx2w3cNBXFYqU?usp=sharing\r\n\r\n\r\n\r\n\r\n","labels":["awaiting review","type:bug","comp:core","awaiting PR merge","TF 2.12"],"created_at":"2023-12-01T09:22:59Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62523"},{"issue_number":265,"repository":"tensorflow\/tensorflow","title":"MemoryError: std::bad_alloc when calling tf.squeeze with a floating tensor axis","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.14.0, tf-nightly\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen calling tf.squeeze with axis to be floating tensor, it will raise memory error. MemoryError looks dangerous to me, a proper input argument handling may be better.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ninput = tf.constant([1,2,3], dtype='float32')\r\naxis = tf.constant(1.0, dtype='float32')\r\nout = tf.squeeze(input,axis)\n```\n\n\n### Relevant log output\n\n```shell\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/ops\/gen_array_ops.py in squeeze(input, axis, name)\r\n  10614   if tld.is_eager:\r\n  10615     try:\r\n> 10616       _result = pywrap_tfe.TFE_Py_FastPathExecute(\r\n  10617         _ctx, \"Squeeze\", name, input, \"squeeze_dims\", axis)\r\n  10618       return _result\r\n\r\nMemoryError: std::bad_alloc\n```\n","labels":["awaiting review","stat:awaiting tensorflower","type:bug","comp:ops","TF2.14"],"created_at":"2023-11-29T19:27:17Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62504"},{"issue_number":266,"repository":"tensorflow\/tensorflow","title":"Model checkpoint not saved to google cloud storage","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.14.1\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.17\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI'm training a model and I want to save the model checkpoints in `.keras` format to google cloud storage. I'm using the `ModelCheckpoint` callback but nothing is saved. I'm also using the `Tensorboard` callback and the logs are saved correctly in the same bucket.\r\n\r\nIf I set the file path to a local directory the model checkpoint is saved without any problem.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nhttps:\/\/gist.github.com\/rcalonso\/f12863b6e2c4669be6875deee2ff6dbf\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF2.14"],"created_at":"2023-11-28T16:43:20Z","comments":5,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62495"},{"issue_number":267,"repository":"tensorflow\/tensorflow","title":"Convolution: CPU memory increase with growing number of different sequence lengths","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.14.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\nCUDA V11.8.89, cuDNN version 8600\r\n\r\n### GPU model and memory\r\n\r\nNVIDIA GeForce GTX 1080 Ti\r\n\r\n### Current behavior?\r\n\r\nI noticed a linear increase of CPU memory usage in my setups when using a convolution on raw waveforms (i.e., sequences which are long in time and 1D in feature). I could isolate the issue and it seems to be related to the number of different sequence lengths that occur.  I.e., if the sequence length is fixed to 100k, the memory consumption is constant. If it is randomly sampled from a given range, the memory consumption asymptotically grows towards a larger value as the range gets larger. This can be observed in the plot below. Also note that the memory consumption is not influenced by the absolute sequence length, just by the size of the range.\r\n\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/45091115\/cbaeaadf-6d95-434f-bf2f-1a4aec232efc)\r\n\r\nI measured the memory consumption using `watch_memory()` from [here](https:\/\/github.com\/rwth-i6\/returnn\/blob\/c230d1408a2d7620a9d00a5171c998d3876e69dc\/returnn\/util\/watch_memory.py#L13). The different runs in the plot correspond to different `n_time_min` and `n_time_max` in the stand-alone code.\r\n\r\nI reproduced the issue with an apptainer image built on top of the [tensorflow 2.14 image from dockerhub](https:\/\/hub.docker.com\/layers\/tensorflow\/tensorflow\/2.14.0-gpu-jupyter\/images\/sha256-981372796921ef7bb75f4fe5fbe98c335824d08233bed57586633199028d5e18?context=explore). The image definition file looks as follows:\r\n\r\n<details>\r\n\r\n```\r\nBootstrap: docker\r\nFrom: tensorflow\/tensorflow:2.14.0-gpu \r\nStage: build\r\n\r\n%post\r\n    apt update -y\r\n\r\n    # all the fundamental basics, zsh is need because calling the cache manager might launch the user shell\r\n    DEBIAN_FRONTEND=noninteractive apt install -y wget git unzip gzip libssl-dev lsb-release zsh \\\r\n        bison libxml2-dev libopenblas-dev libsndfile1-dev libcrypto++-dev libcppunit-dev \\\r\n        parallel xmlstarlet python3-lxml htop strace gdb sox python3-pip cmake ffmpeg vim\r\n\r\n    cd \/usr\/local\r\n    git clone https:\/\/github.com\/rwth-i6\/cache-manager.git\r\n    cd bin\r\n    ln -s ..\/cache-manager\/cf cf\r\n\r\n    echo \/usr\/local\/lib\/python3.11\/dist-packages\/tensorflow > \/etc\/ld.so.conf.d\/tensorflow.conf\r\n    ldconfig\r\n\r\n    apt install -y python3 python3-pip\r\n    pip3 install -U pip setuptools wheel\r\n    pip3 install ipdb\r\n    pip3 install h5py six soundfile librosa==0.10 better-exchook dm-tree psutil\r\n    pip3 install --ignore-installed psutil flask ipython\r\n    pip3 install git+https:\/\/github.com\/rwth-i6\/sisyphus\r\n    pip3 install black==22.3.0 matplotlib typing-extensions typeguard  # sequitur-g2p==1.0.1668.23\r\n    pip3 install memray objgraph Pympler\r\n\r\n```\r\n\r\n<\/details>\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nn_feat = 1\r\nn_out = 30\r\nfilter_size = 5\r\nn_steps = 100000\r\nn_time_min = 10000\r\nn_time_max = 30000\r\nbatch_size_max = 400000\r\n\r\nfilters = tf.Variable(tf.random.normal((filter_size, n_feat, n_out), stddev=0.01))\r\nfor step in range(n_steps):\r\n    n_time = np.random.randint(n_time_min, n_time_max)\r\n    n_batch = batch_size_max \/\/ n_time\r\n    x = tf.random.normal((n_batch, n_time, n_feat))\r\n    y = tf.nn.convolution(\r\n        x,\r\n        filters=filters,\r\n        padding=\"VALID\",\r\n    )\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","type:performance","TF2.14"],"created_at":"2023-11-20T15:51:19Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62441"},{"issue_number":268,"repository":"tensorflow\/tensorflow","title":"Tensorflow 2.15 Docker image cannot find the GPU drivers, but nvidia-smi can.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nTF 2.15.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nCUDA 12.2\n\n### GPU model and memory\n\nNVIDIA TITAN V\n\n### Current behavior?\n\nRunning TensorFlow 2.15.0 from within Docker image does not find GPU drivers.\r\n\r\nnvidia-smi reports the GPUs as available.\r\n\r\nThis works as intended with a TF 2.14 image in the same machine.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\ndocker run --gpus all -it tensorflow\/tensorflow:2.15.0-gpu-jupyter bash\r\n\r\n<within the container>\r\n\r\n# nvidia-smi\r\n\r\n# python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n```\n\n\n### Relevant log output\n\n```shell\n# nvidia-smi\r\n\r\nThu Nov 16 16:47:22 2023\r\n+---------------------------------------------------------------------------------------+\r\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\r\n|-----------------------------------------+----------------------+----------------------+\r\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp   Perf          Pwr:Usage\/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n|                                         |                      |               MIG M. |\r\n|=========================================+======================+======================|\r\n|   0  NVIDIA TITAN V                 Off | 00000000:65:00.0 Off |                  N\/A |\r\n| 29%   46C    P8              27W \/ 250W |   1693MiB \/ 12288MiB |      2%      Default |\r\n|                                         |                      |                  N\/A |\r\n+-----------------------------------------+----------------------+----------------------+\r\n\r\n+---------------------------------------------------------------------------------------+\r\n| Processes:                                                                            |\r\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n|        ID   ID                                                             Usage      |\r\n|=======================================================================================|\r\n+---------------------------------------------------------------------------------------+\r\n\r\n# python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\r\n\r\n2023-11-16 16:46:54.131081: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\r\n2023-11-16 16:46:54.255566: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2023-11-16 16:46:54.255623: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2023-11-16 16:46:54.276648: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2023-11-16 16:46:54.327586: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\r\n2023-11-16 16:46:54.328299: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-11-16 16:46:56.486851: W tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https:\/\/www.tensorflow.org\/install\/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\n[]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.15"],"created_at":"2023-11-16T16:52:27Z","comments":13,"reactions":17,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62412"},{"issue_number":269,"repository":"tensorflow\/tensorflow","title":"jit-compiled `tfnp.take_along_axis` shape bug","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf_nightly-2.16.0.dev20231113-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\ncolab\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n[colab demonstrating the issue](https:\/\/colab.research.google.com\/drive\/1nk6NXUoo7qE7g6NHlFHBgMWkJW0LuTMx?usp=sharing)\r\n\r\n`tf.experimental.numpy.take_along_axis` returns tensor with incorrect shape when used with `tf.function(jit_compile=True)`.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nx = tf.random.normal((5, 3, 2))\r\nindices = tf.constant([[[-1]]], dtype=\"int32\")\r\n\r\n\r\ndef f(x, i):\r\n    return tf.squeeze(tf.experimental.numpy.take_along_axis(x, i, axis=-2), axis=-2)\r\n\r\n\r\nz = f(x, indices)\r\nprint(z.shape)  # (5, 2)\r\n\r\nz1 = tf.function(f, jit_compile=True)(x, indices) # errors\r\nprint(z1.shape)\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-97d5eab12d23> in <cell line: 1>()\r\n----> 1 z1 = tf.function(f, jit_compile=True)(x, indices)\r\n      2 print(z1.shape)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51   try:\r\n     52     ctx.ensure_initialized()\r\n---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                         inputs, attrs, num_outputs)\r\n     55   except core._NotOkStatusException as e:\r\n\r\nInvalidArgumentError: Tried to explicitly squeeze dimension 1 but dimension was not 1: 2\r\n\r\nStack trace for op definition: \r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/colab_kernel_launcher.py\", line 37, in <module>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/traitlets\/config\/application.py\", line 992, in launch_instance\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelapp.py\", line 619, in start\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/platform\/asyncio.py\", line 195, in start\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 685, in <lambda>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 738, in _run_callback\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 825, in inner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 361, in process_one\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 261, in dispatch_shell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 539, in execute_request\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 2975, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3030, in _run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/async_helpers.py\", line 78, in _pseudo_sync_runner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3257, in run_cell_async\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3473, in run_ast_nodes\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3553, in run_code\r\nFile \"<ipython-input-4-97d5eab12d23>\", line 1, in <cell line: 1>\r\nFile \"<ipython-input-2-4f365be98111>\", line 8, in f\r\n\r\n\t [[{{node Squeeze}}]]\r\n\ttf2xla conversion failed while converting __inference_f_283[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=\/path\/to\/dump\/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_f_283]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops"],"created_at":"2023-11-14T07:54:57Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62391"},{"issue_number":270,"repository":"tensorflow\/tensorflow","title":"TypeError: this __dict__ descriptor does not support '_DictWrapper' objects","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\nnightly\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nColab\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Current behavior?\r\n\r\n- Install a clean tensorflow environment with the latest `typing-extensions` pip package.\r\n- Assign a dictionary to a `tf.Module` (will wrap it in a `_DictWrapper` for tracking).\r\n- Output that dictionary from a `tf.function`.\r\n- `TypeError: this __dict__ descriptor does not support '_DictWrapper' objects`.\r\n\r\nThis is somewhat of a zombie bug, see https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60687.\r\n\r\nThis is important because it breaks all `keras` functional models with dictionary output, but is not a `keras` bug. This can be reproduced simply with low-level tensorflow.\r\n\r\nWe either need to continue pinning an older version of typing extensions, or fix tensorflow to work with the latest version of typing extension. The latter seems less likely to keep breaking.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nhttps:\/\/colab.research.google.com\/gist\/mattdangerw\/6904dc4ab29ff936ad3c3b966848f463\/dict-output-bug.ipynb\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-4-4a26ae41286e> in <cell line: 12>()\r\n     10 x = tf.constant([2, 3])\r\n     11 y = tf.constant([3, -2])\r\n---> 12 f(x, y)\r\n\r\n4 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/util\/traceback_utils.py in error_handler(*args, **kwargs)\r\n    151     except Exception as e:\r\n    152       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n--> 153       raise e.with_traceback(filtered_tb) from None\r\n    154     finally:\r\n    155       del filtered_tb\r\n\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/core\/function\/trace_type\/trace_type_builder.py in from_value(value, context)\r\n    142   if context.is_legacy_signature and isinstance(value, trace.TraceType):\r\n    143     return value\r\n--> 144   elif isinstance(value, trace.SupportsTracingProtocol):\r\n    145     generated_type = value.__tf_tracing_type__(context)\r\n    146     if not isinstance(generated_type, trace.TraceType):\r\n\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/typing_extensions.py in __instancecheck__(cls, instance)\r\n    601             for attr in cls.__protocol_attrs__:\r\n    602                 try:\r\n--> 603                     val = inspect.getattr_static(instance, attr)\r\n    604                 except AttributeError:\r\n    605                     break\r\n\r\n\/usr\/lib\/python3.10\/inspect.py in getattr_static(obj, attr, default)\r\n   1741         if (dict_attr is _sentinel or\r\n   1742             type(dict_attr) is types.MemberDescriptorType):\r\n-> 1743             instance_result = _check_instance(obj, attr)\r\n   1744     else:\r\n   1745         klass = obj\r\n\r\n\/usr\/lib\/python3.10\/inspect.py in _check_instance(obj, attr)\r\n   1688     instance_dict = {}\r\n   1689     try:\r\n-> 1690         instance_dict = object.__getattribute__(obj, \"__dict__\")\r\n   1691     except AttributeError:\r\n   1692         pass\r\n\r\nTypeError: this __dict__ descriptor does not support '_DictWrapper' objects\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","type:support","comp:apis"],"created_at":"2023-10-25T01:11:31Z","comments":15,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62217"},{"issue_number":271,"repository":"tensorflow\/tensorflow","title":"tf.strings.to_number cannot convert positive integers prefixed with \"+\" when out_type is tf.int32 or tf.int64","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.11.0, 2.13.0, 2.14.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMacOS 13.1\n\n### Mobile device\n\nMacbook Pro\n\n### Python version\n\n3.10.6\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nConverting string to numbers with \"+\"  throws errors in TF 2.11.0, 2.13.0, 2.14.0 when `out_type=tf.int64` or `out_type=tf.in32`. Not expecting error to be thrown and strings can be correctly converted into integers. For example, I would like to parse timezone information from a string (using substring)\r\n\r\n```python \r\nt = tf.constant([\r\n    \"2023-05-07 17:32:25-08:00\", # utc: next day\r\n    \"2023-05-07 05:32:25+11:00\", # utc: previous day\r\n    \"2023-05-07 05:32:25-08:00\", # utc: same date\r\n    \"2023-02-29 23:32:15-04:00\", # leap year\r\n    ]\r\n)\r\n\r\n```\n\n### Standalone code to reproduce the issue\n\n```shell\nCode to reproduce\r\n\r\n\r\nimport tensorflow\r\n\r\n# these are all okay\r\ntf.string.to_number(tf.constant(\"-11\"), out_type=tf.int64)\r\ntf.string.to_number(tf.constant(\"11\"), out_type=tf.int64)\r\ntf.string.to_number(tf.constant(\"+11\"), out_type=tf.float32)\r\n\r\n# this throws the error below\r\ntf.strings.to_number(tf.constant(\"+11\"), out_type=tf.int64)\n```\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/Users\/edward\/opt\/miniforge3\/envs\/testtf214\/lib\/python3.10\/site-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/Users\/edward\/opt\/miniforge3\/envs\/testtf214\/lib\/python3.10\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 5888, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StringToNumber_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} StringToNumberOp could not correctly convert string: +11 [Op:StringToNumber] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF2.14"],"created_at":"2023-10-21T01:55:21Z","comments":2,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62191"},{"issue_number":272,"repository":"tensorflow\/tensorflow","title":"tf.map_fn and TensorArray do not seem to support backpropagation","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf2.14\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n****\r\nI provide three different snippet implementation of a function. I am calculating multiple losses with respect to different networks. \r\nThe first and third implementation gives `gradients=None` in both graph and eager mode.\r\n\r\nThe second implementation works in eager mode but not in graph mode. \r\n\r\nIt seems that tf.map_fn does not support backpropagation #19897 and this issue could be related to that.\n\n### Standalone code to reproduce the issue\n\n```shell\nFirst snippet ( Gradients are None in both graph and eager mode )\r\n\r\n\r\n    def _compute_unadjusted_ce(self, label_batch, logits):\r\n        ce = tf.map_fn(\r\n            lambda x: tf.reduce_mean(\r\n                tf.keras.losses.categorical_crossentropy(\r\n                    from_logits=True,\r\n                    y_pred=logits[x, ...],\r\n                    y_true=label_batch\r\n                )\r\n            ),\r\n            elems=tf.range(self.num_learners),\r\n            fn_output_signature=tf.float32,\r\n            parallel_iterations=1,\r\n            back_prop=True\r\n        )\r\n        return ce\r\n\r\n\r\n\r\n# The following function does the work but fails in graph mode (`tf.function`) ( similar to #37512   \r\n\r\n\r\n def _compute_unadjusted_ce(self, label_batch, logits):\r\n       ce = list()\r\n         for i in range(self.num_learners):\r\n            ce.append(\r\n                 tf.reduce_mean(\r\n                     tf.keras.losses.categorical_crossentropy(\r\n                         from_logits=True,\r\n                         y_pred=logits[i, ...],\r\n                         y_true=label_batch\r\n                     )\r\n                 )\r\n             )\r\n    \r\n         return ce\r\n\r\n\r\n# The following function causes gradients as None ( in both eager and graph mode )\r\n\r\n\r\n def _compute_unadjusted_ce(self, label_batch, logits):\r\n         ce = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True, clear_after_read=True)\r\n         for i in range(self.num_learners):\r\n             ce = ce.write(\r\n                 ce.size(),\r\n                 tf.reduce_mean(\r\n                     tf.keras.losses.categorical_crossentropy(\r\n                         from_logits=True,\r\n                         y_pred=logits[i, ...],\r\n                         y_true=label_batch\r\n                     )\r\n                 )\r\n             )\r\n    \r\n         return ce.stack()\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting response","stat:awaiting tensorflower","type:bug","type:feature","stale","comp:ops","TF2.14"],"created_at":"2023-10-19T12:34:32Z","comments":12,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62174"},{"issue_number":273,"repository":"tensorflow\/tensorflow","title":"TFLite CNN model quantization error","description":"### 1. System information\r\n\r\n- OS Platform and Distribution : Ubuntu 22.04.3 LTS\r\n- TensorFlow installation: pip install tensorflow (virtual env: venv)\r\n- TensorFlow library: pip package -> tensorflow 2.14.0\r\n\r\n### 2. Code\r\n\r\nColab to build the models and reproduce the issue:\r\n\r\n[Reproduce the issue](https:\/\/colab.research.google.com\/drive\/1sx7qmXfP5RA1ituF5Fmh8X-cy0-M7Xol?usp=sharing)\r\n\r\n\r\n### 3. Failure after conversion\r\nHi, I'm having an issue when trying to use signatures of quantized tflite CNN model.\r\n\r\nThe conversion and quantization go well, but when I try to use infer or fine_tune signatures, I get the following error which seems to be related to the quantization process:\r\n\r\nRuntimeError: tensorflow\/lite\/kernels\/conv.cc:374 affine_quantization->zero_point->data[i] != 0 (-24 != 0)Node number 21 (CONV_2D) failed to prepare.tensorflow\/lite\/kernels\/conv.cc:374 affine_quantization->zero_point->data[i] != 0 (-24 != 0)Node number 41 (CONV_2D) failed to prepare.\r\n\r\n\r\n**Note**: I don't get this error with the exact same code using linear model instead of CNN.\r\n\r\nThanks for your help","labels":["stat:awaiting tensorflower","type:bug","TFLiteConverter","TF2.14"],"created_at":"2023-10-19T09:39:50Z","comments":6,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62171"},{"issue_number":274,"repository":"tensorflow\/tensorflow","title":"tf.truncatemod does not support half and bfloat16 data type","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.14.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nFollowing the documentation (https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/truncatemod) tf.truncatemod supports the half and bfloat16 data type but in practical it does not.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\nx = tf.constant(np.random.rand(2,2), dtype='half')\r\ny = tf.constant(np.random.randint(0, 100, ()), dtype='half')\r\nout = tf.truncatemod(x,y)  # crash\r\nprint(out)\r\n\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport warnings\r\nwarnings.filterwarnings(\"ignore\")\r\nx = tf.constant(np.random.rand(2,2), dtype='bfloat16')\r\ny = tf.constant(np.random.randint(0, 100, ()), dtype='bfloat16')\r\nout = tf.truncatemod(x,y)  # crash\r\nprint(out)\n```\n\n\n### Relevant log output\n\n```shell\nNotFoundError: Could not find device for node: {{node TruncateMod}} = TruncateMod[T=DT_BFLOAT16]\r\nAll kernels registered for op TruncateMod:\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]\r\n  device='DEFAULT'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n [Op:TruncateMod] name: \r\n\r\nNotFoundError: Could not find device for node: {{node TruncateMod}} = TruncateMod[T=DT_HALF]\r\nAll kernels registered for op TruncateMod:\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]\r\n  device='DEFAULT'; T in [DT_INT32]\r\n  device='GPU'; T in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]\r\n [Op:TruncateMod] name:\n```\n","labels":["stat:awaiting response","awaiting review","type:bug","stale","comp:ops","TF2.14"],"created_at":"2023-10-09T08:36:40Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62070"},{"issue_number":275,"repository":"tensorflow\/tensorflow","title":"Custom Gradient Computation not working in TF 2.14","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.14.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI am using W&B's Keras callback `WandbCallback`. This callback has a feature to log gradients of each layer at every step. This feature works fine till TF 2.13.0 but is erroring out in TF 2.14.0.\r\n\r\nThis piece of code works fine in Tf 2.13.0 but errors out in TF 2.14.0:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nimport wandb\r\nfrom wandb.keras import WandbModelCheckpoint\r\nfrom wandb.keras import WandbCallback\r\n\r\nrun = wandb.init(project=\"keras\")\r\n\r\nx = np.random.randint(255, size=(100, 28, 28, 1))\r\ny = np.random.randint(10, size=(100,))\r\n\r\ndataset = (x, y)\r\n\r\n\r\ndef get_model():\r\n    m = tf.keras.Sequential()\r\n    m.add(tf.keras.layers.Conv2D(3, 3, activation=\"relu\", input_shape=(28, 28, 1)))\r\n    m.add(tf.keras.layers.Flatten())\r\n    m.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\r\n    return m\r\n\r\n\r\nmodel = get_model()\r\nmodel.compile(\r\n    loss=\"sparse_categorical_crossentropy\",\r\n    optimizer=\"sgd\",\r\n    metrics=[\"accuracy\"],\r\n)\r\n\r\nmodel.fit(\r\n    x,\r\n    y,\r\n    epochs=5,\r\n    validation_data=(x, y),\r\n    callbacks=[\r\n        WandbCallback(\r\n            save_model=False,\r\n            log_gradients=True,\r\n            training_data=(x,y)\r\n        )\r\n    ],\r\n)\r\n\r\n```\r\n\r\nI investigated further and was able to narrow it down to the gradient logging logic which again works fine for 2.13.0 but not for 2.14.0. \r\n\r\nI think this has to do with the breaking changes with `tf.Tensor`.\r\n\r\nThe piece of code below is the gradient logging logic which errors out in the latest version.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nprint(tf.__version__)\r\nimport wandb\r\nimport numpy as np\r\n\r\n_training_data_x = np.random.randint(255, size=(100, 28, 28, 1))\r\n_training_data_y = np.random.randint(10, size=(100,))\r\n\r\n\r\ndef get_model():\r\n    m = tf.keras.Sequential()\r\n    m.add(tf.keras.layers.Conv2D(3, 3, activation=\"relu\", input_shape=(28, 28, 1)))\r\n    m.add(tf.keras.layers.Flatten())\r\n    m.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\r\n    return m\r\n\r\nmodel = get_model()\r\nmodel.compile(\r\n    loss=\"sparse_categorical_crossentropy\",\r\n    optimizer=\"sgd\",\r\n    metrics=[\"accuracy\"],\r\n)\r\n\r\n\r\ndef _get_custom_optimizer_parent_class():\r\n    from pkg_resources import parse_version\r\n\r\n    if parse_version(tf.__version__) >= parse_version(\"2.9.0\"):\r\n        custom_optimizer_parent_class = tf.keras.optimizers.legacy.Optimizer\r\n    else:\r\n        custom_optimizer_parent_class = tf.keras.optimizers.Optimizer\r\n\r\n    return custom_optimizer_parent_class\r\n\r\n\r\n_custom_optimizer_parent_class = _get_custom_optimizer_parent_class()\r\nprint(_custom_optimizer_parent_class)\r\n\r\n\r\nclass _CustomOptimizer(_custom_optimizer_parent_class):\r\n    def __init__(self):\r\n        super().__init__(name=\"CustomOptimizer\")\r\n        self._resource_apply_dense = tf.function(self._resource_apply_dense)\r\n        self._resource_apply_sparse = tf.function(self._resource_apply_sparse)\r\n        tf.print(self._resource_apply_dense)\r\n\r\n    def _resource_apply_dense(self, grad, var):\r\n        var.assign(grad)\r\n\r\n    # this needs to be implemented to prevent a NotImplementedError when\r\n    # using Lookup layers.\r\n    def _resource_apply_sparse(self, grad, var, indices):\r\n        pass\r\n\r\n    def get_config(self):\r\n        return super().get_config()\r\n\r\n\r\nclass _GradAccumulatorCallback(tf.keras.callbacks.Callback):\r\n    \"\"\"Accumulates gradients during a fit() call when used in conjunction with the CustomOptimizer above.\"\"\"\r\n\r\n    def set_model(self, model):\r\n        super().set_model(model)\r\n        self.og_weights = model.get_weights()\r\n        self.grads = [np.zeros(tuple(w.shape)) for w in model.trainable_weights]\r\n\r\n    def on_batch_end(self, batch, logs=None):\r\n        for g, w in zip(self.grads, self.model.trainable_weights):\r\n            g += w.numpy()\r\n        self.model.set_weights(self.og_weights)\r\n\r\n    def get_grads(self):\r\n        return [g.copy() for g in self.grads]\r\n\r\n\r\ninputs = model.inputs\r\nprint(inputs)\r\noutputs = model(inputs)\r\ngrad_acc_model = tf.keras.models.Model(inputs, outputs)\r\ngrad_acc_model.compile(loss=model.loss, optimizer=_CustomOptimizer())\r\n\r\n_grad_accumulator_model = grad_acc_model\r\n_grad_accumulator_model.summary()\r\n\r\n_grad_accumulator_callback = _GradAccumulatorCallback()\r\n\r\n\r\n_grad_accumulator_model.fit(\r\n    _training_data_x,\r\n    _training_data_y,\r\n    verbose=0,\r\n    callbacks=[_grad_accumulator_callback],\r\n)\r\n\r\nweights = model.trainable_weights\r\ngrads = _grad_accumulator_callback.grads\r\nprint(weights)\r\n\r\nmetrics = {}\r\nfor weight, grad in zip(weights, grads):\r\n    metrics[\r\n        \"gradients\/\" + weight.name.split(\":\")[0] + \".gradient\"\r\n    ] = wandb.Histogram(grad)\r\n\r\nprint(metrics)\n```\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\r\n  File \"\/home\/ayushthakur\/client\/wandb\/test_grad_logging.py\", line 88, in <module>\r\n    _grad_accumulator_model.fit(\r\n  File \"\/opt\/conda\/envs\/tf214\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 70, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/tmp\/__autograph_generated_file4zq8l42d.py\", line 15, in tf__train_function\r\n    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\r\nTypeError: in user code:\r\n\r\n    File \"\/opt\/conda\/envs\/tf214\/lib\/python3.10\/site-packages\/keras\/src\/engine\/training.py\", line 1377, in train_function  *\r\n        return step_function(self, iterator)\r\n    File \"\/opt\/conda\/envs\/tf214\/lib\/python3.10\/site-packages\/keras\/src\/engine\/training.py\", line 1360, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"\/opt\/conda\/envs\/tf214\/lib\/python3.10\/site-packages\/keras\/src\/engine\/training.py\", line 1349, in run_step  **\r\n        outputs = model.train_step(data)\r\n    File \"\/opt\/conda\/envs\/tf214\/lib\/python3.10\/site-packages\/keras\/src\/engine\/training.py\", line 1130, in train_step\r\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\r\n    File \"\/opt\/conda\/envs\/tf214\/lib\/python3.10\/site-packages\/keras\/src\/optimizers\/legacy\/optimizer_v2.py\", line 601, in minimize\r\n        return self.apply_gradients(grads_and_vars, name=name)\r\n    File \"\/opt\/conda\/envs\/tf214\/lib\/python3.10\/site-packages\/keras\/src\/optimizers\/legacy\/optimizer_v2.py\", line 760, in apply_gradients\r\n        return tf.__internal__.distribute.interim.maybe_merge_call(\r\n    File \"\/opt\/conda\/envs\/tf214\/lib\/python3.10\/site-packages\/keras\/src\/optimizers\/legacy\/optimizer_v2.py\", line 844, in _distributed_apply\r\n        with tf.control_dependencies([tf.group(update_ops)]):\r\n\r\n    TypeError: 'inputs' should be zero or more (nested) Tensors. Received 'None' with type '<class 'NoneType'>'.\n```\n","labels":["stat:awaiting response","stat:awaiting tensorflower","type:bug","stale","comp:ops","regression issue","TF2.14"],"created_at":"2023-10-05T09:51:32Z","comments":6,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62053"},{"issue_number":276,"repository":"tensorflow\/tensorflow","title":"`pip install tf-nightly[and-cuda]` fails for recent nightlies","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nnightly\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nGoogle Colab\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n**The issue**\r\n\r\nAttempting to run `pip install tf-nightly[and-cuda]` will download a ton of nightly candidates before installing one from mid-September (before tf bumped to cuda12).\r\n\r\nAttempting to pin the more recent versions shows the error with recent nightlies.\r\n\r\n```shell\r\npip install tf-nightly[and-cuda]==2.15.0.dev20231002\r\n...\r\nERROR: Could not find a version that satisfies the requirement tensorrt-libs==8.6.1; extra == \"and-cuda\" (from tf-nightly[and-cuda]) (from versions: 9.0.0.post11.dev1, 9.0.0.post12.dev1, 9.0.1.post11.dev4, 9.0.1.post12.dev4)\r\nERROR: No matching distribution found for tensorrt-libs==8.6.1; extra == \"and-cuda\"\r\n```\r\n\r\nYou can work around this with `pip install tf-nightly[and-cuda] --extra-index-url https:\/\/pypi.nvidia.com`.\r\n\r\n**What should happen**\r\n`pip install tf-nightly[and-cuda]` should not self conflict, and recent nighties should be installable via PyPI.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nhttps:\/\/colab.research.google.com\/gist\/mattdangerw\/00acd58e43aabe7f80a74d595788bd86\/tf-nightly-and-cuda.ipynb\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","comp:gpu:tensorrt","TF2.14"],"created_at":"2023-10-02T19:25:22Z","comments":7,"reactions":2,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62035"},{"issue_number":277,"repository":"tensorflow\/tensorflow","title":"Tensorflow profiler client does not support IPv6","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.11.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.2\n\n### Bazel version\n\n6.0.0\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n11\/8.2\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n[https:\/\/github.com\/google\/tsl\/blob\/3dee2c5930eb8ee9c6a7486434240dafadf12fb4\/tsl\/profiler\/utils\/session_manager.cc#L188]()\r\n\r\n`std::vectorabsl::string_view parts = absl::StrSplit(host_port, ':');`\r\n\r\nWhen trying to connect to an IPv6 host-port pair, which is typically denoted as [xxxx:xxxx:blah]:port, `profiler_client.trace()` throws an error. It should have logic that supports both IPv4 and IPv6. \n\n### Standalone code to reproduce the issue\n\n```shell\nFile 1:\r\njax.profiler.start_server(9876) #on IPv6 xxxx:xxxx:x:xxxx:xxxx::xxx\r\n\r\nFile 2:\r\nfrom tensorflow.python.profiler import profiler_client\r\nprofiler_client.trace(\r\n        '[xxxx:xxxx:x:xxxx:xxxx::xxx]:9876',\r\n        duration_ms=1000,\r\n        logdir='foo',\r\n    )\n```\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\r\n  File \"foo.py\", line 2, in _trigger_trace\r\n    profiler_client.trace(\r\n  File \"...\/foo.runfiles\/tensorflow_python_deps_tensorflow\/tensorflow\/python\/profiler\/profiler_client.py\", line 129, in trace\r\n    _pywrap_profiler.trace(\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Could not interpret \"[xxxx:xxxx:x:xxxx:xxxx::xxx]:9876\" as a host-port pair.\n```\n","labels":["stat:awaiting response","stat:awaiting tensorflower","type:bug","stale","comp:apis","TF 2.11"],"created_at":"2023-09-29T17:59:57Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62018"},{"issue_number":278,"repository":"tensorflow\/tensorflow","title":"tf.train.Checkpoint.restore does not restore the tf.data.Iterator state.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n11.8\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`tf.data.Dataset` iterators are supported in `tf.train.Checkpoint`.  When I try to restore a checkpoint, I always get a `StopIteration` error.  This should not be the case because : \r\n\r\n1. I save the checkpoint only at the end of an epoch.\r\n2. Before saving the checkpoint, I always call `db_iter=iter(dataset)` ,so it should give me a new iterator.\r\n\r\nIn prior versions of TF there were functions like `make_initializable_iterator()` which are now deprecated. Since, there is no native mechanism to reset a python iterator to beginning, I wonder why `tf.train.Checkpoint.restore()` does not save the iterator state properly.\r\n\r\nI also understand well that one can just loop over the dataset like:\r\n\r\n```\r\nimport tensorflow as tf\r\nds = tf.data.Dataset.range(50)\r\nfor index, data in ds:\r\n    # Do something \r\n```\r\n\r\nHowever, saving the dataset in the checkpoint has two problems:\r\n\r\na) **I have not explicitly tested this**  For a very large dataset like MSCOCO or OpenImages or even Imagenet, what does saving the dataset mean , if I already have the dataset as TFRecords ? Will it end up saving the whole dataset again inside the  checkpoint ? At least, `tf.data.Dataset.save()` points towards this. In case this is it, I would never like to save a large dataset directly into the checkpoint.\r\n\r\nb) **This is fully tested** If I experiment with a small simple dataset ( e.g:- `tf.data.Dataset.range(10)` ) and save it in the checkpoint, then it does not restore the state of the iterator. This observation is the same as #48178 which is still unresolved. \n\n### Standalone code to reproduce the issue\n\n```shell\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom absl import app\r\n\r\n\r\nclass Net(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.x = tf.Variable(tf.keras.initializers.GlorotUniform()(shape=[1]))\r\n\r\n    def call(self, inputs):\r\n        return inputs\r\n\r\n\r\ndef main(argv):\r\n    del argv\r\n    tf.random.set_seed(123)\r\n    net = Net()\r\n    optim = tf.optimizers.Adam(learning_rate=0.001)\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices((np.arange(10)))\r\n    dataset = dataset.shuffle(10, reshuffle_each_iteration=True)\r\n    dataset = dataset.batch(2)\r\n    db_iter = iter(dataset)\r\n    step = tf.Variable(0)\r\n    checkpoint = tf.train.Checkpoint(\r\n        step=step, optimizer=optim, net=net, db_iter=db_iter\r\n    )\r\n    manager = tf.train.CheckpointManager(checkpoint, \".\/ckpts\", max_to_keep=50)\r\n    manager.restore_or_initialize()\r\n    print(step)\r\n    for epoch in range(10):\r\n        manager.restore_or_initialize()\r\n        for _ in range(dataset.cardinality()):\r\n            batch = next(db_iter)\r\n            step.assign_add(1)\r\n            print(batch)\r\n        db_iter = iter(dataset)\r\n        manager.save()\r\n        print(f\"Epoch {epoch} finished.\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    app.run(main)\r\n```\n```\n\n\n### Relevant log output\n\n```shell\nThe first time, it will print something like : \r\n\r\n<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0>\r\ntf.Tensor([3 6], shape=(2,), dtype=int64)\r\ntf.Tensor([4 0], shape=(2,), dtype=int64)\r\ntf.Tensor([1 5], shape=(2,), dtype=int64)\r\ntf.Tensor([7 8], shape=(2,), dtype=int64)\r\ntf.Tensor([2 9], shape=(2,), dtype=int64)\r\nEpoch 0 finished.\r\ntf.Tensor([6 3], shape=(2,), dtype=int64)\r\ntf.Tensor([0 5], shape=(2,), dtype=int64)\r\ntf.Tensor([2 8], shape=(2,), dtype=int64)\r\ntf.Tensor([1 7], shape=(2,), dtype=int64)\r\ntf.Tensor([4 9], shape=(2,), dtype=int64)\r\nEpoch 1 finished.\r\ntf.Tensor([0 3], shape=(2,), dtype=int64)\r\ntf.Tensor([1 4], shape=(2,), dtype=int64)\r\ntf.Tensor([6 7], shape=(2,), dtype=int64)\r\ntf.Tensor([9 8], shape=(2,), dtype=int64)\r\ntf.Tensor([5 2], shape=(2,), dtype=int64)\r\nEpoch 2 finished.\r\ntf.Tensor([3 7], shape=(2,), dtype=int64)\r\ntf.Tensor([0 9], shape=(2,), dtype=int64)\r\ntf.Tensor([4 2], shape=(2,), dtype=int64)\r\ntf.Tensor([1 8], shape=(2,), dtype=int64)\r\ntf.Tensor([5 6], shape=(2,), dtype=int64)\r\n\r\nand so on....\r\n\r\n\r\nFrom next time onwards:\r\n\r\n    raise StopIteration\r\nStopIteration\n```\n","labels":["stat:awaiting response","stat:awaiting tensorflower","type:bug","stale","comp:data","TF 2.13"],"created_at":"2023-09-28T15:49:30Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62006"},{"issue_number":279,"repository":"tensorflow\/tensorflow","title":"TimeDistributed not compatible with multi-output models","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.13\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have been attempting to create a custom CNN-RNN hybrid model that incorporates a pretrained custom DenseNet-derived CNN. My model requires from the CNN both its final output and the intermediate outputs of its dense blocks (as I am attempting to create a U-Net like architecture). I attempted to create a new model from the given CNN that would output these intermediate convolutional layers along with the final output value. However, when I connected it to a TimeDistributed layer, I received a strange error with no clear explanation:\r\n\r\n`AttributeError: 'list' object has no attribute 'shape'`\r\n\r\nAfter some experimentation, I realized this error was arising because the TimeDistributed layer was not programmed to handle multi-output layers passed to it. I understand that this issue will likely require a fix to Tensorflow. In the meanwhile, any suggestions for workarounds would be appreciated.\n\n### Standalone code to reproduce the issue\n\n```shell\ncnn_model = tf.keras.models.load_model(cnn_filepath)\r\ncnn_final_output = cnn_model.layers[-1].output\r\ncnn_intermediate_output = cnn_model.layers[-3].output\r\nnew_model = tf.keras.models.Model(inputs=cnn_input, outputs=[cnn_final_output,cnn_intermediate_output])\r\n\r\noutput = tfl.TimeDistributed(new_model)(input_images, mask=mask)\n```\n\n\n### Relevant log output\n\n```shell\nAttributeError                            Traceback (most recent call last)\r\n\/Users\/[NAME REMOVED]\/[NAME REMOVED] DL\/TRG_project_recurrent_cnn_residual_instantaneous_train.ipynb Cell 1 line 1\r\n    173 cnn_model.trainable = False\r\n    174 model = RNNCNNResidualModel(N_A, cnn_model, [\"average_pooling2d_6\", \"average_pooling2d_19\", \"average_pooling2d_44\", \"activation_239\"])\r\n--> 176 model.build([tf.TensorShape([None, None, 101, 101, 1]), tf.TensorShape([None, None, 1])])\r\n    178 #Initialize inputs\r\n    180 plot_model(model, to_file='rnn_model_plot.png', show_shapes=True, show_layer_names=True)\r\n\r\nFile ~\/mambaforge\/envs\/tensorflow\/lib\/python3.10\/site-packages\/keras\/src\/engine\/training.py:521, in Model.build(self, input_shape)\r\n    516     raise ValueError(\r\n    517         \"You can only call `build()` on a model if its \"\r\n    518         \"`call()` method accepts an `inputs` argument.\"\r\n    519     )\r\n    520 try:\r\n--> 521     self.call(x, **kwargs)\r\n    522 except (tf.errors.InvalidArgumentError, TypeError) as e:\r\n    523     raise ValueError(\r\n    524         \"You cannot build your model by calling `build` \"\r\n    525         \"if your layers do not support float type inputs. \"\r\n   (...)\r\n    529         f\"`call` is: {e}.\"\r\n    530     )\r\n\r\nFile ~\/[NAME REMOVED] DL\/rnn_cnn_residual_model.py:41, in RNNCNNResidualModel.call(self, inputs, states, return_state, training)\r\n     38 if states is None:\r\n     39   states = self.lstm_cell.get_initial_state(input_timesteps)\r\n---> 41 output = tfl.TimeDistributed(self.cnn_model)(input_images, mask=mask)\r\n     42 fgr_output = output[0]\r\n     43 lstm_input = tfl.Concatenate()([fgr_output, input_timesteps])\r\n\r\nFile ~\/mambaforge\/envs\/tensorflow\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\r\n     68     # To get the full stack trace, call:\r\n     69     # `tf.debugging.disable_traceback_filtering()`\r\n---> 70     raise e.with_traceback(filtered_tb) from None\r\n     71 finally:\r\n     72     del filtered_tb\r\n\r\nFile ~\/mambaforge\/envs\/tensorflow\/lib\/python3.10\/site-packages\/keras\/src\/backend.py:1534, in int_shape(x)\r\n   1514 \"\"\"Returns shape of tensor\/variable as a tuple of int\/None entries.\r\n   1515 \r\n   1516 Args:\r\n   (...)\r\n   1531 \r\n   1532 \"\"\"\r\n   1533 try:\r\n-> 1534     shape = x.shape\r\n   1535     if not isinstance(shape, tuple):\r\n   1536         shape = tuple(shape.as_list())\r\n\r\nAttributeError: 'list' object has no attribute 'shape'\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","TF 2.13"],"created_at":"2023-09-28T14:07:56Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62004"},{"issue_number":280,"repository":"tensorflow\/tensorflow","title":"XLA compiled `floordiv` allows `integer division by zero`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0-dev20230914\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nXLA compiled `floordiv` allows `integer division by zero` where an exception `Integer division by zero` will be raised without XLA compilation.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n\"\"\"\r\nXLA Compiled\r\n\"\"\"\r\nclass Model(tf.keras.Model):\r\n    @tf.function(jit_compile=True)\r\n    def call(self, x1):\r\n        x2 = tf.math.floordiv(3, 0)\r\n        x3 = tf.math.multiply(x1, x2)\r\n        return x3\r\n\r\nm = Model()\r\nx1 = tf.constant(1, shape=[])\r\nprint(m(x1)) # tf.Tensor(-1, shape=(), dtype=int32)\r\n\r\n\"\"\"\r\nWithout XLA\r\n\"\"\"\r\nclass Model(tf.keras.Model):\r\n    def call(self, x1):\r\n        x2 = tf.math.floordiv(3, 0)\r\n        x3 = tf.math.multiply(x1, x2)\r\n        return x3\r\n\r\nm = Model()\r\nx1 = tf.constant(1, shape=[])\r\nprint(m(x1))\n```\n\n\n### Relevant log output\n\n```shell\nInvalidArgumentError: Exception encountered when calling layer 'model_21' (type Model).\r\n\r\n{{function_node __wrapped__FloorDiv_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Integer division by zero [Op:FloorDiv] name: \r\n\r\nCall arguments received by layer 'model_21' (type Model):\r\n  \u2022 x1=tf.Tensor(shape=(), dtype=int32)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","comp:tf.function","TF2.14"],"created_at":"2023-09-23T19:51:17Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61965"},{"issue_number":281,"repository":"tensorflow\/tensorflow","title":"calling Model in a loop would leak memory","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf2.13.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nWindows Server 2019\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nthere is a transformer model when i try to decode messages, translate input sentence to target sentence i get memory blow up, memory is good when i use transformer.fit(), but in a loop like below it blows up memory, tf.keras.backend.clear_session() does\u2019t help, also accuracy decrease when i use that, gc.collect() doesn\u2019t work also\r\nhere is my code\r\n\r\n```python\r\ndef decode_sequence(input_sentence):\r\n    tokenized_input_sentence = input_vectorization([input_sentence])\r\n    decoded_sentence = START_TOKEN\r\n    for i in tf.range(max_decoded_sentence_length):\r\n        tokenized_target_sentence = output_vectorization([decoded_sentence])#[:, :-1]\r\n        \r\n        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\r\n        \r\n        \r\n        sampled_token_index = np.argmax(predictions[0, i, :])\r\n        sampled_token = output_index_lookup[sampled_token_index]\r\n        decoded_sentence += sampled_token\r\n\r\n        if sampled_token == END_TOKEN:\r\n            break\r\n    \r\n   \r\n    gc.collect()\r\n    return decoded_sentence\r\n\r\nfrom tqdm import tqdm\r\ndef overall_accuracy(pairs):\r\n    corrects = 0\r\n    inputs = pairs[2739:]\r\n    iter = tqdm(inputs)\r\n    for i, pair in enumerate(iter):\r\n        input_text = pair[0]\r\n        target = pair[1]\r\n        predicted = decode_sequence(input_text)\r\n        #guess = '\u2713' if predicted == target else '\u2717'\r\n        #print('Sample Number : ', i, 'Predicted : ', predicted, 'Real : ', target, guess)\r\n        if predicted == target:\r\n            corrects += 1\r\n        iter.set_postfix(corrects=corrects, accuracy=corrects \/ (i + 1))\r\n    \r\n    return corrects \/ len(inputs)\r\n\r\nprint(\"Overall Acurracy : \", overall_accuracy(test_pairs))```\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\ncalling model in a loop\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:apis","TF 2.13"],"created_at":"2023-09-21T16:51:08Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61942"},{"issue_number":282,"repository":"tensorflow\/tensorflow","title":"tf.linalg.cholesky output normal value on a complex64 matrix that is not positive definite.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn a randomly generated matrix that is not positive definite, tf.linalg.cholesky outputs nan if the matrix's dtype is float and outputs 0 if the matrix' dtype is complex. It may be not appropriate especially when tf.linalg.cholesky outputs 0 on an invalid input without giving any abnormal behaviors. For your inference, np.linalg.cholesky will directly raises with error message when receiving non-positive definite matrix in float or complex data type.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\nimport warnings\r\nimport os\r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\nwarnings.filterwarnings(\"ignore\")\r\nnp.random.seed(1234)\r\narray = np.random.rand(4,4).astype(\"complex64\")\r\ntry:\r\n    print(\"Numpy's result: \", np.linalg.cholesky(array))\r\nexcept:\r\n    print(\"Numpy crash.\")\r\n\r\nprint(\"TensorFlow's result: \", tf.linalg.cholesky(tf.constant(array)))\r\n\r\narray = np.random.rand(4,4).astype(\"float64\")\r\ntry:\r\n    print(\"Numpy's result: \", np.linalg.cholesky(array))\r\nexcept:\r\n    print(\"Numpy crash.\")\r\n\r\nprint(\"TensorFlow's result: \", tf.linalg.cholesky(tf.constant(array)))\n```\n\n\n### Relevant log output\n\n```shell\nNumpy crash.\r\nTensorFlow's result:  tf.Tensor(\r\n[[0.+0.j 0.+0.j 0.+0.j 0.+0.j]\r\n [0.+0.j 0.+0.j 0.+0.j 0.+0.j]\r\n [0.+0.j 0.+0.j 0.+0.j 0.+0.j]\r\n [0.+0.j 0.+0.j 0.+0.j 0.+0.j]], shape=(4, 4), dtype=complex64)\r\nNumpy crash.\r\nTensorFlow's result:  tf.Tensor(\r\n[[nan  0.  0.  0.]\r\n [nan nan  0.  0.]\r\n [nan nan nan  0.]\r\n [nan nan nan nan]], shape=(4, 4), dtype=float64)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-09-19T13:54:42Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61916"},{"issue_number":283,"repository":"tensorflow\/tensorflow","title":"Randomization generating repeated sequences with `tf.function`, `tf.random.set_seed` and `tf.cond`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.15.0-dev20230919\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.2 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWe have encountered an unexpected issue with `tf.function` compilation combined with the global\/graph-level seed setting (`tf.random.set_seed`); under some circumstances the random variables are generating identical sequences within the same function. The issue seems to be connected with TensorFlow conditionals (`tf.cond`) within the function, particularly when the branches contain the randomization calls. Removing `tf.cond` results in the generation of unique random values, as expected.\r\n\r\nWe understand the following expected behavior under `tf.function` compilation when a global\/graph-level seed is set, as per the [TensorFlow documentation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/random\/set_seed):\r\n\r\n> Note that `tf.function` acts like a re-run of a program in this case. When the global seed is set but operation seeds are not set, the sequence of random numbers are the same for each `tf.function`.\r\n\r\nHowever, the problem we are observing arises within a single instance of the function, not across multiple instances. The behavior is as if the internal counters\r\nin `tf.random.uniform` get reset when there is a TensorFlow conditional in the graph. Is this expected behavior?\r\n\r\nTo illustrate this issue, the example below presents two scenarios. The first one (\"SAD\" mode) shows the function producing repeated random sequences when `tf.cond` is present. The second scenario (\"HAPPY\" mode) shows the function generating unique random values once `tf.cond` is removed.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nfrom pprint import pprint\r\n\r\ntf.random.set_seed(42)\r\n\r\nvar = tf.Variable(0, dtype=tf.int32)\r\ndef get_value(happy):\r\n    if happy:\r\n        return tf.stack([tf.random.uniform(()) for _ in range(2)])\r\n    else:\r\n        return tf.cond(\r\n            var == 0,\r\n            lambda: tf.stack([tf.random.uniform(()) for _ in range(2)]),\r\n            lambda: tf.stack([tf.random.uniform(()) for _ in range(2)]),\r\n        )\r\n\r\ndef randomize(happy):\r\n    return [get_value(happy) for _ in range(3)]\r\n\r\nprint(\"SAD\")\r\npprint(tf.function(randomize)(False))\r\nprint(\"HAPPY\")\r\npprint(tf.function(randomize)(True))\n```\n\n\n### Relevant log output\n\n```shell\nSAD\r\n[<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>,\r\n <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>,\r\n <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>]\r\nHAPPY\r\n[<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>,\r\n <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.98781276, 0.63789964], dtype=float32)>,\r\n <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.00857747, 0.02621067], dtype=float32)>]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops"],"created_at":"2023-09-19T10:48:34Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61912"},{"issue_number":284,"repository":"tensorflow\/tensorflow","title":"`tf.device` context manager does not restore `cudaCurrentDevice` under some conditions","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.13\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.5 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.0\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen using `tf.device` context manager, the current device of cuda runtime remains \"dirty\" even after exiting the context manager. This happens when: 1. tensorflow is initializing GPU context on this line (tf.device), 2. there is no materialization of tensors on GPU.\r\n\r\nFor context, keeping a clean state of current device context is important to keep tensorflow in sync with other GPU based libraries such as [cuDF](github.com\/rapidsai\/cuDF). [RMM](github.com\/rapidsai\/rmm) memory allocators also depends on the assumption that the context stays the same throughout the lifetime of allocations.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/gist.github.com\/isVoid\/9eded87fca35e86a2c2dc85f603383c2\n```\n\n\n### Relevant log output\n\n```shell\n# Log output of the first cell. The second and third current device context should be 0.\r\n\r\n(<cudaError_t.cudaSuccess: 0>, 0)\r\n(<cudaError_t.cudaSuccess: 0>, 7)\r\n(<cudaError_t.cudaSuccess: 0>, 7)\r\n(<cudaError_t.cudaSuccess: 0>, 0)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","comp:gpu","TF 2.13"],"created_at":"2023-09-19T10:21:44Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61911"},{"issue_number":285,"repository":"tensorflow\/tensorflow","title":"tf.linalg.cholesky fails on half precision","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nFollowing the documentation: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/linalg\/cholesky, tf.linalg.cholesky is expected to accept tensor in half precision but it fails.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\nnp.random.seed(2023)\r\n\r\n# {'input_ndims': 2}\r\ninput = tf.constant(np.random.rand(3,3), dtype='half')\r\nout = tf.linalg.cholesky(input)\r\n```\n```\n\n\n### Relevant log output\n\n```shell\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/framework\/ops.py in raise_from_not_ok_status(e, name)\r\n   6654 def raise_from_not_ok_status(e, name):\r\n   6655   e.message += (\" name: \" + str(name if name is not None else \"\"))\r\n-> 6656   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n   6657 \r\n   6658 \r\n\r\nNotFoundError: Could not find device for node: {{node Cholesky}} = Cholesky[T=DT_HALF]\r\nAll kernels registered for op Cholesky:\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_HALF]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n [Op:Cholesky] name:\n```\n","labels":["awaiting review","type:bug","comp:ops","TF 2.13"],"created_at":"2023-09-19T05:37:10Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61907"},{"issue_number":286,"repository":"tensorflow\/tensorflow","title":"XLA doesn't do the DCE as autocluster","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0-dev20230914\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nXLA doesn't do the DCE as autocluster. In the example below, the second line `sliced2 = tf.slice(x1, [0, 1, 0, 0], [-1, -1, 4, -1])` is dead code, which is deleted when enabling `autocluster`. However, the XLA compiled model will still execute this line.\r\n\r\nIt is expected to delete this line since it is dead code.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport os\r\n\r\n\"\"\"\r\nAutocluster\r\n\"\"\"\r\n\r\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'\r\n\r\nclass Model(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n\r\n  @tf.function\r\n  def call(self, x1):\r\n    sliced1 = tf.slice(x1, [0, 0, 1, 0], [-1, -1, 1, -1])\r\n    sliced2 = tf.slice(x1, [0, 1, 0, 0], [-1, -1, 4, -1])\r\n    return sliced1\r\n\r\n# Initializing the model\r\nx1_shape = (1, 3, 3, 2)\r\nm = Model()\r\n\r\n# Inputs to the model\r\nx1 = tf.range(18)\r\nx1 = tf.reshape(x1, x1_shape)\r\n\r\n# Call model\r\noutput_tensor = m(x1)\r\n\r\n\"\"\"\r\nXLA\r\n\"\"\"\r\nos.environ['TF_XLA_FLAGS'] = ''\r\n\r\nclass Model(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n\r\n  @tf.function(jit_compile=True)\r\n  def call(self, x1):\r\n    sliced1 = tf.slice(x1, [0, 0, 1, 0], [-1, -1, 1, -1])\r\n    sliced2 = tf.slice(x1, [0, 1, 0, 0], [-1, -1, 4, -1])\r\n    return sliced1\r\n\r\n# Initializing the model\r\nx1_shape = (1, 3, 3, 2)\r\nm = Model()\r\n\r\n# Inputs to the model\r\nx1 = tf.range(18)\r\nx1 = tf.reshape(x1, x1_shape)\r\n\r\n# Call model\r\noutput_tensor = m(x1)\r\n\r\n\"\"\"\r\nInvalidArgumentError: Exception encountered when calling layer 'model_5' (type Model).\r\n\r\nExpected size[2] in [0, 3], but got 4\r\n\"\"\"\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF2.14"],"created_at":"2023-09-17T18:49:54Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61884"},{"issue_number":287,"repository":"tensorflow\/tensorflow","title":"XLA compiled model skip `build` method","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0-dev20230914\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nXLA compiled model skip `build` method, which is not expected since the `build` method is expected to be invoked automatically before the first execution of `call()`invoked automatically before the first execution of call().\r\n\r\nThe example below shows that the XLA compiled model doesn't invoke `build` since `self.w` is still `[[1, 0], [0, 1]]` instead of the result by `add_weight`.\n\n### Standalone code to reproduce the issue\n\n```shell\n\"\"\"\r\nWithout XLA\r\n\"\"\"\r\nimport tensorflow as tf\r\n\r\nclass Model(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.w = tf.Variable([[1., 0.], [0., 1.]])\r\n\r\n  def build(self, input_shape):\r\n    super(Model, self).build(input_shape)\r\n    self.w = self.add_weight(\"weight\", shape=input_shape[1:], trainable=True)\r\n    \r\n  def call(self, x):\r\n    return tf.matmul(x, self.w), self.w\r\n\r\n# Initializing the model\r\nm = Model()\r\n# Input to the model\r\nx1 = tf.constant([[6., 7.], [2., 7.]], shape=[1,2,2])\r\nprint(m(x1))\r\n\"\"\"\r\n(<tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=\r\narray([[[4.0300035, 3.5133138],\r\n        [3.804155 , 5.0799932]]], dtype=float32)>, <tf.Variable 'model_4\/weight:0' shape=(2, 2) dtype=float32, numpy=\r\narray([[ 0.05646205, -0.3916698 ],\r\n       [ 0.5273187 ,  0.83761895]], dtype=float32)>)\r\n\"\"\"\r\n\r\n\"\"\"\r\nWith XLA\r\n\"\"\"\r\n\r\nclass Model(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.w = tf.Variable([[1., 0.], [0., 1.]])\r\n\r\n  def build(self, input_shape):\r\n    super(Model, self).build(input_shape)\r\n    self.w = self.add_weight(\"weight\", shape=input_shape[1:], trainable=True)\r\n\r\n  @tf.function(jit_compile=True)  \r\n  def call(self, x):\r\n    return tf.matmul(x, self.w), self.w\r\n\r\n# Initializing the model\r\nm = Model()\r\n# Input to the model\r\nx1 = tf.constant([[6., 7.], [2., 7.]], shape=[1,2,2])\r\nprint(m(x1))\r\n\r\n\"\"\"\r\n(<tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=\r\narray([[[6., 7.],\r\n        [2., 7.]]], dtype=float32)>, <tf.Tensor: shape=(2, 2), dtype=float32, numpy=\r\narray([[1., 0.],\r\n       [0., 1.]], dtype=float32)>)\r\n\"\"\"\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:tf.function","TF2.14"],"created_at":"2023-09-17T02:11:14Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61882"},{"issue_number":288,"repository":"tensorflow\/tensorflow","title":"XLA compiled `tf.matmul` can work for two size-incompatible matrices","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0-dev20230914\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nXLA compiled `tf.matmul` can work for two size-incompatible matrices, like `[1, 5]` and `[10, 1]`. By contrast, if we run `tf.matmul` directly without XLA compilation, it will raise the error as expected: `Matrix size-incompatible: In[0]: [1,5], In[1]: [10,1] [Op:BatchMatMulV2] name: `\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\"\"\"\r\nWith XLA\r\n\"\"\"\r\nclass Model(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.w = tf.Variable(tf.random.normal([10,1]),\r\n            shape=tf.TensorShape(None),\r\n            dtype='float32')\r\n  @tf.function(jit_compile=True)\r\n  def call(self, x):\r\n    return tf.matmul(x, self.w)\r\n\r\nm = Model()\r\n\r\n# Inputs to the model\r\nx1 = tf.constant([1., 2., 3., -3., 2.5], shape=[1, 5])\r\nprint(m(x1))\r\n# tf.Tensor([[-11.37509]], shape=(1, 1), dtype=float32)\r\n\r\n\"\"\"\r\nWithout XLA\r\n\"\"\"\r\nclass Model(tf.keras.Model):\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.w = tf.Variable(tf.random.normal([10,1]),\r\n            shape=tf.TensorShape(None),\r\n            dtype='float32')\r\n\r\n  def call(self, x):\r\n    return tf.matmul(x, self.w)\r\n\r\nm = Model()\r\n\r\n# Inputs to the model\r\nx1 = tf.constant([1., 2., 3., -3., 2.5], shape=[1, 5])\r\nprint(m(x1))\r\n\"\"\"\r\nInvalidArgumentError: Exception encountered when calling layer 'model' (type Model).\r\n\r\n{{function_node __wrapped__BatchMatMulV2_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} Matrix size-incompatible: In[0]: [1,5], In[1]: [10,1] [Op:BatchMatMulV2] name: \r\n\r\nCall arguments received by layer 'model' (type Model):\r\n  \u2022 x=tf.Tensor(shape=(1, 5), dtype=float32)\r\n\"\"\"\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.13"],"created_at":"2023-09-16T21:02:17Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61881"},{"issue_number":289,"repository":"tensorflow\/tensorflow","title":"Invalid `Conv2d` can be executed without compilation","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.15.0-dev20230914\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nAn invalid `Conv2d` can be executed without compilation. By contrast, after using `@tf.function(jit_compile=True)`, it will raise error `Negative dimension size caused by subtracting 2 from 1 for '{{node conv2d_6\/Conv2D}} ...`\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n\"\"\"\r\nDon't use @tf.function(jit_compile=True)\r\n\"\"\"\r\n\r\nclass Model(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.conv2d = tf.keras.layers.Conv2D(5, 2, activation=tf.nn.relu)\r\n\r\n  def call(self, x):\r\n    conv2d = self.conv2d(x)\r\n    return conv2d\r\n\r\n# Initializing the model\r\nm = Model()\r\n\r\n# Inputs to the model\r\n# This input results in a Conv2D kernel that has the shape (1, 2, 3, 5).\r\nx1 = tf.constant([[[[1., 2., 3.], [4., 5., 6.]]]], shape=[1, 1, 2, 3])\r\n\r\ny = m(x1)\r\nprint(y.shape)\r\n# (1, 0, 1, 5)\r\n\r\n\"\"\"\r\nUsing @tf.function(jit_compile=True)\r\n\"\"\"\r\nclass Model(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n    self.conv2d = tf.keras.layers.Conv2D(5, 2, activation=tf.nn.relu)\r\n  @tf.function(jit_compile=True)\r\n  def call(self, x):\r\n    conv2d = self.conv2d(x)\r\n    return conv2d\r\n# Initializing the model\r\nm = Model()\r\n\r\n# Inputs to the model\r\n# This input results in a Conv2D kernel that has the shape (1, 2, 3, 5).\r\nx1 = tf.constant([[[[1., 2., 3.], [4., 5., 6.]]]], shape=[1, 1, 2, 3])\r\n\r\ny = m(x1)\r\n\"\"\"\r\n\r\n    ValueError: Exception encountered when calling layer 'conv2d_6' (type Conv2D).\r\n    \r\n    Negative dimension size caused by subtracting 2 from 1 for '{{node conv2d_6\/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x, conv2d_6\/Conv2D\/ReadVariableOp)' with input shapes: [1,1,2,3], [2,2,3,5].\r\n    \r\n    Call arguments received by layer 'conv2d_6' (type Conv2D):\r\n      \u2022 inputs=tf.Tensor(shape=(1, 1, 2, 3), dtype=float32)\r\n\r\n\r\nCall arguments received by layer 'model_10' (type Model):\r\n  \u2022 x=tf.Tensor(shape=(1, 1, 2, 3), dtype=float32)\r\n\"\"\"\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:apis","comp:tf.function"],"created_at":"2023-09-15T23:55:07Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61879"},{"issue_number":290,"repository":"tensorflow\/tensorflow","title":"Wrong element_spec when mapping ragged dataset","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.13\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux, Rocky 9\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n```python\r\nds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2, 3], [4, 5]]))\r\nds.element_spec\r\n```\r\n\r\nThe `element_spec` is a `RaggedTensorSpec`\r\n\r\nBut after a `map`, like for instance\r\n```\r\nds.map(tf.square)\r\n````\r\nthe `element_spec` is just a `TensorSpec`\n\n### Standalone code to reproduce the issue\n\n```shell\n.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.13"],"created_at":"2023-09-15T07:17:15Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61869"},{"issue_number":291,"repository":"tensorflow\/tensorflow","title":"StringLookup \/ tf.lookup resource cleanup on model cleared","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.12\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nDescribe the problem.\r\n\r\nDefine a simple network with a StringLookup of non-trivial size\r\n\r\nReload the network in a for loop.\r\n\r\nMemory goes to infinity.\r\n\r\nDescribe the current behavior.\r\nMemory goes up every load\r\n\r\nDescribe the expected behavior.\r\nMemory doesn't go up every load\r\n\r\nWhen using StringLookup: \r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/4107771\/267815955-966e2a02-135a-4e68-a185-c49120e90092.png)\r\n\r\nWhen using Hashing it works:\r\n\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/4107771\/2145a1d1-1eaf-4595-808c-8f53a8a6616f)\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport time\r\nimport os\r\n\r\nimport tensorflow as tf\r\nimport psutil\r\n\r\ndef model_fn():\r\n    X = tf.keras.Input(shape=(1,), dtype=tf.string)\r\n    lookup = tf.keras.layers.StringLookup(\r\n        vocabulary=tf.constant([str(x) for x in range(100_000)])\r\n    )(X)\r\n    Y = tf.math.reduce_sum(lookup, axis=1)\r\n\r\n    return tf.keras.Model(inputs=[X], outputs=[Y])\r\n\r\nmodel = model_fn()\r\nmodel.save(\"\/tmp\/test-model\")\r\n\r\nloaded_model = None\r\n\r\nprocess = psutil.Process()\r\n\r\ndef get_current_mem():\r\n    return process.memory_info().rss \/ 1e6\r\n\r\ndef load():\r\n    global loaded_model\r\n    loaded_model = tf.saved_model.load(\"\/tmp\/test-model\")\r\n\r\nprint('==========================================================')\r\nprint(\"starting process...\")\r\n\r\nfor i in range(100_000):\r\n    start_mem = get_current_mem()\r\n    start = time.time()\r\n\r\n    print(f\"i={i} loading...\", end='')\r\n    load()\r\n\r\n    curr_mem = get_current_mem()\r\n    end = time.time()\r\n    print(f\"done (mem_usage={curr_mem - start_mem}mb took={int(end - start)}s)\")\r\n    time.sleep(0.25)\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:apis","TF 2.12"],"created_at":"2023-09-14T17:44:28Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61866"},{"issue_number":292,"repository":"tensorflow\/tensorflow","title":"Compiled x.shape throws Tuple out-of-range error","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.13, 2.15.0-dev20230913\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen a tensor has shape `(2,)`, `x.shape` returns `2`, and throws an error when I try to get `x.shape[0]`. Without `@tf.function(jit_compile=True)` it works well.\n\n### Standalone code to reproduce the issue\n\n```shell\nclass Model(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n\r\n  @tf.function(jit_compile=True)\r\n  def call(self, x1):\r\n    def select_2_or_one(x):\r\n      print(x.shape)\r\n      if int(x.shape[0]) == 2:\r\n        return x\r\n      else:\r\n        return tf.stack([x,x])\r\n    return tf.cond(tf.less(x1[0], x1[1]),\r\n        lambda: select_2_or_one(x1),\r\n        lambda: select_2_or_one(x1[0]))\r\n\r\nm = Model()\r\n\r\ninput_shape = [2,]\r\nx1 = tf.constant([2.,3.], shape=input_shape)\r\n\r\ny = m(x1)\r\nprint(y)\n```\n\n\n### Relevant log output\n\n```shell\nif int(x.shape[0]) == 2:\r\n\r\n    IndexError: tuple index out of range\r\n\r\n\r\nCall arguments received by layer 'model_1' (type Model):\r\n  \u2022 x1=tf.Tensor(shape=(2,), dtype=float32)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.13"],"created_at":"2023-09-14T15:39:18Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61862"},{"issue_number":293,"repository":"tensorflow\/tensorflow","title":"XLA compiled `tf.reshape` throws ValueError: Shape must be rank 1 but is rank 0","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf2.13, 2.15.0-dev20230913\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nThe following model calls `tf.reshape` on an input tensor, and throws ValueError when compiled with XLA. Without XLA compilation, it runs smoothly. \r\n\r\nThis issue is observed on TF 2.13 and also nightly. See the [gist](https:\/\/colab.research.google.com\/gist\/dengyinlin\/d9368054f16a011f9d1fed3cede96b95\/xla-compiled-tf-reshape-throws-valueerror-shape-must-be-rank-1-but-is-rank-0.ipynb) for more detail.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nclass Model(tf.keras.Model):\r\n\r\n  def __init__(self):\r\n    super(Model, self).__init__()\r\n\r\n  @tf.function(jit_compile=True)\r\n  def call(self, x1):\r\n    x2 = tf.reshape(x1, (2, 2))\r\n    return tf.reshape(x2, (4))\r\n\r\nm = Model()\r\n\r\ninput_shape = (4)\r\nx1 = tf.constant([4.,5.,6.,7.], shape=input_shape)\r\n\r\ny = m(x1)\r\nprint(y)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nValueError: Shape must be rank 1 but is rank 0 for '{{node Reshape_1}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](Reshape, Reshape_1\/shape)' with input shapes: [2,2], [].\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.13"],"created_at":"2023-09-13T21:09:29Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61856"},{"issue_number":294,"repository":"tensorflow\/tensorflow","title":"TFLite - Cannot execute with NNAPI on Samsung Galaxy Tab S9 (Snapdragon 8 Gen 2). On delegate CPU available","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.12.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nAndroid 13\n\n### Mobile device\n\nSamsung Galaxy Tab S9\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\nclang 14\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nSnapdragon 8 Gen 2\n\n### Current behavior?\n\nI have a C++\/Kotlin Android application which make inference using Tensorflow Lite (TFLite library 2.12.0 get using conan center).\r\nI use NNAPI delegate and it work great on Google Pixel tab and Samsung Galaxy Tab S8.\r\n\r\nI try my application on the new tablet, Samsung Galaxy Tab S8 which have a processor 'snapdragon 8 gen 2', and this time, the inference work, BUT it use delegate for CPU and its much slower than S8...\r\n\r\nLooking at the problem, when I call the function to build the interperter, I have this message: `INFO: Created TensorFlow Lite delegate for NNAPI.`, so everything since ok until that.\r\n\r\nBut, when I call `AllocateTensors()` on the interpreter, I have this messages:\r\n```\r\nAccess denied finding property \"ro.mediatek.platform\"\r\nCreated TensorFlow Lite XNNPACK delegate for CPU.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n```\r\n\r\nIs there something special to do to used NNAPI on GPU\/TPU in that case?\r\n\r\n(You can see the full output of the build of interpreter + call to AllocateTensors() below)\n\n### Standalone code to reproduce the issue\n\n```shell\n\/\/ Load the model\r\nstd::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(filename);\r\n\r\n\/\/ Build the interpreter\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n\r\ntflite::StatefulNnApiDelegate::Options options;\r\ntflite::StatefulNnApiDelegate delegate(options);\r\ninterpreter->ModifyGraphWithDelegate(&delegate);\r\n\r\n\/\/ Resize input tensors -> Here the delegate CPU will be used\r\ninterpreter->AllocateTensors();\r\n```\n```\n\n\n### Relevant log output\n\n```shell\n12329-12329 Manager         I  findAvailableDevices\r\n10477-10477 tflite          I  Created TensorFlow Lite delegate for NNAPI.\r\n10477-10584 ....C++-Error   I  INFO: Created TensorFlow Lite delegate for NNAPI.\r\n10477-10477 libc            W  Access denied finding property \"ro.mediatek.platform\"\r\n10477-10477 tflite          I  Created TensorFlow Lite XNNPACK delegate for CPU.\r\n10477-10584 ....C++-Error   I  INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n```\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:lite","TFLiteNNAPIDelegate","TF 2.12"],"created_at":"2023-09-13T10:38:40Z","comments":14,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61854"},{"issue_number":295,"repository":"tensorflow\/tensorflow","title":"tf.linalg.eigvals outputs UnboundLocalError when receiving a float16 tensor","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nGiven a float16 tensor, tf.linalg.eigvals outputs `UnboundLocalError: local variable 'out_dtype' referenced before assignment`. If tf.linalg.eigvals does not accept float16 tensor, it would be better if it can be explicit in the documentation and the error message can point this issue out.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ntensor = tf.constant([[1,2],[3,4]], dtype=tf.float16)\r\ntf_out = tf.linalg.eigvals(tf.constant(tensor))\r\nprint(\"TensorFlow's result: \",tf_out)\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nUnboundLocalError                         Traceback (most recent call last)\r\n<ipython-input-69-5a0470cb5082> in <cell line: 3>()\r\n      1 import tensorflow as tf\r\n      2 tensor = tf.constant([[1,2],[3,4]], dtype=tf.float16)\r\n----> 3 tf_out = tf.linalg.eigvals(tf.constant(tensor))\r\n      4 print(\"TensorFlow's result: \",tf_out)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/ops\/linalg_ops.py in eigvals(tensor, name)\r\n    431   elif tensor.dtype == dtypes.float64 or tensor.dtype == dtypes.complex128:\r\n    432     out_dtype = dtypes.complex128\r\n--> 433   e, _ = gen_linalg_ops.eig(tensor, Tout=out_dtype, compute_v=False, name=name)\r\n    434   return e\r\n    435 \r\n\r\nUnboundLocalError: local variable 'out_dtype' referenced before assignment\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-09-10T15:58:12Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61827"},{"issue_number":296,"repository":"tensorflow\/tensorflow","title":"Inconsistent result between tf.linalg.eigvals and numpy\/scipy on a (3,3) tensor","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nGiven the same tensor, tf.linalg.eigvals outputs results different from np.linalg.eigvals and scipy.linalg.eig. The latter two APIs are also computing the eigenvalues and it may be expected if tf.linalg.eigvals can output the same results as theirs.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport scipy\r\nimport numpy as np\r\n\r\nnp.random.seed(1234)\r\ntensor = np.random.random((3, 3)).astype(np.float32)\r\n\r\ntf_out = tf.linalg.eigvals(tf.constant(tensor))\r\nnp_out = np.linalg.eigvals(tensor)\r\nscipy_out = scipy.linalg.eig(tensor)[0]\r\nprint(\"TensorFlow's result: \",tf_out)\r\nprint(\"Numpy's result: \", np_out)\r\nprint(\"Scipy's result: \", scipy_out)\n```\n\n\n### Relevant log output\n\n```shell\nTensorFlow's result:  tf.Tensor([-0.20270659+0.j  1.7388148 +0.j  0.3935262 +0.j], shape=(3,), dtype=complex64)\r\nNumpy's result:  [ 1.7388151  -0.20270674  0.39352626]\r\nScipy's result:  [ 1.7388152 +0.j -0.20270659+0.j  0.39352617+0.j]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-09-10T15:54:18Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61826"},{"issue_number":297,"repository":"tensorflow\/tensorflow","title":"The `mask` dtype should be asserted to be boolean in `tf.compat.v1.boolean_mask` (and `tf.boolean_mask`)","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.14.0-rc1\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nThe `mask` dtype should be asserted to be boolean in `tf.compat.v1.boolean_mask` (and `tf.boolean_mask`).\r\nThe documentation [`tf.boolean_mask`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/boolean_mask) states that the `mask` is a K-D boolean Tensor, K <= N and K must be known statically.\r\nHowever, currently, the `mask` dtype is not asserted to be boolean in `tf.compat.v1.boolean_mask` (and `tf.boolean_mask`) and cast to boolean if it is not a boolean Tensor, which may lead to unexpected results.\r\nGenerally, only the `0` and `1` values in the `mask` can be directly treated as `False` and `True` respectively, while other values should be determined by certain rules in different scenarios (_e.g._ negative values can be treated as `False` in some scenarios, or close to `0` values can be treated as `False` in some other scenarios).\r\n\r\nI think it's worth updating the documentation to state that the `mask` will be cast to boolean if it is not a boolean Tensor, or warning users that the `mask` dtype should be boolean.\r\nAs mentioned in #54412, it is better to raise an InvalidArgumentError Exception when the `mask` dtype is not boolean, which forces users to cast the `mask` to boolean explicitly.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)\r\nprint(tf.config.list_physical_devices(), flush=True)\r\n\r\n\r\ntry:\r\n    tensor = [0,1,2,3]\r\n    mask = tf.random.uniform([4], dtype=tf.float64)\r\n    x1 = tf.compat.v1.boolean_mask(tensor, mask) \r\n    print(x1, flush=True)\r\nexcept Exception as e:\r\n    print(\"Success! Error:\", str(e), flush=True)\r\nelse:\r\n    print(\"Failed!\", flush=True)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```text\r\nv2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1\r\n[PhysicalDevice(name='\/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='\/physical_device:GPU:0', device_type='GPU')]\r\ntf.Tensor([0 1 2 3], shape=(4,), dtype=int32)\r\nFailed!\r\n```\r\n","labels":["awaiting review","type:bug","comp:ops","TF2.14"],"created_at":"2023-09-09T09:40:31Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61820"},{"issue_number":298,"repository":"tensorflow\/tensorflow","title":"Wrong answer from tflite model with certain configurations of depthwise conv2d","description":"### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 11\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0\r\n\r\n### 2. Code\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntf.keras.utils.set_random_seed(0)\r\n\r\n# Layer parameter values\r\nimageSize = (8, 8)\r\nnumChannels = 2\r\nfmt = 'channels_last'\r\nkernelSize = (3, 1)\r\npadding = 'same'\r\ndepthMult = 1\r\nuseBias = False\r\nstrides = (2, 2)\r\n\r\n# Determine input shape\r\nif fmt == 'channels_last':\r\n    inShape = imageSize + (numChannels,)\r\nelse:\r\n    inShape = (numChannels,) + imageSize\r\n\r\n# Determine input size and create input data\r\ninSize = (1,) + inShape\r\nrng = np.random.default_rng(seed=999)\r\nx = rng.random(size=inSize, dtype=np.float32)\r\n\r\n# Construct Keras model\r\ninp = tf.keras.layers.Input(shape=inShape, batch_size=1)\r\ndconv = tf.keras.layers.DepthwiseConv2D(kernelSize, strides=strides, padding=padding, depth_multiplier=depthMult,\r\n                                        data_format=fmt, activation=None, use_bias=useBias)(inp)\r\nkModel = tf.keras.models.Model(inputs=inp, outputs=dconv)\r\nkModel.compile()\r\n\r\n# Run Keras model predict\r\nkY = kModel.predict(x)\r\nprint(kY)\r\n\r\n# Convert model to tflite\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(kModel)\r\ntfModel = converter.convert()\r\n\r\n# Run tflite interpreter\r\ninterpreter = tf.lite.Interpreter(model_content=tfModel)\r\ninterpreter.allocate_tensors()\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\ninterpreter.set_tensor(input_details[0]['index'], x)\r\ninterpreter.invoke()\r\ntfY = interpreter.get_tensor(output_details[0]['index'])\r\nprint(tfY)\r\n\r\nisequal = np.allclose(kY, tfY, rtol=1e-4, atol=1e-4)\r\nprint(isequal)\r\n```\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- Model produces wrong results - in this case all 0s\r\n\r\n5. (optional) Any other info \/ logs\r\n\r\nSome general observations on when this wrong answer occurs (all of the following must be met):\r\n- One of the kernel_size dimensions is 1 (1xN or Nx1)\r\n- Stride is not 1\r\n- Padding is same\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","TFLiteConverter","TF 2.13"],"created_at":"2023-09-08T14:16:17Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61815"},{"issue_number":299,"repository":"tensorflow\/tensorflow","title":"`tensorflow\/python\/compiler\/xla\/jit_test` fails on PPC with $HOME unset","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n6.3.1\n\n### GCC\/compiler version\n\n12.3\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nRunning the test `\/\/tensorflow\/python\/compiler\/xla:jit_test_cpu` through Bazels build system with a pre-installed numpy (build from source) yields a segmentation fault I cannot explain.\r\n\r\nIt happened after upgrading our toolchain from GCC 11 & Python 3.10 to GCC 12 and Python 3.11 but includes also a couple other software versions which changed but trying to narrow them down wasn't successful. But seemingly only Python and OpenSSL (instead of BoringSSL which doesn't work on PPC) as well as the mentioned numpy should be involved.\r\n\r\nAfter a lot of digging I found that omitting `$HOME` when executing the test (which Bazel does) triggers the segmentation fault.   \r\nAdding `--test_env HOME=\/non-existing` works around this.\r\n\r\nI further traced it to `testJITCreateOpsLambda` in particular keeping only the single `compute` call at https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.13.0\/tensorflow\/python\/compiler\/xla\/jit_test.py#L76 is enough while `self.compute(False, create_ops)` works which means it is related to XLA compilation.\r\n\r\nAlso replacing [`random_uniform`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.13.0\/tensorflow\/python\/compiler\/xla\/jit_test.py#L70) by `constant_op.constant(1)` avoids the error, so it is related to how that call is compiled\/run but I failed to further follow how that happens\r\n\r\nI'm at loss how to debug this further and whether this is an issue with TensorFlows XLA compilation or a bug elsewhere only triggered by that special environment\n\n### Standalone code to reproduce the issue\n\n```shell\n`bazel test --config=noaws --config=nogcp --config=nohdfs --compilation_mode=opt --config=opt --copt=\"-fPIC\" --action_env=CPATH='\/sw\/installed\/OpenSSL\/1.1\/include' --host_action_env=CPATH='\/sw\/installed\/OpenSSL\/1.1\/include' --action_env=LIBRARY_PATH='\/sw\/installed\/OpenSSL\/1.1\/lib' --host_action_env=LIBRARY_PATH='\/sw\/installed\/OpenSSL\/1.1\/lib' --action_env=PYTHONPATH --host_action_env=PYTHONPATH --action_env PYTHON_BIN_PATH --action_env PYTHON_LIB_PATH --python_path=$(which python)  -- \/\/tensorflow\/python\/compiler\/xla:jit_test_cpu`\r\n\r\nor reduced to the actual invocation after bazel failed:\r\n\r\n`export PYTHONPATH=bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/com_google_protobuf\/python:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/tblib_archive\/src:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/absl_py:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/astunparse_archive:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/com_google_protobuf:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/dill_archive:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/gast_archive:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/opt_einsum_archive:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/six_archive:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/tblib_archive:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/termcolor_archive:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/typing_extensions_archive:bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/wrapt:$PYTHONPATH\r\n(cd \/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/bazel-root\/9c88b62e77874bb73aea75868f86ebae\/execroot\/org_tensorflow && \\\r\ncd - &&\r\n  exec env - \\\r\n    LD_LIBRARY_PATH=$LD_LIBRARY_PATH \\\r\n    PATH=$PATH \\\r\n    PYTHONNOUSERSITE=1 \\\r\n    PYTHONPATH=$PYTHONPATH \\\r\n    HOME2='\/fake' \\\r\npython bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/compiler\/xla\/jit_test.py)`\n```\n\n\n### Relevant log output\n\n```shell\nRunning tests under Python 3.11.3: \/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/bin\/python\r\n[ RUN      ] JITTest.testJITCreateOpsLambda\r\n2023-09-08 10:32:04.231849: I tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\r\n2023-09-08 10:32:04.404724: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x201108083080 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2023-09-08 10:32:04.404765: I tensorflow\/compiler\/xla\/service\/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2023-09-08 10:32:04.603193: I .\/tensorflow\/compiler\/jit\/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\nFatal Python error: Segmentation fault\r\n\r\nThread 0x0000200000048800 (most recent call first):\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1455 in _call_tf_sessionrun\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1362 in _run_fn\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1379 in _do_call\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1372 in _do_run\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1192 in _run\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 969 in run\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_util.py\", line 2059 in run\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/compiler\/xla\/jit_test.py\", line 55 in compute\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/compiler\/xla\/jit_test.py\", line 81 in testJITCreateOpsLambda\r\n  File \"\/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/lib\/python3.11\/unittest\/case.py\", line 579 in _callTestMethod\r\n  File \"\/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/lib\/python3.11\/unittest\/case.py\", line 623 in run\r\n  File \"\/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/lib\/python3.11\/unittest\/case.py\", line 678 in __call__\r\n  File \"\/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/lib\/python3.11\/unittest\/suite.py\", line 122 in run\r\n  File \"\/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/lib\/python3.11\/unittest\/suite.py\", line 84 in __call__\r\n  File \"\/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/lib\/python3.11\/unittest\/suite.py\", line 122 in run\r\n  File \"\/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/lib\/python3.11\/unittest\/suite.py\", line 84 in __call__\r\n  File \"\/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/lib\/python3.11\/unittest\/runner.py\", line 217 in run\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/absl_py\/absl\/testing\/_pretty_print_reporter.py\", line 86 in run\r\n  File \"\/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/lib\/python3.11\/unittest\/main.py\", line 274 in runTests\r\n  File \"\/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/lib\/python3.11\/unittest\/main.py\", line 102 in __init__\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2537 in _run_and_get_tests_result\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2568 in run_tests\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2156 in _run_in_app\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2049 in main\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 51 in g_main\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/absl_py\/absl\/app.py\", line 258 in _run_main\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/absl_py\/absl\/app.py\", line 312 in run\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 60 in main_wrapper\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/benchmark.py\", line 489 in benchmarks_main\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 62 in main\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/test.py\", line 56 in main\r\n  File \"\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/compiler\/xla\/jit_test.py\", line 321 in <module>\r\n\r\nExtension modules: google.protobuf.pyext._message, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, tensorflow.python.framework.fast_tensor_util (total: 15)\r\n*** Received signal 11 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/..\/..\/..\/_solib_ppc\/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow\/libtensorflow_framework.so.2(+0xd76744)[0x200b0de86744]\r\n[0x2000000504d8]\r\n\/beegfs\/ws\/1\/s3248973-EasyBuild\/easybuild-ml\/software\/Python\/3.11.3-GCCcore-12.3.0\/lib\/libpython3.11.so.1.0(+0x14090c)[0x2000001b090c]\r\n[0x2000000504d8]\r\n\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2(_ZN3xla3cpu13CpuExecutable22ExecuteComputeFunctionEPKNS_20ExecutableRunOptionsEN4absl12lts_202301254SpanIKNS_23MaybeOwningDeviceMemoryEEEPNS_19HloExecutionProfileE+0x150)[0x200b15956c40]\r\n\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2(+0x5be7c04)[0x200b15957c04]\r\n\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2(_ZN15stream_executor4host10HostStream8WorkLoopEv+0x1ac)[0x200b1d822b0c]\r\n\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2(+0xdab3274)[0x200b1d823274]\r\n\/dev\/shm\/s3248973-EasyBuild\/TensorFlow\/2.13.0\/foss-2022b\/TensorFlow\/tensorflow-2.13.0\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/compiler\/xla\/jit_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/..\/..\/..\/_solib_ppc\/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow\/libtensorflow_framework.so.2(+0xd85088)[0x200b0de95088]\r\n\/lib64\/libpthread.so.0(+0x8b94)[0x2000008c8b94]\r\n\/lib64\/libc.so.6(clone+0xe4)[0x200000a585f4]\r\n*** END MANGLED STACK TRACE ***\r\n\r\n*** Begin stack trace ***\r\n\ttsl::CurrentStackTrace[abi:cxx11]()\r\n\t\r\n\t__kernel_sigtramp_rt64\r\n\t\r\n\t__kernel_sigtramp_rt64\r\n\txla::cpu::CpuExecutable::ExecuteComputeFunction(xla::ExecutableRunOptions const*, absl::lts_20230125::Span<xla::MaybeOwningDeviceMemory const>, xla::HloExecutionProfile*)\r\n\t\r\n\tstream_executor::host::HostStream::WorkLoop()\r\n\t\r\n\t\r\n\t\r\n\tclone\r\n*** End stack trace ***\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","TF 2.13"],"created_at":"2023-09-08T08:32:32Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61814"},{"issue_number":300,"repository":"tensorflow\/tensorflow","title":"Process aborted when running `tf.gather` and `tf.compat.v1.gather` on GPU with large parameters and indices","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.14.0-rc1\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nProcess aborted when running `tf.gather` and `tf.compat.v1.gather` on GPU with certain parameters and indices, while it throws an Exception on CPU.\r\nIt always happens on GPU favor when the `params` and `indices` are large, regardless of the `validate_indices` value.\r\nBesides, the Check failed error still occurs when `validate_indices` is set to `True`, and it should be an Exception instead.\r\nI think the root cause of this behavior should be related to the bug in #60276 and #60756, which face a Check failed error in `tf.raw_ops.Gather\/V2` op.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)\r\n\r\ntry:\r\n    validate_indices = False # True\r\n    params = tf.saturate_cast(tf.random.uniform([13, 15, 7, 13, 14], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.half)\r\n    indices = tf.saturate_cast(tf.random.uniform([11, 12, 6, 15, 11, 3], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int64)\r\n    res = tf.gather(\r\n        validate_indices=validate_indices,\r\n        params=params,\r\n        indices=indices,\r\n    )\r\n    # res = tf.compat.v1.gather(\r\n    #     validate_indices=validate_indices,\r\n    #     params=params,\r\n    #     indices=indices,\r\n    # )\r\nexcept Exception as e:\r\n    print(\"Error:\", str(e), flush=True)\r\nprint(\"Success!\", flush=True)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nOn GPU, it throws a Check failed and core dumped.\r\n\r\n```text\r\nv2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1\r\n2023-09-03 13:08:24.487989: F .\/tensorflow\/core\/util\/gpu_launch_config.h:160] Check failed: work_element_count >= 0 (0 vs. -549025096)\r\nAborted (core dumped)\r\n```\r\n\r\nOn CPU, it throws a Exception.\r\n\r\n```text\r\nv2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1\r\nError: {{function_node __wrapped__GatherV2_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} indices[7,11,2,8,5,1] = 424 is not in [0, 13) [Op:GatherV2] name: \r\nSuccess!\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-09-03T14:31:11Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61780"},{"issue_number":301,"repository":"tensorflow\/tensorflow","title":"Process aborted when running `tf.keras.layers.MaxPooling1D` on GPU with large `pool_size`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.14.0-rc1\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nProcess aborted with Check failed error when running `tf.keras.layers.MaxPooling1D` on GPU with large `pool_size`, while it throws an Exception on CPU.\r\nWhen I reduce the `pool_size_0` to `1e+18`, it throws an Exception on GPU, which is a more reasonable behavior I think.\r\nThis behavior is similar to what described in #61642, which also throws a Check failed error by using a large `pool_size` for `tf.compat.v1.layers.MaxPooling1D` on GPU.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport tensorflow as tf\r\nprint(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)\r\n\r\ntry:\r\n    pool_size_0 = 1e+19\r\n    pool_size = [pool_size_0,]\r\n    strides_0 = 2\r\n    strides = [strides_0,]\r\n    padding = \"same\"\r\n    data_format = \"channels_last\"\r\n    arg_class = tf.keras.layers.MaxPooling1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)\r\n    # arg_class = tf.keras.layers.MaxPool1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)\r\n    # arg_class = tf.compat.v1.keras.layers.MaxPooling1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)\r\n    # arg_class = tf.compat.v1.keras.layers.MaxPool1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)\r\n    arg_input_0_tensor = tf.random.uniform([1, 5, 4], dtype=tf.float32)\r\n    arg_input_0 = tf.identity(arg_input_0_tensor)\r\n    arg_input = [arg_input_0,]\r\n    out = arg_class(*arg_input)\r\nexcept Exception as e:\r\n    print(\"Error:\", str(e), flush=True)\r\nprint(\"Success!\", flush=True)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nOn GPU, it throws a Check failed and core dumped.\r\n\r\n```text\r\nv2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1\r\n2023-09-03 11:34:09.267078: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:442] Loaded cuDNN version 8600\r\n2023-09-03 11:34:09.267132: F tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:1019] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)\r\nAborted (core dumped)\r\n```\r\n\r\nOn CPU, it throws a Exception.\r\n\r\n```text\r\nv2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1\r\nError: Exception encountered when calling layer 'max_pooling1d' (type MaxPooling1D).\r\n\r\n{{function_node __wrapped__MaxPool_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Sliding window ksize for dimension 1 was zero. [Op:MaxPool]\r\n\r\nCall arguments received by layer 'max_pooling1d' (type MaxPooling1D):\r\n  \u2022 inputs=tf.Tensor(shape=(1, 5, 4), dtype=float32)\r\nSuccess!\r\n```\r\n\r\nWhen I change the `pool_size_0` to `1e+18`, it throws a Exception on GPU.\r\n\r\n```text\r\nv2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1\r\n2023-09-03 11:51:28.441325: W tensorflow\/core\/framework\/op_kernel.cc:1816] OP_REQUIRES failed at maxpooling_op.cc:1260 : INVALID_ARGUMENT: Attr ksize has value 1000000000000000000 out of range for an int32\r\nError: Exception encountered when calling layer 'max_pooling1d' (type MaxPooling1D).\r\n\r\n{{function_node __wrapped__MaxPool_device_\/job:localhost\/replica:0\/task:0\/device:GPU:0}} Attr ksize has value 1000000000000000000 out of range for an int32 [Op:MaxPool]\r\n\r\nCall arguments received by layer 'max_pooling1d' (type MaxPooling1D):\r\n  \u2022 inputs=tf.Tensor(shape=(1, 5, 4), dtype=float32)\r\nSuccess!\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","TF 2.13"],"created_at":"2023-09-03T14:26:52Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61778"},{"issue_number":302,"repository":"tensorflow\/tensorflow","title":"FAILED: \/\/tensorflow\/tools\/proto_splitter\/cc:saved_model_splitter_test on mac arm64 wheels","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.14rc0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nmacOS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nTest is failing.\r\nThis checkin the regression started:\r\n\r\n```\r\ncommit e1e4de39de51064a359320f9b32936bf1599d4a0Author: Laura Pak <lpak@google.com>Date: \u00a0 Tue Apr 26 16:51:30 2022 -0700\u00a0 \u00a0 Update highwayhash from fd3d9af80465e4383162e4a7c5e2f406e82dd968 to c13d28517a4db259d738ea4886b1f00352a3cc33.\u00a0 \u00a0 PiperOrigin-RevId: 444703993\r\n```\r\n\r\n\r\nReverting that fixes the issue.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\n\r\nbazel --bazelrc='.\/tensorflow\/tools\/ci_build\/osx\/arm64\/.macos.bazelrc'  test \/\/tensorflow\/tools\/proto_splitter\/cc:saved_model_splitter_test\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nThe test is failing with:\r\n\r\nINFO: From Testing \/\/tensorflow\/tools\/proto_splitter\/cc:saved_model_splitter_test:==================== Test output for \/\/tensorflow\/tools\/proto_splitter\/cc:saved_model_splitter_test:dyld[1906]: symbol not found in flat namespace \r\n\r\n'__ZNK11highwayhash11HighwayHashILj16EEclERA4_KyPKcmPy'==================================================================================================== \r\n\r\nTest output for \/\/tensorflow\/tools\/proto_splitter\/cc:saved_model_splitter_test:dyld[1942]: symbol not found in flat namespace '__ZNK11highwayhash11HighwayHashILj16EEclERA4_KyPKcmPy'==================================================================================================== Test output for \/\/tensorflow\/tools\/proto_splitter\/cc:saved_model_splitter_test:dyld[1976]: symbol not found in flat namespace '__ZNK11highwayhash11HighwayHashILj16EEclERA4_KyPKcmPy'================================================================================\r\n```\r\n```\r\n\r\n\r\ncc @learning-to-play , @nitins17 and @mihaimaruseac \r\n","labels":["stat:awaiting tensorflower","type:bug","subtype:macOS","TF 2.13"],"created_at":"2023-09-01T22:27:27Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61774"},{"issue_number":303,"repository":"tensorflow\/tensorflow","title":"Full integer quantization not possible with grouped convolution","description":"### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 12.5\r\n- TensorFlow installation (pip package or built from source): pip\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13\r\n\r\n### 2. Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n```\r\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\r\n  data_dir,\r\n  validation_split=0.2,\r\n  subset=\"training\",\r\n  seed=123,\r\n  image_size=(img_height, img_width),\r\n  batch_size=batch_size)\r\n\r\ndef representative_data_gen():\r\n  for input_value, labels in train_ds:\r\n    yield [input_value]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model('.\/model.pb')\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_data_gen\r\n\r\ntflite_model = converter.convert()\r\n\r\nwith open('model.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\nThe error that I get is \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[15], line 8\r\n      3 #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]\r\n      4 #converter.inference_input_type = tf.int8\r\n      5 #converter.inference_output_type = tf.int8\r\n      6 converter.representative_dataset = representative_data_gen\r\n----> 8 tflite_model = converter.convert()\r\n     10 with open('model.tflite', 'wb') as f:\r\n     11     f.write(tflite_model)\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py:1065, in _export_metrics.<locals>.wrapper(self, *args, **kwargs)\r\n   1062 @functools.wraps(convert_func)\r\n   1063 def wrapper(self, *args, **kwargs):\r\n   1064   # pylint: disable=protected-access\r\n-> 1065   return self._convert_and_export_metrics(convert_func, *args, **kwargs)\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py:1042, in TFLiteConverterBase._convert_and_export_metrics(self, convert_func, *args, **kwargs)\r\n   1040 self._save_conversion_params_metric()\r\n   1041 start_time = time.process_time()\r\n-> 1042 result = convert_func(self, *args, **kwargs)\r\n   1043 elapsed_time_ms = (time.process_time() - start_time) * 1000\r\n   1044 if result:\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py:1390, in TFLiteSavedModelConverterV2.convert(self)\r\n   1384 else:\r\n   1385   self._debug_info = _get_debug_info(\r\n   1386       _convert_debug_info_func(self._trackable_obj.graph_debug_info),\r\n   1387       graph_def,\r\n   1388   )\r\n-> 1390 return self._convert_from_saved_model(graph_def)\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py:1257, in TFLiteConverterBaseV2._convert_from_saved_model(self, graph_def)\r\n   1254 converter_kwargs.update(quant_mode.converter_flags())\r\n   1256 result = _convert_saved_model(**converter_kwargs)\r\n-> 1257 return self._optimize_tflite_model(\r\n   1258     result, quant_mode, quant_io=self.experimental_new_quantizer\r\n   1259 )\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/convert_phase.py:215, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)\r\n    213 except Exception as error:\r\n    214   report_error_message(str(error))\r\n--> 215   raise error from None\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/convert_phase.py:205, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)\r\n    202 @functools.wraps(func)\r\n    203 def wrapper(*args, **kwargs):\r\n    204   try:\r\n--> 205     return func(*args, **kwargs)\r\n    206   except ConverterError as converter_error:\r\n    207     if converter_error.errors:\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py:991, in TFLiteConverterBase._optimize_tflite_model(self, model, quant_mode, quant_io)\r\n    989   q_allow_float = quant_mode.is_allow_float()\r\n    990   q_variable_quantization = quant_mode.enable_mlir_variable_quantization\r\n--> 991   model = self._quantize(\r\n    992       model,\r\n    993       q_in_type,\r\n    994       q_out_type,\r\n    995       q_activations_type,\r\n    996       q_bias_type,\r\n    997       q_allow_float,\r\n    998       q_variable_quantization,\r\n    999   )\r\n   1001 m_in_type = in_type if in_type else _dtypes.float32\r\n   1002 m_out_type = out_type if out_type else _dtypes.float32\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py:710, in TFLiteConverterBase._quantize(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)\r\n    706 calibrate_quantize = _calibrator.Calibrator(\r\n    707     result, custom_op_registerers_by_name, custom_op_registerers_by_func\r\n    708 )\r\n    709 if self._experimental_calibrate_only or self.experimental_new_quantizer:\r\n--> 710   calibrated = calibrate_quantize.calibrate(\r\n    711       self.representative_dataset.input_gen\r\n    712   )\r\n    714 if self._experimental_calibrate_only:\r\n    715   return calibrated\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/convert_phase.py:215, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)\r\n    213 except Exception as error:\r\n    214   report_error_message(str(error))\r\n--> 215   raise error from None\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/convert_phase.py:205, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)\r\n    202 @functools.wraps(func)\r\n    203 def wrapper(*args, **kwargs):\r\n    204   try:\r\n--> 205     return func(*args, **kwargs)\r\n    206   except ConverterError as converter_error:\r\n    207     if converter_error.errors:\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/optimize\/calibrator.py:254, in Calibrator.calibrate(self, dataset_gen)\r\n    244 @convert_phase(Component.OPTIMIZE_TFLITE_MODEL, SubComponent.CALIBRATE)\r\n    245 def calibrate(self, dataset_gen):\r\n    246   \"\"\"Calibrates the model with specified generator.\r\n    247 \r\n    248   Returns:\r\n   (...)\r\n    252     dataset_gen: A generator that generates calibration samples.\r\n    253   \"\"\"\r\n--> 254   self._feed_tensors(dataset_gen, resize_input=True)\r\n    255   return self._calibrator.Calibrate()\r\n\r\nFile ~\/miniforge3\/envs\/tf213\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/optimize\/calibrator.py:143, in Calibrator._feed_tensors(self, dataset_gen, resize_input)\r\n    139     self._calibrator.Prepare(\r\n    140         [list(s.shape) for s in input_array], signature_key\r\n    141     )\r\n    142   else:\r\n--> 143     self._calibrator.Prepare([list(s.shape) for s in input_array])\r\n    144 else:\r\n    145   if signature_key is not None:\r\n\r\nRuntimeError: tensorflow\/lite\/kernels\/conv.cc:352 input_channel % filter_input_channel != 0 (2 != 0)Node number 6 (CONV_2D) failed to prepare.\r\n```\r\n\r\nThe model itself got converted from pytorch to onnx and then to pb. The issue here is that the model has a grouped convolutional layers. This is fixed for dynamic range quantisation but for full integer quantisation using a representative dataset this still seems to fail. Any quick fix possible?\r\n","labels":["stat:awaiting tensorflower","type:bug","TFLiteConverter","ModelOptimizationToolkit","TF 2.13"],"created_at":"2023-08-31T08:14:44Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61760"},{"issue_number":304,"repository":"tensorflow\/tensorflow","title":"tf.data.Dataset.save deadlocks on error ","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.13.0-rc2-7-g1cb1a030a62 2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.17\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen saving a dataset to disk, TF will silently brick if there's an error in the data input pipeline. Methinks it should rather raise on the save instead of deadlocking.\r\n\r\nHere's a reproducible example:\r\n\r\n```python\r\nimport tensorflow as tf\r\n# '2.13.0'\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])\r\ndataset = dataset.map(lambda x: tf.debugging.check_numerics(1. \/ x, \"error\"))\r\ndataset.save('\/tmp\/hello')\r\n# => deadlock\r\n```\r\n\r\nYou can add an `ignore_errors` to the pipeline and it'll rightfully ignore, but it's nonintuitive to track down why the input bricks. \r\n\r\n```python\r\ndataset = dataset.ignore_errors()\r\n# => OK save\r\n```\n\n### Standalone code to reproduce the issue\n\n```shell\nSee above.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.13"],"created_at":"2023-08-28T22:43:16Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61736"},{"issue_number":305,"repository":"tensorflow\/tensorflow","title":"ExtensionType does not allow creating tf.function methods","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.13.0, tf 2.12.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nColab, Mac\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11.3\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen calling a method on ExtensionType, it seems that `self` is flattened, making it incompatible with `tf.function` and `input_signature`. Is this the intended behavior?\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nColab: https:\/\/colab.research.google.com\/drive\/19b65SIbsUgV4CKGX0DGgS0IcbXeOsOw9?usp=sharing\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\nprint(\"tf.__version__:\", tf.__version__)\r\n\r\nclass A(tf.experimental.ExtensionType):\r\n    x1: tf.Tensor\r\n    x2: tf.Tensor\r\n\r\n_a = A.Spec(tf.TensorSpec((5, 6)), tf.TensorSpec((5, 6)))\r\n\r\nclass B(tf.experimental.ExtensionType):\r\n    y1: tf.Tensor\r\n    y2: tf.Tensor\r\n    y3: int\r\n\r\n_b = B.Spec(tf.TensorSpec((7, 8)), tf.TensorSpec((7, 8)), 9)\r\n\r\nclass C(tf.experimental.ExtensionType):\r\n    a1: A\r\n    a2: A\r\n    a3: A\r\n    a4: A\r\n\r\n    @tf.function(input_signature=[_b])\r\n    def f(self, b: B):\r\n        return b\r\n\r\nx = tf.zeros((5, 6))\r\ny = tf.zeros((7, 8))\r\na = A(x, x)\r\nb = B(y, y, 9)\r\nc = C(a, a, a, a)\r\nc.f(b)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nColab:\r\n```\r\ntf.__version__: 2.12.0\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-6a3d1772c2f1> in <cell line: 33>()\r\n     31 b = B(y, y, 9)\r\n     32 c = C(a, a, a, a)\r\n---> 33 c.f(b)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/polymorphic_function\/function_spec.py in cast_inputs_to_signature(inputs, input_signature)\r\n    538         check_types=False)  # lists are convert to tuples for `tf.data`.\r\n    539   except ValueError:\r\n--> 540     raise ValueError(\"Structure of Python function inputs does not match \"\r\n    541                      \"input_signature:\\n\"\r\n    542                      f\"{format_error_message(inputs, input_signature)}.\")\r\n\r\nValueError: Structure of Python function inputs does not match input_signature:\r\n  inputs: (\r\n    C(a1=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=\r\narray([[0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=\r\narray([[0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.]], dtype=float32)>), a2=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=\r\narray([[0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=\r\narray([[0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.]], dtype=float32)>), a3=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=\r\narray([[0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=\r\narray([[0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.]], dtype=float32)>), a4=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=\r\narray([[0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=\r\narray([[0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0.]], dtype=float32)>)),\r\n    B(y1=<tf.Tensor: shape=(7, 8), dtype=float32, numpy=\r\narray([[0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, y2=<tf.Tensor: shape=(7, 8), dtype=float32, numpy=\r\narray([[0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.],\r\n       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, y3=9))\r\n  input_signature: (\r\n    B.Spec(y1=TensorSpec(shape=(7, 8), dtype=tf.float32, name=None), y2=TensorSpec(shape=(7, 8), dtype=tf.float32, name=None), y3=9)).\r\n```\r\n\r\nMac:\r\n```shell\r\ntf.__version__: 2.13.0\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/a.py\", line 31, in <module>\r\n    c.f(b)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/ml\/lib\/python3.11\/site-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/ml\/lib\/python3.11\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/atomic_function.py\", line 181, in __call__\r\n    raise ValueError(\r\nValueError: Signature specifies 4 arguments, got: 10.\r\n```\r\n\r\nMac (tf_nightly-2.15.0.dev20230827-cp311-cp311-macosx_12_0_arm64):\r\n```shell\r\ntf.__version__: 2.15.0-dev20230827\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/a.py\", line 33, in <module>\r\n    c.f(b)\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/ml\/lib\/python3.11\/site-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/opt\/homebrew\/anaconda3\/envs\/ml\/lib\/python3.11\/site-packages\/tensorflow\/core\/function\/polymorphism\/function_type.py\", line 391, in unpack_inputs\r\n    p.type_constraint._to_tensors(bound_parameters.arguments[p.name])  # pylint: disable=protected-access\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nKeyError: 'y1'\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:tf.function","TF 2.13"],"created_at":"2023-08-27T12:47:18Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61712"},{"issue_number":306,"repository":"tensorflow\/tensorflow","title":"Null pointer exception in constant folding of grappler.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.13.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nSegmentation fault.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n\r\n[NPE.zip](https:\/\/github.com\/tensorflow\/tensorflow\/files\/12444694\/NPE.zip)\r\n\r\nRun the poc.py in the zip.\r\nIn constant folding, after `FoldNode`, fold nodes will be updated with the new name at \u30101\u3011. But later, the old node name will still be referenced in `MergeConcat` (\u30102\u3011) with `GetNode`(\u30103\u3011), leading to an NPE.\r\n\r\n\r\n\r\n```C++\r\n\/\/tensorflow\/core\/grappler\/optimizers\/constant_folding.cc\r\nStatus ConstantFolding::FoldNode(NodeDef* node, GraphDef* output_graph,\r\n                                 bool* result_too_large) {\r\n    ...\r\n    else if (port < static_cast<int>(const_nodes.size()) &&\r\n               !const_nodes[port].name().empty()) {\r\n          \/\/\u30101\u3011\r\n          node_map_->UpdateInput(output->name(), NodeName(output->input(i)),\r\n                                   const_nodes[port].name());\r\n          *output->mutable_input(i) = const_nodes[port].name();\r\n } \r\n\r\nbool ConstantFolding::MergeConcat(bool use_shape_info,\r\n                                  GraphProperties* properties,\r\n                                  GraphDef* optimized_graph, NodeDef* node) {\r\n   \u2026\u2026\r\n   for (int i = 0; i < num_regular_inputs - 1; ++i) {\r\n       \/\/\u30102\u3011\r\n       const NodeDef* input_node = node_map_->GetNode(node->input(i));\r\n       if (!IsReallyConstant(*input_node)) {\u2026\u2026}\r\n}\r\n\r\n\/\/tensorflow\/core\/grappler\/utils.h\r\nNodeDefT* GetNode(const string& name) const {\r\n    const string node_name = NodeName(name);\r\n    auto it = nodes_.find(node_name);\r\n    if (it == nodes_.end()) {\r\n      VLOG(1) << \"Node could not be found: \" << name;\r\n      return nullptr; \/\/\u30103\u3011\r\n    }\r\n    return it->second;\r\n }\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:grappler","TF 2.13"],"created_at":"2023-08-26T06:08:07Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61708"},{"issue_number":307,"repository":"tensorflow\/tensorflow","title":"LLVM conflict with libosmesa6","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nDebian Bullseye 11.7\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorflow 2.13.0 binary causes some conflict with libOSMesa which uses LLVM-11. Tensorflow 2.7.1 didn't have this problem.\r\n\r\nlibosmesa6 (20.3.5-1) is installed through apt-get\r\nhttps:\/\/packages.debian.org\/bullseye\/libosmesa6\r\n\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/4276548\/d7a109e1-f545-4692-a9e9-3088dbb1ed87)\r\n\r\n\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\n#include <stdio.h>\r\n#include <tensorflow\/c\/c_api.h>\r\n\r\n#include <dlfcn.h>\r\n\r\nint main() {\r\n  printf(\"Hello from TensorFlow C library version %s\\n\", TF_Version());\r\n\r\n  dlopen(\"libOSMesa.so.6\", RTLD_NOW);\r\n\r\n  return 0;\r\n}\n```\n\n\n### Relevant log output\n\n```shell\ngcc hello_tf.c -ltensorflow -ldl -o hello_tf\r\n\r\n.\/hello_tf\r\n2023-08-25 13:50:27.935004: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\nHello from TensorFlow C library version 2.13.0\r\n: CommandLine Error: Option 'debug-counter' registered more than once!\r\nLLVM ERROR: inconsistency in registered CommandLine options\r\nAborted\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:runtime","TF 2.13"],"created_at":"2023-08-25T12:00:28Z","comments":0,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61696"},{"issue_number":308,"repository":"tensorflow\/tensorflow","title":"tf.linalg.pinv isn't ","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.6.22\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nPinv should give similar gradient to inv. Here's the code snippet and result.\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/10631563\/de9cdb82-1c93-4a18-a79e-95c0b0834ae6)\r\n\r\n\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/10631563\/cfdadbb9-5506-4181-a868-69830c5ee96b)\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nSee figures in behavior section.\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-08-23T13:32:23Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61675"},{"issue_number":309,"repository":"tensorflow\/tensorflow","title":"oneDNN matmul is not supported to have a dynamic dimension","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\nv1.12.1-98433-gb64e3d4ae2e 2.15.0-dev20230812\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Debian 6.3.11\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nThe following code works for nightly version 2.15.0-dev20230811, but fails for the version 2.15.0-dev20230812. The issue happens when we compile the function.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow.compat.v2 as tf\r\n\r\nclass _Slice2Idx:\r\n  \"\"\"Utility to convert numpy basic slices into TF scatter_nd indices.\"\"\"\r\n\r\n  def __init__(self, tensor):\r\n    self.ranges = [tf.range(d, dtype=tf.int32) for d in tensor.shape]\r\n\r\n  def __getitem__(self, slices):\r\n    grid = tf.meshgrid(*(rng[sl] for rng, sl in zip(self.ranges, slices)), indexing='ij')\r\n    return tf.stack(grid, axis=-1)\r\n\r\ndef no_pivot_ldl(matrix, name='no_pivot_ldl'):\r\n  with tf.name_scope(name) as name:\r\n    matrix = tf.convert_to_tensor(matrix)\r\n    triangular_factor = tf.linalg.band_part(matrix, num_lower=-1, num_upper=0)\r\n    slix = _Slice2Idx(triangular_factor)\r\n\r\n    def fn(triangular_factor, i):\r\n      column_tail = triangular_factor[..., i+1:, i]\r\n      x = tf.einsum('...i,...j->...ij', column_tail, column_tail)\r\n      idx = slix[i+1:, i+1:]\r\n      triangular_factor = tf.tensor_scatter_nd_update(triangular_factor, idx, x)\r\n      return triangular_factor\r\n\r\n    triangular_factor = tf.foldl(\r\n        fn=fn,\r\n        elems=tf.range(tf.shape(triangular_factor)[-1]),\r\n        initializer=triangular_factor)\r\n    return triangular_factor\r\n\r\ninp = tf.Variable([[2., 1.], [1., 2.]])\r\nalt_chol_jit = tf.function(no_pivot_ldl, autograph=False, jit_compile=True)\r\nprint(no_pivot_ldl(inp))\r\nalt_chol_jit(inp)\r\n```\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\ntf.Tensor(\r\n[[2. 0.]\r\n [1. 1.]], shape=(2, 2), dtype=float32)\r\n2023-08-23 11:31:54.059605: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x5db9ec0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2023-08-23 11:31:54.059681: I tensorflow\/compiler\/xla\/service\/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\n2023-08-23 11:31:54.068228: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2023-08-23 11:31:54.093913: W tensorflow\/core\/framework\/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:625 : UNIMPLEMENTED: CustomCall \"__onednn$matmul\" is not supported to have a dynamic dimension\r\n\r\ntensorflow.python.framework.errors_impl.UnimplementedError: CustomCall \"__onednn$matmul\" is not supported to have a dynamic dimension [Op:__inference_no_pivot_ldl_264]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","comp:core"],"created_at":"2023-08-23T11:39:06Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61674"},{"issue_number":310,"repository":"tensorflow\/tensorflow","title":" Test_on_batch() gives the same loss output on different batches in a single run","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n\r\n### TensorFlow version\r\n\r\ntf 2.10.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nwindows 10\r\n\r\n### Python version\r\n\r\n3.10.12\r\n\r\n### Current behavior?\r\n\r\nI noticed the problem when I got a straight horizontal line on plotting the test results on the trained network. I used the sequential models.\r\n\r\nI use train_on_batch(), which gives converging losses. When I switch to test_on_batch(), the losses remain the same for different batches. When I restart the test with different test files, it will give a different loss value, which remains the same for all the batches. In other words, the loss from test_on_batch() remains the some for all batches in a single run.\r\n\r\nIt's a sequential model.\r\n\r\nHere is the code of the section:\r\n\r\n    print('mfccs3 value = ', tf.keras.backend.eval(mfccs3[1,:]) )               \r\n    #logs = vadModel.train_on_batch(mfccs3, vadLabel)\r\n    logs = vadModel.test_on_batch(mfccs3, vadLabel) \r\n    print('string logs = ', str(logs))\r\nThe result is:\r\nindex = 1\r\n\r\n```\r\nmfccs3 value = [[-8.2793800e+01 -5.9538417e+00 9.8302096e-01 -3.5255635e-01\r\n3.0392697e-01 -6.4597696e-01 2.2358397e-02 2.5344249e-02\r\n-6.8171650e-01 -3.7053981e-01 -3.4044239e-01 -8.1056818e-02]]\r\n```\r\n\r\nstring logs = 0.2398043\r\n\r\nindex = 2\r\n\r\n```\r\nmfccs3 value = [[-69.159195 -2.2269542 4.2501264 -1.3486748 0.62957734\r\n-3.2606528 -3.253118 -3.5308673 -1.1313365 -1.1839466\r\n-2.330786 -1.6313086 ]]\r\n```\r\n\r\nstring logs = 0.2398043\r\n\r\nindex = 3\r\n\r\n```\r\nmfccs3 value = [[-64.894104 -1.892648 0.11392474 -0.81098145 -1.4640433\r\n-1.1901256 -1.7744782 -0.85753983 -0.9694403 -0.8149232\r\n-1.0680746 -1.0442001 ]]\r\n```\r\n\r\nstring logs = 0.2398043\r\n\r\nYou can see that the inputs for test_on_batch() have changed. However, the loss remains the same. I use the same code for train_on_batch(), which gives converging losses.\r\n\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nlogs = vadModel.train_on_batch(mfccs3, vadLabel)\r\n\"\"\" vs.  \"\"\"\r\n    logs = vadModel.test_on_batch(mfccs3, vadLabel) \r\n\r\nit's just these two lines for a sequential model.\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nI even tried latest Tensorflow version. It has the same problem.\r\n\r\ntensorflow version: 2.15.0-dev20230817\r\nlistOfFiles 1681\r\n2023-08-17 15:42:12.560549: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nmodel length =  7\r\nModel: \"sequential\"\r\n_________________________________________________________________\r\n Layer (type)                Output Shape              Param #   \r\n=================================================================\r\n lstm (LSTM)                 multiple                  2640      \r\n                                                                 \r\n dense (Dense)               multiple                  210       \r\n                                                                 \r\n dense_1 (Dense)             multiple                  11        \r\n                                                                 \r\n=================================================================\r\nTotal params: 2861 (11.18 KB)\r\nTrainable params: 2861 (11.18 KB)\r\nNon-trainable params: 0 (0.00 Byte)\r\n_________________________________________________________________\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF2.14"],"created_at":"2023-08-17T23:10:29Z","comments":18,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61616"},{"issue_number":311,"repository":"tensorflow\/tensorflow","title":"Build: Protobuf fails with \"File already exists in database\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.13.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n5.1.1\n\n### GCC\/compiler version\n\n11.3\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUsing the TF_SYSTEMLIBS version of protobuf (i.e. a preinstalled protobuf) when building TensorFlow from source results in\r\n\r\n```\r\n[libprotobuf ERROR \/build\/protobuf\/3.19.4\/GCCcore-11.3.0\/protobuf-3.19.4\/src\/google\/protobuf\/descriptor_database.cc:641] File already exists in database: tensorflow\/dtensor\/proto\/layout.proto\r\n[libprotobuf FATAL \/build\/protobuf\/3.19.4\/GCCcore-11.3.0\/protobuf-3.19.4\/src\/google\/protobuf\/descriptor.cc:2021] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\nterminate called after throwing an instance of 'google::protobuf::FatalException'\r\n  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): \r\n```\r\n\r\nI can't make much sense out of that failure and Google yielded results related to having the protobuf file in loaded in shared libraries multiple times.\r\n\r\nAll I could do is indeed trace it to `from tensorflow.dtensor.proto import layout_pb2` in tensorflow\/dtensor\/python\/layout.py\n\n### Standalone code to reproduce the issue\n\n```shell\nThe failing build step invokes `\/bin\/bash -c 'bazel-out\/ppc-opt\/bin\/tensorflow\/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow\/api_template.__init__.py --apidir=bazel-out\/ppc-opt\/bin\/tensorflow\/_api\/v2\/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow\/compat_template_v1.__init__.py --compat_init_template=tensorflow\/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=default bazel-out\/ppc-opt\/bin\/tensorflow\/tf_python_api_gen_v2.params'`\n```\n\n\n### Relevant log output\n\n```shell\n# Configuration: 020ca55738349851eb8a5a672fc7b8d08dc15ebd99b324f52fddbad2f32820b8\r\n# Execution platform: @local_execution_config_platform\/\/:platform\r\nERROR: \/build\/TensorFlow\/tensorflow-2.13.0\/tensorflow\/BUILD:1646:19: Action tensorflow\/_api\/v2\/v2.py failed: (Aborted): bash failed: error executing command \r\n  (cd \/build\/TensorFlow\/bazel-root\/663b1bf019e1a9ec9827eae691fce071\/execroot\/org_tensorflow && \\\r\n  exec env - \\\r\n    CPATH=\/sw\/installed\/cURL\/7.83.0-GCCcore-11.3.0\/include:\/software\/double-conversion\/3.2.0-GCCcore-11.3.0\/include:\/software\/flatbuffers\/2.0.7-GCCcore-11.3.0\/include:\/sw\/installed\/giflib\/5.2.1-GCCcore-11.3.0\/include:\/sw\/installed\/hwloc\/2.7.1-GCCcore-11.3.0\/include:\/sw\/installed\/ICU\/71.1-GCCcore-11.3.0\/include:\/software\/JsonCpp\/1.9.5-GCCcore-11.3.0\/include:\/sw\/installed\/libjpeg-turbo\/2.1.3-GCCcore-11.3.0\/include:\/sw\/installed\/libpng\/1.6.37-GCCcore-11.3.0\/include:\/software\/nsync\/1.25.0-GCCcore-11.3.0\/include:\/software\/protobuf\/3.19.4-GCCcore-11.3.0\/include:\/sw\/installed\/pybind11\/2.9.2-GCCcore-11.3.0\/include:\/software\/snappy\/1.1.9-GCCcore-11.3.0\/include:\/sw\/installed\/SQLite\/3.38.3-GCCcore-11.3.0\/include:\/sw\/installed\/zlib\/1.2.12-GCCcore-11.3.0\/include:\/sw\/installed\/OpenSSL\/1.1\/include \\\r\n    LD_LIBRARY_PATH=\/software\/RE2\/2022-06-01-GCCcore-11.3.0\/lib:\/software\/snappy\/1.1.9-GCCcore-11.3.0\/lib:\/sw\/installed\/libpng\/1.6.37-GCCcore-11.3.0\/lib:\/software\/nsync\/1.25.0-GCCcore-11.3.0\/lib:\/sw\/installed\/libjpeg-turbo\/2.1.3-GCCcore-11.3.0\/lib:\/software\/JsonCpp\/1.9.5-GCCcore-11.3.0\/lib:\/sw\/installed\/ICU\/71.1-GCCcore-11.3.0\/lib:\/sw\/installed\/giflib\/5.2.1-GCCcore-11.3.0\/lib:\/software\/flatbuffers\/2.0.7-GCCcore-11.3.0\/lib:\/software\/double-conversion\/3.2.0-GCCcore-11.3.0\/lib:\/sw\/installed\/HDF5\/1.12.2-gompi-2022a\/lib:\/sw\/installed\/Szip\/2.1.1-GCCcore-11.3.0\/lib:\/sw\/installed\/SciPy-bundle\/2022.05-foss-2022a\/lib\/python3.10\/site-packages\/numpy\/core\/lib:\/sw\/installed\/ScaLAPACK\/2.2.0-gompi-2022a-fb\/lib:\/sw\/installed\/FFTW.MPI\/3.3.10-gompi-2022a\/lib:\/sw\/installed\/FFTW\/3.3.10-GCC-11.3.0\/lib:\/sw\/installed\/FlexiBLAS\/3.2.0-GCC-11.3.0\/lib:\/sw\/installed\/OpenBLAS\/0.3.20-GCC-11.3.0\/lib:\/sw\/installed\/OpenMPI\/4.1.4-GCC-11.3.0\/lib:\/sw\/installed\/UCC\/1.0.0-GCCcore-11.3.0\/lib:\/sw\/installed\/PMIx\/4.1.2-GCCcore-11.3.0\/lib:\/sw\/installed\/libfabric\/1.15.1-GCCcore-11.3.0\/lib:\/sw\/installed\/UCX\/1.12.1-GCCcore-11.3.0\/lib:\/sw\/installed\/libevent\/2.1.12-GCCcore-11.3.0\/lib:\/sw\/installed\/hwloc\/2.7.1-GCCcore-11.3.0\/lib:\/sw\/installed\/libpciaccess\/0.16-GCCcore-11.3.0\/lib:\/sw\/installed\/numactl\/2.0.14-GCCcore-11.3.0\/lib:\/sw\/installed\/Python\/3.10.4-GCCcore-11.3.0\/lib:\/sw\/installed\/libffi\/3.4.2-GCCcore-11.3.0\/lib64:\/sw\/installed\/GMP\/6.2.1-GCCcore-11.3.0\/lib:\/sw\/installed\/SQLite\/3.38.3-GCCcore-11.3.0\/lib:\/sw\/installed\/Tcl\/8.6.12-GCCcore-11.3.0\/lib:\/sw\/installed\/bzip2\/1.0.8-GCCcore-11.3.0\/lib:\/sw\/installed\/binutils\/2.38-GCCcore-11.3.0\/lib:\/sw\/installed\/DB\/18.1.40-GCCcore-11.3.0\/lib:\/sw\/installed\/libreadline\/8.1.2-GCCcore-11.3.0\/lib:\/sw\/installed\/gettext\/0.21-GCCcore-11.3.0\/lib:\/sw\/installed\/ncurses\/6.3-GCCcore-11.3.0\/lib:\/sw\/installed\/libxml2\/2.9.13-GCCcore-11.3.0\/lib:\/sw\/installed\/XZ\/5.2.5-GCCcore-11.3.0\/lib:\/sw\/installed\/expat\/2.4.8-GCCcore-11.3.0\/lib:\/sw\/installed\/cURL\/7.83.0-GCCcore-11.3.0\/lib:\/sw\/installed\/OpenSSL\/1.1\/lib:\/sw\/installed\/zlib\/1.2.12-GCCcore-11.3.0\/lib:\/software\/protobuf\/3.19.4-GCCcore-11.3.0\/lib:\/sw\/installed\/Java\/11.0.6-ppc64le\/lib:\/sw\/installed\/GCCcore\/11.3.0\/lib64:\/usr\/local\/cuda\/lib64 \\\r\n    LIBRARY_PATH=\/sw\/installed\/cURL\/7.83.0-GCCcore-11.3.0\/lib:\/software\/double-conversion\/3.2.0-GCCcore-11.3.0\/lib:\/software\/flatbuffers\/2.0.7-GCCcore-11.3.0\/lib:\/sw\/installed\/giflib\/5.2.1-GCCcore-11.3.0\/lib:\/sw\/installed\/hwloc\/2.7.1-GCCcore-11.3.0\/lib:\/sw\/installed\/ICU\/71.1-GCCcore-11.3.0\/lib:\/software\/JsonCpp\/1.9.5-GCCcore-11.3.0\/lib:\/sw\/installed\/libjpeg-turbo\/2.1.3-GCCcore-11.3.0\/lib:\/sw\/installed\/libpng\/1.6.37-GCCcore-11.3.0\/lib:\/software\/nsync\/1.25.0-GCCcore-11.3.0\/lib:\/software\/protobuf\/3.19.4-GCCcore-11.3.0\/lib:\/sw\/installed\/pybind11\/2.9.2-GCCcore-11.3.0\/lib:\/software\/snappy\/1.1.9-GCCcore-11.3.0\/lib:\/sw\/installed\/SQLite\/3.38.3-GCCcore-11.3.0\/lib:\/sw\/installed\/zlib\/1.2.12-GCCcore-11.3.0\/lib:\/sw\/installed\/OpenSSL\/1.1\/lib \\\r\n    PATH=\/sw\/installed\/libpng\/1.6.37-GCCcore-11.3.0\/bin:\/sw\/installed\/libjpeg-turbo\/2.1.3-GCCcore-11.3.0\/bin:\/sw\/installed\/NASM\/2.15.05-GCCcore-11.3.0\/bin:\/sw\/installed\/ICU\/71.1-GCCcore-11.3.0\/sbin:\/sw\/installed\/ICU\/71.1-GCCcore-11.3.0\/bin:\/sw\/installed\/giflib\/5.2.1-GCCcore-11.3.0\/bin:\/software\/flatbuffers\/2.0.7-GCCcore-11.3.0\/bin:\/software\/dill\/0.3.6-GCCcore-11.3.0\/bin:\/sw\/installed\/HDF5\/1.12.2-gompi-2022a\/bin:\/sw\/installed\/SciPy-bundle\/2022.05-foss-2022a\/bin:\/sw\/installed\/FFTW\/3.3.10-GCC-11.3.0\/bin:\/sw\/installed\/FlexiBLAS\/3.2.0-GCC-11.3.0\/bin:\/sw\/installed\/OpenMPI\/4.1.4-GCC-11.3.0\/bin:\/sw\/installed\/UCC\/1.0.0-GCCcore-11.3.0\/bin:\/sw\/installed\/PMIx\/4.1.2-GCCcore-11.3.0\/bin:\/sw\/installed\/libfabric\/1.15.1-GCCcore-11.3.0\/bin:\/sw\/installed\/UCX\/1.12.1-GCCcore-11.3.0\/bin:\/sw\/installed\/libevent\/2.1.12-GCCcore-11.3.0\/bin:\/sw\/installed\/hwloc\/2.7.1-GCCcore-11.3.0\/bin:\/sw\/installed\/numactl\/2.0.14-GCCcore-11.3.0\/bin:\/sw\/installed\/UnZip\/6.0-GCCcore-11.3.0\/bin:\/sw\/installed\/pybind11\/2.9.2-GCCcore-11.3.0\/bin:\/sw\/installed\/Python\/3.10.4-GCCcore-11.3.0\/bin:\/sw\/installed\/SQLite\/3.38.3-GCCcore-11.3.0\/bin:\/sw\/installed\/Tcl\/8.6.12-GCCcore-11.3.0\/bin:\/sw\/installed\/bzip2\/1.0.8-GCCcore-11.3.0\/bin:\/sw\/installed\/binutils\/2.38-GCCcore-11.3.0\/bin:\/sw\/installed\/git\/2.36.0-GCCcore-11.3.0-nodocs\/bin:\/sw\/installed\/Perl\/5.34.1-GCCcore-11.3.0\/bin:\/sw\/installed\/DB\/18.1.40-GCCcore-11.3.0\/bin:\/sw\/installed\/gettext\/0.21-GCCcore-11.3.0\/bin:\/sw\/installed\/ncurses\/6.3-GCCcore-11.3.0\/bin:\/sw\/installed\/libxml2\/2.9.13-GCCcore-11.3.0\/bin:\/sw\/installed\/XZ\/5.2.5-GCCcore-11.3.0\/bin:\/sw\/installed\/expat\/2.4.8-GCCcore-11.3.0\/bin:\/sw\/installed\/cURL\/7.83.0-GCCcore-11.3.0\/bin:\/sw\/installed\/OpenSSL\/1.1\/bin:\/software\/protobuf\/3.19.4-GCCcore-11.3.0\/bin:\/software\/Bazel\/5.1.1-GCCcore-11.3.0\/bin:\/sw\/installed\/Java\/11.0.6-ppc64le:\/sw\/installed\/Java\/11.0.6-ppc64le\/bin:\/sw\/installed\/GCCcore\/11.3.0\/bin:\/home\/s3248973\/.local\/EasyBuildDev\/easybuild-framework:\/home\/s3248973\/.yarn\/bin:\/home\/s3248973\/.config\/yarn\/global\/node_modules\/.bin:\/home\/s3248973\/.local\/bin:\/usr\/local\/cuda\/bin:\/usr\/lib64\/qt-3.3\/bin:\/sw\/taurus\/tools\/slurmtools\/default\/bin:\/usr\/local\/bin:\/usr\/bin:\/usr\/local\/sbin:\/usr\/sbin:\/opt\/ibutils\/bin:\/opt\/puppetlabs\/bin \\\r\n    PYTHONNOUSERSITE=1 \\\r\n    PYTHONPATH=\/software\/TensorFlow\/2.13.0-foss-2022a\/lib\/python3.10\/site-packages:\/software\/TensorFlow\/2.13.0-foss-2022a\/lib\/python3.10\/site-packages:\/software\/protobuf-python\/3.19.4-GCCcore-11.3.0\/lib\/python3.10\/site-packages:\/software\/flatbuffers\/2.0.7-GCCcore-11.3.0\/lib\/python3.10\/site-packages:\/software\/dill\/0.3.6-GCCcore-11.3.0\/lib\/python3.10\/site-packages:\/software\/h5py\/3.7.0-foss-2022a\/lib\/python3.10\/site-packages:\/sw\/installed\/SciPy-bundle\/2022.05-foss-2022a\/lib\/python3.10\/site-packages:\/sw\/installed\/pybind11\/2.9.2-GCCcore-11.3.0\/lib\/python3.10\/site-packages:\/sw\/installed\/Python\/3.10.4-GCCcore-11.3.0\/easybuild\/python \\\r\n    PYTHON_BIN_PATH=\/sw\/installed\/Python\/3.10.4-GCCcore-11.3.0\/bin\/python \\\r\n    PYTHON_LIB_PATH=\/software\/TensorFlow\/2.13.0-foss-2022a\/lib\/python3.10\/site-packages \\\r\n    TF2_BEHAVIOR=1 \\\r\n    TF_SYSTEM_LIBS=absl_py,astor_archive,astunparse_archive,boringssl,com_google_protobuf,curl,cython,dill_archive,double_conversion,flatbuffers,functools32_archive,gast_archive,gif,hwloc,icu,jsoncpp_git,libjpeg_turbo,nasm,nsync,opt_einsum_archive,org_sqlite,pasta,png,pybind11,six_archive,snappy,tblib_archive,termcolor_archive,typing_extensions_archive,wrapt,zlib \\\r\n  \/bin\/bash -c 'bazel-out\/ppc-opt\/bin\/tensorflow\/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow\/api_template.__init__.py --apidir=bazel-out\/ppc-opt\/bin\/tensorflow\/_api\/v2\/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow\/compat_template_v1.__init__.py --compat_init_template=tensorflow\/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=default bazel-out\/ppc-opt\/bin\/tensorflow\/tf_python_api_gen_v2.params')\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","TF 2.13"],"created_at":"2023-08-16T14:24:31Z","comments":3,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61593"},{"issue_number":312,"repository":"tensorflow\/tensorflow","title":"Segmentation fault when running tensorflow.python.framework.kernels.get_registered_kernels_for_op","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.11.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nnvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nProbably due to feeding None argument\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport os\r\nimport numpy as np\r\nfrom tensorflow.python.framework import kernels\r\ntry:\r\n  arg_0 = None\r\n  out = kernels.get_registered_kernels_for_op(arg_0,)\r\nexcept Exception as e:\r\n  print(\"Error:\"+str(e))\r\n\r\n```\n```\n\n\n### Relevant log output\n\n```shell\n2023-08-12 13:41:10.388491: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\nSegmentation fault\r\n\r\n```\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-08-12T17:41:26Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61531"},{"issue_number":313,"repository":"tensorflow\/tensorflow","title":"quantized range of fake_quant_with_min_max_args is -2**num_bits + 1 to 2 ** num_bits.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.13\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nubuntu 22.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI implemented `fake_quant_with_min_max_args` in numpy according to the source code to get the quantized values. However the quantized range is `-2**num_bits + 1` to `2 ** num_bits` and the quantized weights cannot be represented by `num_bits` int.  What should I do if I have to use signed type to represent the weights?\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\ndef fake_quant_with_min_max_args(inputs, min=-0.99, max=0.99, num_bits=8, narrow_range=False):\r\n    assert min < 0 < max\r\n    quant_min = 1 if narrow_range else 0\r\n    quant_max = (1 << num_bits) - 1\r\n    quant_min_float = np.float32(quant_min)\r\n    quant_max_float = np.float32(quant_max)\r\n    scale = np.float32(max - min) \/ (quant_max_float - quant_min_float)\r\n    inv_scale = (quant_max_float - quant_min_float) \/ np.float32(max - min)\r\n\r\n    zero_point_from_min = quant_min - min \/ scale\r\n    if zero_point_from_min < quant_min:\r\n        nudged_zero_point = quant_min\r\n    elif zero_point_from_min > quant_max:\r\n        nudged_zero_point = quant_max\r\n    else:\r\n        nudged_zero_point = np.round(zero_point_from_min)\r\n\r\n    nudged_min = (quant_min_float - nudged_zero_point) * scale\r\n    nudged_max = (quant_max_float - nudged_zero_point) * scale\r\n\r\n    quant_zero = np.floor(-nudged_min * inv_scale + 0.5)\r\n    # print(quant_min, quant_max, nudged_zero_point)\r\n    # print(nudged_min, nudged_max, scale, inv_scale, quant_zero)\r\n    clamped = np.clip(inputs, nudged_min, nudged_max)\r\n    clamp_shifted = clamped - nudged_min\r\n    # quant = np.clip(np.floor(clamp_shifted * inv_scale - quant_zero + 0.5),\r\n    #                 quant_min - 2 ** (num_bits - 1), quant_max - 2 ** (num_bits - 1))\r\n    quant = np.floor(clamp_shifted * inv_scale - quant_zero + 0.5)\r\n    dequant = quant * scale\r\n    return quant, dequant\r\n\r\n\r\nif __name__ == '__main__':\r\n    import numpy as np\r\n    import tensorflow as tf\r\n    np.random.seed(12345)\r\n    data = np.random.uniform(-1, 1, (2,))\r\n    data = np.r_[1, -1, data]\r\n\r\n    d = fake_quant_with_min_max_args(data, -1, 1, 8, False)\r\n    print(d[0])\r\n    print(d[1])\r\n    print(tf.quantization.fake_quant_with_min_max_args(data, -1, 1, 8, False))\r\n```\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-08-12T09:04:02Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61529"},{"issue_number":314,"repository":"tensorflow\/tensorflow","title":"Incorrect tf.nn.max_pool2d outputs for NCHW and NHWC in the same thread.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.13\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 22.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nThere is an incorrect output of tf.nn.max_pool2d when calling with the input of the same batch size, height, width and depth first in NCHW, then in NHWC format with MKL support. \\\r\nOutput of tf.nn.max_pool2d on the same input (except data format) must be the same (except data format).\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nColab to reproduce https:\/\/colab.research.google.com\/drive\/1Vuo7txDPDq-YXAuCFQcPNowHgOwbT7Oi?usp=sharing\r\n\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:mkl","TF 2.13"],"created_at":"2023-08-04T22:24:18Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61482"},{"issue_number":315,"repository":"tensorflow\/tensorflow","title":"Tensorflow 2.13.0 cannot be imported after install via poetry due to missing wheel metadata","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.13.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nmacOS-13.3-arm64-arm-64bit\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nAfter installing tensorflow  2.13.0 with poetry it cannot be imported\r\n\r\n### Standalone code to reproduce the issue\r\nSee https:\/\/github.com\/python-poetry\/poetry\/issues\/8271\r\n\r\nAccording to @dimbleby, the issue happens due to missing wheel metadata.\r\n\r\n> please encourage the tensorflow folk to publish consistent metadata in all of their wheels, with platform-specific variations described by markers\r\n\r\nTensorflow folks, please publish consistent metadata ;)\r\n\r\n```shell\r\npoetry add tensorflow==2.13.0\r\npoetry shell\r\npython3 \r\nimport tensorflow\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nModuleNotFoundError: No module named 'tensorflow\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype:macOS","TF 2.13"],"created_at":"2023-08-04T15:23:34Z","comments":6,"reactions":8,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61477"},{"issue_number":316,"repository":"tensorflow\/tensorflow","title":"How\/Where decision is made to choose between eager execution & graph execution for Tensorflow kernel OPs","description":"I ran HuggingFace BERT model which uses tensorflow 2.13v with oneDNN support on intel machine and recorded its execution logs by setting TF_CPP_MAX_VLOG_LEVEL=2 & ONEDNN_VERBOSE=1 in file.\r\n\r\n**Observation :** I have observing logs that are produced after model creation and its weight loading. Since model.fit() always run in graph model, all tensorflow kernel OPs (onednn's mkl kernel op and non-mkl kernel ops) should run in graph mode. But i observe only for non-mkl kernel ops (like ADDV2, Mul) are executing in eager mode followed by graph mode. I dont see any mkl kernel ops(like _MklMatMul) running in eager mode.\r\n\r\n**Questions:** I want to know the reason and file where decision making is made for which op there should be eager mode. Since model.fit() runs in graph mode, why I am seeing eager mode execution for all non-mkl ops?\r\n\r\nSample Logs for model.fit() for ADDV2 kernel op:\r\n\r\n```\r\n2023-07-31 03:48:44.632289: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1678] Executing op AddV2 in device \/job:localhost\/replica:0\/task:0\/device:CPU:0 --> executing addv2 eagerly After some other logs in between, I see below log:\r\n\r\n2023-07-31 03:50:01.968512: I tensorflow\/core\/common_runtime\/executor.cc:841] Process node: 8127 step -4458402160563696089 {{node tf_bert_for_sequence_classification\/bert\/encoder\/layer_._0\/output\/LayerNorm\/batchnorm\/add_1}} = AddV2[T=DT_FLOAT, _XlaHasReferenceVars=false, device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](tf_bert_for_sequence_classification\/bert\/encoder\/layer.0\/output\/LayerNorm\/batchnorm\/mul_1, tf_bert_for_sequence_classification\/bert\/encoder\/layer._0\/output\/LayerNorm\/batchnorm\/sub) device: \r\n```\r\n\/job:localhost\/replica:0\/task:0\/device:CPU:0 --> executing addv2 in graph mode i assume\r\n\r\n**Expected to happen:** All kerenl ops should execute in graph mode.\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:core","TF 2.13"],"created_at":"2023-08-04T12:25:08Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61476"},{"issue_number":317,"repository":"tensorflow\/tensorflow","title":"BUG: reference count leak in function `RegisterForwardAccumulatorCleanup` (static analyzer report)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ncommit faad219fc46032a0ae9576ccc3076612cc1f5f72\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\nOur static analyzer uses Clang 13 as its parser\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/53fb0130851ad40d544105432f414b4ebe9e729d\/tensorflow\/python\/eager\/pywrap_tfe_src.cc#L2378-L2379\r\n\r\nAPI `PyCFunction_New` does not steal a reference for the second argument.\r\nAPI `PyLong_FromLong` will return a new reference.\r\nCalling `PyLong_FromLong` directly as the second argument of `PyCFunction_New` will lead to a reference count leak for the PyObject returned by `PyLong_FromLong`.\r\n\r\nInternal report ID: c13984\n\n### Standalone code to reproduce the issue\n\n```shell\nUnnecessary. Whenever this function is called, the problem will be triggered.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:eager"],"created_at":"2023-08-03T08:05:54Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61462"},{"issue_number":318,"repository":"tensorflow\/tensorflow","title":"tf.compat.v1.train.MonitoredTrainingSession failed to restore checkpoint_dir variables from s3","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.7.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.7.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n11.2\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n[TOC]\r\n\r\nOur project uses tf.compat.v1.train.MonitoredTrainingSession to create a training session. Typically, we need to restore a pre-trained model from S3.\r\n\r\n## 1. Error encountered in my project\r\nBefore switching to TensorFlow 1, we used TensorFlow 1.15.1 and passed the S3 path to `checkpoint_dir` like this:\r\n```python\r\nimport tensorflow as tf\r\n.....\r\ncheckpoint_dir = \"s3:\/\/xxx\/xx\/\"\r\ntf.compat.v1.train.MonitoredTrainingSession(...., checkpoint_dir=checkpoint_dir, ...)\r\n```\r\n`checkpoint_dir` contains everything needed to restore variables, including checkpoint, graph.pbtxt, etc. Everything works fine.\r\n\r\nAfter switching to TensorFlow 2.7.0, we realized that the Modular File System has been introduced into TensorFlow. So, we installed TensorFlow-io version 0.23.0, which is compatible with TensorFlow 2.7.0. The code becomes:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_io as tfio\r\n.....\r\ncheckpoint_dir = \"s3:\/\/xxx\/xx\/\"\r\ntf.compat.v1.train.MonitoredTrainingSession(...., checkpoint_dir=checkpoint_dir, ...)\r\n```\r\nHowever, it no longer works, and an error is reported:\r\n```\r\n.....\r\n2023-08-02 16:02:40.147093: W tensorflow\/core\/framework\/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:207 : DATA_LOSS: truncated block read\r\nTraceback (most recent call last):\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/client\/session.py\", line 1380, in _do_call\r\n    return fn(*args)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/client\/session.py\", line 1364, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/client\/session.py\", line 1458, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.\r\n  (0) DATA_LOSS: truncated block read\r\n         [[{{node save\/RestoreV2}}]]\r\n         [[save\/RestoreV2\/_1]]\r\n  (1) DATA_LOSS: truncated block read\r\n         [[{{node save\/RestoreV2}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n.....\r\n```\r\n\r\n## 2. Reproduce the issue using simple code\r\nTo rule out the possibility that the issue is caused by the complexity of the model in my project, I reproduced it using a very simple code.\r\n### 2.1 Step 1: Train the model\r\nFirst, I used the following code to train a very simple model and save it in a local directory:\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.compat.v1.disable_eager_execution()\r\nx = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=\"x\")\r\ny = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=\"y\")\r\n\r\nW = tf.Variable(tf.zeros([1, 1]), name=\"W\")\r\nb = tf.Variable(tf.zeros([1]), name=\"b\")\r\n\r\ny_pred = tf.matmul(x, W) + b\r\nloss = tf.reduce_mean(tf.square(y - y_pred))\r\n\r\noptimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01)\r\n\r\nglobal_step = tf.compat.v1.train.get_or_create_global_step()\r\ntrain_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\nx_train = [[1], [2], [3], [4]]\r\ny_train = [[0], [-1], [-2], [-3]]\r\n\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nhooks = [tf.compat.v1.train.StopAtStepHook(last_step=500)]\r\n\r\ncheckpoint_dir = '.\/checkpoints'\r\n\r\nwith tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,\r\n                                                 config=config,\r\n                                                 hooks=hooks) as sess:\r\n    while not sess.should_stop():\r\n        sess.run(train_op, feed_dict={x: x_train, y: y_train})\r\n```\r\n### 2.2 Step 2: Upload the model to S3\r\nThen, I used S3 tools to upload all materials in `.\/checkpoints` to a remote S3 path:\r\n```\r\ns3cmd put .\/checkpoints\/ s3:\/\/xxxx\/xxx\/checkpoints\/\r\n```\r\n### 2.3 Step 3: Restore the model from S3 (error)\r\nFinally, I restored the model training using the following code, and an error was reported:\r\n```python\r\nimport tensorflow as tf\r\nimport tensorflow_io as tfio\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nx = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=\"x\")\r\ny = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=\"y\")\r\n\r\nW = tf.Variable(tf.zeros([1, 1]), name=\"W\")\r\nb = tf.Variable(tf.zeros([1]), name=\"b\")\r\n\r\ny_pred = tf.matmul(x, W) + b\r\nloss = tf.reduce_mean(tf.square(y - y_pred))\r\n\r\noptimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01)\r\n\r\nglobal_step = tf.compat.v1.train.get_or_create_global_step()\r\n\r\ntrain_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\nx_train = [[1], [2], [3], [4]]\r\ny_train = [[0], [-1], [-2], [-3]]\r\n\r\nconfig = tf.compat.v1.ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\n\r\ncheckpoint_dir = 's3:\/\/xxxx\/xxx\/checkpoints\/'\r\n\r\nhooks = [tf.compat.v1.train.StopAtStepHook(last_step=2000)]\r\nwith tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:\r\n    while not sess.should_stop():\r\n        sess.run(train_op, feed_dict={x: x_train, y: y_train})\r\n```\r\nThe full log is shown below:\r\n```\r\n\r\nWARNING:tensorflow:From \/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/training_util.py:401: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\r\n2023-08-02 16:43:08.483327: I tensorflow\/core\/platform\/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-08-02 16:43:09.090602: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1525] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 38415 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:0e:00.0, compute capability: 8.0\r\n2023-08-02 16:43:09.854875: W tensorflow\/core\/framework\/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:207 : DATA_LOSS: truncated block read\r\nTraceback (most recent call last):\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/client\/session.py\", line 1380, in _do_call\r\n    return fn(*args)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/client\/session.py\", line 1364, in _run_fn\r\n    target_list, run_metadata)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/client\/session.py\", line 1458, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.\r\n  (0) DATA_LOSS: truncated block read\r\n         [[{{node save\/RestoreV2}}]]\r\n         [[save\/RestoreV2\/_1]]\r\n  (1) DATA_LOSS: truncated block read\r\n         [[{{node save\/RestoreV2}}]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"train_s3.py\", line 36, in <module>\r\n    with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 616, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 1062, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 761, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 1267, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 1272, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 914, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 681, in create_session\r\n    init_fn=self._scaffold.init_fn)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/session_manager.py\", line 321, in prepare_session\r\n    config=config)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/session_manager.py\", line 251, in _restore_checkpoint\r\n    sess, saver, ckpt.model_checkpoint_path)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/session_manager.py\", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers\r\n    saver.restore(sess, path)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/saver.py\", line 1405, in restore\r\n    {self.saver_def.filename_tensor_name: save_path})\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/client\/session.py\", line 971, in run\r\n    run_metadata_ptr)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/client\/session.py\", line 1194, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/client\/session.py\", line 1374, in _do_run\r\n    run_metadata)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/client\/session.py\", line 1399, in _do_call\r\n    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\r\ntensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.\r\n  (0) DATA_LOSS: truncated block read\r\n         [[node save\/RestoreV2\r\n (defined at train_s3.py:36)\r\n]]\r\n         [[save\/RestoreV2\/_1]]\r\n  (1) DATA_LOSS: truncated block read\r\n         [[node save\/RestoreV2\r\n (defined at train_s3.py:36)\r\n]]\r\n0 successful operations.\r\n0 derived errors ignored.\r\n\r\nErrors may have originated from an input operation.\r\nInput Source operations connected to node save\/RestoreV2:\r\nIn[0] save\/Const:\r\nIn[1] save\/RestoreV2\/tensor_names:\r\nIn[2] save\/RestoreV2\/shape_and_slices:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"train_s3.py\", line 36, in <module>\r\n>>>     with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:\r\n>>> \r\n\r\nInput Source operations connected to node save\/RestoreV2:\r\nIn[0] save\/Const:\r\nIn[1] save\/RestoreV2\/tensor_names:\r\nIn[2] save\/RestoreV2\/shape_and_slices:\r\n\r\nOperation defined at: (most recent call last)\r\n>>>   File \"train_s3.py\", line 36, in <module>\r\n>>>     with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:\r\n>>> \r\n\r\nOriginal stack trace for 'save\/RestoreV2':\r\n  File \"train_s3.py\", line 36, in <module>\r\n    with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 616, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 1062, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 761, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 1267, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 1272, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 914, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 672, in create_session\r\n    self._scaffold.finalize()\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/monitored_session.py\", line 236, in finalize\r\n    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/saver.py\", line 625, in _get_saver_or_default\r\n    saver = Saver(sharded=True, allow_empty=True)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/saver.py\", line 923, in __init__\r\n    self.build()\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/saver.py\", line 935, in build\r\n    self._build(self._filename, build_save=True, build_restore=True)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/saver.py\", line 973, in _build\r\n    build_restore=build_restore)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/saver.py\", line 528, in _build_internal\r\n    restore_sequentially, reshape)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/saver.py\", line 407, in _AddShardedRestoreOps\r\n    name=\"restore_shard\"))\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/saver.py\", line 354, in _AddRestoreOps\r\n    restore_sequentially)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/training\/saver.py\", line 601, in bulk_restore\r\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/ops\/gen_io_ops.py\", line 1504, in restore_v2\r\n    name=name)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/framework\/op_def_library.py\", line 746, in _apply_op_helper\r\n    attrs=attr_protos, op_def=op_def)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 3705, in _create_op_internal\r\n    op_def=op_def)\r\n  File \"\/root\/miniconda3\/lib\/python3.7\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 2101, in __init__\r\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\r\n\r\n```\r\n\r\n## 3. Test tf.io and s3 connectivity\r\nI also use the following code to test if tf.io can access s3\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_io as tfio\r\ns3_path = \"s3:\/\/xxxxx\/xxx\/checkpoints\/checkpoint\"\r\nret = tf.io.read_file(s3_path)\r\nprint(ret)\r\n```\r\nAnd it works fine:\r\n```\r\n2023-08-02 16:48:17.619754: I tensorflow\/core\/platform\/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-08-02 16:48:18.226059: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1525] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 38415 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:0e:00.0, compute capability: 8.0\r\ntf.Tensor(b'model_checkpoint_path: \"model.ckpt-1000\"\\nall_model_checkpoint_paths: \"model.ckpt-0\"\\nall_model_checkpoint_paths: \"model.ckpt-500\"\\nall_model_checkpoint_paths: \"model.ckpt-1000\"\\n', shape=(), dtype=string)\r\n```\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nsee above\n```\n\n\n### Relevant log output\n\n```shell\nsee above\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:core","TF 2.13"],"created_at":"2023-08-02T09:01:20Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61449"},{"issue_number":319,"repository":"tensorflow\/tensorflow","title":"Unexpected differences in outputs of Conv2D copy with exact subset of weights","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.13.0-rc2-7-g1cb1a030a62 2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMacOS 13.4.1 (c) (22F770820d)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.6\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nGiven a `Conv2D` layer that has some channel's weights completely as zeros (e.g. as a result of structured pruning), when creating a new layer from the previous one with those channels missing, and calling both layers on some input, while the relevant parts of weights are exactly the same, that's not the case for the layers' outputs.\r\n\r\nI would expect them to be exactly the same as well, but get differences up to `~1.7e-7`.\r\n\r\n\r\nNote, I've also observed this behavior for Dense layers.\r\n\r\nMy best guess is that a different mechanism is used under the hood that produces slightly different results?\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom itertools import combinations\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n\r\ndef reduce_to_relevant_part(array: np.ndarray, channel_indices: list[int], channel_axis: int = -1):\r\n    if channel_indices is not None:\r\n        array = np.take(\r\n            array,\r\n            [i for i in range(array.shape[channel_axis]) if i not in channel_indices],\r\n            axis=channel_axis,\r\n        )\r\n    return array\r\n\r\n\r\ndef get_all_combinations(n: int) -> list[list[int]]:\r\n    result = []\r\n    for i in range(n):\r\n        for subset in combinations(range(n), i):\r\n            result.append(list(subset))\r\n    return result\r\n\r\n\r\ndef print_summary_statistics(array: np.ndarray):\r\n    print(\"mean: {}, std: {}, max: {}\".format(np.mean(array), np.std(array), np.max(array)))\r\n\r\n\r\ndef get_layer(n_filters, kernel_size, input_shape):\r\n    layer = tf.keras.layers.Conv2D(\r\n        filters=n_filters,\r\n        kernel_size=kernel_size,\r\n        padding='same',\r\n        use_bias=False,\r\n        kernel_initializer='he_normal',\r\n        strides=kernel_size,\r\n    )\r\n    layer.build(input_shape)\r\n    return layer\r\n\r\n\r\ndef set_channel_weights_to_zero(layer, channel_indices):\r\n    weights = layer.get_weights()\r\n    weights[0][..., channel_indices] = 0\r\n    layer.set_weights(weights)\r\n    return layer\r\n\r\n\r\ndef get_copy_of_layer_without_zero_channels(layer, channel_indices):\r\n    config = layer.get_config()\r\n    config[\"filters\"] -= len(channel_indices)\r\n    weights = layer.get_weights()\r\n    weights[0] = reduce_to_relevant_part(weights[0], channel_indices)\r\n    config[\"weights\"] = weights\r\n    new_layer = tf.keras.layers.Conv2D.from_config(config)\r\n    return new_layer\r\n\r\n\r\ndef main():\r\n    for n_filters in range(2, 9):\r\n        kernel_size = (4, 4)\r\n        input_shape = (1, 8, 12, 1)\r\n        for channel_indices in get_all_combinations(n_filters):\r\n            layer = get_layer(n_filters, kernel_size, input_shape)\r\n            layer = set_channel_weights_to_zero(layer, channel_indices)\r\n            new_layer = get_copy_of_layer_without_zero_channels(layer, channel_indices)\r\n\r\n            x = tf.random.uniform(input_shape)\r\n            output = layer(x).numpy()\r\n            output_subset = reduce_to_relevant_part(output, channel_indices)\r\n            new_output = new_layer(x).numpy()\r\n            weights_subset = reduce_to_relevant_part(layer.get_weights()[0], channel_indices)\r\n            new_weights = new_layer.get_weights()[0]\r\n            if not np.array_equal(weights_subset, new_weights):\r\n                raise ValueError()  # never triggered\r\n            if not np.array_equal(output_subset, new_output):\r\n                print(\r\n                    \"Outputs differ for channel indices {} @ {} filters\".format(\r\n                        channel_indices, n_filters\r\n                    )\r\n                )\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n\r\nFind below the output of the above script. Note that for 3 filters for example, `[2]` and `[0, 1]` do not show up, because in those cases the outputs actually are exactly the same.\n```\n\n\n### Relevant log output\n\n```shell\nOutputs differ for channel indices [0] @ 2 filters\r\nOutputs differ for channel indices [1] @ 2 filters\r\nOutputs differ for channel indices [0] @ 3 filters\r\nOutputs differ for channel indices [1] @ 3 filters\r\nOutputs differ for channel indices [0, 2] @ 3 filters\r\nOutputs differ for channel indices [1, 2] @ 3 filters\r\nOutputs differ for channel indices [0] @ 4 filters\r\nOutputs differ for channel indices [1] @ 4 filters\r\nOutputs differ for channel indices [2] @ 4 filters\r\nOutputs differ for channel indices [3] @ 4 filters\r\nOutputs differ for channel indices [0, 1, 2] @ 4 filters\r\nOutputs differ for channel indices [0, 1, 3] @ 4 filters\r\nOutputs differ for channel indices [0, 2, 3] @ 4 filters\r\nOutputs differ for channel indices [1, 2, 3] @ 4 filters\r\nOutputs differ for channel indices [0] @ 5 filters\r\nOutputs differ for channel indices [1] @ 5 filters\r\nOutputs differ for channel indices [2] @ 5 filters\r\nOutputs differ for channel indices [3] @ 5 filters\r\nOutputs differ for channel indices [0, 4] @ 5 filters\r\nOutputs differ for channel indices [1, 4] @ 5 filters\r\nOutputs differ for channel indices [2, 4] @ 5 filters\r\nOutputs differ for channel indices [3, 4] @ 5 filters\r\nOutputs differ for channel indices [0, 1, 2] @ 5 filters\r\nOutputs differ for channel indices [0, 1, 3] @ 5 filters\r\nOutputs differ for channel indices [0, 2, 3] @ 5 filters\r\nOutputs differ for channel indices [1, 2, 3] @ 5 filters\r\nOutputs differ for channel indices [0, 1, 2, 4] @ 5 filters\r\nOutputs differ for channel indices [0, 1, 3, 4] @ 5 filters\r\nOutputs differ for channel indices [0, 2, 3, 4] @ 5 filters\r\nOutputs differ for channel indices [1, 2, 3, 4] @ 5 filters\r\nOutputs differ for channel indices [0] @ 6 filters\r\nOutputs differ for channel indices [1] @ 6 filters\r\nOutputs differ for channel indices [2] @ 6 filters\r\nOutputs differ for channel indices [3] @ 6 filters\r\nOutputs differ for channel indices [4] @ 6 filters\r\nOutputs differ for channel indices [5] @ 6 filters\r\nOutputs differ for channel indices [0, 1, 2] @ 6 filters\r\nOutputs differ for channel indices [0, 1, 3] @ 6 filters\r\nOutputs differ for channel indices [0, 1, 4] @ 6 filters\r\nOutputs differ for channel indices [0, 1, 5] @ 6 filters\r\nOutputs differ for channel indices [0, 2, 3] @ 6 filters\r\nOutputs differ for channel indices [0, 2, 4] @ 6 filters\r\nOutputs differ for channel indices [0, 2, 5] @ 6 filters\r\nOutputs differ for channel indices [0, 3, 4] @ 6 filters\r\nOutputs differ for channel indices [0, 3, 5] @ 6 filters\r\nOutputs differ for channel indices [0, 4, 5] @ 6 filters\r\nOutputs differ for channel indices [1, 2, 3] @ 6 filters\r\nOutputs differ for channel indices [1, 2, 4] @ 6 filters\r\nOutputs differ for channel indices [1, 2, 5] @ 6 filters\r\nOutputs differ for channel indices [1, 3, 4] @ 6 filters\r\nOutputs differ for channel indices [1, 3, 5] @ 6 filters\r\nOutputs differ for channel indices [1, 4, 5] @ 6 filters\r\nOutputs differ for channel indices [2, 3, 4] @ 6 filters\r\nOutputs differ for channel indices [2, 3, 5] @ 6 filters\r\nOutputs differ for channel indices [2, 4, 5] @ 6 filters\r\nOutputs differ for channel indices [3, 4, 5] @ 6 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 4] @ 6 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 5] @ 6 filters\r\nOutputs differ for channel indices [0, 1, 2, 4, 5] @ 6 filters\r\nOutputs differ for channel indices [0, 1, 3, 4, 5] @ 6 filters\r\nOutputs differ for channel indices [0, 2, 3, 4, 5] @ 6 filters\r\nOutputs differ for channel indices [1, 2, 3, 4, 5] @ 6 filters\r\nOutputs differ for channel indices [0] @ 7 filters\r\nOutputs differ for channel indices [1] @ 7 filters\r\nOutputs differ for channel indices [2] @ 7 filters\r\nOutputs differ for channel indices [3] @ 7 filters\r\nOutputs differ for channel indices [4] @ 7 filters\r\nOutputs differ for channel indices [5] @ 7 filters\r\nOutputs differ for channel indices [0, 6] @ 7 filters\r\nOutputs differ for channel indices [1, 6] @ 7 filters\r\nOutputs differ for channel indices [2, 6] @ 7 filters\r\nOutputs differ for channel indices [3, 6] @ 7 filters\r\nOutputs differ for channel indices [4, 6] @ 7 filters\r\nOutputs differ for channel indices [5, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 2] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 3] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 4] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 5] @ 7 filters\r\nOutputs differ for channel indices [0, 2, 3] @ 7 filters\r\nOutputs differ for channel indices [0, 2, 4] @ 7 filters\r\nOutputs differ for channel indices [0, 2, 5] @ 7 filters\r\nOutputs differ for channel indices [0, 3, 4] @ 7 filters\r\nOutputs differ for channel indices [0, 3, 5] @ 7 filters\r\nOutputs differ for channel indices [0, 4, 5] @ 7 filters\r\nOutputs differ for channel indices [1, 2, 3] @ 7 filters\r\nOutputs differ for channel indices [1, 2, 4] @ 7 filters\r\nOutputs differ for channel indices [1, 2, 5] @ 7 filters\r\nOutputs differ for channel indices [1, 3, 4] @ 7 filters\r\nOutputs differ for channel indices [1, 3, 5] @ 7 filters\r\nOutputs differ for channel indices [1, 4, 5] @ 7 filters\r\nOutputs differ for channel indices [2, 3, 4] @ 7 filters\r\nOutputs differ for channel indices [2, 3, 5] @ 7 filters\r\nOutputs differ for channel indices [2, 4, 5] @ 7 filters\r\nOutputs differ for channel indices [3, 4, 5] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 2, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 3, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 4, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 2, 3, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 2, 4, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 2, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 3, 4, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 3, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 4, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [1, 2, 3, 6] @ 7 filters\r\nOutputs differ for channel indices [1, 2, 4, 6] @ 7 filters\r\nOutputs differ for channel indices [1, 2, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [1, 3, 4, 6] @ 7 filters\r\nOutputs differ for channel indices [1, 3, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [1, 4, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [2, 3, 4, 6] @ 7 filters\r\nOutputs differ for channel indices [2, 3, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [2, 4, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [3, 4, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 4] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 5] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 2, 4, 5] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 3, 4, 5] @ 7 filters\r\nOutputs differ for channel indices [0, 2, 3, 4, 5] @ 7 filters\r\nOutputs differ for channel indices [1, 2, 3, 4, 5] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 4, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 2, 4, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 1, 3, 4, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [0, 2, 3, 4, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [1, 2, 3, 4, 5, 6] @ 7 filters\r\nOutputs differ for channel indices [0] @ 8 filters\r\nOutputs differ for channel indices [1] @ 8 filters\r\nOutputs differ for channel indices [2] @ 8 filters\r\nOutputs differ for channel indices [3] @ 8 filters\r\nOutputs differ for channel indices [4] @ 8 filters\r\nOutputs differ for channel indices [5] @ 8 filters\r\nOutputs differ for channel indices [6] @ 8 filters\r\nOutputs differ for channel indices [7] @ 8 filters\r\nOutputs differ for channel indices [0, 1] @ 8 filters\r\nOutputs differ for channel indices [0, 2] @ 8 filters\r\nOutputs differ for channel indices [0, 3] @ 8 filters\r\nOutputs differ for channel indices [0, 4] @ 8 filters\r\nOutputs differ for channel indices [0, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2] @ 8 filters\r\nOutputs differ for channel indices [1, 3] @ 8 filters\r\nOutputs differ for channel indices [1, 4] @ 8 filters\r\nOutputs differ for channel indices [1, 5] @ 8 filters\r\nOutputs differ for channel indices [1, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 3] @ 8 filters\r\nOutputs differ for channel indices [2, 4] @ 8 filters\r\nOutputs differ for channel indices [2, 5] @ 8 filters\r\nOutputs differ for channel indices [2, 6] @ 8 filters\r\nOutputs differ for channel indices [2, 7] @ 8 filters\r\nOutputs differ for channel indices [3, 4] @ 8 filters\r\nOutputs differ for channel indices [3, 5] @ 8 filters\r\nOutputs differ for channel indices [3, 6] @ 8 filters\r\nOutputs differ for channel indices [3, 7] @ 8 filters\r\nOutputs differ for channel indices [4, 5] @ 8 filters\r\nOutputs differ for channel indices [4, 6] @ 8 filters\r\nOutputs differ for channel indices [4, 7] @ 8 filters\r\nOutputs differ for channel indices [5, 6] @ 8 filters\r\nOutputs differ for channel indices [5, 7] @ 8 filters\r\nOutputs differ for channel indices [6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 4] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 4] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 4] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 4] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 5] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 4] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 5] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [1, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 4] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 5] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 6] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [2, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [2, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [2, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [3, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [3, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [3, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [3, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [3, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [3, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 4] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 4] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 4] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 4] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 5] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [2, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [3, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [3, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [3, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [3, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 4] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [3, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 4, 5] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 4, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 4, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 3, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 3, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [2, 3, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 4, 5, 6] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 4, 5, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 4, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 3, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 2, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 1, 3, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [0, 2, 3, 4, 5, 6, 7] @ 8 filters\r\nOutputs differ for channel indices [1, 2, 3, 4, 5, 6, 7] @ 8 filters\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","comp:ops","TF 2.13"],"created_at":"2023-07-28T13:02:04Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61422"},{"issue_number":320,"repository":"tensorflow\/tensorflow","title":"tf.debugging.experimental.enable_dump_debug_info (Debugger V2) error with TPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.12 (but also 2.14 nightly)\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nGoogle Colab\n\n### Mobile device\n\nno\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nN\/A (using TPU)\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI'm trying to use the `tf.debugging.experimental.enable_dump_debug_info(...)` function with TPU.\r\n\r\nI've tested my code without the TPU strategy bit, and it works, I can use the Debugger V2 fine.\r\n\r\nI've also tested the code without the debugger bit and it trains fine as well.\r\n\r\nBut together it gives me this error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/content\/tbscript.py\", line 47, in <module>\r\n    model = train()\r\n  File \"\/content\/tbscript.py\", line 40, in train\r\n    model.fit(x=x_train, \r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 70, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/debug\/lib\/dumping_callback.py\", line 579, in <listcomp>\r\n    output_tensor_device_ids = [writer.RegisterDeviceAndGetId(output.device)\r\nValueError: Cannot assign a device for operation IteratorGetNextAsOptional: Could not satisfy explicit device specification '' because the node {{colocation_node IteratorGetNextAsOptional}} was colocated with a group of nodes that required incompatible device '\/job:worker\/replica:0\/task:0\/device:TPU:0'. All available devices [\/job:worker\/replica:0\/task:0\/device:CPU:0, \/job:worker\/replica:0\/task:0\/device:TPU:0, \/job:worker\/replica:0\/task:0\/device:TPU:1, \/job:worker\/replica:0\/task:0\/device:TPU:2, \/job:worker\/replica:0\/task:0\/device:TPU:3, \/job:worker\/replica:0\/task:0\/device:TPU:4, \/job:worker\/replica:0\/task:0\/device:TPU:5, \/job:worker\/replica:0\/task:0\/device:TPU:6, \/job:worker\/replica:0\/task:0\/device:TPU:7, \/job:worker\/replica:0\/task:0\/device:TPU_SYSTEM:0, \/job:worker\/replica:0\/task:0\/device:XLA_CPU:0, \/job:localhost\/replica:0\/task:0\/device:CPU:0, \/job:localhost\/replica:0\/task:0\/device:COMPOSITE:0]. \r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=2 requested_device_name_='\/job:worker\/replica:0\/task:0\/device:TPU:0' assigned_device_name_='\/job:worker\/replica:0\/task:0\/device:TPU:0' resource_device_name_='\/job:worker\/replica:0\/task:0\/device:TPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nOptionalGetValue: CPU TPU XLA_CPU \r\nDebugNumericSummaryV2: CPU \r\nIteratorGetNext: CPU TPU XLA_CPU \r\nIdentity: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE \r\nSwitch: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE \r\nIteratorGetNextAsOptional: CPU TPU XLA_CPU \r\nDebugIdentityV2: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE \r\nOptionalHasValue: CPU TPU XLA_CPU \r\n_Arg: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE \r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  iterator_1 (_Arg)  framework assigned device=\/job:worker\/replica:0\/task:0\/device:TPU:0\r\n  IteratorGetNextAsOptional (IteratorGetNextAsOptional) \r\n  OptionalHasValue (OptionalHasValue) \r\n  cond\/IteratorGetNextAsOptional\/_5 (Switch) \r\n  cond\/iterator_1\/_13 (Switch) \r\n  Func\/cond\/then\/_0\/input\/_39 (Identity) \r\n  Func\/cond\/then\/_0\/input\/_47 (Identity) \r\n  cond\/then\/_0\/cond\/OptionalHasValue (OptionalHasValue) \r\n  Func\/cond\/else\/_1\/input\/_73 (Identity) \r\n  Func\/cond\/else\/_1\/input\/_81 (Identity) \r\n  cond\/else\/_1\/cond\/IteratorGetNext (IteratorGetNext) \r\n  cond\/else\/_1\/cond\/IteratorGetNext\/DebugNumericSummaryV2 (DebugNumericSummaryV2) \r\n  cond\/else\/_1\/cond\/IteratorGetNext\/DebugIdentityV2_1511 (DebugIdentityV2) \r\n  cond\/then\/_0\/cond\/cond\/Func\/cond\/then\/_0\/input\/_39\/_111 (Switch) \r\n  Func\/cond\/then\/_0\/cond\/cond\/then\/_106\/input\/_179 (Identity) \r\n  cond\/then\/_0\/cond\/cond\/then\/_106\/cond\/cond\/OptionalGetValue (OptionalGetValue) \/job:worker\/replica:0\/task:0\/device:TPU:0\r\n  cond\/then\/_0\/cond\/cond\/then\/_106\/cond\/cond\/OptionalGetValue\/DebugNumericSummaryV2 (DebugNumericSummaryV2) \/job:worker\/replica:0\/task:0\/device:TPU:0\r\n  cond\/then\/_0\/cond\/cond\/then\/_106\/cond\/cond\/OptionalGetValue\/DebugIdentityV2_1369 (DebugIdentityV2) \/job:worker\/replica:0\/task:0\/device:TPU:0\r\n  Func\/cond\/then\/_0\/cond\/cond\/else\/_107\/input\/_184 (Identity) \r\n\r\n\t [[{{node IteratorGetNex ... [truncated]\r\nException ignored in atexit callback: <function async_wait at 0x7ecd97fc5d80>\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/context.py\", line 2796, in async_wait\r\n    context().sync_executors()\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/context.py\", line 742, in sync_executors\r\n    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation IteratorGetNextAsOptional: Could not satisfy explicit device specification '' because the node {{colocation_node IteratorGetNextAsOptional}} was colocated with a group of nodes that required incompatible device '\/job:worker\/replica:0\/task:0\/device:TPU:0'. All available devices [\/job:worker\/replica:0\/task:0\/device:CPU:0, \/job:worker\/replica:0\/task:0\/device:TPU:0, \/job:worker\/replica:0\/task:0\/device:TPU:1, \/job:worker\/replica:0\/task:0\/device:TPU:2, \/job:worker\/replica:0\/task:0\/device:TPU:3, \/job:worker\/replica:0\/task:0\/device:TPU:4, \/job:worker\/replica:0\/task:0\/device:TPU:5, \/job:worker\/replica:0\/task:0\/device:TPU:6, \/job:worker\/replica:0\/task:0\/device:TPU:7, \/job:worker\/replica:0\/task:0\/device:TPU_SYSTEM:0, \/job:worker\/replica:0\/task:0\/device:XLA_CPU:0, \/job:localhost\/replica:0\/task:0\/device:CPU:0, \/job:localhost\/replica:0\/task:0\/device:COMPOSITE:0]. \r\nColocation Debug Info:\r\nColocation group had the following types and supported devices: \r\nRoot Member(assigned_device_name_index_=2 requested_device_name_='\/job:worker\/replica:0\/task:0\/device:TPU:0' assigned_device_name_='\/job:worker\/replica:0\/task:0\/device:TPU:0' resource_device_name_='\/job:worker\/replica:0\/task:0\/device:TPU:0' supported_device_types_=[CPU] possible_devices_=[]\r\nOptionalGetValue: CPU TPU XLA_CPU \r\nDebugNumericSummaryV2: CPU \r\nIteratorGetNext: CPU TPU XLA_CPU \r\nIdentity: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE \r\nSwitch: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE \r\nIteratorGetNextAsOptional: CPU TPU XLA_CPU \r\nDebugIdentityV2: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE \r\nOptionalHasValue: CPU TPU XLA_CPU \r\n_Arg: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE \r\n\r\nColocation members, user-requested devices, and framework assigned devices, if any:\r\n  iterator_1 (_Arg)  framework assigned device=\/job:worker\/replica:0\/task:0\/device:TPU:0\r\n  IteratorGetNextAsOptional (IteratorGetNextAsOptional) \r\n  OptionalHasValue (OptionalHasValue) \r\n  cond\/IteratorGetNextAsOptional\/_5 (Switch) \r\n  cond\/iterator_1\/_13 (Switch) \r\n  Func\/cond\/then\/_0\/input\/_39 (Identity) \r\n  Func\/cond\/then\/_0\/input\/_47 (Identity) \r\n  cond\/then\/_0\/cond\/OptionalHasValue (OptionalHasValue) \r\n  Func\/cond\/else\/_1\/input\/_73 (Identity) \r\n  Func\/cond\/else\/_1\/input\/_81 (Identity) \r\n  cond\/else\/_1\/cond\/IteratorGetNext (IteratorGetNext) \r\n  cond\/else\/_1\/cond\/IteratorGetNext\/DebugNumericSummaryV2 (DebugNumericSummaryV2) \r\n  cond\/else\/_1\/cond\/IteratorGetNext\/DebugIdentityV2_1511 (DebugIdentityV2) \r\n  cond\/then\/_0\/cond\/cond\/Func\/cond\/then\/_0\/input\/_39\/_111 (Switch) \r\n  Func\/cond\/then\/_0\/cond\/cond\/then\/_106\/input\/_179 (Identity) \r\n  cond\/then\/_0\/cond\/cond\/then\/_106\/cond\/cond\/OptionalGetValue (OptionalGetValue) \/job:worker\/replica:0\/task:0\/device:TPU:0\r\n  cond\/then\/_0\/cond\/cond\/then\/_106\/cond\/cond\/OptionalGetValue\/DebugNumericSummaryV2 (DebugNumericSummaryV2) \/job:worker\/replica:0\/task:0\/device:TPU:0\r\n  cond\/then\/_0\/cond\/cond\/then\/_106\/cond\/cond\/OptionalGetValue\/DebugIdentityV2_1369 (DebugIdentityV2) \/job:worker\/replica:0\/task:0\/device:TPU:0\r\n  Func\/cond\/then\/_0\/cond\/cond\/else\/_107\/input\/_184 (Identity) \r\n\r\n\t [[{{node IteratorGetNex ... [truncated]\r\n2023-07-28 06:05:18.174660: W .\/tensorflow\/core\/distributed_runtime\/eager\/destroy_tensor_handle_node.h:59] Ignoring an error encountered when deleting remote tensors handles: INVALID_ARGUMENT: Unable to find the relevant tensor remote_handle: Op ID: 900, Output num: 0\r\nAdditional GRPC error information from remote target \/job:worker\/replica:0\/task:0 while calling \/tensorflow.eager.EagerService\/Enqueue:\r\n:{\"created\":\"@1690524318.171303502\",\"description\":\"Error received from peer ipv4:10.15.76.74:8470\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/surface\/call.cc\",\"file_line\":1056,\"grpc_message\":\"Unable to find the relevant tensor remote_handle: Op ID: 900, Output num: 0\",\"grpc_status\":3} [type.googleapis.com\/tensorflow.core.platform.ErrorSourceProto='\\x08\\x05']\r\n```\r\n\r\nyeah very long.\r\n\r\noh and I've tried `tf.config.set_soft_device_placement(True)` but no result\n\n### Standalone code to reproduce the issue\n\n```shell\nHere's a reproducible test case for getting the error:\r\nhttps:\/\/colab.research.google.com\/drive\/169agwcqy3-M8hQnSAx5EAr64ya2bSMl3?usp=sharing\r\n\r\nbut you're not supposed to run it on colab, as I've noticed the `tf.debugging.experimental.enable_dump_debug_info` function generally doesn't work with it. So I put it in a .py script and run it with !python3. Without the TPU part it works fine.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:tpus","TF 2.12"],"created_at":"2023-07-28T06:27:06Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61421"},{"issue_number":321,"repository":"tensorflow\/tensorflow","title":"tf.keras.callbacks.SidecarEvaluatorModelExport doc page looks broken.","description":"### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe raw html tag is displayed and the display is collapsed when you access this link. https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/SidecarEvaluatorModelExport\n\n### Standalone code to reproduce the issue\n\n```shell\nPlease access from your browser.If it is not reproduced, I will share my detailed environment.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:docs-bug","awaiting review","type:bug","TF 2.13"],"created_at":"2023-07-25T05:08:46Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61375"},{"issue_number":322,"repository":"tensorflow\/tensorflow","title":"tf.io.gfile.rename not working for directories in S3","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.12\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe following code used to work in Tensorflow < 2.6. Upon Tensorflow 2.6, we had to import tensorflow_io. However, the tf.io.gfile.rename function which used to work on directories in S3 no longer works. I would like to update to a newer version of Tensorflow but this issue is preventing our organization from doing so, as some libraries we use use tf.io.gfile.rename to change folder names during training.\r\n\r\ntf.io.gfile.rename should work on directories according to the Tensorflow documentation\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport tensorflow_io as tfio\r\n\r\nSOURCE_DIR = 's3:\/\/...\/old_name\/'\r\nDEST_DIR = 's3:\/\/...\/new_name\/'\r\n\r\ntf.io.gfile.rename(SOURCE_DIR, DEST_DIR)\n```\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\r\n  File \"\/home\/eigen\/.config\/JetBrains\/PyCharm2023.1\/scratches\/tf2_s3_rename_test.py\", line 9, in <module>\r\n    tf.io.gfile.rename(SOURCE_DIR, DEST_DIR)\r\n  File \"\/home\/eigen\/venvs\/eigen-ml-tf2-12\/lib\/python3.10\/site-packages\/tensorflow\/python\/lib\/io\/file_io.py\", line 622, in rename_v2\r\n    _pywrap_file_io.RenameFile(\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Source is a directory or empty file\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.12"],"created_at":"2023-07-24T14:39:54Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61364"},{"issue_number":323,"repository":"tensorflow\/tensorflow","title":"Unbounded Memory leak when using tf.py_function in tf.data.Dataset.map()","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.13.0-rc2-7-g1cb1a030a62 2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04, Google Colab\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n11.8 \/ 8.6\n\n### GPU model and memory\n\nvarious, e.g. 2080ti, 3080ti mobile, Colab T4\n\n### Current behavior?\n\nUsing tf.py_function in a function that is applied to a tf.data.Dataset via its map() function causes a (C++-level) memory leak.\r\n\r\nIn my real training with more complex code inside the py_function, this lead to the python script eventually consuming upwards of 30 GB of RAM during a model.fit() loop, despite taking less that 3GB of RAM during the initial epoch.\r\n\r\ntf.py_function also more generally causes memory leaks in all kinds of places. See the flags at the top of the linked Collab for details.\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nsee Collab: https:\/\/colab.research.google.com\/drive\/1auVJPyHApl4__4FF-rV3xNcJrqYZc38R?usp=sharing\r\n\r\nIterating through a dataset with a tf.py_function in it causes unbounded linear memory consumption growth.\n```\n\n\n### Relevant log output\n\n```shell\n**Batch 0**\r\nMemory usage: 1732120576\r\nDelta: 1651.88 MiB\r\n**Batch 200**\r\nMemory usage: 1736859648\r\nDelta: 4.52 MiB\r\n**Batch 400**\r\nMemory usage: 1740644352\r\nDelta: 3.61 MiB\r\n**Batch 600**\r\nMemory usage: 1744699392\r\nDelta: 3.87 MiB\r\nAverage Delta since start: 3.87 MiB\/iteration\r\nEstimated growth per 1000 steps: 19.34 MiB\r\n**Batch 800**\r\nMemory usage: 1748750336\r\nDelta: 3.86 MiB\r\nAverage Delta since start: 3.87 MiB\/iteration\r\nEstimated growth per 1000 steps: 19.33 MiB\r\n**Batch 1000**\r\nMemory usage: 1752805376\r\nDelta: 3.87 MiB\r\nAverage Delta since start: 3.87 MiB\/iteration\r\nEstimated growth per 1000 steps: 19.33 MiB\r\n**Batch 1200**\r\nMemory usage: 1757401088\r\nDelta: 4.38 MiB\r\nAverage Delta since start: 4.00 MiB\/iteration\r\nEstimated growth per 1000 steps: 19.98 MiB\r\n**Batch 1400**\r\nMemory usage: 1761456128\r\nDelta: 3.87 MiB\r\nAverage Delta since start: 3.97 MiB\/iteration\r\nEstimated growth per 1000 steps: 19.85 MiB\r\n**Batch 1600**\r\nMemory usage: 1765240832\r\nDelta: 3.61 MiB\r\nAverage Delta since start: 3.91 MiB\/iteration\r\nEstimated growth per 1000 steps: 19.55 MiB\r\n**Batch 1800**\r\nMemory usage: 1769025536\r\nDelta: 3.61 MiB\r\nAverage Delta since start: 3.87 MiB\/iteration\r\nEstimated growth per 1000 steps: 19.33 MiB\r\n**Batch 2000**\r\nMemory usage: 1773621248\r\nDelta: 4.38 MiB\r\nAverage Delta since start: 3.93 MiB\/iteration\r\nEstimated growth per 1000 steps: 19.66 MiB\r\n**Batch 2200**\r\nMemory usage: 1777676288\r\nDelta: 3.87 MiB\r\nAverage Delta since start: 3.92 MiB\/iteration\r\nEstimated growth per 1000 steps: 19.62 MiB\r\n**Batch 2400**\r\nMemory usage: 1781731328\r\nDelta: 3.87 MiB\r\nAverage Delta since start: 3.92 MiB\/iteration\r\nEstimated growth per 1000 steps: 19.59 MiB\r\n**Batch 2600**\r\nMemory usage: 1785786368\r\nDelta: 3.87 MiB\r\nAverage Delta since start: 3.91 MiB\/iteration\r\nEstimated growth per 1000 steps: 19.57 MiB\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:performance","TF 2.13"],"created_at":"2023-07-20T20:36:51Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61344"},{"issue_number":324,"repository":"tensorflow\/tensorflow","title":"Random predictions with intel-tensorflow when OMP_THREAD_LIMIT is set","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nThe problem arises when using *intel-tensorflow*\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\nunknown, 2.13.0 (package intel_tensorflow-2.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl)\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Python version\r\n\r\n3.8, 3.9, 3.10, 3.11\r\n\r\n### Current behavior?\r\n\r\nThis problem only happens when using the *intel-tensorflow* package which uses the mkl library. When the environment variable OMP_THREAD_LIMIT is set, the predictions of some standard models become random when running on cpu.\r\n\r\nDuring the run, a warning from OMP is shown:\r\n```\r\nOMP: Warning #96: Cannot form a team with 36 threads, using 12 instead.\r\nOMP: Hint Consider unsetting KMP_DEVICE_THREAD_LIMIT (KMP_ALL_THREADS), KMP_TEAMS_THREAD_LIMIT, and OMP_THREAD_LIMIT (if any are set).\r\n```\r\nThis warning suggests to unset the variable _OMP_THREAD_LIMIT_ but I think a randomness\/instability in the prediction of models needs more than a warning with a \"Hint\".\r\n\r\nTo reproduce the problem:\r\n* Create an environment with *intel-tensorflow* installed\r\n* Copy the code in the following section into a file *test_script.py*\r\n* Run the following commands:\r\n  * `python test_script.py`\r\n  * `OMP_THREAD_LIMIT=2 python test_script.py`  (edited)\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    with tf.device(\"\/cpu:0\"):\r\n        model = tf.keras.applications.efficientnet.EfficientNetB0()\r\n        img = tf.ones((1, 224, 224, 3))\r\n        pred = model(img)\r\n\r\n        print(f\"Result: {pred[0, :5]}\")\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n# Run without OMP_THREAD_LIMIT\r\n...\r\nResult: [0.00066389 0.00075261 0.00108045 0.00210853 0.00316559]\r\n\r\n# Run with OMP_THREAD_LIMIT\r\n...\r\nOMP: Warning #96: Cannot form a team with 6 threads, using 2 instead.\r\nOMP: Hint Consider unsetting KMP_DEVICE_THREAD_LIMIT (KMP_ALL_THREADS), KMP_TEAMS_THREAD_LIMIT, and OMP_THREAD_LIMIT (if any are set).\r\nResult: [0.00030835 0.00062532 0.00057299 0.00088017 0.00140722]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","subtype:cpu-intel","TF 2.13"],"created_at":"2023-07-19T12:58:52Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61328"},{"issue_number":325,"repository":"tensorflow\/tensorflow","title":"TensorFlow Lite Converter wraps unpack operator with dequantize\/quantize","description":"### System information\r\n\r\n- Linux Ubuntu 20.04\r\n- TensorFlow installed from: pip source\r\n- TensorFlow versions: 2.12.0-current master \r\n\r\n### Code\r\n\r\nProvide code to help us reproduce your issues using one of the following options:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Layer, Input\r\nfrom tensorflow.keras.models import Model\r\nimport numpy as np\r\n\r\nclass UnstackLayer(Layer):\r\n    def __init__(self, **kwargs):\r\n        super(UnstackLayer, self).__init__(**kwargs)\r\n\r\n    def call(self, inputs):\r\n        unstacked = tf.unstack(inputs, axis=1)\r\n        # only last output is used as input to the add operator\r\n        return unstacked[-1]\r\n\r\ninput_tensor = Input(shape=(4, 16, 32))\r\nx = UnstackLayer()(input_tensor)\r\noutput_tensor = tf.add(x, 1)\r\n\r\nmodel = Model(inputs=input_tensor, outputs=output_tensor)\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n\r\ndef representative_data_gen():\r\n  for input_value in [np.random.randn(1, 4, 16, 32).astype(np.float32) for _ in range(10)]:\r\n    yield [input_value]\r\nconverter.representative_dataset = representative_data_gen\r\n\r\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\nconverter.target_spec.supported_types = {tf.int8}\r\n\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n\r\ntflite_model = converter.convert()\r\n\r\n\r\n# Save the TFLite model to a .tflite file\r\nwith open('model.tflite', 'wb') as f:\r\n    f.write(tflite_model)\r\n```\r\n\r\n### Failure after conversion\r\n\r\nInput Model:\r\n\r\n![Screenshot from 2023-07-19 11-05-35](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/112401409\/dabb3863-54c3-4148-bcab-e4afd39a4ac5)\r\n\r\nOutput Model:\r\n\r\n![Screenshot from 2023-07-19 11-28-39](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/112401409\/f42419d0-0365-4738-bd0c-0bf9dc24b798)\r\n\r\n\r\nBehaviour in TF 2.11 and below is that no dequantize\/quantize ops appear, which is expected.  \r\n\r\n### Other info\r\n\r\nThe conversion started failing with version TF 2.12.0. Note that the conversion succeeds intermittently when converting the same network many times, but on average it fails. This intermittent behaviour is still present if one runs the converter on a single core and keeps the representative dataset constant. Similar issues seems to be present when converting split operators as well. \r\n\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","TFLiteConverter","ModelOptimizationToolkit","TF 2.12"],"created_at":"2023-07-19T09:31:11Z","comments":12,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61323"},{"issue_number":326,"repository":"tensorflow\/tensorflow","title":"snapshot op wrongly changes data fingerprint ","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nv2.12.0-rc1-12-g0db597d0d75\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux 4e51bcd72cb8 5.15.109 (Colab)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen constructing a tabular dataset from a given file, I see the snapshot fingerprint changing with repeated attempts. The pipeline and data don't change tough. \r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\ntraffic_volume_csv_gz = tf.keras.utils.get_file(\r\n    'Metro_Interstate_Traffic_Volume.csv.gz', \r\n    \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00492\/Metro_Interstate_Traffic_Volume.csv.gz\",\r\n    cache_dir='.', cache_subdir='traffic'\r\n)\r\n\r\nds = tf.data.experimental.make_csv_dataset(\r\n    traffic_volume_csv_gz,\r\n    batch_size=256,\r\n    label_name='traffic_volume',\r\n    num_epochs=1,\r\n    compression_type=\"GZIP\"\r\n)\r\n\r\nds = ds.enumerate()\r\nds = ds.snapshot('ds.tfsnap')\r\nds = ds.map(lambda i,x: x).repeat(10)\r\n\r\nfor i,_ in enumerate(ds):\r\n  pass\r\n\r\nprint(i)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nIn Colab Notebook, several runs of this code generate many snapshots\r\n```console\r\nls ds.tfsnap\r\n11819476836996993959  2128571372330446365  8272381159243496395\r\n14899783750259964653  3924588669394259065  9335410099383931136\r\n```","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.13"],"created_at":"2023-07-18T15:50:06Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61317"},{"issue_number":327,"repository":"tensorflow\/tensorflow","title":"TensorFlow 2.13 distributed training fail","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3\n\n### Mobile device\n\nLinux Ubuntu 20.04.3\n\n### Python version\n\n3.8.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nCUDA 11.7, cuDNN 8.6\n\n### GPU model and memory\n\n3x NVIDIA GeForce RTX 3090\n\n### Current behavior?\n\nWhen trying to run multiple distributed trainings one after another, one of them fails with an `Collective ops is aborted by: ...` error. \r\n\r\nThe reproducer attached to this issue produces the following error:\r\n```\r\nCollective ops is aborted by: Device \/job:localhost\/replica:0\/task:0\/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)\r\nThe error could be from a previous operation. Restart your program to reset.\r\n\t [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_5585]\r\n```\r\nWhen run with TF 2.12 there is no such error.\r\n\r\nThe original code where I have encountered this problem results in\r\n```\r\nE                                           Collective ops is aborted by: Shape mismatch in the collective instance 100. Op at device \/job:localhost\/replica:0\/task:0\/device:GPU:1 expected shape [517169] but another member in the group expected shape [516734]. This is likely due to different input shapes at different members of the collective op.\r\nE                                           The error could be from a previous operation. Restart your program to reset.\r\nE                                           \t [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_49105]\r\n```\r\nbut I wasn't able to reproduce this with a small code snippet.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport pytest\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\n\r\n@pytest.mark.parametrize(\"devices\", [1, 3, 2])\r\ndef test_distributed_fit(devices):\r\n    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n    mnist_train, mnist_test = datasets['train'], datasets['test']\r\n\r\n    if devices == 1:\r\n        strategy = tf.distribute.OneDeviceStrategy(\"\/gpu:0\")\r\n    else:\r\n        strategy = tf.distribute.MirroredStrategy([f\"\/gpu:{i}\" for i in range(devices)])\r\n\r\n    batch_size = 64 * strategy.num_replicas_in_sync\r\n    train_dataset = mnist_test.cache().shuffle(10000).batch(batch_size)\r\n\r\n    with strategy.scope():\r\n        model = tf.keras.Sequential([\r\n            tf.keras.layers.Flatten(),\r\n            tf.keras.layers.Dense(10)\r\n        ])\r\n\r\n        model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n                      optimizer=tf.keras.optimizers.Adam(),\r\n                      metrics=['accuracy'])\r\n\r\n    model.fit(train_dataset, epochs=1)\r\n\r\n\r\nif __name__ == '__main__':\r\n    test_distributed_fit(1)\r\n    test_distributed_fit(3)\r\n    test_distributed_fit(2)\n```\n\n\n### Relevant log output\n\n```shell\n\/home\/nsavel\/venvs\/nncf_tf_213\/bin\/python \/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py \r\n2023-07-18 16:47:21.693862: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-07-18 16:47:21.722428: E tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:7630] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2023-07-18 16:47:21.722456: E tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2023-07-18 16:47:21.722481: E tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2023-07-18 16:47:21.728124: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-07-18 16:47:22.211027: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nWARNING:tensorflow:From \/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/tensorflow\/python\/ops\/distributions\/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\r\nInstructions for updating:\r\nThe TensorFlow Distributions library has moved to TensorFlow Probability (https:\/\/github.com\/tensorflow\/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\r\nWARNING:tensorflow:From \/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/tensorflow\/python\/ops\/distributions\/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\r\nInstructions for updating:\r\nThe TensorFlow Distributions library has moved to TensorFlow Probability (https:\/\/github.com\/tensorflow\/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\r\n2023-07-18 16:47:24.321508: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1833] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 22292 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:17:00.0, compute capability: 8.6\r\n2023-07-18 16:47:24.322042: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1833] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 22292 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6\r\n2023-07-18 16:47:24.322425: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1833] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:2 with 22292 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:b3:00.0, compute capability: 8.6\r\n2023-07-18 16:47:24.602273: W tensorflow\/core\/grappler\/optimizers\/data\/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\r\n2023-07-18 16:47:25.946425: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x7fcf358b4470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2023-07-18 16:47:25.946450: I tensorflow\/compiler\/xla\/service\/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\r\n2023-07-18 16:47:25.946455: I tensorflow\/compiler\/xla\/service\/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6\r\n2023-07-18 16:47:25.946458: I tensorflow\/compiler\/xla\/service\/service.cc:176]   StreamExecutor device (2): NVIDIA GeForce RTX 3090, Compute Capability 8.6\r\n2023-07-18 16:47:25.950178: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2023-07-18 16:47:26.074588: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:434] Loaded cuDNN version 8600\r\n2023-07-18 16:47:26.171621: I .\/tensorflow\/compiler\/jit\/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n157\/157 [==============================] - 2s 5ms\/step - loss: 25.9054 - accuracy: 0.6873\r\n2023-07-18 16:47:27.474184: W tensorflow\/core\/grappler\/optimizers\/data\/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\r\n2023-07-18 16:47:30.690312: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:434] Loaded cuDNN version 8600\r\n2023-07-18 16:47:30.822607: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:434] Loaded cuDNN version 8600\r\n53\/53 [==============================] - 3s 7ms\/step - loss: 43.9234 - accuracy: 0.5655\r\n2023-07-18 16:47:31.372876: W tensorflow\/core\/grappler\/optimizers\/data\/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.\r\n2023-07-18 16:47:32.398894: E tensorflow\/core\/common_runtime\/base_collective_executor.cc:249] BaseCollectiveExecutor::StartAbort INTERNAL: Device \/job:localhost\/replica:0\/task:0\/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)\r\n2023-07-18 16:47:32.398950: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7416489994643074752\r\n2023-07-18 16:47:32.399024: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1224112818691547746\r\n2023-07-18 16:47:32.399044: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10338356286700713842\r\n2023-07-18 16:47:32.399063: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6809993284794892577\r\n2023-07-18 16:47:32.399081: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12460047264292639245\r\n2023-07-18 16:47:32.399097: I tensorflow\/core\/framework\/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8051515006773529005\r\nTraceback (most recent call last):\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 35, in <module>\r\n    test_distributed_fit(2)\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 29, in test_distributed_fit\r\n    model.fit(train_dataset, epochs=1)\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 70, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/execute.py\", line 53, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InternalError: Graph execution error:\r\n\r\nDetected at node CollectiveReduceV2 defined at (most recent call last):\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 35, in <module>\r\n    test_distributed_fit(2)\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 35, in <module>\r\n    test_distributed_fit(2)\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 29, in test_distributed_fit\r\n    model.fit(train_dataset, epochs=1)\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 35, in <module>\r\n    test_distributed_fit(2)\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 29, in test_distributed_fit\r\n    model.fit(train_dataset, epochs=1)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 65, in error_handler\r\n    return fn(*args, **kwargs)\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 35, in <module>\r\n    test_distributed_fit(2)\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 29, in test_distributed_fit\r\n    model.fit(train_dataset, epochs=1)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 65, in error_handler\r\n    return fn(*args, **kwargs)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/engine\/training.py\", line 1782, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 35, in <module>\r\n    test_distributed_fit(2)\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 29, in test_distributed_fit\r\n    model.fit(train_dataset, epochs=1)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 65, in error_handler\r\n    return fn(*args, **kwargs)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/engine\/training.py\", line 1782, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/engine\/training.py\", line 1376, in train_function\r\n    return step_function(self, iterator)\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 35, in <module>\r\n    test_distributed_fit(2)\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 29, in test_distributed_fit\r\n    model.fit(train_dataset, epochs=1)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 65, in error_handler\r\n    return fn(*args, **kwargs)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/engine\/training.py\", line 1782, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/engine\/training.py\", line 1376, in train_function\r\n    return step_function(self, iterator)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/engine\/training.py\", line 1359, in step_function\r\n    outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 35, in <module>\r\n    test_distributed_fit(2)\r\n\r\n  File \"\/home\/nsavel\/workspace\/nncf_tf_213\/reproducer.py\", line 29, in test_distributed_fit\r\n    model.fit(train_dataset, epochs=1)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 65, in error_handler\r\n    return fn(*args, **kwargs)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/engine\/training.py\", line 1782, in fit\r\n    tmp_logs = self.train_function(iterator)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/engine\/training.py\", line 1376, in train_function\r\n    return step_function(self, iterator)\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/engine\/training.py\", line 1359, in step_function\r\n    outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n\r\n  File \"\/home\/nsavel\/venvs\/nncf_tf_213\/lib\/python3.8\/site-packages\/keras\/src\/optimizers\/utils.py\", line 175, in _all_reduce_sum_fn\r\n    return distribution.extended.batch_reduce_to(\r\n\r\nCollective ops is aborted by: Device \/job:localhost\/replica:0\/task:0\/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)\r\nThe error could be from a previous operation. Restart your program to reset.\r\n\t [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_5585]\r\n\r\nProcess finished with exit code 1\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:dist-strat","TF 2.13"],"created_at":"2023-07-18T14:56:01Z","comments":15,"reactions":5,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61314"},{"issue_number":328,"repository":"tensorflow\/tensorflow","title":"snapshoting failure on dataset made from generator","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nv2.12.0-rc1-12-g0db597d0d75\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux 4e51bcd72cb8 5.15.109 (Colab)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nSimple snapshoting op with sharding fails when applied to a generator-based dataset.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nnp.random.seed(1234)\r\n\r\nIMG_SHAPE = (224,224,3)\r\n\r\ndef gen_img(shape=IMG_SHAPE):\r\n  while True:\r\n    img = np.random.randint(0,256,size=IMG_SHAPE)\r\n    lab = np.random.randint(0,10)\r\n    yield (img,lab)\r\n\r\nds = tf.data.Dataset.from_generator(\r\n      gen_img,\r\n      output_signature=(\r\n        tf.TensorSpec(shape=IMG_SHAPE, dtype=tf.int32),\r\n        tf.TensorSpec(shape=(), dtype=tf.int32)\r\n      )\r\n)\r\nds = ds.take(int(1e3)).batch(32)\r\nds = ds.enumerate()\r\nds = ds.snapshot('.\/my_cached_dataset', shard_func = lambda i,x: i%10)\r\nds = ds.map(lambda i,x: x).repeat(2) # error disappears under 1 epoch !\r\n\r\nfor i,(img,lab) in enumerate(ds):\r\n  pass\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nResourceExhaustedError: {{function_node __wrapped__IteratorGetNext_output_types_2_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Output buffer(size: 262144 bytes) too small. Should be larger than 19267611 bytes. [Op:IteratorGetNext]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.12"],"created_at":"2023-07-18T14:37:30Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61313"},{"issue_number":329,"repository":"tensorflow\/tensorflow","title":"CTC Loss errors on TPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.12\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nKaggle notebook\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nKeras Model with LSTM+ CTC loss is running normally on GPU,\r\n but on VM TPU it  prints grappler errors. The errors usually don't stop the execution but the loss is nan.\r\nIt sometimes also crashes with coredump, but it's not consistent.\r\n\r\nI created a public kaggle notebook with the code producing the issue here:\r\nhttps:\/\/www.kaggle.com\/code\/shaironen\/ctc-example\/notebook\r\nThe grappler errors are:\r\n\r\nE tensorflow\/core\/grappler\/optimizers\/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\r\n2023-07-17 09:06:37.445931: E tensorflow\/core\/grappler\/optimizers\/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\r\nE tensorflow\/core\/grappler\/optimizers\/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\r\n\r\nIn addition, I read it's recommended to use model.compile(jit_compile=True) while on GPU to pre-diagnose TPU issues. It gives similar errors and terminates.\r\n(with jit_compile=False it run normally on GPU only).\r\n\r\nAccording to  tf.nn.ctc_loss documentation it should work on tpu.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/www.kaggle.com\/code\/shaironen\/ctc-example\/notebook\n```\n\n\n### Relevant log output\n\n```shell\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.8\/site-packages\/tensorflow\/python\/ops\/ctc_ops.py:1512: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPrefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\r\n\r\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.8\/site-packages\/tensorflow\/python\/ops\/ctc_ops.py:1512: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPrefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\r\n\r\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.8\/site-packages\/tensorflow\/python\/ops\/ctc_ops.py:1495: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPrefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\r\n\r\nWARNING:tensorflow:From \/usr\/local\/lib\/python3.8\/site-packages\/tensorflow\/python\/ops\/ctc_ops.py:1495: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPrefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\r\n2023-07-17 09:29:45.795306: E tensorflow\/core\/grappler\/optimizers\/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\r\n2023-07-17 09:29:46.168913: E tensorflow\/core\/grappler\/optimizers\/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\r\n\r\n 99\/100 [============================>.] - ETA: 0s - loss: nan\r\n\r\n2023-07-17 09:30:01.881305: E tensorflow\/core\/grappler\/optimizers\/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall.\r\n2023-07-17 09:30:02.118121: E tensorflow\/core\/grappler\/optimizers\/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall.\r\nF0717 09:30:03.134464    3526 throw_delegate.cc:121] RAW: absl::container_internal::raw_hash_map<>::at\r\n    @     0x78bd48329338  (unknown)\r\n    @     0x78bd483271e6  (unknown)\r\n    @     0x78bd465e0f20  (unknown)\r\n    @     0x78bd465d6231  (unknown)\r\n    @     0x78bd465d5b59  (unknown)\r\n    @     0x78bd476ce3c9  (unknown)\r\n    @     0x78bd476cf3d7  (unknown)\r\n    @     0x78bd476ccfba  (unknown)\r\n    @     0x78bd40ef00e6  (unknown)\r\n    @     0x78bd465d4e1f  (unknown)\r\n    @     0x78bd465d6919  (unknown)\r\n    @     0x78bd465d662c  (unknown)\r\n    @     0x78bd46345238  (unknown)\r\n    @     0x78bd430f4143  (unknown)\r\n    @     0x78bd46d5f6b4  (unknown)\r\n    @     0x78bd46d5dc50  (unknown)\r\n    @     0x78bd46d5d54e  (unknown)\r\n    @     0x78bd430cee43  (unknown)\r\n    @     0x78bd430de27a  (unknown)\r\n    @     0x78bd430ee05b  (unknown)\r\n    @     0x78bd430ee945  (unknown)\r\n    @     0x78bd424648a3  (unknown)\r\n    @     0x78bd4245e54c  (unknown)\r\n    @     0x78bd41562909  (unknown)\r\n    @     0x78bd41563a59  (unknown)\r\n    @     0x78bd40f32a1c  TpuCompile_CompileAndBuild\r\n    @     0x78bd515e0225  tensorflow::tpu::TpuProgramGroup::CompileAndBuild()\r\n    @     0x78bd51530d6f  tensorflow::tpu::TpuCompileOpKernelImpl::Compile()\r\n    @     0x78bd515e34df  tensorflow::tpu::TpuCompileOpKernelCommon::CompileLocallyAndFillHostCacheInternal()\r\n    @     0x78bd515e3aa1  tensorflow::tpu::TpuCompileOpKernelCommon::CompileLocallyAndFillHostCache()\r\n    @     0x78bd515e3c8b  tensorflow::tpu::TpuCompileOpKernelCommon::ComputeInternal()::{lambda()#3}::operator()()\r\n    @     0x78bd515e3d6c  std::_Function_handler<>::_M_invoke()\r\n    @     0x78bd515cc070  tensorflow::tpu::TpuCompilationCacheExternal::InitializeEntry()\r\n    @     0x78bd51612e72  tensorflow::tpu::TpuCompilationCacheInterface::CompileIfKeyAbsentHelper()\r\n    @     0x78bd5161391a  tensorflow::tpu::TpuCompilationCacheInterface::CompileIfKeyAbsent()\r\n    @     0x78bd515e4323  tensorflow::tpu::TpuCompileOpKernelCommon::ComputeInternal()\r\n    @     0x78bd515e6fc4  tensorflow::tpu::TpuCompileOpKernelCommon::Compute()\r\n    @     0x78bd62de6b6d  tensorflow::ThreadPoolDevice::Compute()\r\n    @     0x78bd62ecfe2c  tensorflow::(anonymous namespace)::ExecutorState<>::Process()\r\n    @     0x78bd62eb9272  std::_Function_handler<>::_M_invoke()\r\n    @     0x78bd621b7275  Eigen::ThreadPoolTempl<>::WorkerLoop()\r\n    @     0x78bd621b41c7  std::_Function_handler<>::_M_invoke()\r\n    @     0x78bd62cefabf  tsl::(anonymous namespace)::PThread::ThreadFn()\r\n    @     0x78be31abeea7  start_thread\r\nhttps:\/\/symbolize.stripped_domain\/r\/?trace=78bd48329338,78bd483271e5,78bd465e0f1f,78bd465d6230,78bd465d5b58,78bd476ce3c8,78bd476cf3d6,78bd476ccfb9,78bd40ef00e5,78bd465d4e1e,78bd465d6918,78bd465d662b,78bd46345237,78bd430f4142,78bd46d5f6b3,78bd46d5dc4f,78bd46d5d54d,78bd430cee42,78bd430de279,78bd430ee05a,78bd430ee944,78bd424648a2,78bd4245e54b,78bd41562908,78bd41563a58,78bd40f32a1b,78bd515e0224,78bd51530d6e,78bd515e34de,78bd515e3aa0,78bd515e3c8a,78bd515e3d6b,78bd515cc06f,78bd51612e71,78bd51613919,78bd515e4322,78bd515e6fc3,78bd62de6b6c,78bd62ecfe2b,78bd62eb9271,78bd621b7274,78bd621b41c6,78bd62cefabe,78be31abeea6&map=aef7fe2e538f701f46d88df9ee3b51d79ec62b1e:78bd6181e000-78bd637f9728,8f79f803f683427be94b1cfeea32716e6ef365e4:78bd48eb8000-78bd60e0d830,1278088d049ad36cb636fbbc76303cb3:78bd3cc43000-78bd484907c0 \r\nhttps:\/\/symbolize.stripped_domain\/r\/?trace=78be31b11ce1,78be31b11d5f,78bd48329337,78bd483271e5,78bd465e0f1f,78bd465d6230,78bd465d5b58,78bd476ce3c8,78bd476cf3d6,78bd476ccfb9,78bd40ef00e5,78bd465d4e1e,78bd465d6918,78bd465d662b,78bd46345237,78bd430f4142,78bd46d5f6b3,78bd46d5dc4f,78bd46d5d54d,78bd430cee42,78bd430de279,78bd430ee05a,78bd430ee944,78bd424648a2,78bd4245e54b,78bd41562908,78bd41563a58,78bd40f32a1b,78bd515e0224,78bd51530d6e,78bd515e34de,78bd515e3aa0,78bd515e3c8a&map=8f79f803f683427be94b1cfeea32716e6ef365e4:78bd48eb8000-78bd60e0d830,1278088d049ad36cb636fbbc76303cb3:78bd3cc43000-78bd484907c0 \r\n*** SIGABRT received by PID 2614 (TID 3526) on cpu 51 from PID 2614; ***\r\nE0717 09:30:03.650583    3526 coredump_hook.cc:414] RAW: Remote crash data gathering hook invoked.\r\nE0717 09:30:03.650603    3526 client.cc:278] RAW: Coroner client retries enabled (b\/136286901), will retry for up to 30 sec.\r\nE0717 09:30:03.650607    3526 coredump_hook.cc:512] RAW: Sending fingerprint to remote end.\r\nE0717 09:30:03.650615    3526 coredump_socket.cc:120] RAW: Stat failed errno=2 on socket \/var\/google\/services\/logmanagerd\/remote_coredump.socket\r\nE0717 09:30:03.650619    3526 coredump_hook.cc:518] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running?\r\nE0717 09:30:03.650623    3526 coredump_hook.cc:580] RAW: Dumping core locally.\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:tpus","TF 2.12"],"created_at":"2023-07-17T09:34:24Z","comments":12,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61297"},{"issue_number":330,"repository":"tensorflow\/tensorflow","title":"Unable to hide TPUs","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.12\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nKaggle Notebooks\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.8\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nUnable to hide TPUs from TensorFlow. The consequence of this is that if we want to use JAX along with TensorFlow, only one of them will be able to initialize the TPU system, and the other will fail. We won't be able to use `tfds`, `tf.image` or any TF operation per se if we can't hide TPUs from being used by TF. I want all these operations to run on CPU only, and leverage JAX for TPU. Here is the code to test it on a TPU machine:\r\n\r\n```python\r\nimport tenorflow as tf\r\n\r\ntf.config.set_visible_devices([], device_type=\"TPU_SYSTEM\")\r\ntf.config.set_visible_devices([], device_type=\"TPU\")\r\n\r\nprint(tf.config.list_logical_devices())\r\n\r\n# output:\r\n# [LogicalDevice(name='\/device:CPU:0', device_type='CPU'),\r\n#  LogicalDevice(name='\/device:TPU_SYSTEM:0', device_type='TPU_SYSTEM'),\r\n#  LogicalDevice(name='\/device:TPU:0', device_type='TPU'),\r\n#  LogicalDevice(name='\/device:TPU:1', device_type='TPU'),\r\n#  LogicalDevice(name='\/device:TPU:2', device_type='TPU'),\r\n#  LogicalDevice(name='\/device:TPU:3', device_type='TPU'),\r\n#  LogicalDevice(name='\/device:TPU:4', device_type='TPU'),\r\n#  LogicalDevice(name='\/device:TPU:5', device_type='TPU'),\r\n#  LogicalDevice(name='\/device:TPU:6', device_type='TPU'),\r\n# LogicalDevice(name='\/device:TPU:7', device_type='TPU')]\r\n```\r\n\r\nThis also doesn't work:\r\n\r\n```python\r\nphysical_devices = tf.config.list_physical_devices()\r\ntf.config.set_visible_devices(physical_devices[0], 'CPU')\r\n```\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tenorflow as tf\r\n\r\ntf.config.set_visible_devices([], device_type=\"TPU_SYSTEM\")\r\ntf.config.set_visible_devices([], device_type=\"TPU\")\r\n\r\nprint(tf.config.list_logical_devices())\r\n\r\n# This also doesn't work:\r\nphysical_devices = tf.config.list_physical_devices()\r\ntf.config.set_visible_devices(physical_devices[0], 'CPU')\r\n\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:tpus","TF 2.12"],"created_at":"2023-07-17T05:53:45Z","comments":10,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61293"},{"issue_number":331,"repository":"tensorflow\/tensorflow","title":"ValueError: No gradients provided for any variable","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.12\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI have run a PyTorch code that computes the gradient of the gradient w.r.t some computation. It works just fine. Now, I want to translate PyTorch code into TensorFlow but got some errors.\r\n\r\n## Standalone code to reproduce the issue\r\n\r\nHere is the reproducible code. [Gist](https:\/\/colab.research.google.com\/drive\/1GPhctZNrXynrCQ0qNbLyMDmuixQtC0fw?usp=sharing).\r\n\r\nThe above collab is small and quickly reproduces the run of PyTorch and TensorFlow. PyTorch runs as expected but TensorLow doesn't. Below is the main spot to look at:\r\n\r\n\r\n**Main Part**\r\n\r\nIn PyTorch, \r\n\r\n```python\r\nrand_model = Rnadom()\r\nmodel = Model()\r\nran_optim = torch.optim.SGD(\r\n    ran_model.parameters()\r\n)\r\n\r\nmodel_params = model.parameters()\r\nloss_mod  = model.forward(x)\r\nloss_rand = model.forward(y)\r\n\r\nmodel_grad = torch.autograd.grad(loss_mod, model_params)\r\nrand_grad  = torch.autograd.grad(\r\n    loss_rand, \r\n    model_params, \r\n    create_graph=True\r\n)\r\n    \r\nloss = some_method(model_grad, rand_grad)   \r\nrand_model.zero_grad()\r\nloss.backward()\r\nran_optim.step()\r\n```\r\nIn `pytorch`, the above `create_graph=True` is crucial. \r\n\r\nIn TensorFlow, I tried \r\n\r\n```python\r\nran_model = Random()\r\nran_optim = tf.keras.optimizers.SGD()\r\n\r\nmodel = Model()\r\nmodel.build(input_shape=(1, 784))\r\noptim = tf.keras.optimizers.SGD(0.01)\r\nmodel_params = model.trainable_variables\r\n\r\nwith tf.GradientTape(persistent=True) as tape:\r\n    tape.watch(ran_model.trainable_variables)\r\n    loss_mod = tf.reduce_mean(tf.math.log(model(x)[:, i]))\r\n    loss_rand = tf.reduce_mean(tf.math.log(model(y)[:, i]))\r\ngrads_mod = tape.gradient(loss_mod, model_params)\r\ngrads_rand = tape.gradient(loss_rand, model_params)\r\n\r\nloss = some_method(model_grad, rand_grad)  \r\nran_model_grads = tape.gradient(loss, ran_model.trainable_variables)\r\nran_optim.apply_gradients(\r\n  zip(ran_model_grads, ran_model.trainable_variables)\r\n)\r\n```\r\n\r\nThe `tf` code gives the following error. \r\n\r\n```yaml\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-01562609cda8> in <cell line: 33>()\r\n     44         loss += tf.reduce_sum(tf.stack([a, b], axis=0))\r\n     45     ran_model_grads = tape.gradient(loss, ran_model.trainable_variables)\r\n---> 46     ran_optim.apply_gradients(zip(ran_model_grads, ran_model.trainable_variables))\r\n     47 \r\n     48 \r\n\r\n3 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/optimizers\/utils.py in filter_empty_gradients(grads_and_vars)\r\n     75     if not filtered:\r\n     76         variable = ([v.name for _, v in grads_and_vars],)\r\n---> 77         raise ValueError(\r\n     78             f\"No gradients provided for any variable: {variable}. \"\r\n     79             f\"Provided `grads_and_vars` is {grads_and_vars}.\"\r\n\r\nValueError: No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(10, 1, 784) dtype=float32, numpy=\r\n```\r\n\r\n- This is probably because the `ran_model_grads, ran_model.trainable_variables` are not connected. As mentioned in this [doc](https:\/\/www.tensorflow.org\/guide\/autodiff), \r\n\r\n> When a **target** is not connected to a **source**, the gradient will return `None`\r\n\r\n- In PyTorch, `create_graph=True` is used to compute the gradient of the gradient in the later part. To compute [grad-of-grad](https:\/\/www.tensorflow.org\/guide\/advanced_autodiff#example_input_gradient_regularization), but didn't work (shown below). The reason probably is the same as before, source and target are not connected.\r\n\r\n```python\r\nfor i in range(5):\r\n    \r\n    with tf.GradientTape() as tape1:\r\n        loss_mod = tf.reduce_mean(tf.math.log(model(x)[:, i]))\r\n    grads_mod = tape1.gradient(loss_mod, model_params)\r\n    \r\n    \r\n    with tf.GradientTape() as tape3:\r\n        with tf.GradientTape() as tape2:\r\n            loss_rand = tf.reduce_mean(tf.math.log(model(y)[:, i]))\r\n        grads_rand = tape2.gradient(loss_rand, model_params)\r\n\r\n    loss = 0\r\n    for a, b in zip(grads_mod, grads_rand):\r\n        loss += tf.reduce_sum(tf.stack([a, b], axis=0))\r\n    [ISSUE] > ran_model_grads = tape3.gradient(loss, ran_model.trainable_variables)\r\n    ran_optim.apply_gradients(zip(ran_model_grads, ran_model.trainable_variables))\r\n```\r\n\r\nBut in this case, how to resolve this in TensorFlow?","labels":["stat:awaiting tensorflower","type:bug","comp:ops","comp:core","TF 2.12"],"created_at":"2023-07-15T11:35:56Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61285"},{"issue_number":332,"repository":"tensorflow\/tensorflow","title":"keras.models.load_model broken if custom objects present (.keras format)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf2.13\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nAll\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nModels do not load.  Errors vary, depending on the number of training samples being divisible by batch size or not.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport keras\r\nimport numpy as np\r\n\r\n\r\nclass MyCustomLSTM(keras.layers.LSTM):\r\n    \"\"\"\r\n    Custom LSTM\r\n    \"\"\"\r\n\r\n\r\nmodel = keras.models.Sequential([MyCustomLSTM(32), keras.layers.Dense(1)])\r\n\r\nmodel.compile(keras.optimizers.Adam(), keras.losses.mse)\r\n\r\nx = np.zeros((1024, 300, 1))\r\ny = np.zeros((1024, 1))\r\n\r\nhistory = model.fit(x, y, batch_size=32, epochs=1)\r\n\r\nmodel.save('test.keras')\r\n\r\nmodel_loaded = keras.models.load_model('test.keras', custom_objects={'MyCustomLSTM': MyCustomLSTM})\r\n# This throws:\r\n# IndexError: list assignment index out of range\r\n\r\n# If the number of samples is not divisible by the batch size (e.g. 1025 samples, instead of 1024), the error becomes:\r\n# ValueError: as_list() is not defined on an unknown TensorShape.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:keras","comp:model","TF 2.13"],"created_at":"2023-07-13T10:48:41Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61270"},{"issue_number":333,"repository":"tensorflow\/tensorflow","title":"tf.linalg.logdet outputs -inf on a matrix with complex data type","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nGiven the following matrix:\r\n```\r\nt = np.array([[1 + 1j, 4 + 1j], [4 + 2j, 3 + 1j]])\r\n```\r\ntf.linalg.logdet outputs `-inf`, I think the expected output should be a normal value. For reference, PyTorch's torch.logdet outputs `tensor(2.6688-2.5536j, dtype=torch.complex128)`\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\nt = np.array([[1 + 1j, 4 + 1j], [4 + 2j, 3 + 1j]])\r\ntf_res = tf.linalg.logdet(tf.constant(t, dtype=tf.complex128))\r\ntorch_res = torch.logdet(torch.tensor(t))\r\nprint(tf_res, torch_res)\r\n```\n```\n\n\n### Relevant log output\n\n```shell\ntf.Tensor(-inf, shape=(), dtype=float64) tensor(2.6688-2.5536j, dtype=torch.complex128)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-07-12T05:06:25Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61249"},{"issue_number":334,"repository":"tensorflow\/tensorflow","title":"tf.image.adjust_contrast fails on the tf.half data type","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI was trying tf.image.adjust_contrast but I find that it fails when I set the input tensor's data type to be `float16 (tf.half)`, this API raises error. However, if I set the input data type to be `bfloat16`, this API works properly. \n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\nnp.random.seed(1234)\r\nt = tf.constant(np.random.rand(1,2,1), dtype=tf.float16)\r\ni = tf.image.adjust_contrast(t, 2.)\r\nprint(i)\r\n```\n```\n\n\n### Relevant log output\n\n```shell\nNotFoundError: Could not find device for node: {{node AdjustContrastv2}} = AdjustContrastv2[T=DT_HALF]\r\nAll kernels registered for op AdjustContrastv2:\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_HALF]\r\n  device='CPU'; T in [DT_FLOAT]\r\n  device='GPU'; T in [DT_HALF]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_HALF]\r\n [Op:AdjustContrastv2] name:\n```\n","labels":["awaiting review","type:bug","comp:apis","comp:ops","TF 2.13"],"created_at":"2023-07-11T16:05:56Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61246"},{"issue_number":335,"repository":"tensorflow\/tensorflow","title":"Tensorflow profiler is not showing anything. Gives \"No profile data was found\" text on selecting Profile in Tensorboard","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.12, tf 2.13, tf-nightly\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI was learning how to use TensorFlow Profiler according to the [guide](https:\/\/www.tensorflow.org\/tensorboard\/tensorboard_profiling_keras). I ran the same notebook without altering anything in Google Colab and it does now show Profile tab. Moreover on selecting the Profile option from the right hand side drop down list, It shows \r\n\r\n```\r\nNo profile data was found.\r\nIf you have a model running on CPU, GPU, or Google Cloud TPU, you may be able to use the above button to capture a profile.\r\n\r\nIf you're a CPU or GPU user, please use the IP address option. You may want to check out the [tutorial](https:\/\/colab.research.google.com\/github\/tensorflow\/tensorboard\/blob\/master\/docs\/tensorboard_profiling_keras.ipynb) on how to start a TensorFlow profiler server and profile a Keras model on a GPU.\r\n\r\nIf you're a TPU user, please use the TPU name option and you may want to check out the [tutorial](https:\/\/cloud.google.com\/tpu\/docs\/cloud-tpu-tools) on how to interpreting the profiling results.\r\n\r\nIf you think profiling is done properly, please see the page of [Google Cloud TPU Troubleshooting and FAQ](https:\/\/cloud.google.com\/tpu\/docs\/troubleshooting) and consider filing an issue on GitHub.\r\n\r\n```\r\n\r\nInstead it should have shown the Profile\n\n### Standalone code to reproduce the issue\n\n```shell\nKindly use the official [tensorflow profiler guide](https:\/\/www.tensorflow.org\/tensorboard\/tensorboard_profiling_keras)\n```\n\n\n### Relevant log output\n\n```shell\nNo profile data was found.\r\nIf you have a model running on CPU, GPU, or Google Cloud TPU, you may be able to use the above button to capture a profile.\r\n\r\nIf you're a CPU or GPU user, please use the IP address option. You may want to check out the [tutorial](https:\/\/colab.research.google.com\/github\/tensorflow\/tensorboard\/blob\/master\/docs\/tensorboard_profiling_keras.ipynb) on how to start a TensorFlow profiler server and profile a Keras model on a GPU.\r\n\r\nIf you're a TPU user, please use the TPU name option and you may want to check out the [tutorial](https:\/\/cloud.google.com\/tpu\/docs\/cloud-tpu-tools) on how to interpreting the profiling results.\r\n\r\nIf you think profiling is done properly, please see the page of [Google Cloud TPU Troubleshooting and FAQ](https:\/\/cloud.google.com\/tpu\/docs\/troubleshooting) and consider filing an issue on GitHub.\r\n\r\n```\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","TF 2.13"],"created_at":"2023-07-07T13:01:22Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61212"},{"issue_number":336,"repository":"tensorflow\/tensorflow","title":"Tensorflow Profiler does not work on WSL2: Failed to load libcupti (is it installed and accessible?) ","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\nv2.12.0-rc1-12-g0db597d0d75 2.12.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nWindows 10 WSL Ubuntu\r\n\r\n### Mobile device\r\n\r\nUbuntu 22.04\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n11.8\/8.6\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nAfter following exactly the steps mentioned in https:\/\/www.tensorflow.org\/install\/pip for installing Tensorflow on WSL2, and installing the latest version of the profiler plugin, the Tensorboard profiler does not seem to work.\r\n\r\nThis is with a fresh WSL2 install, miniconda install, etc.\r\n\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/11645696\/adbaf3c1-4f03-43fe-80d7-39c484ac91a7)\r\n\r\n```\r\nFailed to load libcupti (is it installed and accessible?) \r\n\r\nNo step marker observed and hence the step time is unknown. This may happen if (1) training steps are not instrumented (e.g., if you are not using Keras) or (2) the profiling duration is shorter than the step time. For (1), you need to add step instrumentation; for (2), you may try to profile longer. \r\n```\r\n\r\nThe problem does not seem to be that lubcupti fails to load (despite what is indicated by Tensorboard); libcupti seems to be found just fine, but there may be some problems - See the attached log output for possible clues as to what's happening. \r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\n# The below code is copied directly from https:\/\/github.com\/keras-team\/keras-io\/blob\/master\/examples\/vision\/mnist_convnet.py - with the single addition of adding Tensorboard profiling.\r\n\r\nimport numpy as np\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\n\r\n\"\"\"\r\n## Prepare the data\r\n\"\"\"\r\n\r\n# Model \/ data parameters\r\nnum_classes = 10\r\ninput_shape = (28, 28, 1)\r\n\r\n# Load the data and split it between train and test sets\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n\r\n# Scale images to the [0, 1] range\r\nx_train = x_train.astype(\"float32\") \/ 255\r\nx_test = x_test.astype(\"float32\") \/ 255\r\n# Make sure images have shape (28, 28, 1)\r\nx_train = np.expand_dims(x_train, -1)\r\nx_test = np.expand_dims(x_test, -1)\r\nprint(\"x_train shape:\", x_train.shape)\r\nprint(x_train.shape[0], \"train samples\")\r\nprint(x_test.shape[0], \"test samples\")\r\n\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\n\"\"\"\r\n## Build the model\r\n\"\"\"\r\n\r\nmodel = keras.Sequential(\r\n    [\r\n        keras.Input(shape=input_shape),\r\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\r\n        layers.MaxPooling2D(pool_size=(2, 2)),\r\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\r\n        layers.MaxPooling2D(pool_size=(2, 2)),\r\n        layers.Flatten(),\r\n        layers.Dropout(0.5),\r\n        layers.Dense(num_classes, activation=\"softmax\"),\r\n    ]\r\n)\r\n\r\nmodel.summary()\r\n\r\n\"\"\"\r\n## Train the model\r\n\"\"\"\r\n\r\nbatch_size = 128\r\nepochs = 15\r\n\r\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\r\n\r\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[\r\n    keras.callbacks.TensorBoard(profile_batch=[20, 30])\r\n])\r\n\r\n\"\"\"\r\n## Evaluate the trained model\r\n\"\"\"\r\n\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\nprint(\"Test loss:\", score[0])\r\nprint(\"Test accuracy:\", score[1])\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n17\/422 [>.............................] - ETA: 2s - loss: 2.0755 - accuracy: 0.38602023-07-07 15:45:09.034256: I tensorflow\/tsl\/profiler\/lib\/profiler_session.cc:104] Profiler session initializing.\r\n2023-07-07 15:45:09.034284: I tensorflow\/tsl\/profiler\/lib\/profiler_session.cc:119] Profiler session started.\r\n2023-07-07 15:45:09.034300: E tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.\r\n2023-07-07 15:45:09.034304: E tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error.\r\n2023-07-07 15:45:09.034307: E tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.\r\n2023-07-07 15:45:09.034309: E tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:1730] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error \r\n 25\/422 [>.............................] - ETA: 2s - loss: 1.8839 - accuracy: 0.46882023-07-07 15:45:09.105818: I tensorflow\/tsl\/profiler\/lib\/profiler_session.cc:70] Profiler session collecting data.\r\n2023-07-07 15:45:09.105940: E tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_error_manager.cc:142] cuptiFinalize: ignored due to a previous error.\r\n2023-07-07 15:45:09.105943: E tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.\r\n2023-07-07 15:45:09.105945: E tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:1822] function cupti_interface_->Finalize()failed with error \r\n2023-07-07 15:45:09.107652: E tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.\r\n2023-07-07 15:45:09.107660: E tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.\r\n2023-07-07 15:45:09.107663: I tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_collector.cc:541]  GpuTracer has collected 0 callback api events and 0 activity events. \r\n2023-07-07 15:45:09.107809: I tensorflow\/tsl\/profiler\/lib\/profiler_session.cc:131] Profiler session tear down.\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","type:others","wsl2"],"created_at":"2023-07-07T05:56:10Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61210"},{"issue_number":337,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/compiler\/xla\/service\/gpu:fusion_merger_test fails on AARCH64","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ngit HEAD\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.17\n\n### Bazel version\n\n6.1.0\n\n### GCC\/compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current behavior?\n\n\/\/tensorflow\/compiler\/xla\/service\/gpu:fusion_merger_test unit test fails when run on AARCH64 machine.\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel test --cache_test_results=no --config=mkl_aarch64_threadpool --jobs=75 --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --build_tests_only -- \/\/tensorflow\/compiler\/xla\/service\/gpu:fusion_merger_test\n```\n\n\n### Relevant log output\n\n```shell\nFAIL: \/\/tensorflow\/compiler\/xla\/service\/gpu:fusion_merger_test (see \/home\/andrew\/src\/tf_test\/tensorflow-git\/bazel-ci_build-cache\/.cache\/bazel\/_bazel_andrew\/eab0d61a99b6696edb3d2aff87b585e8\/execroot\/org_tensorflow\/bazel-out\/aarch64-opt\/testlogs\/tensorflow\/compiler\/xla\/service\/gpu\/fusion_merger_test\/test.log)\r\nINFO: From Testing \/\/tensorflow\/compiler\/xla\/service\/gpu:fusion_merger_test:\r\n==================== Test output for \/\/tensorflow\/compiler\/xla\/service\/gpu:fusion_merger_test:\r\n[==========] Running 21 tests from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 21 tests from FusionMergerTest\r\n[ RUN      ] FusionMergerTest.MergeSharedFusionInstruction\r\n[       OK ] FusionMergerTest.MergeSharedFusionInstruction (9 ms)\r\n[ RUN      ] FusionMergerTest.MoreMemoryAccessIfFused\r\n[       OK ] FusionMergerTest.MoreMemoryAccessIfFused (1 ms)\r\n[ RUN      ] FusionMergerTest.LessMemoryAccessIfFused\r\n[       OK ] FusionMergerTest.LessMemoryAccessIfFused (1 ms)\r\n[ RUN      ] FusionMergerTest.WillMergeIntoInputFusion\r\n[       OK ] FusionMergerTest.WillMergeIntoInputFusion (1 ms)\r\n[ RUN      ] FusionMergerTest.WillMergeIntoUnfusedConsumer\r\n[       OK ] FusionMergerTest.WillMergeIntoUnfusedConsumer (1 ms)\r\n[ RUN      ] FusionMergerTest.WillNotMergeReduceUnfriendlyLayouts\r\n[       OK ] FusionMergerTest.WillNotMergeReduceUnfriendlyLayouts (0 ms)\r\n[ RUN      ] FusionMergerTest.WillMergeReduceNotTooUnfriendlyLayouts\r\n[       OK ] FusionMergerTest.WillMergeReduceNotTooUnfriendlyLayouts (1 ms)\r\n[ RUN      ] FusionMergerTest.AvoidsLargeFusion\r\n[       OK ] FusionMergerTest.AvoidsLargeFusion (1 ms)\r\n[ RUN      ] FusionMergerTest.WillNotMergeIfFusionEmitterIsInefficient\r\n[       OK ] FusionMergerTest.WillNotMergeIfFusionEmitterIsInefficient (1 ms)\r\n[ RUN      ] FusionMergerTest.WillMergeSliceIntoReusingConsumer\r\n[       OK ] FusionMergerTest.WillMergeSliceIntoReusingConsumer (0 ms)\r\n[ RUN      ] FusionMergerTest.WillMergeExpensiveFusionsIfSavesMemory\r\n[       OK ] FusionMergerTest.WillMergeExpensiveFusionsIfSavesMemory (1 ms)\r\n[ RUN      ] FusionMergerTest.WillMergeExpensiveFusionsWithSingleConsumer\r\n[       OK ] FusionMergerTest.WillMergeExpensiveFusionsWithSingleConsumer (0 ms)\r\n[ RUN      ] FusionMergerTest.WillNotMergeExpensiveFusionsWithReusingConsumer\r\n[       OK ] FusionMergerTest.WillNotMergeExpensiveFusionsWithReusingConsumer (0 ms)\r\n[ RUN      ] FusionMergerTest.NoMergeWithBitcast\r\n[       OK ] FusionMergerTest.NoMergeWithBitcast (1 ms)\r\n[ RUN      ] FusionMergerTest.CostBasedMerge\r\n[       OK ] FusionMergerTest.CostBasedMerge (1 ms)\r\n[ RUN      ] FusionMergerTest.CostBasedNoMerge\r\n[       OK ] FusionMergerTest.CostBasedNoMerge (4 ms)\r\n[ RUN      ] FusionMergerTest.NoMergeBecauseTooManyBasicBlockSplits\r\n[       OK ] FusionMergerTest.NoMergeBecauseTooManyBasicBlockSplits (4 ms)\r\n[ RUN      ] FusionMergerTest.CommonElementwiseUsedParameter\r\n[       OK ] FusionMergerTest.CommonElementwiseUsedParameter (1 ms)\r\n[ RUN      ] FusionMergerTest.IncompatibleNonTrivialHeroes\r\n[       OK ] FusionMergerTest.IncompatibleNonTrivialHeroes (0 ms)\r\n[ RUN      ] FusionMergerTest.DoNotMergeDUSFusions\r\n[       OK ] FusionMergerTest.DoNotMergeDUSFusions (1 ms)\r\n[ RUN      ] FusionMergerTest.MergeDUSFusionWithElementwiseFusion\r\ntensorflow\/compiler\/xla\/service\/gpu\/fusion_merger_test.cc:1127: Failure\r\nValue of: fusion_merger_.Run(module.get()).value()\r\n  Actual: false\r\nExpected: true\r\n[  FAILED  ] FusionMergerTest.MergeDUSFusionWithElementwiseFusion (0 ms)\r\n[----------] 21 tests from FusionMergerTest (41 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 21 tests from 1 test suite ran. (42 ms total)\r\n[  PASSED  ] 20 tests.\r\n[  FAILED  ] 1 test, listed below:\r\n[  FAILED  ] FusionMergerTest.MergeDUSFusionWithElementwiseFusion\r\n\r\n 1 FAILED TEST\r\n================================================================================\r\nTarget \/\/tensorflow\/compiler\/xla\/service\/gpu:fusion_merger_test up-to-date:\r\n  bazel-bin\/tensorflow\/compiler\/xla\/service\/gpu\/fusion_merger_test\r\nINFO: Elapsed time: 392.381s, Critical Path: 276.82s\r\nINFO: 1497 processes: 613 internal, 884 local.\r\nINFO: Build completed, 1 test FAILED, 1497 total actions\r\n\/\/tensorflow\/compiler\/xla\/service\/gpu:fusion_merger_test                 FAILED in 1.2s\r\n  \/home\/andrew\/src\/tf_test\/tensorflow-git\/bazel-ci_build-cache\/.cache\/bazel\/_bazel_andrew\/eab0d61a99b6696edb3d2aff87b585e8\/execroot\/org_tensorflow\/bazel-out\/aarch64-opt\/testlogs\/tensorflow\/compiler\/xla\/service\/gpu\/fusion_merger_test\/test.log\r\n\r\nExecuted 1 out of 1 test: 1 fails locally.\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux"],"created_at":"2023-07-05T16:03:35Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61180"},{"issue_number":338,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/distribute:parameter_server_strategy_v2_test_cpu is flaky","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ngit HEAD\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n6.1.0\n\n### GCC\/compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current behavior?\n\n\/\/tensorflow\/python\/distribute:parameter_server_strategy_v2_test_cpu timeouts sometimes\r\n\r\nx86 log\r\nhttps:\/\/source.cloud.google.com\/results\/invocations\/75eaa47f-92bd-47d5-9c7a-020a0c67dda9\/log\r\n\r\nAARCH64 log\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5399333727\/jobs\/9806323589#step:5:7344\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel --bazelrc=\/usertools\/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https:\/\/storage.googleapis.com\/tensorflow-devinfra-bazel-cache\/norbe --google_default_credentials\n```\n\n\n### Relevant log output\n\n```shell\nTIMEOUT: \/\/tensorflow\/python\/distribute:parameter_server_strategy_v2_test_cpu (Summary)\r\n      \/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/testlogs\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu\/test.log\r\nINFO: From Testing \/\/tensorflow\/python\/distribute:parameter_server_strategy_v2_test_cpu:\r\n==================== Test output for \/\/tensorflow\/python\/distribute:parameter_server_strategy_v2_test_cpu:\r\n2023-07-04 08:53:48.311167: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-07-04 08:53:48.373167: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nRunning tests under Python 3.9.17: \/usr\/bin\/python3\r\n[ RUN      ] ClusterTypeNameTest.testArbitraryCurrentTaskType\r\nINFO:tensorflow:Using local port 38369\r\nI0704 08:53:53.254792 140378756265792 test_util.py:3796] Using local port 38369\r\nINFO:tensorflow:Using local port 44357\r\nI0704 08:53:53.255300 140378756265792 test_util.py:3796] Using local port 44357\r\nINFO:tensorflow:Using local port 46287\r\nI0704 08:53:53.255505 140378756265792 test_util.py:3796] Using local port 46287\r\nINFO:tensorflow:time(__main__.ClusterTypeNameTest.testArbitraryCurrentTaskType): 0.0s\r\nI0704 08:53:53.256115 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testArbitraryCurrentTaskType): 0.0s\r\n[       OK ] ClusterTypeNameTest.testArbitraryCurrentTaskType\r\n[ RUN      ] ClusterTypeNameTest.testArbitraryJobName\r\nINFO:tensorflow:Using local port 43625\r\nI0704 08:53:53.256868 140378756265792 test_util.py:3796] Using local port 43625\r\nINFO:tensorflow:Using local port 38445\r\nI0704 08:53:53.257118 140378756265792 test_util.py:3796] Using local port 38445\r\nINFO:tensorflow:Using local port 33687\r\nI0704 08:53:53.257305 140378756265792 test_util.py:3796] Using local port 33687\r\nINFO:tensorflow:Using local port 43947\r\nI0704 08:53:53.257477 140378756265792 test_util.py:3796] Using local port 43947\r\nINFO:tensorflow:time(__main__.ClusterTypeNameTest.testArbitraryJobName): 0.0s\r\nI0704 08:53:53.258199 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testArbitraryJobName): 0.0s\r\n[       OK ] ClusterTypeNameTest.testArbitraryJobName\r\n[ RUN      ] ClusterTypeNameTest.testLessThanOnePs\r\nINFO:tensorflow:Using local port 37051\r\nI0704 08:53:53.258810 140378756265792 test_util.py:3796] Using local port 37051\r\nINFO:tensorflow:Using local port 44491\r\nI0704 08:53:53.259010 140378756265792 test_util.py:3796] Using local port 44491\r\nINFO:tensorflow:time(__main__.ClusterTypeNameTest.testLessThanOnePs): 0.0s\r\nI0704 08:53:53.259491 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testLessThanOnePs): 0.0s\r\n[       OK ] ClusterTypeNameTest.testLessThanOnePs\r\n[ RUN      ] ClusterTypeNameTest.testLessThanOneWorker\r\nINFO:tensorflow:Using local port 33303\r\nI0704 08:53:53.260033 140378756265792 test_util.py:3796] Using local port 33303\r\nINFO:tensorflow:Using local port 33109\r\nI0704 08:53:53.260214 140378756265792 test_util.py:3796] Using local port 33109\r\nINFO:tensorflow:time(__main__.ClusterTypeNameTest.testLessThanOneWorker): 0.0s\r\nI0704 08:53:53.260598 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testLessThanOneWorker): 0.0s\r\n[       OK ] ClusterTypeNameTest.testLessThanOneWorker\r\n[ RUN      ] ClusterTypeNameTest.testMoreThanOneChief\r\nINFO:tensorflow:Using local port 36459\r\nI0704 08:53:53.261005 140378756265792 test_util.py:3796] Using local port 36459\r\nINFO:tensorflow:Using local port 33799\r\nI0704 08:53:53.261153 140378756265792 test_util.py:3796] Using local port 33799\r\nINFO:tensorflow:Using local port 39497\r\nI0704 08:53:53.261297 140378756265792 test_util.py:3796] Using local port 39497\r\nINFO:tensorflow:Using local port 36657\r\nI0704 08:53:53.261423 140378756265792 test_util.py:3796] Using local port 36657\r\nINFO:tensorflow:Using local port 37059\r\nI0704 08:53:53.261545 140378756265792 test_util.py:3796] Using local port 37059\r\nINFO:tensorflow:time(__main__.ClusterTypeNameTest.testMoreThanOneChief): 0.0s\r\nI0704 08:53:53.261911 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testMoreThanOneChief): 0.0s\r\n[       OK ] ClusterTypeNameTest.testMoreThanOneChief\r\n[ RUN      ] ClusterTypeNameTest.test_session\r\n[  SKIPPED ] ClusterTypeNameTest.test_session\r\nINFO:tensorflow:Now creating a MultiProcessCluster with num_workers=2, num_ps=3.\r\nI0704 08:53:53.262382 140378756265792 multi_worker_test_base.py:335] Now creating a MultiProcessCluster with num_workers=2, num_ps=3.\r\nINFO:tensorflow:Using local port 34725\r\nI0704 08:53:53.262561 140378756265792 test_util.py:3796] Using local port 34725\r\nINFO:tensorflow:Using local port 41389\r\nI0704 08:53:53.262695 140378756265792 test_util.py:3796] Using local port 41389\r\nINFO:tensorflow:Using local port 42953\r\nI0704 08:53:53.262822 140378756265792 test_util.py:3796] Using local port 42953\r\nINFO:tensorflow:Using local port 39597\r\nI0704 08:53:53.262954 140378756265792 test_util.py:3796] Using local port 39597\r\nINFO:tensorflow:Using local port 40735\r\nI0704 08:53:53.263074 140378756265792 test_util.py:3796] Using local port 40735\r\n2023-07-04 08:53:54.005149: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-07-04 08:53:54.092559: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-07-04 08:53:54.232593: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-07-04 08:53:54.295324: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n-- Test timed out at 2023-07-04 08:58:46 UTC --\r\nThread 0x00007fac735de700 (most recent call first):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 258 in _continuously_readline_from_sub\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 917 in run\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 980 in _bootstrap_inner\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 937 in _bootstrap\r\n\r\nThread 0x00007fac765a0700 (most recent call first):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 258 in _continuously_readline_from_sub\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 917 in run\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 980 in _bootstrap_inner\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 937 in _bootstrap\r\n\r\nThread 0x00007fac025a4700 (most recent call first):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 258 in _continuously_readline_from_sub\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 917 in run\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 980 in _bootstrap_inner\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 937 in _bootstrap\r\n\r\nThread 0x00007fabffda3700 (most recent call first):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 527 in _process_watchdog\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 917 in run\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 980 in _bootstrap_inner\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 937 in _bootstrap\r\n\r\nThread 0x00007fabff5a2700 (most recent call first):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 258 in _continuously_readline_from_sub\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 917 in run\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 980 in _bootstrap_inner\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 937 in _bootstrap\r\n\r\nCurrent thread 0x00007fac79e6b740 (most recent call first):\r\n  File \"\/usr\/lib\/python3.9\/multiprocessing\/connection.py\", line 379 in _recv\r\n  File \"\/usr\/lib\/python3.9\/multiprocessing\/connection.py\", line 414 in _recv_bytes\r\n  File \"\/usr\/lib\/python3.9\/multiprocessing\/connection.py\", line 250 in recv\r\n  File \"\/usr\/lib\/python3.9\/multiprocessing\/managers.py\", line 810 in _callmethod\r\n  File \"\/usr\/lib\/python3.9\/multiprocessing\/managers.py\", line 1085 in wait\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_worker_test_base.py\", line 270 in start\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_worker_test_base.py\", line 348 in create_multi_process_cluster\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test.py\", line 62 in setUpClass\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 166 in _handleClassSetUp\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 114 in run\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 84 in __call__\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 122 in run\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 84 in __call__\r\n  File \"\/usr\/lib\/python3.9\/unittest\/runner.py\", line 184 in run\r\n  File \"\/usr\/lib\/python3.9\/unittest\/main.py\", line 271 in runTests\r\n  File \"\/usr\/lib\/python3.9\/unittest\/main.py\", line 101 in __init__\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2537 in _run_and_get_tests_result\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2568 in run_tests\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2156 in _run_in_app\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2049 in main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 51 in g_main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/absl_py\/absl\/app.py\", line 258 in _run_main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/absl_py\/absl\/app.py\", line 312 in run\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 60 in main_wrapper\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/benchmark.py\", line 489 in benchmarks_main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 62 in main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/test.py\", line 56 in main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/eager\/test.py\", line 25 in main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_lib.py\", line 167 in test_main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 1455 in test_main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/parameter_server_strategy_v2_test.py\", line 713 in <module>\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux"],"created_at":"2023-07-04T10:39:13Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61169"},{"issue_number":339,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/data\/experimental\/kernel_tests\/service:worker_tags_test is flaky","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ngit HEAD\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n6.1.0\n\n### GCC\/compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current behavior?\n\n\/\/tensorflow\/python\/data\/experimental\/kernel_tests\/service:worker_tags_test fails occasionally\r\n\r\nx86 log\r\nhttps:\/\/source.cloud.google.com\/results\/invocations\/0bc426bf-e5ae-4fb6-993f-6199c00d1139\/log\r\n\r\nAARCH64 log\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5411569978\/jobs\/9834485829#step:5:8675\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel --bazelrc=\/usertools\/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https:\/\/storage.googleapis.com\/tensorflow-devinfra-bazel-cache\/norbe --google_default_credentials\n```\n\n\n### Relevant log output\n\n```shell\n======================================================================\r\nERROR: testMultipleTags_test_mode_eager_tfapiversion_2 (__main__.WorkerTagsTest)\r\nWorkerTagsTest.testMultipleTags_test_mode_eager_tfapiversion_2\r\ntestMultipleTags_test_mode_eager_tfapiversion_2(mode='eager', tf_api_version=2)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.runfiles\/absl_py\/absl\/testing\/parameterized.py\", line 314, in bound_param_test\r\n    return test_method(self, **testcase_params)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_combinations.py\", line 360, in decorated\r\n    execute_test_method()\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_combinations.py\", line 343, in execute_test_method\r\n    test_method(**kwargs_to_pass)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.py\", line 179, in testMultipleTags\r\n    cluster = multi_process_cluster.MultiProcessCluster(\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/multi_process_cluster.py\", line 81, in __init__\r\n    self._start_local_workers(num_local_workers, worker_tags)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/multi_process_cluster.py\", line 100, in _start_local_workers\r\n    self.start_local_worker(worker_tags)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/multi_process_cluster.py\", line 114, in start_local_worker\r\n    worker.start()\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/test_base.py\", line 110, in start\r\n    self._server.start()\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/service\/server_lib.py\", line 415, in start\r\n    self._server.start()\r\nNotImplementedError: Failed to get dispatcher version from dispatcher running at localhost:45069:\r\n\r\n----------------------------------------------------------------------\r\nRan 6 tests in 6.597s\r\n\r\nFAILED (errors=1)\r\nException ignored in: <function MultiProcessCluster.__del__ at 0x7f20f09658b0>\r\nTraceback (most recent call last):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/multi_process_cluster.py\", line 160, in __del__\r\n    self._stop()\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/worker_tags_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/multi_process_cluster.py\", line 155, in _stop\r\n    for (_, worker_process) in self._remote_workers:\r\nAttributeError: 'MultiProcessCluster' object has no attribute '_remote_workers'\r\n2023-07-02 09:48:51.833944: I tensorflow\/core\/data\/service\/server_lib.cc:94] Shut down DispatchServer server running at port 45069\r\n2023-07-02 09:48:51.934278: I tensorflow\/core\/data\/service\/server_lib.cc:94] Shut down WorkerServer server running at port 44313\r\n================================================================================\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux"],"created_at":"2023-07-03T16:32:51Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61166"},{"issue_number":340,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/dtensor\/mlir\/tests:spmd_expansion.mlir.test is flaky","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ngit HEAD\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n6.1.0\n\n### GCC\/compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current behavior?\n\n\/\/tensorflow\/dtensor\/mlir\/tests:spmd_expansion.mlir.test unit test fails occasionally\r\n\r\nx86 log\r\nhttps:\/\/source.cloud.google.com\/results\/invocations\/88910868-abeb-4aa0-8954-df2b79ef5a26\/log\r\n\r\nAARCH64 log\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5420661339\/jobs\/9855097285\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel --bazelrc=\/usertools\/cpu.bazelrc test --config=rbe --config=pycpp --config=build_event_export\n```\n\n\n### Relevant log output\n\n```shell\nFAIL: \/\/tensorflow\/dtensor\/mlir\/tests:spmd_expansion.mlir.test (see \/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/testlogs\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test\/test.log)\r\nINFO: From Testing \/\/tensorflow\/dtensor\/mlir\/tests:spmd_expansion.mlir.test:\r\n==================== Test output for \/\/tensorflow\/dtensor\/mlir\/tests:spmd_expansion.mlir.test:\r\n-- Testing: 1 tests, 1 workers --\r\nFAIL: MLIR tests :: spmd_expansion.mlir (1 of 1)\r\n******************** TEST 'MLIR tests :: spmd_expansion.mlir' FAILED ********************\r\nScript:\r\n--\r\n: 'RUN: at line 1';   \/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test.runfiles\/org_tensorflow\/tensorflow\/dtensor\/mlir\/tests\/dtensor-opt -- \/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test.runfiles\/org_tensorflow\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir -split-input-file -dtensor-annotate-global-shape -dtensor-spmd-expansion -verify-diagnostics | \/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test.runfiles\/llvm-project\/llvm\/FileCheck \/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test.runfiles\/org_tensorflow\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir\r\n--\r\nExit Code: 1\r\n\r\nCommand Output (stderr):\r\n--\r\n2023-07-03 09:20:51.133345: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\nTensorFlow crashed, please file a bug on https:\/\/github.com\/tensorflow\/tensorflow\/issues with the trace below.\r\nStack dump:\r\n0.\tProgram arguments: \/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test.runfiles\/org_tensorflow\/tensorflow\/dtensor\/mlir\/tests\/dtensor-opt \/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test.runfiles\/org_tensorflow\/tensorflow\/dtensor\/mlir\/tests\/dtensor-opt \/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test.runfiles\/org_tensorflow\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir -split-input-file -dtensor-annotate-global-shape -dtensor-spmd-expansion -verify-diagnostics\r\n1.\tProgram arguments: \/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test.runfiles\/org_tensorflow\/tensorflow\/dtensor\/mlir\/tests\/dtensor-opt \/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test.runfiles\/org_tensorflow\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir -split-input-file -dtensor-annotate-global-shape -dtensor-spmd-expansion -verify-diagnostics\r\nStack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):\r\n0  libtensorflow_framework.so.2 0x00007f49dfe75e6e llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 46\r\n1  libtensorflow_framework.so.2 0x00007f49dfe73f07 llvm::sys::RunSignalHandlers() + 87\r\n2  libtensorflow_framework.so.2 0x00007f49dfe76692\r\n3  libpthread.so.0              0x00007f49dea05420\r\n4  dtensor-opt                  0x0000562b92fdd978\r\n5  dtensor-opt                  0x0000562b92fdd11a\r\n6  dtensor-opt                  0x0000562b92fde2b9\r\n7  dtensor-opt                  0x0000562b92f94499\r\n8  dtensor-opt                  0x0000562b92fb7b91\r\n9  dtensor-opt                  0x0000562b92fb85bf\r\n10 dtensor-opt                  0x0000562b92eca421\r\n11 dtensor-opt                  0x0000562b94e800d4\r\n12 dtensor-opt                  0x0000562b94e82d17\r\n13 dtensor-opt                  0x0000562b94e82ae5\r\n14 dtensor-opt                  0x0000562b948df256\r\n15 dtensor-opt                  0x0000562b948de53a\r\n16 dtensor-opt                  0x0000562b9502d603\r\n17 dtensor-opt                  0x0000562b9502d34e\r\n18 dtensor-opt                  0x0000562b948da857\r\n19 dtensor-opt                  0x0000562b948dad3d\r\n20 dtensor-opt                  0x0000562b92d80fcc\r\n21 libc.so.6                    0x00007f49de626083 __libc_start_main + 243\r\n22 dtensor-opt                  0x0000562b92d80d39\r\nStack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):\r\n0  libtensorflow_framework.so.2 0x00007f49dfe75e6e llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 46\r\n1  libtensorflow_framework.so.2 0x00007f49dfe73f37 llvm::sys::RunSignalHandlers() + 135\r\n2  libtensorflow_framework.so.2 0x00007f49dfe76692\r\n3  libpthread.so.0              0x00007f49dea05420\r\n4  dtensor-opt                  0x0000562b92fdd978\r\n5  dtensor-opt                  0x0000562b92fdd11a\r\n6  dtensor-opt                  0x0000562b92fde2b9\r\n7  dtensor-opt                  0x0000562b92f94499\r\n8  dtensor-opt                  0x0000562b92fb7b91\r\n9  dtensor-opt                  0x0000562b92fb85bf\r\n10 dtensor-opt                  0x0000562b92eca421\r\n11 dtensor-opt                  0x0000562b94e800d4\r\n12 dtensor-opt                  0x0000562b94e82d17\r\n13 dtensor-opt                  0x0000562b94e82ae5\r\n14 dtensor-opt                  0x0000562b948df256\r\n15 dtensor-opt                  0x0000562b948de53a\r\n16 dtensor-opt                  0x0000562b9502d603\r\n17 dtensor-opt                  0x0000562b9502d34e\r\n18 dtensor-opt                  0x0000562b948da857\r\n19 dtensor-opt                  0x0000562b948dad3d\r\n20 dtensor-opt                  0x0000562b92d80fcc\r\n21 libc.so.6                    0x00007f49de626083 __libc_start_main + 243\r\n22 dtensor-opt                  0x0000562b92d80d39\r\n\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test.runfiles\/org_tensorflow\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir:277:17: error: CHECK-LABEL: expected string not found in input\r\n\/\/ CHECK-LABEL: module @test_spmd_softmax_rank_3\r\n                ^\r\n<stdin>:153:45: note: scanning from here\r\nmodule @test_spmd_softmax_last_dim_unsharded {\r\n                                            ^\r\n<stdin>:156:303: note: possible intended match here\r\n %0 = \"tf.Softmax\"(%arg0) {_global_shape = [#tf_type.shape<32x32>], _layout = [\"sharding_specs:x,unsharded, mesh:TPU|x=2,y=2|0,1,2,3|0,1,2,3|\/job:localhost\/task:0\/device:TPU:0,\/job:localhost\/task:0\/device:TPU:1,\/job:localhost\/task:0\/device:TPU:2,\/job:localhost\/task:0\/device:TPU:3\"]} : (tensor<16x32xf32>) -> tensor<16x32xf32>\r\n                                                                                                                                                                                                                                                                                                              ^\r\n\r\nInput file: <stdin>\r\nCheck file: \/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir.test.runfiles\/org_tensorflow\/tensorflow\/dtensor\/mlir\/tests\/spmd_expansion.mlir\r\n\r\n-dump-input=help explains the following input dump.\r\n\r\nInput was:\r\n<<<<<<\r\n             .\r\n             .\r\n             .\r\n           148:  }\r\n           149: }\r\n           150:\r\n           151:\r\n           152: \/\/ -----\r\n           153: module @test_spmd_softmax_last_dim_unsharded {\r\nlabel:277'0                                                 X~~ error: no match found\r\n           154:  func.func @main(%arg0: tensor<16x32xf32> {tf._global_shape = #tf_type.shape<32x32>, tf._layout = \"sharding_specs:x,unsharded, mesh:TPU|x=2,y=2|0,1,2,3|0,1,2,3|\/job:localhost\/task:0\/device:TPU:0,\/job:localhost\/task:0\/device:TPU:1,\/job:localhost\/task:0\/device:TPU:2,\/job:localhost\/task:0\/device:TPU:3\"}) {\r\nlabel:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           155:  \"tf_device.cluster\"() ({\r\nlabel:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           156:  %0 = \"tf.Softmax\"(%arg0) {_global_shape = [#tf_type.shape<32x32>], _layout = [\"sharding_specs:x,unsharded, mesh:TPU|x=2,y=2|0,1,2,3|0,1,2,3|\/job:localhost\/task:0\/device:TPU:0,\/job:localhost\/task:0\/device:TPU:1,\/job:localhost\/task:0\/device:TPU:2,\/job:localhost\/task:0\/device:TPU:3\"]} : (tensor<16x32xf32>) -> tensor<16x32xf32>\r\nlabel:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\nlabel:277'1                                                                                                                                                                                                                                                                                                                   ?                         possible intended match\r\n           157:  tf_device.return {_global_shape = [], _layout = []}\r\nlabel:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           158:  }) {_global_shape = [], _mesh = \"TPU|x=2,y=2|0,1,2,3|0,1,2,3|\/job:localhost\/task:0\/device:TPU:0,\/job:localhost\/task:0\/device:TPU:1,\/job:localhost\/task:0\/device:TPU:2,\/job:localhost\/task:0\/device:TPU:3\"} : () -> ()\r\nlabel:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n           159:  return\r\nlabel:277'0     ~~~~~~~~\r\n           160:  }\r\nlabel:277'0     ~~~\r\n           161: }\r\nlabel:277'0     ~~\r\n           162:\r\nlabel:277'0     ~\r\n>>>>>>\r\n\r\n--\r\n\r\n********************\r\n********************\r\nFailed Tests (1):\r\n  MLIR tests :: spmd_expansion.mlir\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux"],"created_at":"2023-07-03T16:06:21Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61164"},{"issue_number":341,"repository":"tensorflow\/tensorflow","title":"Vectorizing a mapping function containing tf.io.read_file in tf.data pipeline is not working.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.12.0-rc1-12-g0db597d0d75 2.12.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nRHEL8 .8\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have a function that reads input files, does some processing and returns the files. Upon using that function to do processing without batching first works fine. i.e. `dataset.map(some_func).batch(batch_size)`\r\n\r\nI was trying to speed up the data pipeline by vectorizing the processing part by batching the dataset and then mapping it to the tf.py_function i.e `dataset.batch(batch_size).map(tf.py_func_wrapped_function)`\r\n\r\nI followed the tensorflow guide for [optimizing pipeline performance](https:\/\/www.tensorflow.org\/guide\/data_performance#vectorizing_mapping)\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport pathlib\r\nimport os\r\nimport matplotlib.pyplot as plt\r\n\r\nflowers = tf.keras.utils.get_file(\r\n    'flower_photos',\r\n    'https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/flower_photos.tgz',\r\n    untar=True)\r\n\r\nflowers = pathlib.Path(flowers)\r\n\r\n\r\nlist_ds = tf.data.Dataset.list_files(str(flowers\/'*\/*'),shuffle=False)\r\n\r\nlist_ds\r\n\r\nfor f in list_ds.take(10):\r\n    print(f.numpy())\r\n\r\n\r\n# Reads an image from a file, decodes it into a dense tensor, and resizes it\r\n# to a fixed shape.\r\ndef parse_image(filename):\r\n  parts = tf.strings.split(filename, os.sep)\r\n  label = parts[-2]\r\n\r\n  image = tf.io.read_file(filename)\r\n  image = tf.image.decode_jpeg(image)\r\n  image = tf.image.convert_image_dtype(image, tf.float32)\r\n  image = tf.image.resize(image, [128, 128])\r\n  return image, label\r\n\r\n\r\ndataset1 = list_ds.map(parse_image).batch(32)#map then batch, scalar mapping\r\n\r\nel = next(iter(dataset1))\r\n\r\nplt.imshow(el[0][0])\r\nplt.title(el[1][0].numpy().decode('utf-8'))\r\n\r\ndataset2 = list_ds.batch(32).map(parse_image) #batch then map(vectorized mapping), should work\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[38], line 1\r\n----> 1 dataset2 = list_ds.batch(32).map(parse_image) #batch then map(vectorized mapping), should work\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/data\/ops\/dataset_ops.py:2240, in DatasetV2.map(self, map_func, num_parallel_calls, deterministic, name)\r\n   2236 # Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\r\n   2237 # dataset_ops).\r\n   2238 # pylint: disable=g-import-not-at-top,protected-access\r\n   2239 from tensorflow.python.data.ops import map_op\r\n-> 2240 return map_op._map_v2(\r\n   2241     self,\r\n   2242     map_func,\r\n   2243     num_parallel_calls=num_parallel_calls,\r\n   2244     deterministic=deterministic,\r\n   2245     name=name)\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/data\/ops\/map_op.py:37, in _map_v2(input_dataset, map_func, num_parallel_calls, deterministic, name)\r\n     34   if deterministic is not None and not debug_mode.DEBUG_MODE:\r\n     35     warnings.warn(\"The `deterministic` argument has no effect unless the \"\r\n     36                   \"`num_parallel_calls` argument is specified.\")\r\n---> 37   return _MapDataset(\r\n     38       input_dataset, map_func, preserve_cardinality=True, name=name)\r\n     39 else:\r\n     40   return _ParallelMapDataset(\r\n     41       input_dataset,\r\n     42       map_func,\r\n   (...)\r\n     45       preserve_cardinality=True,\r\n     46       name=name)\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/data\/ops\/map_op.py:107, in _MapDataset.__init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\r\n    105 self._use_inter_op_parallelism = use_inter_op_parallelism\r\n    106 self._preserve_cardinality = preserve_cardinality\r\n--> 107 self._map_func = structured_function.StructuredFunctionWrapper(\r\n    108     map_func,\r\n    109     self._transformation_name(),\r\n    110     dataset=input_dataset,\r\n    111     use_legacy_function=use_legacy_function)\r\n    112 self._name = name\r\n    113 variant_tensor = gen_dataset_ops.map_dataset(\r\n    114     input_dataset._variant_tensor,  # pylint: disable=protected-access\r\n    115     self._map_func.function.captured_inputs,\r\n   (...)\r\n    118     preserve_cardinality=self._preserve_cardinality,\r\n    119     **self._common_args)\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/data\/ops\/structured_function.py:261, in StructuredFunctionWrapper.__init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\r\n    254       warnings.warn(\r\n    255           \"Even though the `tf.config.experimental_run_functions_eagerly` \"\r\n    256           \"option is set, this option does not apply to tf.data functions. \"\r\n    257           \"To force eager execution of tf.data functions, please use \"\r\n    258           \"`tf.data.experimental.enable_debug_mode()`.\")\r\n    259     fn_factory = trace_tf_function(defun_kwargs)\r\n--> 261 self._function = fn_factory()\r\n    262 # There is no graph to add in eager mode.\r\n    263 add_to_graph &= not context.executing_eagerly()\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py:232, in TracingCompiler.get_concrete_function(self, *args, **kwargs)\r\n    223 def get_concrete_function(self, *args, **kwargs):\r\n    224   \"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\r\n    225 \r\n    226   Args:\r\n   (...)\r\n    230       `tf.Tensor` or `tf.TensorSpec`.\r\n    231   \"\"\"\r\n--> 232   concrete_function = self._get_concrete_function_garbage_collected(\r\n    233       *args, **kwargs)\r\n    234   concrete_function._garbage_collector.release()  # pylint: disable=protected-access\r\n    235   return concrete_function\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py:202, in TracingCompiler._get_concrete_function_garbage_collected(self, *args, **kwargs)\r\n    199   self._function_spec.make_canonicalized_monomorphic_type(args, kwargs)\r\n    201 with self._lock:\r\n--> 202   concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)\r\n    203   seen_names = set()\r\n    204   concrete_function._arg_keywords = []  # pylint: disable=protected-access\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py:166, in TracingCompiler._maybe_define_concrete_function(self, args, kwargs)\r\n    163   args = self.input_signature\r\n    164   kwargs = {}\r\n--> 166 return self._maybe_define_function(args, kwargs)\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py:396, in TracingCompiler._maybe_define_function(self, args, kwargs)\r\n    393   args = placeholder_bound_args.args\r\n    394 kwargs = placeholder_bound_args.kwargs\r\n--> 396 concrete_function = self._create_concrete_function(\r\n    397     args, kwargs, func_graph)\r\n    399 # TODO(b\/263520817): Remove access to private attribute.\r\n    400 graph_capture_container = concrete_function.graph._function_captures  # pylint: disable=protected-access\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py:300, in TracingCompiler._create_concrete_function(self, args, kwargs, func_graph)\r\n    296 else:\r\n    297   arg_names = base_arg_names\r\n    299 concrete_function = monomorphic_function.ConcreteFunction(\r\n--> 300     func_graph_module.func_graph_from_py_func(\r\n    301         self._name,\r\n    302         self._python_function,\r\n    303         args,\r\n    304         kwargs,\r\n    305         None,\r\n    306         func_graph=func_graph,\r\n    307         autograph=self._autograph,\r\n    308         autograph_options=self._autograph_options,\r\n    309         arg_names=arg_names,\r\n    310         capture_by_value=self._capture_by_value,\r\n    311         create_placeholders=False),\r\n    312     self._function_attributes,\r\n    313     spec=self.function_spec,\r\n    314     # Tell the ConcreteFunction to clean up its graph once it goes out of\r\n    315     # scope. This is not the default behavior since it gets used in some\r\n    316     # places (like Keras) where the FuncGraph lives longer than the\r\n    317     # ConcreteFunction.\r\n    318     shared_func_graph=False)\r\n    319 return concrete_function\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/framework\/func_graph.py:1214, in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\r\n   1211 else:\r\n   1212   _, original_func = tf_decorator.unwrap(python_func)\r\n-> 1214 func_outputs = python_func(*func_args, **func_kwargs)\r\n   1216 # invariant: `func_outputs` contains only Tensors, CompositeTensors,\r\n   1217 # TensorArrays and `None`s.\r\n   1218 func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/data\/ops\/structured_function.py:238, in StructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn(*args)\r\n    232 @eager_function.defun_with_attributes(\r\n    233     input_signature=structure.get_flat_tensor_specs(\r\n    234         self._input_structure),\r\n    235     autograph=False,\r\n    236     attributes=defun_kwargs)\r\n    237 def wrapped_fn(*args):  # pylint: disable=missing-docstring\r\n--> 238   ret = wrapper_helper(*args)\r\n    239   ret = structure.to_tensor_list(self._output_structure, ret)\r\n    240   return [ops.convert_to_tensor(t) for t in ret]\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/data\/ops\/structured_function.py:169, in StructuredFunctionWrapper.__init__.<locals>.wrapper_helper(*args)\r\n    167 if not _should_unpack(nested_args):\r\n    168   nested_args = (nested_args,)\r\n--> 169 ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)\r\n    170 ret = variable_utils.convert_variables_to_tensors(ret)\r\n    171 if _should_pack(ret):\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py:692, in convert.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\r\n    690 except Exception as e:  # pylint:disable=broad-except\r\n    691   if hasattr(e, 'ag_error_metadata'):\r\n--> 692     raise e.ag_error_metadata.to_exception(e)\r\n    693   else:\r\n    694     raise\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py:689, in convert.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\r\n    687 try:\r\n    688   with conversion_ctx:\r\n--> 689     return converted_call(f, args, kwargs, options=options)\r\n    690 except Exception as e:  # pylint:disable=broad-except\r\n    691   if hasattr(e, 'ag_error_metadata'):\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)\r\n    437 try:\r\n    438   if kwargs is not None:\r\n--> 439     result = converted_f(*effective_args, **kwargs)\r\n    440   else:\r\n    441     result = converted_f(*effective_args)\r\n\r\nFile \/tmp\/__autograph_generated_filezgyxt99w.py:12, in outer_factory.<locals>.inner_factory.<locals>.tf__parse_image(filename)\r\n     10 parts = ag__.converted_call(ag__.ld(tf).strings.split, (ag__.ld(filename), ag__.ld(os).sep), None, fscope)\r\n     11 label = ag__.ld(parts)[-2]\r\n---> 12 image = ag__.converted_call(ag__.ld(tf).io.read_file, (ag__.ld(filename),), None, fscope)\r\n     13 image = ag__.converted_call(ag__.ld(tf).image.decode_jpeg, (ag__.ld(image),), None, fscope)\r\n     14 image = ag__.converted_call(ag__.ld(tf).image.convert_image_dtype, (ag__.ld(image), ag__.ld(tf).float32), None, fscope)\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py:331, in converted_call(f, args, kwargs, caller_fn_scope, options)\r\n    329 if conversion.is_in_allowlist_cache(f, options):\r\n    330   logging.log(2, 'Allowlisted %s: from cache', f)\r\n--> 331   return _call_unconverted(f, args, kwargs, options, False)\r\n    333 if ag_ctx.control_status_ctx().status == ag_ctx.Status.DISABLED:\r\n    334   logging.log(2, 'Allowlisted: %s: AutoGraph is disabled in context', f)\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py:459, in _call_unconverted(f, args, kwargs, options, update_cache)\r\n    457 if kwargs is not None:\r\n    458   return f(*args, **kwargs)\r\n--> 459 return f(*args)\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/ops\/io_ops.py:133, in read_file(filename, name)\r\n     96 @tf_export(\"io.read_file\", v1=[\"io.read_file\", \"read_file\"])\r\n     97 def read_file(filename, name=None):\r\n     98   \"\"\"Reads the contents of file.\r\n     99 \r\n    100   This operation returns a tensor with the entire contents of the input\r\n   (...)\r\n    131     A tensor of dtype \"string\", with the file contents.\r\n    132   \"\"\"\r\n--> 133   return gen_io_ops.read_file(filename, name)\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/ops\/gen_io_ops.py:586, in read_file(filename, name)\r\n    584     pass  # Add nodes to the TensorFlow graph.\r\n    585 # Add nodes to the TensorFlow graph.\r\n--> 586 _, _, _op, _outputs = _op_def_library._apply_op_helper(\r\n    587       \"ReadFile\", filename=filename, name=name)\r\n    588 _result = _outputs[:]\r\n    589 if _execute.must_record_gradient():\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/framework\/op_def_library.py:795, in _apply_op_helper(op_type_name, name, **keywords)\r\n    790 must_colocate_inputs = [val for arg, val in zip(op_def.input_arg, inputs)\r\n    791                         if arg.is_ref]\r\n    792 with _MaybeColocateWith(must_colocate_inputs):\r\n    793   # Add Op to graph\r\n    794   # pylint: disable=protected-access\r\n--> 795   op = g._create_op_internal(op_type_name, inputs, dtypes=None,\r\n    796                              name=scope, input_types=input_types,\r\n    797                              attrs=attr_protos, op_def=op_def)\r\n    799 # `outputs` is returned as a separate return value so that the output\r\n    800 # tensors can the `op` per se can be decoupled so that the\r\n    801 # `op_callbacks` can function properly. See framework\/op_callbacks.py\r\n    802 # for more details.\r\n    803 outputs = op.outputs\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/framework\/func_graph.py:707, in FuncGraph._create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\r\n    705   inp = self.capture(inp)\r\n    706   captured_inputs.append(inp)\r\n--> 707 return super()._create_op_internal(  # pylint: disable=protected-access\r\n    708     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\r\n    709     compute_device)\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/framework\/ops.py:3814, in Graph._create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\r\n   3811 # _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\r\n   3812 # Session.run call cannot occur between creating and mutating the op.\r\n   3813 with self._mutation_lock():\r\n-> 3814   ret = Operation(\r\n   3815       node_def,\r\n   3816       self,\r\n   3817       inputs=inputs,\r\n   3818       output_types=dtypes,\r\n   3819       control_inputs=control_inputs,\r\n   3820       input_types=input_types,\r\n   3821       original_op=self._default_original_op,\r\n   3822       op_def=op_def)\r\n   3823   self._create_op_helper(ret, compute_device=compute_device)\r\n   3824 return ret\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/framework\/ops.py:2112, in Operation.__init__(***failed resolving arguments***)\r\n   2109     control_input_ops.append(control_op)\r\n   2111 # Initialize c_op from node_def and other inputs\r\n-> 2112 c_op = _create_c_op(g, node_def, inputs, control_input_ops, op_def=op_def)\r\n   2113 self._init_from_c_op(c_op=c_op, g=g)\r\n   2115 self._original_op = original_op\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/util\/traceback_utils.py:153, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n    151 except Exception as e:\r\n    152   filtered_tb = _process_traceback_frames(e.__traceback__)\r\n--> 153   raise e.with_traceback(filtered_tb) from None\r\n    154 finally:\r\n    155   del filtered_tb\r\n\r\nFile ~\/anaconda3\/envs\/tf2_12\/lib\/python3.10\/site-packages\/tensorflow\/python\/framework\/ops.py:1973, in _create_c_op(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\r\n   1970   c_op = pywrap_tf_session.TF_FinishOperation(op_desc)\r\n   1971 except errors.InvalidArgumentError as e:\r\n   1972   # Convert to ValueError for backwards compatibility.\r\n-> 1973   raise ValueError(e.message)\r\n   1975 # Record the current Python stack trace as the creating stacktrace of this\r\n   1976 # TF_Operation.\r\n   1977 if extract_traceback:\r\n\r\nValueError: in user code:\r\n\r\n    File \"\/tmp\/ipykernel_26855\/3929380215.py\", line 28, in parse_image  *\r\n        image = tf.io.read_file(filename)\r\n\r\n    ValueError: Shape must be rank 0 but is rank 1 for '{{node ReadFile}} = ReadFile[](args_0)' with input shapes: [?].\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","comp:data","TF 2.12"],"created_at":"2023-07-03T13:08:00Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61162"},{"issue_number":342,"repository":"tensorflow\/tensorflow","title":"[MLIR] Fix tf.StridedSlice lowering to tosa with new_axis_mask\/shrink_axis_mask","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.13.0rc2\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubunto 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.1\n\n### Bazel version\n\n6.1.2\n\n### GCC\/compiler version\n\nclang 15.0.2\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe following MLIR input is not support:\r\n\r\n```\r\nfunc.func @test_strided_slice_new_axis_mask(%arg0: tensor<1x14x8xf32>) -> tensor<1x14x8x1xf32> {\r\n  %strides = \"tf.Const\"() {device = \"\", value = dense<1> : tensor<4xi32>} : () -> tensor<4xi32>\r\n  %begin_end = \"tf.Const\"() {device = \"\", value = dense<0> : tensor<4xi32>} : () -> tensor<4xi32>\r\n  %res = \"tf.StridedSlice\"(%arg0, %begin_end, %begin_end, %strides) {begin_mask = 7 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 8 : i64, shrink_axis_mask = 0 : i64} : (tensor<1x14x8xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<1x14x8x1xf32>\r\n  func.return %res : tensor<1x14x8x1xf32>\r\n}\r\n\r\n\/\/ -----\r\n\r\nfunc.func @test_strided_slice_shrink_axis_mask(%arg0: tensor<1x14x8x1xf32>) -> tensor<1x14x8xf32> {\r\n  %strides = \"tf.Const\"() {device = \"\", value = dense<1> : tensor<4xi32>} : () -> tensor<4xi32>\r\n  %begin = \"tf.Const\"() {device = \"\", value = dense<0> : tensor<4xi32>} : () -> tensor<4xi32>\r\n  %end = \"tf.Const\"() {device = \"\", value = dense<[0, 0, 0, 1]> : tensor<4xi32>} : () -> tensor<4xi32>\r\n  %res = \"tf.StridedSlice\"(%arg0, %begin, %end, %strides) {begin_mask = 7 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 8 : i64} : (tensor<1x14x8x1xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<1x14x8xf32>\r\n  func.return %res : tensor<1x14x8xf32>\r\n}\r\n```\n\n### Standalone code to reproduce the issue\n\n```shell\nUse MLIR new tests in https:\/\/github.com\/tensorflow\/tensorflow\/pull\/60939 to see the error.\n```\n\n\n### Relevant log output\n\n```shell\nI create pull request to fix the issue:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/pull\/60939\n```\n","labels":["type:bug","comp:ops","awaiting PR merge","TF 2.13"],"created_at":"2023-07-03T07:56:16Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61157"},{"issue_number":343,"repository":"tensorflow\/tensorflow","title":"Internal error: Error applying delegate when trying to use TensorFlow Lite NNAPI delegate on Google Pixel 7","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\ntensorflow-lite 2.12.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\n_No response_\n\n### Mobile device\n\nGoogle Pixel 7\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nWhen I try to use the NNAPI delegate to run the OpenAI whisper model on a Pixel 7, it crashes when trying to initialize the TFLite interpreter.\n\n### Standalone code to reproduce the issue\n\n```shell\nMainActivity.kt\r\nclass MainActivity : ComponentActivity() {\r\n    override fun onCreate(savedInstanceState: Bundle?) {\r\n        super.onCreate(savedInstanceState)\r\n\r\n        setContent {\r\n            NNAPIWhisperTestTheme {\r\n                \/\/ A surface container using the 'background' color from the theme\r\n                Surface(\r\n                    modifier = Modifier.fillMaxSize(),\r\n                    color = MaterialTheme.colorScheme.background\r\n                ) {\r\n                    Test(applicationContext)\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n@Composable\r\nfun Test(applicationContext: Context, modifier: Modifier = Modifier, viewModel: NnApiViewModel = viewModel()) {\r\n    Scaffold { innerPadding ->\r\n        Column {\r\n            Button(modifier = Modifier.padding(innerPadding), onClick = { viewModel.initialize(applicationContext) }) {\r\n                Text(text = \"INITIALIZE\")\r\n            }\r\n            Button(\r\n                onClick = {\r\n                    viewModel.runInference()\r\n                },\r\n                modifier = Modifier.padding(innerPadding)\r\n            ) {\r\n                Text(viewModel.output + viewModel.elapsed)\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n\r\n\r\nNnApiViewModel.kt\r\nclass NnApiViewModel : ViewModel() {\r\n\r\n    var options = Interpreter.Options()\r\n    var nnApiDelegate: NnApiDelegate? = null\r\n    var tfLite: Interpreter? = null\r\n    var output by mutableStateOf(\"\")\r\n    var elapsed by mutableStateOf(0L)\r\n\r\n    \/\/ Initialize interpreter with NNAPI delegate for Android Pie or above\r\n    fun initialize(applicationContext: Context) {\r\n        nnApiDelegate = NnApiDelegate()\r\n        options.useNNAPI = true\r\n        NnApiDelegate.Options().useNnapiCpu = false\r\n        options.addDelegate(nnApiDelegate)\r\n\r\n        val model = applicationContext.assets.open(\"models\/whisper-tiny.tflite\")\r\n        val file = model.readBytes()\r\n\r\n        val fileName = \"whisper-tiny.tflite\"\r\n        applicationContext.openFileOutput(fileName, Context.MODE_PRIVATE).use {\r\n            it.write(file)\r\n        }\r\n\r\n        val modelFile = File(applicationContext.filesDir,\"whisper-tiny.tflite\")\r\n\r\n        \/\/ Initialize TFLite interpreter\r\n        try {\r\n            tfLite = Interpreter(modelFile, options)\r\n        } catch (e: Exception) {\r\n            throw RuntimeException(e)\r\n        }\r\n    }\r\n\r\n    fun runInference() {\r\n        try {\r\n            val outputShape = tfLite?.getOutputTensor(0)\r\n            val input = TensorBuffer.createFixedSize(intArrayOf(1, 80, 3000), DataType.FLOAT32)\r\n            val output = TensorBuffer.createFixedSize(outputShape?.shape(), DataType.FLOAT32)\r\n\r\n            val start = System.currentTimeMillis()\r\n            tfLite?.run(input.buffer, output.buffer)\r\n            elapsed = System.currentTimeMillis() - start\r\n        } catch (e: RuntimeException) {\r\n            throw RuntimeException(e)\r\n        }\r\n    }\r\n\r\n    fun unload() {\r\n        tfLite?.close()\r\n        nnApiDelegate?.close()\r\n    }\r\n}\n```\n\n\n### Relevant log output\n\n```shell\nI  Loaded native library: tensorflowlite_jni\r\nI  Didn't load native library: tensorflowlite_jni_gms_client\r\nI  Initialized TensorFlow Lite runtime.\r\nI  DeviceManager::DeviceManager\r\nI  findAvailableDevices\r\nE  Error opening trace file: No such file or directory (2)\r\nI  Found interface google-edgetpu (version = 2.0)\r\nI  Found interface google-armnn (version = ArmNN)\r\nI  Created TensorFlow Lite delegate for NNAPI.\r\nE  FATAL EXCEPTION: main                                                                                                                                                                                                   java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Error applying delegate:\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","TFLiteNNAPIDelegate","TF 2.12"],"created_at":"2023-06-30T04:21:02Z","comments":18,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61126"},{"issue_number":344,"repository":"tensorflow\/tensorflow","title":"`tf.math.reduce_prod` produces wrong second-order gradient","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.14.0-dev20230628\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n`tf.math.reduce_prod` produces wrong second-order gradient when the input tensor has a `0` element. \r\n\r\nIn the following example, `d2z_dydx` should be `1`, but the output value is `0`. Note that, if we replace `z = tf.math.reduce_prod(tf.stack([x, y]))` with `z = x * y`, the assertion passes.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nx = tf.Variable(0.0)\r\ny = tf.Variable(1.0)\r\nwith tf.GradientTape(persistent=True) as t1:\r\n    t1.watch(x)\r\n    t1.watch(y)\r\n    with tf.GradientTape(persistent=True) as t2:\r\n        t2.watch(x)\r\n        t2.watch(y)\r\n        z = tf.math.reduce_prod(tf.stack([x, y]))\r\n    print('z = x * y: ', z)  \r\n    dz_dx = t2.gradient(z, x) # dz_dx = y\r\n    print('dz_dx: ', dz_dx)\r\n    dz_dy = t2.gradient(z, y) # dz_dy = x\r\n    print('dz_dy: ', dz_dy)\r\n\r\nd2z_dx2 = t1.gradient(dz_dx, x) # d2z_dx2 = 0\r\nprint('d2z_dx2', d2z_dx2)\r\nd2z_dxdy = t1.gradient(dz_dx, y) # d2z_dxdy = 1\r\nprint('d2z_dxdy', d2z_dxdy)\r\nassert d2z_dxdy == 1\r\nd2z_dydx = t1.gradient(dz_dy, x) # d2z_dydx = 1\r\nprint('d2z_dydx', d2z_dydx)\r\nassert d2z_dydx == 1, 'd2z_dydx = {}'.format(d2z_dydx)\r\n# AssertionError: d2z_dydx = 0.0\n```\n\n\n### Relevant log output\n\n```shell\nAssertionError: d2z_dydx = 0.0\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-06-28T16:17:39Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61117"},{"issue_number":345,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/distribute\/experimental\/rpc:rpc_ops_test is flaky","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n6.1.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\n\/\/tensorflow\/python\/distribute\/experimental\/rpc:rpc_ops_test sometimes fails.\r\n\r\nx86 log\r\nhttps:\/\/source.cloud.google.com\/results\/invocations\/e7ac5e31-66d5-4f2e-a095-045cb52cc20f\/log\r\n\r\nAARCH64 log\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5348094575\/jobs\/9697499180#step:5:8263\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel --bazelrc=\/usertools\/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https:\/\/storage.googleapis.com\/tensorflow-devinfra-bazel-cache\/norbe --google_default_credentials\n```\n\n\n### Relevant log output\n\n```shell\nFAIL: \/\/tensorflow\/python\/distribute\/experimental\/rpc:rpc_ops_test (shard 4 of 7) (see \/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/testlogs\/tensorflow\/python\/distribute\/experimental\/rpc\/rpc_ops_test\/shard_4_of_7\/test.log)\r\nINFO: From Testing \/\/tensorflow\/python\/distribute\/experimental\/rpc:rpc_ops_test (shard 4 of 7):\r\n==================== Test output for \/\/tensorflow\/python\/distribute\/experimental\/rpc:rpc_ops_test (shard 4 of 7):\r\n2023-06-27 21:51:01.865535: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-27 21:51:01.915964: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nRunning tests under Python 3.9.17: \/usr\/bin\/python3\r\n[ RUN      ] RpcOpsTest.test_client_timeout\r\nE0627 21:51:07.067311728 3191958 server_chttp2.cc:40]        {\"created\":\"@1687902667.067282680\",\"description\":\"Only 1 addresses added out of total 2 resolved\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/ext\/transport\/chttp2\/server\/chttp2_server.cc\",\"file_line\":404,\"referenced_errors\":[{\"created\":\"@1687902667.067278835\",\"description\":\"Address family not supported by protocol\",\"errno\":97,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/socket_utils_common_posix.cc\",\"file_line\":420,\"os_error\":\"Address family not supported by protocol\",\"syscall\":\"socket\",\"target_address\":\"[::1]:34603\"}]}\r\n2023-06-27 21:51:07.067457: I tensorflow\/distribute\/experimental\/rpc\/kernels\/rpc_ops.cc:347] Server listening on: localhost:34603\r\n2023-06-27 21:51:08.565889: I tensorflow\/distribute\/experimental\/rpc\/kernels\/rpc_ops.cc:314] Shutting down server listening on: localhost:34603\r\nINFO:tensorflow:time(__main__.RpcOpsTest.test_client_timeout): 2.71s\r\nI0627 21:51:08.574481 139721505019712 test_util.py:2464] time(__main__.RpcOpsTest.test_client_timeout): 2.71s\r\n[  FAILED  ] RpcOpsTest.test_client_timeout\r\n[ RUN      ] RpcOpsTest.test_queue_resource\r\n\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/experimental\/rpc\/rpc_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/saved_model\/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tf.NoneTensorSpec; loading this StructuredValue will require that this type be imported and registered.\r\n  warnings.warn(\"Encoding a StructuredValue with type %s; loading this \"\r\nE0627 21:51:08.630379336 3163442 server_chttp2.cc:40]        {\"created\":\"@1687902668.630352103\",\"description\":\"Only 1 addresses added out of total 2 resolved\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/ext\/transport\/chttp2\/server\/chttp2_server.cc\",\"file_line\":404,\"referenced_errors\":[{\"created\":\"@1687902668.630348956\",\"description\":\"Address family not supported by protocol\",\"errno\":97,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/socket_utils_common_posix.cc\",\"file_line\":420,\"os_error\":\"Address family not supported by protocol\",\"syscall\":\"socket\",\"target_address\":\"[::1]:43377\"}]}\r\n2023-06-27 21:51:08.630532: I tensorflow\/distribute\/experimental\/rpc\/kernels\/rpc_ops.cc:347] Server listening on: localhost:43377\r\n2023-06-27 21:51:08.686495: I tensorflow\/distribute\/experimental\/rpc\/kernels\/rpc_ops.cc:314] Shutting down server listening on: localhost:43377\r\nINFO:tensorflow:time(__main__.RpcOpsTest.test_queue_resource): 0.14s\r\nI0627 21:51:08.712013 139721505019712 test_util.py:2464] time(__main__.RpcOpsTest.test_queue_resource): 0.14s\r\n[       OK ] RpcOpsTest.test_queue_resource\r\n[ RUN      ] RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods\r\nE0627 21:51:08.774584099 3163442 server_chttp2.cc:40]        {\"created\":\"@1687902668.774551355\",\"description\":\"Only 1 addresses added out of total 2 resolved\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/ext\/transport\/chttp2\/server\/chttp2_server.cc\",\"file_line\":404,\"referenced_errors\":[{\"created\":\"@1687902668.774547071\",\"description\":\"Address family not supported by protocol\",\"errno\":97,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/socket_utils_common_posix.cc\",\"file_line\":420,\"os_error\":\"Address family not supported by protocol\",\"syscall\":\"socket\",\"target_address\":\"[::1]:39177\"}]}\r\n2023-06-27 21:51:08.774736: I tensorflow\/distribute\/experimental\/rpc\/kernels\/rpc_ops.cc:347] Server listening on: localhost:39177\r\n2023-06-27 21:51:08.798979: I tensorflow\/distribute\/experimental\/rpc\/kernels\/rpc_ops.cc:314] Shutting down server listening on: localhost:39177\r\nINFO:tensorflow:time(__main__.RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods): 0.09s\r\nI0627 21:51:08.802805 139721505019712 test_util.py:2464] time(__main__.RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods): 0.09s\r\n[       OK ] RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods\r\n======================================================================\r\nERROR: test_client_timeout (__main__.RpcOpsTest)\r\nRpcOpsTest.test_client_timeout\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/experimental\/rpc\/rpc_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/experimental\/rpc\/rpc_ops_test.py\", line 487, in test_client_timeout\r\n    client = rpc_ops.GrpcClient(\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/experimental\/rpc\/rpc_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/experimental\/rpc\/rpc_ops.py\", line 342, in __init__\r\n    self._client_handle, methods = gen_rpc_ops.rpc_client(\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/experimental\/rpc\/rpc_ops_test.runfiles\/org_tensorflow\/tensorflow\/distribute\/experimental\/rpc\/kernels\/gen_rpc_ops.py\", line 333, in rpc_client\r\n    return rpc_client_eager_fallback(\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/experimental\/rpc\/rpc_ops_test.runfiles\/org_tensorflow\/tensorflow\/distribute\/experimental\/rpc\/kernels\/gen_rpc_ops.py\", line 407, in rpc_client_eager_fallback\r\n    _result = _execute.execute(b\"RpcClient\", 2, inputs=_inputs_flat,\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/experimental\/rpc\/rpc_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/eager\/execute.py\", line 53, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.UnavailableError: {{function_node __wrapped__RpcClient_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} GOAWAY received\r\nAdditional GRPC error information while calling \/tensorflow.rpc.RpcService\/List:\r\n:{\"created\":\"@1687902668.574079798\",\"description\":\"Error received from peer ipv4:127.0.0.1:34603\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/surface\/call.cc\",\"file_line\":1056,\"grpc_message\":\"GOAWAY received\",\"grpc_status\":14} [Op:RpcClient]\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","subtype: ubuntu\/linux","TF 2.13"],"created_at":"2023-06-28T15:54:29Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61115"},{"issue_number":346,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/data\/experimental\/kernel_tests\/service:local_workers_test is flaky","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n6.1.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\n\/\/tensorflow\/python\/data\/experimental\/kernel_tests\/service:local_workers_test sometimes fails or timeouts.\r\n\r\nx86 log\r\nhttps:\/\/source.cloud.google.com\/results\/invocations\/e41a9dd4-19a3-4298-b34f-6a32eca50e08\/log\r\n\r\nAARCH64 log\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5383187293\/jobs\/9769619751#step:5:8335\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel --bazelrc=\/usertools\/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https:\/\/storage.googleapis.com\/tensorflow-devinfra-bazel-cache\/norbe --google_default_credentials\n```\n\n\n### Relevant log output\n\n```shell\nINFO: From Testing \/\/tensorflow\/python\/data\/experimental\/kernel_tests\/service:local_workers_test (shard 1 of 24):\r\n==================== Test output for \/\/tensorflow\/python\/data\/experimental\/kernel_tests\/service:local_workers_test (shard 1 of 24):\r\n2023-06-27 23:02:53.397107: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-27 23:02:53.528865: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nRunning tests under Python 3.9.17: \/usr\/bin\/python3\r\n[ RUN      ] LocalTaskGarbageCollectTest.testMultipleEpochsSharedJob_test_mode_eager_tfapiversion_1_numremoteworkers_0\r\n[  SKIPPED ] LocalTaskGarbageCollectTest.testMultipleEpochsSharedJob_test_mode_eager_tfapiversion_1_numremoteworkers_0\r\n[ RUN      ] LocalTaskGarbageCollectTest.testReadFromDeletedTask_test_mode_eager_tfapiversion_1_numremoteworkers_0\r\n[  SKIPPED ] LocalTaskGarbageCollectTest.testReadFromDeletedTask_test_mode_eager_tfapiversion_1_numremoteworkers_0\r\n[ RUN      ] LocalWorkersTest.testAnonymousJobWithDifferentTargetWorkers_test_mode_graph_tfapiversion_2\r\nINFO:tensorflow:Using local port 43055\r\nI0627 23:02:57.097305 139801719478080 test_util.py:3796] Using local port 43055\r\n2023-06-27 23:02:57.099356: I tensorflow\/core\/data\/service\/dispatcher_impl.cc:223] Attempting to restore dispatcher state from journal in \/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/_tmp\/dbc16dd8be682dbdabf0b589b8e98f16j99fj6xd\/tmptm5ix2lv\/tf_data_dispatcher_journal\r\n2023-06-27 23:02:57.099421: I tensorflow\/core\/data\/service\/dispatcher_impl.cc:230] No journal found. Starting dispatcher from new state.\r\n2023-06-27 23:02:57.099574: I tensorflow\/core\/data\/service\/server_lib.cc:82] Started tf.data DispatchServer running at 0.0.0.0:43055\r\nINFO:tensorflow:Using local port 44899\r\nI0627 23:02:57.099842 139801719478080 test_util.py:3796] Using local port 44899\r\n2023-06-27 23:02:57.101683: I tensorflow\/core\/data\/service\/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055\r\n2023-06-27 23:02:57.101828: I tensorflow\/core\/data\/service\/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:44899\r\nINFO:tensorflow:Using local port 41615\r\nI0627 23:02:57.102149 139801719478080 test_util.py:3796] Using local port 41615\r\n2023-06-27 23:02:57.103351: I tensorflow\/core\/data\/service\/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055\r\n2023-06-27 23:02:57.103477: I tensorflow\/core\/data\/service\/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:41615\r\nINFO:tensorflow:Using local port 37691\r\nI0627 23:02:57.103664 139801719478080 test_util.py:3796] Using local port 37691\r\n2023-06-27 23:02:57.104726: I tensorflow\/core\/data\/service\/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055\r\n2023-06-27 23:02:57.104854: I tensorflow\/core\/data\/service\/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:37691\r\nINFO:tensorflow:Using local port 39883\r\nI0627 23:02:57.107191 139801719478080 test_util.py:3796] Using local port 39883\r\n2023-06-27 23:02:57.584682: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-27 23:02:57.619502: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-27 23:02:57.634234: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-06-27 23:02:57.788004: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-06-27 23:02:59.175789: I tensorflow\/core\/data\/service\/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055\r\n2023-06-27 23:02:59.176006: I tensorflow\/core\/data\/service\/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:39883\r\nINFO:tensorflow:Using local port 34849\r\nI0627 23:02:59.176878 139801719478080 test_util.py:3796] Using local port 34849\r\n2023-06-27 23:02:59.252223: I tensorflow\/core\/data\/service\/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055\r\n2023-06-27 23:02:59.252462: I tensorflow\/core\/data\/service\/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:34849\r\nINFO:tensorflow:Using local port 46439\r\nI0627 23:02:59.253159 139801719478080 test_util.py:3796] Using local port 46439\r\n2023-06-27 23:02:59.287010: I tensorflow\/core\/data\/service\/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055\r\n2023-06-27 23:02:59.287229: I tensorflow\/core\/data\/service\/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:46439\r\n\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/ops\/dataset_ops.py:458: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\r\n  warnings.warn(\"To make it possible to preserve tf.data options across \"\r\nWARNING:tensorflow:From \/usr\/lib\/python3.9\/contextlib.py:87: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `self.session()` or `self.cached_session()` instead.\r\nW0627 23:02:59.486639 139801719478080 deprecation.py:364] From \/usr\/lib\/python3.9\/contextlib.py:87: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse `self.session()` or `self.cached_session()` instead.\r\n2023-06-27 23:02:59.508867: I tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled\r\n-- Test timed out at 2023-06-27 23:07:52 UTC --\r\nCurrent thread 0x00007f261fd41740 (most recent call first):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1477 in _call_tf_sessionrun\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1384 in _run_fn\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1401 in _do_call\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1394 in _do_run\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1214 in _run\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 971 in run\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_util.py\", line 2061 in run\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_util.py\", line 2691 in evaluate\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/kernel_tests\/test_base.py\", line 237 in assertDatasetProduces\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.py\", line 238 in testAnonymousJobWithDifferentTargetWorkers\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_combinations.py\", line 343 in execute_test_method\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_combinations.py\", line 360 in decorated\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/absl_py\/absl\/testing\/parameterized.py\", line 314 in bound_param_test\r\n  File \"\/usr\/lib\/python3.9\/unittest\/case.py\", line 550 in _callTestMethod\r\n  File \"\/usr\/lib\/python3.9\/unittest\/case.py\", line 592 in run\r\n  File \"\/usr\/lib\/python3.9\/unittest\/case.py\", line 651 in __call__\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 122 in run\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 84 in __call__\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 122 in run\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 84 in __call__\r\n  File \"\/usr\/lib\/python3.9\/unittest\/runner.py\", line 184 in run\r\n  File \"\/usr\/lib\/python3.9\/unittest\/main.py\", line 271 in runTests\r\n  File \"\/usr\/lib\/python3.9\/unittest\/main.py\", line 101 in __init__\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2537 in _run_and_get_tests_result\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2568 in run_tests\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2156 in _run_in_app\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2049 in main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 51 in g_main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/absl_py\/absl\/app.py\", line 258 in _run_main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/absl_py\/absl\/app.py\", line 312 in run\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 60 in main_wrapper\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/benchmark.py\", line 489 in benchmarks_main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 62 in main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/test.py\", line 56 in main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/eager\/test.py\", line 25 in main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_lib.py\", line 167 in test_main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/multi_process_cluster.py\", line 165 in test_main\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/local_workers_test.py\", line 441 in <module>\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux","TF 2.13"],"created_at":"2023-06-28T15:34:17Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61113"},{"issue_number":347,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/distribute:vars_test_2gpu is flaky","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n6.1.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\n\/\/tensorflow\/python\/distribute:vars_test_2gpu timeouts sometimes.\r\n\r\nx86 log\r\nhttps:\/\/source.cloud.google.com\/results\/invocations\/769764d8-8dc9-46fa-a284-78062efe3bd9\/log\r\n\r\nAARCH64 log\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5363572382\/jobs\/9731245377#step:5:9313\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel --bazelrc=\/usertools\/cpu.bazelrc test --config=rbe --config=pycpp --config=build_event_export\n```\n\n\n### Relevant log output\n\n```shell\n[ RUN      ] SyncOnReadScatterReplicaTest.testScatterMax_test_aggregation_VariableAggregationMEAN_distribution_MultiWorkerMirrored2x1CPU_mode_graph_usevarpolicy_True\r\nW0628 01:02:37.513197 140343714740032 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.\r\nWARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\r\nW0628 01:02:37.513532 140343714740032 collective_all_reduce_strategy.py:394] Collective ops is not configured at program startup. Some performance features may not be enabled.\r\nINFO:tensorflow:Using MirroredStrategy with devices ('\/device:CPU:0',)\r\nI0628 01:02:37.516878 140343714740032 mirrored_strategy.py:423] Using MirroredStrategy with devices ('\/device:CPU:0',)\r\nINFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('\/device:CPU:0',), communication = CommunicationImplementation.AUTO\r\nI0628 01:02:37.517337 140343714740032 collective_all_reduce_strategy.py:446] Single-worker MultiWorkerMirroredStrategy with local_devices = ('\/device:CPU:0',), communication = CommunicationImplementation.AUTO\r\n[chief-0]:     W0628 01:02:37.520201 140325400614720 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.\r\nI0628 01:02:37.521484 140343714740032 multi_process_runner.py:989] Waiting for the result from chief-0\r\n[worker-0]:    W0628 01:02:37.523296 140325400614720 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.\r\n[chief-0]:     W0628 01:02:37.521138 140325400614720 context.py:881] Enabling collective ops after program startup may cause error when accessing previously created tensors.\r\n[worker-0]:    W0628 01:02:37.524588 140325400614720 context.py:881] Enabling collective ops after program startup may cause error when accessing previously created tensors.\r\n[chief-0]:     INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['\/job:chief\/replica:0\/task:0\/device:CPU:0', '\/job:chief\/replica:0\/task:0\/device:CPU:1']\r\n[chief-0]:     I0628 01:02:37.525059 140325400614720 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['\/job:chief\/replica:0\/task:0\/device:CPU:0', '\/job:chief\/replica:0\/task:0\/device:CPU:1']\r\n[worker-0]:    INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['\/job:worker\/replica:0\/task:0\/device:CPU:0', '\/job:worker\/replica:0\/task:0\/device:CPU:1']\r\n[worker-0]:    I0628 01:02:37.527691 140325400614720 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['\/job:worker\/replica:0\/task:0\/device:CPU:0', '\/job:worker\/replica:0\/task:0\/device:CPU:1']\r\n[chief-0]:     INFO:tensorflow:Using MirroredStrategy with devices ('\/job:chief\/task:0\/device:CPU:0',)\r\n[chief-0]:     I0628 01:02:37.529053 140325400614720 mirrored_strategy.py:423] Using MirroredStrategy with devices ('\/job:chief\/task:0\/device:CPU:0',)\r\n[chief-0]:     INFO:tensorflow:Check health not enabled.\r\n[chief-0]:     I0628 01:02:37.529462 140325400614720 collective_all_reduce_strategy.py:574] Check health not enabled.\r\n[chief-0]:     INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('\/job:chief\/task:0\/device:CPU:0',), communication = CommunicationImplementation.AUTO\r\n[chief-0]:     I0628 01:02:37.529720 140325400614720 collective_all_reduce_strategy.py:576] MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('\/job:chief\/task:0\/device:CPU:0',), communication = CommunicationImplementation.AUTO\r\n[worker-0]:    INFO:tensorflow:Using MirroredStrategy with devices ('\/job:worker\/task:0\/device:CPU:0',)\r\n[worker-0]:    I0628 01:02:37.531570 140325400614720 mirrored_strategy.py:423] Using MirroredStrategy with devices ('\/job:worker\/task:0\/device:CPU:0',)\r\n[worker-0]:    INFO:tensorflow:Check health not enabled.\r\n[worker-0]:    I0628 01:02:37.532161 140325400614720 collective_all_reduce_strategy.py:574] Check health not enabled.\r\n[worker-0]:    INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('\/job:worker\/task:0\/device:CPU:0',), communication = CommunicationImplementation.AUTO\r\n[worker-0]:    I0628 01:02:37.532691 140325400614720 collective_all_reduce_strategy.py:576] MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('\/job:worker\/task:0\/device:CPU:0',), communication = CommunicationImplementation.AUTO\r\n[worker-0]:    2023-06-28 01:02:37.540481: I tensorflow\/core\/distributed_runtime\/master.cc:240] Scanning workers for devices: 1 total workers\r\n[chief-0]:     2023-06-28 01:02:37.541763: I tensorflow\/core\/distributed_runtime\/master.cc:240] Scanning workers for devices: 1 total workers\r\n-- Test timed out at 2023-06-28 01:07:29 UTC --\r\nThread 0x00007fa429ef8700 (most recent call first):\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 258 in _continuously_readline_from_sub\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 917 in run\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 980 in _bootstrap_inner\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 937 in _bootstrap\r\n\r\nThread 0x00007fa42a779700 (most recent call first):\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 527 in _process_watchdog\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 917 in run\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 980 in _bootstrap_inner\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 937 in _bootstrap\r\n\r\nThread 0x00007fa44d0fc700 (most recent call first):\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 258 in _continuously_readline_from_sub\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 917 in run\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 980 in _bootstrap_inner\r\n  File \"\/usr\/lib\/python3.9\/threading.py\", line 937 in _bootstrap\r\n\r\nCurrent thread 0x00007fa451437740 (most recent call first):\r\n  File \"\/usr\/lib\/python3.9\/multiprocessing\/connection.py\", line 379 in _recv\r\n  File \"\/usr\/lib\/python3.9\/multiprocessing\/connection.py\", line 414 in _recv_bytes\r\n  File \"\/usr\/lib\/python3.9\/multiprocessing\/connection.py\", line 250 in recv\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 991 in run\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/combinations.py\", line 580 in decorator\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_combinations.py\", line 343 in execute_test_method\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_combinations.py\", line 360 in decorated\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/absl_py\/absl\/testing\/parameterized.py\", line 314 in bound_param_test\r\n  File \"\/usr\/lib\/python3.9\/unittest\/case.py\", line 550 in _callTestMethod\r\n  File \"\/usr\/lib\/python3.9\/unittest\/case.py\", line 592 in run\r\n  File \"\/usr\/lib\/python3.9\/unittest\/case.py\", line 651 in __call__\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 122 in run\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 84 in __call__\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 122 in run\r\n  File \"\/usr\/lib\/python3.9\/unittest\/suite.py\", line 84 in __call__\r\n  File \"\/usr\/lib\/python3.9\/unittest\/runner.py\", line 184 in run\r\n  File \"\/usr\/lib\/python3.9\/unittest\/main.py\", line 271 in runTests\r\n  File \"\/usr\/lib\/python3.9\/unittest\/main.py\", line 101 in __init__\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2537 in _run_and_get_tests_result\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2568 in run_tests\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2156 in _run_in_app\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 2049 in main\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 51 in g_main\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/absl_py\/absl\/app.py\", line 258 in _run_main\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/absl_py\/absl\/app.py\", line 312 in run\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 60 in main_wrapper\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/benchmark.py\", line 489 in benchmarks_main\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/googletest.py\", line 62 in main\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/platform\/test.py\", line 56 in main\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/eager\/test.py\", line 25 in main\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_lib.py\", line 167 in test_main\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 1455 in test_main\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/test_util.py\", line 138 in main\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/vars_test_2gpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/vars_test.py\", line 1336 in <module>\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux"],"created_at":"2023-06-28T15:03:57Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61112"},{"issue_number":348,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/distribute\/failure_handling:gce_failure_handler_test is flaky","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n6.1.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\n\/\/tensorflow\/python\/distribute\/failure_handling:gce_failure_handler_test sometimes fails or timeouts.\r\n\r\nx86 log\r\nhttps:\/\/source.cloud.google.com\/results\/invocations\/66d8ddaa-3dbe-4464-837c-053157b11659\/log\r\n\r\nAARCH64 log\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5354839739\/jobs\/9712379821#step:5:12470\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel --bazelrc=\/usertools\/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https:\/\/storage.googleapis.com\/tensorflow-devinfra-bazel-cache\/norbe --google_default_credentials\n```\n\n\n### Relevant log output\n\n```shell\nINFO:tensorflow:time(__main__.GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker): 5.11s\r\nI0628 05:40:30.221971 140074262497088 test_util.py:2464] time(__main__.GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker): 5.11s\r\n[       OK ] GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker\r\n======================================================================\r\nFAIL: test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker (__main__.GceFailureHandlingTest)\r\nGceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker\r\ntest_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker(api_wrapping_train=False, grace_period=7, input_arg='checkpoint', strategy_option='MWMS_multi_worker')\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/absl_py\/absl\/testing\/parameterized.py\", line 314, in bound_param_test\r\n    return test_method(self, **testcase_params)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_combinations.py\", line 360, in decorated\r\n    execute_test_method()\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_combinations.py\", line 343, in execute_test_method\r\n    test_method(**kwargs_to_pass)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/combinations.py\", line 559, in decorator\r\n    test_method(self, **kwargs)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.py\", line 417, in test_multiple_workers_preempted_consecutively\r\n    mpr.join(timeout=250)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 649, in join\r\n    self._reraise_if_subprocess_error(process_statuses)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 565, in _reraise_if_subprocess_error\r\n    six.reraise(*process_status.exc_info)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/six_archive\/six.py\", line 719, in reraise\r\n    raise value\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 1060, in _run_contained\r\n    return_value = fn(*args, **kwargs)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.py\", line 211, in worker_fn\r\n    self.assertNotEmpty(checkpoint_index)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 972, in assertNotEmpty\r\n    self.fail('{!r} has length of 0.'.format(container), msg)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/gce_failure_handler_test.runfiles\/absl_py\/absl\/testing\/absltest.py\", line 1814, in fail\r\n    return super(TestCase, self).fail(self._formatMessage(prefix, msg))\r\n  File \"\/usr\/lib\/python3.9\/unittest\/case.py\", line 676, in fail\r\n    raise self.failureException(msg)\r\nAssertionError: [] has length of 0.\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux","TF 2.13"],"created_at":"2023-06-28T14:07:42Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61111"},{"issue_number":349,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/data\/experimental\/kernel_tests\/service:auto_shard_test is flaky","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n6.1.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\n\/\/tensorflow\/python\/data\/experimental\/kernel_tests\/service:auto_shard_test will sometimes timeout.\r\n\r\nx86 log\r\nhttps:\/\/source.cloud.google.com\/results\/invocations\/75230523-dc04-40ad-bcf3-06df3b94a119\/log\r\n\r\nAARCH64 log\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5383235016\/jobs\/9769729147#step:5:8442\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel --bazelrc=\/usertools\/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https:\/\/storage.googleapis.com\/tensorflow-devinfra-bazel-cache\/norbe --google_default_credentials\n```\n\n\n### Relevant log output\n\n```shell\nTIMEOUT: \/\/tensorflow\/python\/data\/experimental\/kernel_tests\/service:auto_shard_test (Summary)\r\n      \/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/testlogs\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/auto_shard_test\/shard_6_of_32\/test.log\r\nINFO: From Testing \/\/tensorflow\/python\/data\/experimental\/kernel_tests\/service:auto_shard_test (shard 6 of 32):\r\n==================== Test output for \/\/tensorflow\/python\/data\/experimental\/kernel_tests\/service:auto_shard_test (shard 6 of 32):\r\n2023-06-28 08:01:34.863480: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-28 08:01:34.953626: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nRunning tests under Python 3.9.17: \/usr\/bin\/python3\r\n[ RUN      ] AutoShardTest.testBatchDataset_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA\r\n[  SKIPPED ] AutoShardTest.testBatchDataset_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA\r\n[ RUN      ] AutoShardTest.testEnumerateShardingPolicies_test_mode_graph_tfapiversion_1_shardingpolicy_ShardingPolicyDYNAMIC\r\n[  SKIPPED ] AutoShardTest.testEnumerateShardingPolicies_test_mode_graph_tfapiversion_1_shardingpolicy_ShardingPolicyDYNAMIC\r\n[ RUN      ] AutoShardTest.testRangeDataset_AutoShard_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA\r\n[  SKIPPED ] AutoShardTest.testRangeDataset_AutoShard_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA\r\n[ RUN      ] AutoShardTest.testRangeDataset_ShardHintUsedInWrongShardingPolicy_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyOFF\r\n[  SKIPPED ] AutoShardTest.testRangeDataset_ShardHintUsedInWrongShardingPolicy_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyOFF\r\n[ RUN      ] AutoShardTest.testTFRecordDataset_FewerFilesThanWorkers_DataShard_test_mode_eager_tfapiversion_2\r\nINFO:tensorflow:Using local port 35531\r\nI0628 08:02:03.552320 139750982424384 test_util.py:3796] Using local port 35531\r\n2023-06-28 08:02:03.553738: I tensorflow\/core\/data\/service\/dispatcher_impl.cc:223] Attempting to restore dispatcher state from journal in \/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/_tmp\/df4595e40f65712a12ed239eda341855ao10glmf\/tmplcxh6ash\/tf_data_dispatcher_journal\r\n2023-06-28 08:02:03.553799: I tensorflow\/core\/data\/service\/dispatcher_impl.cc:230] No journal found. Starting dispatcher from new state.\r\n2023-06-28 08:02:03.553987: I tensorflow\/core\/data\/service\/server_lib.cc:82] Started tf.data DispatchServer running at 0.0.0.0:35531\r\nINFO:tensorflow:Using local port 43623\r\nI0628 08:02:03.556421 139750982424384 test_util.py:3796] Using local port 43623\r\n2023-06-28 08:02:04.452927: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-28 08:02:04.495706: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-28 08:02:04.557281: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-06-28 08:02:04.586634: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-06-28 08:02:06.406403: I tensorflow\/core\/data\/service\/worker_impl.cc:186] Worker registered with dispatcher running at localhost:35531\r\n2023-06-28 08:02:06.406887: I tensorflow\/core\/data\/service\/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:43623\r\nINFO:tensorflow:Using local port 40059\r\nI0628 08:02:06.407576 139750982424384 test_util.py:3796] Using local port 40059\r\nE0628 08:02:06.423661297  858602 server_chttp2.cc:40]        {\"created\":\"@1687939326.423469587\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/ext\/transport\/chttp2\/server\/chttp2_server.cc\",\"file_line\":395,\"referenced_errors\":[{\"created\":\"@1687939326.423463864\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_posix.cc\",\"file_line\":342,\"referenced_errors\":[{\"created\":\"@1687939326.423438207\",\"description\":\"Address family not supported by protocol\",\"errno\":97,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/socket_utils_common_posix.cc\",\"file_line\":420,\"os_error\":\"Address family not supported by protocol\",\"syscall\":\"socket\",\"target_address\":\"[::]:40059\"},{\"created\":\"@1687939326.423463182\",\"description\":\"Unable to configure socket\",\"fd\":8,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1687939326.423455518\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\r\nProcess _RemoteWorkerProcess-2:\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.9\/multiprocessing\/process.py\", line 315, in _bootstrap\r\n    self.run()\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/auto_shard_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_lib.py\", line 54, in _run_with_absl\r\n    app.run(lambda _: self._run_impl())\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/auto_shard_test.runfiles\/absl_py\/absl\/app.py\", line 312, in run\r\n    _run_main(main, args)\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/auto_shard_test.runfiles\/absl_py\/absl\/app.py\", line 258, in _run_main\r\n    sys.exit(main(argv))\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/auto_shard_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_lib.py\", line 54, in <lambda>\r\n    app.run(lambda _: self._run_impl())\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/auto_shard_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/multi_process_cluster.py\", line 42, in run\r\n    self.start_worker()\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/auto_shard_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/multi_process_cluster.py\", line 50, in start_worker\r\n    self._worker.start()\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/auto_shard_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/test_base.py\", line 110, in start\r\n    self._server.start()\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/data\/experimental\/kernel_tests\/service\/auto_shard_test.runfiles\/org_tensorflow\/tensorflow\/python\/data\/experimental\/service\/server_lib.py\", line 415, in start\r\n    self._server.start()\r\nRuntimeError: Could not start gRPC server\r\n-- Test timed out at 2023-06-28 08:06:34 UTC --\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux","subtype:bazel"],"created_at":"2023-06-28T13:20:03Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61109"},{"issue_number":350,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/distribute:cross_device_ops_test_2gpu is flaky","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n6.1.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\n\/\/tensorflow\/python\/distribute:cross_device_ops_test_2gpu fails or timeouts sometimes\r\n\r\nx86 log\r\nhttps:\/\/source.cloud.google.com\/results\/invocations\/3008a6bc-b49f-4776-871c-1c5ae046a470\/log\r\n\r\nAARCH64 log\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5379162649\/jobs\/9759994992#step:5:9913\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel --bazelrc=\/usertools\/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https:\/\/storage.googleapis.com\/tensorflow-devinfra-bazel-cache\/norbe --google_default_credentials\n```\n\n\n### Relevant log output\n\n```shell\n[ RUN      ] CollectiveOpsTest.testBatchReduceDense_test_implementation_CommunicationImplementationRING_numprocesses_2_preferuniqueinstancekey_False_reduceop_ReduceOpSUM_requiredgpus_0\r\n[worker-0]:    W0628 09:23:51.698908 140256382134080 context.py:881] Enabling collective ops after program startup may cause error when accessing previously created tensors.\r\nI0628 09:23:51.700115 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-0\r\nI0628 09:23:51.702625 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-0\r\nI0628 09:23:51.702917 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-1\r\nI0628 09:23:51.711031 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-0\r\n[worker-0]:    2023-06-28 09:23:51.734731: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:457] Started server with target: grpc:\/\/localhost:38213\r\n[worker-1]:    E0628 09:23:51.740952892 1194361 server_chttp2.cc:40]        {\"created\":\"@1687944231.740904797\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/ext\/transport\/chttp2\/server\/chttp2_server.cc\",\"file_line\":395,\"referenced_errors\":[{\"created\":\"@1687944231.740902919\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_posix.cc\",\"file_line\":342,\"referenced_errors\":[{\"created\":\"@1687944231.740878363\",\"description\":\"Address family not supported by protocol\",\"errno\":97,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/socket_utils_common_posix.cc\",\"file_line\":420,\"os_error\":\"Address family not supported by protocol\",\"syscall\":\"socket\",\"target_address\":\"[::]:36443\"},{\"created\":\"@1687944231.740901905\",\"description\":\"Unable to configure socket\",\"fd\":9,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1687944231.740897648\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\r\n[worker-1]:    2023-06-28 09:23:51.741121: E tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server\r\n[worker-1]:    2023-06-28 09:23:51.741306: E tensorflow\/core\/common_runtime\/eager\/context_distributed_manager.cc:703] Could not start gRPC server\r\n-- Test timed out at 2023-06-28 09:28:39 UTC --\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux"],"created_at":"2023-06-28T12:15:43Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61107"},{"issue_number":351,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/distribute:moving_averages_test is flaky","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n6.1.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\n\/\/tensorflow\/python\/distribute:moving_averages_test fails sometimes.\r\n\r\nx86 log\r\nhttps:\/\/source.cloud.google.com\/results\/invocations\/1e048065-76db-4dab-b1a5-093dd542b27d\/log\r\n\r\nAARCH64 log\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5383507734\/jobs\/9770356325#step:5:8417\r\n\r\nLooks like a network port conflict issue\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel --bazelrc=\/usertools\/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https:\/\/storage.googleapis.com\/tensorflow-devinfra-bazel-cache\/norbe --google_default_credentials\n```\n\n\n### Relevant log output\n\n```shell\nINFO: From Testing \/\/tensorflow\/python\/distribute:moving_averages_test_cpu (shard 3 of 5):\r\n==================== Test output for \/\/tensorflow\/python\/distribute:moving_averages_test_cpu (shard 3 of 5):\r\n2023-06-28 10:24:27.228811: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-28 10:24:27.325938: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nRunning tests under Python 3.9.17: \/usr\/bin\/python3\r\n[ RUN      ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph\r\nINFO:tensorflow:time(__main__.AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph): 0.03s\r\nI0628 10:24:32.088060 139996622530368 test_util.py:2464] time(__main__.AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph): 0.03s\r\n[  SKIPPED ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph\r\n[ RUN      ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MultiWorkerMirrored4x1CPU_mode_graph\r\nW0628 10:24:32.126455 139996622530368 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.\r\nWARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\r\nW0628 10:24:32.126971 139996622530368 collective_all_reduce_strategy.py:394] Collective ops is not configured at program startup. Some performance features may not be enabled.\r\nINFO:tensorflow:Using MirroredStrategy with devices ('\/device:CPU:0',)\r\nI0628 10:24:32.164073 139996622530368 mirrored_strategy.py:423] Using MirroredStrategy with devices ('\/device:CPU:0',)\r\nINFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('\/device:CPU:0',), communication = CommunicationImplementation.AUTO\r\nI0628 10:24:32.171577 139996622530368 collective_all_reduce_strategy.py:446] Single-worker MultiWorkerMirroredStrategy with local_devices = ('\/device:CPU:0',), communication = CommunicationImplementation.AUTO\r\nINFO:tensorflow:Using local port 43531\r\nI0628 10:24:32.173116 139996622530368 test_util.py:3796] Using local port 43531\r\nINFO:tensorflow:Using local port 34115\r\nI0628 10:24:32.173517 139996622530368 test_util.py:3796] Using local port 34115\r\nINFO:tensorflow:Using local port 34873\r\nI0628 10:24:32.173699 139996622530368 test_util.py:3796] Using local port 34873\r\nINFO:tensorflow:Using local port 40455\r\nI0628 10:24:32.173828 139996622530368 test_util.py:3796] Using local port 40455\r\n2023-06-28 10:24:32.849027: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-28 10:24:32.904538: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-06-28 10:24:32.917529: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-28 10:24:32.995322: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n[chief-0]:     I0628 10:24:34.534415 140662937323328 multi_process_runner.py:840] Subprocess with PID 1244 (chief, 0) is now being started.\r\n[chief-0]:     I0628 10:24:34.534823 140662937323328 multi_process_runner.py:842] TF_CONFIG: '{\"cluster\": {\"chief\": [\"localhost:43531\"], \"worker\": [\"localhost:34115\", \"localhost:34873\", \"localhost:40455\"]}, \"task\": {\"type\": \"chief\", \"index\": 0}, \"rpc_layer\": \"grpc\"}'\r\nI0628 10:24:34.561606 139996622530368 multi_process_runner.py:989] Waiting for the result from chief-0\r\n[worker-1]:    I0628 10:24:34.596723 140662937323328 multi_process_runner.py:840] Subprocess with PID 1479 (worker, 1) is now being started.\r\n[worker-1]:    I0628 10:24:34.597120 140662937323328 multi_process_runner.py:842] TF_CONFIG: '{\"cluster\": {\"chief\": [\"localhost:43531\"], \"worker\": [\"localhost:34115\", \"localhost:34873\", \"localhost:40455\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}, \"rpc_layer\": \"grpc\"}'\r\n[worker-1]:    2023-06-28 10:24:34.655212: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:457] Started server with target: grpc:\/\/localhost:34873\r\n[worker-1]:    INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['\/job:worker\/replica:0\/task:1\/device:CPU:0']\r\n[worker-1]:    I0628 10:24:34.661444 140662937323328 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['\/job:worker\/replica:0\/task:1\/device:CPU:0']\r\n[chief-0]:     E0628 10:24:34.621842982    1244 server_chttp2.cc:40]        {\"created\":\"@1687947874.621805165\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/ext\/transport\/chttp2\/server\/chttp2_server.cc\",\"file_line\":395,\"referenced_errors\":[{\"created\":\"@1687947874.621803913\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_posix.cc\",\"file_line\":342,\"referenced_errors\":[{\"created\":\"@1687947874.621786452\",\"description\":\"Address family not supported by protocol\",\"errno\":97,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/socket_utils_common_posix.cc\",\"file_line\":420,\"os_error\":\"Address family not supported by protocol\",\"syscall\":\"socket\",\"target_address\":\"[::]:43531\"},{\"created\":\"@1687947874.621803346\",\"description\":\"Unable to configure socket\",\"fd\":9,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1687947874.621800946\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\r\n[chief-0]:     2023-06-28 10:24:34.621944: E tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server\r\n[worker-0]:    I0628 10:24:34.608072 140662937323328 multi_process_runner.py:840] Subprocess with PID 1472 (worker, 0) is now being started.\r\n[chief-0]:     2023-06-28 10:24:34.622255: E tensorflow\/core\/common_runtime\/eager\/context_distributed_manager.cc:703] Could not start gRPC server\r\n[chief-0]:     Process _Process-3:\r\n[chief-0]:     Traceback (most recent call last):\r\n[chief-0]:       File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/moving_averages_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/strategy_combinations.py\", line 207, in skip_if_cannot_start_grpc_server\r\n[chief-0]:         return _create_multi_worker_mirrored()\r\n[chief-0]:       File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/moving_averages_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/strategy_combinations.py\", line 189, in _create_multi_worker_mirrored\r\n[chief-0]:         strategy = CollectiveAllReduceStrategy(cluster_resolver=resolver)\r\n[chief-0]:       File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/moving_averages_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/collective_all_reduce_strategy.py\", line 186, in __init__\r\n[chief-0]:         CollectiveAllReduceExtended(\r\n[chief-0]:       File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/moving_averages_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/collective_all_reduce_strategy.py\", line 339, in __init__\r\n[chief-0]:         self._initialize_strategy(self._cluster_resolver, devices=devices)\r\n[chief-0]:       File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/moving_averages_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/collective_all_reduce_strategy.py\", line 358, in _initialize_strategy\r\n[chief-0]:         self._initialize_multi_worker(cluster_resolver)\r\n[chief-0]:       File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/moving_averages_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/collective_all_reduce_strategy.py\", line 530, in _initialize_multi_worker\r\n[chief-0]:         context.context().ensure_initialized()\r\n[chief-0]:       File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/moving_averages_test_cpu.runfiles\/org_tensorflow\/tensorflow\/python\/eager\/context.py\", line 610, in ensure_initialized\r\n[chief-0]:         pywrap_tfe.TFE_EnableCollectiveOps(context_handle, server_def_str)\r\n[chief-0]:     tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux"],"created_at":"2023-06-28T11:56:02Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61105"},{"issue_number":352,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/distribute\/failure_handling:failure_handler_test is flaky","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n6.1.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\n\/\/tensorflow\/python\/distribute\/failure_handling:failure_handler_test will timeout sometimes\r\n\r\nx86 log\r\nhttps:\/\/source.cloud.google.com\/results\/invocations\/302ca1a4-593b-430c-b278-038351228670\/log\r\n\r\nAARCH64 log\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5363735926\/jobs\/9731502578#step:5:11034\r\n\r\nLooks like a network port conflict.\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel --bazelrc=\/usertools\/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https:\/\/storage.googleapis.com\/tensorflow-devinfra-bazel-cache\/norbe --google_default_credentials\n```\n\n\n### Relevant log output\n\n```shell\nINFO: From Testing \/\/tensorflow\/python\/distribute\/failure_handling:failure_handler_test (shard 4 of 8):\r\n==================== Test output for \/\/tensorflow\/python\/distribute\/failure_handling:failure_handler_test (shard 4 of 8):\r\n2023-06-22 16:46:57.933478: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-22 16:46:58.014195: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nRunning tests under Python 3.9.17: \/usr\/bin\/python3\r\n[ RUN      ] PreemptionCheckpointTest.test_grace_period_continue_training_test_inputarg_checkpoint_strategyoption_MWMSmultiworker\r\nINFO:tensorflow:Using local port 38723\r\nI0622 16:47:02.418146 140436227843904 test_util.py:3796] Using local port 38723\r\nINFO:tensorflow:Using local port 41053\r\nI0622 16:47:02.418907 140436227843904 test_util.py:3796] Using local port 41053\r\nINFO:tensorflow:Using local port 45087\r\nI0622 16:47:02.419116 140436227843904 test_util.py:3796] Using local port 45087\r\nINFO:tensorflow:Using local port 38125\r\nI0622 16:47:02.419290 140436227843904 test_util.py:3796] Using local port 38125\r\n2023-06-22 16:47:03.130184: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-22 16:47:03.185580: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-06-22 16:47:03.185763: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-06-22 16:47:03.241253: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nINFO:tensorflow:Cluster starting.\r\nI0622 16:47:04.817156 140436227843904 failure_handler_test.py:432] Cluster starting.\r\n[worker-0]:    I0622 16:47:04.842670 140652189284160 multi_process_runner.py:840] Subprocess with PID 588346 (worker, 0) is now being started.\r\n[worker-0]:    I0622 16:47:04.842952 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{\"cluster\": {\"worker\": [\"localhost:38723\", \"localhost:41053\", \"localhost:45087\", \"localhost:38125\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}, \"rpc_layer\": \"grpc\"}'\r\n[worker-1]:    I0622 16:47:04.858144 140652189284160 multi_process_runner.py:840] Subprocess with PID 588646 (worker, 1) is now being started.\r\n[worker-1]:    I0622 16:47:04.858554 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{\"cluster\": {\"worker\": [\"localhost:38723\", \"localhost:41053\", \"localhost:45087\", \"localhost:38125\"]}, \"task\": {\"type\": \"worker\", \"index\": 1}, \"rpc_layer\": \"grpc\"}'\r\n[worker-0]:    2023-06-22 16:47:04.907068: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:457] Started server with target: grpc:\/\/localhost:38723\r\n[worker-2]:    I0622 16:47:04.927917 140652189284160 multi_process_runner.py:840] Subprocess with PID 588933 (worker, 2) is now being started.\r\n[worker-0]:    2023-06-22 16:47:04.943758: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:551] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 14520256932767538069\r\n[worker-0]:    2023-06-22 16:47:04.944905: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service_agent.cc:299] Coordination agent has successfully connected.\r\n[worker-2]:    I0622 16:47:04.928328 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{\"cluster\": {\"worker\": [\"localhost:38723\", \"localhost:41053\", \"localhost:45087\", \"localhost:38125\"]}, \"task\": {\"type\": \"worker\", \"index\": 2}, \"rpc_layer\": \"grpc\"}'\r\n[worker-1]:    2023-06-22 16:47:04.995414: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:457] Started server with target: grpc:\/\/localhost:41053\r\n[worker-0]:    2023-06-22 16:47:04.997341: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:551] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 7622272584115739518\r\n[worker-1]:    2023-06-22 16:47:04.998064: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service_agent.cc:299] Coordination agent has successfully connected.\r\n[worker-3]:    I0622 16:47:05.006252 140652189284160 multi_process_runner.py:840] Subprocess with PID 589461 (worker, 3) is now being started.\r\n[worker-2]:    E0622 16:47:05.032748121  588933 server_chttp2.cc:40]        {\"created\":\"@1687452425.032699864\",\"description\":\"No address added out of total 1 resolved\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/ext\/transport\/chttp2\/server\/chttp2_server.cc\",\"file_line\":395,\"referenced_errors\":[{\"created\":\"@1687452425.032698162\",\"description\":\"Failed to add any wildcard listeners\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_posix.cc\",\"file_line\":342,\"referenced_errors\":[{\"created\":\"@1687452425.032676237\",\"description\":\"Address family not supported by protocol\",\"errno\":97,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/socket_utils_common_posix.cc\",\"file_line\":420,\"os_error\":\"Address family not supported by protocol\",\"syscall\":\"socket\",\"target_address\":\"[::]:45087\"},{\"created\":\"@1687452425.032697070\",\"description\":\"Unable to configure socket\",\"fd\":9,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":216,\"referenced_errors\":[{\"created\":\"@1687452425.032694857\",\"description\":\"Address already in use\",\"errno\":98,\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/iomgr\/tcp_server_utils_posix_common.cc\",\"file_line\":189,\"os_error\":\"Address already in use\",\"syscall\":\"bind\"}]}]}]}\r\n[worker-2]:    2023-06-22 16:47:05.032844: E tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server\r\n[worker-2]:    2023-06-22 16:47:05.033076: E tensorflow\/core\/common_runtime\/eager\/context_distributed_manager.cc:703] Could not start gRPC server\r\n[worker-3]:    I0622 16:47:05.006778 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{\"cluster\": {\"worker\": [\"localhost:38723\", \"localhost:41053\", \"localhost:45087\", \"localhost:38125\"]}, \"task\": {\"type\": \"worker\", \"index\": 3}, \"rpc_layer\": \"grpc\"}'\r\n[worker-2]:    Process _Process-4:\r\n[worker-2]:    Traceback (most recent call last):\r\n[worker-2]:      File \"\/usr\/lib\/python3.9\/multiprocessing\/process.py\", line 315, in _bootstrap\r\n[worker-2]:        self.run()\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 755, in _run_with_setenv\r\n[worker-2]:        return self._actual_run()\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_lib.py\", line 54, in _run_with_absl\r\n[worker-2]:        app.run(lambda _: self._run_impl())\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/absl_py\/absl\/app.py\", line 312, in run\r\n[worker-2]:        _run_main(main, args)\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/absl_py\/absl\/app.py\", line 258, in _run_main\r\n[worker-2]:        sys.exit(main(argv))\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_lib.py\", line 54, in <lambda>\r\n[worker-2]:        app.run(lambda _: self._run_impl())\r\n[worker-2]:      File \"\/usr\/lib\/python3.9\/multiprocessing\/process.py\", line 108, in run\r\n[worker-2]:        self._target(*self._args, **self._kwargs)\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 866, in __call__\r\n[worker-2]:        six.reraise(*info.exc_info)\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/six_archive\/six.py\", line 719, in reraise\r\n[worker-2]:        raise value\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/multi_process_runner.py\", line 1060, in _run_contained\r\n[worker-2]:        return_value = fn(*args, **kwargs)\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.py\", line 146, in worker_fn\r\n[worker-2]:        strategy = collective_all_reduce_strategy.CollectiveAllReduceStrategy()\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/collective_all_reduce_strategy.py\", line 186, in __init__\r\n[worker-2]:        CollectiveAllReduceExtended(\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/collective_all_reduce_strategy.py\", line 339, in __init__\r\n[worker-2]:        self._initialize_strategy(self._cluster_resolver, devices=devices)\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/collective_all_reduce_strategy.py\", line 358, in _initialize_strategy\r\n[worker-2]:        self._initialize_multi_worker(cluster_resolver)\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/distribute\/collective_all_reduce_strategy.py\", line 530, in _initialize_multi_worker\r\n[worker-2]:        context.context().ensure_initialized()\r\n[worker-2]:      File \"\/root\/.cache\/bazel\/_bazel_root\/fbac33eb30dbfb6b11b15a7ff5ac830d\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/distribute\/failure_handling\/failure_handler_test.runfiles\/org_tensorflow\/tensorflow\/python\/eager\/context.py\", line 610, in ensure_initialized\r\n[worker-2]:        pywrap_tfe.TFE_EnableCollectiveOps(context_handle, server_def_str)\r\n[worker-2]:    tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux"],"created_at":"2023-06-28T10:57:28Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61104"},{"issue_number":353,"repository":"tensorflow\/tensorflow","title":"tf.data.Dataset prefetch not fetching data asynchronously","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.11\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nDebian\/Linux 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.7\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nAfter implementing a data pipeline using tf.data.Dataset to pull image data from Google Cloud Storage, TensorBoard profiler shows that the GPU compute and CPU prefetch are running synchronously. I used data.Dataset.AUTOTUNE to determine the appropriate prefetch batch size. Monitoring GPU usage while the model is running confirms this with the GPU at 0% utilization to actually computing something for about a 2:1 ratio, which is reflected in the profiler. CPU usage when monitored does not appear to max out.\r\n\r\nI expected the prefetch to occur concurrently with GPU processing as described in the data.Dataset documentation and tutorials.\r\n\r\n![ch](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/79778984\/e96ad312-12b0-4bbb-b06e-f4e4976714b3)\r\n\r\n![cp](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/79778984\/f52959d1-23fd-45ed-ba57-5a532afd0972)\r\n\r\n![gp](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/79778984\/2b2e15b6-2cf7-40d8-89b1-98889f151863)\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\nos.environ['TF_GPU_ALLOCATOR'] = \"cuda_malloc_async\"\r\nconfig = ConfigProto()\r\nconfig.gpu_options.allow_growth = True\r\nsession = InteractiveSession(config=config)\r\n\r\ndef get_label(file_path):\r\n    parts = tf.strings.split(file_path, os.path.sep)\r\n    one_hot = parts[-2] == class_names\r\n    return tf.argmax(one_hot)\r\n\r\ndef decode_img(img):\r\n    img = tf.io.decode_image(img, channels=3, expand_animations = False)\r\n    img = tf.image.resize(img, [244, 244])\r\n    img = tf.cast(img, tf.float32)\r\n    return img\r\n\r\ndef process_path(file_path):\r\n    label = get_label(file_path)\r\n    img = tf.io.read_file(file_path)\r\n    img = decode_img(img)\r\n    return img, label\r\n\r\ndef configure_for_performance(ds):\r\n    ds = ds.batch(128)\r\n    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\r\n    return ds\r\n\r\nfiles = tf.data.Dataset.list_files((data_dir + '\/*\/*.png'), shuffle=False)\r\nfiles = files.shuffle(image_count, reshuffle_each_iteration=False)\r\n\r\nval_size = int(image_count * 0.2)\r\n\r\ntrain_files = files.skip(val_size)\r\nval_files = files.take(val_size)\r\n\r\ntrain_ds = train_files.interleave(lambda x: tf.data.Dataset.from_tensor_slices([x]), cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE)\r\ntrain_ds = train_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\r\n\r\nval_ds = val_files.interleave(lambda x: tf.data.Dataset.from_tensor_slices([x]), cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE)\r\nval_ds = val_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\r\n\r\ntrain_ds = configure_for_performance(train_ds)\r\nval_ds = configure_for_performance(val_ds)\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:data","type:performance","TF 2.11"],"created_at":"2023-06-26T19:39:17Z","comments":4,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61084"},{"issue_number":354,"repository":"tensorflow\/tensorflow","title":"MHLO -> HLO does not respect sharding, inserting a tuple without sharding","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2e896fbe1e0ea4df33fbcfe780a1036f431b4e89\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nLinux Ubunto 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n10.3\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nConverting an MHLO program to HLO that is fully annotated with shardings, results in HLO that has a tuple instruction that is without sharding.\r\n\r\nInput MLIR `sharding-not-respected-mhlo-to-hlo.mlir`:\r\n```mlir\r\nfunc.func @main(%arg0: tensor<2x2xi32> {mhlo.sharding = \"{devices=[2,1]0,1}\"}) -> (tensor<2x2xi32> {mhlo.sharding = \"{devices=[2,1]0,1}\"}) {\r\n  %0 = mhlo.add %arg0, %arg0 {mhlo.sharding = \"{devices=[2,1]0,1}\"} : tensor<2x2xi32>\r\n  return %0 : tensor<2x2xi32>\r\n}\r\n```\r\n\r\nCommand:\r\n```\r\nxla-translate -mlir-hlo-to-hlo-text sharding-not-respected-mhlo-to-hlo.mlir\r\n```\r\n\r\nResult:\r\n```hlo\r\nHloModule main, entry_computation_layout={(s32[2,2]{1,0})->s32[2,2]{1,0}}\r\n\r\nENTRY %main.5 (Arg_0.1: s32[2,2]) -> s32[2,2] {\r\n  %Arg_0.1 = s32[2,2] parameter(0), sharding={devices=[2,1]0,1}\r\n  %add.2 = s32[2,2] add(s32[2,2] %Arg_0.1, s32[2,2] %Arg_0.1), sharding={devices=[2,1]0,1}, metadata={source_file=\"sharding-not-respected-mhlo-to-hlo.mlir\" source_line=2}\r\n  %tuple.3 = (s32[2,2]) tuple(s32[2,2] %add.2)\r\n  ROOT %get-tuple-element.4 = s32[2,2] get-tuple-element((s32[2,2]) %tuple.3), index=0, sharding={devices=[2,1]0,1}\r\n}\r\n```\r\n\r\nYou can see that a tuple instruction has been inserted that has no sharding annotation.\r\n```hlo\r\n  %tuple.3 = (s32[2,2]) tuple(s32[2,2] %add.2)\r\n```\r\n\r\nThis [test](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/2e896fbe1e0ea4df33fbcfe780a1036f431b4e89\/tensorflow\/compiler\/xla\/translate\/mhlo_to_hlo\/tests\/sharding.mlir#L21) expects a tuple without sharding. Is this really the expected behavior?\r\nFor example this tuple instruction would cause a problem if you want to do SPMD partitioning. Then the partitioner would insert an unwanted all-gather instruction.\r\n\r\nIf you do a conversion without sharding of the same MLIR\r\n```mlir\r\nfunc.func @main(%arg0: tensor<2x2xi32>) -> tensor<2x2xi32> {\r\n  %0 = mhlo.add %arg0, %arg0 : tensor<2x2xi32>\r\n  return %0 : tensor<2x2xi32>\r\n}\r\n```\r\nYou get a nicer result without the redundant `tuple` and `get-tuple-element` instructions\r\n```hlo\r\nHloModule main, entry_computation_layout={(s32[2,2]{1,0})->s32[2,2]{1,0}}\r\n\r\nENTRY %main.3 (Arg_0.1: s32[2,2]) -> s32[2,2] {\r\n  %Arg_0.1 = s32[2,2] parameter(0)\r\n  ROOT %add.2 = s32[2,2] add(s32[2,2] %Arg_0.1, s32[2,2] %Arg_0.1), metadata={source_file=\"sharding-not-respected-mhlo-to-hlo.mlir\" source_line=2}\r\n}\r\n```\r\n\r\nI can see two solutions here. \r\n1. Make the tuple instruction inherit the correct sharding.\r\n2. Return directly the result of the `add` operation.\n\n### Standalone code to reproduce the issue\n\n```shell\nSee the description.\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:runtime","comp:xla"],"created_at":"2023-06-26T10:50:30Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61080"},{"issue_number":355,"repository":"tensorflow\/tensorflow","title":"Errors when custom gradients are being used in TPU","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\nv2.12.0-rc1-12-g0db597d0d75 2.12.0\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nWhen writing custom gradient modules in TensorFlow (not using tf.GradientTape()) and applying it using an existing optimizer causes errors. I don't see any visible difference in tf.GradientShape() gradients and custom ones.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nwith strategy.scope():\r\n  with tf.GradientTape() as tape:\r\n    model = ... #using any model\r\n    loss = ... #using any loss function here\r\n    loss_n = loss(y_batch, model(x_batch))\r\n\r\n  grads = tape.gradient(loss_n, model.trainable_weights)\r\n  new_grads = []\r\n  \r\n  for g in grads:\r\n    new_grads += [tf.ones((tf.shape(g)))]\r\n  \r\n  optimizer.apply_gradients(zip(new_grads, model.trainable_weights)) #error here\r\n```\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nShould be similar to below:\r\n\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-25-c9f1f22a039f> in <cell line: 1>()\r\n     50       grads = tape.gradient(l, model.trainable_weights)\r\n     51       c = grads[0]\r\n---> 52       opt.apply_gradients(zip(grads, model.trainable_weights))\r\n     53       print('Loss this batch: ' + closure().numpy())\r\n     54       opt.next_steps()\r\n\r\n2 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/optimizers\/optimizer.py in apply_gradients(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\r\n   1171         )\r\n   1172         if not skip_gradients_aggregation and experimental_aggregate_gradients:\r\n-> 1173             grads_and_vars = self.aggregate_gradients(grads_and_vars)\r\n   1174         return super().apply_gradients(grads_and_vars, name=name)\r\n   1175 \r\n\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/optimizers\/optimizer.py in aggregate_gradients(self, grads_and_vars)\r\n   1137           List of (gradient, variable) pairs.\r\n   1138         \"\"\"\r\n-> 1139         return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)\r\n   1140 \r\n   1141     def apply_gradients(\r\n\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/optimizers\/utils.py in all_reduce_sum_gradients(grads_and_vars)\r\n     40         else:\r\n     41             # TODO(b\/183257003): Remove this branch\r\n---> 42             reduced = tf.distribute.get_replica_context().merge_call(\r\n     43                 _all_reduce_sum_fn, args=(filtered_grads_and_vars,)\r\n     44             )\r\n\r\nAttributeError: 'NoneType' object has no attribute 'merge_call'\r\n```\r\n```\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.12"],"created_at":"2023-06-17T04:52:55Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60906"},{"issue_number":356,"repository":"tensorflow\/tensorflow","title":"TF throws: \"'visible_device_list' listed an invalid Device id\" when using non-GPU PluggableDevices","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.12.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nLinux CentOS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n11.8\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nWhen using PluggableDevice API together with GPU devices, TF crashes with `tensorflow.python.framework.errors_impl.InvalidArgumentError: 'visible_device_list' listed an invalid Device id '2' but visible device count is 2` when calling `tf.device('\/GPU:0')`\r\n\r\n**For reproducing the error, it is necessary to have a PluggableDevice Plugin loaded and to have GPUs within the same system!!!**\r\n\r\nHere the list of devices within my system:\r\n```python3\r\n# tf.config.list_physical_devices()\r\n[PhysicalDevice(name='\/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='\/physical_device:VE:0', device_type='VE'), PhysicalDevice(name='\/physical_device:VE:1', device_type='VE'), PhysicalDevice(name='\/physical_device:GPU:0', device_type='GPU')]\r\n```\r\n\r\nI traced down the error to be thrown here. It gets thrown in\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/e32f5b90ec16e88b23be8a5189e52ea9a420e999\/tensorflow\/tsl\/framework\/device_id_utils.cc#L46\r\n\r\nHowever, it is caused by wrong values stored in the gpu_options, which get initialized here:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/0db597d0d758aba578783b5bf46c889700a45085\/tensorflow\/python\/eager\/context.py#L1206\r\n\r\nThe list of gpu_devices and ALL pluggable_devices get combined, even if they are not of the same device_type. So the list of `compatible_devices` will be:\r\n```\r\n[\r\n\tPhysicalDevice(name='\/physical_device:GPU:0', device_type='GPU'),\r\n\tPhysicalDevice(name='\/physical_device:VE:0', device_type='VE'),\r\n\tPhysicalDevice(name='\/physical_device:VE:1', device_type='VE')\r\n]\r\n```\r\n\r\nThis causes the `visible_device_list` to be `['0', '1', '2']`, which contains invalid GPU device indices. These then get passed to `ParseVisibleDeviceList`, which throws this error.\r\n\r\nTo fix this error, it suffices to change this line:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/0db597d0d758aba578783b5bf46c889700a45085\/tensorflow\/python\/eager\/context.py#L1216\r\n\r\nand replace it to:\r\n```python\r\nif dev not in gpu_devices and dev.device_type == \"GPU\":\r\n```\r\n\r\nThis way, the list of `compatible_devices` will only populated with other GPUs, not with any other device types.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nprint(*tf.config.list_physical_devices(), sep='\\n')\r\ntf.device('\/GPU:0')\n```\n\n\n### Relevant log output\n\n```shell\nPhysicalDevice(name='\/physical_device:CPU:0', device_type='CPU')\r\nPhysicalDevice(name='\/physical_device:VE:0', device_type='VE')\r\nPhysicalDevice(name='\/physical_device:VE:1', device_type='VE')\r\nPhysicalDevice(name='\/physical_device:GPU:0', device_type='GPU')\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/...\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 5577, in device_v2\r\n    return device(device_name)\r\n  File \"\/...\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 5526, in device\r\n    return context.device(device_name_or_function)\r\n  File \"\/...\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/context.py\", line 2348, in device\r\n    ensure_initialized()\r\n  File \"\/...\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/context.py\", line 2143, in ensure_initialized\r\n    context().ensure_initialized()\r\n  File \"\/...\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/context.py\", line 583, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: 'visible_device_list' listed an invalid Device id '2' but visible device count is 2\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.13"],"created_at":"2023-06-16T07:46:52Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60895"},{"issue_number":357,"repository":"tensorflow\/tensorflow","title":"Spurious(?) type inference failed warning for flattened tf.data.Dataset with a RaggedTensor","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.12.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nMac OS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nAs far as I can tell the code still runs as expected, but a seemly spurious warning is still issued.\r\n\r\nThe issue is pretty niche, removing the `_merge` function or the `vals` part to the initial dataset will make the error go away.\r\n\r\nThis occurs on 2.11, 2.12, and nightly on mac.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nlabels = tf.ragged.constant([[\"a\", \"b\"], [\"a\"]])\r\nvals = tf.constant([0.1, 0.2])\r\nds = tf.data.Dataset.from_tensors(dict(labels=labels, vals=vals, other=vals))\r\n\r\nparts = [\"labels\", \"vals\"]\r\n\r\ndef _flatten(ex):\r\n  flat_ds = tf.data.Dataset.from_tensor_slices({k: ex[k] for k in parts})\r\n\r\n  def _merge(_flat_ex):\r\n    _flat_ex[\"other\"] = tf.constant([0.1, 0.2])\r\n    return _flat_ex\r\n\r\n  return flat_ds.map(_merge)\r\nds = ds.flat_map(_flatten)\r\n\r\nfor ex in ds.as_numpy_iterator():\r\n  print(ex)\n```\n\n\n### Relevant log output\n\n```shell\n2023-06-15 13:44:43.909272: W tensorflow\/core\/common_runtime\/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: type mismatch for node 'TensorSliceDataset': expected a subtype of:\r\ntype_id: TFT_PRODUCT\r\nargs {\r\n  type_id: TFT_DATASET\r\n  args {\r\n    type_id: TFT_PRODUCT\r\n    args {\r\n      type_id: TFT_TENSOR\r\n      args {\r\n        type_id: TFT_LEGACY_VARIANT\r\n      }\r\n    }\r\n    args {\r\n      type_id: TFT_TENSOR\r\n      args {\r\n        type_id: TFT_FLOAT\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n  got:\r\ntype_id: TFT_PRODUCT\r\nargs {\r\n  type_id: TFT_DATASET\r\n  args {\r\n    type_id: TFT_PRODUCT\r\n    args {\r\n      type_id: TFT_RAGGED\r\n      args {\r\n        type_id: TFT_STRING\r\n      }\r\n    }\r\n    args {\r\n    }\r\n  }\r\n}\r\n\r\n  \r\n\twhile updating its output type.\r\n{'labels': array([b'a', b'b'], dtype=object), 'vals': 0.1, 'other': array([0.1, 0.2], dtype=float32)}\r\n{'labels': array([b'a'], dtype=object), 'vals': 0.2, 'other': array([0.1, 0.2], dtype=float32)}\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.12"],"created_at":"2023-06-15T20:54:26Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60890"},{"issue_number":358,"repository":"tensorflow\/tensorflow","title":"Unnecessary memcopies between CPU and GPU when using tf.function","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.12.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nLinux CentOS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n11.8\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nWe recognized that TensorFlow creates unnecessary copies between CPU and GPU. To reproduce, save the code below as `error.py` and execute it using `nvprof --print-gpu-trace --openacc-profiling off python3 error.py MODE`, with mode being 0, 1, 2 or 3.\r\n\r\nHere what we have observed, I simplyfied the profiler output and commented inline:\r\n\r\n## Mode 0 (generates input on GPU, runs model on GPU using tf.function):\r\n```python\r\n    Size Name\r\n## RANDOM NUMBER GENERATION ##\r\n      8B [CUDA memcpy HtoD]\r\n1.0039KB [CUDA memset]\r\n       - void tensorflow::functor::FillPhiloxRandomKernelLaunch...\r\n      8B [CUDA memcpy HtoD]\r\n      8B [CUDA memcpy DtoH]\r\n      8B [CUDA memcpy HtoD]\r\n      4B [CUDA memcpy HtoD]\r\n      8B [CUDA memcpy HtoD]\r\n## HERE THE COMPUTATION STARTS\r\n18.375MB [CUDA memcpy DtoH]\t## TF copies data to CPU\r\n18.375MB [CUDA memcpy HtoD] ## 1. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [928]\r\n      8B [CUDA memcpy DtoD]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 2. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [963]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 3. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [997]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 4. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1031]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 5. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1063]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 6. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1099]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 7. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1133]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 8. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1167]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 9. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1201]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 10. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1235]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n## Merging of all results into a single Tensor and ten copies back to host\r\n     30B [CUDA memcpy HtoD]\r\n       - void tensorflow::functor::ColumnReduceMax16ColumnsKernel...\r\n       - void tensorflow::functor::BlockReduceKernel...\r\n      1B [CUDA memcpy DtoH]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n183.75MB [CUDA memcpy DtoH]\r\n```\r\n\r\nSo we see that TF copies the data, that is already on the GPU to the CPU, and then in every iteration copies back to GPU, instead of just using the data that is already on the GPU.\r\n\r\n## Mode 1 (generates input on CPU, runs model on GPU using tf.function):\r\n```python\r\n    Size Name\r\n      8B [CUDA memcpy HtoD]\r\n1.0039KB [CUDA memset]\r\n      8B [CUDA memcpy HtoD]\r\n      8B [CUDA memcpy DtoH]\r\n      8B [CUDA memcpy HtoD]\r\n      4B [CUDA memcpy HtoD]\r\n      8B [CUDA memcpy HtoD]\r\n## HERE THE COMPUTATION STARTS\r\n18.375MB [CUDA memcpy HtoD] ## 1. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [911]\r\n      8B [CUDA memcpy DtoD]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 2. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [946]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 3. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [980]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 4. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1012]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 5. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1046]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 6. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1080]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 7. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1116]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 8. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1150]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 9. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1184]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy HtoD] ## 10. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1218]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n## Merging of all results into a single Tensor and ten copies back to host\r\n     30B [CUDA memcpy HtoD]\r\n       - void tensorflow::functor::ColumnReduceMax16ColumnsKernel...\r\n       - void tensorflow::functor::BlockReduceKernel...\r\n      1B [CUDA memcpy DtoH]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n18.375MB [CUDA memcpy DtoD]\r\n183.75MB [CUDA memcpy DtoH]\r\n```\r\n\r\nNearly identical to Mode 0, but here it is actually expected that the data gets copied over from CPU to GPU in every iteration.\r\n\r\n## Mode 2 (generates input on GPU, runs model on GPU using tf.function): \r\n```python\r\n    Size Name\r\n## RANDOM NUMBER GENERATION ##\r\n      8B [CUDA memcpy HtoD]\r\n1.0039KB [CUDA memset]\r\n       - void tensorflow::functor::FillPhiloxRandomKernelLaunch...\r\n## HERE THE COMPUTATION STARTS\r\n18.375MB [CUDA memcpy DtoH] ## TF copies data to CPU\r\n      4B [CUDA memcpy HtoD]\r\n      8B [CUDA memcpy HtoD]\r\n18.375MB [CUDA memcpy HtoD] ## 1. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [907]\r\n      8B [CUDA memcpy DtoD]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy DtoH] ## 1. TF copies result to CPU\r\n18.375MB [CUDA memcpy HtoD] ## 2. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [991]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy DtoH] ## 2. TF copies result to CPU\r\n18.375MB [CUDA memcpy HtoD] ## 3. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1076]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy DtoH] ## 3. TF copies result to CPU\r\n18.375MB [CUDA memcpy HtoD] ## 4. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1161]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy DtoH] ## 4. TF copies result to CPU\r\n18.375MB [CUDA memcpy HtoD] ## 5. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1246]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy DtoH] ## 5. TF copies result to CPU\r\n18.375MB [CUDA memcpy HtoD] ## 6. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1333]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy DtoH] ## 6. TF copies result to CPU\r\n18.375MB [CUDA memcpy HtoD] ## 7. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1420]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy DtoH] ## 7. TF copies result to CPU\r\n18.375MB [CUDA memcpy HtoD] ## 8. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1509]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy DtoH] ## 8. TF copies result to CPU\r\n18.375MB [CUDA memcpy HtoD] ## 9. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1594]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy DtoH] ## 9. TF copies result to CPU\r\n18.375MB [CUDA memcpy HtoD] ## 10. TF copies data to GPU\r\n       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1679]\r\n       - void Eigen::internal::EigenMetaKernel...\r\n18.375MB [CUDA memcpy DtoH] ## 10. TF copies result to CPU\r\n```\r\n\r\nSimilar to mode 0, but here the results get immediatly copied back to the host.\r\n\r\n## Mode 3 (generates input on GPU, runs model on GPU using eager mode): \r\n```python\r\n    Size Name\r\n## RANDOM NUMBER GENERATION ##\r\n      8B [CUDA memcpy HtoD]\r\n1.0039KB [CUDA memset]\r\n       - void tensorflow::functor::FillPhiloxRandomKernelLaunch...\r\n## HERE THE COMPUTATION STARTS\r\n       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [806]\r\n       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [810]\r\n       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [814]\r\n       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [818]\r\n       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [822]\r\n       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [826]\r\n       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [830]\r\n       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [834]\r\n       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [838]\r\n       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [842]\r\n```\r\n\r\nIn this case no uncessary memcopies occur, data is kept on GPU all the time.\r\n\r\n## Summary\r\nWhen we use `model.predict(...)` or `model.predict_on_batch()` (or any other of these Keras.Model functions that use `tf.function`), then the input data ALWAYS gets copied to the host first, and then in every iteration back to the GPU. This causes an significant performance penalty.\r\n\r\nI have not been able to find any documentation about this behavior, if it is intended, or a way to prevent this to happen.\r\n\r\nHere also the `tf.debugging.set_log_device_placement(True)` output. I think the line `input: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0` indicates that for whatever reason the `tf.function`'s input is expected to be on the CPU.\r\n\r\n```python\r\n2023-06-15 14:44:14.959681: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op _EagerConst in device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nresource_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.960017: I tensorflow\/core\/common_runtime\/placer.cc:114] resource_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nVarHandleOp: (VarHandleOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.960041: I tensorflow\/core\/common_runtime\/placer.cc:114] VarHandleOp: (VarHandleOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.960475: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op VarHandleOp in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nresource: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.960895: I tensorflow\/core\/common_runtime\/placer.cc:114] resource: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nvalue: (_DeviceArg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.960922: I tensorflow\/core\/common_runtime\/placer.cc:114] value: (_DeviceArg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nAssignVariableOp: (AssignVariableOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.960945: I tensorflow\/core\/common_runtime\/placer.cc:114] AssignVariableOp: (AssignVariableOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.961429: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op AssignVariableOp in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.963362: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op _EagerConst in device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\ncomponents_0: (_DeviceArg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.963837: I tensorflow\/core\/common_runtime\/placer.cc:114] components_0: (_DeviceArg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nTensorDataset: (TensorDataset): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.963872: I tensorflow\/core\/common_runtime\/placer.cc:114] TensorDataset: (TensorDataset): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nhandle_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.963889: I tensorflow\/core\/common_runtime\/placer.cc:114] handle_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.964369: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op TensorDataset in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\ninput__dataset: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.980981: I tensorflow\/core\/common_runtime\/placer.cc:114] input__dataset: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nFlatMapDataset: (FlatMapDataset): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.981006: I tensorflow\/core\/common_runtime\/placer.cc:114] FlatMapDataset: (FlatMapDataset): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nhandle_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.981024: I tensorflow\/core\/common_runtime\/placer.cc:114] handle_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.982115: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op FlatMapDataset in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.982841: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op _EagerConst in device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\ninput__dataset: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.983325: I tensorflow\/core\/common_runtime\/placer.cc:114] input__dataset: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nbuffer__size: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.983348: I tensorflow\/core\/common_runtime\/placer.cc:114] buffer__size: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nPrefetchDataset: (PrefetchDataset): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.983387: I tensorflow\/core\/common_runtime\/placer.cc:114] PrefetchDataset: (PrefetchDataset): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nhandle_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.983403: I tensorflow\/core\/common_runtime\/placer.cc:114] handle_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.984096: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op PrefetchDataset in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nresource: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.984658: I tensorflow\/core\/common_runtime\/placer.cc:114] resource: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nReadVariableOp: (ReadVariableOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.984692: I tensorflow\/core\/common_runtime\/placer.cc:114] ReadVariableOp: (ReadVariableOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nvalue_RetVal: (_DeviceRetval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.984710: I tensorflow\/core\/common_runtime\/placer.cc:114] value_RetVal: (_DeviceRetval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.985212: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op ReadVariableOp in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\ninput: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.985724: I tensorflow\/core\/common_runtime\/placer.cc:114] input: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nIdentity: (Identity): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:14.985761: I tensorflow\/core\/common_runtime\/placer.cc:114] Identity: (Identity): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\noutput_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:14.985778: I tensorflow\/core\/common_runtime\/placer.cc:114] output_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:14.986346: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op Identity in device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:14.986722: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op ReadVariableOp in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.986812: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op Identity in device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:14.988278: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op _EagerConst in device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nresource: (_Arg): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:14.988585: I tensorflow\/core\/common_runtime\/placer.cc:114] resource: (_Arg): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nvalue: (_Arg): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:14.988624: I tensorflow\/core\/common_runtime\/placer.cc:114] value: (_Arg): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nAssignVariableOp: (AssignVariableOp): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:14.988643: I tensorflow\/core\/common_runtime\/placer.cc:114] AssignVariableOp: (AssignVariableOp): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:14.989152: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op AssignVariableOp in device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:14.989472: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op ReadVariableOp in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.989562: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op Identity in device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:14.989661: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op ReadVariableOp in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.989731: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op Identity in device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nhandle_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.990139: I tensorflow\/core\/common_runtime\/placer.cc:114] handle_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nAnonymousIteratorV3: (AnonymousIteratorV3): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.990173: I tensorflow\/core\/common_runtime\/placer.cc:114] AnonymousIteratorV3: (AnonymousIteratorV3): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.990613: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op AnonymousIteratorV3 in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\ndataset: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.990976: I tensorflow\/core\/common_runtime\/placer.cc:114] dataset: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\niterator: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.990998: I tensorflow\/core\/common_runtime\/placer.cc:114] iterator: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nMakeIterator: (MakeIterator): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.991016: I tensorflow\/core\/common_runtime\/placer.cc:114] MakeIterator: (MakeIterator): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.991473: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op MakeIterator in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:14.992481: I tensorflow\/core\/common_runtime\/executor.cc:1197] [\/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder\/_0' with dtype int32\r\n         [[{{node Placeholder\/_0}}]]\r\nargs_0: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.002874: I tensorflow\/core\/common_runtime\/placer.cc:114] args_0: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nGeneratorDataset: (GeneratorDataset): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.002917: I tensorflow\/core\/common_runtime\/placer.cc:114] GeneratorDataset: (GeneratorDataset): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nNoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.002935: I tensorflow\/core\/common_runtime\/placer.cc:114] NoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nIdentity: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.002966: I tensorflow\/core\/common_runtime\/placer.cc:114] Identity: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nFakeSink0: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.002983: I tensorflow\/core\/common_runtime\/placer.cc:114] FakeSink0: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nidentity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.003007: I tensorflow\/core\/common_runtime\/placer.cc:114] identity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.007846: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op AnonymousIteratorV3 in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.007991: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op MakeIterator in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nargs_0: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.011294: I tensorflow\/core\/common_runtime\/placer.cc:114] args_0: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nGeneratorDataset: (GeneratorDataset): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.011320: I tensorflow\/core\/common_runtime\/placer.cc:114] GeneratorDataset: (GeneratorDataset): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nNoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.011339: I tensorflow\/core\/common_runtime\/placer.cc:114] NoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nIdentity: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.011355: I tensorflow\/core\/common_runtime\/placer.cc:114] Identity: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nFakeSink0: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.011373: I tensorflow\/core\/common_runtime\/placer.cc:114] FakeSink0: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nidentity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.011389: I tensorflow\/core\/common_runtime\/placer.cc:114] identity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.018501: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op ReadVariableOp in device \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.018640: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op Identity in device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\niterator: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.046638: I tensorflow\/core\/common_runtime\/placer.cc:114] iterator: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nassignaddvariableop_resource: (_Arg): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:15.046661: I tensorflow\/core\/common_runtime\/placer.cc:114] assignaddvariableop_resource: (_Arg): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nIteratorGetNext: (IteratorGetNext): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.046678: I tensorflow\/core\/common_runtime\/placer.cc:114] IteratorGetNext: (IteratorGetNext): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nmodel\/tf.__operators__.add\/AddV2: (AddV2): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:15.046695: I tensorflow\/core\/common_runtime\/placer.cc:114] model\/tf.__operators__.add\/AddV2: (AddV2): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nAssignAddVariableOp: (AssignAddVariableOp): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:15.046711: I tensorflow\/core\/common_runtime\/placer.cc:114] AssignAddVariableOp: (AssignAddVariableOp): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nNoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:15.046726: I tensorflow\/core\/common_runtime\/placer.cc:114] NoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nIdentity: (Identity): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:15.046759: I tensorflow\/core\/common_runtime\/placer.cc:114] Identity: (Identity): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nidentity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:15.046783: I tensorflow\/core\/common_runtime\/placer.cc:114] identity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nConst: (Const): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:15.046798: I tensorflow\/core\/common_runtime\/placer.cc:114] Const: (Const): \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\n2023-06-15 14:44:15.051534: I tensorflow\/core\/common_runtime\/eager\/execute.cc:1525] Executing op __inference_predict_function_68 in device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nargs_0: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.054027: I tensorflow\/core\/common_runtime\/placer.cc:114] args_0: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nNoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.054141: I tensorflow\/core\/common_runtime\/placer.cc:114] NoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nIdentity: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.054201: I tensorflow\/core\/common_runtime\/placer.cc:114] Identity: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nFakeSink0: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.054272: I tensorflow\/core\/common_runtime\/placer.cc:114] FakeSink0: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nidentity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.054342: I tensorflow\/core\/common_runtime\/placer.cc:114] identity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nPyFunc: (PyFunc): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.054415: I tensorflow\/core\/common_runtime\/placer.cc:114] PyFunc: (PyFunc): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nargs_0: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.061278: I tensorflow\/core\/common_runtime\/placer.cc:114] args_0: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nPyFunc: (PyFunc): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.061356: I tensorflow\/core\/common_runtime\/placer.cc:114] PyFunc: (PyFunc): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nNoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.061419: I tensorflow\/core\/common_runtime\/placer.cc:114] NoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nIdentity: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.061476: I tensorflow\/core\/common_runtime\/placer.cc:114] Identity: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nFakeSink0: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.061556: I tensorflow\/core\/common_runtime\/placer.cc:114] FakeSink0: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nidentity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.061612: I tensorflow\/core\/common_runtime\/placer.cc:114] identity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nargs_0: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.067674: I tensorflow\/core\/common_runtime\/placer.cc:114] args_0: (_Arg): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nPyFunc: (PyFunc): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.067767: I tensorflow\/core\/common_runtime\/placer.cc:114] PyFunc: (PyFunc): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nNoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.067834: I tensorflow\/core\/common_runtime\/placer.cc:114] NoOp: (NoOp): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nIdentity: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.067905: I tensorflow\/core\/common_runtime\/placer.cc:114] Identity: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nFakeSink0: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.067969: I tensorflow\/core\/common_runtime\/placer.cc:114] FakeSink0: (Identity): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nidentity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n2023-06-15 14:44:15.068020: I tensorflow\/core\/common_runtime\/placer.cc:114] identity_RetVal: (_Retval): \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\n```\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport sys\r\n\r\nassert len(sys.argv) == 2, \"needs to be run as `python3 error.py MODE`\"\r\n\r\nmode = int(sys.argv[1])\r\n\r\ninp = tf.keras.Input((3, 224, 224))\r\nout = inp + inp\r\nmodel = tf.keras.Model(inp, out)\r\n\r\nclass Sequence(tf.keras.utils.Sequence):\r\n        def __init__(self, x):          self.x = x\r\n        def __len__(self):              return 10\r\n        def __getitem__(self, idx):     return self.x\r\n\r\nwith tf.device('\/GPU:0' if mode != 1 else '\/CPU:0'):\r\n        data = tf.random.uniform((32, 3, 224, 224))\r\n\r\nwith tf.device('\/GPU:0'):\r\n        if mode == 2:\r\n                for _ in range(10):\r\n                        model.predict_on_batch(data)\r\n        elif mode == 3:\r\n                for _ in range(10):\r\n                        model(data)\r\n        else:\r\n                seq = Sequence(data)\r\n                model.predict(seq)\n```\n\n\n### Relevant log output\n\n```shell\nsee above\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:performance","TF 2.12"],"created_at":"2023-06-15T12:51:29Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60883"},{"issue_number":359,"repository":"tensorflow\/tensorflow","title":"tf.data debug mode breaks dataset.save()","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.13.0rc0\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\ndataset.save() does not work if the experimental debug mode has been enabled for the datasets. A simple reproducer and the exception are below. I think the issue is due to the following code in `set_save_dataset_attributes` function in `tensorflow\/python\/data\/ops\/save_op.py`:\r\n\r\n`shard_func = lambda *x: None  # a dummy function that will not be used`\r\n\r\nReplacing this line with e.g.\r\n\r\n`shard_func = lambda *x: 0`\r\n\r\nseems to fix this issue, so apparently returning `None` doesn't work in the debug mode. Btw the comment on this line is a bit misleading, because this function is still traced, so it's not completely unused.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport tempfile\r\ntf.data.experimental.enable_debug_mode()\r\nds = tf.data.Dataset.from_tensor_slices([1.0, 2.0, 3.0])\r\nwith tempfile.TemporaryDirectory() as tmpdir:\r\n     ds.save(tmpdir)\n```\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/framework\/func_graph.py\", line 1042, in convert\r\n    x = ops.convert_to_tensor_or_composite(x)\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 1547, in convert_to_tensor_or_composite\r\n    return internal_convert_to_tensor_or_composite(\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 1582, in internal_convert_to_tensor_or_composite\r\n    return convert_to_tensor(\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/profiler\/trace.py\", line 183, in wrapped\r\n    return func(*args, **kwargs)\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 1443, in convert_to_tensor\r\n    return tensor_conversion_registry.convert(\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/framework\/tensor_conversion_registry.py\", line 209, in convert\r\n    return overload(dtype, name)  #  pylint: disable=not-callable\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 2177, in __tf_tensor__\r\n    raise TypeError(\"can't convert Operation '{}' to Tensor\".format(self.name))\r\nTypeError: can't convert Operation 'EagerPyFunc' to Tensor\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/data\/ops\/dataset_ops.py\", line 1746, in save\r\n    return save_op._save(self, path, compression, shard_func, checkpoint_args)\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/data\/ops\/save_op.py\", line 57, in _save\r\n    dataset, shard_func, use_shard_func, path = set_save_dataset_attributes(\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/data\/ops\/save_op.py\", line 103, in set_save_dataset_attributes\r\n    wrapped_func = structured_function.StructuredFunctionWrapper(\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/data\/ops\/structured_function.py\", line 272, in __init__\r\n    self._function = fn_factory()\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py\", line 1189, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py\", line 1169, in _get_concrete_function_garbage_collected\r\n    self._initialize(args, kwargs, add_initializers_to=initializers)\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py\", line 694, in _initialize\r\n    self._variable_creation_fn    # pylint: disable=protected-access\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py\", line 176, in _get_concrete_function_internal_garbage_collected\r\n    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py\", line 171, in _maybe_define_concrete_function\r\n    return self._maybe_define_function(args, kwargs)\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py\", line 398, in _maybe_define_function\r\n    concrete_function = self._create_concrete_function(\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py\", line 305, in _create_concrete_function\r\n    func_graph_module.func_graph_from_py_func(\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/framework\/func_graph.py\", line 1060, in func_graph_from_py_func\r\n    func_outputs = nest.map_structure(\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/util\/nest.py\", line 624, in map_structure\r\n    return nest_util.map_structure(\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/util\/nest_util.py\", line 1054, in map_structure\r\n    return _tf_core_map_structure(func, *structure, **kwargs)\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/util\/nest_util.py\", line 1094, in _tf_core_map_structure\r\n    [func(*x) for x in entries],\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/util\/nest_util.py\", line 1094, in <listcomp>\r\n    [func(*x) for x in entries],\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/tensorflow\/python\/framework\/func_graph.py\", line 1044, in convert\r\n    raise TypeError(\r\nTypeError: To be compatible with tf.function, Python functions must return zero or more Tensors or ExtensionTypes or None values; in compilation of <function StructuredFunctionWrapper.__init__.<locals>.trace_py_function.<locals>.wrapped_fn at 0x7f88c9224c10>, found return value of type Operation, which is not a Tensor or ExtensionType.\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.12"],"created_at":"2023-06-14T14:21:08Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60861"},{"issue_number":360,"repository":"tensorflow\/tensorflow","title":"CopyTensor::ViaDMA function, allocator type sometimes not match actual input underlying memory type","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.12.0rc0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nCentOS Linux 7\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.7.5\n\n### Bazel version\n\nbazel 3.7.2\n\n### GCC\/Compiler version\n\ngcc-9\n\n### CUDA\/cuDNN version\n\ncuda 11, cudnn 8\n\n### GPU model and memory\n\nTesla V100S\n\n### Current Behaviour?\n\nIn `CopyTensor::ViaDMA`, `alloc_attr `decides the direction of memory copy. However, sometimes `alloc_attr `does not keep the same as the Tensor pointer's underlying memory type. In my case,`src_alloc_attr.on_host()` is `True`, but `input->GetMemoryType()` equals to `kDevice`. So this results the memory copy direction in this function is cpu->gpu, but actually the direction should be gpu -> gpu. \r\nI think this bug does not reveal is because the cuda driver api, like `cuMemcpyHtoD()`, does not care about the direction if it's H to D or others, it only cares about the pointer attribute, if the src pointer is on device and dst pointer is also on device, even if we call `cuMemcpyHtoD()`, cuda driver would still do D to D copy. This feature would cover many bugs. \r\n\r\nI haven't figured out where did the `on_host `attribute is set. From my understanding so far, same allocator object would be reused on different tensors, but the `on_host `attribute is one-way, once it's been set `on_host`, it cannot be unset later. This might cause some issue? Also, why wouln't we just use `input->GetMemoryType()` to decieds the memory copy direction, instead of the `on_host `attribute of `alloc_attr`\r\n\r\nI meet this issue when I run horovod unit test case. Add some log message in \r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/core\/common_runtime\/copy_tensor.cc#L219\r\n, such as \r\n```\r\nif(!src_alloc_attr.on_host() && (input->GetMemoryType()==AllocatorMemoryType::kHostPageable || input->GetMemoryType()==AllocatorMemoryType::kHostPinned)){\r\n    std::cout<<\"!!!!!!! src alloc not on host, but input mem type is on host\"<< std::endl;\r\n  }\r\n  if( src_alloc_attr.on_host() && input->GetMemoryType()==AllocatorMemoryType::kDevice) {\r\n    std::cout<<\"!!!!!!! src alloc on host, but input mem is on device\" << std::endl;\r\n  }\r\n\r\n  if(!dst_alloc_attr.on_host() && (output->GetMemoryType()==AllocatorMemoryType::kHostPageable || output->GetMemoryType()==AllocatorMemoryType::kHostPinned)){\r\n    std::cout<<\"!!!!!!! dst alloc not on host, but output mem type is on host\"<< std::endl;\r\n  }\r\n  if( dst_alloc_attr.on_host() && output->GetMemoryType()==AllocatorMemoryType::kDevice) {\r\n    std::cout<<\"!!!!!!! dst alloc on host, but output mem is on device\" << std::endl;\r\n  }\r\n\r\n```\r\nFor me, I ran horovod `alltoall Op` unit test case to reproduce this issue. But this issue might reveal in other cases.\n\n### Standalone code to reproduce the issue\n\n```shell\nRun horovod unit test case can reproduce this issue:\r\nhttps:\/\/github.com\/horovod\/horovod\/blob\/master\/test\/parallel\/test_tensorflow.py\r\nhorovodrun --mpi -np 2 pytest -s -v test_tensorflow.py::TensorFlowTests::test_horovod_alltoall_gpu.\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:runtime","TF 2.12"],"created_at":"2023-06-14T03:09:42Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60856"},{"issue_number":361,"repository":"tensorflow\/tensorflow","title":"OMP_PROC_BIND or OMP_PLACES either ignored or respected incorrectly","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.11\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.4\n\n### Bazel version\n\n5.1.1\n\n### GCC\/Compiler version\n\n11.3.0\n\n### CUDA\/cuDNN version\n\nN\/A\n\n### GPU model and memory\n\nN\/A\n\n### Current Behaviour?\n\nWhen I run TensorFlow on CPU and try to enable core binding with `OMP_PROC_BIND=close`, all threads get bound to core 0 (rather than thread 0 to core 0, thread 1 to core 1, etc).\r\n\r\nExpected output:\r\n\r\n```\r\n$ OMP_PROC_BIND=true OMP_PLACES=cores python tf_example.py\r\nInter_op_threads: 1\r\nIntra_op_threads: 4\r\nThread count: 4\r\nChild affinity is {96}\r\nChild affinity is {97}\r\nChild affinity is {98}\r\nChild affinity is {99}\r\n```\r\nI.e. I'd expect four threads, bound to subsequent cores. Now, I guess TensorFlow simply uses some threads for management of the framework. Though I'm surprised by the large number of threads, 11 extra threads on top of the Intra_op_thread count, see the log output for the custom TF-2.11 case, this is not really an 'issue' (although one might wonder how management threads ought to behave, they should probably remain unbound even if the compute threads are bound 1 per core).\r\n\r\nWhat I'm seeing however is that with our custom built TF-2.11, all threads are bound to the first core in my cgroup (core ID 96 in this case). For tf-nightly, all binding is completely ignored.\r\n\r\nNot sure if it's useful, but the Custom built had this build command:\r\n\r\n<details>\r\n<summary>build command<\/summary>\r\n\r\n```\r\nbazel --output_user_root=\/tmp\/jenkins\/build\/TensorFlow\/2.11.0\/foss-2022a\/TensorFlow\/bazel-root --local_startup_timeout_secs=300 --host_jvm_args=-Xms512m --host_jvm_args=-Xmx4096m build --config=noaws --config=nogcp --config=nohdfs --compilation_mode=opt --config=opt --subcommands --verbose_failures --jobs=128 --copt=\"-fPIC\" --distinct_host_configuration=false --action_env=CPATH='\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/cURL\/7.83.0-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/double-conversion\/3.2.0-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/flatbuffers\/2.0.7-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/giflib\/5.2.1-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/hwloc\/2.7.1-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/ICU\/71.1-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/JsonCpp\/1.9.5-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/libjpeg-turbo\/2.1.3-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/libpng\/1.6.37-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/LMDB\/0.9.29-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/nsync\/1.25.0-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/protobuf\/3.19.4-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/pybind11\/2.9.2-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/snappy\/1.1.9-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/SQLite\/3.38.3-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/zlib\/1.2.12-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/OpenSSL\/1.1\/include' --host_action_env=CPATH='\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/cURL\/7.83.0-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/double-conversion\/3.2.0-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/flatbuffers\/2.0.7-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/giflib\/5.2.1-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/hwloc\/2.7.1-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/ICU\/71.1-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/JsonCpp\/1.9.5-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/libjpeg-turbo\/2.1.3-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/libpng\/1.6.37-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/LMDB\/0.9.29-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/nsync\/1.25.0-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/protobuf\/3.19.4-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/pybind11\/2.9.2-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/snappy\/1.1.9-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/SQLite\/3.38.3-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/zlib\/1.2.12-GCCcore-11.3.0\/include:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/OpenSSL\/1.1\/include' --action_env=LIBRARY_PATH='\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/cURL\/7.83.0-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/double-conversion\/3.2.0-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/flatbuffers\/2.0.7-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/giflib\/5.2.1-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/hwloc\/2.7.1-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/ICU\/71.1-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/JsonCpp\/1.9.5-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/libjpeg-turbo\/2.1.3-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/libpng\/1.6.37-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/LMDB\/0.9.29-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/nsync\/1.25.0-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/protobuf\/3.19.4-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/pybind11\/2.9.2-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/snappy\/1.1.9-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/SQLite\/3.38.3-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/zlib\/1.2.12-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/OpenSSL\/1.1\/lib' --host_action_env=LIBRARY_PATH='\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/cURL\/7.83.0-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/double-conversion\/3.2.0-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/flatbuffers\/2.0.7-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/giflib\/5.2.1-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/hwloc\/2.7.1-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/ICU\/71.1-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/JsonCpp\/1.9.5-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/libjpeg-turbo\/2.1.3-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/libpng\/1.6.37-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/LMDB\/0.9.29-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/nsync\/1.25.0-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/protobuf\/3.19.4-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/pybind11\/2.9.2-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/snappy\/1.1.9-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/SQLite\/3.38.3-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/zlib\/1.2.12-GCCcore-11.3.0\/lib:\/sw\/arch\/RHEL8\/EB_production\/2022\/software\/OpenSSL\/1.1\/lib' --action_env=PYTHONNOUSERSITE='1' --host_action_env=PYTHONNOUSERSITE='1' --action_env=PYTHONPATH --host_action_env=PYTHONPATH  \/\/tensorflow\/tools\/pip_package:build_pip_package\r\n```\r\n\r\n<\/details>\r\n\r\nI'm not 100% sure of all the details of the custom build, I used EasyBuild to build TF from sources, and the build recipy wasn't made by me.\r\n\r\nI don't understand enough of the TensorFlow threading model to know how to debug this issue. My specific questions would be:\r\n\r\n- Why does my custom TF 2.11 build bind all threads to one core? Other GOMP-based packages (e.g. `scipy`) do show correct binding on my system, with the same (OMP_) environment variables.\r\n- Why does tf-nightly not bind threads at all?\r\n\r\nAny general explanations of which potential threading models can be used in TF are also welcome.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\ntf.config.threading.set_intra_op_parallelism_threads(4)\r\n\r\nprint(f\"Inter_op_threads: {tf.config.threading.get_inter_op_parallelism_threads()}\")\r\nprint(f\"Intra_op_threads: {tf.config.threading.get_intra_op_parallelism_threads()}\")\r\n\r\nA = tf.random.normal([20000,20000])\r\nfor i in range(0,1):\r\n    B = tf.multiply(A,A)\r\n\r\nimport psutil\r\nimport os\r\n\r\ncurrent_process = psutil.Process()\r\nthreads = current_process.threads()\r\nprint(f\"Thread count: {len(threads)}\")\r\nfor thread in threads:\r\n    print('Child affinity is {}'.format(os.sched_getaffinity(thread.id)))\n```\n\n\n### Relevant log output\n\n```shell\n# output for custom build TF-2.11\r\n$ OMP_PROC_BIND=true OMP_PLACES=cores python tf_example.py\r\nInter_op_threads: 1\r\nIntra_op_threads: 4\r\nThread count: 15\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\nChild affinity is {96}\r\n\r\n# output for tf-nightly\r\n# Not sure what threading model tf-nightly uses, so I've set both OMP and KMP variables:\r\n$ OMP_PROC_BIND=true OMP_PLACES=cores KMP_AFFINITY=granularity=fine,verbose,compact,1,0 python tf_example.py\r\n\r\nInter_op_threads: 1\r\nIntra_op_threads: 4\r\nThread count: 14\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\nChild affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}\r\n\r\n\r\n# Note that this is generated on an HPC system in which I'm in a CGROUP with access to core 96-127, hence the core IDs start at 96.\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:performance","TF 2.11"],"created_at":"2023-06-12T16:23:43Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60843"},{"issue_number":362,"repository":"tensorflow\/tensorflow","title":"Duplicate logging inside custom training loop","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\nTF 2.12.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nWindows WSL Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\ncuDNN version 8600\n\n### GPU model and memory\n\nGTX 1070\n\n### Current Behaviour?\n\nHey guys.\r\nI am experiencing a strange behavior when I try to print something from inside my custom training loop.\r\nI want to run the code example as a .ipynb from a Windows machine with WSL Ubuntu 20.04. (https:\/\/github.com\/m5k5\/tf-example). \r\nThe code loops over the dataset and prints out the log statements but it somehow resets the steps after a while:\r\n\"\r\nLog at step 80\r\ntrain step at step  81\r\ntrain step at step  82\r\ntrain step at step  83\r\ntrain step at step  84\r\ntrain step at step  85\r\ntrain step at step  86\r\ntrain step at step  0\r\nLog at step 0\r\ntrain step at step  1\r\ntrain step at step  2\r\ntrain step at step  3\r\ntrain step at step  4\r\ntrain step at step  5\r\n\"\r\n\r\nOriginally, I have a project where I need to use the WSL to get the GPU processing within Windows. The print statements are used for logging metrics like accuracy and loss. The values themselves are calculated correctly but there are a lot of duplicates that I can not explain.\r\nDoes anyone have similar issues?\r\nThanks in advance!\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/github.com\/m5k5\/tf-example\n```\n\n\n### Relevant log output\n\n```shell\nStart of epoch 1\r\n2023-06-10 19:30:47.154065: I tensorflow\/core\/common_runtime\/executor.cc:1197] [\/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder\/_1' with dtype uint8 and shape [60000]\r\n\t [[{{node Placeholder\/_1}}]]\r\n2023-06-10 19:30:47.154065: I tensorflow\/core\/common_runtime\/executor.cc:1197] [\/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder\/_1' with dtype uint8 and shape [60000]\r\n\t [[{{node Placeholder\/_1}}]]\r\n2023-06-10 19:30:47.766468: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:424] Loaded cuDNN version 8600\r\n2023-06-10 19:30:47.154065: I tensorflow\/core\/common_runtime\/executor.cc:1197] [\/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder\/_1' with dtype uint8 and shape [60000]\r\n\t [[{{node Placeholder\/_1}}]]\r\n2023-06-10 19:30:47.766468: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:424] Loaded cuDNN version 8600\r\n2023-06-10 19:30:48.254207: I tensorflow\/compiler\/xla\/service\/service.cc:169] XLA service 0x7f2e52d1a840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2023-06-10 19:30:48.254240: I tensorflow\/compiler\/xla\/service\/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1\r\n2023-06-10 19:30:48.256794: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2023-06-10 19:30:48.348961: I .\/tensorflow\/compiler\/jit\/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\ntrain step at step  0\r\nLog at step 0\r\ntrain step at step  1\r\ntrain step at step  2\r\ntrain step at step  3\r\ntrain step at step  4\r\ntrain step at step  5\r\ntrain step at step  6\r\ntrain step at step  7\r\ntrain step at step  8\r\ntrain step at step  9\r\ntrain step at step  10\r\ntrain step at step  11\r\ntrain step at step  12\r\ntrain step at step  13\r\ntrain step at step  14\r\ntrain step at step  15\r\ntrain step at step  16\r\ntrain step at step  17\r\ntrain step at step  18\r\ntrain step at step  19\r\ntrain step at step  20\r\nLog at step 20\r\ntrain step at step  21\r\ntrain step at step  22\r\ntrain step at step  23\r\ntrain step at step  24\r\ntrain step at step  25\r\ntrain step at step  26\r\ntrain step at step  27\r\ntrain step at step  28\r\ntrain step at step  29\r\ntrain step at step  30\r\ntrain step at step  31\r\ntrain step at step  32\r\ntrain step at step  33\r\ntrain step at step  34\r\ntrain step at step  35\r\ntrain step at step  36\r\ntrain step at step  37\r\ntrain step at step  38\r\ntrain step at step  39\r\ntrain step at step  40\r\nLog at step 40\r\ntrain step at step  41\r\ntrain step at step  42\r\ntrain step at step  43\r\ntrain step at step  44\r\ntrain step at step  45\r\ntrain step at step  46\r\ntrain step at step  47\r\ntrain step at step  48\r\ntrain step at step  49\r\ntrain step at step  50\r\ntrain step at step  51\r\ntrain step at step  52\r\ntrain step at step  53\r\ntrain step at step  54\r\ntrain step at step  55\r\ntrain step at step  56\r\ntrain step at step  57\r\ntrain step at step  58\r\ntrain step at step  59\r\ntrain step at step  60\r\nLog at step 60\r\ntrain step at step  61\r\ntrain step at step  62\r\ntrain step at step  63\r\ntrain step at step  64\r\ntrain step at step  65\r\ntrain step at step  66\r\ntrain step at step  67\r\ntrain step at step  68\r\ntrain step at step  69\r\ntrain step at step  70\r\ntrain step at step  71\r\ntrain step at step  72\r\ntrain step at step  73\r\ntrain step at step  74\r\ntrain step at step  75\r\ntrain step at step  76\r\ntrain step at step  77\r\ntrain step at step  78\r\ntrain step at step  79\r\ntrain step at step  80\r\nLog at step 80\r\ntrain step at step  81\r\ntrain step at step  82\r\ntrain step at step  83\r\ntrain step at step  84\r\ntrain step at step  85\r\ntrain step at step  86\r\ntrain step at step  0\r\nLog at step 0\r\ntrain step at step  1\r\ntrain step at step  2\r\ntrain step at step  3\r\ntrain step at step  4\r\ntrain step at step  5\r\ntrain step at step  6\r\ntrain step at step  7\r\ntrain step at step  8\r\ntrain step at step  9\r\ntrain step at step  10\r\ntrain step at step  11\r\ntrain step at step  12\r\ntrain step at step  13\r\ntrain step at step  14\r\ntrain step at step  15\r\ntrain step at step  16\r\ntrain step at step  17\r\ntrain step at step  18\r\ntrain step at step  19\r\ntrain step at step  20\r\nLog at step 20\r\ntrain step at step  21\r\ntrain step at step  22\r\ntrain step at step  23\r\ntrain step at step  24\r\ntrain step at step  25\r\ntrain step at step  26\r\ntrain step at step  27\r\ntrain step at step  28\r\ntrain step at step  29\r\ntrain step at step  30\r\ntrain step at step  31\r\ntrain step at step  32\r\ntrain step at step  33\r\ntrain step at step  34\r\ntrain step at step  35\r\ntrain step at step  36\r\ntrain step at step  37\r\ntrain step at step  38\r\ntrain step at step  39\r\ntrain step at step  40\r\nLog at step 40\r\ntrain step at step  41\r\ntrain step at step  42\r\ntrain step at step  43\r\ntrain step at step  44\r\ntrain step at step  45\r\ntrain step at step  46\r\ntrain step at step  47\r\ntrain step at step  48\r\ntrain step at step  49\r\ntrain step at step  50\r\ntrain step at step  51\r\ntrain step at step  52\r\ntrain step at step  53\r\ntrain step at step  54\r\ntrain step at step  55\r\ntrain step at step  56\r\ntrain step at step  57\r\ntrain step at step  58\r\ntrain step at step  59\r\ntrain step at step  60\r\nLog at step 60\r\ntrain step at step  61\r\ntrain step at step  62\r\ntrain step at step  63\r\ntrain step at step  64\r\ntrain step at step  65\r\ntrain step at step  66\r\ntrain step at step  67\r\ntrain step at step  68\r\ntrain step at step  69\r\ntrain step at step  70\r\ntrain step at step  71\r\ntrain step at step  72\r\ntrain step at step  73\r\ntrain step at step  74\r\ntrain step at step  75\r\ntrain step at step  76\r\ntrain step at step  77\r\ntrain step at step  78\r\ntrain step at step  79\r\ntrain step at step  80\r\nLog at step 80\r\ntrain step at step  81\r\ntrain step at step  82\r\ntrain step at step  83\r\ntrain step at step  84\r\ntrain step at step  85\r\ntrain step at step  86\r\ntrain step at step  87\r\ntrain step at step  88\r\ntrain step at step  89\r\ntrain step at step  90\r\ntrain step at step  91\r\ntrain step at step  92\r\ntrain step at step  93\r\ntrain step at step  94\r\ntrain step at step  95\r\ntrain step at step  96\r\ntrain step at step  97\r\ntrain step at step  98\r\ntrain step at step  99\r\ntrain step at step  100\r\nLog at step 100\r\ntrain step at step  101\r\ntrain step at step  102\r\ntrain step at step  103\r\ntrain step at step  104\r\ntrain step at step  105\r\ntrain step at step  106\r\ntrain step at step  107\r\ntrain step at step  108\r\ntrain step at step  109\r\ntrain step at step  110\r\ntrain step at step  111\r\ntrain step at step  112\r\ntrain step at step  113\r\ntrain step at step  114\r\ntrain step at step  115\r\ntrain step at step  116\r\ntrain step at step  117\r\ntrain step at step  118\r\ntrain step at step  119\r\ntrain step at step  120\r\nLog at step 120\r\ntrain step at step  121\r\ntrain step at step  122\r\ntrain step at step  123\r\ntrain step at step  124\r\ntrain step at step  125\r\ntrain step at step  126\r\ntrain step at step  127\r\ntrain step at step  128\r\ntrain step at step  129\r\ntrain step at step  130\r\ntrain step at step  131\r\ntrain step at step  132\r\ntrain step at step  133\r\ntrain step at step  134\r\ntrain step at step  135\r\ntrain step at step  136\r\ntrain step at step  137\r\ntrain step at step  138\r\ntrain step at step  139\r\ntrain step at step  140\r\nLog at step 140\r\ntrain step at step  141\r\ntrain step at step  142\r\ntrain step at step  143\r\ntrain step at step  144\r\ntrain step at step  145\r\ntrain step at step  146\r\ntrain step at step  147\r\ntrain step at step  148\r\ntrain step at step  149\r\ntrain step at step  150\r\ntrain step at step  151\r\ntrain step at step  152\r\ntrain step at step  153\r\ntrain step at step  154\r\ntrain step at step  155\r\ntrain step at step  156\r\ntrain step at step  157\r\ntrain step at step  158\r\ntrain step at step  159\r\ntrain step at step  160\r\nLog at step 160\r\ntrain step at step  161\r\ntrain step at step  162\r\ntrain step at step  163\r\ntrain step at step  164\r\ntrain step at step  165\r\ntrain step at step  166\r\ntrain step at step  167\r\ntrain step at step  168\r\ntrain step at step  169\r\ntrain step at step  170\r\ntrain step at step  171\r\ntrain step at step  172\r\ntrain step at step  173\r\ntrain step at step  174\r\ntrain step at step  0\r\nLog at step 0\r\ntrain step at step  1\r\ntrain step at step  2\r\ntrain step at step  3\r\ntrain step at step  4\r\ntrain step at step  5\r\ntrain step at step  6\r\ntrain step at step  7\r\ntrain step at step  8\r\ntrain step at step  9\r\ntrain step at step  10\r\ntrain step at step  11\r\ntrain step at step  12\r\ntrain step at step  13\r\ntrain step at step  14\r\ntrain step at step  15\r\ntrain step at step  16\r\ntrain step at step  17\r\ntrain step at step  18\r\ntrain step at step  19\r\ntrain step at step  20\r\nLog at step 20\r\ntrain step at step  21\r\ntrain step at step  22\r\ntrain step at step  23\r\ntrain step at step  24\r\ntrain step at step  25\r\ntrain step at step  26\r\ntrain step at step  27\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:core","TF 2.12"],"created_at":"2023-06-10T17:49:40Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60833"},{"issue_number":363,"repository":"tensorflow\/tensorflow","title":"Unable to run RNN Model","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.10.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nWSL2 on Windows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n11.2\/8.1.0\n\n### GPU model and memory\n\nNVIDIA Quadro P4200, 16gb\n\n### Current Behaviour?\n\nAfter setting up the system to use gpu from WSL2 on windows 11, I am able to connect to GPU. But when I run  a tensorflow model, I am able to run CNN model on GPU but when i try to run RNN model I get the below mentioned message and the model does not run:\r\n\r\n\"W tensorflow\/core\/framework\/op_kernel.cc:1780] OP_REQUIRES failed at partitioned_function_ops.cc:115 : INVALID_ARGUMENT: No OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", seed2=0, is_training=true]\r\nRegistered devices: [CPU, GPU]\r\nRegistered kernels:\r\n  <no registered kernels>\r\n\r\n\t [[CudnnRNN]]\"\n\n### Standalone code to reproduce the issue\n\n```shell\nTrying RNN model with LSTM\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","comp:apis","wsl2","TF 2.10"],"created_at":"2023-06-09T12:01:54Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60826"},{"issue_number":364,"repository":"tensorflow\/tensorflow","title":"tf.keras.Model.predict passing x=tf.keras.utils.Sequence causing exceptions  ValueError: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.12.0\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n```Python\r\ntf_model.predict(test_generator)\r\n```\r\n\r\nwill cause exceptions\r\n\r\n```Python\r\nValueError: in user code:\r\n\r\n    File \"\/DATA\/home\/yuehc\/.local\/lib\/python3.8\/site-packages\/keras\/engine\/training.py\", line 2169, in predict_function  *\r\n        return step_function(self, iterator)\r\n    File \"\/DATA\/home\/yuehc\/.local\/lib\/python3.8\/site-packages\/keras\/engine\/training.py\", line 2155, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"\/DATA\/home\/yuehc\/.local\/lib\/python3.8\/site-packages\/keras\/engine\/training.py\", line 2143, in run_step  **\r\n        outputs = model.predict_step(data)\r\n    File \"\/DATA\/home\/yuehc\/.local\/lib\/python3.8\/site-packages\/keras\/engine\/training.py\", line 2110, in predict_step\r\n        x, _, _ = data_adapter.unpack_x_y_sample_weight(data)\r\n    File \"\/DATA\/home\/yuehc\/.local\/lib\/python3.8\/site-packages\/keras\/engine\/data_adapter.py\", line 1775, in unpack_x_y_sample_weight\r\n        raise ValueError(error_msg)\r\n\r\n    ValueError: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: (<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, None, None) dtype=float32>)\r\n```\r\n\r\nThe codes works well back in tf 2.4.0 and should still be working per release notes and the latest documentations.\r\n\r\nthe `test_generator` is a sub-class of `tf.keras.utils.Sequence` which works normally in tf 2.4.0\r\n\r\nThe return format for `__getitem__` is a tuple with a List element that indicate multiple inputs of the model\r\n```Python\r\ndef __getitem__(self, i) -> Tuple[List[numpy.array]]:\r\n    ...\r\n```\r\n\r\nFor an example, see below\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/45007045\/b8948609-693e-4918-8f1b-990fb1fa8bc8)\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/45007045\/0c41d2ce-7460-405b-867a-103e6a794cd2)\r\n![image](https:\/\/github.com\/tensorflow\/tensorflow\/assets\/45007045\/51b80f7a-9c39-4ea4-93ee-7812c9765fa7)\r\n\r\nI assume some behavors behind the `predict` interface are changed after the 2.4.0 to 2.12.0 upgrades.\r\n\r\nAny idea how to fix it? Thx in advance.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```Python\r\nimport tensorflow as tf\r\nimport math\r\nimport numpy as np\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import LSTM, Dense, Input\r\n\r\nclass BasicGenerator(tf.keras.utils.Sequence):\r\n\r\n    def __init__(self, x, y=None, w=None, *, obj=None, batch_size=256, **kwargs):\r\n        \"\"\"\r\n        a simple example of data generator by utilizing tf.keras.utils.Sequence\r\n\r\n        Parameters\r\n        ----------\r\n        x: List[np.array], input data, the array size: (n_samples, n_features)\r\n\r\n        y: np.arrya, input label, size: (n_samples, n_labels)\r\n\r\n        w: sample weights, default None\r\n\r\n        batch_size: batch_size, default 256\r\n        \"\"\"\r\n        super(BasicGenerator, self).__init__()\r\n\r\n        # basic params\r\n        self.x = x\r\n        self.y = y\r\n        self.w = w\r\n        self.batch_size = batch_size\r\n        \r\n    def __len__(self):\r\n        return math.ceil(self.x[0].shape[0] \/ self.batch_size)\r\n\r\n    def __getitem__(self, index):\r\n        \r\n        res = ()\r\n\r\n        b_x = [_x[index * self.batch_size:(index + 1) * self.batch_size] for _x in self.x]\r\n        res += (b_x,)\r\n\r\n        if self.y is not None:\r\n            b_y = self.y[index * self.batch_size:(index + 1) * self.batch_size]\r\n            res += (b_y,)\r\n\r\n        if self.w is not None:\r\n            b_w = self.w[index * self.batch_size:(index + 1) * self.batch_size]\r\n            res += (b_w,)\r\n        \r\n        return res\r\n\r\n# Define the model\r\ninput1 = Input(shape=(3, 1))\r\ninput2 = Input(shape=(3, 1))\r\n\r\nlstm1 = LSTM(50, activation='relu')(input1)\r\nlstm2 = LSTM(50, activation='relu')(input2)\r\n\r\nconcat = tf.keras.layers.concatenate([lstm1, lstm2])\r\noutput = Dense(1)(concat)\r\n\r\nmodel = Model(inputs=[input1, input2], outputs=output)\r\nmodel.compile(optimizer='adam', loss='mse')\r\n\r\n# Generate some fake data\r\nn_samples = 10000\r\nX_1 = np.random.rand(n_samples, 3, 1)\r\nX_2 = np.random.rand(n_samples, 3, 1)\r\ny = np.random.rand(n_samples, 1)\r\n\r\n# generator sub-classing from utils.Sequence\r\ntrain_generator = BasicGenerator(x=[X_1, X_2], y=y)\r\ntest_generator = BasicGenerator(x=[X_1, X_2])\r\n\r\n# fit\r\nmodel.fit(train_generator)\r\n\r\n# the problem occurs\r\nmodel.predict(\r\n    test_generator,\r\n    verbose=1,\r\n    max_queue_size=30,\r\n    workers=1,\r\n    use_multiprocessing=False\r\n)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```Python\r\n40\/40 [==============================] - 7s 14ms\/step - loss: 0.1985\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-13-82ba4c04abb5> in <module>\r\n     76 \r\n     77 # the problem occurs\r\n---> 78 model.predict(\r\n     79     test_generator,\r\n     80     verbose=1,\r\n\r\n\/DATA1\/anaconda3\/envs\/py38tf2.4\/lib\/python3.8\/site-packages\/keras\/utils\/traceback_utils.py in error_handler(*args, **kwargs)\r\n     68             # To get the full stack trace, call:\r\n     69             # `tf.debugging.disable_traceback_filtering()`\r\n---> 70             raise e.with_traceback(filtered_tb) from None\r\n     71         finally:\r\n     72             del filtered_tb\r\n\r\n\/DATA1\/anaconda3\/envs\/py38tf2.4\/lib\/python3.8\/site-packages\/keras\/engine\/training.py in tf__predict_function(iterator)\r\n     13                 try:\r\n     14                     do_return = True\r\n---> 15                     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\r\n     16                 except:\r\n     17                     do_return = False\r\n\r\nValueError: in user code:\r\n\r\n    File \"\/DATA1\/anaconda3\/envs\/py38tf2.4\/lib\/python3.8\/site-packages\/keras\/engine\/training.py\", line 2169, in predict_function  *\r\n        return step_function(self, iterator)\r\n    File \"\/DATA1\/anaconda3\/envs\/py38tf2.4\/lib\/python3.8\/site-packages\/keras\/engine\/training.py\", line 2155, in step_function  **\r\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"\/DATA1\/anaconda3\/envs\/py38tf2.4\/lib\/python3.8\/site-packages\/keras\/engine\/training.py\", line 2143, in run_step  **\r\n        outputs = model.predict_step(data)\r\n    File \"\/DATA1\/anaconda3\/envs\/py38tf2.4\/lib\/python3.8\/site-packages\/keras\/engine\/training.py\", line 2111, in predict_step\r\n        return self(x, training=False)\r\n    File \"\/DATA1\/anaconda3\/envs\/py38tf2.4\/lib\/python3.8\/site-packages\/keras\/utils\/traceback_utils.py\", line 70, in error_handler\r\n        raise e.with_traceback(filtered_tb) from None\r\n    File \"\/DATA1\/anaconda3\/envs\/py38tf2.4\/lib\/python3.8\/site-packages\/keras\/engine\/input_spec.py\", line 219, in assert_input_compatibility\r\n        raise ValueError(\r\n\r\n    ValueError: Layer \"model_3\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None) dtype=float32>]\r\n```\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF 2.12"],"created_at":"2023-06-08T10:16:56Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60813"},{"issue_number":365,"repository":"tensorflow\/tensorflow","title":"Exception in tf.function due to operation in inactive condition branch","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.14.0-dev20230606\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nWindows 11 x64\n\n### Mobile device\n\nNA\n\n### Python version\n\n3.10\n\n### Bazel version\n\nNA\n\n### GCC\/Compiler version\n\nNA\n\n### CUDA\/cuDNN version\n\nNA\n\n### GPU model and memory\n\nNA\n\n### Current Behaviour?\n\nA function decorated with `@tf.function` may fail to execute if it contains a conditional operation where the non-executing branch cannot be executed correctly for the current inputs. Whether the issue arises or not may depend on the input signature given to `tf.function`. See example for clarity.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n@tf.function(input_signature=[tf.TensorSpec([None], tf.int32)])\r\ndef f(x):\r\n    return tf.cond(tf.size(x) == 1,\r\n                   # The reshape in this branch can only execute properly when the condition is true\r\n                   lambda: tf.fill(tf.shape(x), tf.reshape(x, ())),\r\n                   lambda: x)\r\n\r\n# Works: this input is valid for both condition branches\r\ntf.print(f(tf.constant([1])))\r\n# [1]\r\n\r\n# Fails: this input is only valid for the false branch, which is the active one\r\ntf.print(f(tf.constant([1, 2])))\r\n# tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error: (see below)\r\n\r\n\r\n# If the shape in the function input signature is left completely undefined it works\r\n# If the tf.function is defined with no input signature it works correctly as well\r\n\r\n@tf.function(input_signature=[tf.TensorSpec(None, tf.int32)])\r\ndef f(x):\r\n    return tf.cond(tf.size(x) == 1,\r\n                   lambda: tf.fill(tf.shape(x), tf.reshape(x, ())),\r\n                   lambda: x)\r\n\r\ntf.print(f(tf.constant([1])))\r\n# [1]\r\n\r\ntf.print(f(tf.constant([1, 2])))\r\n# [1 2]\n```\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\r\n  File ...\r\n  File \"...\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"...\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 53, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\r\n\r\nDetected at node cond\/Reshape defined at (most recent call last):\r\n  ...\r\n\r\nInput to reshape is a tensor with 2 values, but the requested shape has 1\r\n         [[{{node cond\/Reshape}}]] [Op:__inference_f_23]\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:tf.function","TF 2.12"],"created_at":"2023-06-07T16:12:49Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60805"},{"issue_number":366,"repository":"tensorflow\/tensorflow","title":"could not run GPU on jupyter ","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.12.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nI could not get Tensorflow to run on GPUs.\r\n\r\nTF sees the GPUs on terminal, but not on jupyter lab.\r\n\r\n**Edited**\r\nFound a solution to see it on jupyterlab, but must manually repeat . Still, erratic misconfiguration seems to happen.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nI could not get Tensorflow to run on GPUs.\r\n\r\nTF sees the GPUs on terminal, but not on jupyter lab.\r\n\r\n** edited **\r\n\r\n\r\nEventually, after hours, I found a temporary solution in setting the paths each time, before I launch `jupyter lab` :\r\n\r\n\r\n\r\nCUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX\/lib\/:$CUDNN_PATH\/lib\r\n\r\n```\r\n\r\nNB : If I was to set \r\n```\r\nmkdir -p $CONDA_PREFIX\/etc\/conda\/activate.d\r\necho 'CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))' >> $CONDA_PREFIX\/etc\/conda\/activate.d\/env_vars.sh\r\necho 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX\/lib\/:$CUDNN_PATH\/lib' >> $CONDA_PREFIX\/etc\/conda\/activate.d\/env_vars.sh\r\n```\r\n\r\nas in `https:\/\/www.tensorflow.org\/install\/pip`\r\n\r\nSomehow it messy with jupyter, bcause the selected kernel from jupyter would not correspond to the kernel set from the script.\r\n\r\nNow I can see the GPUs on jupyter, but still - it crashes! And before, on CPU, it was not.\r\n\r\nWhen I run this script, using this library :\r\n\r\nhttps:\/\/pypi.org\/project\/umap-learn\/\r\n\r\ninstalled as in the description:\r\n\r\n```\r\nembedder = ParametricUMAP(\r\n    ## all the params ...\r\n)\r\n\r\n# now launch on GPU\r\nwith tf.device('\/GPU:0'):\r\n    embedding =  embedder.fit_transform(np.array([t.ravel() for t in train_data]))\r\n```\r\n\r\nthe code fails with the log output below.\r\n\r\nIf I close the jupyter lab connection, go back on the conda environment, \r\nset again:\r\n\r\n```\r\nCUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))\r\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX\/lib\/:$CUDNN_PATH\/lib\r\n```\r\nI\r\nand \r\n\r\n```\r\n# Install NVCC\r\nconda install -c nvidia cuda-nvcc=11.3.58\r\n# Configure the XLA cuda directory\r\nmkdir -p $CONDA_PREFIX\/etc\/conda\/activate.d\r\nprintf 'export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX\/lib\/\\n' >> $CONDA_PREFIX\/etc\/conda\/activate.d\/env_vars.sh\r\nsource $CONDA_PREFIX\/etc\/conda\/activate.d\/env_vars.sh\r\n# Copy libdevice file to the required path\r\nmkdir -p $CONDA_PREFIX\/lib\/nvvm\/libdevice\r\ncp $CONDA_PREFIX\/lib\/libdevice.10.bc $CONDA_PREFIX\/lib\/nvvm\/libdevice\/\r\n\r\n```\r\n\r\nAnd relaunch jupterlab, this time I get the GPU seen also in jupyter lab.\r\n\r\nRunning \r\n\r\n```\r\nwith tf.device('\/GPU:0'):\r\n    spec_embedding = spec_embedder.fit_transform(np.array([t.ravel() for t in train_data]))\r\n\r\n```\r\n\r\nIt yields\r\n\r\n```\r\n2023-06-06 21:55:04.944257: I tensorflow\/core\/common_runtime\/executor.cc:1197] [\/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients\/split_grad\/concat\/split\/split_dim' with dtype int32\r\n\t [[{{node gradients\/split_grad\/concat\/split\/split_dim}}]]\r\n```\r\n\r\nAnd cannot understand why it raise warning or error with `CPU` device, if I run on `GPU`. \r\n\r\nNot even sure it falled back to CPU, actually.\r\n\r\n\r\n---\r\n\r\nPlease advice how to sync jupter lab and conda.\r\nI did follow up with `ikernel` but it seems, after a lot of checking, that environment variables are not correctly read. Not sure if `kernel.json` fails to be updated properly, with thepath of the cuda libraries.\r\n\r\nPlease consider add a guide on:\r\nhttps:\/\/www.tensorflow.org\/install\/pip\r\n\r\nfor running TF on jupyter.\r\n\r\nMy situation is that I need to run from a remote cluster, and I think it is a frequent situation.\r\nhope this feedback is useful.\n```\n\n\n### Relevant log output\n\n```shell\n2023-06-06 21:40:08.385763: I tensorflow\/core\/common_runtime\/executor.cc:1197] [\/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients\/split_grad\/concat\/split\/split_dim' with dtype int32\r\n\t [[{{node gradients\/split_grad\/concat\/split\/split_dim}}]]\r\n2023-06-06 21:40:10.938429: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:424] Loaded cuDNN version 8600\r\n2023-06-06 21:40:11.641291: I tensorflow\/compiler\/xla\/service\/service.cc:169] XLA service 0x7f89f489b8d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2023-06-06 21:40:11.641337: I tensorflow\/compiler\/xla\/service\/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2023-06-06 21:40:11.641346: I tensorflow\/compiler\/xla\/service\/service.cc:177]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2023-06-06 21:40:11.641353: I tensorflow\/compiler\/xla\/service\/service.cc:177]   StreamExecutor device (2): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2023-06-06 21:40:11.641359: I tensorflow\/compiler\/xla\/service\/service.cc:177]   StreamExecutor device (3): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\r\n2023-06-06 21:40:11.646324: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2023-06-06 21:40:11.673008: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:530] Can't find libdevice directory ${CUDA_DIR}\/nvvm\/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\r\nSearched for CUDA in the following directories:\r\n  .\/cuda_sdk_lib\r\n  \/usr\/local\/cuda-11.8\r\n  \/usr\/local\/cuda\r\n  .\r\nYou can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=\/path\/to\/cuda will work.\r\n2023-06-06 21:40:11.673259: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.673658: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.673685: I tensorflow\/core\/common_runtime\/executor.cc:1197] [\/job:localhost\/replica:0\/task:0\/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n\t [[{{node StatefulPartitionedCall_16}}]]\r\n2023-06-06 21:40:11.700045: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.700414: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.729242: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.729590: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.755959: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.756308: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.783041: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.783397: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.809786: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.810134: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.836777: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.837129: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.864411: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.864767: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.892388: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.892738: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.919296: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.919647: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.946506: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.946866: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.974287: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:11.974699: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.245782: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.246188: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.272303: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.272657: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.322559: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.322899: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.350255: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.350736: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.379576: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.379966: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.406780: W tensorflow\/compiler\/xla\/service\/gpu\/llvm_gpu_backend\/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at .\/libdevice.10.bc\r\n2023-06-06 21:40:12.407233: W tensorflow\/core\/framework\/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at .\/libdevice.10.bc\r\n\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\nCell In[33], line 31\r\n      7 spec_embedder = ParametricUMAP(\r\n      8     metric = 'euclidean',\r\n      9     min_dist = 0.1, \r\n   (...)\r\n     27     n_training_epochs=1\r\n     28 )\r\n     30 with tf.device('\/GPU:0'):\r\n---> 31     spec_embedding = spec_embedder.fit_transform(np.array([t.ravel() for t in train_data]))\r\n\r\nFile ~\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/umap\/parametric_umap.py:217, in ParametricUMAP.fit_transform(self, X, y, precomputed_distances)\r\n    215     return super().fit_transform(precomputed_distances, y)\r\n    216 else:\r\n--> 217     return super().fit_transform(X, y)\r\n\r\nFile ~\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/umap\/umap_.py:2772, in UMAP.fit_transform(self, X, y)\r\n   2742 def fit_transform(self, X, y=None):\r\n   2743     \"\"\"Fit X into an embedded space and return that transformed\r\n   2744     output.\r\n   2745 \r\n   (...)\r\n   2770         Local radii of data points in the embedding (log-transformed).\r\n   2771     \"\"\"\r\n-> 2772     self.fit(X, y)\r\n   2773     if self.transform_mode == \"embedding\":\r\n   2774         if self.output_dens:\r\n\r\nFile ~\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/umap\/parametric_umap.py:202, in ParametricUMAP.fit(self, X, y, precomputed_distances)\r\n    200     return super().fit(precomputed_distances, y)\r\n    201 else:\r\n--> 202     return super().fit(X, y)\r\n\r\nFile ~\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/umap\/umap_.py:2684, in UMAP.fit(self, X, y)\r\n   2681     print(ts(), \"Construct embedding\")\r\n   2683 if self.transform_mode == \"embedding\":\r\n-> 2684     self.embedding_, aux_data = self._fit_embed_data(\r\n   2685         self._raw_data[index],\r\n   2686         self.n_epochs,\r\n   2687         init,\r\n   2688         random_state,  # JH why raw data?\r\n   2689     )\r\n   2690     # Assign any points that are fully disconnected from our manifold(s) to have embedding\r\n   2691     # coordinates of np.nan.  These will be filtered by our plotting functions automatically.\r\n   2692     # They also prevent users from being deceived a distance query to one of these points.\r\n   2693     # Might be worth moving this into simplicial_set_embedding or _fit_embed_data\r\n   2694     disconnected_vertices = np.array(self.graph_.sum(axis=1)).flatten() == 0\r\n\r\nFile ~\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/umap\/parametric_umap.py:462, in ParametricUMAP._fit_embed_data(self, X, n_epochs, init, random_state)\r\n    459     validation_data = None\r\n    461 # create embedding\r\n--> 462 history = self.parametric_model.fit(\r\n    463     edge_dataset,\r\n    464     epochs=self.loss_report_frequency * self.n_training_epochs,\r\n    465     steps_per_epoch=steps_per_epoch,\r\n    466     max_queue_size=100,\r\n    467     validation_data=validation_data,\r\n    468     **self.keras_fit_kwargs\r\n    469 )\r\n    470 # save loss history dictionary\r\n    471 self._history = history.history\r\n\r\nFile ~\/.local\/lib\/python3.10\/site-packages\/keras\/utils\/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n     67     filtered_tb = _process_traceback_frames(e.__traceback__)\r\n     68     # To get the full stack trace, call:\r\n     69     # `tf.debugging.disable_traceback_filtering()`\r\n---> 70     raise e.with_traceback(filtered_tb) from None\r\n     71 finally:\r\n     72     del filtered_tb\r\n\r\nFile ~\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/tensorflow\/python\/eager\/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     50 try:\r\n     51   ctx.ensure_initialized()\r\n---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     53                                       inputs, attrs, num_outputs)\r\n     54 except core._NotOkStatusException as e:\r\n     55   if name is not None:\r\n\r\nInternalError: Graph execution error:\r\n\r\nDetected at node 'StatefulPartitionedCall_16' defined at (most recent call last):\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\n      return _run_code(code, main_globals, None,\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\n      exec(code, run_globals)\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/ipykernel_launcher.py\", line 17, in <module>\r\n      app.launch_new_instance()\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/traitlets\/config\/application.py\", line 992, in launch_instance\r\n      app.start()\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/ipykernel\/kernelapp.py\", line 711, in start\r\n      self.io_loop.start()\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/tornado\/platform\/asyncio.py\", line 215, in start\r\n      self.asyncio_loop.run_forever()\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\n      self._run_once()\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\n      handle._run()\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\n      self._context.run(self._callback, *self._args)\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 510, in dispatch_queue\r\n      await self.process_one()\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 499, in process_one\r\n      await dispatch(*args)\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 406, in dispatch_shell\r\n      await result\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 729, in execute_request\r\n      reply_content = await reply_content\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/ipykernel\/ipkernel.py\", line 411, in do_execute\r\n      res = shell.run_cell(\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/ipykernel\/zmqshell.py\", line 531, in run_cell\r\n      return super().run_cell(*args, **kwargs)\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3006, in run_cell\r\n      result = self._run_cell(\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3061, in _run_cell\r\n      result = runner(coro)\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/IPython\/core\/async_helpers.py\", line 129, in _pseudo_sync_runner\r\n      coro.send(None)\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3266, in run_cell_async\r\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3445, in run_ast_nodes\r\n      if await self.run_code(code, result, async_=asy):\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3505, in run_code\r\n      exec(code_obj, self.user_global_ns, self.user_ns)\r\n    File \"\/tmp\/ipykernel_3110734\/3903349550.py\", line 31, in <module>\r\n      spec_embedding = spec_embedder.fit_transform(np.array([t.ravel() for t in train_data]))\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/umap\/parametric_umap.py\", line 217, in fit_transform\r\n      return super().fit_transform(X, y)\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/umap\/umap_.py\", line 2772, in fit_transform\r\n      self.fit(X, y)\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/umap\/parametric_umap.py\", line 202, in fit\r\n      return super().fit(X, y)\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/umap\/umap_.py\", line 2684, in fit\r\n      self.embedding_, aux_data = self._fit_embed_data(\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/umap\/parametric_umap.py\", line 462, in _fit_embed_data\r\n      history = self.parametric_model.fit(\r\n    File \"\/home\/h21\/luas6629\/.local\/lib\/python3.10\/site-packages\/keras\/utils\/traceback_utils.py\", line 65, in error_handler\r\n      return fn(*args, **kwargs)\r\n    File \"\/home\/h21\/luas6629\/.local\/lib\/python3.10\/site-packages\/keras\/engine\/training.py\", line 1685, in fit\r\n      tmp_logs = self.train_function(iterator)\r\n    File \"\/home\/h21\/luas6629\/.local\/lib\/python3.10\/site-packages\/keras\/engine\/training.py\", line 1284, in train_function\r\n      return step_function(self, iterator)\r\n    File \"\/home\/h21\/luas6629\/.local\/lib\/python3.10\/site-packages\/keras\/engine\/training.py\", line 1268, in step_function\r\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\r\n    File \"\/home\/h21\/luas6629\/.local\/lib\/python3.10\/site-packages\/keras\/engine\/training.py\", line 1249, in run_step\r\n      outputs = model.train_step(data)\r\n    File \"\/home\/h21\/luas6629\/miniconda3\/envs\/conda01\/lib\/python3.10\/site-packages\/umap\/parametric_umap.py\", line 1150, in train_step\r\n      self.optimizer.apply_gradients(zip(gradients, trainable_vars))\r\n    File \"\/home\/h21\/luas6629\/.local\/lib\/python3.10\/site-packages\/keras\/optimizers\/optimizer.py\", line 1174, in apply_gradients\r\n      return super().apply_gradients(grads_and_vars, name=name)\r\n    File \"\/home\/h21\/luas6629\/.local\/lib\/python3.10\/site-packages\/keras\/optimizers\/optimizer.py\", line 650, in apply_gradients\r\n      iteration = self._internal_apply_gradients(grads_and_vars)\r\n    File \"\/home\/h21\/luas6629\/.local\/lib\/python3.10\/site-packages\/keras\/optimizers\/optimizer.py\", line 1200, in _internal_apply_gradients\r\n      return tf.__internal__.distribute.interim.maybe_merge_call(\r\n    File \"\/home\/h21\/luas6629\/.local\/lib\/python3.10\/site-packages\/keras\/optimizers\/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\r\n      distribution.extended.update(\r\n    File \"\/home\/h21\/luas6629\/.local\/lib\/python3.10\/site-packages\/keras\/optimizers\/optimizer.py\", line 1245, in apply_grad_to_update_var\r\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\r\nNode: 'StatefulPartitionedCall_16'\r\nlibdevice not found at .\/libdevice.10.bc\r\n\t [[{{node StatefulPartitionedCall_16}}]] [Op:__inference_train_function_4496]\n```\n<\/details>","labels":["type:bug","type:feature","comp:gpu","TF 2.12"],"created_at":"2023-06-06T20:06:53Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60790"},{"issue_number":367,"repository":"tensorflow\/tensorflow","title":"Inference model error when xla enabled with error message \"OP_REQUIRES failed at xla_ops.cc:462 : NOT_FOUND: could not find registered platform with id: 0x7f7537df9c24\"","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ntf2.9.2\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nLinux Ubuntu 16.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython 3.10.6\n\n### Bazel version\n\n5.3.2\n\n### GCC\/Compiler version\n\ngcc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0\n\n### CUDA\/cuDNN version\n\n11.6\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nA bug happened!\n\n### Standalone code to reproduce the issue\n\n```shell\ninference model error in c++ running with xla enabled. when xla disabled, all works fine.\r\n\r\nrun with option:\r\nexport XLA_FLAGS=\"--xla_dump_to=\/tmp\/generated --xla_hlo_profile\"\r\nexport TF_XLA_FLAGS=\"--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit\"\n```\n\n\n### Relevant log output\n\n```shell\n2023-06-06 13:54:27.650731: W tensorflow\/core\/framework\/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:462 : NOT_FOUND: could not find registered platform with id: 0x7f7537df9c2\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:runtime","comp:xla","TF 2.9"],"created_at":"2023-06-06T07:25:18Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60785"},{"issue_number":368,"repository":"tensorflow\/tensorflow","title":"inconsistent .proto file package names break gRPC message\/field parsing in Wireshark","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.12\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nLinux Ubuntu 18.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nAs noted in #12445, there is inconsistency among the package names in the TensorFlow `.proto` files. Searching for `.proto` file package declarations within the codebase reveals a wide variety of package names, including `tensorflow.dummy`.\r\nhttps:\/\/github.com\/search?q=repo%3Atensorflow%2Ftensorflow+%22package+tensorflow%22&type=code&p=2\r\n\r\nThis has a problematic effect when trying to parse the Protobuf fields in TensorFlow gRPC messages within the [Wireshark](https:\/\/www.wireshark.org\/) network capturing tool. In Wireshark, the built-in parsing functionality requires the package\/service names within the `.proto` files to match the package\/service names in the captured gRPC messages, so currently, [CoordinationService](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/tsl\/protobuf\/coordination_service.proto) (`package tensorflow`) messages parse properly, while message types and field names in [WorkerService](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/core\/protobuf\/worker_service.proto) (`package tensorflow.grpc`) messages cannot be parsed, and appear as _unknown_.\r\n\r\nCurrent workaround: using a script to replace all instances of `\"tensorflow.grpc\"` with `\"tensorflow\"` in the `.proto` files.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/d0863698de84277282df6f2865795aaa1e22ace5\/tensorflow\/tsl\/protobuf\/coordination_service.proto#L3\r\n\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/d0863698de84277282df6f2865795aaa1e22ace5\/tensorflow\/core\/protobuf\/worker_service.proto#L18\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:core","TF 2.12"],"created_at":"2023-06-03T20:22:40Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60773"},{"issue_number":369,"repository":"tensorflow\/tensorflow","title":"Inconsistency-bug in `tf.raw_ops.AddN` between jit mode and normal mode","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.14.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nUbuntu 20.04\n\n### Python version\n\n3.8.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nThere is an inconsistency bug between **jit compile mode** and **normal mode** in `tf.raw_ops.AddN` which result in inconsistent computational result especially when the data type is `half`. But expectedly, the computational result have to be the same.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os \r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ninputs = 44 * [tf.random.uniform([1, 2, 4, 3], dtype=tf.dtypes.half, maxval=1000)] # the half datatype!\r\n\r\n@tf.function(jit_compile=True)\r\ndef fuzz_jit():\r\n    y = tf.raw_ops.AddN(\r\n        inputs = inputs\r\n    )\r\n    return y\r\n\r\ndef fuzz_normal():\r\n    y = tf.raw_ops.AddN(\r\n        inputs = inputs\r\n    )\r\n    return y\r\n\r\ny1 = fuzz_jit()\r\nprint('[+] JIT ok')\r\ny2 = fuzz_normal()\r\nprint('[+] Normal ok')\r\nnp.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)\n```\n\n\n### Relevant log output\n\n```shell\n% python test.py\r\n[+] JIT ok\r\n[+] Normal ok\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 25, in <module>\r\n    np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/numpy\/testing\/_private\/utils.py\", line 1530, in assert_allclose\r\n    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/numpy\/testing\/_private\/utils.py\", line 844, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\n\r\nMismatched elements: 20 \/ 24 (83.3%)\r\nMax absolute difference: 160.\r\nMax relative difference: 0.004887\r\n x: array([[[[14864., 41344., 39872.],\r\n         [ 2492.,  6852., 17664.],\r\n         [17344., 33216., 39968.],...\r\n y: array([[[[14912., 41376., 39808.],\r\n         [ 2488.,  6824., 17696.],\r\n         [17408., 33280., 40096.],...\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-06-02T10:59:11Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60765"},{"issue_number":370,"repository":"tensorflow\/tensorflow","title":"Inconsistency-bug in `tf.raw_ops.Acos` between jit mode and normal mode","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.14.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nUbuntu 20.04\n\n### Python version\n\n3.8.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nThere is an inconsistency bug between **jit compile mode** and **normal mode** in `tf.raw_ops.Acos` which result in inconsistent computational result (mainly occur while dtype is complex, it seems that there are something wrong in the support of complex number).\r\nBut expectedly, the computational result have to be the same.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os \r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nx = tf.cast(tf.random.uniform([2, 1, 3, 4], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex128)\r\n\r\n@tf.function(jit_compile=True)\r\ndef fuzz_jit():\r\n    y = tf.raw_ops.Acos(\r\n        x = x\r\n    )\r\n    return y\r\n\r\ndef fuzz_normal():\r\n    y = tf.raw_ops.Acos(\r\n        x = x\r\n    )\r\n    return y\r\n\r\ny1 = fuzz_jit()\r\nprint('[+] JIT ok')\r\ny2 = fuzz_normal()\r\nprint('[+] Normal ok')\r\nnp.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)\n```\n\n\n### Relevant log output\n\n```shell\n% python test.py\r\n[+] JIT ok\r\n[+] Normal ok\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 25, in <module>\r\n    np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/numpy\/testing\/_private\/utils.py\", line 1530, in assert_allclose\r\n    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/numpy\/testing\/_private\/utils.py\", line 844, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=0.0001\r\n\r\nMismatched elements: 24 \/ 24 (100%)\r\nMax absolute difference: 23.34874158\r\nMax relative difference: 2.00000002\r\n x: array([[[[4.020810e-07+11.649195j, 3.918355e-07+11.63629j ,\r\n          6.167066e-09 +9.56048j , 2.750275e-10 +8.005427j],\r\n         [3.286194e-07+11.548319j, 8.673153e-08+10.882277j,...\r\n y: array([[[[0.-11.649196j, 0.-11.63629j , 0. -9.56048j , 0. -8.005427j],\r\n         [0.-11.548319j, 0.-10.882278j, 0.-10.521193j, 0.-10.125081j],\r\n         [0.-10.732039j, 0. -7.92731j , 0.-11.674371j, 0.-11.332818j]]],...\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.12"],"created_at":"2023-06-02T10:50:29Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60764"},{"issue_number":371,"repository":"tensorflow\/tensorflow","title":"Configure script automatically selects CUDA\/cuDNN path instead of waiting for user input","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\nTF 2.10\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nFedora 37\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n5.3.0\n\n### GCC\/Compiler version\n\n12.3.1\n\n### CUDA\/cuDNN version\n\n11.8,12.1\/8.0\n\n### GPU model and memory\n\nGTX 1660 Ti, 6 GB\n\n### Current Behaviour?\n\nI am having multiple CUDA versions, and I am trying to build Tensorflow from source with CUDA support.\r\n\r\nNow the problem lays when I try to configure the build system using `.\/configure`. It will asks for relevant information for the build system. This includes:\r\n\r\n1. Python path\r\n2. Python packages path\r\n3. Whether to support mROC\r\n4. Whether to support CUDA\r\n5. Whether to support TensorRT\r\n\r\nNow, when I select CUDA support. the script seems to automatically selects my CUDA\/cuDNN versions, and does not give me the possibility to select it manually, which is contradictory to what the documentation suggests at  [https:\/\/www.tensorflow.org\/install\/source#gpu_support](url):  _\"If your system has multiple versions of CUDA or cuDNN installed, explicitly set the version instead of relying on the default\"_\r\n\r\nNow, I was able to trace the issue exactly to the `configure.py` file. \r\nIn fact, I strongly suspects that there is a logical error on the section that parses the user input (Line 1244 on branch r2.11):\r\n```python\r\n  environ_save = dict(environ_cp)\r\n  for _ in range(_DEFAULT_PROMPT_ASK_ATTEMPTS):\r\n    if validate_cuda_config(environ_cp):\r\n      cuda_env_names = [\r\n          'TF_CUDA_VERSION',\r\n          'TF_CUBLAS_VERSION',\r\n          'TF_CUDNN_VERSION',\r\n          'TF_TENSORRT_VERSION',\r\n          'TF_NCCL_VERSION',\r\n          'TF_CUDA_PATHS',\r\n          # Items below are for backwards compatibility when not using\r\n          # TF_CUDA_PATHS.\r\n          'CUDA_TOOLKIT_PATH',\r\n          'CUDNN_INSTALL_PATH',\r\n          'NCCL_INSTALL_PATH',\r\n          'NCCL_HDR_PATH',\r\n          'TENSORRT_INSTALL_PATH'\r\n      ]\r\n      # Note: set_action_env_var above already writes to bazelrc.\r\n      for name in cuda_env_names:\r\n        if name in environ_cp:\r\n          write_action_env_to_bazelrc(name, environ_cp[name])\r\n      break\r\n\r\n    # Restore settings changed below if CUDA config could not be validated.\r\n    environ_cp = dict(environ_save)\r\n\r\n    set_tf_cuda_version(environ_cp)\r\n    set_tf_cudnn_version(environ_cp)\r\n    if is_windows():\r\n      set_tf_tensorrt_version(environ_cp)\r\n    if is_linux():\r\n      set_tf_tensorrt_version(environ_cp)\r\n      set_tf_nccl_version(environ_cp)\r\n\r\n    set_tf_cuda_paths(environ_cp)\r\n```\r\n\r\nNow, from my understanding, the script will validate the given environment, and then if that fails will ask for user input.\r\nWith that, on the first iteration of the loop, the validation will not contain the required environment variables.\r\n\r\nI was able to solve the issue by swapping the order as follow:\r\n```python\r\n    environ_save = dict(environ_cp)\r\n    for _ in range(_DEFAULT_PROMPT_ASK_ATTEMPTS):\r\n      # Restore settings changed below if CUDA config could not be validated.\r\n      environ_cp = dict(environ_save)\r\n\r\n      set_tf_cuda_version(environ_cp)\r\n      set_tf_cudnn_version(environ_cp)\r\n      if is_windows():\r\n        set_tf_tensorrt_version(environ_cp)\r\n      if is_linux():\r\n        set_tf_tensorrt_version(environ_cp)\r\n        set_tf_nccl_version(environ_cp)\r\n\r\n      set_tf_cuda_paths(environ_cp)\r\n      if validate_cuda_config(environ_cp):\r\n        cuda_env_names = [\r\n            'TF_CUDA_VERSION',\r\n            'TF_CUBLAS_VERSION',\r\n            'TF_CUDNN_VERSION',\r\n            'TF_TENSORRT_VERSION',\r\n            'TF_NCCL_VERSION',\r\n            'TF_CUDA_PATHS',\r\n            # Items below are for backwards compatibility when not using\r\n            # TF_CUDA_PATHS.\r\n            'CUDA_TOOLKIT_PATH',\r\n            'CUDNN_INSTALL_PATH',\r\n            'NCCL_INSTALL_PATH',\r\n            'NCCL_HDR_PATH',\r\n            'TENSORRT_INSTALL_PATH'\r\n        ]\r\n        # Note: set_action_env_var above already writes to bazelrc.\r\n        for name in cuda_env_names:\r\n          if name in environ_cp:\r\n            write_action_env_to_bazelrc(name, environ_cp[name])\r\n        break\r\n```\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nAssumption: Multiple CUDA versions on \/usr\/local\r\n\r\nCommand:\r\n.\/configure\r\n\r\nInput Example:\r\n1. [Default Setting]\r\n2. [Default Setting]\r\n3. N\r\n4. y\r\n5. N\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","TF 2.10"],"created_at":"2023-06-02T04:15:37Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60760"},{"issue_number":372,"repository":"tensorflow\/tensorflow","title":"Check fail can be triggered in `tf.raw_ops.EmptyTensorList` due to overflow under jit compile mode.","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.14.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nUbuntu 20.04\n\n### Python version\n\n3.8.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nCheck fail can be triggered in `tf.raw_ops.EmptyTensorList` under jit compile mode. While in normal mode, it won't be triggered but through an InvalidArgumentError.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os \r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nelement_shape=tf.random.uniform([3], dtype=tf.dtypes.int64, minval=-100000000, maxval=1000000000)\r\nmax_num_elements=tf.random.uniform([], dtype=tf.dtypes.int32, minval=-100000, maxval=1000000)\r\nelement_dtype=tf.dtypes.int32\r\n\r\n@tf.function(jit_compile=True)\r\ndef fuzz_jit():\r\n    y = tf.raw_ops.EmptyTensorList(element_shape=element_shape, max_num_elements=max_num_elements, element_dtype=element_dtype)\r\n    return y\r\n\r\ndef fuzz_normal():\r\n    y = tf.raw_ops.EmptyTensorList(element_shape=element_shape, max_num_elements=max_num_elements, element_dtype=element_dtype)\r\n    return y\r\n\r\ny1 = fuzz_jit() # trigger the check fail under jit compile mode.\r\nprint('[+] JIT ok')\r\ny2 = fuzz_normal() # if you run y2 first, it will through error rather than check fail.\r\nprint('[+] Normal ok')\n```\n\n\n### Relevant log output\n\n```shell\n% python test.py\r\n2023-06-02 11:41:06.810728: F tensorflow\/core\/framework\/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 246417692688323817 with 562326056, result: -1\r\nzsh: abort (core dumped)  python test.py\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-06-02T03:41:51Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60758"},{"issue_number":373,"repository":"tensorflow\/tensorflow","title":"Check fail can be triggered in `tf.raw_ops.GatherV2` under jit compile mode.","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.14.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nUbuntu 20.04\n\n### Python version\n\n3.8.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nCheck fail can be triggered in `tf.raw_ops.GatherV2` under jit compile mode. While in normal mode, it won't be triggered but through an `InvalidArgumentError`.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport os \r\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nparams=tf.random.uniform([4], dtype=tf.dtypes.float32, maxval=100000000)\r\nindices=tf.random.uniform([4, 0], dtype=tf.dtypes.int32, minval=-10000, maxval=60000)\r\naxis=tf.random.uniform([], dtype=tf.dtypes.int64, minval=-100000000, maxval=1000000000)\r\nbatch_dims=11282\r\n\r\n@tf.function(jit_compile=True)\r\ndef fuzz_jit():\r\n    y = tf.raw_ops.GatherV2(params=params, indices=indices, axis=axis, batch_dims=batch_dims)\r\n    return y\r\n\r\ndef fuzz_normal():\r\n    y = tf.raw_ops.GatherV2(params=params, indices=indices, axis=axis, batch_dims=batch_dims)\r\n    return y\r\n\r\ny1 = fuzz_jit()  # trigger the check fail under jit compile mode.\r\nprint('[+] JIT ok')\r\ny2 = fuzz_normal()  # if you run y2 first, it will through error rather than check fail.\r\nprint('[+] Normal ok')\n```\n\n\n### Relevant log output\n\n```shell\n% python test.py\r\n2023-06-02 11:10:06.042625: F tensorflow\/core\/framework\/shape_inference.cc:705] Check failed: rank >= 0 (0 vs. -11280)rank must not be negative\r\nzsh: abort (core dumped)  python test.py\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-06-02T03:14:31Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60756"},{"issue_number":374,"repository":"tensorflow\/tensorflow","title":"[TF 2.0] Signature for unranked tensor not working as intended","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.14\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nLinux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\ntrying to save a model where one of the functions in the model is defined as:\r\n\r\n```\r\nclass XYZ(tf.keras.models.Model):\r\n\r\n  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])\r\n  def coefficients(self, input_tensor: tf.Tensor):\r\n  ....\r\n  ....\r\n```\r\n\r\n\r\nWhen I train the model, and try to build it, I get the following error:\r\n\r\n```\r\nFile \"\/build\/work\/83bb824f51c38f677e7ad84666ae2d5f4deb\/google3\/runfiles\/google3\/vr\/perception\/computational_photography\/ml\/video_enhancement\/nets\/hdrnet.py\", [line 320](https:\/\/cs.corp.google.com\/piper\/\/\/depot\/google3\/vr\/perception\/computational_photography\/ml\/video_enhancement\/nets\/hdrnet.py?l=320&ws=mnatraj\/3402&snapshot=40), in call  *\r\n        grid, _ = self.coefficients(inputs[0])\r\n    File \"\/build\/work\/83bb824f51c38f677e7ad84666ae2d5f4deb\/google3\/runfiles\/google3\/third_party\/tensorflow\/python\/util\/traceback_utils.py\", [line 141](https:\/\/cs.corp.google.com\/piper\/\/\/depot\/google3\/third_party\/tensorflow\/python\/util\/traceback_utils.py?l=141&ws=mnatraj\/3402&snapshot=40), in error_handler  **\r\n        return fn(*args, **kwargs)\r\n    File \"\/build\/work\/83bb824f51c38f677e7ad84666ae2d5f4deb\/google3\/runfiles\/google3\/third_party\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py\", [line 820](https:\/\/cs.corp.google.com\/piper\/\/\/depot\/google3\/third_party\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py?l=820&ws=mnatraj\/3402&snapshot=40), in __call__\r\n        result = self._call(*args, **kwds)\r\n    File \"\/build\/work\/83bb824f51c38f677e7ad84666ae2d5f4deb\/google3\/runfiles\/google3\/third_party\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py\", [line 864](https:\/\/cs.corp.google.com\/piper\/\/\/depot\/google3\/third_party\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py?l=864&ws=mnatraj\/3402&snapshot=40), in _call\r\n        self._initialize(args, kwds, add_initializers_to=initializers)\r\n    File \"\/build\/work\/83bb824f51c38f677e7ad84666ae2d5f4deb\/google3\/runfiles\/google3\/third_party\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py\", [line 687](https:\/\/cs.corp.google.com\/piper\/\/\/depot\/google3\/third_party\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py?l=687&ws=mnatraj\/3402&snapshot=40), in _initialize\r\n        self._variable_creation_fn.get_concrete_function(args, kwds)\r\n    File \"\/build\/work\/83bb824f51c38f677e7ad84666ae2d5f4deb\/google3\/runfiles\/google3\/third_party\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py\", [line 182](https:\/\/cs.corp.google.com\/piper\/\/\/depot\/google3\/third_party\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py?l=182&ws=mnatraj\/3402&snapshot=40), in get_concrete_function\r\n        args, kwargs = function_type_utils.bind_function_inputs(\r\n    File \"\/build\/work\/83bb824f51c38f677e7ad84666ae2d5f4deb\/google3\/runfiles\/google3\/third_party\/tensorflow\/python\/eager\/polymorphic_function\/function_type_utils.py\", [line 451](https:\/\/cs.corp.google.com\/piper\/\/\/depot\/google3\/third_party\/tensorflow\/python\/eager\/polymorphic_function\/function_type_utils.py?l=451&ws=mnatraj\/3402&snapshot=40), in bind_function_inputs\r\n        raise TypeError(\r\n\r\n    TypeError: Binding inputs to tf.function failed due to `Can not cast TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32, name=None) to TensorSpec(shape=(None,), dtype=tf.float32, name=None)`. Received args: (<tf.Tensor 'Placeholder:0' shape=(1, 256, 256, 3) dtype=float32>,) and kwargs: {} for signature: (input_tensor: TensorSpec(shape=(None,), dtype=tf.float32, name=None)).\r\n```\r\n\r\n\r\nShouldn't specifying input signature as `None` imply that you can pass any input into this function? (unranked tensor)\n\n### Standalone code to reproduce the issue\n\n```shell\n(no standalone code)\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:apis"],"created_at":"2023-05-30T22:11:27Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60732"},{"issue_number":375,"repository":"tensorflow\/tensorflow","title":"[TFLite C++] Signature calculating CategoricalCrossentropy loss produces wrong result","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.13\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nWindows 10\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n5.3.0\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nI've created a simple model in Python (TF version 2.10) and converted it for tflite. The model has two signatures, one for inference and other for training. When I run those signatures in Python, everything works correctly, I get good inference result and good training loss. When I load the converted tflite model with the C++ TFLite API (built from source, from branch r2.13) and run those signatures: inference works as intended, training works as intended (the accuracy on the test set is steadily rising), but the reported loss is totally random. At first I thought that loss might be accumulated since it is rising to five digits, but that is not the case since it rises and falls in a random fashion. It seems like there is some bug in the ops used for CategoricalCrossentropy C++ TFLite implementation.\r\n\r\nI've tried building tensorflow from r2.12 and r2.13 and I get the same behavior. I've tried r2.10 also but then I couldn't even run the signatures with C++ TFLite API, I was getting bunch of segmentation faults. I couldn't find anywhere the documentation on what ops for backward prop are available in C++ TFLite API, maybe some of those which are used in CategoricalCrossentropy loss calculation are not yet available, or there is a bug in their implementation.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nHere is a Python code I am using to create model with signatures:\r\n```\r\nIMG_SIZE = 28\r\n\r\nclass Model(tf.Module):\r\n    def __init__(self):\r\n        self.model = tf.keras.Sequential([\r\n            tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE), name='flatten'),\r\n            tf.keras.layers.Dense(\r\n                units=10,\r\n                kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),\r\n                bias_initializer=tf.keras.initializers.Ones(),\r\n                name='dense'\r\n            ),\r\n        ])\r\n\r\n        opt = tf.keras.optimizers.SGD(learning_rate=0.1)\r\n        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\r\n        self.model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])\r\n\r\n    # The `train` function takes a batch of input images and labels.\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec([32, IMG_SIZE, IMG_SIZE], tf.float32),\r\n        tf.TensorSpec([32, 10], tf.float32),\r\n    ])\r\n    def train(self, x, y):\r\n        with tf.GradientTape() as tape:\r\n            prediction = self.model(x)\r\n            loss = self.model.loss(y, prediction)\r\n        gradients = tape.gradient(loss, self.model.trainable_variables)\r\n        self.model.optimizer.apply_gradients(\r\n            zip(gradients, self.model.trainable_variables))\r\n        result = {\"loss\": loss}\r\n        return result\r\n\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec([1, IMG_SIZE, IMG_SIZE], tf.float32),\r\n    ])\r\n    def infer(self, x):\r\n        logits = self.model(x)\r\n        probabilities = tf.nn.softmax(logits, axis=-1)\r\n        return {\r\n            \"output\": probabilities,\r\n            \"logits\": logits\r\n        }\r\n```\r\n\r\nAnd here is the C++ code I am using to run the tflite model:\r\n```\r\nstd::unique_ptr<tflite::FlatBufferModel> model =\r\n    tflite::FlatBufferModel::BuildFromFile(tflite_model_path);\r\nif (model == nullptr)\r\n{\r\n    std::cout << \"Failed to load model\" << std::endl;\r\n    return;\r\n}\r\n\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\ntflite::InterpreterBuilder builder(*model, resolver);\r\nstd::unique_ptr<tflite::Interpreter> interpreter;\r\nbuilder(&interpreter);\r\nif (interpreter == nullptr)\r\n{\r\n    std::cout << \"Failed to create interpreter\" << std::endl;\r\n    return;\r\n}\r\n\r\nif (interpreter->AllocateTensors() != kTfLiteOk)\r\n{\r\n    std::cout << \"Failed to alocate interpreter tensors\" << std::endl;\r\n    return;\r\n}\r\n\r\ntflite::SignatureRunner* train_runner = interpreter->GetSignatureRunner(\"train\");\r\n\r\nTfLiteTensor* input_data_tensor = train_runner->input_tensor(train_runner->input_names()[0]);\r\nfloat* input_data = input_data_tensor->data.f;\r\nTfLiteTensor* input_labels_tensor = train_runner->input_tensor(train_runner->input_names()[1]);\r\nfloat* input_labels = input_labels_tensor->data.f;\r\n\r\n\/\/ Here I fill in the input data and labels, code redacted for brevity.\r\n\r\nif (train_runner->Invoke() != kTfLiteOk)\r\n{\r\n    std::cout << \"Error invoking train interpreter signature\" << std::endl;\r\n    return;\r\n}\r\n\r\nconst TfLiteTensor* output_tensor = train_runner->output_tensor(train_runner->output_names()[0]);\r\nfloat* output = output_tensor->data.f;\r\nstd::cout << \"Training finished with loss: \" << output[0] << std::endl;\r\n```\r\n\r\nPlease let me know if you need more details, or full source code.\r\n\r\n\r\n### Relevant log output\r\n\r\nHere are the losses from batch to batch, as you can see they are too high and pretty much random. I repeat: the model is training correctly which I can see because the accuracy on the test set is steadily rising, so these loss values do not make sense.\r\n```\r\nTraining of batch 1 finished with loss: 172.813\r\nTraining of batch 2 finished with loss: 30406.2\r\nTraining of batch 3 finished with loss: 35372.7\r\nTraining of batch 4 finished with loss: 30955.9\r\nTraining of batch 5 finished with loss: 30645.5\r\nTraining of batch 6 finished with loss: 39069.4\r\nTraining of batch 7 finished with loss: 25181.5\r\nTraining of batch 8 finished with loss: 28106.7\r\nTraining of batch 9 finished with loss: 12969.1\r\nTraining of batch 10 finished with loss: 3079.69\r\nTraining of batch 11 finished with loss: 3693.12\r\nTraining of batch 12 finished with loss: 3314.77\r\nTraining of batch 13 finished with loss: 4591.12\r\nTraining of batch 14 finished with loss: 5880.76\r\nTraining of batch 15 finished with loss: 5654.75\r\nTraining of batch 16 finished with loss: 10133.1\r\nTraining of batch 17 finished with loss: 9301.94\r\nTraining of batch 18 finished with loss: 11654.5\r\nTraining of batch 19 finished with loss: 11827.8\r\nTraining of batch 20 finished with loss: 22028.1\r\nTraining of batch 21 finished with loss: 8553.58\r\n```\r\n\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:lite","type:performance","TF 2.13"],"created_at":"2023-05-27T22:40:40Z","comments":16,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60718"},{"issue_number":376,"repository":"tensorflow\/tensorflow","title":"Incorrect gradient in divide_no_nan and reciprocal_no_nan when divide by 0","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\ntf-nightly\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nUbuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nWhen we perform divide by zero in `tf.math.divide_no_nan` and `tf.math.reciprocal_no_nan`, the theoretical and numerical gradient given by `tf.test.compute_gradient` do not match. However, if the input is valid (without dividing by zero), the gradients are fine.\r\n\r\nMore examples in [gist here](https:\/\/colab.research.google.com\/drive\/1U102ToL3El9wHduuDyjQXtpsZkaByDg5?usp=sharing)\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\nx = tf.constant([10.0, 20.0])\r\ny = tf.constant([2.0, 0.0])\r\nth, nu= tf.test.compute_gradient(tf.math.divide_no_nan, [x, y])\r\nprint(th)\r\nprint(nu)\r\nprint(tf.experimental.numpy.allclose(th, nu, atol=1e-3))\r\n\r\n\r\nimport tensorflow as tf\r\nx = tf.constant([2.0, 0.0])\r\nth, nu= tf.test.compute_gradient(tf.math.reciprocal_no_nan, [x])\r\nprint(th)\r\nprint(nu)\r\nprint(tf.experimental.numpy.allclose(th, nu, atol=1e-3))\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n(array([[0.5, 0. ],\r\n       [0. , 0. ]], dtype=float32), array([[-2.5,  0. ],\r\n       [-0. ,  0. ]], dtype=float32))\r\n(array([[0.5, 0. ],\r\n       [0. , 0. ]], dtype=float32), array([[-2.5002441e+00,  0.0000000e+00],\r\n       [ 0.0000000e+00,  2.0971520e+07]], dtype=float32))\r\ntf.Tensor(False, shape=(), dtype=bool)\r\n\r\n\r\n\r\n(array([[-0.25,  0.  ],\r\n       [-0.  ,  0.  ]], dtype=float32),)\r\n(array([[-2.500000e-01,  0.000000e+00],\r\n       [ 0.000000e+00,  1.048576e+06]], dtype=float32),)\r\ntf.Tensor(False, shape=(), dtype=bool)\r\n```\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops"],"created_at":"2023-05-26T17:59:07Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60715"},{"issue_number":377,"repository":"tensorflow\/tensorflow","title":"`tf.split` or `tf.transpose` cause errors for quantize-aware training with `quantize_apply`","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.7 & 2.12\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nUbuntu 18.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.8\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nWe are trying to implement some network like [ShuffleNetV2](https:\/\/arxiv.org\/abs\/1807.11164) but encounter some error when `quantize_apply` the model.\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/22385182\/233325404-e2e502fe-1151-4f4c-9377-570a01681848.png)\r\n\r\nI believe ShuffleNet or related ideas are popular in edge devices, please kindly help us to resolve this proble.\r\n\r\nAny advice is welcome.\r\n\r\n_I apologize for this should have been posted as an issue on Tensorflow Model Optimization. However, since it seems that this problem is not unique to me, I'm posting it here in the hope of receiving appropriate suggestions or assistance._\r\n\r\n\r\n### System information\r\n\r\nTensorFlow version (installed from source or binary): 2.7.0\r\n\r\nTensorFlow Model Optimization version (installed from source or binary): 0.7.0\r\n\r\nPython version: 3.8.13\r\n\r\n**We also try on latest release of both module, but also not working.**\r\n\r\n### Describe the current behavior\r\n\r\nWhen running the provided code, either the `tf.transpose` or `tf.split` will cause error to Tensorflow Model Optimization.\r\n\r\nThe error message due to `tf.split` before convolution layers:\r\n\r\n```\r\nValueError: Exception encountered when calling layer \"bn3\" (type BatchNormalization).\r\n\r\nShape must be rank 4 but is rank 5 for '{{node bn3\/FusedBatchNormV3}} = FusedBatchNormV3[T=DT_FLOAT, U=DT_FLOAT, data_format=\"NHWC\", epsilon=0.001, exponential_avg_factor=1, is_training=false](Placeholder, bn3\/ReadVariableOp, bn3\/ReadVariableOp_1, bn3\/FusedBatchNormV3\/ReadVariableOp, bn3\/FusedBatchNormV3\/ReadVariableOp_1)' with input shapes: [1,?,128,128,32], [32], [32], [32], [32].\r\n```\r\n\r\nThe error message due to `tf.transpose`:\r\n\r\n```\r\nValueError: Exception encountered when calling layer \"tf.compat.v1.transpose\" (type TFOpLambda).\r\n\r\nDimension must be 6 but is 5 for '{{node tf.compat.v1.transpose\/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](tf.compat.v1.transpose\/transpose\/a, tf.compat.v1.transpose\/transpose\/perm)' with input shapes: [1,?,128,128,2,32], [5].\r\n```\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nJust run the following code you will get the error message due to `tf.split`.\r\n\r\n```python\r\nfrom __future__ import annotations\r\n\r\nfrom typing import Callable, Optional\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_model_optimization as tfmot\r\nfrom tensorflow.keras import layers\r\n\r\n\r\nSKIP_LAYER = [\r\n    \"resize\",\r\n    \"Resize\",\r\n    \"reshape\",\r\n    \"Reshape\",\r\n    \"concat\",\r\n    \"Concat\",\r\n    \"ExpandDims\",\r\n    \"Repeats\",\r\n    \"Shape\",\r\n    \"strided_slice\",\r\n    \"Tile\",\r\n]\r\n\r\n\r\ndef quantize_model(\r\n    model: tf.keras.Model,\r\n    annotate: Optional[Callable] = None,\r\n    quantize_scope: Optional[dict[str, tf.keras.layers.Layer]] = None,\r\n) -> tf.keras.Model:\r\n    quantize_scope = {} if quantize_scope is None else quantize_scope\r\n\r\n    def annotate(layer):\r\n        if any([name in layer.name for name in SKIP_LAYER]):\r\n            return layer\r\n        else:\r\n            return tfmot.quantization.keras.quantize_annotate_layer(layer)\r\n\r\n    anno_model = tf.keras.models.clone_model(model, clone_function=annotate)\r\n    with tfmot.quantization.keras.quantize_scope(quantize_scope):\r\n        model = tfmot.quantization.keras.quantize_apply(anno_model)\r\n\r\n    return model\r\n\r\n\r\ndef channel_shuffle(tensor: tf.Tensor, groups: int = 2) -> tf.Tensor:\r\n    \"\"\"Channel shuffle operation.\"\"\"\r\n    _, height, width, num_channels = tensor.shape.as_list()\r\n    assert num_channels % groups == 0\r\n\r\n    tensor = tf.reshape(tensor, [-1, height, width, groups, num_channels \/\/ groups])\r\n    tensor = tf.transpose(tensor, [0, 1, 2, 4, 3])\r\n    tensor = tf.identity(tensor, name=\"channel_shuffle\")\r\n\r\n    tensor = tf.reshape(tensor, [-1, height, width, num_channels])\r\n    return tensor\r\n\r\n\r\ndef simple_nn(img_input: tf.Tensor) -> tf.Tensor:\r\n    latent = layers.Conv2D(32, 1, padding=\"same\", use_bias=False, name=\"conv1\")(img_input)\r\n    latent = layers.BatchNormalization(name=\"bn1\")(latent)\r\n    latent = layers.ReLU(name=\"relu1\")(latent)\r\n\r\n    latent = layers.DepthwiseConv2D(3, 1, padding=\"same\", name=\"conv2\")(img_input)\r\n    latent = layers.BatchNormalization(name=\"bn2\")(latent)\r\n\r\n    latent = layers.Conv2D(32, 1, padding=\"same\", use_bias=False, name=\"conv3\")(img_input)\r\n    latent = layers.BatchNormalization(name=\"bn3\")(latent)\r\n    latent = layers.ReLU(name=\"relu3\")(latent)\r\n\r\n    return latent\r\n\r\n\r\ndef split_like_nn(img_input: tf.Tensor) -> tf.Tensor:\r\n    latent = layers.Conv2D(64, 1, padding=\"same\", use_bias=False, name=\"conv0\")(img_input)\r\n    latent = layers.BatchNormalization(name=\"bn0\")(latent)\r\n    latent = layers.ReLU(name=\"relu0\")(latent)\r\n\r\n    latent_0, latent_1 = tf.split(latent, 2, axis=-1)\r\n    latent_0 = simple_nn(latent_0)\r\n    latent = tf.concat([latent_0, latent_1], axis=-1)\r\n\r\n    latent = channel_shuffle(latent)\r\n\r\n    return latent\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    img_input = tf.keras.Input((128, 128, 1), dtype=tf.float32, name=\"img\")\r\n\r\n    outputs = split_like_nn(img_input)\r\n\r\n    model = tf.keras.Model(inputs=img_input, outputs=outputs, name=\"PoseNetV2\")\r\n    model.summary()\r\n\r\n    model_qat = quantize_model(model)\r\n    model_qat.summary()\r\n```\r\n\r\n\r\nYou can just comment the following three lines of code will get the error message from `tf.transpose`.\r\n\r\n```python\r\n latent_0, latent_1 = tf.split(latent, 2, axis=-1)\r\n latent_0 = simple_nn(latent_0)\r\n latent = tf.concat([latent_0, latent_1], axis=-1)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.12"],"created_at":"2023-05-26T17:33:07Z","comments":5,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60714"},{"issue_number":378,"repository":"tensorflow\/tensorflow","title":"Dataset.ragged_batch does not produce correct specs with tf.py_function and tf.numpy_function","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.12\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\ndocker container nvcr.io\/nvidia\/tensorflow:23.04-tf2-py3 on Ubuntu 22.04.2 LTS host\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nI'm trying to train an object detection model where images may have a different number of bounding boxes. Also I want to add some augmentations, and since tf does not support augmentations of bounding boxes I choose albumentations to do the job. I can't use albumentations' augmentations directly, so I need to use either `tf.py_function` or `tf.numpy_function`. I used `Dataset.ragged_batch` instead of `Dataset.batch` (because the dimension of bbox tensor may vary), but it did not provide me the correct `element_spec` and I was unable to make it work.\r\n\r\nThese are three scenarios that should help to understand the issue:\r\n\r\n### Scenario 1:\r\nI don't use any augmentations, `ragged_batch` returns the correct element spec, but I really need those augmentations\r\n```\r\n(TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=None),\r\n {'classes': RaggedTensorSpec(TensorShape([None, None]), tf.float32, 1, tf.int64),\r\n  'boxes': RaggedTensorSpec(TensorShape([None, None, 4]), tf.float64, 1, tf.int64)})\r\n```\r\n### Scenario 2:\r\nI use `tf.numpy_function` fo perform the augmentations. The spec is incorrect, I can't batch the items\r\n```\r\n(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\r\n {'classes': TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\r\n  'boxes': TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)})\r\n```\r\n### Scenario 3\r\nI use `tf.py_function`, provide something, that looks like correct spec to `Tout` param:\r\n```\r\nTout=[\r\n    tf.TensorSpec(shape=[None, 512, 512, 3], dtype=tf.float32), \r\n    tf.RaggedTensorSpec(shape=[None, 4], dtype=tf.float32), \r\n    tf.RaggedTensorSpec(shape=[None, None], dtype=tf.float32),\r\n],\r\n```\r\nbut spec for image still does not look good\r\n```\r\n(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\r\n {'classes': RaggedTensorSpec(TensorShape([None, None, None]), tf.float32, 2, tf.int64),\r\n  'boxes': RaggedTensorSpec(TensorShape([None, None, 4]), tf.float32, 2, tf.int64)})\r\n```\r\nand I had to add an extra dimension for labels spec in order to convert it to `RaggedTensor` (which is probably not good as well). Model refuses to be trained because of incorrect image dimensions\r\n\r\nThe (non)working code is here - [colab link](https:\/\/colab.research.google.com\/drive\/148i78QRnF98guvx1Y0gZXtBVrv-day42?usp=sharing)\n\n### Standalone code to reproduce the issue\n\n```shell\nimport keras_cv\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport albumentations as A\r\n\r\ndef generate_random_data():\r\n    while True:\r\n        image = np.random.randint(0, 256, size=(512, 512, 3), dtype=np.uint8)\r\n        num_bboxes = np.random.randint(1, 200)\r\n        bboxes = []\r\n        labels = []\r\n        for _ in range(num_bboxes):\r\n            x_min, y_min, x_max, y_max = np.sort(np.random.uniform(0, 512, size=4) \/ 512)\r\n            bbox = [x_min, y_min, x_max, y_max]\r\n            label = np.random.choice([0, 1])\r\n            bboxes.append(bbox)\r\n            labels.append(label)\r\n\r\n        data = {\r\n            'image': tf.convert_to_tensor(image),\r\n            'bboxes': {\r\n                'bbox': tf.convert_to_tensor(bboxes),\r\n                'label': tf.convert_to_tensor(labels, dtype=tf.int64),\r\n            }\r\n        }\r\n        yield data\r\n\r\n# Create the random dataset\r\ndataset = tf.data.Dataset.from_generator(generate_random_data, output_signature={\r\n    'image': tf.TensorSpec(shape=(512, 512, 3), dtype=tf.uint8),\r\n    'bboxes': {\r\n        'bbox': tf.TensorSpec(shape=(None, 4), dtype=tf.float64),\r\n        'label': tf.TensorSpec(shape=(None,), dtype=tf.int64),\r\n    },\r\n})\r\n\r\n\r\n# Scenario 1\r\ndef preprocess_data_1(inputs):\r\n    bounding_boxes = {\r\n        \"classes\": tf.cast(inputs[\"bboxes\"][\"label\"], dtype=tf.float32),\r\n        \"boxes\": inputs[\"bboxes\"][\"bbox\"],\r\n    }\r\n    return tf.image.convert_image_dtype(inputs['image'], tf.float32), bounding_boxes\r\nds_1 = dataset.map(preprocess_data_1).ragged_batch(2)\r\n\r\n# Scenario 2\r\ndef transform_2(image, bboxes, labels):\r\n    transforms = A.Compose(\r\n        [\r\n            A.Rotate(limit=40),\r\n        ],\r\n        bbox_params=A.BboxParams(\r\n            format='albumentations',\r\n            label_fields=['label'],\r\n        )\r\n    )\r\n\r\n    transformed = transforms(\r\n        image=image, \r\n        label=labels,\r\n        bboxes=bboxes,\r\n    )\r\n\r\n    return transformed\r\n\r\ndef aug_fn_2(image, bboxes, labels):\r\n    aug_data = transform_2(image, bboxes, labels)\r\n    return (\r\n        tf.image.convert_image_dtype(aug_data[\"image\"], tf.float32), \r\n        tf.convert_to_tensor(aug_data[\"bboxes\"], dtype=tf.float32), \r\n        tf.cast(aug_data[\"label\"], tf.float32)\r\n    )\r\n\r\ndef preprocess_data_2(inputs):\r\n    bboxes = inputs['bboxes']['bbox']\r\n    labels = inputs['bboxes']['label']\r\n    aug_image, aug_bboxes, aug_labels = tf.numpy_function(\r\n        func=aug_fn_2, \r\n        inp=[inputs[\"image\"], bboxes, labels], \r\n        Tout=[tf.float32, tf.float32, tf.float32],\r\n    )\r\n    \r\n    bounding_boxes = {\r\n        \"classes\": aug_labels,\r\n        \"boxes\":  aug_bboxes\r\n    }\r\n\r\n    return aug_image, bounding_boxes\r\n\r\nds_2 = dataset.map(preprocess_data_2).ragged_batch(2)\r\nfor item in ds_2:\r\n    break\r\n\r\n# Scenario 3\r\ndef transform_3(image, bboxes, labels):\r\n    transforms = A.Compose(\r\n        [\r\n            A.Rotate(limit=40),\r\n        ],\r\n        bbox_params=A.BboxParams(\r\n            format='albumentations',\r\n            label_fields=['label'],\r\n        )\r\n    )\r\n\r\n    transformed = transforms(\r\n        image=image.numpy(), \r\n        label=labels.numpy(),\r\n        bboxes=bboxes.numpy(),\r\n    )\r\n\r\n    return transformed\r\n\r\ndef aug_fn_3(image, bboxes, labels):\r\n    aug_data = transform_3(image, bboxes, labels)\r\n\r\n    return (\r\n        tf.image.convert_image_dtype(aug_data[\"image\"], tf.float32), \r\n        tf.RaggedTensor.from_tensor(tf.convert_to_tensor(aug_data[\"bboxes\"], dtype=tf.float32)),\r\n        tf.RaggedTensor.from_tensor(tf.cast([aug_data[\"label\"]], tf.float32)),\r\n    )\r\n\r\ndef preprocess_data_3(inputs):\r\n    bboxes = inputs['bboxes']['bbox']\r\n    labels = inputs['bboxes']['label']\r\n    aug_image, aug_bboxes, aug_labels = tf.py_function(\r\n        func=aug_fn_3, \r\n        inp=[inputs[\"image\"], bboxes, labels], \r\n        Tout=[\r\n            tf.TensorSpec(shape=[None, 512, 512, 3], dtype=tf.float32), \r\n            tf.RaggedTensorSpec(shape=[None, 4], dtype=tf.float32), \r\n            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.float32),\r\n        ],\r\n    )\r\n    \r\n    bounding_boxes = {\r\n        \"classes\": aug_labels,\r\n        \"boxes\":  aug_bboxes,\r\n    }\r\n\r\n    return aug_image, bounding_boxes\r\n\r\nds_3 = dataset.map(preprocess_data_3).ragged_batch(2)\r\nfor batch in ds_3:\r\n    break\r\n\r\nmodel = keras_cv.models.RetinaNet.from_preset(\r\n    \"resnet50_imagenet\",\r\n    num_classes=2,\r\n    bounding_box_format=\"rel_xyxy\",\r\n)\r\n\r\nmodel.compile(\r\n    classification_loss=\"focal\",\r\n    box_loss=\"smoothl1\",\r\n    optimizer=tf.optimizers.Adam(),\r\n)\r\n\r\nmodel.fit(\r\n    ds_3.take(1),\r\n    validation_data=ds_3.take(1),\r\n    epochs=100,\r\n)\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.12"],"created_at":"2023-05-25T18:52:03Z","comments":1,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60710"},{"issue_number":379,"repository":"tensorflow\/tensorflow","title":"Incorrect gradient after divide operation when result contains inf","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\ntf-nightly\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nUbuntu 20.04.5 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nThe gradient of `tf.experimental.numpy.divide` or simply `\/` operator is incorrect when the division result contains `inf`. See example below, the gradient of `result[0]` with respect to `x1` should be `1\/2` instead of `nan`.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\nx1 = tf.constant([3], dtype=tf.float32)\r\nx2 = tf.constant([2, 3, 0, 1], dtype=tf.float32)\r\nwith tf.GradientTape() as tape:\r\n  tape.watch(x1)\r\n  tape.watch(x2)\r\n  result = tf.experimental.numpy.divide(x1, x2)    # or result = x1 \/ x2\r\nactual_grad = tape.jacobian(result, x1)\r\nexpected_grad = tf.constant([[1\/2], [1\/3], [np.inf], [1\/1]])\r\nprint(actual_grad)\r\nprint(expected_grad)\r\nactual_grad == expected_grad\r\n\r\n\r\nSee gist: https:\/\/colab.research.google.com\/drive\/1xbzvQ99nrEehhBabpgCSEiBMSh4qXojR?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\ntf.Tensor(\r\n[[nan]\r\n [nan]\r\n [inf]\r\n [nan]], shape=(4, 1), dtype=float32)\r\ntf.Tensor(\r\n[[0.5       ]\r\n [0.33333334]\r\n [       inf]\r\n [1.        ]], shape=(4, 1), dtype=float32)\r\n<tf.Tensor: shape=(4, 1), dtype=bool, numpy=\r\narray([[False],\r\n       [False],\r\n       [ True],\r\n       [False]])>\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.13"],"created_at":"2023-05-24T17:14:14Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60695"},{"issue_number":380,"repository":"tensorflow\/tensorflow","title":"typing_extensions >= 4.6.0 causes pip unit test failure","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.10\n\n### Bazel version\n\n5.3.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\n\/\/bazel_pip\/tensorflow\/python\/trackable:data_structures_test will fail with typing_extensions >= 4.6.0 installed when run as a pip test against an installed TensorFlow wheel.\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel test --build_tests_only --cache_test_results=no --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --jobs=75 --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true \/\/bazel_pip\/tensorflow\/python\/trackable:data_structures_test\n```\n\n\n### Relevant log output\n\n```shell\n======================================================================\r\nERROR: testFunctionCaching (__main__.MappingTests)\r\nMappingTests.testFunctionCaching\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/eab0d61a99b6696edb3d2aff87b585e8\/execroot\/org_tensorflow\/bazel-out\/aarch64-opt\/bin\/bazel_pip\/tensorflow\/python\/trackable\/data_structures_test.runfiles\/org_tensorflow\/bazel_pip\/tensorflow\/python\/trackable\/data_structures_test.py\", line 507, in testFunctionCaching\r\n    second_trace = f.get_concrete_function(\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py\", line 1198, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py\", line 1189, in _get_concrete_function_garbage_collected\r\n    concrete = self._variable_creation_fn.get_concrete_function(\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py\", line 197, in get_concrete_function\r\n    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py\", line 172, in _maybe_define_concrete_function\r\n    return self._maybe_define_function(args, kwargs)\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py\", line 294, in _maybe_define_function\r\n    function_type_utils.make_canonicalized_monomorphic_type(\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/function_type_utils.py\", line 378, in make_canonicalized_monomorphic_type\r\n    function_type_lib.canonicalize_to_monomorphic(\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/core\/function\/polymorphism\/function_type.py\", line 481, in canonicalize_to_monomorphic\r\n    _make_validated_mono_param(name, arg, poly_parameter.kind,\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/core\/function\/polymorphism\/function_type.py\", line 421, in _make_validated_mono_param\r\n    mono_type = trace_type.from_value(value, type_context)\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/core\/function\/trace_type\/trace_type_builder.py\", line 142, in from_value\r\n    elif isinstance(value, trace.SupportsTracingProtocol):\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/typing_extensions.py\", line 605, in __instancecheck__\r\n    val = inspect.getattr_static(instance, attr)\r\n  File \"\/usr\/lib\/python3.8\/inspect.py\", line 1596, in getattr_static\r\n    instance_result = _check_instance(obj, attr)\r\n  File \"\/usr\/lib\/python3.8\/inspect.py\", line 1543, in _check_instance\r\n    instance_dict = object.__getattribute__(obj, \"__dict__\")\r\nTypeError: this __dict__ descriptor does not support '_DictWrapper' objects\r\n\r\n======================================================================\r\nERROR: testFunctionCaching (__main__.TupleTests)\r\nTupleTests.testFunctionCaching\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/root\/.cache\/bazel\/_bazel_root\/eab0d61a99b6696edb3d2aff87b585e8\/execroot\/org_tensorflow\/bazel-out\/aarch64-opt\/bin\/bazel_pip\/tensorflow\/python\/trackable\/data_structures_test.runfiles\/org_tensorflow\/bazel_pip\/tensorflow\/python\/trackable\/data_structures_test.py\", line 716, in testFunctionCaching\r\n    second_trace = f.get_concrete_function(\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py\", line 1198, in get_concrete_function\r\n    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/polymorphic_function.py\", line 1189, in _get_concrete_function_garbage_collected\r\n    concrete = self._variable_creation_fn.get_concrete_function(\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py\", line 197, in get_concrete_function\r\n    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py\", line 172, in _maybe_define_concrete_function\r\n    return self._maybe_define_function(args, kwargs)\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/tracing_compiler.py\", line 294, in _maybe_define_function\r\n    function_type_utils.make_canonicalized_monomorphic_type(\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/polymorphic_function\/function_type_utils.py\", line 378, in make_canonicalized_monomorphic_type\r\n    function_type_lib.canonicalize_to_monomorphic(\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/core\/function\/polymorphism\/function_type.py\", line 481, in canonicalize_to_monomorphic\r\n    _make_validated_mono_param(name, arg, poly_parameter.kind,\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/core\/function\/polymorphism\/function_type.py\", line 421, in _make_validated_mono_param\r\n    mono_type = trace_type.from_value(value, type_context)\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/tensorflow\/core\/function\/trace_type\/trace_type_builder.py\", line 142, in from_value\r\n    elif isinstance(value, trace.SupportsTracingProtocol):\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.8\/site-packages\/typing_extensions.py\", line 605, in __instancecheck__\r\n    val = inspect.getattr_static(instance, attr)\r\n  File \"\/usr\/lib\/python3.8\/inspect.py\", line 1596, in getattr_static\r\n    instance_result = _check_instance(obj, attr)\r\n  File \"\/usr\/lib\/python3.8\/inspect.py\", line 1543, in _check_instance\r\n    instance_dict = object.__getattribute__(obj, \"__dict__\")\r\nTypeError: this __dict__ descriptor does not support '_TupleWrapper' objects\r\n\r\n----------------------------------------------------------------------\r\nRan 74 tests in 1.021s\r\n\r\nFAILED (errors=2, skipped=4)\r\n================================================================================\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:support"],"created_at":"2023-05-24T09:25:30Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60687"},{"issue_number":381,"repository":"tensorflow\/tensorflow","title":"api_compatibility_test fails on Python 3.11","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n5.3.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\n\/\/tensorflow\/tools\/api\/tests:api_compatibility_test fails on Python 3.11\r\nSee https:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5053005537\/jobs\/9066419128#step:6:5566\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel test --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --repo_env=PYTHON_BIN_PATH=\/home\/ubuntu\/actions-runner\/_work\/tensorflow\/tensorflow\/bazel-ci_build-cache\/.venv\/tf\/bin\/python --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py38,-no_oss_py39,-no_oss_py310 --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py38,-no_oss_py39,-no_oss_py310 --local_test_jobs=64 --build_tests_only -- \/\/tensorflow\/... -\/\/tensorflow\/compiler\/tf2tensorrt\/... -\/\/tensorflow\/compiler\/xrt\/... -\/\/tensorflow\/core\/tpu\/... -\/\/tensorflow\/go\/... -\/\/tensorflow\/java\/... -\/\/tensorflow\/python\/integration_testing\/... -\/\/tensorflow\/tools\/toolchains\/... -\/\/tensorflow\/lite\/... -\/\/tensorflow\/python\/kernel_tests\/nn_ops:atrous_conv2d_test -\/\/tensorflow\/python\/kernel_tests\/nn_ops:conv_ops_test -\/\/tensorflow\/compiler\/mlir\/tfr\/examples\/mnist:mnist_ops_test -\/\/tensorflow\/core\/grappler\/optimizers:auto_mixed_precision_test_cpu -\/\/tensorflow\/core\/grappler\/optimizers:remapper_test_cpu\n```\n\n\n### Relevant log output\n\n```shell\nRunning tests under Python 3.11.3: \/home\/ubuntu\/actions-runner\/_work\/tensorflow\/tensorflow\/bazel-ci_build-cache\/.venv\/tf\/bin\/python\r\n[ RUN      ] ApiCompatibilityTest.testAPIBackwardsCompatibility\r\nERROR:tensorflow:TensorFlow API backwards compatibility test\r\nThis test ensures all changes to the public API of TensorFlow are intended.\r\n\r\nIf this test fails, it means a change has been made to the public API. Backwards\r\nincompatible changes are not allowed. You can run the test as follows to update\r\ntest goldens and package them with your change.\r\n\r\n    $ bazel run tensorflow\/tools\/api\/tests:api_compatibility_test \\\r\n    #     -- --update_goldens True\r\n\r\nYou will need an API approval to make changes to the public TensorFlow API. This\r\nincludes additions to the API.\r\n\r\nE0523 05:00:41.556244 281473269493776 api_compatibility_test.py:370] TensorFlow API backwards compatibility test\r\nThis test ensures all changes to the public API of TensorFlow are intended.\r\n\r\nIf this test fails, it means a change has been made to the public API. Backwards\r\nincompatible changes are not allowed. You can run the test as follows to update\r\ntest goldens and package them with your change.\r\n\r\n    $ bazel run tensorflow\/tools\/api\/tests:api_compatibility_test \\\r\n    #     -- --update_goldens True\r\n\r\nYou will need an API approval to make changes to the public TensorFlow API. This\r\nincludes additions to the API.\r\n\r\nERROR:tensorflow:1 differences found between API and golden.\r\nE0523 05:00:41.556445 281473269493776 api_compatibility_test.py:371] 1 differences found between API and golden.\r\nERROR:tensorflow:    Change detected in python object: tensorflow.train.\r\nE0523 05:00:41.556506 281473269493776 api_compatibility_test.py:392]     Change detected in python object: tensorflow.train.\r\nERROR:tensorflow:    \r\n  path: \"tensorflow.train\"\r\n  tf_module {\r\n    member {\r\n      name: \"BytesList\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"Checkpoint\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"CheckpointManager\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"CheckpointOptions\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"CheckpointView\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"ClusterDef\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"ClusterSpec\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"Coordinator\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"Example\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"ExponentialMovingAverage\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"Feature\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"FeatureList\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"FeatureLists\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"Features\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"FloatList\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"Int64List\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"JobDef\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"SequenceExample\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"ServerDef\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"TrackableView\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"experimental\"\r\n      mtype: \"<type \\'module\\'>\"\r\n    }\r\n    member_method {\r\n      name: \"checkpoints_iterator\"\r\n      argspec: \"args=[\\'checkpoint_dir\\', \\'min_interval_secs\\', \\'timeout\\', \\'timeout_fn\\'], varargs=None, keywords=None, defaults=[\\'0\\', \\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"get_checkpoint_state\"\r\n      argspec: \"args=[\\'checkpoint_dir\\', \\'latest_filename\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"latest_checkpoint\"\r\n      argspec: \"args=[\\'checkpoint_dir\\', \\'latest_filename\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"list_variables\"\r\n      argspec: \"args=[\\'ckpt_dir_or_file\\'], varargs=None, keywords=None, defaults=None\"\r\n    }\r\n    member_method {\r\n      name: \"load_checkpoint\"\r\n      argspec: \"args=[\\'ckpt_dir_or_file\\'], varargs=None, keywords=None, defaults=None\"\r\n    }\r\n    member_method {\r\n      name: \"load_variable\"\r\n      argspec: \"args=[\\'ckpt_dir_or_file\\', \\'name\\'], varargs=None, keywords=None, defaults=None\"\r\n    }\r\n  }\r\n\r\nE0523 05:00:41.556555 281473269493776 api_compatibility_test.py:393]     \r\n  path: \"tensorflow.train\"\r\n  tf_module {\r\n    member {\r\n      name: \"BytesList\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"Checkpoint\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"CheckpointManager\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"CheckpointOptions\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"CheckpointView\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"ClusterDef\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"ClusterSpec\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"Coordinator\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"Example\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"ExponentialMovingAverage\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"Feature\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"FeatureList\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"FeatureLists\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"Features\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"FloatList\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"Int64List\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"JobDef\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"SequenceExample\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"ServerDef\"\r\n-     mtype: \"<class \\'google.protobuf.internal.python_message.GeneratedProtocolMessageType\\'>\"\r\n?                                      ---------   ^^^\r\n+     mtype: \"<class \\'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\\'>\"\r\n?                                        ++ ^^^^\r\n    }\r\n    member {\r\n      name: \"TrackableView\"\r\n      mtype: \"<type \\'type\\'>\"\r\n    }\r\n    member {\r\n      name: \"experimental\"\r\n      mtype: \"<type \\'module\\'>\"\r\n    }\r\n    member_method {\r\n      name: \"checkpoints_iterator\"\r\n      argspec: \"args=[\\'checkpoint_dir\\', \\'min_interval_secs\\', \\'timeout\\', \\'timeout_fn\\'], varargs=None, keywords=None, defaults=[\\'0\\', \\'None\\', \\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"get_checkpoint_state\"\r\n      argspec: \"args=[\\'checkpoint_dir\\', \\'latest_filename\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"latest_checkpoint\"\r\n      argspec: \"args=[\\'checkpoint_dir\\', \\'latest_filename\\'], varargs=None, keywords=None, defaults=[\\'None\\'], \"\r\n    }\r\n    member_method {\r\n      name: \"list_variables\"\r\n      argspec: \"args=[\\'ckpt_dir_or_file\\'], varargs=None, keywords=None, defaults=None\"\r\n    }\r\n    member_method {\r\n      name: \"load_checkpoint\"\r\n      argspec: \"args=[\\'ckpt_dir_or_file\\'], varargs=None, keywords=None, defaults=None\"\r\n    }\r\n    member_method {\r\n      name: \"load_variable\"\r\n      argspec: \"args=[\\'ckpt_dir_or_file\\', \\'name\\'], varargs=None, keywords=None, defaults=None\"\r\n    }\r\n  }\r\n\r\n[  FAILED  ] ApiCompatibilityTest.testAPIBackwardsCompatibility\r\nINFO:tensorflow:time(__main__.ApiCompatibilityTest.testAPIBackwardsCompatibility): 2.5s\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","comp:mkl","subtype: ubuntu\/linux"],"created_at":"2023-05-23T16:40:28Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60679"},{"issue_number":382,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/ops\/ragged:ragged_cross_op_test is flaky","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.8.13\n\n### Bazel version\n\n5.3.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\nTest sometimes fails with segfault\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel test --build_tests_only --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --local_test_jobs=64 --test_tag_filters=-nopip,-no_pip,-oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 -k -- \/\/bazel_pip\/tensorflow\/... -\/\/bazel_pip\/tensorflow\/compiler\/tf2tensorrt\/... -\/\/bazel_pip\/tensorflow\/compiler\/xrt\/... -\/\/bazel_pip\/tensorflow\/core\/tpu\/... -\/\/bazel_pip\/tensorflow\/go\/... -\/\/bazel_pip\/tensorflow\/java\/... -\/\/bazel_pip\/tensorflow\/python\/integration_testing\/... -\/\/bazel_pip\/tensorflow\/tools\/toolchains\/... -\/\/bazel_pip\/tensorflow\/lite\/... -\/\/bazel_pip\/tensorflow\/python\/kernel_tests\/nn_ops:atrous_conv2d_test -\/\/bazel_pip\/tensorflow\/python\/kernel_tests\/nn_ops:conv_ops_test\n```\n\n\n### Relevant log output\n\n```shell\n[ RUN      ] RaggedCrossOpTest.testRaggedCrossInvalidValue\r\nINFO:tensorflow:Running testRaggedCrossInvalidValue in GRAPH mode.\r\nI0522 15:40:37.724678 281472914997264 test_util.py:1494] Running testRaggedCrossInvalidValue in GRAPH mode.\r\nFatal Python error: Segmentation fault\r\n\r\nThread 0x0000ffff851cc010 (most recent call first):\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/client\/session.py\", line 1477 in _call_tf_sessionrun\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/client\/session.py\", line 1384 in _run_fn\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/client\/session.py\", line 1401 in _do_call\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/client\/session.py\", line 1394 in _do_run\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/client\/session.py\", line 1214 in _run\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/client\/session.py\", line 971 in run\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/framework\/test_util.py\", line 2061 in run\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/framework\/test_util.py\", line 2693 in evaluate\r\n  File \"\/tmpfs\/bazel_output\/_bazel_ubuntu\/eab0d61a99b6696edb3d2aff87b585e8\/execroot\/org_tensorflow\/bazel-out\/aarch64-opt\/bin\/bazel_pip\/tensorflow\/python\/ops\/ragged\/ragged_cross_op_test.runfiles\/org_tensorflow\/bazel_pip\/tensorflow\/python\/ops\/ragged\/ragged_cross_op_test.py\", line 478 in testRaggedCrossInvalidValue\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/framework\/test_util.py\", line 1498 in decorated\r\n  File \"\/usr\/lib\/python3.10\/unittest\/case.py\", line 549 in _callTestMethod\r\n  File \"\/usr\/lib\/python3.10\/unittest\/case.py\", line 591 in run\r\n  File \"\/usr\/lib\/python3.10\/unittest\/case.py\", line 650 in __call__\r\n  File \"\/usr\/lib\/python3.10\/unittest\/suite.py\", line 122 in run\r\n  File \"\/usr\/lib\/python3.10\/unittest\/suite.py\", line 84 in __call__\r\n  File \"\/usr\/lib\/python3.10\/unittest\/suite.py\", line 122 in run\r\n  File \"\/usr\/lib\/python3.10\/unittest\/suite.py\", line 84 in __call__\r\n  File \"\/usr\/lib\/python3.10\/unittest\/runner.py\", line 184 in run\r\n  File \"\/usr\/lib\/python3.10\/unittest\/main.py\", line 271 in runTests\r\n  File \"\/usr\/lib\/python3.10\/unittest\/main.py\", line 101 in __init__\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/absl\/testing\/absltest.py\", line 2527 in _run_and_get_tests_result\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/absl\/testing\/absltest.py\", line 2561 in run_tests\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/absl\/testing\/absltest.py\", line 2155 in _run_in_app\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/absl\/testing\/absltest.py\", line 2060 in main\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/platform\/googletest.py\", line 51 in g_main\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/absl\/app.py\", line 254 in _run_main\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/absl\/app.py\", line 308 in run\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/platform\/googletest.py\", line 60 in main_wrapper\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/platform\/benchmark.py\", line 489 in benchmarks_main\r\n  File \"\/workspace\/pip_test\/venv_clean\/lib\/python3.10\/site-packages\/tensorflow\/python\/platform\/googletest.py\", line 62 in main\r\n  File \"\/tmpfs\/bazel_output\/_bazel_ubuntu\/eab0d61a99b6696edb3d2aff87b585e8\/execroot\/org_tensorflow\/bazel-out\/aarch64-opt\/bin\/bazel_pip\/tensorflow\/python\/ops\/ragged\/ragged_cross_op_test.runfiles\/org_tensorflow\/bazel_pip\/tensorflow\/python\/ops\/ragged\/ragged_cross_op_test.py\", line 497 in <module>\r\n\r\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, tensorflow.python.framework.fast_tensor_util, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label (total: 72)\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","comp:mkl","subtype: ubuntu\/linux"],"created_at":"2023-05-23T09:42:28Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60670"},{"issue_number":383,"repository":"tensorflow\/tensorflow","title":"ROCm: Importing PyTorch before TensorFlow causes TensorFlow to fail completely","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\nv2.11.1-3812-gef4eebff7d4 2.11.1\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nArch Linux (EndeavourOS)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\nROCm 5.4.3\r\n\r\n### GPU model and memory\r\n\r\nRadeon VII\/16 GiB\r\n\r\n### Current Behaviour?\r\n\r\nThe title suffices. A workaround is to ensure `tensorflow` is imported before `torch`.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\npython -c \"import torch; import tensorflow as tf; tf.zeros(1)\"\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2023-05-19 22:22:33.756830: I tensorflow\/core\/platform\/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nv2.11.1-3812-gef4eebff7d4 2.11.1\r\n(sd) [habbasi@hameer-imacpro11 kohya-trainer]$ python -c \"import tensorflow as tf; import torch; tf.zeros(1)\"\r\n2023-05-19 22:25:50.708366: I tensorflow\/core\/platform\/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-05-19 22:25:53.172076: I tensorflow\/compiler\/xla\/stream_executor\/rocm\/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2023-05-19 22:25:53.172177: I tensorflow\/compiler\/xla\/stream_executor\/rocm\/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2023-05-19 22:25:53.172216: I tensorflow\/compiler\/xla\/stream_executor\/rocm\/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2023-05-19 22:25:53.172542: I tensorflow\/core\/platform\/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-05-19 22:25:53.173978: I tensorflow\/compiler\/xla\/stream_executor\/rocm\/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2023-05-19 22:25:53.174095: I tensorflow\/compiler\/xla\/stream_executor\/rocm\/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2023-05-19 22:25:53.174137: I tensorflow\/compiler\/xla\/stream_executor\/rocm\/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/habbasi\/mambaforge\/envs\/sd\/lib\/python3.10\/site-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/home\/habbasi\/mambaforge\/envs\/sd\/lib\/python3.10\/site-packages\/tensorflow\/python\/eager\/context.py\", line 588, in ensure_initialized\r\n    context_handle = pywrap_tfe.TFE_NewContext(opts)\r\ntensorflow.python.framework.errors_impl.InternalError: hipGetDevice() failed. Status: invalid device ordinal\r\n```\r\n<\/details>\r\n\r\nxref pytorch\/pytorch#101900","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","TF 2.11"],"created_at":"2023-05-19T20:26:46Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60642"},{"issue_number":384,"repository":"tensorflow\/tensorflow","title":"control_flow_ops_test unit test is flaky","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n5.3.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\n\/\/tensorflow\/python\/ops\/parallel_for:control_flow_ops_test fails occasionally due to difference exceeding tolerance.\r\n\r\nSee https:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/5012758324\/jobs\/8985082872#step:5:29789\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel test --build_tests_only --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --local_test_jobs=64 --test_tag_filters=-nopip,-no_pip,-oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 -k -- \/\/bazel_pip\/tensorflow\/... -\/\/bazel_pip\/tensorflow\/compiler\/tf2tensorrt\/... -\/\/bazel_pip\/tensorflow\/compiler\/xrt\/... -\/\/bazel_pip\/tensorflow\/core\/tpu\/... -\/\/bazel_pip\/tensorflow\/go\/... -\/\/bazel_pip\/tensorflow\/java\/... -\/\/bazel_pip\/tensorflow\/python\/integration_testing\/... -\/\/bazel_pip\/tensorflow\/tools\/toolchains\/... -\/\/bazel_pip\/tensorflow\/lite\/... -\/\/bazel_pip\/tensorflow\/python\/kernel_tests\/nn_ops:atrous_conv2d_test -\/\/bazel_pip\/tensorflow\/python\/kernel_tests\/nn_ops:conv_ops_test\n```\n\n\n### Relevant log output\n\n```shell\nAssertionError: \r\nNot equal to tolerance rtol=0.0001, atol=1e-05\r\nMismatched value: a is different from b. \r\nnot close where = (array([0]), array([0]), array([0]), array([1]), array([4]), array([0]))\r\nnot close lhs = [0.]\r\nnot close rhs = [0.77603436]\r\nnot close dif = [0.77603436]\r\nnot close tol = [8.760343e-05]\r\ndtype = float32, shape = (3, 3, 2, 12, 12, 3)\r\nMismatched elements: 1 \/ 7776 (0.0129%)\r\nMax absolute difference: 0.77603436\r\nMax relative difference: 1.\r\n x: array([[[[[[0.      , 0.      , 0.712515],\r\n           [0.      , 0.889897, 0.      ],\r\n           [0.      , 0.      , 0.      ],...\r\n y: array([[[[[[0.      , 0.      , 0.712515],\r\n           [0.      , 0.889897, 0.      ],\r\n           [0.      , 0.      , 0.      ],...\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","comp:ops","subtype: ubuntu\/linux"],"created_at":"2023-05-18T15:22:49Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60629"},{"issue_number":385,"repository":"tensorflow\/tensorflow","title":"Numpy and tf experimental Numpy differ in vander matrix creation case for N=0","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.11.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nUbuntu 22.04 jammy\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n11.6\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nThe behaviour of `tf.experimental.numpy.vander` is different than `np.vander` for `N=0` where both value and shape of the output differ.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np        # 1.23.5\r\nimport tensorflow as tf   # 2.11.0\r\n\r\nxn = np.array([1], dtype=np.int32)\r\nx = tf.constant([1], dtype=tf.int32)\r\nprint(np.vander(xn, 0))\r\nprint()\r\nprint(tf.experimental.numpy.vander(x, 0))\n```\n\n\n### Relevant log output\n\n```shell\n[]\r\n\r\ntf.Tensor([[1]], shape=(1, 1), dtype=int32)\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.11"],"created_at":"2023-05-18T15:11:22Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60628"},{"issue_number":386,"repository":"tensorflow\/tensorflow","title":"Weird memory usage of shuffling in `tf.data.Dataset` ","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.12\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nUbuntu 22.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.8\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nHi, I've read the [tf.data doc](https:\/\/www.tensorflow.org\/guide\/data#randomly_shuffling_input_data), which says using a large `buffer size` in data shuffling is not recommended, but still the shuffling behavior costs way more memory than I would expect. For example, I expect the full shuffling of 50,000,000 `int` data may only use 1 GB of memory, but the following code after `print` essentially uses 10 GB.\r\n\r\nThis leads to some practical concerns. If I set `buffer_size=1024` in data shuffling, would the *actual* memory usage of the buffer size be 10 times that of 1024 elements?\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndata_size = 50000000\r\ntf_dataset = tf.data.Dataset.from_tensor_slices(np.arange(data_size))\r\ntf_dataset = iter(tf_dataset.shuffle(data_size))\r\nprint(next(tf_dataset))\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n<tf.Tensor: shape=(), dtype=int64, numpy=24774043>\r\n```\r\n<\/details>","labels":["type:bug","comp:data","type:performance","TF 2.12"],"created_at":"2023-05-15T14:54:49Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60599"},{"issue_number":387,"repository":"tensorflow\/tensorflow","title":"rejection_resample loses track of ragged tensors","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.14.0-dev20230512\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04.6\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nA `tf.data.Dataset` initialized from RaggedTensors normally will successfully batch into ragged batches. However after passing it through `rejection_resample`, it loses track of which input tensors were ragged, and so batching fails.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput = tf.ragged.constant([[1,2,3], [4,5], [7,8,9]])\r\nds = tf.data.Dataset.from_tensor_slices(input)\r\ntf.random.set_seed(0)\r\n# Removing this line makes everything work fine\r\nds = ds.rejection_resample(\r\n    class_func=lambda t: 1,\r\n    target_dist=(0.1, 0.9),\r\n)\r\nds = ds.batch(2)\r\nds.take(1).get_single_element()\n```\n\n\n### Relevant log output\n\n```shell\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DatasetToSingleElement_output_types_2_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Cannot batch tensors with different shapes in component 1. First element had shape [3] and element 1 had shape [2]. [Op:DatasetToSingleElement] name:\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.12"],"created_at":"2023-05-12T10:16:39Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60583"},{"issue_number":388,"repository":"tensorflow\/tensorflow","title":"Large inconsistencies in tf.signal.stft's and tf.signal.inverse_stft's results with @tf.function decorator for certain inputs","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.14.0-dev20230509\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nN\/A\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n`tf.signal.stft` and `tf.signal.inverse_stft` has large inconsistencies in their results with or without @tf.function for some inputs. This issue seems to be unrelated to precision errors, as previously discussed under issues (#57960 and #57961), given that the inconsistencies can reach very high values, such as 7.530909102308483e+252+6.2143661415679e-310j. I open this issue because the behavior still exists in the latest nightly version of tensorfow.\r\n\r\nFurther investigation finds that it is because the results are different during each run and thus the inconsistencies are different, where sometimes the discrepancies are extremely large, while at other times they are relatively small. It appears that the inconsistencies are non-deterministic, which indicates a potential issue with the underlying implementation.\r\n\r\nI rerun the reproduction code several times and record the large inconsistencies below in the log file.\r\n\r\nThe reproduction colab links are here:\r\nFor tf.signal.stft, https:\/\/colab.research.google.com\/drive\/1WleKXby71iZXOL12r8nIN8B_jd2wJQks?usp=sharing.\r\nFor tf.signal.inverse_stft, https:\/\/colab.research.google.com\/drive\/1MhNfkZgltqQqHw8kKG2zQi8ivwvKfRoj?usp=sharing.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\n# for tf.signal.stft\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(tf.__version__)\r\n\r\ninput = {'fft_length': 46, 'frame_step': 19, 'frame_length': 0, 'signals': np.array([[[[-8.75314539e+307, -4.03838038e+307,  8.23775798e+307, -1.32627219e+307,  1.19815521e+307,  4.57117750e+307],\r\n                                                                              [-4.74761327e+307, -4.71580522e+307, -5.88832102e+307, -6.48759076e+307, -4.36028464e+307, -4.77775171e+307],\r\n                                                                              [ 1.20113701e+307, -7.60106094e+307,  7.22716917e+307, 2.17687950e+307, -5.25271143e+306,  5.41182394e+307]]]])}\r\n\r\noutput1 = tf.signal.stft(**input)\r\n\r\n@tf.function\r\ndef fun_wrapper(x):\r\n    return tf.signal.stft(**x)\r\n\r\noutput2 = fun_wrapper(input)\r\n\r\nprint(np.allclose(output1, output2))\r\nprint(np.max(np.subtract(output1, output2)))\r\n\r\n# for tf.signal.inverse_stft\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nprint(tf.__version__)\r\n\r\ninput = {'frame_step': 29343, 'frame_length': 61, 'stfts': np.array([[]], dtype=np.complex64)}\r\n\r\noutput1 = tf.signal.inverse_stft(**input)\r\n\r\n@tf.function\r\ndef fun_wrapper(x):\r\n    return tf.signal.inverse_stft(**x)\r\n\r\noutput2 = fun_wrapper(input)\r\n\r\nprint(np.allclose(output1, output2))\r\nprint(np.max(np.subtract(output1, output2)))\n```\n\n\n### Relevant log output\n\n```shell\n### for tf.signal.stft\r\nFalse\r\n(1.2623837153272947e+180+2.19373012209e-312j)\r\n\r\nFalse\r\n(6.443468248812391e+278-3.2e-322j)\r\n\r\nFalse\r\n(2.347922071768121e+228+1.74e-321j)\r\n\r\n### for tf.signal.inverse_stft\r\nFalse\r\n1.4412957e+32\r\n\r\nFalse\r\n7.529253e+23\r\n\r\nFalse\r\n7800730000.0\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","comp:autograph","TF 2.12"],"created_at":"2023-05-09T20:14:39Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60549"},{"issue_number":389,"repository":"tensorflow\/tensorflow","title":"tf.linalg.matrix_rank  results has different results with or without @tf.function for numpy inputs under tensorflow-cpu","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.14.0-dev20230509\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\nN\/A\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n`tf.linalg.matrix_rank` has different results with or without `@tf.function` when the input is a numpy tensor and **tensorFlow-cpu** is used. \r\n\r\nInterestingly, this issue does not occur when the numpy array is explicitly converted to a TensorFlow tensor before being passed as an argument to tf.linalg.matrix_rank. This explicit conversion shouldn't be necessary, as per the TensorFlow tutorial (https:\/\/www.tensorflow.org\/tutorials\/customization\/basics#:~:text=TensorFlow%20operations%20automatically%20convert%20NumPy%20ndarrays%20to%20Tensors), which states that \"TensorFlow operations automatically convert NumPy ndarrays to Tensors\". This discrepancy seems to indicate a bug that prevents the utilization of this automatic conversion feature.\r\n\r\nThis issue was previously raised and discussed under issue (#57959), where the proposed solution was the explicit conversion of numpy arrays to TensorFlow tensors. While this solution works, it does not align with the functionality of TensorFlow's automatic conversion of numpy arrays to tensors, and it requires users to perform an additional step that should not be necessary.\r\n\r\nIn essence, this bug seems to affect the user's ability to leverage TensorFlow's automatic conversion of numpy arrays to tensors, particularly when using TensorFlow-CPU.\r\n\r\nI open this issue because the same behavior still exists in the latest nightly version and I believe it should not be a user issue according to the tutorial.\r\n\r\nThe reproduction colab link is here: https:\/\/colab.research.google.com\/drive\/1wEYxe5b-m7_3pqBP1iTrjSvydMd_jD_B?usp=sharing. \r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nprint(tf.__version__)\r\n\r\ninput = {'name': 'matrix_rank', 'a': np.array([[-7.24721292e+307,  4.66389010e+307, -5.40181227e+307,\r\n         7.28793100e+307,  5.19885794e+307],\r\n       [-5.74381106e+307,  2.21923437e+307,  4.96898538e+307,\r\n         4.26402766e+307,  7.42174751e+307],\r\n       [-2.62810171e+307,  1.71425915e+307, -6.99349881e+307,\r\n        -8.11519519e+307,  4.04358640e+307],\r\n       [-8.52726304e+307,  1.44214314e+307, -4.53927548e+307,\r\n        -4.79571993e+307, -4.59672928e+307]])}\r\nprint(input['a'].dtype)\r\n\r\noutput1 = tf.linalg.matrix_rank(**input)\r\nprint(output1)\r\n\r\n@tf.function\r\ndef fun_wrapper(x):\r\n    return tf.linalg.matrix_rank(**x)\r\n\r\noutput2 = fun_wrapper(input)\r\nprint(output2)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2.14.0-dev20230509\r\nfloat64\r\ntf.Tensor(0, shape=(), dtype=int32)\r\ntf.Tensor(4, shape=(), dtype=int32)\r\n```\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:apis","comp:tf.function"],"created_at":"2023-05-09T19:42:32Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60547"},{"issue_number":390,"repository":"tensorflow\/tensorflow","title":"XLA size-inference integration bug (tf.where, tf.TensorArray, slice, loop)","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\n2.14.0.dev20230502\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nGoogle Colab\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nThe attached code produces difference results for `jit_compile=True` and `jit_compile=False`, the XLA version produces the wrong results. The bug reproduces on both TF 2.12 and nightly (2.14.0.dev20230502) on Google Colab.\r\n\r\nIt appears to be an issue with `tf.size(tf.where(x == 1))`. While the visible front-end output is correct, I suspect there is an integration bug on the compiler back-end. When combined with a slice that is too big x[0:16] (`x.shape = (3, 5)`), plus many more code details, the output has same size as the input (`5` in the example). As if `tf.size(tf.where(x == 1)) == tf.size(x) == 5`, when actually `tf.size(tf.where(x == 1)) == 2`. I suspect this because changing the input shape to `(4, )` changes the output shape to `(4, )`.\r\n\r\nThis bug was very subtle and difficult to find, especially because `tf.print()` is not supported by XLA. I hope you appreciate it.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nGoogle Colab link: https:\/\/colab.research.google.com\/drive\/1cccEssWkCVLO515uf3RoOuEMDp5V-2ZZ?usp=sharing\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef fn(x, y):\r\n    token_idx_to_mask = tf.where(x == 1)\r\n    n_samples = tf.size(token_idx_to_mask)\r\n    x_repeated = tf.repeat(tf.expand_dims(x, 0), n_samples, axis=0)\r\n\r\n    predict_all_array = tf.TensorArray(x_repeated.dtype, size=1, infer_shape=False, element_shape=(None, ))\r\n    for batch_i in tf.range(1):\r\n        x_batch = x_repeated[0:(batch_i + 1)*16, ...]\r\n        y_batch = x_batch[:, y]\r\n        predict_all_array = predict_all_array.write(batch_i, y_batch)\r\n    predicts_all = predict_all_array.concat()\r\n\r\n    return n_samples, predicts_all\r\n\r\nfn_jit = tf.function(reduce_retracing=True, jit_compile=True)(fn)\r\nfn_std = tf.function(reduce_retracing=True)(fn)\r\n\r\nprint('XLA': fn_jit(tf.constant([0, 1, 1, 0, 0], dtype=tf.dtypes.int32), tf.constant(0)))\r\nprint('STD': fn_std(tf.constant([0, 1, 1, 0, 0], dtype=tf.dtypes.int32), tf.constant(0)))\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nXLA: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 0, 0, 0], dtype=int32)>)\r\nSTD: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)\r\n```\r\n\r\nExpected results would be:\r\n\r\n```shell\r\nXLA: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)\r\nSTD: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)\r\n```\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.12"],"created_at":"2023-05-02T16:34:56Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60472"},{"issue_number":391,"repository":"tensorflow\/tensorflow","title":"Graph\/memory corruption involving custom_gradient of functions involving pow","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.9.1, 2.11, presumably more\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nLinux Ubuntu 18.04, 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nI have implemented `custom_gradient`s for `tf.function`s that smooth various Nth roots to avoid the singularity at `x=0`. I expect the value computed by the custom gradient to be used whenever that function is differentiated, and not substituted for any other value.\r\n\r\n**HOWEVER** when the following conditions hold:\r\n- input is a complex dtype\r\n- the `GradientTape` is persistent\r\n- the `tf.function` being differentiated invokes `tf.pow` directly (or indirectly through `**` operator)\r\n- the argument is `0`\r\n\r\nThen the custom gradient is invoked and the proper value is computed, but then discarded somewhere between returning from my custom gradient and returning from the gradient tape's internal computation.\r\n\r\nNote: this problem does not appear when wrapping `sqrt()` as that has its own gradient implementation that presumably avoids `pow()`.\r\n\r\nThis seems likely related to numerical issues highlighted in tensorflow\/tfjs#346, PLUS, some kind of graph\/memory corruption, although why\/how that would be conditional on the argument is beyond me.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n\r\n@tf.function\r\n@tf.custom_gradient\r\ndef cbrt(p):\r\n  third = 1. \/ 3.\r\n  real_part_nonnegative = tf.math.real(p) >= 0\r\n  signs = tf.cast(tf.where(real_part_nonnegative, 1.0, -1.0), p.dtype)\r\n  p = signs * p\r\n  unsigned_root = p ** third\r\n\r\n  @tf.function\r\n  def grad(upstream):\r\n    unsigned_root_plus_eps = unsigned_root + 1e-4\r\n    denom = 3 * unsigned_root_plus_eps * unsigned_root_plus_eps + 1e-4\r\n    droot_dp = (1. \/ denom)\r\n    grad_root = droot_dp * upstream\r\n    tf.print(\"custom gradient:\", tf.math.real(grad_root), tf.math.imag(grad_root))\r\n    return grad_root\r\n\r\n  root = signs * unsigned_root\r\n  return root, grad\r\n  \r\n\r\nif __name__ == \"__main__\":\r\n  tf.config.run_functions_eagerly(True)\r\n  x = tf.Variable([-1e-2, -1e-10, 0, 1e-10, 1e-2], dtype=tf.complex64)\r\n  with tf.GradientTape(persistent=True) as tape:\r\n    cbrt_x = cbrt(x)\r\n  gradients = tape.gradient(cbrt_x, x)\r\n  tf.print(\"all good:\", tf.math.real(gradients), tf.math.imag(gradients))\r\n\r\n  tf.config.run_functions_eagerly(False)\r\n  with tf.GradientTape() as tape:\r\n    cbrt_x = cbrt(x)\r\n  gradients = tape.gradient(cbrt_x, x)\r\n  tf.print(\"all good:\", tf.math.real(gradients), tf.math.imag(gradients))\r\n\r\n  @tf.function\r\n  def now_again_in_graph_mode(y):\r\n    with tf.GradientTape(persistent=True) as tape:\r\n      cbrt_y = cbrt(y)\r\n    return tape.gradient(cbrt_y, y)\r\n  gradients = now_again_in_graph_mode(x)\r\n  tf.print(\"chaos reigns:\", tf.math.real(gradients), tf.math.imag(gradients))\n```\n\n\n### Relevant log output\n\n```shell\ncustom gradient: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]\r\nall good: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]\r\ncustom gradient: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]\r\nall good: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]\r\ncustom gradient: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]\r\nchaos reigns: [7.16964388 9905.4209 -nan 9905.4209 7.16964388] [0 0 -nan 0 0]\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:autograph","TF 2.12"],"created_at":"2023-05-02T13:48:05Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60468"},{"issue_number":392,"repository":"tensorflow\/tensorflow","title":"AutoGraph error when function is inside a match ","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\n2.13.0-dev20230501\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nGoogle Colab\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nWhen a `tf.data.Dataset.map` function is defined inside a `match` scope and used by `tf.data.Dataset.apply` the AutoGraph fails. This should not happen, as the match statement should be irrelevant to the function definition. I suspect this error happens in more AutoGraph cases than just `tf.data.Dataset.map` via `tf.data.Dataset.apply`.\r\n\r\nThe issue is reproduced on Google Colab with both nightly (2.13.0-dev20230501) and 2.12.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nSee https:\/\/colab.research.google.com\/drive\/1x07aUgISrYQohsG7FYmc70LeYjVcGixG#scrollTo=auSxkDSinh3R\r\n\r\n```python\r\nimport tensorflow as tf\r\nparameter = 'a'\r\n\r\nmatch parameter:\r\n    case 'a':\r\n        def fn(ds):\r\n            return ds.map(lambda x: x + 1)\r\n\r\ndataset = tf.data.Dataset.range(1) \\\r\n    .apply(fn)\r\n\r\nfor x in dataset:\r\n    print(x)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 427, in converted_call\r\n    converted_f = _convert_actual(target_entity, program_ctx)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 269, in _convert_actual\r\n    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/autograph\/pyct\/transpiler.py\", line 282, in transform\r\n    return self.transform_function(obj, user_context)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/autograph\/pyct\/transpiler.py\", line 466, in transform_function\r\n    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/autograph\/pyct\/transpiler.py\", line 342, in transform_function\r\n    node, source = parser.parse_entity(fn, future_features=future_features)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/autograph\/pyct\/parser.py\", line 145, in parse_entity\r\n    return _parse_lambda(entity)\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/autograph\/pyct\/parser.py\", line 275, in _parse_lambda\r\n    lambda_nodes.extend(\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/autograph\/pyct\/parser.py\", line 275, in <genexpr>\r\n    lambda_nodes.extend(\r\n  File \"\/usr\/lib\/python3.10\/ast.py\", line 390, in walk\r\n    todo.extend(iter_child_nodes(node))\r\n  File \"\/usr\/lib\/python3.10\/ast.py\", line 272, in iter_child_nodes\r\n    for name, field in iter_fields(node):\r\n  File \"\/usr\/lib\/python3.10\/ast.py\", line 260, in iter_fields\r\n    for field in node._fields:\r\nAttributeError: 'NoneType' object has no attribute '_fields'\r\nWARNING:tensorflow:AutoGraph could not transform <function fn.<locals>.<lambda> at 0x7f9fc56e9870> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'NoneType' object has no attribute '_fields'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n2.13.0-dev20230501\r\nConverted call: <function fn.<locals>.<lambda> at 0x7f9fc56e9870>\r\n    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\r\n    kwargs: {}\r\n\r\nNot allowed: <method-wrapper '__call__' of function object at 0x7f9fc56e9870>: default rule\r\nNot allowed: <function fn.<locals>.<lambda> at 0x7f9fc56e9870>: default rule\r\n<function fn.<locals>.<lambda> at 0x7f9fc56e9870> is not cached for subkey ConversionOptions[{}]\r\nError transforming entity <function fn.<locals>.<lambda> at 0x7f9fc56e9870>\r\nWARNING: AutoGraph could not transform <function fn.<locals>.<lambda> at 0x7f9fc56e9870> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: 'NoneType' object has no attribute '_fields'\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\ntf.Tensor(1, shape=(), dtype=int64)\r\n```\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:autograph","TF 2.12"],"created_at":"2023-05-01T14:17:26Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60458"},{"issue_number":393,"repository":"tensorflow\/tensorflow","title":"Inconsistency in SyncBatchNormalization\/BatchNormalization(synchronized=True) Results during Distributed Training on CPUs","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.11.0\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux Ubuntu 20.04.5 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.8.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n11.7\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nI am reporting an issue encountered during the distributed training of a model with `SyncBatchNormalization` layers on CPUs. The issue presents as a relatively large inconsistency between the results produced when training the same model on a single device versus multiple devices.\r\n\r\nWe've prepared a reproduction script demonstrating this issue. We set the weights of the `SyncBatchNormalization`\/`BatchNormalization` layer to reproduce this bug. After conducting a one-step training, the predictions from the identical model trained on a single CPU and two CPUs exhibited relatively large differences (measured by L-infinity distance, Linf = 0.00074467063). As a comparison, we build another model with exactly the same architecture but removing the `SyncBatchNormalization` layer, and the prediction difference from this model trained on a single CPU and two CPUs is Linf = 1.0131771e-09.\r\n\r\nAnother comparison experiment is on GPUs. We trained the same model containing `SyncBatchNormalization` layer on a single GPU and two GPUs. The Linf of model\u2019s prediction result is 2.5331974e-07. It\u2019s much smaller than 1CPU vs 2CPU (Linf = 0.00074467063). This is weird because I would expect the CPU executions to be more deterministic than GPUs, and I would expect larger inconsistencies in the GPU runs if the inconsistencies are caused by variance.\r\n\r\nTo ensure a controlled environment, we used the same training data for one-step training on both setups, and then evaluated the model. To guarantee that the distributed trainings are expected to produce the same results given different number of devices, we use `MirroredStrategy`, keep the global batch size the same, and use `tf.keras.losses.Reduction.AUTO` reduction. To make the difference more apparent given a limited amount of training data, we deliberately chose a relatively high learning rate (lr=10).\r\n\r\nIt's noteworthy that `SyncBatchNormalization` has been deprecated as of TensorFlow version 2.12. Therefore, we also conducted the same experiment using TensorFlow's nightly build (2.13.0-dev20230428), replacing `SyncBatchNormalization()` with `BatchNormalization(synchronized=True)`. This experiment still manifested the same inconsistency. The prediction from the model containing batchnorm layer trained on a single CPU and two CPUs exhibits a substantial difference of Linf = 0.00010111928, while for the model removing batchnorm layer it\u2019s Linf = 6.212488e-36.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nFor TF 2.11.0 CPU experiments, the model contains `SyncBatchNormalization` layer:\r\nhttps:\/\/colab.research.google.com\/drive\/11EseXxq_uHweY7omt7JCr1mc-42Kf1H5?usp=sharing\r\n\r\nFor TF 2.11.0 GPU experiments, the model contains `SyncBatchNormalization` layer (fix seed to generate same inputs):\r\nhttps:\/\/colab.research.google.com\/drive\/1m_gCWzb_OKVTYAMMs8YbdKUYM3aINNPV?usp=sharing \r\n\r\nFor TF nightly 2.13.0-dev20230428, the model contains `BatchNormalization(synchronized=True)` layer:\r\nhttps:\/\/colab.research.google.com\/drive\/1N-aXPcfckVb8fPDSmghMlzBOFEGq6RzM?usp=sharing\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nFor TF 2.11.0:\r\n\r\n```shell\r\ncontain syncbatchnorm:  True\r\n1CPU vs 2CPU: 0.00074467063\r\n\r\ncontain syncbatchnorm:  False\r\n1CPU vs 2CPU: 1.0131771e-09\r\n\r\ncontain syncbatchnorm:  True\r\n1GPU vs 2GPU: 2.5331974e-07\r\n```\r\n\r\nFor TF nightly 2.13.0-dev20230428:\r\n\r\n```shell\r\ncontain syncbatchnorm:  True\r\n1CPU vs 2CPU: 0.00010111928\r\n\r\ncontain syncbatchnorm:  False\r\n1CPU vs 2CPU: 6.212488e-36\r\n```\r\n\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:dist-strat","TF 2.11"],"created_at":"2023-04-29T03:41:12Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60438"},{"issue_number":394,"repository":"tensorflow\/tensorflow","title":"The dim size of inferred shape in GraphProperties is less than -1, which is inconsistent with TensorShapeProto","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.12.0\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nUbuntu 22.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n5.3.0\r\n\r\n### GCC\/Compiler version\r\n\r\n11.3.0\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n### Behavior\r\nViewed dim size less than -1 after called `GraphProperties::InferStatically`, while in [TensorShapeProto](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/core\/framework\/tensor_shape.proto#L17) the comment says either the dim size is greater than 0 or -1 (meaning unknown). \r\n```\r\nmessage Dim {\r\n    \/\/ Size of the tensor in that dimension.\r\n    \/\/ This value must be >= -1, but values of -1 are reserved for \"unknown\"\r\n    \/\/ shapes (values of -1 mean \"unknown\" dimension). \r\n    int64 size = 1;\r\n};\r\n```\r\nThe two are inconsistent.\r\n### The Cause\r\nIn [this code](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/core\/grappler\/costs\/graph_properties.cc#L118) which constructs the inferred dims, it assigns a negative id to unknown dimensions, starting at -2. \r\n```c++\r\n\/\/ Assign a negative id to unknown dimensions, starting at -2 (the -1 id\r\n\/\/ reserved by TensorFlow).\r\nvoid ExtractValue(DimensionHandle d, int64_t* result) {\r\n  if (!InferenceContext::ValueKnown(d)) {\r\n    *result = -counter;\r\n    counter++;\r\n  } else {\r\n    ...\r\n  }\r\n}\r\n```\r\nIt can be seen from the code that size<=-1 means unknown, but the unknown size in [this code](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/core\/grappler\/utils\/symbolic_shapes.cc#L42) is identified with `==-1`.\r\n`bool IsUnknown(const TensorShapeProto::Dim& dim) { return dim.size() == -1; }`\r\nAnd the [`ShapeIsSymbolicallyDefined`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/core\/grappler\/utils\/symbolic_shapes.cc#L44), which calls `IsUnknown `may cause bugs.\r\n```c++\r\nbool ShapeIsSymbolicallyDefined(const TensorShapeProto& shape) {\r\n  return !shape.unknown_rank() &&\r\n         std::all_of(\r\n             shape.dim().begin(), shape.dim().end(),\r\n             [](const TensorShapeProto::Dim& dim) { return !IsUnknown(dim); });\r\n}\r\n```\r\n[`ShapesSymbolicallyEqual`](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/core\/grappler\/utils\/symbolic_shapes.cc#L76) function is the same problem. \r\n### Questions:\r\n1. The inferred dim size is less than -1, inconsistent with TensorShapeProto, is this expected?\r\n2. Is the `IsUnknown` function reasonable?\r\n3. Do the `ShapeIsSymbolicallyDefined` and `ShapesSymbolicallyEqual` cause undefined behavior? I see some graph optimization path will call this function after shape inference.\r\n\r\n### Standalone code to reproduce the issue\r\nI manually print the shape inference log in the Tensorflow source code. First, patch my log into the Tensorflow source code, then compile the source code, and finally run the python script I gave. It can be seen from the log that the result of shape inference has dimensions less than -1.\r\n```shell\r\ngit checkout v2.12.0\r\n\r\n# apply patch\r\necho 'diff --git a\/tensorflow\/core\/grappler\/optimizers\/arithmetic_optimizer.cc b\/tensorflow\/core\/grappler\/optimizers\/arithmetic_optimizer.cc\r\nindex 40c27828d26..899a9d059cb 100644\r\n--- a\/tensorflow\/core\/grappler\/optimizers\/arithmetic_optimizer.cc\r\n+++ b\/tensorflow\/core\/grappler\/optimizers\/arithmetic_optimizer.cc\r\n@@ -4452,6 +4452,26 @@ Status ArithmeticOptimizer::Optimize(Cluster* \/*cluster*\/,\r\n     VLOG(1) << \"Shape inference failed.\" << status.error_message();\r\n   }\r\n\r\n+  for (auto& node : optimized_graph_->node()) {\r\n+    VLOG(0) << \"node_name: \" << node.name();\r\n+    const auto& input_properties =\r\n+        graph_properties_->GetInputProperties(node.name());\r\n+    for (int i = 0; i < input_properties.size(); i++) {\r\n+      auto& property = input_properties[i];\r\n+      VLOG(0) << \"input\" << i << \": \";\r\n+      const TensorShapeProto& tsp = property.shape();\r\n+      if (tsp.unknown_rank()) {\r\n+        VLOG(0) << \"unknown shape\";\r\n+        continue;\r\n+      }\r\n+      VLOG(0) << \"input_rank: \" << tsp.dim_size();\r\n+      for (int j = 0; j < tsp.dim_size(); j++) {\r\n+        VLOG(0) << \"dim\" << j << \" size: \" << tsp.dim(j).size();\r\n+      }\r\n+    }\r\n+    VLOG(0);\r\n+  }\r\n+\r\n   \/\/ Perform the optimizations.\r\n   TF_RETURN_IF_ERROR(SimplifyArithmeticOps(can_use_shapes));\r\n   *optimized_graph = std::move(*optimized_graph_);\r\n' | git apply\r\n\r\n# build\r\nbazel build \/\/tensorflow\/tools\/pip_package:build_pip_package\r\n...(continue to build)\r\n```\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n@tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.float32)])\r\ndef fun(x):\r\n    y = tf.constant(np.ones((2, 4)), dtype=tf.float32)\r\n    return tf.add(x, y)\r\n\r\noutput = fun(np.ones((2, 4), dtype=np.float32))\r\nprint(\"output: \", output)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nnode_name: Add\r\ninput0:\r\ninput_rank: 2\r\ndim0 size: 2\r\ndim1 size: 4\r\ninput1:\r\ninput_rank: 2\r\ndim0 size: -2\r\ndim1 size: -3\r\n```\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","subtype: ubuntu\/linux","TF 2.12"],"created_at":"2023-04-24T04:07:40Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60406"},{"issue_number":395,"repository":"tensorflow\/tensorflow","title":"The inferred shape of `tf.RaggedTensor.row_lengths(axis=2)` in Keras graph is incorrect for ragged tensor with uniform row lengths","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\nTF 2.12, TF nightly 2.13.0-dev20230420\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nAssume a Keras graph gets a ragged tensor with uniform row lengths in the axis 1, so for example\r\n```python\r\ninputs = tf.keras.layers.Input([64, None], ragged=True)\r\n```\r\n\r\nThen `inputs.row_lengths(axis=2)` is a `KerasTensor` with a correct spec `RaggedTensorSpec(TensorShape([None, 64]), ...)`.\r\n\r\nHowever, when you index the first axis using a \"full\" slice, i.e.\r\n```python\r\ninputs.row_lengths(axis=2)[:]\r\n```\r\nyou should get the same tensor -- but you get a `KerasTensor` with an incorrect spec `RaggedTensorSpec(TensorShape([1, 64]), ...)`.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nA Colab notebook reproducing the issue both in TF 2.12.0 and in TF nightly 2.13.0-dev20230420 can be found at https:\/\/colab.research.google.com\/drive\/1RKvNdB_81yKZkfzDpefIPJU2pgqZuYUY?usp=sharing\r\n\r\nThe full source also follows:\r\n\r\n```python\r\ninputs = tf.keras.layers.Input([64, None], ragged=True)\r\nprint(inputs)\r\n\r\nprint(inputs.row_lengths(axis=1))\r\nprint(inputs.row_lengths(axis=1)[:]) # OK, is the same as above\r\n\r\nprint(inputs.row_lengths(axis=2))\r\nprint(inputs.row_lengths(axis=2)[:]) # Problem, should be the same as above\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```python\r\nKerasTensor(type_spec=RaggedTensorSpec(TensorShape([None, 64, None]), tf.float32, 2, tf.int64), name='input_1', description=\"created by layer 'input_1'\")\r\nKerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None), name='input.row_lengths\/sub:0', description=\"created by layer 'input.row_lengths'\")\r\nKerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None), name='tf.__operators__.getitem\/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem'\")\r\nKerasTensor(type_spec=RaggedTensorSpec(TensorShape([None, 64]), tf.int64, 1, tf.int64), description=\"created by layer 'input.row_lengths_2'\")\r\nKerasTensor(type_spec=RaggedTensorSpec(TensorShape([1, 64]), tf.int64, 1, tf.int64), description=\"created by layer 'tf.__operators__.ragged_getitem'\")\r\n```\r\n<\/details>","labels":["stat:awaiting response","type:bug","comp:keras","comp:ops","TF 2.12"],"created_at":"2023-04-22T18:30:48Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60400"},{"issue_number":396,"repository":"tensorflow\/tensorflow","title":"\/\/tensorflow\/python\/client:session_partial_run_test is flaky","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ngit HEAD\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\nn\/a\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n5.3.0\n\n### GCC\/Compiler version\n\n10.2.1\n\n### CUDA\/cuDNN version\n\nn\/a\n\n### GPU model and memory\n\nn\/a\n\n### Current Behaviour?\n\nUnit test reports as FLAKY or FAILED.\r\n\r\nSee https:\/\/source.cloud.google.com\/results\/invocations\/dea422ff-7e14-4fc1-b324-0129ecd7ffbc\/log or https:\/\/github.com\/tensorflow\/tensorflow\/actions\/runs\/4731924097\/jobs\/8397430880#step:5:23224\n\n### Standalone code to reproduce the issue\n\n```shell\ndocker exec tf bazel --bazelrc=\/usertools\/cpu.bazelrc test --config=rbe --config=pycpp --config=build_event_export\n```\n\n\n### Relevant log output\n\n```shell\n======================================================================\r\nERROR: testPartialRunMissingPlaceholderFeedExceptionDist (__main__.PartialRunTest)\r\nPartialRunTest.testPartialRunMissingPlaceholderFeedExceptionDist\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/client\/session_partial_run_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1379, in _do_call\r\n    return fn(*args)\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/client\/session_partial_run_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1369, in _prun_fn\r\n    return self._call_tf_sessionprun(handle, feed_dict, fetch_list)\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/client\/session_partial_run_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1460, in _call_tf_sessionprun\r\n    return tf_session.TF_SessionPRun_wrapper(self._session, handle, feed_dict,\r\ntensorflow.python.framework.errors_impl.InternalError: From \/job:localhost\/replica:0\/task:0:\r\nValidateDevices called before initialization.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/client\/session_partial_run_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_util.py\", line 1629, in decorated\r\n    return f(self, *args, **kwargs)\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/client\/session_partial_run_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session_partial_run_test.py\", line 269, in testPartialRunMissingPlaceholderFeedExceptionDist\r\n    self.RunTestPartialRunMissingPlaceholderFeedException(\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/client\/session_partial_run_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session_partial_run_test.py\", line 119, in RunTestPartialRunMissingPlaceholderFeedException\r\n    sess.partial_run(handle, fetches[0])\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/client\/session_partial_run_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1026, in partial_run\r\n    return self._run(handle, fetches, feed_dict, None, None)\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/client\/session_partial_run_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1192, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/client\/session_partial_run_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1375, in _do_run\r\n    return self._do_call(_prun_fn, handle, feeds, fetches)\r\n  File \"\/b\/f\/w\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/client\/session_partial_run_test.runfiles\/org_tensorflow\/tensorflow\/python\/client\/session.py\", line 1398, in _do_call\r\n    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\r\ntensorflow.python.framework.errors_impl.InternalError: Graph execution error:\r\n\r\nFrom \/job:localhost\/replica:0\/task:0:\r\nValidateDevices called before initialization.\r\n\r\n----------------------------------------------------------------------\r\nRan 25 tests in 2.625s\r\n\r\nFAILED (errors=1, skipped=1)\r\n================================================================================\n```\n<\/details>","labels":["stat:awaiting response","type:bug","type:build\/install","subtype: ubuntu\/linux","TF 2.12"],"created_at":"2023-04-18T13:30:50Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60351"},{"issue_number":397,"repository":"tensorflow\/tensorflow","title":"TPU Tensorflow mapping string label to int with ","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.12\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nColab\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nNone\n\n### GPU model and memory\n\nNone\n\n### Current Behaviour?\n\nI currently get an error when trying to get my batch from a tf.dataset. I am mapping the string label in the tfrecord into int with tf.lookup.StaticHashTable. \r\n```\r\nInternalError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused\r\nAdditional GRPC error information from remote target \/job:localhost\/replica:0\/task:0\/device:CPU:0:\r\n```\r\n\r\nBecause of that I can't get the batch of my dataset, and train a model with TPU. It works fine with GPU.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1vAADMl5fBulmSnbmbOTrMjyzAYAgHhFl?authuser=1#scrollTo=_zv9OlXbIqDf\n```\n\n\n### Relevant log output\n\n```shell\nAttributeError                            Traceback (most recent call last)\r\n\r\n\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py in _next_internal(self)\r\n    786         # Fast path for the case `self._structure` is not a nested structure.\r\n--> 787         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    788       except AttributeError:\r\n\r\nAttributeError: 'tuple' object has no attribute '_from_compatible_tensor_list'\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n\r\n13 frames\r\n\r\nInternalError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused\r\nAdditional GRPC error information from remote target \/job:localhost\/replica:0\/task:0\/device:CPU:0:\r\n:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2023-04-16T09:15:30.550805248+00:00\"}\r\nExecuting non-communication op <MakeIterator> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInternalError                             Traceback (most recent call last)\r\n\r\n\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/eager\/executor.py in wait(self)\r\n     63   def wait(self):\r\n     64     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 65     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     66 \r\n     67   def clear_error(self):\r\n\r\nInternalError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused\r\nAdditional GRPC error information from remote target \/job:localhost\/replica:0\/task:0\/device:CPU:0:\r\n:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2023-04-16T09:15:30.550805248+00:00\"}\r\nExecuting non-communication op <MakeIterator> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.\n```\n<\/details>","labels":["stat:awaiting response","type:bug","comp:tpus","TF 2.12"],"created_at":"2023-04-16T09:22:35Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60336"},{"issue_number":398,"repository":"tensorflow\/tensorflow","title":"ImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.13.0\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n6.1.1\r\n\r\n### GCC\/Compiler version\r\n\r\n9.5.0\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nI build the lastest TensorFlow code from source successfully with\r\n`bazel build --config=opt \/\/tensorflow\/tools\/pip_package:build_pip_package`\r\n\r\nThen I generate a TensorFlow whl successfully with\r\n`.\/bazel-bin\/tensorflow\/tools\/pip_package\/build_pip_package \/tmp\/tensorflow_pkg`\r\n\r\nBut when I pip install this whl, import tensorflow got \"ImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory\". \r\n\r\nI found the whl file generated is only 80.64M. But I think it should be about 200M.\r\n\r\nBut there has libtensorflow_cc.so.2 file under path tensorflow\/bazel-bin\/tensorflow. I don't know why it wasn't packed into the whl file.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/data1\/envs\/xtc3.9\/lib\/python3.9\/site-packages\/tensorflow\/__init__.py\", line 38, in <module>\r\n    from tensorflow.python.tools import module_util as _module_util\r\n  File \"\/data1\/envs\/xtc3.9\/lib\/python3.9\/site-packages\/tensorflow\/python\/__init__.py\", line 36, in <module>\r\n    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\r\n  File \"\/data1\/envs\/xtc3.9\/lib\/python3.9\/site-packages\/tensorflow\/python\/pywrap_tensorflow.py\", line 26, in <module>\r\n    self_check.preload_check()\r\n  File \"\/data1\/envs\/xtc3.9\/lib\/python3.9\/site-packages\/tensorflow\/python\/platform\/self_check.py\", line 63, in preload_check\r\n    from tensorflow.python.platform import _pywrap_cpu_feature_guard\r\nImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory\r\n```\r\n<\/details>","labels":["type:bug","type:build\/install","subtype: ubuntu\/linux","TF 2.13"],"created_at":"2023-04-14T09:42:40Z","comments":16,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60326"},{"issue_number":399,"repository":"tensorflow\/tensorflow","title":"Stride-1 tf.nn.conv2d with XLA is 1.5x slower then without XLA, as far as stride-2 tf.nn.depthwise_conv2d","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.12.0, 2.13.0-dev20230412\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nGoogle Colab\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nGoogle Colab\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n11.8\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nSee example below to reproduce. Here is speed test results:\r\n\r\nstride-1 conv 40.66\r\nstride-1 conv_jit 64.11  \/\/ conv2d is slower with JIT but only if stride=1\r\nstride-2 conv 40.18\r\nstride-2 conv_jit 28.05  \/\/ when stride=2 it is FASTER with JIT\r\n\r\nstride-1 dwconv 9.82\r\nstride-1 dwconv_jit 5.72  \/\/ dwconv is faster with JIT but only if stride=1\r\nstride-2 dwconv 2.59\r\nstride-2 dwconv_jit 4.2  \/\/ when stride=2 it is SLOWER with JIT\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1zqqPVVKt4ILRA1rCoWjB1uOtB3D0hDc-?usp=sharing\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting response","type:bug","comp:xla","type:performance","TF 2.12"],"created_at":"2023-04-13T10:01:27Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60312"},{"issue_number":400,"repository":"tensorflow\/tensorflow","title":"Training stopping because of  BufferError: Existing exports of data: object cannot be re-sized or something wrong with tornado","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nNo\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.12.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nNAME=\"CentOS Linux\" VERSION=\"7 (Core)\"\n\n### Mobile device\n\nNAME=\"CentOS Linux\" VERSION=\"7 (Core)\"\n\n### Python version\n\n3.9.16\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n 11.8.0 \n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\nThe model training would just stop abruptly \r\n\r\nhttps:\/\/colab.research.google.com\/drive\/1WiqyF7dCdnNBIANEY80Pxw_mVz4fyV-S?usp=sharing\n\n### Standalone code to reproduce the issue\n\n```shell\nVoxelmoprh library training\n```\n\n\n### Relevant log output\n\n```shell\n(tf) vr-lab@pop-os:~$ jupyter notebook\r\n\r\n  _   _          _      _\r\n | | | |_ __  __| |__ _| |_ ___\r\n | |_| | '_ \\\/ _` \/ _` |  _\/ -_)\r\n  \\___\/| .__\/\\__,_\\__,_|\\__\\___|\r\n       |_|\r\n                       \r\nRead the migration plan to Notebook 7 to learn about the new features and the actions to take if you are using extensions.\r\n\r\nhttps:\/\/jupyter-notebook.readthedocs.io\/en\/latest\/migrate_to_notebook7.html\r\n\r\nPlease note that updating to Notebook 7 might break some of your extensions.\r\n\r\n[I 00:02:49.290 NotebookApp] Serving notebooks from local directory: \/home\/vr-lab\r\n[I 00:02:49.290 NotebookApp] Jupyter Notebook 6.5.4 is running at:\r\n[I 00:02:49.290 NotebookApp] http:\/\/localhost:8888\/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466\r\n[I 00:02:49.290 NotebookApp]  or http:\/\/127.0.0.1:8888\/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466\r\n[I 00:02:49.290 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n[C 00:02:49.334 NotebookApp] \r\n    \r\n    To access the notebook, open this file in a browser:\r\n        file:\/\/\/home\/vr-lab\/.local\/share\/jupyter\/runtime\/nbserver-405435-open.html\r\n    Or copy and paste one of these URLs:\r\n        http:\/\/localhost:8888\/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466\r\n     or http:\/\/127.0.0.1:8888\/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466\r\n[I 00:03:15.170 NotebookApp] Kernel started: 4915aa8a-d4aa-4d50-885f-810d53eae7db, name: python3\r\n[I 00:03:20.670 NotebookApp] Kernel restarted: 4915aa8a-d4aa-4d50-885f-810d53eae7db\r\n[W 00:03:20.684 NotebookApp] Replacing stale connection: 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d\r\n[W 00:03:21.180 NotebookApp] zmq message arrived on closed channel\r\n[I 00:03:21.181 NotebookApp] Starting buffering for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d\r\n[I 00:03:21.183 NotebookApp] Restoring connection for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d\r\n[I 00:03:21.689 NotebookApp] Replaying 1 buffered messages\r\n[E 00:03:21.761 NotebookApp] Uncaught exception, closing connection.\r\n    Traceback (most recent call last):\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 702, in _handle_events\r\n        self._handle_write()\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 976, in _handle_write\r\n        self._write_buffer.advance(num_bytes)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 182, in advance\r\n        assert 0 < size <= self._size\r\n    AssertionError\r\n[W 00:03:21.764 NotebookApp] Write error on <socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>: [Errno 9] Bad file descriptor\r\n[W 00:03:21.766 NotebookApp] zmq message arrived on closed channel\r\n[W 00:03:21.767 NotebookApp] zmq message arrived on closed channel\r\nException in callback None()\r\nhandle: <Handle cancelled>\r\nTraceback (most recent call last):\r\n  File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/asyncio\/events.py\", line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/platform\/asyncio.py\", line 206, in _handle_events\r\n    handler_func(fileobj, events)\r\n  File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 702, in _handle_events\r\n    self._handle_write()\r\n  File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 976, in _handle_write\r\n    self._write_buffer.advance(num_bytes)\r\n  File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 182, in advance\r\n    assert 0 < size <= self._size\r\nAssertionError\r\n[I 00:03:21.768 NotebookApp] Starting buffering for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d\r\n2023-04-11 00:03:22.084618: I tensorflow\/core\/platform\/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-04-11 00:03:22.225493: I tensorflow\/core\/util\/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n[I 00:03:22.803 NotebookApp] Restoring connection for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d\r\n[I 00:03:22.803 NotebookApp] Replaying 1 buffered messages\r\n2023-04-11 00:03:22.815590: W tensorflow\/compiler\/xla\/stream_executor\/platform\/default\/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/:\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/nvidia\/cudnn\/lib\r\n2023-04-11 00:03:22.815709: W tensorflow\/compiler\/xla\/stream_executor\/platform\/default\/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/:\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/nvidia\/cudnn\/lib\r\n2023-04-11 00:03:22.815716: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\r\n2023-04-11 00:03:25.015062: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1613] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 12776 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:af:00.0, compute capability: 8.6\r\n2023-04-11 00:03:40.078576: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:428] Loaded cuDNN version 8600\r\n[I 00:05:15.159 NotebookApp] Saving file at \/Music\/HybridMorph Please don't delete\/HybridMorph_proof of concept.ipynb\r\nTask exception was never retrieved\r\nfuture: <Task finished name='Task-76' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at \/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py:1090> exception=WebSocketClosedError()>\r\nTraceback (most recent call last):\r\n  File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 1092, in wrapper\r\n    await fut\r\ntornado.iostream.StreamClosedError: Stream is closed\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/asyncio\/tasks.py\", line 256, in __step\r\n    result = coro.send(None)\r\n  File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 1094, in wrapper\r\n    raise WebSocketClosedError()\r\ntornado.websocket.WebSocketClosedError\r\n[E 01:03:52.904 NotebookApp] Exception in callback <bound method WebSocketMixin.send_ping of ZMQChannelsHandler(4915aa8a-d4aa-4d50-885f-810d53eae7db)>\r\n    Traceback (most recent call last):\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/ioloop.py\", line 921, in _run\r\n        val = self.callback()\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/notebook\/base\/zmqhandlers.py\", line 188, in send_ping\r\n        self.ping(b'')\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 445, in ping\r\n        self.ws_connection.write_ping(data)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 1101, in write_ping\r\n        self._write_frame(True, 0x9, data)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 1061, in _write_frame\r\n        return self.stream.write(frame)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 540, in write\r\n        self._write_buffer.append(data)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 157, in append\r\n        b += data  # type: ignore\r\n    BufferError: Existing exports of data: object cannot be re-sized\r\n[E 01:13:22.812 NotebookApp] Uncaught exception in ZMQStream callback\r\n    Traceback (most recent call last):\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 584, in _run_callback\r\n        f = callback(*args, **kwargs)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 308, in stream_callback\r\n        return callback(self, msg)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/notebook\/services\/kernels\/handlers.py\", line 572, in _on_zmq_reply\r\n        super()._on_zmq_reply(stream, msg)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/notebook\/base\/zmqhandlers.py\", line 256, in _on_zmq_reply\r\n        self.write_message(msg, binary=isinstance(msg, bytes))\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 339, in write_message\r\n        return self.ws_connection.write_message(message, binary=binary)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 1086, in write_message\r\n        fut = self._write_frame(True, opcode, message, flags=flags)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 1061, in _write_frame\r\n        return self.stream.write(frame)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 540, in write\r\n        self._write_buffer.append(data)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 157, in append\r\n        b += data  # type: ignore\r\n    BufferError: Existing exports of data: object cannot be re-sized\r\n[E 01:13:22.815 NotebookApp] Uncaught exception in zmqstream callback\r\n    Traceback (most recent call last):\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 634, in _handle_events\r\n        self._handle_recv()\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 663, in _handle_recv\r\n        self._run_callback(callback, msg)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 584, in _run_callback\r\n        f = callback(*args, **kwargs)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 308, in stream_callback\r\n        return callback(self, msg)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/notebook\/services\/kernels\/handlers.py\", line 572, in _on_zmq_reply\r\n        super()._on_zmq_reply(stream, msg)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/notebook\/base\/zmqhandlers.py\", line 256, in _on_zmq_reply\r\n        self.write_message(msg, binary=isinstance(msg, bytes))\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 339, in write_message\r\n        return self.ws_connection.write_message(message, binary=binary)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 1086, in write_message\r\n        fut = self._write_frame(True, opcode, message, flags=flags)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 1061, in _write_frame\r\n        return self.stream.write(frame)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 540, in write\r\n        self._write_buffer.append(data)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 157, in append\r\n        b += data  # type: ignore\r\n    BufferError: Existing exports of data: object cannot be re-sized\r\n[E 01:13:22.815 NotebookApp] Exception in callback functools.partial(<function ZMQStream._update_handler.<locals>.<lambda> at 0x7f1de4ff4b80>)\r\n    Traceback (most recent call last):\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/ioloop.py\", line 740, in _run_callback\r\n        ret = callback()\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 718, in <lambda>\r\n        self.io_loop.add_callback(lambda: self._handle_events(self.socket, 0))\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 634, in _handle_events\r\n        self._handle_recv()\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 663, in _handle_recv\r\n        self._run_callback(callback, msg)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 584, in _run_callback\r\n        f = callback(*args, **kwargs)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/zmq\/eventloop\/zmqstream.py\", line 308, in stream_callback\r\n        return callback(self, msg)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/notebook\/services\/kernels\/handlers.py\", line 572, in _on_zmq_reply\r\n        super()._on_zmq_reply(stream, msg)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/notebook\/base\/zmqhandlers.py\", line 256, in _on_zmq_reply\r\n        self.write_message(msg, binary=isinstance(msg, bytes))\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 339, in write_message\r\n        return self.ws_connection.write_message(message, binary=binary)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 1086, in write_message\r\n        fut = self._write_frame(True, opcode, message, flags=flags)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/websocket.py\", line 1061, in _write_frame\r\n        return self.stream.write(frame)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 540, in write\r\n        self._write_buffer.append(data)\r\n      File \"\/home\/vr-lab\/anaconda3\/envs\/tf\/lib\/python3.9\/site-packages\/tornado\/iostream.py\", line 157, in append\r\n        b += data  # type: ignore\r\n    BufferError: Existing exports of data: object cannot be re-sized\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:apis","TF 2.12"],"created_at":"2023-04-13T05:24:00Z","comments":28,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60309"},{"issue_number":401,"repository":"tensorflow\/tensorflow","title":"Respect Keras layer names for output operations in Concrete Function","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nFeature Request\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\n2.12.0\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.8.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nWhen converting a Keras model to concrete function, you can preserve the input name by creating a named TensorSpec, but the outputs are always created for you by just slapping `tf.identity` on top of whatever you had there, even if it was a custom named `tf.identity` operation. Since many converters rely on concrete functions to make their own representation (TFLite, ONNX, CoreML, etc), this behavior messes up the output operation names, often making them inconsistent with each other. \r\nThere's currently no workaround for that. You *can* access previous graph nodes by calling a layer  named like {model_name}\/{output_layer_name} when doing inference on frozen graph itself, but it won't help you in any way to convert the model.\r\n\r\nSo I'd be happy to see one of those things as a solution to that:\r\n1) Add an option to explicitly specify the TensorSpec for outputs, just the way we do it for inputs. This would be the most obvious and convenient way of doing it from a user standpoint\r\n2) Don't add new identity operations on top of existing ones. More of a kludge, but would get the job done\r\n3) Add an option to rename operations in concrete function post factum.\r\n4) Add an option to cut off the operations in concrete function past a certain node. \r\n5) Add an option to convert a graph into a concrete function. Since you can directly modify graphs, this could work as well\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\n# Build simple model\r\ninputs = tf.keras.Input((224, 224, 3), name='custom_input_layer')\r\nx = tf.keras.layers.Flatten()(inputs)\r\nx = tf.keras.layers.Dense(512, activation='relu')(x)\r\nx = tf.keras.layers.Dense(256, activation='relu')(x)\r\nx = tf.keras.layers.Dense(128, activation='relu')(x)\r\nx = tf.keras.layers.Dense(1, activation='sigmoid', name='custom_output_layer')(x)\r\n\r\nmodel = tf.keras.Model(inputs=inputs, outputs=x, name='my_custom_model_name')\r\nmodel.summary()\r\n\r\ninput_tensors = [tf.TensorSpec(shape=inp.shape, dtype=tf.float32, name=inp.name) for inp in model.inputs]\r\nconcrete_function = tf.function(lambda x: model(x)).get_concrete_function(x=input_tensors)\r\n\r\nprint(concrete_function.inputs)  # we can see 'custom_input_layer:0' is there. So is the 'true' output 'my_custom_model_name\/custom_output_layer\/BiasAdd\/ReadVariableOp\/resource:0'\r\nprint(concrete_function.outputs)  # pesky Identity node gets inserted\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nModel: \"my_custom_model_name\"\r\n_________________________________________________________________\r\n Layer (type)                Output Shape              Param #   \r\n=================================================================\r\n custom_input_layer (InputLa  [(None, 224, 224, 3)]    0         \r\n yer)                                                            \r\n                                                                 \r\n flatten (Flatten)           (None, 150528)            0         \r\n                                                                 \r\n dense (Dense)               (None, 512)               77070848  \r\n                                                                 \r\n dense_1 (Dense)             (None, 256)               131328    \r\n                                                                 \r\n dense_2 (Dense)             (None, 128)               32896     \r\n                                                                 \r\n custom_output_layer (Dense)  (None, 1)                129       \r\n                                                                 \r\n=================================================================\r\nTotal params: 77,235,201\r\nTrainable params: 77,235,201\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n[<tf.Tensor 'custom_input_layer:0' shape=(None, 224, 224, 3) dtype=float32>, <tf.Tensor 'my_custom_model_name\/dense\/MatMul\/ReadVariableOp\/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name\/dense\/BiasAdd\/ReadVariableOp\/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name\/dense_1\/MatMul\/ReadVariableOp\/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name\/dense_1\/BiasAdd\/ReadVariableOp\/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name\/dense_2\/MatMul\/ReadVariableOp\/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name\/dense_2\/BiasAdd\/ReadVariableOp\/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name\/custom_output_layer\/MatMul\/ReadVariableOp\/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name\/custom_output_layer\/BiasAdd\/ReadVariableOp\/resource:0' shape=() dtype=resource>]\r\n[<tf.Tensor 'Identity:0' shape=(None, 1) dtype=float32>]\r\n```\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.12"],"created_at":"2023-04-11T08:14:38Z","comments":9,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60289"},{"issue_number":402,"repository":"tensorflow\/tensorflow","title":"Collected Tensorflow profiles are not recognized in Tensorboard","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.13.0-dev20230406\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux Ubuntu 22.04\r\n\r\n### Mobile device\r\n\r\nn\/a\r\n\r\n### Python version\r\n\r\n3.9.16\r\n\r\n### Bazel version\r\n\r\nn\/a\r\n\r\n### GCC\/Compiler version\r\n\r\nn\/a\r\n\r\n### CUDA\/cuDNN version\r\n\r\n11.8.0\/8.6.0.163\r\n\r\n### GPU model and memory\r\n\r\nNVIDIA GeForce RTX 3080 Ti 12GiB\r\n\r\n### Current Behaviour?\r\n\r\nI am following the tutorial at https:\/\/www.tensorflow.org\/tutorials\/quickstart\/beginner\r\n\r\nI modified the code according to the instructions at https:\/\/www.tensorflow.org\/tensorboard\/tensorboard_profiling_keras in order to enable profiling for a range of batches during training.\r\n\r\nWith this change, training seems to proceed as normal, with the logs indicating that a profiler session is created, and a profile is collected.\r\n\r\nThe logs directory contains one non-empty `plugins\/profile\/<date>\/<host>.xplane.pb` file.\r\n\r\nBut when I run tensorboard (either main or tb-nightly) on the logs, it fails to detect a profile (the Profile tab is missing from the UI). I also confirm I ran `pip install -U tensorboard-plugin-profile` first.\r\n\r\nI would have expected one of these two outcomes: either (a) tensorboard would show me the profiles, or (b) if something went wrong either when collecting or displaying the profiles, an error message would have indicated it so I can fix the issue.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\n# The code is at https:\/\/www.tensorflow.org\/tutorials\/quickstart\/beginner\r\n# I change the model.fit() call to use the Tensorboard callback to collect a profile:\r\n\r\nlog_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\r\n    log_dir=log_dir,\r\n    histogram_freq=1,\r\n    profile_batch=(500, 600))\r\nmodel.fit(x_train, y_train, epochs=5, callbacks=[tensorboard_callback])\r\n```\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2023-04-06 23:17:28.048863: I tensorflow\/tsl\/profiler\/lib\/profiler_session.cc:104] Profiler session initializing.\r\n2023-04-06 23:17:28.048880: I tensorflow\/tsl\/profiler\/lib\/profiler_session.cc:119] Profiler session started.\r\n2023-04-06 23:17:28.048915: I tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:1671] Profiler found 1 GPUs\r\n2023-04-06 23:17:28.237604: I tensorflow\/tsl\/profiler\/lib\/profiler_session.cc:131] Profiler session tear down.\r\n2023-04-06 23:17:28.237742: I tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:1805] CUPTI activity buffer flushed\r\nEpoch 1\/5\r\n2023-04-06 23:17:28.747772: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x7f08c0180cf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2023-04-06 23:17:28.747785: I tensorflow\/compiler\/xla\/service\/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\r\n2023-04-06 23:17:28.751189: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2023-04-06 23:17:28.834436: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_dnn.cc:426] Loaded cuDNN version 8600\r\n2023-04-06 23:17:28.868033: I tensorflow\/tsl\/platform\/default\/subprocess.cc:304] Start cannot spawn child process: No such file or directory\r\n2023-04-06 23:17:28.900180: I .\/tensorflow\/compiler\/jit\/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n 522\/1875 [=======>......................] - ETA: 5s - loss: 0.4875 - accuracy: 0.8590\r\n2023-04-06 23:17:30.991051: I tensorflow\/tsl\/profiler\/lib\/profiler_session.cc:104] Profiler session initializing.\r\n2023-04-06 23:17:30.991106: I tensorflow\/tsl\/profiler\/lib\/profiler_session.cc:119] Profiler session started.\r\n 645\/1875 [=========>....................] - ETA: 4s - loss: 0.4499 - accuracy: 0.8701\r\n2023-04-06 23:17:31.542500: I tensorflow\/tsl\/profiler\/lib\/profiler_session.cc:70] Profiler session collecting data.\r\n2023-04-06 23:17:31.545123: I tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_tracer.cc:1805] CUPTI activity buffer flushed\r\n2023-04-06 23:17:31.570874: I tensorflow\/compiler\/xla\/backends\/profiler\/gpu\/cupti_collector.cc:541]  GpuTracer has collected 6158 callback api events and 5891 activity events. \r\n2023-04-06 23:17:31.598454: I tensorflow\/tsl\/profiler\/lib\/profiler_session.cc:131] Profiler session tear down.\r\n1875\/1875 [==============================] - 8s 4ms\/step - loss: 0.3017 - accuracy: 0.9121\r\nEpoch 2\/5\r\n1875\/1875 [==============================] - 7s 4ms\/step - loss: 0.1441 - accuracy: 0.9570\r\nEpoch 3\/5\r\n1875\/1875 [==============================] - 7s 4ms\/step - loss: 0.1075 - accuracy: 0.9685\r\nEpoch 4\/5\r\n1875\/1875 [==============================] - 6s 3ms\/step - loss: 0.0878 - accuracy: 0.9732\r\nEpoch 5\/5\r\n1875\/1875 [==============================] - 6s 3ms\/step - loss: 0.0737 - accuracy: 0.9771\r\n```\r\n<\/details>","labels":["comp:tensorboard","stat:awaiting response","type:bug","TF 2.12"],"created_at":"2023-04-07T03:33:16Z","comments":6,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60262"},{"issue_number":403,"repository":"tensorflow\/tensorflow","title":"Use after free in propagator_state.cc","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\ntf2.12\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\nPointer `next_iter` from function `PropagatorState::FrameState::IncrementIteration` is passed \r\nas the 1st parameter into `ActivateLoopInvs` where it is passed as the 1st parameter into \r\n`AdjustOutstandingOpsLocked`, then inside this function it is passed into `CleanupIterations` \r\nwhere it is deleted.\r\n\r\nThen in `PropagatorState::FrameState::IncrementIteration` this possibly freed pointer is used \r\nin `return`-statement.\r\n\r\nThis behavior was introduced by https:\/\/github.com\/tensorflow\/tensorflow\/commit\/ae2a0e5c473f2a575767262021c26852d22886f8. \r\nBefore this commit, no `return` was performed on the possibly freed pointer.\r\n```\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nBug was found by Svace static analysis tool.\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_<\/details>","labels":["stat:awaiting response","type:bug","comp:runtime","TF 2.12"],"created_at":"2023-04-06T11:55:35Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60255"},{"issue_number":404,"repository":"tensorflow\/tensorflow","title":"In tf.data.experimental.enable_debug_mode, tf.data.Dataset.ragged_batch fails with an error","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\nTF 2.12.0, TF nightly 2.13.0-dev20230404\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nConsider the following code creating ragged batches using `tf.data.Dataset.ragged_batch`:\r\n\r\n```python\r\ndata = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2], [3]]))\r\nlist(data.ragged_batch(2))\r\n```\r\n\r\nThe above code works fine in normal mode. However, if you enable debug mode using `tf.data.experimental.enable_debug_mode()`, the same code crashes with an error.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nI reproduced the error in https:\/\/colab.research.google.com\/drive\/1nf1BHjssx2YhF0ZbbgPg1QSALSS4Z89r?usp=sharing , both for TF 2.12.0 and TF nightly 2.13.0-dev20230404.\r\n\r\nThe code for triggering the bug is the following:\r\n\r\n```python\r\ntf.data.experimental.enable_debug_mode()\r\ndata = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2], [3]]))\r\nlist(data.ragged_batch(2))\r\n```\r\n\r\n### Relevant log output\r\n\r\nHere is the error printed by TF 2.12.0\r\n\r\n```\r\n---------------------------------------------------------------------------\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n\r\n<ipython-input-3-34b7e4bb8c4b> in <cell line: 4>()\r\n      2 tf.data.experimental.enable_debug_mode()\r\n      3 data = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2], [3]]))\r\n----> 4 list(data.ragged_batch(2))\r\n\r\n3 frames\r\n\r\n\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/framework\/ops.py in raise_from_not_ok_status(e, name)\r\n   6651 def raise_from_not_ok_status(e, name):\r\n   6652   e.message += (\" name: \" + str(name if name is not None else \"\"))\r\n-> 6653   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n   6654 \r\n   6655 \r\n\r\nInvalidArgumentError: {{function_node __wrapped__IteratorGetNext_output_types_1_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} ValueError: Value [1 2] is not convertible to a tensor with dtype <dtype: 'variant'> and shape ().\r\nTraceback (most recent call last):\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/data\/util\/structure.py\", line 347, in reduce_fn\r\n    component = ops.convert_to_tensor(component, spec.dtype)\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/profiler\/trace.py\", line 183, in wrapped\r\n    return func(*args, **kwargs)\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/framework\/ops.py\", line 1440, in convert_to_tensor\r\n    return tensor_conversion_registry.convert(\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/framework\/tensor_conversion_registry.py\", line 209, in convert\r\n    return overload(dtype, name)  #  pylint: disable=not-callable\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/framework\/ops.py\", line 1335, in __tf_tensor__\r\n    return super().__tf_tensor__(dtype, name)\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/framework\/ops.py\", line 967, in __tf_tensor__\r\n    raise ValueError(\r\n\r\nValueError: Tensor conversion requested dtype variant for Tensor with dtype int32: <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\r\nTraceback (most recent call last):\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/ops\/script_ops.py\", line 266, in __call__\r\n    return func(device, token, args)\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/ops\/script_ops.py\", line 144, in __call__\r\n    outputs = self._call(device, args)\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/ops\/script_ops.py\", line 151, in _call\r\n    ret = self._func(*args)\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 643, in wrapper\r\n    return func(*args, **kwargs)\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/data\/ops\/structured_function.py\", line 213, in py_function_wrapper\r\n    ret = structure.to_tensor_list(self._output_structure, ret)\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/data\/util\/structure.py\", line 410, in to_tensor_list\r\n    return _to_tensor_list_helper(\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/data\/util\/structure.py\", line 360, in _to_tensor_list_helper\r\n    return functools.reduce(\r\n\r\n  File \"\/usr\/local\/lib\/python3.9\/dist-packages\/tensorflow\/python\/data\/util\/structure.py\", line 349, in reduce_fn\r\n    raise ValueError(\r\n\r\nValueError: Value [1 2] is not convertible to a tensor with dtype <dtype: 'variant'> and shape ().\r\n\r\n\r\n\t [[{{node EagerPyFunc}}]] [Op:IteratorGetNext] name:\r\n```\r\n<\/details>","labels":["stat:awaiting response","type:bug","comp:data","TF 2.12"],"created_at":"2023-04-05T10:26:05Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60239"},{"issue_number":405,"repository":"tensorflow\/tensorflow","title":"Distributed training using multiple GPUs hangs when enabling eager execution","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\ntf 2.11.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nLinux Ubuntu 20.04.5 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.10 (docker tensorflow:devel-gpu)\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n11.7\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nAfter adding the following code to enable eager execution:\r\n`\r\ntf.config.run_functions_eagerly(True)\r\ntf.data.experimental.enable_debug_mode()\r\n`\r\n\r\nDistributed training using MirroredStrategy with more than 2 GPUs hangs forever.\r\n\r\nUsing 1 GPU does not trigger the bug. Using MirroredStrategy with multiple logical CPUs also does not trigger the bug.\r\n\r\nWhether adding the second line (`tf.data.experimental.enable_debug_mode()`) or not does not matter.\r\n\r\nThe model we used is a simple Sequential model trained on CIFAR-10 dataset.\r\nThe model structure is given as follows:\r\n`\r\nInput - GlobalMaxPool2D - BatchNormalization- Dense\r\n`\r\n\r\nCurrent behavior:\r\nUsing tf 2.11.0, the program hangs at the line of code:\r\n`loss = model.fit(train_input, train_label)`\r\nAlso, the program cannot be stopped gracefully by keyboard interruption.\r\n\r\nUsing tf nightly, the program reports an error (see below).\r\n\r\nExpected behavior:\r\nThe program should print the values of loss as an eager tensor.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nThe code is also available at https:\/\/colab.research.google.com\/drive\/1BiAxZEO9IGak_4XOeZcyXRhJEByZ-X6B?usp=sharing\r\n\r\nimport keras\r\nimport tensorflow as tf\r\n\r\ntf.config.run_functions_eagerly(True)\r\ntf.data.experimental.enable_debug_mode()\r\n\r\nif __name__ == \"__main__\":\r\n    # Using at least 2 GPU triggers the bug\r\n    strategy = tf.distribute.MirroredStrategy([\"\/GPU:0\", \"\/GPU:1\"])\r\n\r\n    # Training data, batch_size=240\r\n    train_input = tf.random.uniform(shape=(240, 32, 32, 3))\r\n    train_label = tf.random.uniform(shape=(240, 10), minval=0, maxval=2, dtype=tf.int32)\r\n\r\n    with strategy.scope():\r\n        model = keras.Sequential([\r\n            keras.layers.Input(shape=(32, 32, 3)),\r\n            keras.layers.GlobalMaxPool2D(),\r\n            keras.layers.BatchNormalization(axis=-1),\r\n            keras.layers.Dense(10, ), ])\r\n        optimizer = keras.optimizers.Adam(learning_rate=0.1)\r\n        loss = tf.keras.losses.MeanSquaredError(\r\n            reduction=tf.keras.losses.Reduction.NONE)\r\n        model.compile(optimizer=optimizer, loss=loss)\r\n\r\n    # One step training\r\n    loss = model.fit(train_input, train_label)\r\n    print(loss)\n```\n\n\n### Relevant log output\n\n```shell\n-------------- Output using tf 2.11.0 -----------------------------\r\ntf-docker \/mnt\/src\/reproduce > python .\/reproduce_eager.py \r\n2023-04-05 03:23:45.884503: I tensorflow\/core\/platform\/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-04-05 03:23:48.001421: I tensorflow\/core\/platform\/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-04-05 03:23:53.074754: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1613] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 9636 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5\r\n2023-04-05 03:23:53.076188: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1613] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 9636 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3f:00.0, compute capability: 7.5\r\n2023-04-05 03:23:53.077442: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1613] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:2 with 9636 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:40:00.0, compute capability: 7.5\r\n2023-04-05 03:23:53.078666: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1613] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:3 with 9636 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\r\n2023-04-05 03:23:53.079887: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1613] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:4 with 9636 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:61:00.0, compute capability: 7.5\r\n2023-04-05 03:23:53.081109: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1613] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:5 with 9636 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:62:00.0, compute capability: 7.5\r\n2023-04-05 03:23:53.082252: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1613] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:6 with 9636 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:63:00.0, compute capability: 7.5\r\n2023-04-05 03:23:53.083395: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1613] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:7 with 9636 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:64:00.0, compute capability: 7.5\r\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\r\n(program hangs here)\r\n--------------------------------------------------------------------\r\n\r\n-------------- Output using tf nightly -----------------------------\r\ntf-docker \/mnt\/src\/reproduce > python .\/reproduce_eager.py \r\n2023-04-05 03:29:24.607254: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-04-05 03:29:25.559092: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2023-04-05 03:29:32.898888: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1635] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 9598 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5\r\n2023-04-05 03:29:32.900404: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1635] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 9598 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3f:00.0, compute capability: 7.5\r\n2023-04-05 03:29:32.901638: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1635] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:2 with 9598 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:40:00.0, compute capability: 7.5\r\n2023-04-05 03:29:32.902870: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1635] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:3 with 9598 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5\r\n2023-04-05 03:29:32.904072: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1635] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:4 with 9598 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:61:00.0, compute capability: 7.5\r\n2023-04-05 03:29:32.905328: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1635] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:5 with 9598 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:62:00.0, compute capability: 7.5\r\n2023-04-05 03:29:32.906501: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1635] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:6 with 9598 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:63:00.0, compute capability: 7.5\r\n2023-04-05 03:29:32.907653: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1635] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:7 with 9598 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:64:00.0, compute capability: 7.5\r\n2023-04-05 03:29:33.578635: I tensorflow\/core\/common_runtime\/executor.cc:1210] [\/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder\/_9' with dtype int32 and shape [240,10]\r\n         [[{{node Placeholder\/_9}}]]\r\n2023-04-05 03:29:33.578971: I tensorflow\/core\/common_runtime\/executor.cc:1210] [\/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder\/_9' with dtype int32 and shape [240,10]\r\n         [[{{node Placeholder\/_9}}]]\r\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\r\n2023-04-05 03:29:35.301668: E tensorflow\/core\/common_runtime\/base_collective_executor.cc:249] BaseCollectiveExecutor::StartAbort UNKNOWN: Error invoking NCCL: unhandled cuda error\r\nTraceback (most recent call last):\r\n  File \".\/reproduce_eager.py\", line 27, in <module>\r\n    loss = model.fit(train_input, train_label)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 70, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/usr\/lib\/python3.8\/multiprocessing\/pool.py\", line 364, in map\r\n    return self._map_async(func, iterable, mapstar, chunksize).get()\r\n  File \"\/usr\/lib\/python3.8\/multiprocessing\/pool.py\", line 771, in get\r\n    raise self._value\r\n  File \"\/usr\/lib\/python3.8\/multiprocessing\/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"\/usr\/lib\/python3.8\/multiprocessing\/pool.py\", line 48, in mapstar\r\n    return list(map(*args))\r\ntensorflow.python.framework.errors_impl.UnknownError: {{function_node __wrapped__CollectiveReduceV2_Nordering_token_1_device_\/job:localhost\/replica:0\/task:0\/device:GPU:1}} Collective ops is aborted by: Error invoking NCCL: unhandled cuda error\r\nThe error could be from a previous operation. Restart your program to reset. [Op:CollectiveReduceV2] name:\r\n--------------------------------------------------------------------\n```\n<\/details>","labels":["stat:awaiting response","type:bug","comp:gpu","TF 2.11"],"created_at":"2023-04-05T03:33:55Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60236"},{"issue_number":406,"repository":"tensorflow\/tensorflow","title":"A check fail can be triggered in ThreadUnsafeUnigramCandidateSampler","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.13.0-dev20230331\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nCUDA 11.8\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nThe following code can trigger a crash in `tf.raw_ops.ThreadUnsafeUnigramCandidateSampler` due to check-fail in the latest version of TensorFlow.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nwith tf.device(\"GPU:0\"):\r\n    num_true = 11\r\n    num_sampled = 2\r\n    unique = False\r\n    range_max = 7612169259283414040\r\n    seed = -111\r\n    seed2 = -11\r\n    true_classes = tf.saturate_cast(tf.random.uniform([12, 11], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int64)\r\n    res = tf.raw_ops.ThreadUnsafeUnigramCandidateSampler(\r\n        num_true=num_true,\r\n        num_sampled=num_sampled,\r\n        unique=unique,\r\n        range_max=range_max,\r\n        seed=seed,\r\n        seed2=seed2,\r\n        true_classes=true_classes,\r\n    )\n```\n\n\n### Relevant log output\n\n```shell\n2023-04-01 16:33:33.009606: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-04-01 16:33:33.057487: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-04-01 16:33:33.874853: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2023-04-01 16:33:35.397082: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1635] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\r\n2023-04-01 16:33:40.359234: F tensorflow\/core\/kernels\/range_sampler.cc:183] Check failed: range < kint32max (7612169259283414040 vs. 2147483647)\r\nAborted (core dumped)\n```\n<\/details>","labels":["awaiting review","type:bug","comp:ops","TF 2.12"],"created_at":"2023-04-01T08:36:16Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60200"},{"issue_number":407,"repository":"tensorflow\/tensorflow","title":"A check fail can be triggered in LearnedUnigramCandidateSampler","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.13.0-dev20230331\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nCUDA 11.8\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nThe following code can trigger a crash in `tf.raw_ops.LearnedUnigramCandidateSampler` due to check-fail in the latest version of TensorFlow.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nwith tf.device(\"GPU:0\"):\r\n    num_true = 13\r\n    num_sampled = 48\r\n    unique = True\r\n    range_max = 3031324185113192368\r\n    seed = 93\r\n    seed2 = 11\r\n    true_classes = tf.saturate_cast(tf.random.uniform([14, 13], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int64)\r\n    res = tf.raw_ops.LearnedUnigramCandidateSampler(\r\n        num_true=num_true,\r\n        num_sampled=num_sampled,\r\n        unique=unique,\r\n        range_max=range_max,\r\n        seed=seed,\r\n        seed2=seed2,\r\n        true_classes=true_classes,\r\n    )\n```\n\n\n### Relevant log output\n\n```shell\n2023-04-01 16:20:32.160750: I tensorflow\/core\/util\/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2023-04-01 16:20:32.211959: I tensorflow\/core\/platform\/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2023-04-01 16:20:33.026789: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2023-04-01 16:20:34.550122: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1635] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0\r\n2023-04-01 16:20:39.689581: F tensorflow\/core\/kernels\/range_sampler.cc:183] Check failed: range < kint32max (3031324185113192368 vs. 2147483647)\r\nAborted (core dumped)\n```\n<\/details>","labels":["stat:awaiting response","type:bug","comp:ops","TF 2.12"],"created_at":"2023-04-01T08:23:48Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60197"},{"issue_number":408,"repository":"tensorflow\/tensorflow","title":"tf.data.Dataset.from_generator crashes with abortion","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.13.0-dev20230208\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\ntf.data.Dataset.from_generator crashes with abortion\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\nds = tf.data.Dataset.from_tensors([1]).repeat(-1)\r\ndef gen():\r\n  for _ in ds:\r\n    yield _\r\nds = tf.data.Dataset.from_generator(\r\n    gen, output_types=tf.int32)\r\nlist(ds.take(2).as_numpy_iterator())\n```\n\n\n### Relevant log output\n\n```shell\n2023-03-28 12:06:28.440209: F tensorflow\/tsl\/platform\/default\/env.cc:74] Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed.\r\nAborted (core dumped)\n```\n<\/details>","labels":["stat:awaiting response","type:bug","comp:data","TF 2.12"],"created_at":"2023-03-28T17:06:53Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60149"},{"issue_number":409,"repository":"tensorflow\/tensorflow","title":"Importing TF 2.12, then torch, hangs, but not the other way around","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.12.0\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nLinux Ubuntu 20.04.5\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n11.8\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nIf I import tensorflow and then import torch, the torch import line hangs forever without completing. On the other hand, if I import torch first and then import tensorflow there is no problem.\r\n\r\nThe hang is so severe that no amount of ctr-c can kill it. You have to kill the python process from a separate terminal to free the hung terminal. \r\n\r\nThis issue does not exist in tensorflow 2.11.1 or earlier. It also doesn't happen when using older versions of torch like 1.13.1. Since torch followed by tf works but tf followed by torch doesn't, this seems like an issue tf is causing.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\ndocker pull tensorflow\/tensorflow:2.12.0-gpu\r\ndocker run -it tensorflow\/tensorflow:2.12.0-gpu\r\n\r\npip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1+cu118 -f https:\/\/download.pytorch.org\/whl\/cu118\/torch_stable.html\r\n\r\npython\r\n\r\nimport tensorflow as tf\r\nimport torch\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting response","type:bug","type:build\/install","subtype: ubuntu\/linux","TF 2.12"],"created_at":"2023-03-24T23:56:20Z","comments":16,"reactions":6,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60109"},{"issue_number":410,"repository":"tensorflow\/tensorflow","title":"Linking external\/org_tensorflow\/tensorflow\/libtensorflow_cc.so.2.13.0 failed: (Exit 1): gcc failed: error executing command ","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\nHead\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n[Dynamic Pywrap PR 58734](https:\/\/github.com\/tensorflow\/tensorflow\/pull\/58734) seems to introduce a a TensorFlow Serving linking error.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nRun\r\n```shell\r\n$ git clone git@github.com:tensorflow\/serving.git\r\n$ cd serving\r\n$ sudo .\/tools\/run_in_docker.sh bazel test --action_env=TF_REVISION=6147c03eb9af1e5d2ae155045b33e909ef96944e tensorflow_serving\/... &> \/tmp\/tfs_bazel_output.log\r\n```\r\nand search `Linking external` in \/tmp\/tfs_bazel_output.log\r\n\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n\/usr\/local\/google\/home\/rostam\/Workspace\/tmp\/serving\/.cache\/_bazel_root\/7c4195127656842bcf1efda48d8fd065\/external\/org_tensorflow\/tensorflow\/BUILD:1214:21: Linking external\/org_tensorflow\/tensorflow\/libtensorflow_cc.so.2.12.0 failed: (Exit 1): gcc failed: error executing command \r\n  (cd \/usr\/local\/google\/home\/rostam\/Workspace\/tmp\/serving\/.cache\/_bazel_root\/7c4195127656842bcf1efda48d8fd065\/execroot\/tf_serving && \\\r\n  exec env - \\\r\n    PATH=\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin \\\r\n    PWD=\/proc\/self\/cwd \\\r\n    TF_REVISION=6147c03eb9af1e5d2ae155045b33e909ef96944e \\\r\n  \/usr\/bin\/gcc @bazel-out\/k8-opt\/bin\/external\/org_tensorflow\/tensorflow\/libtensorflow_cc.so.2.12.0-2.params)\r\n\r\n```","labels":["stat:awaiting response","type:bug","type:build\/install","subtype: ubuntu\/linux","TF 2.12"],"created_at":"2023-03-22T02:34:16Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/60061"},{"issue_number":411,"repository":"tensorflow\/tensorflow","title":"Missing Window GPU prebuilt binary for 2.11.0","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\ntf 2.11\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nWindows\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nhttps:\/\/storage.googleapis.com\/tensorflow\/libtensorflow\/libtensorflow-gpu-windows-x86_64-2.11.0.zip is missing, which is breaking TensorFlow Rust for Windows with GPU: https:\/\/github.com\/tensorflow\/rust\/issues\/400\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nN\/A\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting response","type:bug","type:build\/install","subtype:windows","TF 2.11"],"created_at":"2023-02-28T03:46:40Z","comments":10,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/59828"},{"issue_number":412,"repository":"tensorflow\/tensorflow","title":"XLA inference fails complaning about branch shape mismatches","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.10.0\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nColab\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nA100 40 GB\n\n### Current Behaviour?\n\n```shell\nI have the following function. \r\n\r\nFirst, I initialize a CLIP-based text encoder:\r\n\r\n\r\nfrom keras_cv.models.stable_diffusion.text_encoder import TextEncoder\r\n\r\nMAX_PROMPT_LENGTH = 77\r\ntext_encoder = TextEncoder(MAX_PROMPT_LENGTH)\r\n\r\n\r\nThen, I am using the `text_encoder` like so in a function that I use to serialize the `text_encoder` as a `SavedModel`:\r\n\r\n```python\r\nfrom keras_cv.models.stable_diffusion.constants import _UNCONDITIONAL_TOKENS\r\nimport tensorflow as tf \r\n\r\nsignature_dict = {\r\n    \"tokens\": tf.TensorSpec(shape=[None, 77], dtype=tf.int32, name=\"tokens\"),\r\n}\r\n\r\ndef text_encoder_exporter(model: tf.keras.Model):\r\n    BATCH_SIZE = 3\r\n    MAX_PROMPT_LENGTH = 77\r\n    POS_IDS = tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)\r\n    UNCONDITIONAL_TOKENS = tf.convert_to_tensor([_UNCONDITIONAL_TOKENS], dtype=tf.int32)\r\n\r\n    @tf.function(input_signature=[signature_dict])\r\n    def serving_fn(inputs):\r\n        # context\r\n        encoded_text = model([inputs[\"tokens\"], POS_IDS], training=False)\r\n        encoded_text = tf.squeeze(encoded_text)\r\n\r\n        if tf.rank(encoded_text) == 2:\r\n            encoded_text = tf.repeat(\r\n                tf.expand_dims(encoded_text, axis=0), BATCH_SIZE, axis=0\r\n            )\r\n\r\n        # unconditional context\r\n        unconditional_context = model([UNCONDITIONAL_TOKENS, POS_IDS], training=False)\r\n\r\n        unconditional_context = tf.repeat(unconditional_context, BATCH_SIZE, axis=0)\r\n        return {\"context\": encoded_text, \"unconditional_context\": unconditional_context}\r\n\r\n    return serving_fn\r\n```\r\n\r\nSerialization:\r\n\r\n```python\r\ntf.saved_model.save(\r\n    text_encoder,\r\n    \".\/text_encoder\/1\/\",\r\n    signatures={\"serving_default\": text_encoder_exporter(text_encoder)},\r\n)\r\n```\r\n\r\nNow, while attempting to XLA-compile:\r\n\r\n```python\r\nfrom tensorflow.python.saved_model import tag_constants\r\n\r\nbatch_size = 3\r\nsaved_model_loaded = tf.saved_model.load(\r\n    \".\/text_encoder\/1\/\", tags=[tag_constants.SERVING]\r\n)\r\ntext_encoder_predict_fn = saved_model_loaded.signatures[\"serving_default\"]\r\n# Raises error\r\nxla_text_encoder_predict_fn = tf.function(text_encoder_predict_fn, jit_compile=True)\r\nxla_text_encoder_predict_fn(\r\n    tokens=tf.ones((batch_size, MAX_PROMPT_LENGTH), tf.int32)\r\n).keys()\r\n```\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/gist\/sayakpaul\/d7dafc252752a6c1ce10e85d8162b8ea\/scratchpad.ipynb\n```\n\n\n### Relevant log output\n\n```shell\n\/usr\/local\/lib\/python3.8\/dist-packages\/tensorflow\/python\/util\/traceback_utils.py in error_handler(*args, **kwargs)\r\n    151     except Exception as e:\r\n    152       filtered_tb = _process_traceback_frames(e.__traceback__)\r\n--> 153       raise e.with_traceback(filtered_tb) from None\r\n    154     finally:\r\n    155       del filtered_tb\r\n\r\n\/usr\/local\/lib\/python3.8\/dist-packages\/tensorflow\/python\/eager\/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     50   try:\r\n     51     ctx.ensure_initialized()\r\n---> 52     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     53                                         inputs, attrs, num_outputs)\r\n     54   except core._NotOkStatusException as e:\r\n\r\nInvalidArgumentError: left_branch_shape.rank() != right_branch_shape.rank() (4 vs 3)\r\n\t [[{{function_node __inference_serving_fn_55983}}{{node cond}}]] [Op:__inference_function_60000]\n```\n<\/details>","labels":["stat:awaiting response","type:bug","comp:keras","comp:xla","TF 2.10"],"created_at":"2023-02-27T05:43:50Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/59818"},{"issue_number":413,"repository":"tensorflow\/tensorflow","title":"AutoGraph did convert this function: NameError: name 'Tuple' is not defined","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TF nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.11.0\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nMac, Linux, Google Colab\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n<\/details>\r\n\r\n### Current Behaviour?\r\n\r\n`tf.function` fails when type annotations are used which are locally imported. I get:\r\n\r\n```\r\nCause: name 'Tuple' is not defined\r\n```\r\n\r\nNote:\r\n\r\n* It seems important that `Tuple` is locally imported. If it is imported globally in the module, there does not seem to be a problem.\r\n* When I use `from __future__ import annotations`, there is also no error. But I assume because this will just not evaluate it directly, but it still lacks the `Tuple` reference, although it's maybe really not relevant then.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nprint(\"TensorFlow:\", tf.__version__)\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nsession = tf.compat.v1.Session()\r\n\r\n\r\n# https:\/\/www.tensorflow.org\/guide\/function\r\n\r\n\r\ndef f(x: tf.Tensor):\r\n  from typing import Tuple\r\n\r\n  @tf.function\r\n  def local_func(x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\r\n    while tf.reduce_sum(x) > 1:\r\n      tf.print(x)\r\n      x = tf.tanh(x)\r\n    return x, x\r\n\r\n  return local_func(x)\r\n\r\n\r\nsession.run(f(tf.random.uniform([5])))\r\n```\r\n\r\n\r\nColab link: https:\/\/colab.research.google.com\/drive\/1K69XH_RsU-Ux-RBfUd0B4eR8iJFTXoFM?usp=sharing\r\n\r\n### Relevant log output\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 427, in converted_call\r\n    converted_f = _convert_actual(target_entity, program_ctx)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 269, in _convert_actual\r\n    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/tensorflow\/python\/autograph\/pyct\/transpiler.py\", line 282, in transform\r\n    return self.transform_function(obj, user_context)\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/tensorflow\/python\/autograph\/pyct\/transpiler.py\", line 490, in transform_function\r\n    transformed_fn = factory.instantiate(\r\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/tensorflow\/python\/autograph\/pyct\/transpiler.py\", line 213, in instantiate\r\n    new_fn = bound_factory(**self._extra_locals)  # pylint:disable=not-callable\r\n  File \"\/tmp\/__autograph_generated_filem5lq6_hq.py\", line 6, in inner_factory\r\n    def tf__local_func(x: tf.Tensor) -> Tuple[(tf.Tensor, tf.Tensor)]:\r\nNameError: name 'Tuple' is not defined\r\nWARNING:tensorflow:AutoGraph could not transform <function f.<locals>.local_func at 0x7f15f5ff3af0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: name 'Tuple' is not defined\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\nTensorFlow: 2.11.0\r\nConverted call: <function f.<locals>.local_func at 0x7f15f5ff3af0>\r\n    args: (<tf.Tensor 'x:0' shape=(5,) dtype=float32>,)\r\n    kwargs: {}\r\n\r\n<function f.<locals>.local_func at 0x7f15f5ff3af0> is not cached for subkey ConversionOptions[{}]\r\nSource code of <function f.<locals>.local_func at 0x7f15f5ff3af0>:\r\n\r\n@tf.function\r\ndef local_func(x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\r\n  while tf.reduce_sum(x) > 1:\r\n    tf.print(x)\r\n    x = tf.tanh(x)\r\n  return x, x\r\n\r\n\r\nTransformed <function f.<locals>.local_func at 0x7f15f5ff3af0>:\r\n\r\n# coding=utf-8\r\ndef tf__local_func(x: tf.Tensor) -> Tuple[(tf.Tensor, tf.Tensor)]:\r\n    with ag__.FunctionScope('local_func', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\r\n        do_return = False\r\n        retval_ = ag__.UndefinedReturnValue()\r\n\r\n        def get_state():\r\n            return (x,)\r\n\r\n        def set_state(vars_):\r\n            nonlocal x\r\n            (x,) = vars_\r\n\r\n        def loop_body():\r\n            nonlocal x\r\n            ag__.converted_call(ag__.ld(tf).print, (ag__.ld(x),), None, fscope)\r\n            x = ag__.converted_call(ag__.ld(tf).tanh, (ag__.ld(x),), None, fscope)\r\n\r\n        def loop_test():\r\n            return (ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.ld(x),), None, fscope) > 1)\r\n        ag__.while_stmt(loop_test, loop_body, get_state, set_state, ('x',), {})\r\n        try:\r\n            do_return = True\r\n            retval_ = (ag__.ld(x), ag__.ld(x))\r\n        except:\r\n            do_return = False\r\n            raise\r\n        return fscope.ret(retval_, do_return)\r\n\r\nError transforming entity <function f.<locals>.local_func at 0x7f15f5ff3af0>\r\nWARNING: AutoGraph could not transform <function f.<locals>.local_func at 0x7f15f5ff3af0> and will run it as-is.\r\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\r\nCause: name 'Tuple' is not defined\r\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:tf.function","TF 2.11"],"created_at":"2023-02-24T14:12:46Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/59796"},{"issue_number":414,"repository":"tensorflow\/tensorflow","title":"Status of int8 dot\/conv with XLA+CUDA","description":"Are tensorcore-accelerated int8 dot\/convs with XLA accessible via tensorflow APIs? https:\/\/github.com\/tensorflow\/tensorflow\/pull\/30771 suggests that int8 convs are supported, and https:\/\/github.com\/tensorflow\/tensorflow\/issues\/49140 also suggest that this functionality exists. However, the tensorflow convolution operations don't allow int8 arguments. The matmul operator allows int8 x int8 -> int32 via the output_type argument, and I've successfully compiled with XLA using this option. However, when doing this, I'm not getting the throughput that I'd expect with int8 tensorcores. I was expecting something approaching 2x throughput compared to fp16 for large matmuls ([8192, 8192] inputs). Admittedly my benchmarking is not very rigorous, but I'd be curious to know if my expectation of ~2x throughput is reasonable and whether XLA has the functionality to facilitate it.\r\n\r\nHere's a minimal implementation of an XLA-compiled int8 matmul:\r\n```python\r\n@tf.function(jit_compile=True)\r\ndef int8_matmul(x, w):\r\n    return tf.matmul(x, w, output_type=tf.int32)\r\n\r\n\r\nx = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)\r\nw = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)\r\ny = int8_matmul(x, w)\r\n\r\nprint(int8_matmul.experimental_get_compiler_ir(x, w)(stage=\"optimized_hlo\"))\r\n# HloModule a_inference_int8_matmul_17__.10, alias_passthrough_params=true, entry_computation_layout={(s8[2048,2048]{1,0},s8[2048,2048]{1,0})->s32[2048,2048]{1,0}}\r\n\r\n# ENTRY %a_inference_int8_matmul_17__.10 (arg0.1: s8[2048,2048], arg1.2: s8[2048,2048]) -> s32[2048,2048] {\r\n#   %arg0.1 = s8[2048,2048]{1,0} parameter(0), parameter_replication={false}, metadata={op_name=\"XLA_Args\"}\r\n#   %arg1.2 = s8[2048,2048]{1,0} parameter(1), parameter_replication={false}, metadata={op_name=\"XLA_Args\"}\r\n#   %copy = s8[2048,2048]{0,1} copy(s8[2048,2048]{1,0} %arg1.2), metadata={op_name=\"XLA_Args\"}\r\n#   ROOT %cublas-gemm.1 = s32[2048,2048]{1,0} custom-call(s8[2048,2048]{1,0} %arg0.1, s8[2048,2048]{0,1} %copy), custom_call_target=\"__cublas$gemm\", metadata={op_type=\"BatchMatMulV3\" op_name=\"MatMul\" source_file=\"int8_xla.py\" source_line=22}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\"\r\n# }\r\n```\r\n\r\nAnd here's my attempt at implementing a 'canonical' quantized linear layer.  I think this is a fairly standard sequence of operations for a quantized layer, and I believe TensorRT would be able to fuse this entire operation.\r\n```python\r\n@tf.function(jit_compile=True)\r\ndef int8_linear_layer(x, w, b, s):\r\n    # read in x and w as pre-quantized int8 tensors\r\n    y = tf.matmul(x, w, output_type=tf.int32)\r\n\r\n    # add bias and apply activation in fp32\r\n    y = tf.cast(y, tf.float32)\r\n    y = y + b\r\n    y = tf.nn.relu(y)\r\n\r\n    # quantize and store output as int8\r\n    y = tf.round(y \/ s)\r\n    y = tf.clip_by_value(y, -128, 127)\r\n    y = tf.cast(y, tf.int8)\r\n    return y\r\n\r\n\r\nx = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)\r\nw = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)\r\nb = tf.random.normal([2048], dtype=tf.float32)  # bias\r\ns = tf.random.normal([], dtype=tf.float32)  # per-tensor quantization scale for output activation\r\ny = int8_linear_layer(x, w, b, s)\r\n\r\nprint(int8_linear_layer.experimental_get_compiler_ir(x, w, b, s)(stage=\"optimized_hlo\"))\r\n# HloModule a_inference_int8_linear_layer_65__.32, alias_passthrough_params=true, entry_computation_layout={(s8[2048,2048]{1,0},s8[2048,2048]{1,0},f32[2048]{0},f32[])->s8[2048,2048]{1,0}}\r\n\r\n# %fused_computation (param_0.2: f32[], param_1.4: f32[2048], param_2.7: s32[2048,2048]) -> s8[2048,2048] {\r\n#   %constant_2 = f32[] constant(-128), metadata={op_type=\"Maximum\" op_name=\"clip_by_value\" source_file=\"int8_xla.py\" source_line=52}\r\n#   %broadcast.4 = f32[2048,2048]{1,0} broadcast(f32[] %constant_2), dimensions={}, metadata={op_type=\"Maximum\" op_name=\"clip_by_value\" source_file=\"int8_xla.py\" source_line=52}\r\n#   %param_2.7 = s32[2048,2048]{1,0} parameter(2)\r\n#   %convert.1 = f32[2048,2048]{1,0} convert(s32[2048,2048]{1,0} %param_2.7), metadata={op_type=\"Cast\" op_name=\"Cast\" source_file=\"int8_xla.py\" source_line=46}\r\n#   %param_1.4 = f32[2048]{0} parameter(1)\r\n#   %broadcast.3 = f32[2048,2048]{1,0} broadcast(f32[2048]{0} %param_1.4), dimensions={1}, metadata={op_type=\"AddV2\" op_name=\"add\" source_file=\"int8_xla.py\" source_line=47}\r\n#   %add.0 = f32[2048,2048]{1,0} add(f32[2048,2048]{1,0} %convert.1, f32[2048,2048]{1,0} %broadcast.3), metadata={op_type=\"AddV2\" op_name=\"add\" source_file=\"int8_xla.py\" source_line=47}\r\n#   %constant_1 = f32[] constant(0), metadata={op_type=\"Relu\" op_name=\"Relu\" source_file=\"int8_xla.py\" source_line=48}\r\n#   %broadcast.2 = f32[2048,2048]{1,0} broadcast(f32[] %constant_1), dimensions={}, metadata={op_type=\"Relu\" op_name=\"Relu\"}\r\n#   %maximum.0 = f32[2048,2048]{1,0} maximum(f32[2048,2048]{1,0} %add.0, f32[2048,2048]{1,0} %broadcast.2), metadata={op_type=\"Relu\" op_name=\"Relu\"}\r\n#   %param_0.2 = f32[] parameter(0)\r\n#   %broadcast.1 = f32[2048,2048]{1,0} broadcast(f32[] %param_0.2), dimensions={}, metadata={op_type=\"RealDiv\" op_name=\"truediv\" source_file=\"int8_xla.py\" source_line=51}\r\n#   %divide.0 = f32[2048,2048]{1,0} divide(f32[2048,2048]{1,0} %maximum.0, f32[2048,2048]{1,0} %broadcast.1), metadata={op_type=\"RealDiv\" op_name=\"truediv\" source_file=\"int8_xla.py\" source_line=51}\r\n#   %round-nearest-even.0 = f32[2048,2048]{1,0} round-nearest-even(f32[2048,2048]{1,0} %divide.0), metadata={op_type=\"Round\" op_name=\"Round\" source_file=\"int8_xla.py\" source_line=51}\r\n#   %constant_0 = f32[] constant(127), metadata={op_type=\"Minimum\" op_name=\"clip_by_value\/Minimum\" source_file=\"int8_xla.py\" source_line=52}\r\n#   %broadcast.0 = f32[2048,2048]{1,0} broadcast(f32[] %constant_0), dimensions={}, metadata={op_type=\"Minimum\" op_name=\"clip_by_value\/Minimum\" source_file=\"int8_xla.py\" source_line=52}\r\n#   %clamp.1 = f32[2048,2048]{1,0} clamp(f32[2048,2048]{1,0} %broadcast.4, f32[2048,2048]{1,0} %round-nearest-even.0, f32[2048,2048]{1,0} %broadcast.0), metadata={op_type=\"Maximum\" op_name=\"clip_by_value\" source_file=\"int8_xla.py\" source_line=52}\r\n#   ROOT %convert.0 = s8[2048,2048]{1,0} convert(f32[2048,2048]{1,0} %clamp.1), metadata={op_type=\"Cast\" op_name=\"Cast_1\" source_file=\"int8_xla.py\" source_line=53}\r\n# }\r\n\r\n# ENTRY %a_inference_int8_linear_layer_65__.32 (arg0.1: s8[2048,2048], arg1.2: s8[2048,2048], arg2.3: f32[2048], arg3.4: f32[]) -> s8[2048,2048] {\r\n#   %arg3.4 = f32[] parameter(3), parameter_replication={false}, metadata={op_name=\"XLA_Args\"}\r\n#   %arg2.3 = f32[2048]{0} parameter(2), parameter_replication={false}, metadata={op_name=\"XLA_Args\"}\r\n#   %arg0.1 = s8[2048,2048]{1,0} parameter(0), parameter_replication={false}, metadata={op_name=\"XLA_Args\"}\r\n#   %arg1.2 = s8[2048,2048]{1,0} parameter(1), parameter_replication={false}, metadata={op_name=\"XLA_Args\"}\r\n#   %copy = s8[2048,2048]{0,1} copy(s8[2048,2048]{1,0} %arg1.2), metadata={op_name=\"XLA_Args\"}\r\n#   %cublas-gemm.1 = s32[2048,2048]{1,0} custom-call(s8[2048,2048]{1,0} %arg0.1, s8[2048,2048]{0,1} %copy), custom_call_target=\"__cublas$gemm\", metadata={op_type=\"BatchMatMulV3\" op_name=\"MatMul\" source_file=\"int8_xla.py\" source_line=43}, backend_config=\"{\\\"alpha_real\\\":1,\\\"alpha_imag\\\":0,\\\"beta\\\":0,\\\"dot_dimension_numbers\\\":{\\\"lhs_contracting_dimensions\\\":[\\\"1\\\"],\\\"rhs_contracting_dimensions\\\":[\\\"0\\\"],\\\"lhs_batch_dimensions\\\":[],\\\"rhs_batch_dimensions\\\":[]},\\\"precision_config\\\":{\\\"operand_precision\\\":[\\\"DEFAULT\\\",\\\"DEFAULT\\\"]},\\\"epilogue\\\":\\\"DEFAULT\\\"}\"\r\n#   ROOT %fusion = s8[2048,2048]{1,0} fusion(f32[] %arg3.4, f32[2048]{0} %arg2.3, s32[2048,2048]{1,0} %cublas-gemm.1), kind=kLoop, calls=%fused_computation, metadata={op_type=\"Cast\" op_name=\"Cast_1\" source_file=\"int8_xla.py\" source_line=53}\r\n# }\r\n```\r\n\r\nSystem info: Ubuntu 20.04.5 LTS, TF 2.11.0 via pip, A100 GPU, CUDA Version 12.0","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","comp:xla","type:performance"],"created_at":"2023-02-02T19:47:49Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/59530"},{"issue_number":415,"repository":"tensorflow\/tensorflow","title":"ConvertFusedBatchNorm returns uninitialized value when data_format = \"NDHWC\"","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Have you reproduced the bug with TF nightly?\n\nYes\n\n### Source\n\nsource\n\n### Tensorflow Version\n\nv1.12.1-88697-g620bee79ab3 2.12.0-dev20230201\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython 3.10\n\n### Bazel version\n\n5.3.0\n\n### GCC\/Compiler version\n\ngcc-11\n\n### CUDA\/cuDNN version\n\nCUDA-11.8\/cudnn-8.7.0\/TensorRT-8.5.3\n\n### GPU model and memory\n\nRTX3090\n\n### Current Behaviour?\n\n```shell\nSee code snippet:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/4aec415b3f06b19c380d1a0ca92cc2de0d74cc21\/tensorflow\/compiler\/tf2tensorrt\/convert\/convert_nodes.cc#L4399-L4436\r\n\r\nIn the case of NDHWC layout (triggered by the code below) an uninitialized value is returned from ConvertFusedBatchNorm which causes an exception to be raised.\r\n\r\nI would expect it to build correctly. Changing ConvertFusedBatchNorm to do the same thing for NDHWC as for NHWC gets rid of the crash, but I don't know if this is correct.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import (\r\n    BatchNormalization,\r\n    Conv3D,\r\n    Dense,\r\n    Flatten,\r\n    Input,\r\n)\r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.python.compiler.tensorrt import trt_convert as trt\r\n\r\ninputs = Input(shape=(24, 24, 64, 1), name=\"x\")\r\nx = inputs\r\nx = Conv3D(16, (3, 3, 3), activation=\"relu\", padding=\"same\")(x)\r\nx = BatchNormalization()(x)\r\nx = Flatten()(x)\r\nx = Dense(128, activation=\"relu\")(x)\r\nx = Dense(128)(x)\r\nm = Model(inputs=[inputs], outputs=[x])\r\n\r\nm.compile(\r\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\r\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\r\n)\r\n\r\nmodel_dir = \"\/tmp\/model\"\r\ntf.keras.models.save_model(m, model_dir)\r\n\r\nconverter = trt.TrtGraphConverterV2(input_saved_model_dir=model_dir,\r\n                        precision_mode=trt.TrtPrecisionMode.FP16)\r\n\r\ntrt_func = converter.convert()\r\n\r\ndef input_fn():\r\n    a = np.random.rand(1024, 24, 24, 64, 1).astype(np.float32)\r\n    yield [a]\r\n\r\nconverter.build(input_fn=input_fn)\n```\n\n\n### Relevant log output\n\n```shell\n2023-02-02 11:32:14.336729: W tensorflow\/compiler\/tf2tensorrt\/kernels\/trt_engine_op.cc:1104] TF-TRT Warning: Engine creation for TRTEngineOp_000_000 failed. The native segment will be used instead. Reason: INVALID_ARGUMENT: Rank of perm for transpose does not match with that of the input.\n```\n<\/details>","labels":["stat:awaiting response","type:bug","subtype: ubuntu\/linux","comp:gpu:tensorrt","TF 2.11"],"created_at":"2023-02-02T11:20:23Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/59525"},{"issue_number":416,"repository":"tensorflow\/tensorflow","title":"Add warning in `Dataset.shuffle` for silent data leakage","description":" ### Issue Type\r\n\r\nDocumentation Feature Request\r\n\r\nAdd a pitfall warning in the docs of \"tf.data.Dataset\", where the `shuffle` method using together with `reshuffle_each_iteration=True` (which is **True by default**) might lead to validation data leakage.\r\n\r\n### Tensorflow Version\r\n\r\n2.9, 2.10, 2.11\r\n\r\n### Issue description\r\n\r\nUsing shuffle method with `reshuffle_each_iteration=True`, followed by take\/skip methods to generate train\/test\/validation sets could lead to validation\/test data leaking into the training set (as **the full dataset is shuffled after each epoch**, which undergoes yet another split and therefore the perimeter between training and validation sets is broken).\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\ndataset = tf.data.Dataset.from_tensor_slices((features, labels))\r\ndataset = dataset.shuffle(buffer_size=BUFFER_SIZE)             # \"reshuffle_each_iteration\" is True by default\r\ndataset = dataset.batch(BATCH_SIZE)\r\n\r\ntrain_dataset = dataset.take(TRAIN_BATCH_SIZE)\r\nval_dataset = dataset.skip(TRAIN_BATCH_SIZE)\r\n\r\nmodel.fit(train_dataset, validation_data=val_dataset, batch_size=BATCH_SIZE, epochs=EPOCHS)\r\n```\r\n\r\n### Pitfall analysis\r\n\r\nThis data leakage is **dangerously silent** for the following reason:\r\n\r\nIn the [Dataset doc](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset) under the `shuffle` method, it reads:\r\n\r\n> shuffle(\r\n>     buffer_size, seed=None, reshuffle_each_iteration=None, name=None\r\n> )\r\n\r\nwhere the `reshuffle_each_iteration=None` is **super misleading**, as it be easily misinterpreted that reshuffled is off by default (**which is not true at all**).\r\n\r\nThe shuffle method, which called `shuffle_op._shuffle`:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/4dacf3f368eb7965e9b5c3bbdd5193986081c3b2\/tensorflow\/python\/data\/ops\/dataset_ops.py#L1472-L1473\r\n  \r\nwhich then called `_ShuffleDataset`:\r\n  https:\/\/github.com\/tensorflow\/tensorflow\/blob\/b756c44e3f3ed52ccb4f05736569b95f4481eea0\/tensorflow\/python\/data\/ops\/shuffle_op.py#L25-L32\r\n  \r\nwhich finally init the class `_ShuffleDataset`:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/b756c44e3f3ed52ccb4f05736569b95f4481eea0\/tensorflow\/python\/data\/ops\/shuffle_op.py#L35-L50\r\n\r\nhas the following dangerous definition:\r\n```\r\nif reshuffle_each_iteration is None:\r\n      reshuffle_each_iteration = True\r\n```\r\n\r\nAs a result, the default `reshuffle_each_iteration is None` would be interpreted to `reshuffle_each_iteration = True` (which is truly unexpected).\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.11"],"created_at":"2023-01-17T10:29:54Z","comments":14,"reactions":3,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/59279"},{"issue_number":417,"repository":"tensorflow\/tensorflow","title":"Deleting legacy Java client from TensorFlow main repository","description":"TensorFlow main repository still contains the old [Java client](https:\/\/github.com\/tensorflow\/tensorflow\/tree\/master\/tensorflow\/java) based on TF1.x that has been replaced a few years ago by the [new version](https:\/\/github.com\/tensorflow\/java) maintained by SIG-JVM.\r\n\r\nThis is very misleading for users who wants to discover the capabilities of running TensorFlow models on Java (just this week a new example of such [question](https:\/\/discuss.tensorflow.org\/t\/what-does-it-mean-for-the-java-api-that-warning-this-api-is-deprecated-and-will-be-removed-in-a-future-version-of-tensorflow-after-the-replacement-is-stable\/12757) appeared on the forum).\r\n\r\nThis issue is to start the process of deleting this client for good in TF main repo. We could start by replacing this [README](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/java\/README.md) for simply saying that this client is deprecated and provide links to the new repo. Then we can proceed to the folder deletion, making sure it won't break any code, CI jobs or external scripts (like the documentation one).\r\n\r\nIf need be, we at SIG-JVM can take care of pushing a series of pull requests to achieve this goal. \r\n\r\nCC\\ @bhack , @craigacp","labels":["stat:awaiting tensorflower","type:bug","comp:apis"],"created_at":"2022-11-03T01:16:18Z","comments":16,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/58424"},{"issue_number":418,"repository":"tensorflow\/tensorflow","title":"GPU memory usage depends on data size","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\n2.6-2.10\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux Fedora 36\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n11.7\r\n\r\n### GPU model and memory\r\n\r\nRTX 2070, 8GiB\r\n\r\n### Current Behaviour?\r\n\r\n\r\nTraining a small model on small batches runs out of GPU memory if the total amount of data is large.\r\n\r\nThe same code works fine on TF 2.5, but fails on 2.6, 2.7, 2.8, and 2.9.\r\n\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n\r\nHere is a handy script: https:\/\/gist.github.com\/Dapid\/3f58697edf91c6610ed5e7b681db440f\r\n\r\nIn short, a tiny model is trained on a fixed array of data. If the total amount of data is small, it runs; otherwise, it crashes.\r\n\r\nRunning `python benchmark.py` works, but `python benchmark.py --big` doesn't. Using the `tf.data` API doesn't make a difference.\r\n\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n-09-06 10:52:22.131172: W tensorflow\/core\/common_runtime\/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 15.26GiB (rounded to 16384000000)requested by op _EagerConst\r\n```\r\n\r\nDefining `TF_GPU_ALLOCATOR=cuda_malloc_async`:\r\n\r\n```shell\r\n2022-09-06 10:58:16.939018: W tensorflow\/core\/framework\/cpu_allocator_impl.cc:82] Allocation of 16384000000 exceeds 10% of free system memory.\r\n2022-09-06 10:58:23.313713: E tensorflow\/core\/common_runtime\/gpu\/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 16384000000 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)\r\n Reported by CUDA: Free memory\/Total memory: 1111293952\/8369799168\r\n2022-09-06 10:58:23.313737: E tensorflow\/core\/common_runtime\/gpu\/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                      6184304640\r\nInUse:                        67126312\r\nMaxInUse:                    201327628\r\nNumAllocs:                          13\r\nMaxAllocSize:                 67108864\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2022-09-06 10:58:23.313745: E tensorflow\/core\/common_runtime\/gpu\/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;\r\n2022-09-06 10:58:23.313749: E tensorflow\/core\/common_runtime\/gpu\/gpu_cudamallocasync_allocator.cc:59] 4, 5\r\n2022-09-06 10:58:23.313753: E tensorflow\/core\/common_runtime\/gpu\/gpu_cudamallocasync_allocator.cc:59] 8, 2\r\n2022-09-06 10:58:23.313755: E tensorflow\/core\/common_runtime\/gpu\/gpu_cudamallocasync_allocator.cc:59] 1028, 1\r\n2022-09-06 10:58:23.313758: E tensorflow\/core\/common_runtime\/gpu\/gpu_cudamallocasync_allocator.cc:59] 16384, 1\r\n2022-09-06 10:58:23.313761: E tensorflow\/core\/common_runtime\/gpu\/gpu_cudamallocasync_allocator.cc:59] 67108864, 1\r\n```\r\n\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.9"],"created_at":"2022-09-06T09:00:12Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/57623"},{"issue_number":419,"repository":"tensorflow\/tensorflow","title":"Convert FloorModOp to tosa","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.9.1\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n5.1\n\n### GCC\/Compiler version\n\n9\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nfrom source code: https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/compiler\/mlir\/tosa\/transforms\/legalize_common.cc#L2457\r\nfloormod(x, y) = x \/ y - floor(x \/ y)\r\n\r\nfrom doc:https:\/\/tensorflow.google.cn\/api_docs\/python\/tf\/raw_ops\/FloorMod?version=nightly\r\nfloormod(x, y) = x - floor(x \/ y) * y\r\n\r\nI wonder if I misunderstood\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\ntosa ir I get:\r\n'\r\nmodule attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 561 : i32}, tf_saved_model.semantics} {\r\n  func.func @serving_default(%arg0: tensor<11x10xf32> {tf_saved_model.index_path = [\"model_input1\"]}, %arg1: tensor<f32> {tf_saved_model.index_path = [\"model_input2\"]}) -> (tensor<11x10xf32> {tf_saved_model.index_path = [\"model_output\"]}) attributes {tf.entry_function = {control_outputs = \"\", inputs = \"model_input1:0,model_input2:0\", outputs = \"FloorMod:0\"}, tf_saved_model.exported_names = [\"serving_default\"]} {\r\n    %0 = \"tosa.reciprocal\"(%arg1) : (tensor<f32>) -> tensor<f32>\r\n    %1 = \"tosa.reshape\"(%0) {new_shape = [1, 1]} : (tensor<f32>) -> tensor<1x1xf32>\r\n    %2 = \"tosa.mul\"(%arg0, %1) {shift = 0 : i32} : (tensor<11x10xf32>, tensor<1x1xf32>) -> tensor<11x10xf32>\r\n    %3 = \"tosa.floor\"(%2) : (tensor<11x10xf32>) -> tensor<11x10xf32>\r\n    %4 = \"tosa.sub\"(%2, %3) : (tensor<11x10xf32>, tensor<11x10xf32>) -> tensor<11x10xf32>\r\n    return %4 : tensor<11x10xf32>\r\n  }\r\n}\r\n'\n```\n\n\n### Relevant log output\n\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","awaiting PR merge","TF 2.9","comp:lite-tosa"],"created_at":"2022-08-30T04:40:53Z","comments":11,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/57520"},{"issue_number":420,"repository":"tensorflow\/tensorflow","title":"[BUG] Gradient tape *inside* tf.function broken for tf.Variable argument.","description":" \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.7, 2.9, nightly\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nUbuntu 22.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.1)\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\ntf.function (since TF 2.7 I believe) now remembers the storage of the variable and treats them as tensors (using the `__tf_retrace__` mechanism). This works fine whenever the value of the variable is needed and not it's identity. However, when taking the gradient, the identity matters, not the value.\r\n\r\nThis means when decorating a function that takes the gradient of y with respect to x (x being a parameter here), it traces the function, uses the value of the TensorLike object and **creates the gradient with respect to the parameter's identity (watching the parameter)**.\r\nCalling the same function with a different parameter won't retrace (!) and just use the value, however since it doesn't retrace, it won't re-calculate the gradient with respect to the *identity* of the new variable given as argument but instead use the remembered gradient.\r\n\r\nBasically, the problem arises since tf.function and tensor-like objects function functionally (i.e. gradient with respect to argument one, two etc) and tf.Variable, that are global statebased objects. Using a statebased object as a \"functional\" object breaks the correct behavior of GradientTape.\r\n\r\n## Bug or feature?\r\n\r\nI do understand the merits of this in a function that doesn't take gradients. However, it breaks for the logic of the gradient.\r\n\r\n## Possible fixes\r\n\r\nThere are workarounds, i.e. to always give parameters in a dict with their name, the names triggering a retrace.\r\n```\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nLink to colab with the code below: https:\/\/colab.research.google.com\/drive\/1z26ZIoiFEBO9icGJ5bYusu4XY5u-vpkG?usp=sharing\r\n\r\n\r\nimport tensorflow as tf\r\n\r\nx1 = tf.Variable(2.0)\r\nx2 = tf.Variable(4.0)\r\n\r\n\r\ndef f():\r\n    res = x1 + x2 ** 2 \/ 2\r\n    return res\r\n\r\n\r\ndef grad(param):\r\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\r\n        tape.watch(param)\r\n        value = f()\r\n    return tape.gradient(value, param)\r\n\r\njitted_grad = tf.function(grad)\r\n\r\ny1 = grad(x1)\r\ny1_jit = jitted_grad(x1)\r\nassert abs(y1 - 1.0) < 1e-5  # because d x1 \/ dx1 = 1\r\nassert abs(y1_jit - 1.0) < 1e-5\r\ny2 = grad(x2)\r\ny2_jit = jitted_grad(x2)\r\nprint(f\"y2: {y2}, should be 4\")  # but is 1 because it uses the derivative of x1\r\nprint(f\"y2_jit: {y2_jit}, should also be 4\")  # but is 1 because it uses the derivative of x1\r\nassert abs(y2 - 4.0) < 1e-5  # because d \/ dx x**2\/2 = x -> 4\r\nassert abs(y2_jit - 4.0) < 1e-5  # fails!\r\n```\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.9"],"created_at":"2022-08-22T13:35:38Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/57365"},{"issue_number":421,"repository":"tensorflow\/tensorflow","title":"AutoGraph cannot handle python 3.10's structural pattern matching","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\n2.9.2\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nmacOS 12.5\r\n\r\n### Mobile device\r\n\r\nnone\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\nnone\r\n\r\n### GPU model and memory\r\n\r\nApple's METAL (though unlikely to be relevant here)\r\n\r\n### Current Behaviour?\r\n\r\nTrying to AutoGraph a function containing a [PEP 634\/635\/636 structural pattern matching statement](https:\/\/peps.python.org\/pep-0634\/) from python 3.10 (i.e. `match\/case`) results in the `WARNING:tensorflow:AutoGraph could not transform <function test at 0x2c4c99120> and will run it as-is.`\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.function\r\ndef test(x, selector):\r\n    match selector:\r\n        case \"square\":\r\n            return x**2\r\n        case \"double\":\r\n            return x*2.\r\n        case _:\r\n            raise ValueError\r\n            \r\ntest(tf.linspace(-1,1,10),\"double\")\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nINFO:tensorflow:Error transforming entity <function test at 0x2c4c99120>\r\nTraceback (most recent call last):\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 427, in converted_call\r\n    converted_f = _convert_actual(target_entity, program_ctx)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 269, in _convert_actual\r\n    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/pyct\/transpiler.py\", line 282, in transform\r\n    return self.transform_function(obj, user_context)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/pyct\/transpiler.py\", line 466, in transform_function\r\n    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/pyct\/transpiler.py\", line 359, in transform_function\r\n    result = self.transform_ast(node, context)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 237, in transform_ast\r\n    node = self.initial_analysis(node, ctx)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/impl\/api.py\", line 223, in initial_analysis\r\n    graphs = cfg.build(node)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/pyct\/cfg.py\", line 970, in build\r\n    visitor.visit(node)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/ast.py\", line 410, in visit\r\n    return visitor(node)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/pyct\/cfg.py\", line 766, in visit_FunctionDef\r\n    self._process_function_def(node, is_lambda=False)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/site-packages\/tensorflow\/python\/autograph\/pyct\/cfg.py\", line 757, in _process_function_def\r\n    self.visit(stmt)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/ast.py\", line 410, in visit\r\n    return visitor(node)\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/ast.py\", line 414, in generic_visit\r\n    for field, value in iter_fields(node):\r\n  File \"\/Users\/yves\/.pyenv\/versions\/3.10.4\/Library\/Frameworks\/Python.framework\/Versions\/3.10\/lib\/python3.10\/ast.py\", line 252, in iter_fields\r\n    for field in node._fields:\r\nAttributeError: 'NoneType' object has no attribute '_fields'\r\n```\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:autograph","TF 2.9"],"created_at":"2022-08-16T06:28:12Z","comments":6,"reactions":2,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/57166"},{"issue_number":422,"repository":"tensorflow\/tensorflow","title":" Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.9.1\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nWhen running `model.predict` I am getting the warning. What does that mean and how can I fix it ?\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nThe model is a keras.functional.Functional model where the first layers are from tensorflow hub pretrained models. I have two dense layers in the end. compiling, fitting do not give any warning however when running `model.predict([\"examplestring\"])` I get the warning.\n```\n\n\n### Relevant log output\n\n```shell\n2022-08-09 15:37:17.301561: W tensorflow\/core\/common_runtime\/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected a subtype of type_id: TFT_TENSOR\r\nargs {\r\n  type_id: TFT_LEGACY_VARIANT\r\n}\r\n for input 2 of a homogeneous container 1001, got type_id: TFT_RAGGED\r\nargs {\r\n  type_id: TFT_INT32\r\n}\r\n\r\n        while inferring type of node 'model\/preprocessing\/StatefulPartitionedCall\/StatefulPartitionedCall\/StatefulPartitionedCall\/StatefulPartitionedCall\/StatefulPartitionedCall\/bert_pack_inputs\/PartitionedCall\/map\/while\/body\/_337\/map\/while\/TensorArrayV2Write\/TensorListSetItem'\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF 2.9"],"created_at":"2022-08-09T11:41:24Z","comments":30,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/57052"},{"issue_number":423,"repository":"tensorflow\/tensorflow","title":"AttributeError: '_UserObject' object has no attribute 'add_slot'","description":"Issue Type: Bug\r\nSource: binary\r\nTensorflow Version: 2.6\r\nCustom Code: Yes\r\n\r\n\r\n# Current Behaviour?\r\n\r\nI have model, saved using `SavedModel` format. The model is trained on Kaggle TPU and saved in the following ways:\r\n\r\n```python\r\n# in kaggle tpu, it must be saved in the following way!\r\nsave_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\r\nfinal_model.save('.\/final_model', options=save_locally)\r\n ```\r\n\r\nNow, it seems like, if I try to load this model in the following way, \r\n\r\n```python\r\nmodel = tf.saved_model.load(\"final_model\")\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n\/tmp\/ipykernel_17\/3033429325.py in <module>\r\n----> 1 model = tf.saved_model.load(\"final_model\")\r\n...\r\n...\r\nAttributeError: '_UserObject' object has no attribute 'add_slot'\r\n```\r\n\r\nBut if I do as follows, it works.\r\n\r\n```\r\nmodel = tf.keras.models.load_model(\"final_model\")\r\n```\r\n\r\nBut for some reason, I need to make `tf.saved_model.load` API work instead `tf.keras.models.load_model` API. What approach should we take here?\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:dist-strat","2.6.0"],"created_at":"2022-08-03T08:44:46Z","comments":16,"reactions":3,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/56997"},{"issue_number":424,"repository":"tensorflow\/tensorflow","title":"Differentiate a tuple with `tape.jacobian`","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\ntf 2.9.1\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nMacOs\n\n### Mobile device\n\n-\n\n### Python version\n\n3.9\n\n### Bazel version\n\n-\n\n### GCC\/Compiler version\n\n-\n\n### CUDA\/cuDNN version\n\n-\n\n### GPU model and memory\n\n-\n\n### Current Behaviour?\n\n```shell\nI expect to able to differentiate a function returning a tuple of TensorFlow tensors, but unfortunately it is not possible because tuples have no shape. Could you provide me a work around for this? Maybe something with ragged tensors is possible here?\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ndef circuit(params):\r\n    return params[0] ** 2, params[1::] ** 3\r\n\r\nx = tf.Variable([0.1, 0.2, 0.3])\r\n\r\nwith tf.GradientTape() as tape:\r\n    out = circuit(x)\r\n\r\njac = tape.jacobian(out, x)\n```\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\r\n  File \"\/Users\/...\/test.py\", line 14, in <module>\r\n    jac = tape.jacobian(out, x)\r\n  File \"\/Users\/...\/lib\/python3.9\/site-packages\/tensorflow\/python\/eager\/backprop.py\", line 1183, in jacobian\r\n    target_static_shape = target.shape\r\nAttributeError: 'tuple' object has no attribute 'shape'\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.9"],"created_at":"2022-07-26T18:03:56Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/56912"},{"issue_number":425,"repository":"tensorflow\/tensorflow","title":"`Error in PredictCost() for the op: op: \"CropAndResize\"` when using the `tf.image.crop_and_resize` op","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\n2.9.1\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nRedHat Linux Enterprise 8.4\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.7\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\nCUDA: 11.6\r\n\r\n### GPU model and memory\r\n\r\nV100, 32GB\r\n\r\n### Current Behaviour?\r\n\r\n\r\nI wanted to code the equivalent of [RandomResizedCrop](https:\/\/pytorch.org\/vision\/main\/generated\/torchvision.transforms.RandomResizedCrop.html) from torchvision.\r\nI used the model from [this official keras tutorial](https:\/\/keras.io\/examples\/vision\/nnclr\/#random-resized-crops) and integrated it in my data pipeline.\r\nWhen using it, I got the following warning:\r\n\r\n```\r\n2022-07-26 14:10:05.665573: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: 40 } dim { size: 40 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 16 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 16 } dim { size: 16 } dim { size: 3 } } }\r\n2022-07-26 14:10:05.815003: W tensorflow\/core\/kernels\/data\/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\r\n```\r\n\r\nAt first, I didn't care because my code was running fine, but I had a CPU memory error after about 9 epochs of ImageNet training. This suggests that there is somehow a memory leak during training.\r\n\r\nThe same behaviour (although on GPU from that I understand) was also observed in this [SO question](https:\/\/stackoverflow.com\/q\/72642906\/4332585).\r\n\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n\r\nUnfortunately, the warning does not appear on Colab, but here is a link with the appropriate minimal example anyway: https:\/\/colab.research.google.com\/drive\/1QHa4kxPLfCjkfDvmwv9wj5yq19za5SpA?usp=sharing\r\n\r\nHowever, locally (on my laptop without GPU) and on my server, the warning is thrown.\r\n\r\nThe full code is the following:\r\n\r\n```python\r\n\r\nimport tensorflow as tf\r\n\r\nclass RandomResizedCrop(tf.keras.layers.Layer):\r\n    # taken from\r\n    # https:\/\/keras.io\/examples\/vision\/nnclr\/#random-resized-crops\r\n    def __init__(self, scale, ratio, crop_shape):\r\n        super(RandomResizedCrop, self).__init__()\r\n        self.scale = scale\r\n        self.log_ratio = (tf.math.log(ratio[0]), tf.math.log(ratio[1]))\r\n        self.crop_shape = crop_shape\r\n\r\n    def call(self, images):\r\n        batch_size = tf.shape(images)[0]\r\n\r\n        random_scales = tf.random.uniform(\r\n            (batch_size,),\r\n            self.scale[0],\r\n            self.scale[1]\r\n        )\r\n        random_ratios = tf.exp(tf.random.uniform(\r\n            (batch_size,),\r\n            self.log_ratio[0],\r\n            self.log_ratio[1]\r\n        ))\r\n\r\n        new_heights = tf.clip_by_value(\r\n            tf.sqrt(random_scales \/ random_ratios),\r\n            0,\r\n            1,\r\n        )\r\n        new_widths = tf.clip_by_value(\r\n            tf.sqrt(random_scales * random_ratios),\r\n            0,\r\n            1,\r\n        )\r\n        height_offsets = tf.random.uniform(\r\n            (batch_size,),\r\n            0,\r\n            1 - new_heights,\r\n        )\r\n        width_offsets = tf.random.uniform(\r\n            (batch_size,),\r\n            0,\r\n            1 - new_widths,\r\n        )\r\n\r\n        bounding_boxes = tf.stack(\r\n            [\r\n                height_offsets,\r\n                width_offsets,\r\n                height_offsets + new_heights,\r\n                width_offsets + new_widths,\r\n            ],\r\n            axis=1,\r\n        )\r\n        images = tf.image.crop_and_resize(\r\n            images,\r\n            bounding_boxes,\r\n            tf.range(batch_size),\r\n            self.crop_shape,\r\n        )\r\n        return images\r\n\r\nimport tensorflow_datasets as tfds\r\n\r\n\r\nds = tfds.load('cifar10', split='train', as_supervised=True)\r\nimage_width = 16\r\ncrop = RandomResizedCrop(\r\n    scale=(0.08, 1.0),\r\n    ratio=(0.75, 1.33),\r\n    crop_shape=(image_width, image_width),\r\n)\r\ndata_aug_list = [\r\n    tf.keras.layers.ZeroPadding2D(padding=4),\r\n    crop,\r\n]\r\ndata_aug_layer = tf.keras.models.Sequential(data_aug_list)\r\nds = ds.map(\r\n  lambda x, y: (data_aug_layer(x[None], training=True)[0], y),\r\n  num_parallel_calls=tf.data.experimental.AUTOTUNE,\r\n)\r\nds = ds.shuffle(\r\n  buffer_size=1000,  # For now a hardcoded value\r\n  reshuffle_each_iteration=True,\r\n).batch(\r\n  32,\r\n  num_parallel_calls=tf.data.experimental.AUTOTUNE,\r\n)\r\nds = ds.prefetch(\r\n  buffer_size=tf.data.experimental.AUTOTUNE,\r\n)\r\n\r\nres = next(iter(ds))  # warning is thrown here\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2022-07-25 15:28:12.176012: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }\r\n2022-07-25 16:47:53.335022: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }\r\n2022-07-25 18:07:19.352253: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }\r\n2022-07-25 19:26:43.163855: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }\r\n2022-07-25 20:46:07.180534: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }\r\n2022-07-25 22:05:27.767552: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }\r\n2022-07-25 23:24:49.840029: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }\r\n2022-07-26 00:44:13.601504: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }\r\n2022-07-26 02:03:37.488186: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }\r\n2022-07-26 03:22:57.948498: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }\r\n2022-07-26 04:42:09.083767: W tensorflow\/core\/grappler\/costs\/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_UINT8 } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"101\" frequency: 2500 num_cores: 10 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }\r\nslurmstepd: error: Detected 1 oom-kill event(s) in StepId=1562147.0. Some of your processes may have been killed by the cgroup out-of-memory handler.\r\nsrun: error: r10i0n2: task 0: Out Of Memory\r\nsrun: launch\/slurm: _step_signal: Terminating StepId=1562147.0\r\nslurmstepd: error: Detected 1 oom-kill event(s) in StepId=1562147.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.\r\n```\r\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.9"],"created_at":"2022-07-26T12:43:07Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/56904"},{"issue_number":426,"repository":"tensorflow\/tensorflow","title":"Nondeterministic result on TPU","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.9\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nTPU v3-8\n\n### Current Behaviour?\n\n```shell\nUse \"TF_DETERMINISTIC_OPS = 1\" or \"tf.config.experimental.enable_op_determinism()\" can get determistic result on GPU.\r\n\r\nBut the results are nondetermistic on cloud TPU.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/github.com\/edwardyehuang\/CAR\r\n\r\nThe code on repo above can get determistic result on GPU, but the result is nondetermistic on TPU.\n```\n\n\n### Relevant log output\n\n```shell\nTPU 1st RUN (1000 steps):\r\n\r\n1000\/1000 [==============================] - 506s 395ms\/step - loss: 1.6268 - IOU: 0.3178 - g_1_orl: 0.6300 - g_1_sal: 0.0168 - val_loss: 1.3395 - val_IOU: 0.2370\r\n\r\nTPU 2nd RUN (1000 steps):\r\n\r\n1000\/1000 [==============================] - 645s 448ms\/step - loss: 1.6488 - IOU: 0.3095 - g_1_orl: 0.6314 - g_1_sal: 0.0162 - val_loss: 1.7416 - val_IOU: 0.1793\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:tpus","TF 2.9"],"created_at":"2022-07-21T08:52:46Z","comments":20,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/56847"},{"issue_number":427,"repository":"tensorflow\/tensorflow","title":"Wrong is_gpu flag computation in graph_memory.cc","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.9.1\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nI work for Sonar and as part of our internal tests, we run our static analyzers on multiple open source codes. While reviewing our tests we saw the following code that is almost certainly not intended:\r\nOn the following line: https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/core\/grappler\/costs\/graph_memory.cc#L175\r\nthe method `std::string::find` will return `std::string::npos` if the searched substring is not present. Because `std::string::npos` is different from 0, casting `std::string::find` as a boolean does not return what one would expect and there is no way the test on this line returns anything other than true, whether there is a GPU or not.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nI don't know how this bug impacts the functional behavior of the tool. It might be completely innocuous, given its age.\r\nI am also unsure whether it should be fixed by removing the flag altogether to keep the current years-old behavior or fixed to actually handle non-GPU devices.\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_<\/details>","labels":["type:bug","comp:core","TF 2.9"],"created_at":"2022-06-16T14:45:48Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/56481"},{"issue_number":428,"repository":"tensorflow\/tensorflow","title":"Use of Keras `jit_compile` in a distribution strategy causes a `std::system_error`","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.9.1\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n11.2\/8.1.1.33\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n\r\nThe following error is thrown during training after a number of steps \/ epochs \r\n```shell\r\nterminate called after throwing an instance of 'std::system_error'\r\nwhat():  Resource temporarily unavailable\r\n```\r\nI am able to reproduce this error in colab with my sample code\r\n\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport keras\r\nimport tensorflow as tf\r\n\r\ndef build_model_() -> keras.Model:\r\n    input = tf.keras.layers.Input(shape=(5,), name='input_a')\r\n    x = tf.keras.layers.Dense(512, activation = 'relu')(input)\r\n    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\r\n    output = tf.keras.layers.Dense(1, name='output')(x)\r\n    model = tf.keras.models.Model(inputs=input, outputs=output)\r\n    return model\r\n\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\nprint(f\"Can see {strategy.num_replicas_in_sync} gpus\")\r\nwith strategy.scope():\r\n  model = build_model_()\r\n  model.compile(loss = 'mse', jit_compile=True)\r\n\r\nBATCH_SIZE_PER_REPLICA = 1024\r\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n\r\noptions = tf.data.Options()\r\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\r\n\r\ndataset = tf.data.Dataset.from_tensors(\r\n    (tf.ones(5), 1)\r\n).repeat(10_000_000).batch(GLOBAL_BATCH_SIZE).with_options(options)\r\n\r\nhistory = model.fit(\r\n    x = dataset,\r\n    epochs=7,\r\n    verbose = 1,\r\n)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:dist-strat","TF 2.9"],"created_at":"2022-06-10T18:14:23Z","comments":17,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/56423"},{"issue_number":429,"repository":"tensorflow\/tensorflow","title":"tf.data batching slows down on Windows","description":"<details><summary>Click to expand!<\/summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\ntf 2.8\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nWindows 10\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.8\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\nCUDA 11.2 cuDNN 8.1.0\r\n\r\n### GPU model and memory\r\n\r\nNvidia A40 48GB, 128GB System RAM\r\n\r\n### Current Behaviour?\r\n\r\n\r\nWhile training a keras model using a [custom tensorflow dataset](https:\/\/www.tensorflow.org\/datasets\/add_dataset), the training steps slow down once the remaining sample count in the first epoch is smaller than the shuffle buffer size. The slowdown carries over to the following epochs and takes longer and longer.\r\n\r\nI would expect the opposite behavior since all remaining samples should already be in the buffer and do not need to be loaded anymore. I have tested it on multiple Windows systems and this slowdown occurred on all of them. However, it does not happen on Linux-based systems.\r\n\r\nI then used the tensorboard profiling plugin to investigate what is causing the slowdown. As you can see here, it seems to be an input-related problem:\r\n![profiler_1](https:\/\/user-images.githubusercontent.com\/1509163\/166112234-b795f11e-934f-4631-86b1-c8d79e1e1448.png)\r\n\r\nFrom the input operations you can see that `Iterator::Root::Prefetch::BatchV2` takes the most time (this is also the case when removing the prefetching):\r\n![profiler_2](https:\/\/user-images.githubusercontent.com\/1509163\/166112243-798a33a9-b329-4103-9d24-b0b70306411d.png)\r\n\r\nThe slowdown can also be seen in the trace viewer:\r\n![profiler_3](https:\/\/user-images.githubusercontent.com\/1509163\/166112328-9e637031-1166-4ac6-afef-47a8a534bf9b.png)\r\n\r\nHere is a more detailed comparison of `BatchV2` durations from different steps:\r\n![profiler_4](https:\/\/user-images.githubusercontent.com\/1509163\/166112335-119bb08e-7cd2-4a4b-a149-0de1580332a4.png)\r\n![profiler_5](https:\/\/user-images.githubusercontent.com\/1509163\/166112342-dd0ea8a4-9d79-4ec3-adc9-87ac133e118e.png)\r\n\r\n\r\nHere is the [profiling_data.zip](https:\/\/github.com\/tensorflow\/tensorflow\/files\/8597248\/profiling_data.zip).\r\n\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nIn order to reproduce and profile this issue the following code can be used. When running the `main.py` for the first time it will generate the dummy dataset which takes around 20 minutes to write 24GB of dummy data.\r\n\r\n**dummy_dataset.py**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\n\r\n\r\nclass DummyDataset(tfds.core.GeneratorBasedBuilder):\r\n    VERSION = tfds.core.Version('1.0.0')\r\n    RELEASE_NOTES = {\r\n        '1.0.0': 'Initial release.',\r\n    }\r\n\r\n    def _info(self) -> tfds.core.DatasetInfo:\r\n        return tfds.core.DatasetInfo(\r\n            builder=self,\r\n            features=tfds.features.FeaturesDict({\r\n                'audio': tfds.features.Audio(shape=(16000,), dtype=tf.float32),\r\n                'label': tfds.features.Tensor(shape=(2,), dtype=tf.float32),\r\n            }),\r\n            supervised_keys=('audio', 'label')\r\n        )\r\n\r\n    def _split_generators(self, dl_manager: tfds.download.DownloadManager):\r\n        return {\r\n            'train': self._generate_examples(),\r\n        }\r\n\r\n    def _generate_examples(self):\r\n        for i in range(400000): # If sample count is increased, issue starts later (tested with 1M+)\r\n            yield i, {\r\n                'audio': np.random.sample(16000).astype(dtype=np.float32),\r\n                'label': tf.one_hot(np.random.choice(2), depth=2).numpy()\r\n            }\r\n```\r\n\r\n\r\n\r\n**main.py**\r\n```python\r\nfrom dummy_dataset import DummyDataset\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nimport os\r\nimport datetime\r\nfrom tensorflow import keras\r\n\r\n# Configuration\r\nshuffle_buffer = 300000 # Uses ~30GB RAM, can be lowered to 150000 if only 16GB RAM available\r\nbatch_size = 512\r\nepochs = 3\r\n\r\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\r\nlosses = ['categorical_crossentropy']\r\nmetrics = [keras.metrics.CategoricalAccuracy()]\r\n\r\nfolder_name = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\r\n\r\n# Prepare dataset (takes ~20min for the first time to generate ~24GB of dummy data)\r\nds_train, ds_info = tfds.load(\r\n    name='DummyDataset',\r\n    split='train',\r\n    as_supervised=True,\r\n    with_info=True,\r\n    shuffle_files=True,\r\n)\r\nds_train = ds_train.shuffle(shuffle_buffer)\r\nds_train = ds_train.batch(batch_size)\r\nds_train = ds_train.prefetch(tf.data.AUTOTUNE)\r\n\r\n# Configure profiling\r\nsample_count = ds_info.splits['train'].num_examples\r\nslowdown_start = int((sample_count - shuffle_buffer) \/ batch_size)\r\nprofile_stop = slowdown_start + 150\r\n\r\ncallbacks = [keras.callbacks.TensorBoard(\r\n    log_dir=os.path.join(os.getcwd(), 'tensorboard', folder_name),\r\n    profile_batch=[slowdown_start, profile_stop]\r\n)]\r\n\r\n# Build and train model\r\ninput_layer = keras.layers.Input(shape=(16000,), dtype=tf.float32, name='audio_input')\r\noutput_layer = keras.layers.Dense(2, activation='softmax', name='prediction')(input_layer)\r\nmodel = keras.Model(inputs=input_layer, outputs=output_layer, name='dummy_model')\r\n\r\nmodel.compile(optimizer=optimizer, loss=losses, metrics=metrics)\r\nmodel.summary()\r\n\r\nprint(f'Expected slowdown to start at batch {slowdown_start}, profiling batches {slowdown_start}-{profile_stop}')\r\n\r\nmodel.fit(x=ds_train, epochs=epochs, verbose=1, callbacks=callbacks)\r\n```\r\n\r\n**requirements.txt**\r\n```\r\ntensorflow==2.8.0\r\ntensorflow-datasets==4.5.2\r\ntensorboard-plugin-profile\r\nnumpy\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.8"],"created_at":"2022-04-30T15:34:42Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/55815"},{"issue_number":430,"repository":"tensorflow\/tensorflow","title":"tf.make_tensor_proto() does not respect byte order of numpy input array","description":"<details><summary>Click to expand!<\/summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\ntf 2.8\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/Compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nByte order is not checked when creating a TensorProto with tf.make_tensor_proto() from a numpy.ndarray with non-native byte order. This leads to wrong tensor data.\r\n\r\nI would expect that byte order is checked and swapped if necessary before assigning to tensor_proto.tensor_content in [python\/framework\/tensor_util.py](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.8.0\/tensorflow\/python\/framework\/tensor_util.py#L523)\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nx = np.ones(shape=(1, 2), dtype=np.float32)\r\nx_bswap = x.astype('>f4') # assuming native byte order is little endian\r\n\r\ny = tf.make_ndarray(tf.make_tensor_proto(x))\r\ny_bswap = tf.make_ndarray(tf.make_tensor_proto(x_bswap))\r\n\r\nprint(y)\r\nprint(y_bswap)\r\n\r\nassert np.array_equal(x, y)\r\nassert np.array_equal(x, y_bswap)\n```\n\n\n### Relevant log output\n\n```shell\n[[1. 1.]]\r\n[[4.6006e-41 4.6006e-41]]\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\nin <cell line: 7>()\r\n      5 print(y_bswap)\r\n      6 assert np.array_equal(x, y)\r\n----> 7 assert np.array_equal(x, y_bswap)\r\n\r\nAssertionError:\n```\n<\/details>","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.8"],"created_at":"2022-04-28T13:00:16Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/55789"},{"issue_number":431,"repository":"tensorflow\/tensorflow","title":"[XLA:GPU] Failed to determine best cudnn convolution algorithm","description":"I got `Failed to determine best cudnn convolution algorithm`  error when running `facebook\/wav2vec2-base-960h` model using torch_xla on GPU in fp16 mode. This error only occurs when using fp16 and fp32 works fine.\r\n\r\nMinimal HLO to reproduce:\r\n```llvm\r\nHloModule Test\r\n\r\nENTRY main {\r\n  x = f16[44,768,1,49]{3,2,1,0} parameter(0)\r\n  y = f16[44,768,1,50]{3,2,1,0} parameter(1)\r\n  ROOT %convolution.10022 = f16[1,128,48,768]{3,2,1,0} convolution(f16[44,768,1,49]{3,2,1,0} %x, f16[44,768,1,50]{3,2,1,0} %y), window={size=1x50 pad=0_0x64_64}, dim_labels=fb01_io01->01bf, batch_group_count=16\r\n}\r\n```\r\n\r\nThe following is the full log file.\r\n```\r\n(base) ubuntu@xla-p3-8x:~\/src\/tensorflow\/xla_benchmark$ ..\/bazel-bin\/tensorflow\/compiler\/xla\/tools\/replay_computation_gpu --use_fake_data=true --num_runs=1 --print_result=false conv.hlo \r\n2022-04-15 00:10:23.507808: I tensorflow\/compiler\/xla\/service\/platform_util.cc:69] platform Host present but no XLA compiler available: could not find registered compiler for platform Host -- check target linkage (hint: try adding tensorflow\/compiler\/jit:xla_cpu_jit as a dependency)\r\n2022-04-15 00:10:24.852458: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:24.935392: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:24.944338: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:24.967499: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:24.983119: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:25.002219: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:25.025011: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:25.050733: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-04-15 00:10:25.052806: I tensorflow\/compiler\/xla\/service\/service.cc:174] XLA service 0x562e5c814b50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n2022-04-15 00:10:25.052832: I tensorflow\/compiler\/xla\/service\/service.cc:182]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052839: I tensorflow\/compiler\/xla\/service\/service.cc:182]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052844: I tensorflow\/compiler\/xla\/service\/service.cc:182]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052849: I tensorflow\/compiler\/xla\/service\/service.cc:182]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052854: I tensorflow\/compiler\/xla\/service\/service.cc:182]   StreamExecutor device (4): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052859: I tensorflow\/compiler\/xla\/service\/service.cc:182]   StreamExecutor device (5): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052863: I tensorflow\/compiler\/xla\/service\/service.cc:182]   StreamExecutor device (6): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\n2022-04-15 00:10:25.052868: I tensorflow\/compiler\/xla\/service\/service.cc:182]   StreamExecutor device (7): Tesla V100-SXM2-32GB, Compute Capability 7.0\r\nconv.hlo: is not HloSnapshot. Trying HloProto.\r\nconv.hlo: is not HloProto. Trying HLO text.\r\n2022-04-15 00:10:25.053972: I tensorflow\/compiler\/xla\/tools\/replay_computation.cc:470] Compiling 1 modules in parallel.\r\n2022-04-15 00:10:25.622602: I tensorflow\/stream_executor\/cuda\/cuda_dnn.cc:384] Loaded cuDNN version 8204\r\n2022-04-15 00:10:26.108045: I tensorflow\/compiler\/xla\/tools\/replay_computation.cc:487] Done compiling; now running the modules.\r\n2022-04-15 00:10:26.108867: E tensorflow\/compiler\/xla\/tools\/replay_computation.cc:491] Compilation failed: UNKNOWN: Failed to determine best cudnn convolution algorithm for:\r\n%cudnn-conv = (f16[1,128,48,768]{3,1,0,2}, u8[0]{0}) custom-call(f16[704,48,1,49]{0,3,2,1} %bitcast.2, f16[48,768,1,50]{0,3,2,1} %pad), window={size=1x50 pad=0_0x64_64}, dim_labels=fb01_io01->01bf, feature_group_count=16, custom_call_target=\"__cudnn$convForward\", backend_config=\"{\\\"conv_result_scale\\\":1,\\\"activation_mode\\\":\\\"0\\\",\\\"side_input_scale\\\":0}\"\r\n\r\nOriginal error: UNKNOWN: CUDNN_STATUS_BAD_PARAM\r\nin tensorflow\/stream_executor\/cuda\/cuda_dnn.cc(3520): 'op' CUDNN_BACKEND_OPERATION: cudnnFinalize Failed\r\n\r\nTo ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.: hlo { hlo_module { name: \"Test\" entry_computation_name: \"main\" computations { name: \"main\" instructions { name: \"x\" opcode: \"parameter\" shape { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 49 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } metadata { } frontend_attributes { } } instructions { name: \"y\" opcode: \"parameter\" shape { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 50 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } metadata { } parameter_number: 1 id: 1 frontend_attributes { } } instructions { name: \"convolution.10022\" opcode: \"convolution\" shape { element_type: F16 dimensions: 1 dimensions: 128 dimensions: 48 dimensions: 768 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } metadata { } window { dimensions { size: 1 stride: 1 window_dilation: 1 base_dilation: 1 } dimensions { size: 50 stride: 1 padding_low: 64 padding_high: 64 window_dilation: 1 base_dilation: 1 } } convolution_dimension_numbers { kernel_output_feature_dimension: 1 kernel_spatial_dimensions: 2 kernel_spatial_dimensions: 3 input_batch_dimension: 1 output_batch_dimension: 2 output_feature_dimension: 3 input_spatial_dimensions: 2 input_spatial_dimensions: 3 output_spatial_dimensions: 0 output_spatial_dimensions: 1 } id: 2 operand_ids: 0 operand_ids: 1 feature_group_count: 1 precision_config { operand_precision: DEFAULT operand_precision: DEFAULT } batch_group_count: 16 frontend_attributes { } } program_shape { parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 49 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 50 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } result { element_type: F16 dimensions: 1 dimensions: 128 dimensions: 48 dimensions: 768 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameter_names: \"x\" parameter_names: \"y\" } id: 2 root_id: 2 } host_program_shape { parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 49 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 50 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } result { element_type: F16 dimensions: 1 dimensions: 128 dimensions: 48 dimensions: 768 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameter_names: \"p0\" parameter_names: \"p1\" } entry_computation_id: 2 input_output_alias { } dynamic_parameter_binding { } } }\r\n```\r\n\r\nTested with tensorflow commit 75861c43005523e2552bb3f85b2f0defc16ea9cf, CUDA 11.4, CUDNN 8.2.","labels":["stat:awaiting tensorflower","type:bug","comp:xla"],"created_at":"2022-04-15T00:15:40Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/55633"},{"issue_number":432,"repository":"tensorflow\/tensorflow","title":"TFLite Converter: how to skip Dequantize node in FP16 models for GPU Delegate?","description":"<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/ISSUES.md),\r\nwe only address code\/doc bugs, performance issues, feature requests and\r\nbuild\/installation issues on GitHub. tag:bug_template<\/em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yocto dunfell \r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: aarch64\r\n- TensorFlow installed from (source or binary):\r\n- TensorFlow version (use command below): 2.6.1\r\n- Python version: 3.9.9\r\n- Bazel version (if compiling from source):\r\n- GCC\/Compiler version (if compiling from source):\r\n- CUDA\/cuDNN version:\r\n- GPU model and memory: Mali G72\r\n\r\n\r\n**Describe the current behavior**\r\nI converted mobilenet_v2_1.0_224 [https:\/\/storage.googleapis.com\/download.tensorflow.org\/models\/tflite_11_05_08\/mobilenet_v2_1.0_224_quant.tgz](url) to fp16 tflite model using the steps mentioned here [https:\/\/www.tensorflow.org\/lite\/performance\/post_training_quantization#float16_quantization](url)\r\n\r\nthe converted mobilenet_v2_fp16.tflite model has multiple Dequantize steps inserted within which seem to convert float16 to float32, even though the model is supposed to be quantized to float16.\r\nBelow is a snippet of the model visualization from netron. The inputs to the converted model are float32 and its outputs are also float32. \r\n \r\n![test](https:\/\/user-images.githubusercontent.com\/78979784\/155630251-9e58343d-95a9-420c-8101-86fed9165372.PNG)\r\n\r\n\r\nAll the dequantize nodes have float16 input and float32 output. Node 0 dequantize node properties are visualized below:\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/78979784\/155630719-1c989e54-22d8-4423-846f-93eb911c22ff.png)\r\n\r\nall dequantize nodes have similar properties where the inputs to the layer comprise of only weights and\/or bias. \r\n\r\nnow according to the official documentation, _\"By default, a float16 quantized model will \"dequantize\" the weights values to float32 when run on the CPU. (Note that the GPU delegate will not perform this dequantization, since it can operate on float16 data.)\"_ This means dequantize step is added just for cpu execution and gpu will not perform or override this node. But the question I have is, the dequantize node is already present, can a delegate simply skip a node in a model architecture during execution runtime, since the node is a part of the model graph and every node is executed with no op delegated to CPU? \r\n\r\n\r\nWhat is the main use of this dequantize node? And why is a dequantization step inserted during fp16 quantization? Is there a way to skip these dequantize nodes in the GPU delegate during post-training float16 quantization itself, for I want to run the model only on GPU and not CPU? The node seems to be executed by GPU delegate, as none of the actions are delegated to the CPU at runtime. Or are they being simply ignored by the GPU delegate?\r\n\r\nmobilenet_v2_fp16.tflite model is attached for your reference.\r\n[mobilenet_v2_fp16.zip](https:\/\/github.com\/tensorflow\/tensorflow\/files\/8137542\/mobilenet_v2_fp16.zip)\r\n \r\nthanks\r\n\r\n\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","TFLiteConverter","2.6.0"],"created_at":"2022-02-25T01:01:05Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/54559"},{"issue_number":433,"repository":"tensorflow\/tensorflow","title":"InvalidArgumentError: Node '...\/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Colab\r\n- TensorFlow version (use command below): 2.8.0 (Colab)\r\n- Python version: 3.7.12\r\n- Bazel version (if compiling from source):\r\n- GCC\/Compiler version (if compiling from source):\r\n- CUDA\/cuDNN version:\r\n- GPU model and memory:\r\n\r\n**Describe the current behavior**\r\n\r\nI get the exception\r\n```\r\nNode 'gradients\/while_grad\/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).\r\n```\r\nwhen executing a `NoOp` without any control dependencies.\r\n\r\nBut also when executing my real code, I get the same exception. I'm not sure if these are two separate issues or the same.\r\n\r\n**Describe the expected behavior**\r\n\r\nA NoOp without control dependencies should not depend on anything, so I would never expect such exception.\r\n\r\nFor my real code, I also would not expect such exception. \r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nCode (to be executed with disabled eager mode):\r\n```\r\nv = tf.Variable(1.)\r\n\r\ndef cond(i, x):\r\n  return tf.less(i, 10)\r\n\r\ndef body(i, x):\r\n  return i + 1, x * 1.\r\n\r\nj, y = tf.while_loop(cond, body, [0., v])\r\nloss = tf.reduce_sum(y ** 2)\r\n\r\nsession.run(tf.compat.v1.global_variables_initializer())\r\n\r\nopt = tf.compat.v1.train.GradientDescentOptimizer(0.1)\r\nopt_op = opt.minimize(loss)\r\n\r\nno_op = tf.no_op()\r\nsession.run(no_op)  # here it crashes already!\r\n\r\nprint(session.run((j, y, opt_op)))\r\n```\r\n\r\nColab: https:\/\/colab.research.google.com\/drive\/1jjXpz8SAeU-8Cg6nuWu7tjxK_sAU5lVc?usp=sharing\r\n\r\n**Other info \/ logs**\r\n\r\nWhen executing locally, I additionally see this log output:\r\n```\r\n...\r\n2022-02-19 23:07:57.636413: W tensorflow\/c\/c_api.cc:349] Operation '{name:'while' id:11 op device:{requested: '', assigned: ''} def:{{{node while}} = StatelessWhile[T=[DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=5, _read_only_resource_inputs=[], _stateful_parallelism=false, body=while_body_8_rewritten[], cond=while_cond_7_rewritten[], output_shapes=[[], [], [], [], []], parallel_iterations=10](while\/loop_counter, while\/maximum_iterations, Const, ReadVariableOp, gradients\/while_grad\/Placeholder_1_0\/accumulator:0)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\r\nTraceback (most recent call last):\r\n  File \"\/Users\/az\/miniforge3\/lib\/python3.9\/site-packages\/tensorflow\/python\/client\/session.py\", line 1380, in _do_call\r\n    return fn(*args)\r\n  File \"\/Users\/az\/miniforge3\/lib\/python3.9\/site-packages\/tensorflow\/python\/client\/session.py\", line 1362, in _run_fn\r\n    self._extend_graph()\r\n  File \"\/Users\/az\/miniforge3\/lib\/python3.9\/site-packages\/tensorflow\/python\/client\/session.py\", line 1403, in _extend_graph\r\n    tf_session.ExtendSession(self._session)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'gradients\/while_grad\/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/Users\/az\/Programmierung\/playground\/tf-while-v2.py\", line 37, in <module>\r\n    main()\r\n  File \"\/Users\/az\/Programmierung\/playground\/tf-while-v2.py\", line 31, in main\r\n    session.run(no_op)\r\n  File \"\/Users\/az\/miniforge3\/lib\/python3.9\/site-packages\/tensorflow\/python\/client\/session.py\", line 970, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n  File \"\/Users\/az\/miniforge3\/lib\/python3.9\/site-packages\/tensorflow\/python\/client\/session.py\", line 1193, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"\/Users\/az\/miniforge3\/lib\/python3.9\/site-packages\/tensorflow\/python\/client\/session.py\", line 1373, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n  File \"\/Users\/az\/miniforge3\/lib\/python3.9\/site-packages\/tensorflow\/python\/client\/session.py\", line 1399, in _do_call\r\n    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'gradients\/while_grad\/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).\r\n```\r\n\r\nMy hypothesis is that the first session call `session.run(tf.compat.v1.global_variables_initializer())` will somehow freeze the while loop function graph (which is strange though as it would not depend on it), and then the further code which adds the gradients causes this warning `Operation ... StatelessWhile ... was changed by setting attribute after it was run by a session`.\r\n\r\nI don't really understand why the first session call does that even though it is independent from the loop.\r\n\r\nI don't really understand why it causes the error for the NoOp execution.\r\n\r\nIs there any workaround?\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.8"],"created_at":"2022-02-19T22:24:17Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/54458"},{"issue_number":434,"repository":"tensorflow\/tensorflow","title":"SelfAdjointEigV2 GPU operation takes a lot of temporary memory.","description":"**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.9.0\r\n- Python version: 3.8.10\r\n- CUDA\/cuDNN version: 11.5\r\n- GPU model and memory: GTX 1660 Ti\r\n\r\n**Describe the current behavior**\r\nA single call to `tf.linalg.eigh()` takes a linear amount of memory in batch size, despite being processed matrix by matrix. I think the ScratchSpace is only freed in the end of the call after ALL matrices in the batch are processed, instead of on the fly matrix, per matrix. This I conclude from the enormous amount of allocations reported by the allocator during the OOM, and looking at the code.\r\n\r\n**[Contributing](https:\/\/www.tensorflow.org\/community\/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes\/no): maybe\r\n- Briefly describe your candidate solution(if contributing):\r\n  - **Best solution:** reuse the ScratchSpace for every matrix.  \r\n  - **Next best solution:** free the ScratchSpace after every matrix in the batch.\r\n\r\n**Standalone code to reproduce the issue**\r\n```py\r\nimport tensorflow as tf\r\ntensor = tf.random.uniform((1024 * 256, 4, 4)) # just make sure the batch size is big enough\r\ntensor = tf.matmul(tensor, tensor, transpose_b=True)\r\nt = tf.linalg.eigvalsh(tensor)\r\n```\r\n\r\n**Other info \/ logs**\r\nSee below the summary of the allocator:\r\n```\r\n2022-01-03 16:18:32.407928: W tensorflow\/core\/common_runtime\/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 215.4KiB (rounded to 220672)requested by op SelfAdjointEigV2\r\n\r\n2022-01-03 16:18:32.518897: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \r\n2022-01-03 16:18:32.518906: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1074] 1 Chunks of size 256 totalling 256B\r\n2022-01-03 16:18:32.518916: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\r\n2022-01-03 16:18:32.518924: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1074] 1 Chunks of size 65536 totalling 64.0KiB\r\n2022-01-03 16:18:32.518930: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1074] 11841 Chunks of size 220672 totalling 2.43GiB\r\n2022-01-03 16:18:32.518937: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1074] 1 Chunks of size 251648 totalling 245.8KiB\r\n2022-01-03 16:18:32.518944: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1074] 1 Chunks of size 410880 totalling 401.2KiB\r\n2022-01-03 16:18:32.518951: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1074] 1 Chunks of size 4194304 totalling 4.00MiB\r\n2022-01-03 16:18:32.518958: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1074] 2 Chunks of size 268435456 totalling 512.00MiB\r\n2022-01-03 16:18:32.518964: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1078] Sum Total of in-use chunks: 2.94GiB\r\n2022-01-03 16:18:32.518970: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1080] total_region_allocated_bytes_: 3154771968 memory_limit_: 3154771968 available bytes: 0 curr_region_allocation_bytes_: 6309543936\r\n2022-01-03 16:18:32.518982: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1086] Stats: \r\nLimit:                      3154771968\r\nInUse:                      3154771968\r\nMaxInUse:                   3154771968\r\nNumAllocs:                       11850\r\nMaxAllocSize:                268435456\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2022-01-03 16:18:32.519244: W tensorflow\/core\/common_runtime\/bfc_allocator.cc:474] ****************************************************************************************************\r\n2022-01-03 16:18:32.519294: F .\/tensorflow\/core\/util\/gpu_solvers.h:533] Non-OK-status: context->allocate_temp(DataTypeToEnum<Scalar>::value, shape, &scratch_tensor_, alloc_attr) status: RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[55137] and type float on \/job:localhost\/replica:0\/task:0\/device:GPU:0 by allocator GPU_0_bfc\r\n```\r\n\r\nEspecially this line:  \r\n```\r\n...r.cc:1074] 11841 Chunks of size 220672 totalling 2.43GiB\r\n```\r\nSo here, I conclude that it crashes when it's trying to process matrix number 11842 out of 16384.\r\n\r\nPointers:\r\n - ScratchSpace request in the `HeevdImpl` call (at line 635): https:\/\/github.com\/tensorflow\/tensorflow\/blob\/322cba072c9313689aeb2fc3174f18fce57194d7\/tensorflow\/core\/util\/cuda_solvers.cc#L620-L643\r\n - Batch processing matrix by matrix: https:\/\/github.com\/tensorflow\/tensorflow\/blob\/322cba072c9313689aeb2fc3174f18fce57194d7\/tensorflow\/core\/kernels\/linalg\/self_adjoint_eig_v2_op_gpu.cc#L130-L140","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","comp:core","TF 2.9"],"created_at":"2022-01-03T15:35:17Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/53615"},{"issue_number":435,"repository":"tensorflow\/tensorflow","title":"tf.data.Dataset .map().batch() pattern is not matched to use fused implementation.","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): v1.12.1-69264-g0cdf35562dc 2.9.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source):\r\n- GCC\/Compiler version (if compiling from source):\r\n- CUDA\/cuDNN version: 11.5 \/ 8.3\r\n- GPU model and memory: GTX1660 Ti\r\n\r\n**Describe the current behavior**\r\ncombining `tf.data.Dataset.map()` with `.batch()` does not use the fused BatchAndMap implementation.\r\n\r\n**Describe the expected behavior**\r\nIt does use the fused implementation. Currently, it's only possible to use the fused implementation when using the deprecated `experimental.map_and_batch()` transformation.\r\n\r\n**[Contributing](https:\/\/www.tensorflow.org\/community\/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes\/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```py\r\nimport os\r\nimport datetime\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\nprint('TF version', tf.__version__)\r\n\r\ngpus = tf.config.experimental.list_physical_devices('GPU')\r\nif gpus:\r\n    try:\r\n        # Currently, memory growth needs to be the same across GPUs\r\n        for gpu in gpus:\r\n            tf.config.experimental.set_memory_growth(gpu, True)\r\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n        print(len(gpus), 'Physical GPUs,', len(logical_gpus), 'Logical GPUs')\r\n    except RuntimeError as e:\r\n        # Memory growth must be set before GPUs have been initialized\r\n        print(e)\r\n\r\n\r\n@tf.function\r\ndef do_stuff(wmat, tf_var):\r\n    with tf.device(\"\/gpu:0\"):\r\n        S = tf.constant(0.0)\r\n        for i in tf.range(4):\r\n            fi = tf.cast(i, dtype=tf.float32)\r\n            A = tf.math.lgamma(tf.tanh(tf.matmul(wmat + fi, tf.transpose(wmat - fi, [0, 2, 1]))))\r\n            S += tf.reduce_sum(A)\r\n        error = tf.reduce_mean(tf_var)\r\n        return error, S\r\n\r\nexp_uuid = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n\r\nn_batches = 512\r\n\r\n\r\ndef gen():\r\n    for i in range(n_batches):\r\n        with tf.device(\"\/cpu:0\"): # Make sure it comes from CPU\r\n            r = tf.ones((400,800))\r\n        yield r\r\n\r\noption_names = ['map().batch()', 'map_and_batch()']\r\nfor option in range(2):\r\n\r\n    with tf.device(\"\/cpu:0\"):\r\n        dataset = tf.data.Dataset.from_generator(gen, output_types=tf.float32)\r\n\r\n        def my_identity(x):\r\n            with tf.device(\"\/cpu:0\"):\r\n                print(\"my_identity input:\", x, x.device)\r\n                y = tf.identity(x)\r\n                print(\"my_identity output:\", y, y.device)\r\n                return y\r\n\r\n        if option == 0:\r\n            ## Option 0: map().batch()\r\n            dataset = dataset.map(my_identity).batch(16)\r\n\r\n        elif option == 1:\r\n            ## Option 1: deprecated map_and_batch()\r\n            dataset = dataset.apply(tf.data.experimental.map_and_batch(my_identity, 16))\r\n\r\n    gpu_transform = tf.data.experimental.prefetch_to_device('\/gpu:0', buffer_size=4)\r\n    dataset = dataset.apply(gpu_transform)\r\n\r\n\r\n    tf_var = tf.Variable(tf.zeros(3))\r\n    adam = tf.keras.optimizers.Adam(1e-4)\r\n    logpath = os.path.join('data', 'logs', 'pa_' + exp_uuid + '_' + option_names[option])\r\n\r\n    tf.profiler.experimental.start(logpath)\r\n    start = datetime.datetime.now()\r\n    for b, wmat in tqdm(enumerate(dataset)):\r\n        with tf.GradientTape() as tape:\r\n\r\n            if b == 0:\r\n                print('\\n\\n dataset element device', wmat.device)\r\n                print('\\n')\r\n\r\n            # Do some calculations\r\n            result = do_stuff(wmat, tf_var)\r\n\r\n        grads = tape.gradient(result[0], [tf_var])\r\n        adam.apply_gradients(zip(grads, [tf_var]))\r\n    stop = datetime.datetime.now()\r\n    tf.profiler.experimental.stop()\r\n\r\n    print(f'\\n\\nOption {option_names[option]}\\n===========================\\n')\r\n    print(logpath)\r\n    print('Time lapsed=', stop - start)\r\n    print(\"\\n\\n\")\r\n```\r\n\r\n**Other info \/ logs**\r\n\r\n**Option 1**:\r\n![image](https:\/\/user-images.githubusercontent.com\/845012\/147661299-a7f72017-00ff-47b6-bb71-8812bd5163d3.png)\r\nSymptoms:\r\n - See the blocks `Iterator::FlapMap` and `Iterator::BatchV2` stacked on top of each other.\r\n - The MemcpyH2D (selected, see the details panel) is comping from pagable memory, instead of pinned memory (which is what MapAndBatch does). Because of the source being pagable memory, it can't overlap with kernel computations.\r\n \r\n**Option 2**:\r\n![image](https:\/\/user-images.githubusercontent.com\/845012\/147661558-9f316201-a4e5-4df1-a77b-032272537321.png)\r\nEvidence:\r\n - The MapAndBatch block is used.\r\n - The MemcopyH2D comes from pinned memory (see details pane) and overlaps with kernel computations.\r\n\r\nThe whole deal about pinned memory is to allow parallel data upload and kernel computations. So the dataset needs to be produced into pinned host memory, which then can be uploaded asynchronously by the driver without an extra copy. See https:\/\/github.com\/tensorflow\/tensorflow\/issues\/43905#issuecomment-823675760 and https:\/\/github.com\/tensorflow\/tensorflow\/issues\/43905#issuecomment-824145184 and:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/40e9b534962989af7486bc6567ca472d71eb5049\/tensorflow\/core\/kernels\/data\/experimental\/map_and_batch_dataset_op.cc#L522\r\n\r\nThis is a follow up on https:\/\/github.com\/tensorflow\/tensorflow\/issues\/43905.\r\n","labels":["comp:tensorboard","type:bug","comp:data","TF 2.7"],"created_at":"2021-12-29T12:25:16Z","comments":12,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/53572"},{"issue_number":436,"repository":"tensorflow\/tensorflow","title":"Tensorflow conv1d computed incorrectly on GPU","description":"<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/ISSUES.md),\r\nwe only address code\/doc bugs, performance issues, feature requests and\r\nbuild\/installation issues on GitHub. tag:bug_template<\/em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \r\n- TensorFlow installed from (source or binary):  binary\r\n- TensorFlow version (use command below): 2.7\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC\/Compiler version (if compiling from source):\r\n- CUDA\/cuDNN version: Cuda-11.2\r\n- GPU model and memory: Tesla K80\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https:\/\/github.com\/tensorflow\/tensorflow\/tree\/master\/tools\/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\nComputing a single filter 1d convolution on GPU with kernel_size=15, padding='valid' and strides=1 on a (1, 64, 32) input tensor (1 sample X 64 coordinates X 32 input channels) results in data from the end of the input tensor effecting the beginning of the output vector. \r\n\r\nThis behavior only happens when performing the calculation on GPU (specifically nvidia Telsa K80). \r\nOn CPU results are fine.\r\n\r\n**Describe the expected behavior**\r\nReceptive field should be limited to the kernel size. \r\n\r\n**[Contributing](https:\/\/www.tensorflow.org\/community\/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes\/no): no\r\n- Briefly describe your candidate solution(if contributing):\r\n\r\n**Standalone code to reproduce the issue**\r\nIssue reproduces on colab with GPU:\r\nhttps:\/\/colab.research.google.com\/drive\/1pK1tNxWVtC9qilcX0u5sbpGmQEZiCDEX?usp=sharing\r\n\r\n\r\n**Other info \/ logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n[Tensorflow-bug.ipynb - Colaboratory.pdf](https:\/\/github.com\/tensorflow\/tensorflow\/files\/7769397\/Tensorflow-bug.ipynb.-.Colaboratory.pdf)\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.7"],"created_at":"2021-12-23T13:10:33Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/53533"},{"issue_number":437,"repository":"tensorflow\/tensorflow","title":"issue about XLA compile MirroredStrategy","description":"System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): I have tested on Ubuntu 18.04.\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.7.0\r\n- Python version: 3.7\r\n- CUDA\/cuDNN version: 11.0\r\n- GPU model and memory: Tesla P100\r\n\r\n**Describe the current behavior**\r\nWhen I train my model on multi-gpu with XLA compiling below error is occurred.\r\n\r\n```\r\n021-11-20 12:57:08.333476: I tensorflow\/core\/platform\/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-11-20 12:57:09.302772: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1525] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 15397 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 6.0\r\n2021-11-20 12:57:09.303502: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1525] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 15397 MB memory:  -> device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:b1:00.0, compute capability: 6.0\r\n2021-11-20 12:57:10.556310: W tensorflow\/core\/framework\/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:241 : INVALID_ARGUMENT: Trying to access resource _AnonymousVar3 (defined @ \/home\/sdb\/wda\/tf_xla\/lib\/python3.7\/site-packages\/keras\/engine\/base_layer_utils.py:129) located in device \/job:localhost\/replica:0\/task:0\/device:GPU:1 from device \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nTraceback (most recent call last):\r\n  File \"\/home\/sdb\/wda\/TF2-jit-compile-on-multi-gpu\/xla_tf_function_distributed.py\", line 59, in <module>\r\n    train_step_dist(images, labels)\r\n  File \"\/home\/sdb\/wda\/tf_xla\/lib\/python3.7\/site-packages\/tensorflow\/python\/util\/traceback_utils.py\", line 153, in error_handler\r\n    raise e.with_traceback(filtered_tb) from None\r\n  File \"\/home\/sdb\/wda\/tf_xla\/lib\/python3.7\/site-packages\/tensorflow\/python\/eager\/execute.py\", line 59, in quick_execute\r\n    inputs, attrs, num_outputs)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource _AnonymousVar3 (defined @ \/home\/sdb\/wda\/tf_xla\/lib\/python3.7\/site-packages\/keras\/engine\/base_layer_utils.py:129) located in device \/job:localhost\/replica:0\/task:0\/device:GPU:1 from device \/job:localhost\/replica:0\/task:0\/device:GPU:0 [Op:__inference_train_step_dist_650]\r\n```\r\n\r\nDescribe the expected behavior\r\nI want to use XLA  compile MirroredStrategy\uff0c because I found that _XLA can now compile MirroredStrategy: the step function passed to`strategy.run` can now be annoted with `jit_compile=True`._ from RELEASE.md from  2.5.0\r\n\r\nStandalone code to reproduce the issue:\r\n\r\n```\r\nimport tensorflow as tf\r\ntf.compat.v1.enable_eager_execution()\r\n\r\n# Size of each input image, 28 x 28 pixels\r\nIMAGE_SIZE = 28 * 28\r\n# Number of distinct number labels, [0..9]\r\nNUM_CLASSES = 10\r\n# Number of examples in each training batch (step)\r\nTRAIN_BATCH_SIZE = 100\r\n# Number of training steps to run\r\nTRAIN_STEPS = 1000\r\n\r\n# Loads MNIST dataset.\r\ntrain, test = tf.keras.datasets.mnist.load_data()\r\ntrain_ds = tf.data.Dataset.from_tensor_slices(train).batch(TRAIN_BATCH_SIZE).repeat()\r\n\r\n\r\n# Casting from raw data to the required datatypes.\r\ndef cast(images, labels):\r\n    images = tf.cast(\r\n        tf.reshape(images, [-1, IMAGE_SIZE]), tf.float32)\r\n    labels = tf.cast(labels, tf.int64)\r\n    return (images, labels)\r\n\r\n\r\nlayer = tf.keras.layers.Dense(NUM_CLASSES)\r\noptimizer = tf.keras.optimizers.Adam()\r\n\r\n\r\n@tf.function(jit_compile=True)\r\ndef compiled_step(images, labels):\r\n    images, labels = cast(images, labels)\r\n\r\n    with tf.GradientTape() as tape:\r\n        predicted_labels = layer(images)\r\n        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n            logits=predicted_labels, labels=labels\r\n        ))\r\n    gradients = tape.gradient(loss, layer.trainable_variables)\r\n    return loss, predicted_labels, gradients\r\n\r\n@tf.function()\r\ndef train_step(images, labels):\r\n    loss, pred, gradients = compiled_step(images, labels)\r\n    optimizer.apply_gradients(zip(gradients, layer.trainable_variables))\r\n\r\n\r\nstrategy = tf.distribute.MirroredStrategy()\r\n\r\n@tf.function(jit_compile=True)\r\ndef train_step_dist(image, labels):\r\n    strategy.run(train_step, args=(image, labels))\r\n\r\n\r\nfor images, labels in train_ds:\r\n    if optimizer.iterations > TRAIN_STEPS:\r\n        break\r\n    train_step_dist(images, labels)\r\n\r\n```","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.7"],"created_at":"2021-11-20T05:06:22Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/53140"},{"issue_number":438,"repository":"tensorflow\/tensorflow","title":"Issue with conversion of dilated Convolutions #29509 still happening in version 2.6.0","description":"### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- TensorFlow installation (pip package or built from source): pip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6.0\r\n\r\n### 2. Code\r\nStandalone code to reproduce the issue - \r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow_model_optimization.python.core.quantization.keras import quantize\r\nfrom tensorflow.python import keras\r\n\r\nl = tf.keras.layers\r\n\r\ntf.config.run_functions_eagerly(True)\r\n\r\ndef functional_model():\r\n    \"\"\"Builds an MNIST functional model.\"\"\"\r\n    inp = tf.keras.Input(shape=image_input_shape())\r\n    x = l.Conv2D(filters=32, kernel_size=5, padding='same', activation='relu',\r\n                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(inp)\r\n    x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\r\n    # TODO(pulkitb): Add BatchNorm when transformations are ready.\r\n    # x = l.BatchNormalization()(x)\r\n    x = l.Conv2D(filters=64, kernel_size=5, padding='same', activation='relu',\r\n                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)\r\n    x = l.Conv2D(filters=64, kernel_size=3, dilation_rate=(3, 3), padding='same', activation='relu',\r\n                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)\r\n    x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)\r\n    x = l.Flatten()(x)\r\n    x = l.Dense(1024, activation='relu')(x)\r\n    x = l.Dropout(0.4)(x)\r\n    out = l.Dense(10, activation='softmax')(x)\r\n\r\n    return tf.keras.Model(inp, [out])\r\n\r\n\r\ndef image_input_shape(img_rows=28, img_cols=28):\r\n    if tf.keras.backend.image_data_format() == 'channels_first':\r\n        return 1, img_rows, img_cols\r\n    else:\r\n        return img_rows, img_cols, 1\r\n\r\n\r\ndef preprocessed_data(img_rows=28,\r\n                      img_cols=28,\r\n                      num_classes=10):\r\n    \"\"\"Get data for mnist training and evaluation.\"\"\"\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\n    if tf.keras.backend.image_data_format() == 'channels_first':\r\n        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n    else:\r\n        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n\r\n    x_train = x_train.astype('float32')\r\n    x_test = x_test.astype('float32')\r\n    x_train \/= 255\r\n    x_test \/= 255\r\n\r\n    # convert class vectors to binary class matrices\r\n    y_train = tf.keras.utils.to_categorical(y_train, num_classes)\r\n    y_test = tf.keras.utils.to_categorical(y_test, num_classes)\r\n\r\n    return x_train, y_train, x_test, y_test\r\n\r\n\r\nmodel = functional_model()\r\nmodel.summary()\r\nx_train, y_train, x_test, y_test = preprocessed_data()\r\n\r\nmodel.compile(\r\n    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, batch_size=500)\r\n_, model_accuracy = model.evaluate(x_test, y_test, verbose=0)\r\n\r\nprint(\"Quantizing model\")\r\n\r\nquantized_model = quantize.quantize_model(model)\r\nquantized_model.compile(\r\n    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\r\n\r\nquantized_model.fit(x_train, y_train, batch_size=500)\r\n_, quantized_model_accuracy = quantized_model.evaluate(\r\n    x_test, y_test, verbose=0)\r\nmodel.save(\"\/home\/anurag\/git\/train_data\/testOrig.h5\")\r\nquantized_model.save(\"\/home\/anurag\/git\/train_data\/test.h5\")\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\nconverter.change_concat_input_ranges = True\r\nconverter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\r\ntfliteModel = converter.convert()\r\nwith open(\"\/home\/anurag\/git\/train_data\/test.tflite\", 'wb') as outfile:\r\n    outfile.write(tfliteModel)\r\n```\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- The latency in model is high due to the fact that atrous convolution is being broken down to spacetodepth, conv2d and depth to space and then does not apply quantization to conv2d.\r\n- Model produces correct results, but it is slower than expected.\r\n\r\nThis is the same as #29509. Issue was solved but appears again on newer releases.","labels":["stat:awaiting tensorflower","type:bug","TFLiteConverter","2.6.0"],"created_at":"2021-11-10T22:40:29Z","comments":15,"reactions":25,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/53025"},{"issue_number":439,"repository":"tensorflow\/tensorflow","title":"Model checkpoint load ignores wrong shape","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 11.6 + Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below):  v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.9.6 + 3.8\r\n- Bazel version (if compiling from source): -\r\n- GCC\/Compiler version (if compiling from source): -\r\n- CUDA\/cuDNN version: None\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nSaving a variable of shape (3,3,40,32) into a checkpoint, and then loading the variable with shape (3,3,1,32) from this checkpoint does not cause an error. Instead, it loads fine. The variable shape still claims to be (3,3,1,32), but when evaluated, one can see that it is indeed (3,3,40,32).\r\n\r\nSo there does not seem to be any validation of the shape during checkpoint loading.\r\n\r\n**Describe the expected behavior**\r\n\r\nIn general, if a tensor or variable claims to be of some static shape, I think it should never be possible that its actual shape when evaluated is different.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\n\r\nimport tensorflow as tf\r\n\r\nprint(\"TF:\", tf.__version__)\r\ntf.compat.v1.disable_eager_execution()\r\n\r\nfilename = \"test-ckpt-diff-shape.model\"\r\n\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    with tf.compat.v1.Session(graph=graph) as session:\r\n        shape1 = (3,3,40,32)\r\n        v = tf.compat.v1.get_variable(name=\"W\", shape=shape1)\r\n        print(v)\r\n        saver = tf.compat.v1.train.Saver(var_list=[v])\r\n        session.run(tf.compat.v1.global_variables_initializer())\r\n        saver.save(sess=session, save_path=filename)\r\n\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    with tf.compat.v1.Session(graph=graph) as session:\r\n        shape2 = (3,3,1,32)\r\n        v = tf.compat.v1.get_variable(name=\"W\", shape=shape2)\r\n        print(v)\r\n        saver = tf.compat.v1.train.Saver(var_list=[v])\r\n        saver.restore(sess=session, save_path=filename)\r\n        v_raw = session.run(v)\r\n        print(v)\r\n        print(v_raw.shape)\r\n        assert v.shape.as_list() == list(shape2)\r\n        assert v.shape.as_list() == list(v_raw.shape)\r\n```\r\n\r\n**Other info \/ logs**\r\n\r\nI stumbled upon this problem because it causes a seemingly unrelated error for some following 2D convolution where this variable is used as a kernel:\r\n```\r\n2021-09-30 12:52:25.768174: W tensorflow\/core\/framework\/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops.cc:1\r\n115 : Not found: No algorithm worked!\r\nTensorFlow exception: 2 root error(s) found.\r\n  (0) Not found: No algorithm worked!\r\n         [[node conv0\/convolution (defined at u\/schmitt\/src\/returnn\/returnn\/tf\/layers\/basic.py:4061) ]]\r\n         [[output\/rec\/while\/Switch_14\/_553]]\r\n  (1) Not found: No algorithm worked!\r\n         [[node conv0\/convolution (defined at u\/schmitt\/src\/returnn\/returnn\/tf\/layers\/basic.py:4061) ]]\r\n0 successful operations.\r\n```\r\nThis was reported here: https:\/\/github.com\/rwth-i6\/returnn\/issues\/703\r\n\r\nThere are a couple of these errors also reported here:\r\n* https:\/\/github.com\/tensorflow\/tensorflow\/issues\/43174\r\n* https:\/\/github.com\/tensorflow\/tensorflow\/issues\/45044\r\n* https:\/\/github.com\/tensorflow\/tensorflow\/issues\/48117\r\n\r\nI wonder if some of them have a similar source.\r\n\r\nBut anyway, this issue is not really about the convolution error, but about the checkpoint loading behavior, and the variable shape.\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.6.0"],"created_at":"2021-10-01T19:37:38Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/52220"},{"issue_number":440,"repository":"tensorflow\/tensorflow","title":"No registered 'Const' OpKernel for GPU devices with constant folding","description":"\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): pip binary\r\n- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0\r\n- Python version: 3.8.10\r\n- Bazel version (if compiling from source):\r\n- GCC\/Compiler version (if compiling from source):\r\n- CUDA\/cuDNN version: 11.4 \/ 8.2.4.15\r\n- GPU model and memory: NVIDIA GeForce RTX 2070 \r\n\r\n**Describe the current behavior**\r\n\r\nThe code below fails with an exception.\r\nThis is the full output:\r\n```\r\nTF: 2.6.0\r\n2021-09-30 15:52:24.159169: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.162278: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.162637: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.163155: I tensorflow\/core\/platform\/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2021-09-30 15:52:24.163754: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.164103: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.164431: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.456691: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.457036: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.457342: I tensorflow\/stream_executor\/cuda\/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2021-09-30 15:52:24.457640: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1510] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 5732 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:09:00.0, compute capability: 7.5\r\n2021-09-30 15:52:24.466132: W tensorflow\/core\/grappler\/utils\/graph_view.cc:836] No registered 'Const' OpKernel for GPU devices compatible with node {{node ConstantFolding\/Const_enter}}\r\n         (OpKernel was found, but attributes didn't match) Requested Attributes: dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=\"\/job:localhost\/replica:0\/task:0\/device:GPU:0\"\r\n        .  Registered:  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_VARIANT]\r\n  device='DEFAULT'; dtype in [DT_BOOL]\r\n  device='DEFAULT'; dtype in [DT_QUINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT32]\r\n  device='DEFAULT'; dtype in [DT_QUINT8]\r\n  device='DEFAULT'; dtype in [DT_QINT8]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX128]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX64]\r\n  device='DEFAULT'; dtype in [DT_INT8]\r\n  device='DEFAULT'; dtype in [DT_UINT8]\r\n  device='DEFAULT'; dtype in [DT_INT16]\r\n  device='DEFAULT'; dtype in [DT_UINT16]\r\n  device='DEFAULT'; dtype in [DT_UINT32]\r\n  device='DEFAULT'; dtype in [DT_INT64]\r\n  device='DEFAULT'; dtype in [DT_UINT64]\r\n  device='DEFAULT'; dtype in [DT_DOUBLE]\r\n  device='DEFAULT'; dtype in [DT_FLOAT]\r\n  device='DEFAULT'; dtype in [DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_HALF]\r\n  device='DEFAULT'; dtype in [DT_INT32]\r\n  device='CPU'\r\n  device='TPU_SYSTEM'\r\n  device='GPU'; dtype in [DT_VARIANT]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_UINT64]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_QINT32]\r\n  device='GPU'; dtype in [DT_UINT32]\r\n  device='GPU'; dtype in [DT_QUINT16]\r\n  device='GPU'; dtype in [DT_QINT16]\r\n  device='GPU'; dtype in [DT_INT16]\r\n  device='GPU'; dtype in [DT_UINT16]\r\n  device='GPU'; dtype in [DT_QINT8]\r\n  device='GPU'; dtype in [DT_INT8]\r\n  device='GPU'; dtype in [DT_UINT8]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_BFLOAT16]\r\n  device='GPU'; dtype in [DT_HALF]\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/az\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/client\/session.py\", line 1375, in _do_call\r\n    return fn(*args)\r\n  File \"\/home\/az\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/client\/session.py\", line 1359, in _run_fn\r\n    return self._call_tf_sessionrun(options, feed_dict, fetch_list,\r\n  File \"\/home\/az\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/client\/session.py\", line 1451, in _call_tf_sessionrun\r\n    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'Const' OpKernel for 'GPU' devices compatible with node {{node ConstantFolding\/Const_enter}}\r\n         (OpKernel was found, but attributes didn't match) Requested Attributes: _XlaHasReferenceVars=false, dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=\"\/job:localhost\/replica:0\/task:0\/device:GPU:0\"\r\n        .  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_VARIANT]\r\n  device='DEFAULT'; dtype in [DT_BOOL]\r\n  device='DEFAULT'; dtype in [DT_QUINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT32]\r\n  device='DEFAULT'; dtype in [DT_QUINT8]\r\n  device='DEFAULT'; dtype in [DT_QINT8]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX128]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX64]\r\n  device='DEFAULT'; dtype in [DT_INT8]\r\n  device='DEFAULT'; dtype in [DT_UINT8]\r\n  device='DEFAULT'; dtype in [DT_INT16]\r\n  device='DEFAULT'; dtype in [DT_UINT16]\r\n  device='DEFAULT'; dtype in [DT_UINT32]\r\n  device='DEFAULT'; dtype in [DT_INT64]\r\n  device='DEFAULT'; dtype in [DT_UINT64]\r\n  device='DEFAULT'; dtype in [DT_DOUBLE]\r\n  device='DEFAULT'; dtype in [DT_FLOAT]\r\n  device='DEFAULT'; dtype in [DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_HALF]\r\n  device='DEFAULT'; dtype in [DT_INT32]\r\n  device='CPU'\r\n  device='TPU_SYSTEM'\r\n  device='GPU'; dtype in [DT_VARIANT]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_UINT64]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_QINT32]\r\n  device='GPU'; dtype in [DT_UINT32]\r\n  device='GPU'; dtype in [DT_QUINT16]\r\n  device='GPU'; dtype in [DT_QINT16]\r\n  device='GPU'; dtype in [DT_INT16]\r\n  device='GPU'; dtype in [DT_UINT16]\r\n  device='GPU'; dtype in [DT_QINT8]\r\n  device='GPU'; dtype in [DT_INT8]\r\n  device='GPU'; dtype in [DT_UINT8]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_BFLOAT16]\r\n  device='GPU'; dtype in [DT_HALF]\r\n\r\n         [[ConstantFolding\/Const_enter]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"tf-const-gpu.py\", line 18, in <module>\r\n    session.run(n)\r\n  File \"\/home\/az\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/client\/session.py\", line 967, in run\r\n    result = self._run(None, fetches, feed_dict, options_ptr,\r\n  File \"\/home\/az\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/client\/session.py\", line 1190, in _run\r\n    results = self._do_run(handle, final_targets, final_fetches,\r\n  File \"\/home\/az\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/client\/session.py\", line 1368, in _do_run\r\n    return self._do_call(_run_fn, feeds, fetches, targets, options,\r\n  File \"\/home\/az\/.local\/lib\/python3.8\/site-packages\/tensorflow\/python\/client\/session.py\", line 1394, in _do_call\r\n    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'Const' OpKernel for 'GPU' devices compatible with node {{node ConstantFolding\/Const_enter}}\r\n         (OpKernel was found, but attributes didn't match) Requested Attributes: _XlaHasReferenceVars=false, dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=\"\/job:localhost\/replica:0\/task:0\/device:GPU:0\"\r\n        .  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]\r\n  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_VARIANT]\r\n  device='DEFAULT'; dtype in [DT_BOOL]\r\n  device='DEFAULT'; dtype in [DT_QUINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT16]\r\n  device='DEFAULT'; dtype in [DT_QINT32]\r\n  device='DEFAULT'; dtype in [DT_QUINT8]\r\n  device='DEFAULT'; dtype in [DT_QINT8]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX128]\r\n  device='DEFAULT'; dtype in [DT_COMPLEX64]\r\n  device='DEFAULT'; dtype in [DT_INT8]\r\n  device='DEFAULT'; dtype in [DT_UINT8]\r\n  device='DEFAULT'; dtype in [DT_INT16]\r\n  device='DEFAULT'; dtype in [DT_UINT16]\r\n  device='DEFAULT'; dtype in [DT_UINT32]\r\n  device='DEFAULT'; dtype in [DT_INT64]\r\n  device='DEFAULT'; dtype in [DT_UINT64]\r\n  device='DEFAULT'; dtype in [DT_DOUBLE]\r\n  device='DEFAULT'; dtype in [DT_FLOAT]\r\n  device='DEFAULT'; dtype in [DT_BFLOAT16]\r\n  device='DEFAULT'; dtype in [DT_HALF]\r\n  device='DEFAULT'; dtype in [DT_INT32]\r\n  device='CPU'\r\n  device='TPU_SYSTEM'\r\n  device='GPU'; dtype in [DT_VARIANT]\r\n  device='GPU'; dtype in [DT_BOOL]\r\n  device='GPU'; dtype in [DT_COMPLEX128]\r\n  device='GPU'; dtype in [DT_COMPLEX64]\r\n  device='GPU'; dtype in [DT_UINT64]\r\n  device='GPU'; dtype in [DT_INT64]\r\n  device='GPU'; dtype in [DT_QINT32]\r\n  device='GPU'; dtype in [DT_UINT32]\r\n  device='GPU'; dtype in [DT_QUINT16]\r\n  device='GPU'; dtype in [DT_QINT16]\r\n  device='GPU'; dtype in [DT_INT16]\r\n  device='GPU'; dtype in [DT_UINT16]\r\n  device='GPU'; dtype in [DT_QINT8]\r\n  device='GPU'; dtype in [DT_INT8]\r\n  device='GPU'; dtype in [DT_UINT8]\r\n  device='GPU'; dtype in [DT_DOUBLE]\r\n  device='GPU'; dtype in [DT_FLOAT]\r\n  device='GPU'; dtype in [DT_BFLOAT16]\r\n  device='GPU'; dtype in [DT_HALF]\r\n\r\n         [[ConstantFolding\/Const_enter]]\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe code below should work without error on a GPU.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\n\r\nprint(\"TF:\", tf.__version__)\r\ntf.compat.v1.disable_eager_execution()\r\ntf.compat.v1.disable_control_flow_v2()\r\n\r\n\r\nwith tf.compat.v1.Session() as session:\r\n  x = tf.constant(\"foo\")\r\n\r\n  def body(i):\r\n    with tf.control_dependencies([tf.print(x)]):\r\n      return i + 1\r\n\r\n  n = tf.while_loop(cond=lambda i: tf.less(i, 1), body=body, loop_vars=[0])\r\n  session.run(n)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","2.6.0"],"created_at":"2021-09-30T13:54:11Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/52200"},{"issue_number":441,"repository":"tensorflow\/tensorflow","title":"unable to create tf.variables inside a function that is decorated with @tf.function","description":"tf 2.5\r\n\r\n\r\n```\r\n@tf.function\r\ndef weight_fn():\r\n    w = tf.Variable(tf.truncated_normal())\r\n```\r\n\r\nI have a function like above that would be called about **50 times**, each time it should generate a new variable and return. But according to the [rule](https:\/\/tensorflow.google.cn\/guide\/function#creating_tfvariables) and the hint below,\r\n\r\n```\r\n    ValueError: A tf.Variable created inside your tf.function has been garbage-collected. Your code needs to keep Python references to variables created inside `tf.function`s.\r\n    \r\n    A common way to raise this error is to create and return a variable only referenced inside your function:\r\n    \r\n    @tf.function\r\n    def f():\r\n      v = tf.Variable(1.0)\r\n      return v\r\n    \r\n    v = f()  # Crashes with this error message!\r\n    \r\n    The reason this crashes is that @tf.function annotated function returns a **`tf.Tensor`** with the **value** of the variable when the function is called rather than the variable instance itself. As such there is no code holding a reference to the `v` created inside the function and Python garbage collects it.\r\n    \r\n    The simplest way to fix this issue is to create variables outside the function and capture them:\r\n    \r\n    v = tf.Variable(1.0)\r\n    \r\n    @tf.function\r\n    def f():\r\n      return v\r\n    \r\n    f()  # <tf.Tensor: numpy=1.>\r\n    v.assign_add(1.)\r\n    f()  # <tf.Tensor: numpy=2.>\r\n\r\n```\r\n\r\nI should define the weight variable outside the tf.function, which means I should manually define over 50 weight variables, each line with a weight variable.\r\n\r\n```\r\nw1 = tf.Variable(tf.truncated_normal())\r\nw2 = tf.Variable(tf.truncated_normal())\r\nw3 = tf.Variable(tf.truncated_normal())\r\n......\r\nw50 = tf.Variable(tf.truncated_normal())\r\n\r\n```\r\n\r\nUndoubtedly, this kind of behavior is really stupid, any solutions to this kind of unreasonable rule?\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","TF 2.5","comp:tf.function"],"created_at":"2021-08-12T12:20:06Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/51453"},{"issue_number":442,"repository":"tensorflow\/tensorflow","title":"Inconsistent eager\/tf.function behavior for rank 0 shape in tf.reshape","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8\r\n\r\n**Describe the current behavior**\r\n\r\nThe eager version of `tf.reshape` takes a rank 0 tensor as a shape parameter while the jitted (`tf.function` decorated) does not.\r\n\r\n**Describe the expected behavior**\r\n\r\nIt should be consistent, either fail in both or allow in both. To be consistent with other methods, I think it should fail in both.\r\n\r\n**[Contributing](https:\/\/www.tensorflow.org\/community\/contribute)**\r\n\r\n- Do you want to contribute a PR? (yes\/no): maybe\r\n- Briefly describe your candidate solution(if contributing): raise an error, such as done in other methods like `tf.random.uniform`\r\n\r\n**Standalone code to reproduce the issue**\r\n[executable example here](https:\/\/colab.research.google.com\/drive\/15wQJVDzEHc5puK6twrV9HzxcdWZWZPtC?usp=sharing)\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\ndef func():\r\n    return tf.reshape([[42]], 1)\r\n\r\n\r\nfunc_jit = tf.function(func=func)\r\n\r\nfunc()  # works\r\nfunc_jit()  # fails\r\n```","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.9"],"created_at":"2021-08-05T10:20:10Z","comments":8,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/51241"},{"issue_number":443,"repository":"tensorflow\/tensorflow","title":"Out of memory when using MultiWorkerMirroredStrategy","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): REHEL 7.9\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.7.1\r\n- GCC\/Compiler version (if compiling from source): 8.3.0\r\n- CUDA\/cuDNN version: 10.2\r\n- GPU model and memory: GTX 1080Ti\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running a trivial example TF starts allocation gigabytes of memory continously until it runs out of memory.\r\nThis does only happen when running on more than 2 nodes, not when running on only 1 or 2 nodes and it happens in the creation of MultiWorkerMirroredStrategy\r\nI can also start e.g. 6 tasks on 2 nodes (3 tasks each) without problems, but 3 tasks on 3 nodes (1 task each) does not work\r\n\r\nI also observed that it happens only one 1 of the ranks used, the other workers are fine and waiting for that one to finish init (I suppose)\r\n\r\n**Describe the expected behavior**\r\n\r\nIt works or a reasonable error.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nfrom mpi_cluster_resolver import MPIClusterResolver\r\n\r\nresolver = MPIClusterResolver()\r\nstrategy = tf.distribute.MultiWorkerMirroredStrategy(cluster_resolver=resolver)\r\n```\r\n\r\nWith \r\n[mpi_cluster_resolver.py.txt](https:\/\/github.com\/tensorflow\/tensorflow\/files\/6823734\/mpi_cluster_resolver.py.txt)\r\n\r\nThis is so that running distributed TF is possible via SLURM\/MPI on HPC clusters","labels":["stat:awaiting tensorflower","type:bug","comp:dist-strat","TF 2.5"],"created_at":"2021-07-15T14:14:24Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/50790"},{"issue_number":444,"repository":"tensorflow\/tensorflow","title":"Wrong gradient from complex determinant","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur, Version: 11.2.3 (20D91)\r\n- TensorFlow installed from (source or binary): pip\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nI implemented a straightforward example which illustrates the issue:\r\nI have a 2-dimensional tensor which is mapped to a complex tensor as:\r\n\r\nx_i -> z_i = (x_i, lamb_i * x_i)\r\n\r\nwith some real values for lambda = [lambda_1, lambda_2].\r\nAfterwards I calculate the Jacobian (with respect to x) and its determinant, which is\r\nalso easy to do as it is just a 2x2 matrix.\r\nIn the end, I take |det|^2 as final output L.\r\n\r\nAnalytically, you would now get for the gradient:\r\ngrad L = [ 2 * lambda_1 * (1+lambda_2^2), 2 * lambda_2 * (1+lambda_1^2)\r\n\r\nSo if you insert lambda_test = [ 1.0 , 2.0]\r\nYou should obtain: grad L(lambda_test) = [ 10, 8]\r\n\r\nHowever, Tensorflow yields: TF-Grad = [-14, -8.8],\r\nwhich obviously is completely off. \r\n\r\nAdditionally, we also implemented a simple numerical derivative ourselves (just the basic definition of the gradient in terms of difference quotient). This calculation does yield the correct gradient (within numerical uncertainties)\r\n\r\n**Standalone code to reproduce the issue**\r\nThe issue can be reproduced in this [gist](https:\/\/colab.research.google.com\/drive\/1C8PQKBWS-ykraftMqVq6MWmOjcmdQbhH?usp=sharing).\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:core","TF 2.9"],"created_at":"2021-06-01T09:48:41Z","comments":6,"reactions":3,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/49946"},{"issue_number":445,"repository":"tensorflow\/tensorflow","title":"QuantizedOpsTest.testAxis fails on cascade lake CPUs","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no \r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux RHEL 8.3\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.5.0\r\n- Python version: 3.8.6\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC\/Compiler version (if compiling from source): GCC 10.2.0\r\n- CUDA\/cuDNN version: None\r\n\r\n**Describe the current behavior**\r\n\r\nQuantizedOpsTest.testAxis fails on cascade lake systems when native optimizations are enabled\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nRun the TF test `\/\/tensorflow\/python:quantized_ops_test` through bazel\r\n\r\n**Other info \/ logs**\r\n```\r\nFAIL: testAxis (__main__.QuantizedOpsTest)\r\nQuantizedOpsTest.testAxis\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/dev\/shm\/build-branfosj-admin\/branfosj-admin-up\/TensorFlow\/2.5.0\/foss-2020b\/tmp8qildda2-bazel-tf\/1b512851602cc5932dcb1cd30c5fbde4\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/quantized_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/ops\/quantized_ops_test.py\", line 94, in testAxis\r\n    self.assertAllEqual(quantized, expected_quantized)\r\n  File \"\/dev\/shm\/build-branfosj-admin\/branfosj-admin-up\/TensorFlow\/2.5.0\/foss-2020b\/tmp8qildda2-bazel-tf\/1b512851602cc5932dcb1cd30c5fbde4\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/quantized_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_util.py\", line 1253, in decorated\r\n    return f(*args, **kwds)\r\n  File \"\/dev\/shm\/build-branfosj-admin\/branfosj-admin-up\/TensorFlow\/2.5.0\/foss-2020b\/tmp8qildda2-bazel-tf\/1b512851602cc5932dcb1cd30c5fbde4\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/python\/quantized_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/test_util.py\", line 2881, in assertAllEqual\r\n    np.testing.assert_array_equal(a, b, err_msg=\"\\n\".join(msgs))\r\n  File \"\/rds\/bear-apps\/devel\/eb-sjb-up\/EL8\/EL8-cas\/software\/SciPy-bundle\/2020.11-foss-2020b\/lib\/python3.8\/site-packages\/numpy\/testing\/_private\/utils.py\", line 930, in assert_array_equal\r\n    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,\r\n  File \"\/rds\/bear-apps\/devel\/eb-sjb-up\/EL8\/EL8-cas\/software\/SciPy-bundle\/2020.11-foss-2020b\/lib\/python3.8\/site-packages\/numpy\/testing\/_private\/utils.py\", line 840, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nArrays are not equal\r\n\r\nnot equal where = (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\r\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\r\n       2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0]), array([0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1,\r\n       1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1,\r\n       1, 1, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0]), array([0, 1, 2, 3, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,\r\n       1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,\r\n       3, 4, 0, 1, 2, 3, 4, 0, 0, 1, 2, 3]))\r\nnot equal lhs = array([ -64,    0,   38,  102,   38,  102,   71,   64,   64, -128,  -64,\r\n          0,  102,   71,   64, -128, -128,  -64,    0,   38,   64, -128,\r\n        -64,    0,    0,   38,  102,   71,    0,   38,  102,   71,   71,\r\n         64, -128,  -64,  102,   71,   64, -128, -128,  -64,    0,   38,\r\n         71,   64, -128,  -64,  -64,    0,   38,  102,   38,  102,   71,\r\n         64], dtype=int8)\r\nnot equal rhs = array([-128,  -64,    0,   38,  -64,    0,   38,  102,   71,   64, -128,\r\n        -64,    0,   38,  102,   71,   64, -128,  -64,    0,   38,  102,\r\n         71,   64, -128,  -64,    0,   38,  102,   71,   64, -128,  -64,\r\n          0,   38,  102,   71,   64, -128,  -64,    0,   38,  102,   71,\r\n         64, -128,  -64,    0,   38,  102,   71,   64,  102,   71,   64,\r\n       -128], dtype=int32)\r\nMismatched elements: 56 \/ 120 (46.7%)\r\nMax absolute difference: 230\r\nMax relative difference: 4.36842105\r\n x: array([[[[ -64,    0,   38,  102,  102],\r\n         [  71,   64, -128,   38,  102],\r\n         [  71,   64,   64, -128,  -64],...\r\n y: array([[[[-128,  -64,    0,   38,  102],\r\n         [  71,   64, -128,  -64,    0],\r\n         [  38,  102,   71,   64, -128],...\r\n```\r\n\r\nThis seems to be related to https:\/\/github.com\/tensorflow\/tensorflow\/issues\/47179 which can also only be observed on cascade lake systems.","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.9"],"created_at":"2021-06-01T08:35:45Z","comments":14,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/49944"},{"issue_number":446,"repository":"tensorflow\/tensorflow","title":"Compilation fails on Ubuntu 20.04 when using TensorRT 8. ","description":"**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\/A\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version: 2.4.1, 2.5, etc\r\n- Python version: 3.8\r\n- Installed using virtualenv? pip? conda?: no, built from source\r\n- Bazel version (if compiling from source): 3.1 (for TF 2.4.1), 3.7.2 (for TF 2.5.0-rcx)\r\n- GCC\/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\r\n- CUDA\/cuDNN version: Cuda 11.1, cudnn8 (8.0.5.39-1+cuda11.1) or Cuda-11-2, libcudnn 8.1.1, 8.2, \r\n- GPU model and memory: GTX-1080ti\r\n- TensorRT (crucial): 8.0.0-1+cuda11.0, or 8.0.0-1+cuda11.3\r\n\r\n**Describe the problem**\r\nWhen compiling with support for TensorRT 8 (via libnvinfer8), compilation fails (log is below). \r\n\r\n**Provide the exact sequence of commands \/ steps that you executed before running into the problem**\r\nWhen configuring the build, make sure you build with TensorRT support, and make sure TensorRT version 8 is selected. Build TF as usual. Compilation will fail. \r\n\r\nIf you install  TensorRT version 7 manually (from debs available for Ubuntu 18.04), compilation will complete just fine.\r\n\r\n**Any other info \/ logs**\r\nRelevant error: \r\n`C++ compilation of rule '\/\/tensorflow\/compiler\/tf2tensorrt:tensorrt_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command`\r\n\r\n`In file included from bazel-out\/k8-opt\/bin\/external\/local_config_tensorrt\/_virtual_includes\/tensorrt_headers\/third_party\/tensorrt\/NvInfer.h:54,\r\n                 from tensorflow\/compiler\/tf2tensorrt\/stub\/nvinfer_stub.cc:17:\r\nbazel-out\/k8-opt\/bin\/external\/local_config_tensorrt\/_virtual_includes\/tensorrt_headers\/third_party\/tensorrt\/NvInferRuntime.h:2264:51: note: from previous declaration 'nvinfer1::IPluginRegistry* getPluginRegistry() noexcept'\r\n 2264 | extern \"C\" TENSORRTAPI nvinfer1::IPluginRegistry* getPluginRegistry() noexcept;`\r\n\r\nFull log here: \r\n[gesdm-tf2.5.0rc3-error.txt](https:\/\/github.com\/tensorflow\/tensorflow\/files\/6469944\/gesdm-tf2.5.0rc3-error.txt)\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","subtype: ubuntu\/linux","comp:gpu:tensorrt","TF 2.5"],"created_at":"2021-05-13T02:02:32Z","comments":24,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/49150"},{"issue_number":447,"repository":"tensorflow\/tensorflow","title":"TF_AddGradientsWithPrefix doesn't seem to lock the gradients properly","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Using TF-Java\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\n- TensorFlow installed from (source or binary): Maven Central binary\r\n- TensorFlow version (use command below): v2.4.1\r\n\r\n**Describe the current behavior**\r\n\r\nWhen training a model I'm hitting a non-determinism issue, the models are initialized identically, and fed identical data in an identical order, with inter-op and intra-op parallelism both set to 1, but I can get complete training failure when building simple MLPs and CNNs on MNIST.\r\n\r\nTF-Java uses TF_AddGradientsWithPrefix to construct the gradients. We then wrap that in Optimizers to make a higher level front end to training, similar to the v1 optimizers package, and Keras in v2. As gradients are only available in Graph mode in the C API our code is most similar to the v1 optimizers package. I think we want to have the gating behaviour set to `GATE_OP` (https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/compat\/v1\/train\/Optimizer), but that's all implemented entirely in python, and the C API codepath doesn't seem to add the required control dependencies to ensure the gradients are all computed before being back-propped any further (in Python that's performed here - https:\/\/github.com\/tensorflow\/tensorflow\/blob\/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff\/tensorflow\/python\/ops\/gradients_util.py#L692, but there doesn't seem to be an equivalent in the C API here - https:\/\/github.com\/tensorflow\/tensorflow\/blob\/5dcfc51118817f27fad5246812d83e5dccdc5f72\/tensorflow\/cc\/framework\/gradients.cc#L568).\r\n\r\n**Describe the expected behavior**\r\n\r\nThe model should train identically with identical gradients.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nSee this test in TF-Java - https:\/\/github.com\/Craigacp\/tensorflow-java\/blob\/03402b30e271a46438c1feff12409095737f3d3e\/tensorflow-framework\/src\/test\/java\/org\/tensorflow\/framework\/optimizers\/GradientDescentTest.java#L121. This test when run complains that out of the 20 runs some of them diverged even when trained using identical graph defs on identical data, for an identical number of steps, when using single threaded computation.\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.8"],"created_at":"2021-04-30T19:39:37Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/48855"},{"issue_number":448,"repository":"tensorflow\/tensorflow","title":"tf.io.gfile.walk broken on Windows","description":"It seems that the path isn't split correctly. Filenames come with an extra `\\` prefix. Minimal reproducible example:\r\n\r\n```python3\r\nimport os\r\nimport tensorflow as tf\r\n\r\n\r\ntf.io.gfile.makedirs(\"ram:\/\/folder\")\r\nwith tf.io.gfile.GFile(\"ram:\/\/folder\/file.txt\", mode=\"w\") as f:\r\n    f.write(\"data\")\r\n\r\nfor root, _, filenames in tf.io.gfile.walk(\"ram:\/\/folder\"):\r\n    for filename in filenames:\r\n        assert tf.io.gfile.exists(os.path.join(root, filename))\r\n```\r\n\r\nThis passes on *nix but not on Windows. Here is a quick CI run in GitHub actions showing this: https:\/\/github.com\/adriangb\/tensorflow-test\/actions\/runs\/688190284\r\n\r\nccing @mihaimaruseac @bhack ","labels":["stat:awaiting tensorflower","type:bug","comp:ops"],"created_at":"2021-03-25T23:10:59Z","comments":38,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/48086"},{"issue_number":449,"repository":"tensorflow\/tensorflow","title":"Incompatibility between `set_visible_devices()` and `from_dlpack()`","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1\r\n- Python version: 3.8.8\r\n- CUDA\/cuDNN version: 11.2\r\n- GPU model and memory: GTX 1080Ti 11GB\r\n\r\n**Describe the current behavior**\r\n\r\nAfter selecting which GPU to use via `tf.config.experimental.set_visible_devices()`, converting external arrays from dlpack with `tf.experimental.dlpack.from_dlpack()` only works for device 0. Using other devices results in the error `InvalidArgumentError: GPU:1 unknown device.`\r\n\r\n**Describe the expected behavior**\r\n\r\nConverting external arrays from dlpack with `tf.experimental.dlpack.from_dlpack()` should work for all devices.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport cupy\r\nimport random\r\n\r\nimport tensorflow as tf\r\n\r\n# gpu_to_use = 0      # Works\r\ngpu_to_use = 1        # Errors\r\n\r\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\r\nif gpus:\r\n    tf.config.experimental.set_visible_devices(gpus[gpu_to_use], \"GPU\")\r\n\r\n# Converting from TF to CuPy with dlpack works for both devices\r\ntensor = tf.random.uniform((10,))\r\n\r\ndltensor = tf.experimental.dlpack.to_dlpack(tensor)\r\narray1 = cupy.fromDlpack(dltensor)\r\n\r\n# Converting from CuPy to TF with dlpack only works for device 0\r\narray1 = cupy.array([random.uniform(0.0, 1.0) for i in range(10)], dtype=cupy.float32)\r\ndltensor = array1.toDlpack()\r\nx = tf.experimental.dlpack.from_dlpack(dltensor)\r\n\r\n# Using device 1 results in the following error\r\n\r\n# Traceback (most recent call last):\r\n#   File \"examples\/multi-gpu\/tf-dlpack-repro.py\", line 22, in <module>\r\n#     x = tf.experimental.dlpack.from_dlpack(dltensor)\r\n#   File \"\/home\/karl\/miniconda3\/envs\/nvtabular_dev_11.0\/lib\/python3.8\/site-packages\/tensorflow\/python\/dlpack\/dlpack.py\", line 66, in from_dlpack\r\n#     return pywrap_tfe.TFE_FromDlpackCapsule(dlcapsule, context.context()._handle)\r\n# tensorflow.python.framework.errors_impl.InvalidArgumentError: GPU:1 unknown device.\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.4"],"created_at":"2021-03-17T16:28:56Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/47866"},{"issue_number":450,"repository":"tensorflow\/tensorflow","title":"unique operation on strings returns bogus indices","description":"**System information**\r\n- Have I written custom code: No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.9\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.6.8.\r\n- Bazel version (if compiling from source): 3.7.2\r\n- GCC\/Compiler version (if compiling from source): 8.3.0\r\n- CUDA\/cuDNN version: 10.1.243\r\n- GPU model and memory: None\r\n\r\n**Describe the current behavior**\r\n\r\nExecuting the test that runs `unique` on strings\/chars returns invalid values for the index tensor. Some values are plain out of bounds (very big or negative) and others seem to point to wrong values. This very much looks like some kind of memory corruption.\r\nSo far this has only been reproducible on a cascade lake CPU, no GPUs in the system but TF was built with CUDA support.\r\n\r\n**Describe the expected behavior**\r\n\r\nValid values returned\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\n    import numpy as np\r\n    from tensorflow.python.ops import array_ops\r\n\r\n    def testString():\r\n        indx = np.random.randint(65, high=122, size=7000)\r\n        x = [chr(i) for i in indx]\r\n        y, idx = array_ops.unique(x)\r\n        tf_y, tf_idx = (y.numpy(), idx.numpy())\r\n        print(','.join(str(i) for i in tf_idx))\r\n\r\n        assert len(x) == len(tf_idx)\r\n        assert len(tf_y) == len(np.unique(x))\r\n\r\n        for i in range(len(x)):\r\n            assert x[i] == tf_y[tf_idx[i]].decode('ascii')\r\n\r\n    testString()\r\n```\r\nThis test was extracted from the TF test suite as-is.\r\nErrors include the last assert failing as well as errors like `IndexError: index 7566446 is out of bounds for axis 0 with size 57`\r\n\r\n**Other info \/ logs**\r\n\r\nFor a part of the log from the TF test suite see https:\/\/gist.github.com\/boegel\/26f768c82080e593add3924fc7bc76cf\r\nThe `print` of `tf_idx` shows things like:\r\n`-15829468,126746879,293189,50530320,291592,3,-16616924,58720256,66085632,66085120,1,0,7352690,946952,1538050,51149349,10,126746628,11,10,7,33583628,-16551388,126747138,126746628,50491941,101480976,126746879,16742418,14,20,752147,19,134209791,126749222,17,126747141,3840,16,2,33585676,18,387021328,70664,219117604,486963728,27,66343024,50530320,24,50756133,27,126762239,4096,7,-16223708,126749187,33977217,20,9,4,293189,20,4,40,24,23,8,13,-16025575,31,26,2375,40,22,126749187,8,19,36,6,26,126746627,22,36,133292287,133292287,44,21,34,36,13,31,26,13,24,43,10,-16354780,31,41,419459596,38,39,8,1,43,2,44,1,4,11,2,45,7,22,41,29,6,18,39,126746630,5,8,3,1,26,14,40,36,32,8,988163,25,7,42,18,3,41,50,32,2,8,22,22,17,39,18,5,21,30,14,22,34,43,43,48,6,24,50,20,32,24,29,35,8,40,9,16,18,46,50,18,7,13,31,37,47,12,2,27,39,6,37,38,44,45,29,16,0,2,31,1,29,8,26,9,30,32,66084976,29,29,29,24,15,39,35,49,37,30,0,27,39,386169362,9,41,4,7,126749211,5,24,46,4,0,49 [...]` (remaining values look ok)\r\n\r\n\r\nTo stress that: This seems to be highly system dependent. We compile with `-march=native` and see it only on the cascade lake system. The string test is the only one that fails, all other unique tests succeed.\r\nThis sounds to me like some case of undefined behavior where an optimization corrupts the absl hashmap used.","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.8"],"created_at":"2021-02-16T08:06:00Z","comments":7,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/47179"},{"issue_number":451,"repository":"tensorflow\/tensorflow","title":"c_api_distributed_test creates huge amount of threads and segfaults","description":"**System information**\r\n- Have I written custom code: no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.1\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.7.1\r\n- GCC\/Compiler version (if compiling from source): 8.3.0\r\n- CUDA\/cuDNN version: 10.1\r\n\r\n**Describe the current behavior**\r\n\r\nWhen running `bazel test` on a system with a large physical core count the test `\/\/tensorflow\/c\/eager:c_api_distributed_test` finishes and then segfaults on exit.\r\n\r\nWhen I manually set `OMP_NUM_THREADS=80` the test succeeds without a segfault but at around 85 it again crashes.\r\n\r\nI'm unable to get a stacktrace neither through TensorFlow nor through gdb and even valgrind gives up with\r\n\r\n> valgrind: the 'impossible' happened:\r\n   Max number of threads is too low\r\n\r\nIt then prints the stacks of 500(!) threads. In GDB I was sometimes able to catch a part of the stack pointing to libiomp from the included llvm-OpenMP, but that was difficult and hard to reproduce. Usually the process would just be terminated even when in GDB.\r\n\r\nSomething I noticed: The ThreadPool(Device) creates a large amount of threads which don't terminate until program exit. I don't think this is intended and expect this to be the cause which triggers some limitation in the OpenMP runtime.\r\n\r\nAlso the crash does not happen when not all subtests are run (via the GTest filter), excluding any of the 5 (or 6?) makes the crash disappear\r\n\r\n**Describe the expected behavior**\r\n\r\nThreads exit when ThreadPool is destroyed and no crash happens.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n- build with bazel\r\n- `CUDA_VISIBLE_DEVICES=-1 gdb \/dev\/shm\/\/tmpzWGWuq-bazel-tf\/fdff6046a749a079864ed2bee7e018bf\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/c\/eager\/c_api_distributed_test`\r\n\r\n**Other info \/ logs**\r\n```\r\nExecuting tests from \/\/tensorflow\/c\/eager:c_api_distributed_test\r\n-----------------------------------------------------------------------------\r\n2021-02-08 19:58:39.296267: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\r\nRunning main() from test_main.cc\r\n[==========] Running 6 tests from 1 test suite.\r\n[----------] Global test environment set-up.\r\n[----------] 6 tests from CAPI\r\n[ RUN      ] CAPI.TestLocalFunctionWithPackedInput\r\n2021-02-08 19:58:39.510017: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\r\n2021-02-08 19:58:39.748990: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\r\n2021-02-08 19:58:39.749037: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: taurusi8028\r\n2021-02-08 19:58:39.749045: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: taurusi8028\r\n2021-02-08 19:58:39.749373: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: 460.32.3\r\n2021-02-08 19:58:39.749416: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 460.32.3\r\n2021-02-08 19:58:39.749430: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.32.3\r\n2021-02-08 19:58:39.749508: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:39.796167: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:39.841581: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:50055\r\n2021-02-08 19:58:39.841721: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:40.095546: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:40.095796: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:31398\r\n2021-02-08 19:58:40.095865: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:40.095922: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.258067: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:40.406436: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:40.406478: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:40.406627: I tensorflow\/core\/distributed_runtime\/eager\/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 \/job:localhost\/replica:0\/task:1\r\n2021-02-08 19:58:40.406634: I tensorflow\/core\/distributed_runtime\/eager\/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 \/job:localhost\/replica:0\/task:2\r\n2021-02-08 19:58:40.406664: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.406670: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.408781: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}\r\n2021-02-08 19:58:40.409412: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:59685\r\n2021-02-08 19:58:40.647385: I tensorflow\/core\/common_runtime\/eager\/kernel_and_device.cc:92] Ignoring error status when releasing multi-device function handle Unimplemented: Releasing a multi-device component \r\nhandle on a remote device is not yet implemented.\r\n[       OK ] CAPI.TestLocalFunctionWithPackedInput (1209 ms)\r\n[ RUN      ] CAPI.TestRemoteFunctionWithPackedInput\r\n2021-02-08 19:58:40.647938: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:40.673207: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.673481: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:31692\r\n2021-02-08 19:58:40.673544: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:40.775266: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.778366: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:39353\r\n2021-02-08 19:58:40.778517: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:40.778614: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.850707: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.857370: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.857557: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.857947: I tensorflow\/core\/distributed_runtime\/eager\/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 \/job:localhost\/replica:0\/task:2\r\n2021-02-08 19:58:40.857974: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.858061: I tensorflow\/core\/distributed_runtime\/eager\/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 \/job:localhost\/replica:0\/task:1\r\n2021-02-08 19:58:40.858100: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:40.865056: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}\r\n2021-02-08 19:58:40.866478: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:62634\r\n[       OK ] CAPI.TestRemoteFunctionWithPackedInput (367 ms)\r\n[ RUN      ] CAPI.DistributedFunctionGraphPassOnlyOnce\r\n2021-02-08 19:58:41.014661: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.024886: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.025155: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:44179\r\n2021-02-08 19:58:41.025233: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.101530: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.103305: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:57750\r\n2021-02-08 19:58:41.103445: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.103492: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.265757: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.272939: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.273025: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.273080: I tensorflow\/core\/distributed_runtime\/eager\/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 \/job:localhost\/replica:0\/task:1\r\n2021-02-08 19:58:41.273111: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.273240: I tensorflow\/core\/distributed_runtime\/eager\/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 \/job:localhost\/replica:0\/task:2\r\n2021-02-08 19:58:41.273276: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.275488: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}\r\n2021-02-08 19:58:41.275920: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:45659\r\n[       OK ] CAPI.DistributedFunctionGraphPassOnlyOnce (316 ms)\r\n[ RUN      ] CAPI.DistributedFunctionNoError\r\n2021-02-08 19:58:41.331075: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.405806: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.406048: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:34434\r\n2021-02-08 19:58:41.406115: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.445328: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.446777: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:37620\r\n2021-02-08 19:58:41.446932: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.447020: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.709247: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.713390: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.713392: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.713504: I tensorflow\/core\/distributed_runtime\/eager\/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 \/job:localhost\/replica:0\/task:2\r\n2021-02-08 19:58:41.713531: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.713555: I tensorflow\/core\/distributed_runtime\/eager\/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 \/job:localhost\/replica:0\/task:1\r\n2021-02-08 19:58:41.713588: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.714723: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}\r\n2021-02-08 19:58:41.715081: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:59564\r\n[       OK ] CAPI.DistributedFunctionNoError (448 ms)\r\n[ RUN      ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPC\r\n2021-02-08 19:58:41.778430: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.843268: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}\r\n2021-02-08 19:58:41.843574: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:32251\r\n2021-02-08 19:58:41.843637: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.843678: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.896621: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}\r\n2021-02-08 19:58:41.952439: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}\r\n2021-02-08 19:58:41.952676: I tensorflow\/core\/distributed_runtime\/eager\/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 \/job:localhost\/replica:0\/task:1\r\n2021-02-08 19:58:41.952707: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:41.953443: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}\r\n2021-02-08 19:58:41.953942: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:60792\r\n[       OK ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPC (178 ms)\r\n[ RUN      ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPCAsync\r\n2021-02-08 19:58:41.956466: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.980731: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}\r\n2021-02-08 19:58:41.980969: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:32519\r\n2021-02-08 19:58:41.981026: I tensorflow\/compiler\/jit\/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\r\n2021-02-08 19:58:41.981060: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:42.347899: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}\r\n2021-02-08 19:58:42.350811: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}\r\n2021-02-08 19:58:42.350917: I tensorflow\/core\/distributed_runtime\/eager\/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 \/job:localhost\/replica:0\/task:1\r\n2021-02-08 19:58:42.350946: I tensorflow\/core\/common_runtime\/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n2021-02-08 19:58:42.358578: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}\r\n2021-02-08 19:58:42.359091: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:411] Started server with target: grpc:\/\/localhost:63496\r\n[       OK ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPCAsync (403 ms)\r\n[----------] 6 tests from CAPI (2921 ms total)\r\n\r\n[----------] Global test environment tear-down\r\n[==========] 6 tests from 1 test suite ran. (2921 ms total)\r\n[  PASSED  ] 6 tests.\r\n\r\n  YOU HAVE 1 DISABLED TEST\r\n\r\n*** Received signal 11 ***\r\n*** BEGIN MANGLED STACK TRACE ***\r\n\r\n```\r\n\r\n(yes the log ends here, no stack trace!)","labels":["stat:awaiting tensorflower","type:bug","comp:apis","TF 2.4"],"created_at":"2021-02-09T17:43:57Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/47047"},{"issue_number":452,"repository":"tensorflow\/tensorflow","title":"Missing GPU op for zeros_like for RaggedTensorVariant, error occurs when Ragged Tensor fed thru tf.map_fn","description":"**System information**\r\n\r\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, included below\r\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10\r\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n\/a\r\nTensorFlow installed from (source or binary): pip binary\r\nTensorFlow version (use command below): v1.12.1-49539-g18d8bcbe72b 2.5.0-dev20210123\r\nPython version: '3.8.6 | packaged by conda-forge | (default, Nov 27 2020, 19:31:52) \\n[GCC 9.3.0]'\r\nBazel version (if compiling from source): n\/a\r\nGCC\/Compiler version (if compiling from source): n\/a\r\nCUDA\/cuDNN version: 11.0 \/ 8\r\nGPU model and memory: TITAN X (Pascal) computeCapability: 6.1\r\n\r\n**Describe the current behavior**\r\n\r\nI have a keras layer `RescaleB` that accepts a ragged tensor with shape [batch, (time), in_dim]. The layer calls `map_fn` to process each example in the batch separately, scaling the values along the inner dimension by a trainable gain vector. (The details of the operation aren't critical, but the ragged tensor going into map_fn is.)\r\n\r\nUsing this layer fails with `No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU` on a node whose name ends with `rescale_b\/map\/while\/TensorArrayV2Write\/TensorListSetItem_grad\/zeros_like` which suggests that the zeros_like operation \r\nisn't defined for Ragged Tensors on GPU?\r\n\r\nIn this simple example, i also include `RescaleA`, which accomplishes the same task using `tf.ragged.map_flat_values`, although in my real use case I need `map_fn`. This is a simplified example.\r\n\r\n**Describe the expected behavior**\r\n\r\nI'd expect `RescaleB` and `RescaleA` to function identically.\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/1mHycCXJL94VuCGkXIJ0bIXtbYamyZo78\r\n\r\nI've reproduced the issue locally with tf-nightly-gpu TF 2.5, but I can't seem to get the nightly version to see the GPU on Colab. The Colab notebook is using TF 2.4, but the issue remains in TF 2.5 nightly.\r\n\r\n**Other info \/ logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nThis may be the same issue as #44231 but hopefully the additional detail here is helpful. \r\n","labels":["type:bug","comp:keras","comp:gpu","TF 2.4","TF 2.16"],"created_at":"2021-01-24T04:27:07Z","comments":18,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/46635"},{"issue_number":453,"repository":"tensorflow\/tensorflow","title":"Add Windows build to nightly libtensorflow C packages","description":"The [C API page](https:\/\/www.tensorflow.org\/install\/lang_c#nightly_libtensorflow_c_packages) says that \"libtensorflow packages are built nightly and uploaded to GCS for all supported platforms\" but the [libtensorflow-nightly GCS bucket](https:\/\/storage.googleapis.com\/libtensorflow-nightly) does not have builds for Windows. The README's list of [offical builds](https:\/\/github.com\/tensorflow\/tensorflow#official-builds) also indicates that Windows nightlies should be available.\r\n\r\nPlease make Windows libtensorflow nightlies available for download.","labels":["type:bug","comp:apis"],"created_at":"2021-01-19T16:41:57Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/46538"},{"issue_number":454,"repository":"tensorflow\/tensorflow","title":"Memory leak in Conv2D\/Activation on GPU","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): Binary, the standard docker distribution\r\n- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0\r\n- Python version: 3.6.9\r\n- Bazel version (if compiling from source):\r\n- GCC\/Compiler version (if compiling from source):\r\n- CUDA\/cuDNN version: 11.0\r\n- GPU model and memory: GeForce RTX 2070, 8GB\r\n\r\n**Describe the current behavior**\r\nI upgraded to TF 2.4.0 from TF 2.1.2, and training a very simple convolutional network, which worked fine in 2.1.2, started running out of memory during training. I distilled a simple reproducible example that demonstrates the issue. Each training epoch consumes about 50MB of additional memory and, given enough epochs, it grows to infinity (or 32 GB in my case). It only occurs on GPU, the same thing runs fine on CPU.\r\n\r\n**Describe the expected behavior**\r\nMemory not growing, or growing only very little\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport gc\r\nimport os\r\nimport psutil\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, BatchNormalization, Activation\r\n\r\nphysical_devices = tf.config.list_physical_devices('GPU')\r\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\r\n\r\n\r\ninput_tensor = tf.keras.layers.Input(shape=(512,64,1))\r\n\r\nx = Conv2D(filters=32, kernel_size=(5,5), strides=(2,2), padding='same')(input_tensor)\r\n# Commented out on purpose - see Note 1 below\r\n# x = BatchNormalization()(x)\r\nx = Activation('relu')(x)\r\n\r\nx = Conv2D(filters=64, kernel_size=(4,4), strides=(2,2), padding='same')(x)\r\n# Commented out on purpose - see Note 1 below\r\n# x = BatchNormalization()(x)\r\nx = Activation('relu')(x)\r\n\r\nx = Conv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same')(x)\r\n# Commented out on purpose - see Note 1 below\r\n# x = BatchNormalization()(x)\r\nx = Activation('relu')(x)\r\n\r\nx = Conv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same')(x)\r\n# Commented out on purpose - see Note 1 below\r\n# x = BatchNormalization()(x)\r\nx = Activation('relu')(x)\r\n\r\nx = Flatten()(x)\r\n\r\nx = Dense(5, activation='sigmoid')(x)\r\n\r\nmodel = tf.keras.Model(inputs=input_tensor, outputs=x)\r\n\r\n\r\ntrain_x = np.random.random((2048, 512, 64, 1))\r\ntrain_y = np.random.random((2048, 5))\r\n\r\nmodel.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam())\r\n\r\nprocess = psutil.Process(os.getpid())\r\n\r\nfor i in range(50):\r\n    model.fit(train_x, train_y, epochs=1, batch_size=32, verbose=0)\r\n    gc.collect()\r\n    print(i, process.memory_info().rss \/\/ 1000000)\r\n```\r\n\r\n**Note 1**\r\nNow, if you uncomment the BatchNormalization() layers creation, the memory problem disappears. So, it is somehow caused by the Activation layer following immediately the Conv2D\r\n\r\n**Note 2**\r\nThe memory problem also occurs if I train multiple epochs in a single fit() call, such as \r\n```\r\nmodel.fit(train_x, train_y, epochs=50, batch_size=32)\r\n```\r\nI used the for loop only to be able to call garbage collection and print the memory.\r\n\r\n**Note 3**\r\nA Conv2D layer with activation embedded in it, such as\r\n```\r\nConv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same', activation='relu')\r\n```\r\nalso causes the memory issue\r\n\r\n\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.9"],"created_at":"2021-01-16T04:08:45Z","comments":28,"reactions":2,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/46475"},{"issue_number":455,"repository":"tensorflow\/tensorflow","title":"tensorflow\/c\/eager\/c_api_test fails to find GPU implementation of MatMulOp","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.7.1\r\n- GCC\/Compiler version (if compiling from source): 8.3.0\r\n- CUDA\/cuDNN version: 10.1\/7.6.4.38\r\n- GPU model and memory: GTX1080TI\r\n\r\n**Describe the current behavior**\r\n\r\nThe 4 tests `CAPI.TensorHandleSilentCopy*` from `tensorflow\/c\/eager\/c_api_test` fail each with \r\n\r\n```\r\ntensorflow\/c\/eager\/c_api_test.cc:442: Failure\r\nExpected equality of these values:\r\n  TF_GetCode(status.get())\r\n    Which is: 3\r\n  TF_OK\r\n    Which is: 0\r\nCould not satisfy device specification '\/job:localhost\/replica:0\/task:0\/device:GPU:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [\/job:localhost\/replica:0\/task:0\/device:GPU:\r\n0, \/job:localhost\/replica:0\/task:0\/device:CPU:0].\r\n```\r\n\r\nReason seems to be that `MatMulOp` is not found for GPU which is odd as seemingly everything else works.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n`\/tmp\/ebbuild\/TensorFlow\/2.4.0\/fosscuda-2019b-Python-3.7.4\/tmp004t3B-bazel-tf\/7277201245461b79db55d0e3e6d95f77\/execroot\/org_tensorflow\/bazel-out\/k8-opt\/bin\/tensorflow\/c\/eager\/c_api_test_gpu --gtest_filter=*TensorHandleSilentCopy*`\r\n\r\n**Other info \/ logs** \r\n\r\nFor some reason this test works on our POWER9 nodes which have otherwise the exact same environment (same software versions etc), but use V100s","labels":["stat:awaiting tensorflower","type:bug","comp:eager","TF 2.4"],"created_at":"2021-01-08T16:45:27Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/46295"},{"issue_number":456,"repository":"tensorflow\/tensorflow","title":"Inexact numeric jacobian causes test failures","description":"Using TensorFlow 2.4 with GPUs\r\n\r\n**Describe the current behavior**\r\n\r\n\/\/tensorflow\/python\/keras\/integration_test:gradients_test fails when run on a node with GPUs. I traced the problem to inexact computation in `_compute_numeric_jacobian`\r\n\r\nOutput is:\r\n```\r\nFAIL: testLSTMBatchJacobian (__main__.GradientsTest)\r\nGradientsTest.testLSTMBatchJacobian\r\n...\r\nAssertionError: \r\nNot equal to tolerance rtol=0.01, atol=1e-06\r\nMismatched value: a is different from b. \r\nnot close where = (array([0]), array([0]), array([2]))\r\nnot close lhs = [0.00074506]\r\nnot close rhs = [0.00076706]\r\nnot close dif = [2.20043e-05]\r\nnot close tol = [8.670623e-06]\r\ndtype = float32, shape = (1, 1, 6)\r\nMismatch: 16.7%\r\nMax absolute difference: 2.20043e-05\r\nMax relative difference: 0.02868646\r\n x: array([[[-0.013396,  0.007078,  0.000745, -0.02031 ,  0.010461,\r\n         -0.004366]]], dtype=float32)\r\n y: array([[[-0.0134  ,  0.007075,  0.000767, -0.020303,  0.010446,\r\n         -0.004386]]], dtype=float32)\r\n```\r\n\r\nThe numeric result is `array([-0.013396,  0.007078,  0.000745, -0.02031 ,  0.010461, -0.004366], dtype=float32)`\r\nwhile the eager_result, function_result and backprop_result all are `y: array([-0.0134  ,  0.007075,  0.000767, -0.020303,  0.010446, -0.004386], dtype=float32)`\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\nReduced extracted test code:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass GradientsTest(tf.test.TestCase):\r\n  def testLSTMBatchJacobian(self):\r\n    class HasLSTM(tf.keras.Model):\r\n\r\n      def __init__(self):\r\n        super(HasLSTM, self).__init__()\r\n        self.lstm = tf.keras.layers.LSTM(units=5)\r\n        self.dense = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\r\n\r\n      def call(self, x):\r\n        return self.dense(self.lstm(x))\r\n\r\n    m = HasLSTM()\r\n\r\n    def jacobian(x):\r\n      with tf.GradientTape() as tape:\r\n        tape.watch(x)\r\n        y = m(x)  # pylint: disable=not-callable\r\n      return tape.batch_jacobian(y, x)\r\n\r\n    inp = tf.nn.l2_normalize(tf.ones([1, 2, 3]), axis=[1, 2])\r\n    eager_result = jacobian(inp)\r\n    #function_result = tf.function(jacobian)(inp)\r\n    #self.assertAllClose(eager_result, function_result)\r\n    backprop_result, numeric_result = tf.test.compute_gradient(\r\n        m, [inp], delta=1e-3)\r\n\r\n    self.assertAllClose(numeric_result, backprop_result, rtol=1e-2)\r\n    self.assertAllClose(tf.reshape(numeric_result, [-1]),\r\n                        tf.reshape(eager_result, [-1]), rtol=1e-2)\r\n\r\nif __name__ == \"__main__\":\r\n  tf.test.main()\r\n```\r\n\r\n\r\n**Other info \/ logs**\r\nIncreasing the delta to `1e-2` makes the test pass.\r\n\r\nSide note: Uncommenting the `tf.function` line yields a couple wrong-looking errors\/warnings:\r\n```\r\nWARNING:tensorflow:Using a while_loop for converting CudnnRNNBackprop\r\npfor.py:1052] Using a while_loop for converting CudnnRNNBackprop\r\nI tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\r\nE tensorflow\/core\/grappler\/optimizers\/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body\/PartitionedCall\/pfor\/PartitionedCall\/gradients\/zeros_like\/pfor\/ZerosLike was passed float from has_lstm\/lstm\/PartitionedCall:6 incompatible with expected variant.\r\nE tensorflow\/core\/grappler\/optimizers\/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body\/PartitionedCall\/pfor\/PartitionedCall\/gradients\/zeros_like\/pfor\/ZerosLike was passed float from has_lstm\/lstm\/PartitionedCall:6 incompatible with expected variant.\r\n W tensorflow\/core\/common_runtime\/process_function_library_runtime.cc:805] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node loop_body\/PartitionedCall\/pfor\/PartitionedCall\/gradients\/zeros_like\/pfor\/ZerosLike was passed float from has_lstm\/lstm\/PartitionedCall:6 incompatible with expected variant.\r\n```\r\n\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.9"],"created_at":"2020-12-18T14:52:12Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/45849"},{"issue_number":457,"repository":"tensorflow\/tensorflow","title":"Invalid index list in batch_scatter_ops_test.py ","description":"**Describe the current behavior**\r\n\r\nFor current master the tests in batch_scatter_ops_test.py produce an invalid index list\/tensor.\r\n\r\nCheck: https:\/\/github.com\/tensorflow\/tensorflow\/blob\/dec8e0b11f4f87693b67e125e67dfbc68d26c205\/tensorflow\/python\/kernel_tests\/batch_scatter_ops_test.py#L59-L62\r\n\r\nThe comment says, non-duplicate values are required but `randint` is used which does produce duplicates. Hence the test fails.\r\n\r\n**Standalone code to reproduce the issue**\r\nA reduced test code which reproduces this on my machine:\r\n\r\n```\r\nimport numpy as np\r\nfrom tensorflow.python.ops import state_ops\r\nfrom tensorflow.python.ops import variables\r\nfrom tensorflow.python.framework import ops\r\n\r\ndef _NumpyUpdate(ref, indices, updates):\r\n  for i, indx in np.ndenumerate(indices):\r\n    indx = i[:-1] + (indx,)\r\n    ref[indx] = updates[i]\r\n\r\ndef _VariableRankTest(vtype, itype):\r\n  np.random.seed(8)\r\n  indices_shape = (2,)\r\n  for extra_shape in (), (5,):\r\n    # Generate random indices with no duplicates for easy numpy comparison\r\n    sparse_dim = len(indices_shape) - 1\r\n    indices = np.random.randint(indices_shape[sparse_dim], size=indices_shape, dtype=itype)\r\n    updates = np.random.randn(*(indices_shape + extra_shape)).astype(vtype)\r\n\r\n    old = np.random.randn(*(indices_shape + extra_shape)).astype(vtype)\r\n    print(\"indices: %s\" % indices)\r\n    if not extra_shape:\r\n      continue\r\n\r\n    # Scatter via numpy\r\n    new = old.copy()\r\n    _NumpyUpdate(new, indices, updates)\r\n    # Scatter via tensorflow\r\n    ref = variables.Variable(old)\r\n    variables.variables_initializer([ref])\r\n\r\n    #state_ops.batch_scatter_update(ref, indices, updates)\r\n    ref.batch_scatter_update(ops.IndexedSlices(indices=indices, values=updates))\r\n    ref = ref.numpy()\r\n    assert np.allclose(ref, new, rtol=1e-6, atol=1e-6), \"Failed:\\nlhs: %s\\nrhs: %s\" % (ref, new)\r\n\r\n_VariableRankTest(np.float32, np.int32)\r\n```\r\n\r\nI see an output of `indices: [1 1]` followed by:\r\n```\r\nAssertionError: Failed:\r\nlhs: [[-0.37835857 -0.79161525  0.8595481  -0.23078899 -0.06566103]\r\n [-2.2964916   2.4098344   1.7278361   2.2045562   0.79482764]]\r\nrhs: [[-0.37835857 -0.79161525  0.8595481  -0.23078899 -0.06566103]\r\n [ 0.9764211  -1.1834271   1.9163636  -1.1233268  -0.6640355 ]]\r\n```\r\n\r\n**Other info \/ logs**\r\ntensorflow\/python\/kernel_tests\/scatter_ops_test.py contains a valid implementation using `arange` and `shuffle` which would fix the issue\r\n\r\nSide note: the code raises a warning:\r\n> tensorflow\/python\/ops\/resource_variable_ops.py:1124: batch_scatter_update (from tensorflow.python.ops.state_ops) is deprecated and will be removed after 2018-11-29.\r\nInstructions for updating:\r\nUse the batch_scatter_update method of Variable instead.\r\n\r\nNote how the date is well passed but there is not even a way to avoid this as I'm already doing what is suggested.","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","regression issue","TF 2.11"],"created_at":"2020-12-17T14:34:27Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/45789"},{"issue_number":458,"repository":"tensorflow\/tensorflow","title":"Out of memory in some tests due to GPU memory limit confusion","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0rc4\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC\/Compiler version (if compiling from source): GCC 8.3.0\r\n- CUDA\/cuDNN version: 10.1\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\n\r\nI have V100 GPUs with ~32GB memory. During startup of the test (many tests show this) I see lines like \r\n` W tensorflow\/stream_executor\/stream_executor_pimpl.cc:490] Not enough memory to allocate 31614597888 on device 0 within provided limit. [used=0, limit=1073741824]`\r\n\r\nSome tests then fail after allocating about 1GB of memory trying to allocate more. The failure message includes the 31GB and shows almost 1GB as used.\r\nE.g. \/\/tensorflow\/python\/keras\/applications:applications_test or \/\/tensorflow\/python\/keras\/layers:convolutional_recurrent_test\r\n\r\n**Standalone code to reproduce the issue**\r\nRun bazel test\r\n\r\n**Other info \/ logs**\r\n```\r\n2020-12-14 12:27:32.863559: W tensorflow\/stream_executor\/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]\r\n2020-12-14 12:27:32.863607: W tensorflow\/stream_executor\/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]\r\n2020-12-14 12:27:42.864373: W tensorflow\/stream_executor\/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]\r\n2020-12-14 12:27:42.864404: W tensorflow\/stream_executor\/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]\r\n2020-12-14 12:27:42.864421: W tensorflow\/core\/common_runtime\/bfc_allocator.cc:433] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.00MiB (rounded to 4194304)requested by op Tanh\r\nCurrent allocation summary follows.\r\n2020-12-14 12:27:42.864434: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:972] BFCAllocator dump for GPU_0_bfc\r\n...\r\n2020-12-14 12:27:42.866982: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1040] Sum Total of in-use chunks: 928.00MiB\r\n2020-12-14 12:27:42.866989: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1042] total_region_allocated_bytes_: 976990208 memory_limit_: 31614597888 available bytes: 30637607680 curr_region_allocation_bytes_: 63229195776\r\n2020-12-14 12:27:42.867000: I tensorflow\/core\/common_runtime\/bfc_allocator.cc:1048] Stats: \r\nLimit:                     31614597888\r\nInUse:                       973083648\r\nMaxInUse:                    973083648\r\nNumAllocs:                        2238\r\nMaxAllocSize:                 86081536\r\nReserved:                            0\r\nPeakReserved:                        0\r\nLargestFreeBlock:                    0\r\n\r\n2020-12-14 12:27:42.867013: W tensorflow\/core\/common_runtime\/bfc_allocator.cc:441] ****************************************************************************************************\r\n2020-12-14 12:27:42.867035: W tensorflow\/core\/framework\/op_kernel.cc:1763] OP_REQUIRES failed at cwise_op_gpu_base.cc:97 : Resource exhausted: OOM when allocating tensor with shape[32,32,32,32] and type float on \/job:localhost\/replica:0\/task:0\/device:GPU:0 by allocator GPU_0_bfc\r\n\r\n```","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.9","comp:lite-tosa"],"created_at":"2020-12-14T17:09:34Z","comments":16,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/45664"},{"issue_number":459,"repository":"tensorflow\/tensorflow","title":"No registered 'ResourceScatterNdUpdate' OpKernel for 'GPU' ","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version (use command below): 2.4.0rc4\r\n- Python version: 3.7.4\r\n- Bazel version (if compiling from source): 3.4.1\r\n- GCC\/Compiler version (if compiling from source): GCC 8.3.0\r\n- CUDA\/cuDNN version: 10.1\r\n- GPU model and memory: V100\r\n\r\n**Describe the current behavior**\r\nA test shows that a GPU implementation for BOOL inputs of ResourceScatterNdUpdate is seemingly missing.\r\nThe test is \/\/tensorflow\/python\/kernel_tests:batch_scatter_ops_test -> ScatterTest.testBooleanScatterUpdate\r\n\r\n**Standalone code to reproduce the issue**\r\nRun bazel test\r\n\r\n**Other info \/ logs**\r\n```\r\nERROR: testBooleanScatterUpdate (__main__.ScatterTest)\r\nScatterTest.testBooleanScatterUpdate\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"\/tmp\/bazel-tf\/20db8ac50b74c328e6dea9b20829b459\/execroot\/org_tensorflow\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/kernel_tests\/batch_scatter_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/kernel_tests\/batch_scatter_ops_test.py\", line 91, in testBooleanScatterUpdate\r\n    update0 = state_ops.batch_scatter_update(var, [1], [True])\r\n  File \"\/tmp\/bazel-tf\/20db8ac50b74c328e6dea9b20829b459\/execroot\/org_tensorflow\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/kernel_tests\/batch_scatter_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/util\/deprecation.py\", line 340, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"\/tmp\/bazel-tf\/20db8ac50b74c328e6dea9b20829b459\/execroot\/org_tensorflow\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/kernel_tests\/batch_scatter_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/ops\/state_ops.py\", line 915, in batch_scatter_update\r\n    ref, final_indices, updates, use_locking=use_locking)\r\n  File \"\/tmp\/bazel-tf\/20db8ac50b74c328e6dea9b20829b459\/execroot\/org_tensorflow\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/kernel_tests\/batch_scatter_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/ops\/state_ops.py\", line 368, in scatter_nd_update\r\n    name=name))\r\n  File \"\/tmp\/bazel-tf\/20db8ac50b74c328e6dea9b20829b459\/execroot\/org_tensorflow\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/kernel_tests\/batch_scatter_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/ops\/gen_state_ops.py\", line 740, in resource_scatter_nd_update\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"\/tmp\/bazel-tf\/20db8ac50b74c328e6dea9b20829b459\/execroot\/org_tensorflow\/bazel-out\/ppc-opt\/bin\/tensorflow\/python\/kernel_tests\/batch_scatter_ops_test.runfiles\/org_tensorflow\/tensorflow\/python\/framework\/ops.py\", line 6862, in raise_from_not_ok_status\r\n    six.raise_from(core._status_to_exception(e.code, message), None)\r\n  File \"<string>\", line 3, in raise_from\r\ntensorflow.python.framework.errors_impl.NotFoundError: No registered 'ResourceScatterNdUpdate' OpKernel for 'GPU' devices compatible with node {{node ResourceScatterNdUpdate}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_BOOL, Tindices=DT_INT32, use_locking=true\r\n\t.  Registered:  device='GPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT32]\r\n  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT64]\r\n  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]\r\n  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT32]\r\n [Op:ResourceScatterNdUpdate]\r\n```\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","comp:gpu","TF 2.9"],"created_at":"2020-12-14T17:04:54Z","comments":12,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/45663"},{"issue_number":460,"repository":"tensorflow\/tensorflow","title":"GFile does not create file when nothing is written.","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary (pip3)\r\n- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): NA\r\n- GCC\/Compiler version (if compiling from source): NA\r\n- CUDA\/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\n**Describe the current behavior**\r\n\r\n`tf.io.gfile.GFile` does nothing to the filesystem when it is closed without any write operations:\r\n```python\r\nwith tf.io.gfile.GFile('foo', 'wb') as fp:\r\n  fp.write('test')  # 'foo' is created.\r\n\r\nwith tf.io.gfile.GFile('bar', 'wb'):\r\n  pass  # 'bar' is not created.\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\nThe same behavior with builtin `open`: creates a file even if no write operation is invoked.\r\n\r\n**Standalone code to reproduce the issue**\r\nAlready described above.\r\n\r\n**Other info \/ logs**\r\nNA","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.9"],"created_at":"2020-11-25T06:04:08Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/45171"},{"issue_number":461,"repository":"tensorflow\/tensorflow","title":"tf.debugging.assert_shapes inconsistent behaviour for scalars","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n\/a\r\n- TensorFlow installed from (source or binary): pip install tensorflow\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7\r\n- Bazel version (if compiling from source): n\/a\r\n- GCC\/Compiler version (if compiling from source): n\/a\r\n- CUDA\/cuDNN version: n\/a\r\n- GPU model and memory: n\/a\r\n\r\n**Describe the current behavior**\r\n`tf.debugging.assert_shapes` has seemingly inconsistent behaviour for scalars\r\n```\r\nimport tensorflow as tf\r\n\r\ntf.debugging.assert_shapes([(0, ())])  # passes as expected\r\ntf.debugging.assert_shapes([(0, (1,))])  # passes unexpectedly\r\n```\r\nI think this is inconsistent, because `tf.constant(0)` does not have shape `(1,)`.\r\n\r\n**Describe the expected behavior**\r\nI would expect `tf.debugging.assert_shapes` to error if the `.shape` attribute is different from the shape constraint","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.15"],"created_at":"2020-11-16T01:24:29Z","comments":12,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/44888"},{"issue_number":462,"repository":"tensorflow\/tensorflow","title":"BUG: Keras SaveModel does not properly save optimizer state","description":"EDIT: looks like this is a dupe of #42749, I'll leave this up for now in case since that issue does not have as reproducible high level example, but feel free to close.\r\n\r\nThis happens at least for Adam (does not apply to SGD for example, did not test with others).\r\n\r\nTested on `tf-nightly` and `tf==2.3.0`.\r\n\r\nTL;DR: running a `tf.kerasModel` through `tf.keras.models.load(model.save)` does not properly preserve the state of optimizers for certain optimizers (see #42749 for more details).\r\n\r\nThe [docs](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model#the_savefile_includes_2) read:\r\n\r\n> The savefile includes:\r\n> * The model architecture, allowing to re-instantiate the model.\r\n> * The model weights.\r\n> * The state of the optimizer, allowing to resume training exactly where you left off.\r\n\r\nFull example:\r\n\r\n```python3\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n\r\n# Define a minimal model\r\ninp = keras.layers.Input((1, ))\r\nout = keras.layers.Dense(1)(inp)\r\nm1 = keras.Model(inp, out)\r\nm1.compile(loss=\"mae\", optimizer=\"adam\")\r\n\r\n# Create some test data\r\nX, y = np.random.random((100, )), np.random.random((100, ))\r\n\r\n# Fit the model to the test data to get everything initialized\r\nm1.fit(X, y, verbose=0)\r\n\r\n\r\ndef roundtrip(model: keras.Model) -> keras.Model:\r\n    save_dir = \"\/tmp\/mymodel\"\r\n    model.save(save_dir)\r\n    restored = keras.models.load_model(save_dir)\r\n    return restored\r\n\r\n\r\n# Create a copy of the fitted m1\r\nm2 = roundtrip(m1)\r\n\r\n# Weights are preserved correctly, this passes\r\nnp.testing.assert_allclose(m1.predict(X), m2.predict(X))\r\n\r\n# New lets train once more round\r\nm1.fit(X, y, verbose=0)\r\n# Since optimizer weights\/state is not preserved, this fit call\r\n# results in different weights in m2, which makes the predictions differ\r\nm2.fit(X, y, verbose=0)\r\ntry:\r\n    np.testing.assert_allclose(m1.predict(X), m2.predict(X), rtol=0.1)  # large relative tolerance\r\nexcept AssertionError:\r\n    print(\"AssertionError: model predictions differ\")\r\n\r\n# Diagnosis: optimizer weights are not preserved\r\nweights1 = m1.optimizer.get_weights()\r\nm3 = roundtrip(m1)\r\nweights3 = m3.optimizer.get_weights()\r\n\r\ntry:\r\n    assert weights1 == weights3\r\nexcept AssertionError:\r\n    print(\"AssertionError: optimizer weights differ\")\r\n    print(f\"    {weights1}\\n    vs\\n    {weights3}\")\r\n\r\n# Further, we can't even restore the weights without training!\r\ntry:\r\n    m3.optimizer.set_weights(weights1)\r\nexcept Exception as e:\r\n    print(str(e).split(\" Provided weights:\")[0])\r\n```","labels":["type:bug","comp:keras","TF 2.3"],"created_at":"2020-11-07T07:15:55Z","comments":13,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/44670"},{"issue_number":463,"repository":"tensorflow\/tensorflow","title":"Concrete Function output shape sometimes changes after save\/load cycle","description":"Output of environment capture script:\r\n\r\n```\r\n== check python ===================================================\r\npython version: 3.7.8\r\npython branch:\r\npython build version: ('default', 'Aug 10 2020 13:15:25')\r\npython compiler version: Clang 10.0.0 (clang-1000.10.44.4)\r\npython implementation: CPython\r\n\r\n\r\n== check os platform ===============================================\r\nos: Darwin\r\nos kernel version: Darwin Kernel Version 17.7.0: Thu Dec 20 21:47:19 PST 2018; root:xnu-4570.71.22~1\/RELEASE_X86_64\r\nos release version: 17.7.0\r\nos platform: Darwin-17.7.0-x86_64-i386-64bit\r\nlinux distribution: ('', '', '')\r\nlinux os distribution: ('', '', '')\r\nmac version: ('10.13.6', ('', '', ''), 'x86_64')\r\nuname: uname_result(system='Darwin', node='...', release='17.7.0', version='Darwin Kernel Version 17.7.0: Thu Dec 20 21:47:19 PST 2018; root:xnu-4570.71.22~1\/RELEASE_X86_64', machine='x86_64', processor='i386')\r\narchitecture: ('64bit', '')\r\nmachine: x86_64\r\n\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nApple LLVM version 10.0.0 (clang-1000.10.44.4)\r\nTarget: x86_64-apple-darwin17.7.0\r\nThread model: posix\r\nInstalledDir: \/Library\/Developer\/CommandLineTools\/usr\/bin\r\n\r\n== check pips ===================================================\r\nnumpy                         1.18.5\r\nprotobuf                      3.10.0\r\ntensorflow                    2.3.1\r\ntensorflow-estimator          2.3.0\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.version.VERSION = 2.3.1\r\ntf.version.GIT_VERSION = v2.3.0-54-gfcc4b966f1\r\ntf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n.\/tf_env_collect.sh: line 147: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n== tensorflow installed from info ==================\r\nName: tensorflow\r\nVersion: 2.3.1\r\nSummary: TensorFlow is an open source machine learning framework for everyone.\r\nHome-page: https:\/\/www.tensorflow.org\/\r\nAuthor-email: packages@tensorflow.org\r\nLicense: Apache 2.0\r\nLocation: ...\/python3.7\/site-packages\r\nRequired-by: \r\n\r\n== python version  ==============================================\r\n(major, minor, micro, releaselevel, serial)\r\n(3, 7, 8, 'final', 0)\r\n\r\n== bazel version  ===============================================\r\n```\r\n\r\n**Describe the current behavior**\r\n\r\nThe `structured_outputs` of a concrete function might not match after saving and restoring a model containing that function via tf.saved_model. In the example below, the function doesn't change the shape of the tensor, but, when requesting a concrete function for a tensor with shape [None, 3], a more specific output [7, 3] is returned instead.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe output shape should not be more specific than the true output.\r\n\r\n**Standalone code to reproduce the issue**\r\nReproduction script and stdout of `for i in $(seq 10); do echo $i; python repro.py; done` in this Gist: https:\/\/gist.github.com\/gmacon\/057cc64bf849c8d65974daec56a037b7\r\n\r\n**Other info \/ logs**\r\nInitially, I thought this bug was deterministic, but it's clearly actually random. Based on running the reproduction script in a loop, it appears to work correctly about half of the time. Question: does this have something to do with the traversal order of a randomized hash table?\r\n\r\nEdit again: I initially thought it was deterministic because it was happening every time in the system I'm building. However, I was reusing the same serialized model over and over in those tests. It appears to work in about half the *saves*, not half the *loads*.\r\n\r\n~As far as I can tell, this was working correctly in Tensorflow 2.2.~ Edit: I'm not sure about that any more.\r\n\r\nEdit again again: I reproduced this in Tensorflow 2.0.0.","labels":["stat:awaiting tensorflower","type:bug","comp:apis","TF 2.3"],"created_at":"2020-10-26T19:33:30Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/44337"},{"issue_number":464,"repository":"tensorflow\/tensorflow","title":"Unexpected output shape when trying to convert to Frozen Graph using convert_variables_to_constants_v2","description":"<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/ISSUES.md),\r\nwe only address code\/doc bugs, performance issues, feature requests and\r\nbuild\/installation issues on GitHub. tag:bug_template<\/em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary):  Using Deep Learning VM Instance on GCP. Not sure.\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.7.0\r\n- Bazel version (if compiling from source): NA\r\n- GCC\/Compiler version (if compiling from source): NA\r\n- CUDA\/cuDNN version: 11.0\r\n- GPU model and memory: Tesla T4 16GB\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https:\/\/github.com\/tensorflow\/tensorflow\/tree\/master\/tools\/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nI am using this [answer](https:\/\/stackoverflow.com\/a\/61004639\/5052482) by TF_Support on StackOverflow to convert a model to to a frozen graph. However, the output shape that I am getting for my custom model is is very different (weird) from the one that's there in my model. More information in the code snippet below.\r\n\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab\/Jupyter\/any notebook.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\r\n\r\ndef blah():\r\n    jumps = tf.keras.layers.Input(shape=(269, 8))\r\n    loc_a = tf.keras.layers.Input(shape=(32))\r\n    loc_b = tf.keras.layers.Input(shape=(256))\r\n    loc_c = tf.keras.layers.Input(shape=(2))\r\n    def conv_block(x):\r\n        x = tf.keras.layers.Conv1D(filters=32, kernel_size=1, padding='same')(x)\r\n        x = tf.keras.layers.BatchNormalization()(x)\r\n        x = tf.keras.layers.Activation('linear')(x)\r\n        return x\r\n    \r\n    x = jumps\r\n    for i in range(1):\r\n        x = conv_block(x)\r\n    _loc_a = tf.keras.layers.Dense(269)(loc_a)\r\n    _loc_b = tf.keras.layers.Dense(269)(loc_b)\r\n    _loc_c = tf.keras.layers.Dense(269)(loc_c)\r\n    _loc = tf.keras.layers.Concatenate()([_loc_a, _loc_b, _loc_c])\r\n    _loc = tf.keras.layers.Dense(269)(_loc)\r\n    _loc = tf.keras.layers.Reshape((269, 1))(_loc)\r\n    \r\n    x = tf.keras.layers.Concatenate()([_loc, x])\r\n    x= tf.keras.layers.Dense(9)(x)\r\n#     x= tf.keras.layers.Dense(9)(x)\r\n#     x = tf.keras.layers.Flatten()(x)\r\n    \r\n    return tf.keras.Model(inputs=[jumps, loc_a, loc_b, loc_c], outputs=[x])\r\n\r\nmodel = blah()\r\n\r\nmodel.summary()\r\n\r\nModel: \"model\"\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to                     \r\n==================================================================================================\r\ninput_2 (InputLayer)            [(None, 32)]         0                                            \r\n__________________________________________________________________________________________________\r\ninput_3 (InputLayer)            [(None, 256)]        0                                            \r\n__________________________________________________________________________________________________\r\ninput_4 (InputLayer)            [(None, 2)]          0                                            \r\n__________________________________________________________________________________________________\r\ndense (Dense)                   (None, 269)          8877        input_2[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_1 (Dense)                 (None, 269)          69133       input_3[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_2 (Dense)                 (None, 269)          807         input_4[0][0]                    \r\n__________________________________________________________________________________________________\r\ninput_1 (InputLayer)            [(None, 269, 8)]     0                                            \r\n__________________________________________________________________________________________________\r\nconcatenate (Concatenate)       (None, 807)          0           dense[0][0]                      \r\n                                                                 dense_1[0][0]                    \r\n                                                                 dense_2[0][0]                    \r\n__________________________________________________________________________________________________\r\nconv1d (Conv1D)                 (None, 269, 32)      288         input_1[0][0]                    \r\n__________________________________________________________________________________________________\r\ndense_3 (Dense)                 (None, 269)          217352      concatenate[0][0]                \r\n__________________________________________________________________________________________________\r\nbatch_normalization (BatchNorma (None, 269, 32)      128         conv1d[0][0]                     \r\n__________________________________________________________________________________________________\r\nreshape (Reshape)               (None, 269, 1)       0           dense_3[0][0]                    \r\n__________________________________________________________________________________________________\r\nactivation (Activation)         (None, 269, 32)      0           batch_normalization[0][0]        \r\n__________________________________________________________________________________________________\r\nconcatenate_1 (Concatenate)     (None, 269, 33)      0           reshape[0][0]                    \r\n                                                                 activation[0][0]                 \r\n__________________________________________________________________________________________________\r\ndense_4 (Dense)                 (None, 269, 9)       306         concatenate_1[0][0]              \r\n==================================================================================================\r\nTotal params: 296,891\r\nTrainable params: 296,827\r\nNon-trainable params: 64\r\n\r\n# convert model to a frozen graph\r\n\r\nfull_model = tf.function(lambda x: model(x))\r\n\r\nfull_model = full_model.get_concrete_function([tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype, name=\"input_0\"),\r\n                                              tf.TensorSpec(model.inputs[1].shape, model.inputs[0].dtype, name=\"input_1\"),\r\n                                              tf.TensorSpec(model.inputs[2].shape, model.inputs[0].dtype, name=\"input_2\"),\r\n                                              tf.TensorSpec(model.inputs[3].shape, model.inputs[0].dtype, name=\"input_3\")])\r\n\r\nfrozen_func = convert_variables_to_constants_v2(full_model)\r\nfrozen_func.graph.as_graph_def(add_shapes=True)\r\nlayers = [op.name for op in frozen_func.graph.get_operations()]\r\nprint(\"-\" * 50)\r\n# print(\"Frozen model layers: \")\r\n# for layer in layers:\r\n#     print(layer)\r\nprint(\"-\" * 50)\r\nprint(\"Frozen model inputs: \")\r\nprint(frozen_func.inputs)\r\nprint(\"Frozen model outputs: \")\r\nprint(frozen_func.outputs)\r\n# Save frozen graph from frozen ConcreteFunction to hard drive\r\ntf.io.write_graph(graph_or_graph_def=frozen_func.graph,\r\n                  logdir=\".\/frozen_models\",\r\n                  name=\"frozen_graph.pb\",\r\n                  as_text=False)\r\n\r\nFrozen model inputs: \r\n[<tf.Tensor 'input_0:0' shape=(None, 269, 8) dtype=float32>, <tf.Tensor 'input_1:0' shape=(None, 32) dtype=float32>, <tf.Tensor 'input_2:0' shape=(None, 256) dtype=float32>, <tf.Tensor 'input_3:0' shape=(None, 2) dtype=float32>]\r\nFrozen model outputs: \r\n[<tf.Tensor 'Identity:0' shape=(None, None, 9) dtype=float32>]\r\n'.\/frozen_models\/frozen_graph.pb'\r\n\r\n```\r\n\r\nThe output shape of the Frozen Model should match the output shape of the model created. However it does not. \r\n\r\nThe output shape of the model should be `(None, 269, 9)` whereas the output shape of the Frozen Graph is `(None, None, 9)`\r\n\r\n**Other info \/ logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:core","2.6.0"],"created_at":"2020-10-07T08:02:02Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/43839"},{"issue_number":465,"repository":"tensorflow\/tensorflow","title":"LookupError when computing nested gradient with UpSampling2D","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): 2.3.1\r\n- Python version: 3.7.4\r\n- CUDA\/cuDNN version: 10.1\r\n- GPU model and memory: NVIDIA GeForce GTX 1070 8GB VRAM\r\n\r\n**Describe the current behavior**\r\nThe provided code fails with the following error (see Colab link for full stacktrace):\r\n`LookupError: gradient registry has no entry for: ResizeNearestNeighborGrad`\r\n\r\n**Describe the expected behavior**\r\nThe GradientTape.gradient method should be able to compute the gradient.\r\n\r\n**Standalone code to reproduce the issue**\r\n[Google Colab standalone code](https:\/\/colab.research.google.com\/drive\/1eGMvXJxVvY7zb8OISkS1H3D90N0J380d?usp=sharing)\r\n\r\n**Other info \/ logs**\r\nI encountered the error while developing a GAN. I used Conv2DTranspose for upsampling at first but encountered artifacts. After changing Conv2DTranspose to a combination of UpSampling2D and Conv2D (as is common in GAN's) the inner gradient stopped working.","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF 2.11"],"created_at":"2020-09-29T11:16:25Z","comments":18,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/43648"},{"issue_number":466,"repository":"tensorflow\/tensorflow","title":"TensorArray issues in XLA","description":"<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/ISSUES.md),\r\nwe only address code\/doc bugs, performance issues, feature requests and\r\nbuild\/installation issues on GitHub. tag:bug_template<\/em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.3 (exists in 1.5 as well)\r\n- Python version: 3.7.6\r\n- Bazel version (if compiling from source): NA\r\n- GCC\/Compiler version (if compiling from source): NA\r\n- CUDA\/cuDNN version: NA\r\n- GPU model and memory: NA\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https:\/\/github.com\/tensorflow\/tensorflow\/tree\/master\/tools\/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\nI know it was documented in xla official doc that xla doesn't support TensorArray with `dynamic_size=True`, however, I still ran into issues even w\/ `dynamic_size=False`(it is default).\r\n\r\n**Describe the expected behavior**\r\nexpect the tensorarray related stuffs are supported.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab\/Jupyter\/any notebook.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.compat.v1.enable_eager_execution()\r\n\r\n@tf.function(experimental_compile=True)\r\ndef test_tensor_array_scatter_gather():\r\n    dtype = \"float32\"\r\n    t = tf.constant(np.array([[1.0], [2.0], [3.0]]).astype(dtype))\r\n    scatter_indices = tf.constant([2, 1, 0])\r\n    gather_indices = tf.constant([1, 2])\r\n    ta1 = tf.TensorArray(dtype=dtype, size=3, infer_shape=True)\r\n    ta2 = ta1.scatter(scatter_indices, t)\r\n    t1 = ta2.gather(gather_indices)\r\n    return t1\r\n\r\ntest_tensor_array_scatter_gather()\r\n```\r\n\r\n**Other info \/ logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.11"],"created_at":"2020-09-02T05:15:04Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/42878"},{"issue_number":467,"repository":"tensorflow\/tensorflow","title":"Memory leak when using MultiWorkerMirroredStrategy for distributed training","description":"**System information**\r\n\r\n- Have I written custom code: YES\r\n- OS Platform and Distribution: CentOS 7.3\r\n- TensorFlow installed from: pip\r\n- TensorFlow version: 2.3.0\r\n- Python version:3.7.7\r\n- CPU ONLY\r\n\r\n**Describe the current behavior**\r\n\r\nWhen I use `MultiWorkerMirroredStrategy` for distributed training, as the number of training epochs increases, memory usage of tensorflow is also increasing, until beyond the memory limitation.\r\n\r\nBut the memory usage of stand-alone(not distributed) training is always stable.\r\n\r\nBecause I use cpu only for distributed training, I can't get any memory infomation from tensorboard using profiler.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\nNote that I don't know how to use `MultiWorkerMirroredStrategy` in `colab`, so I just give the reproduce steps here, and it's very easy.\r\n\r\n1. Training Code (worker.py)\r\n\r\n```python\r\nimport os\r\nimport json\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom absl import app, flags\r\nimport numpy as np\r\n\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_string(\"logs\", \"logs\", \"logs dir\")\r\nflags.DEFINE_integer(\"index\", 0, \"worker index\")\r\n\r\nclass ThreeLayerMLP(keras.Model):\r\n    def __init__(self, name=None):\r\n        super().__init__(name=name)\r\n        self.dense_1 = layers.Dense(32, activation='relu', name='dense_1')\r\n        self.dense_2 = layers.Dense(16, activation='relu', name='dense_2')\r\n        self.pred_layer = layers.Dense(\r\n            1,\r\n            activation='sigmoid',\r\n            name='predictions',\r\n        )\r\n\r\n    def call(self, inputs, training=None):\r\n        print(inputs.shape)\r\n        x = self.dense_1(inputs)\r\n        x = self.dense_2(x)\r\n        return self.pred_layer(x)\r\n\r\n\r\ndef prepare_data():\r\n    np.random.seed(0)\r\n    x_train, y_train = (\r\n        np.random.random((6000000, 31)),\r\n        np.random.randint(2, size=(6000000, 1)),\r\n    )\r\n\r\n    x_val, y_val = (\r\n        np.random.random((10000, 31)),\r\n        np.random.randint(2, size=(10000, 1)),\r\n    )\r\n\r\n    return ((x_train, y_train), (x_val, y_val))\r\n\r\n\r\ndef main(argv):\r\n    del argv  # Unused args\r\n    tf_config = {\r\n        \"cluster\": {\r\n            \"worker\": [\"ip1:12345\", \"ip2:12345\"],\r\n        },\r\n        \"task\": {\r\n            \"index\": FLAGS.index,\r\n            \"type\": \"worker\"\r\n        }\r\n    }\r\n    os.environ[\"TF_CONFIG\"] = json.dumps(tf_config)\r\n    print(json.loads(os.environ[\"TF_CONFIG\"]))\r\n    # distributed strategy\r\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n    BATCH_SIZE_PER_REPLICA = 128\r\n    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\r\n    print('Number of devices: %d' % strategy.num_replicas_in_sync)\r\n\r\n    with strategy.scope():\r\n        model = ThreeLayerMLP(name='3_layer_mlp')\r\n        model.compile(\r\n            loss=tf.keras.losses.BinaryCrossentropy(),\r\n            optimizer=keras.optimizers.RMSprop(),\r\n            metrics=[\"AUC\"],\r\n        )\r\n\r\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(\r\n        log_dir=FLAGS.logs,\r\n        histogram_freq=10,\r\n        update_freq=100,\r\n    )\r\n\r\n    ((x_train, y_train), (x_val, y_val)) = prepare_data()\r\n\r\n    model.fit(\r\n        x_train,\r\n        y_train,\r\n        epochs=100,\r\n        batch_size=BATCH_SIZE,\r\n        validation_data=(x_val, y_val),\r\n        callbacks=[tensorboard_callback],\r\n    )\r\n\r\n\r\nif __name__ == '__main__':\r\n    app.run(main)\r\n```\r\n\r\n2. Distributed training: change the `ip1` and `ip2` to your machine's ip in the codes above, and execute the command below seperately:\r\n\r\n```shell\r\npython worker.py --index=0\r\npython worker.py --index=1\r\n```\r\n\r\n3. The memory change curve of distributed training in my machine is shown as below\uff1a\r\n![image](https:\/\/user-images.githubusercontent.com\/15494997\/91021608-94ee1c80-e626-11ea-9adc-c775b3ff575a.png)\r\n\r\n4. The memory usage of stand-alone training is only 3-4G.","labels":["stat:awaiting tensorflower","type:bug","comp:dist-strat"],"created_at":"2020-08-24T08:27:07Z","comments":27,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/42616"},{"issue_number":468,"repository":"tensorflow\/tensorflow","title":"tf.data.experimental.dense_to_ragged_batch fails with inputs from generator with unspecified shape in TF 2.3","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Both\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows + Linux (Colab)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\/A\r\n- TensorFlow installed from (source or binary): binary (PyPI)\r\n- TensorFlow version (use command below): 2.1, 2.2, 2.3\r\n- Python version: 3.6\r\n- Bazel version (if compiling from source): N\/A\r\n- GCC\/Compiler version (if compiling from source): N\/A\r\n- CUDA\/cuDNN version: 10.1\/7.6\r\n- GPU model and memory: Titan RTX\/P100\r\n\r\n**Describe the expected behavior**\r\nIn TF 2.1, 2.2, and 2.3 batching variable length elements work fine when generated from tensor slices:\r\n```python\r\nds = tf.data.Dataset.from_tensor_slices(tf.range(4))\r\n\r\n# Generate variable length elements via map.\r\n# First batch will have length 1. Subsequent batches will have length 2.\r\ndef f(x):\r\n  if x == 0:\r\n    return tf.ones([1,])\r\n  else:\r\n    return tf.ones([2,])\r\nds = ds.map(f)\r\n\r\n# Inspect individual elements.\r\nprint(\"Unbatched shapes:\")\r\nfor batch in ds:\r\n  print(batch.shape)\r\nprint()\r\n\r\n# Batch into ragged tensors.\r\nds = ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=2))\r\n\r\n# Inspect batched elements.\r\nprint(\"Batched shapes:\")\r\nfor batch in ds:\r\n  print(batch.to_tensor().shape)\r\n```\r\nOutputs:\r\n```\r\nUnbatched shapes:\r\n(1,)\r\n(2,)\r\n(2,)\r\n(2,)\r\n\r\nBatched shapes:\r\n(2, 2)\r\n(2, 2)\r\n```\r\n\r\nNow, in TF 2.1 and 2.2, this also works when the dataset consumes elements from a generator:\r\n```python\r\n# Generate elements via a generator.\r\n# First batch will have length 1. Subsequent batches will have length 2.\r\ndef gen():\r\n  for i in range(4):\r\n    if i == 0:\r\n      yield tf.ones((1,))\r\n    else:\r\n      yield tf.ones((2,))\r\nds = tf.data.Dataset.from_generator(gen, output_types=tf.float32)\r\n\r\n# Inspect individual elements.\r\nprint(\"Unbatched shapes:\")\r\nfor batch in ds:\r\n  print(batch.shape)\r\nprint()\r\n\r\n# Batch into ragged tensors.\r\nds = ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=2))\r\n\r\n# Inspect batched elements.\r\nprint(\"Batched shapes:\")\r\nfor batch in ds:\r\n  print(batch.to_tensor().shape)\r\n```\r\n\r\nOutputs:\r\n```\r\nUnbatched shapes:\r\n(1,)\r\n(2,)\r\n(2,)\r\n(2,)\r\n\r\nBatched shapes:\r\n(2, 2)\r\n(2, 2)\r\n```\r\nAs expected, we get identical outputs both before and after batching.\r\n\r\n\r\n**Describe the current behavior**\r\nIn TF 2.3, the generator version results in an error:\r\n```\r\n[...]\r\nInvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2]. [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-313ce2c9fab5> in <module>()\r\n     16 \r\n     17 print(\"Batched shapes:\")\r\n---> 18 for batch in ds:\r\n     19   print(batch.to_tensor().shape)\r\n[...]\r\nInvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2].\r\n```\r\n\r\nThe release notes for 2.3 mention:\r\n\r\n> - tf.data.experimental.dense_to_ragged_batch works correctly with tuples.\r\n> - tf.data.experimental.dense_to_ragged_batch to output variable ragged rank.\r\n\r\nPresumably this issue is related to these changes.\r\n\r\nHere's the relevant implementation for the actual batching in TF 2.2:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/2b96f3662bd776e277f86997659e61046b56c315\/tensorflow\/python\/data\/experimental\/ops\/batching.py#L371-L426\r\n\r\nAnd in TF 2.3:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/b36436b087bd8e8701ef51718179037cccdfc26e\/tensorflow\/python\/data\/experimental\/ops\/batching.py#L380-L452\r\n\r\nAs suggested by the changes above, the behavior prior to 2.3 is achieved again when the output shape is specified, even if unknown:\r\n```python\r\n# Generate elements via a generator.\r\n# First batch will have length 1. Subsequent batches will have length 2.\r\ndef gen():\r\n  for i in range(4):\r\n    if i == 0:\r\n      yield tf.ones((1,))\r\n    else:\r\n      yield tf.ones((2,))\r\n\r\n# Creating the generator explicitly specifying the unknown shape.\r\nds = tf.data.Dataset.from_generator(\r\n    gen,\r\n    output_types=tf.float32,\r\n    output_shapes=tf.TensorShape([None])\r\n)\r\n\r\n# Inspect individual elements.\r\nprint(\"Unbatched shapes:\")\r\nfor batch in ds:\r\n  print(batch.shape)\r\nprint()\r\n\r\n# Batch into ragged tensors.\r\nds = ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=2))\r\n\r\n# Inspect batched elements.\r\nprint(\"Batched shapes:\")\r\nfor batch in ds:\r\n  print(batch.to_tensor().shape)\r\n```\r\n\r\nOutputs:\r\n```\r\nUnbatched shapes:\r\n(1,)\r\n(2,)\r\n(2,)\r\n(2,)\r\n\r\nBatched shapes:\r\n(2, 2)\r\n(2, 2)\r\n```\r\n\r\nIt's definitely less convenient to have to specify the output shapes in the generator, requiring some refactoring when updating to 2.3 -- maybe the shapes could just default to unknown when batching if unspecified in the generator?\r\n\r\nI appreciate that this is an experimental function, which is noted in the [tf.data.experimental](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/experimental) docs:\r\n\r\n> Note that the tf.data.experimental API is not subject to the same backwards compatibility guarantees as tf.data, but **we will provide deprecation advice in advance of removing existing functionality**.\r\n\r\nIf this is intended behavior, then perhaps it could be documented somewhere as it does remove the \"functionality\" of being able to ragged batch elements from a generator without specified output shape :)\r\n\r\n\r\n**Standalone code to reproduce the issue**\r\n- [Colab: TF 2.1](https:\/\/colab.research.google.com\/drive\/1lm9aOqtGob7PD_LMflmCONuXrpA-M1Kg?usp=sharing) (Works)\r\n- [Colab: TF 2.2](https:\/\/colab.research.google.com\/drive\/1clcicwg22-DVYirFlR_IAlbWaAn-NfUf?usp=sharing) (Works)\r\n- [Colab: TF 2.3](https:\/\/colab.research.google.com\/drive\/1NK5SWwqUaMsisDBjsRe8E-gpc3kcLI5S?usp=sharing) (Fails)\r\n- [Colab: TF 2.3 with `output_shape` specified](https:\/\/colab.research.google.com\/drive\/1WKYYNm96xHIMfdBn_nNwAaN_BmurVuM1?usp=sharing) (Works)\r\n\r\n\r\n**Other info \/ logs**\r\nFull traceback of the error in TF 2.3:\r\n```\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/context.py in execution_mode(mode)\r\n   2101       ctx.executor = executor_new\r\n-> 2102       yield\r\n   2103     finally:\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py in _next_internal(self)\r\n    757             output_types=self._flat_output_types,\r\n--> 758             output_shapes=self._flat_output_shapes)\r\n    759 \r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/ops\/gen_dataset_ops.py in iterator_get_next(iterator, output_types, output_shapes, name)\r\n   2609     except _core._NotOkStatusException as e:\r\n-> 2610       _ops.raise_from_not_ok_status(e, name)\r\n   2611     except _core._FallbackException:\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/ops.py in raise_from_not_ok_status(e, name)\r\n   6842   # pylint: disable=protected-access\r\n-> 6843   six.raise_from(core._status_to_exception(e.code, message), None)\r\n   6844   # pylint: enable=protected-access\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/six.py in raise_from(value, from_value)\r\n\r\nInvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2]. [Op:IteratorGetNext]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-313ce2c9fab5> in <module>()\r\n     16 \r\n     17 print(\"Batched shapes:\")\r\n---> 18 for batch in ds:\r\n     19   print(batch.to_tensor().shape)\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py in __next__(self)\r\n    734 \r\n    735   def __next__(self):  # For Python 3 compatibility\r\n--> 736     return self.next()\r\n    737 \r\n    738   def _next_internal(self):\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py in next(self)\r\n    770   def next(self):\r\n    771     try:\r\n--> 772       return self._next_internal()\r\n    773     except errors.OutOfRangeError:\r\n    774       raise StopIteration\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py in _next_internal(self)\r\n    762         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    763       except AttributeError:\r\n--> 764         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    765 \r\n    766   @property\r\n\r\n\/usr\/lib\/python3.6\/contextlib.py in __exit__(self, type, value, traceback)\r\n     97                 value = type()\r\n     98             try:\r\n---> 99                 self.gen.throw(type, value, traceback)\r\n    100             except StopIteration as exc:\r\n    101                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/context.py in execution_mode(mode)\r\n   2103     finally:\r\n   2104       ctx.executor = executor_old\r\n-> 2105       executor_new.wait()\r\n   2106 \r\n   2107 \r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/executor.py in wait(self)\r\n     65   def wait(self):\r\n     66     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     68 \r\n     69   def clear_error(self):\r\n\r\nInvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2].\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.11"],"created_at":"2020-08-14T02:27:05Z","comments":3,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/42349"},{"issue_number":469,"repository":"tensorflow\/tensorflow","title":"tf.keras cannot weight classes when using multiple outputs","description":"This post is a mirror of https:\/\/github.com\/keras-team\/keras\/issues\/11735, showing the need to handle class weight for multiple outputs.\r\n\r\nVersion 2.2.0 used.\r\n\r\n------\r\n \r\nThis is a minimal source code, by @GalAvineri, to reproduce the issue (please comment\/uncomment the class weight line):\r\n\r\n````python3\r\nfrom tensorflow.python.keras.models import Model\r\nfrom tensorflow.python.keras.layers import Input, Dense\r\nfrom tensorflow.python.data import Dataset\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\ndef preprocess_sample(features, labels):\r\n    label1, label2 = labels\r\n    label1 = tf.one_hot(label1, 2)\r\n    label2 = tf.one_hot(label2, 3)\r\n    return features, (label1, label2)\r\n\r\n\r\nbatch_size = 32\r\n\r\nnum_samples = 1000\r\nnum_features = 10\r\n\r\nfeatures = np.random.rand(num_samples, num_features)\r\nlabels1 = np.random.randint(2, size=num_samples)\r\nlabels2 = np.random.randint(3, size=num_samples)\r\n\r\ntrain = Dataset.from_tensor_slices((features, (labels1, labels2))).map(preprocess_sample).batch(batch_size).repeat()\r\n\r\n# Model\r\ninputs = Input(shape=(num_features, ))\r\noutput1 = Dense(2, activation='softmax', name='output1')(inputs)\r\noutput2 = Dense(3, activation='softmax', name='output2')(inputs)\r\nmodel = Model(inputs, [output1, output2])\r\n\r\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\r\nclass_weights = {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}\r\nmodel.fit(train, epochs=10, steps_per_epoch=num_samples \/\/ batch_size,\r\n         #  class_weight=class_weights\r\n          )\r\n````\r\n\r\nUncommenting yields this error:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-38-d137ff6fb3f9> in <module>\r\n     33 class_weights = {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}\r\n     34 model.fit(train, epochs=10, steps_per_epoch=num_samples \/\/ batch_size,\r\n---> 35            class_weight=class_weights\r\n     36           )\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    813           workers=workers,\r\n    814           use_multiprocessing=use_multiprocessing,\r\n--> 815           model=self)\r\n    816 \r\n    817       # Container that configures and calls `tf.keras.Callback`s.\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)\r\n   1115     dataset = self._adapter.get_dataset()\r\n   1116     if class_weight:\r\n-> 1117       dataset = dataset.map(_make_class_weight_map_fn(class_weight))\r\n   1118     self._inferred_steps = self._infer_steps(steps_per_epoch, dataset)\r\n   1119     self._dataset = strategy.experimental_distribute_dataset(dataset)\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/data_adapter.py in _make_class_weight_map_fn(class_weight)\r\n   1233         \"Expected `class_weight` to be a dict with keys from 0 to one less \"\r\n   1234         \"than the number of classes, found {}\").format(class_weight)\r\n-> 1235     raise ValueError(error_msg)\r\n   1236 \r\n   1237   class_weight_tensor = ops.convert_to_tensor_v2(\r\n\r\nValueError: Expected `class_weight` to be a dict with keys from 0 to one less than the number of classes, found {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}\r\n````","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF 2.5"],"created_at":"2020-07-16T10:11:48Z","comments":55,"reactions":16,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/41448"},{"issue_number":470,"repository":"tensorflow\/tensorflow","title":"Windows - tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: ","description":"**System information**\r\n\r\nOS Name:                   Microsoft Windows 10 Enterprise\r\nOS Version:                10.0.17763 N\/A Build 17763\r\nTensorFlow installed using 'conda'.\r\ntensorflow v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\nPython 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 15:18:16) [MSC v.1916 64 bit (AMD64)] on win32\r\n\r\n**Describe the current behavior**\r\n\r\nSaving checkpoint files from tensorflow is failing on Windows 10.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"C:\\Git\\<redacted>\\tests\\integration\\validate_train_model.py\", line 216, in <module>\r\n    main()\r\n  File \"C:\\Git\\<redacted>\\tests\\integration\\validate_train_model.py\", line 176, in main\r\n    fig_save_freq = fig_save_freq)\r\n  File \"c:\\git\\<redacted>\\src\\pointnet\\model.py\", line 640, in fit\r\n    self.save_best_model()\r\n  File \"c:\\git\\<redacted>\\src\\pointnet\\model.py\", line 493, in save_best_model\r\n    check_interval = False)\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\", line 823, in save\r\n    self._record_state()\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\", line 728, in _record_state\r\n    save_relative_paths=True)\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py\", line 248, in update_checkpoint_state_internal\r\n    text_format.MessageToString(ckpt))\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 532, in atomic_write_string_to_file\r\n    rename(temp_pathname, filename, overwrite)\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 491, in rename\r\n    rename_v2(oldname, newname, overwrite)\r\n  File \"C:\\Users\\<redacted>\\Miniconda3\\envs\\<redacted>\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\", line 508, in rename_v2\r\n    compat.as_bytes(src), compat.as_bytes(dst), overwrite)\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to rename: tests\\files\\checkpoints\\0000_00_00_00_00_00\\checkpoint.tmpc6ee5d6bc5a445c884bba8c3acadf01f to: tests\\files\\checkpoints\\0000_00_00_00_00_00\\checkpoint : Access is denied.\r\n; Input\/output error\r\n```\r\n\r\nProblem traced to:\r\ntensorflow.python.lib.io.file_io, line 532, function atomic_write_string_to_file\r\n\r\nFrom debugging, tensorflow attempts to create, then overwrite a file while saving a checkpoint.  For some reason, the 'overwrite' parameter, although set to True, does nothing.  This causes the rename to fail (since the file seems to get created earlier in the checkpoint save process).\r\n\r\nWe tried deleting the 'checkpoint' file before the 'save', but the checkpoint file that it's trying to overwrite appears to be created as a part of the 'save' call.\r\n\r\nI was able to get checkpoint saving working again by modifying atomic_write_string_to_file as follows.  My change checks for existence of the rename target and deletes it using os.remove if overwrite is True, rather than relying on the tensorflow custom machinery that doesn't seem to be working:\r\n\r\n```python\r\ndef atomic_write_string_to_file(filename, contents, overwrite=True):\r\n  if not has_atomic_move(filename):\r\n    write_string_to_file(filename, contents)\r\n  else:\r\n    temp_pathname = filename + \".tmp\" + uuid.uuid4().hex\r\n    write_string_to_file(temp_pathname, contents)\r\n    try:\r\n      if overwrite and os.path.exists(filename):\r\n        os.remove(filename)\r\n      rename(temp_pathname, filename, overwrite)\r\n    except errors.OpError:\r\n      delete_file(temp_pathname)\r\n      raise\r\n```\r\n\r\nThe stack trace we got suggested that this is the same issue as someone was reporting for tensorflow.models:\r\nhttps:\/\/github.com\/tensorflow\/models\/issues\/4177\r\n\r\n**Describe the expected behavior**\r\n\r\nWe should be able to successfully save a checkpoint on Windows 10.\r\n","labels":["type:bug","comp:gpu","TF 2.5"],"created_at":"2020-07-14T17:04:28Z","comments":31,"reactions":9,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/41380"},{"issue_number":471,"repository":"tensorflow\/tensorflow","title":"Autograph applied to Keras Custom Loss during Eager Execution","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, x64\r\n- TensorFlow installed from (source or binary): binary (pip)\r\n- TensorFlow version (use command below): TF 2.4.0.dev20200707\r\n- Python version: 3.7\r\n- CUDA\/cuDNN version: 10.1 \/ 7.6\r\n- GPU model and memory: Bug appears on several computers with different GPU\r\n\r\n\r\n**Describe the current behavior**\r\nTensorflow applies AutoGraph to keras custom loss even in eager execution, meaning that we can't debug the loss anymore (unless using tf.print). This did not happen in previous versions of Tensorflow.\r\nNotice that it both happens when run_eagerly is set to True in model.compile() and when tf.config.run_functions_eagerly is set to True.\r\n\r\n**Describe the expected behavior**\r\nWhen run_eagerly=True is passed to the model during compilation, we should expect Tensorflow to run eagerly in the loss function.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\n# Custom Model. Autograph is not applied in eager execution so debugging is possible.\r\nclass CustomModel(keras.models.Model):\r\n\tdef __init__(self):\r\n\t\tsuper(CustomModel, self).__init__()\r\n\t\tself.layer = tf.keras.layers.Dense(3) # Can debug here\r\n\r\n\tdef call(self, inputs, training=None, mask=None):\r\n\t\tx = self.layer(inputs) # Can debug here\r\n\t\treturn x\r\n\r\n# Custom Loss. AutoGraph is applied in eager execution so debugging is impossible.\r\nclass CustomLoss(keras.losses.Loss):\r\n\tdef call(self, y_true, y_pred):\r\n\t\tx = tf.reduce_mean(tf.abs(y_pred-y_true)) # Cannot debug here\r\n\t\treturn x\r\n\r\nif __name__ == '__main__':\r\n\tdata = np.random.random((1000, 3)).astype(np.float32)\r\n\r\n\tmodel = CustomModel()\r\n\r\n\tmodel.compile(loss=CustomLoss(), run_eagerly=True)\r\n\tmodel.fit(x=data, y=data, batch_size=32)\r\n```\r\n\r\n\r\n**Other info \/ logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n","labels":["type:bug","comp:keras","comp:autograph","TF 2.2"],"created_at":"2020-07-08T09:06:35Z","comments":19,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/41189"},{"issue_number":472,"repository":"tensorflow\/tensorflow","title":"Zero AUROC when validation set is only comprised of positive labels","description":"**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tested on Ubuntu 18.04 and MacOS Catalina 10.15.5\r\n- TensorFlow installed from (source or binary): installed from Conda (tensorflow-gpu) on Ubuntu and from pip on MacOS\r\n- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version: 3.7.6 (on both machines)\r\n- CUDA\/cuDNN version: 11.0 (on Ubuntu machine)\r\n- GPU model and memory: 4 NVIDIA V100 GPUs\r\n\r\n**Describe the current behavior**\r\nThe AUROC is zero when there are no negatives in the validation set. This may lead users to believe that the model is not performing correctly, while it is just an error related to a misuse of the AUROC metric.\r\n\r\n**Describe the expected behavior**\r\nThe AUROC should be NaN when no negatives are present in the validation set. This way the user is aware that there is something wrong with the metric that is being used, with the possibility to add a warning since there is a 0\/0 somewhere in the computation of the AUROC.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nfrom tensorflow.keras.metrics import AUC\r\nmetric = AUC()\r\nmetric.update_state([1, 1, 1], [1, 0.5, 0.3])\r\nmetric.result()\r\n```\r\n\r\nI believe that similar issues also happen for other metrics, as uncanny values were displayed also for Recall and Precision, but I have not identified a simple reproducible example.","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF 2.11"],"created_at":"2020-07-04T13:03:01Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/41081"},{"issue_number":473,"repository":"tensorflow\/tensorflow","title":"\u201cLayer is not connected\u201d issue while accessing intermediate layer from custom callback if model is built by sub-classing","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nLinux Ubuntu 18.04.2 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN\/A\r\n- TensorFlow installed from (source or binary):\r\nBinary\r\n- TensorFlow version (use command below):\r\nv2.2.0-rc4-8-g2b96f3662b 2.2.0\r\n- Python version:\r\nPython 3.7.3\r\n- Bazel version (if compiling from source):\r\nN\/A\r\n- GCC\/Compiler version (if compiling from source):\r\nN\/A\r\n- CUDA\/cuDNN version:\r\nCUDA 10.2\r\n- GPU model and memory:\r\nNVIDIA TITAN X (Pascal), ~12GB\r\n\r\n**Describe the current behavior**\r\nI've a simple model and need access of intermediate layers within a custom callback to get intermediate predictions. If I build the model by sub-classing, I get the error `AttributeError: Layer dense is not connected`.\r\n\r\n**Describe the expected behavior**\r\nIt shouldn't cause any error and be able to get predictions using intermediate layers.\r\n\r\n**Standalone code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nX = np.ones((8,16))\r\ny = np.sum(X, axis=1)\r\n\r\nclass CustomCallback(tf.keras.callbacks.Callback):\r\n    def on_epoch_end(self, epoch, logs=None):\r\n        get_output = tf.keras.backend.function(\r\n            inputs = self.model.layers[0].input,\r\n            outputs = self.model.layers[1].output\r\n        )\r\n        print(\"\\nLayer output: \", get_output(X))\r\n\r\nclass Model(tf.keras.Model):\r\n    def build(self, input_shape):\r\n        self.dense1 = tf.keras.layers.Dense(units=32)\r\n        self.dense2 = tf.keras.layers.Dense(units=1)\r\n        \r\n    def call(self, input_tensor):\r\n        x = self.dense1(input_tensor)\r\n        x = self.dense2(x)\r\n        return x\r\n\r\nmodel = Model()\r\nmodel.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')\r\nmodel.fit(X,y, epochs=2, callbacks=[CustomCallback()])\r\n```\r\n\r\n**Other info \/ logs** \r\nTraceback:\r\n```---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-3-dd6e118e08d6> in <module>\r\n     11 model = Model()\r\n     12 model.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')\r\n---> 13 model.fit(X,y, epochs=2, callbacks=[CustomCallback()])\r\n\r\n\/opt\/anaconda3\/envs\/brats\/lib\/python3.7\/site-packages\/tensorflow\/python\/keras\/engine\/training.py in _method_wrapper(self, *args, **kwargs)\r\n     64   def _method_wrapper(self, *args, **kwargs):\r\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\r\n---> 66       return method(self, *args, **kwargs)\r\n     67 \r\n     68     # Running inside `run_distribute_coordinator` already.\r\n\r\n\/opt\/anaconda3\/envs\/brats\/lib\/python3.7\/site-packages\/tensorflow\/python\/keras\/engine\/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\r\n    874           epoch_logs.update(val_logs)\r\n    875 \r\n--> 876         callbacks.on_epoch_end(epoch, epoch_logs)\r\n    877         if self.stop_training:\r\n    878           break\r\n\r\n\/opt\/anaconda3\/envs\/brats\/lib\/python3.7\/site-packages\/tensorflow\/python\/keras\/callbacks.py in on_epoch_end(self, epoch, logs)\r\n    363     logs = self._process_logs(logs)\r\n    364     for callback in self.callbacks:\r\n--> 365       callback.on_epoch_end(epoch, logs)\r\n    366 \r\n    367   def on_train_batch_begin(self, batch, logs=None):\r\n\r\n<ipython-input-2-a1f33c1e2e52> in on_epoch_end(self, epoch, logs)\r\n      8     def on_epoch_end(self, epoch, logs=None):\r\n      9         get_output = tf.keras.backend.function(\r\n---> 10             inputs = self.model.layers[0].input,\r\n     11             outputs = self.model.layers[1].output\r\n     12         )\r\n\r\n\/opt\/anaconda3\/envs\/brats\/lib\/python3.7\/site-packages\/tensorflow\/python\/keras\/engine\/base_layer.py in input(self)\r\n   1806     if not self._inbound_nodes:\r\n   1807       raise AttributeError('Layer ' + self.name +\r\n-> 1808                            ' is not connected, no input to return.')\r\n   1809     return self._get_node_attribute_at_index(0, 'input_tensors', 'input')\r\n   1810 \r\n\r\nAttributeError: Layer dense is not connected, no input to return.\r\n```\r\nIf I build the model using functional API as shown below, it works fine:\r\n```\r\ninitial = tf.keras.layers.Input((16,))\r\nx = tf.keras.layers.Dense(units=32)(initial)\r\nfinal = tf.keras.layers.Dense(units=1)(x)\r\n\r\nmodel = tf.keras.Model(initial, final)\r\nmodel.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')\r\nmodel.fit(X,y, epochs=2, callbacks=[CustomCallback()])\r\n```\r\n[Here's](https:\/\/stackoverflow.com\/q\/62668398\/2679778) the stackoverflow question I created on the same issue.","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF 2.11"],"created_at":"2020-07-01T23:15:59Z","comments":5,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/41009"},{"issue_number":474,"repository":"tensorflow\/tensorflow","title":"tf.nn.ctc_beam_search_decoder does not pick path with highest probability at next time step","description":"<em>Please make sure that this is a bug. As per our\r\n[GitHub Policy](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/ISSUES.md),\r\nwe only address code\/doc bugs, performance issues, feature requests and\r\nbuild\/installation issues on GitHub. tag:bug_template<\/em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nYes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nColab Env (Linux Ubuntu 18.04)\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\nN\/A\r\n- TensorFlow installed from (source or binary):\r\nBinary (I think)\r\n- TensorFlow version (use command below):\r\nv2.2.0-0-g2b96f3662b 2.2.0\r\n- Python version:\r\n3.6.9\r\n- Bazel version (if compiling from source):\r\nN\/A\r\n- GCC\/Compiler version (if compiling from source):\r\nN\/A\r\n- CUDA\/cuDNN version:\r\nN\/A\r\n- GPU model and memory:\r\nNone\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https:\/\/github.com\/tensorflow\/tensorflow\/tree\/master\/tools\/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n1. TF 1.0: `python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"`\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nThe built-in CTC beam search decoder sometimes chooses a less probable path to keep in the beam by default than it could've when expanding a given step. In the Colab example provided below, I give a concrete example of when the path (0,) is retained in the beam instead of (0, 1, 0) despite the latter having a greater log-probability.\r\n\r\n**Describe the expected behavior**\r\n\r\n(0, 1, 0) should remain in the beam; (0,) should not.\r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab\/Jupyter\/any notebook.\r\n\r\nhttps:\/\/colab.research.google.com\/drive\/1i9gvj0VN2gMNloohbHW6ad3Ti7eiQQCM?usp=sharing\r\n\r\n**Other info \/ logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n\r\nWhen I was comparing against a simple python version [based off this Medium article](https:\/\/towardsdatascience.com\/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7), I noticed that whenever the results diverged, the root problem was always that a path that should've been kept in the beam wasn't. I suspect top-k sorting might be mucking up somewhere.\r\n\r\nThanks,\r\nSean\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","TF 2.4"],"created_at":"2020-06-23T20:25:10Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/40727"},{"issue_number":475,"repository":"tensorflow\/tensorflow","title":"tf.io.gfile \/ GCS fails to work on OpenSUSE","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux OpenSUSE Tumbleweed\r\n- TensorFlow installed from (source or binary): Binary (conda)\r\n- TensorFlow version (use command below): unknown 2.1.0\r\n- Python version: 3.7.5\r\n- CUDA\/cuDNN version: N\/A\r\n- GPU model and memory: N\/A\r\n\r\n**Describe the current behavior**\r\n```python\r\nimport tensorflow as tf\r\ntf.io.gfile.listdir(\"gs:\/\/some-bucket\") # replace w\/ bucket of your choice\r\n```\r\n\r\nThis code gives an error:\r\n```\r\n2020-06-01 15:43:56.684531: W tensorflow\/core\/platform\/cloud\/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"Unavailable: Error executing an HTTP request: libcurl code 77 meaning 'Problem with the SSL CA cert (path? access rights?)', error details: error setting certificate verify locations:\r\n  CAfile: \/etc\/ssl\/certs\/ca-certificates.crt\r\n  CApath: none\". Retrieving token from GCE failed with \"Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'\".\r\n```\r\n\r\nAfter that it hangs for a while, and then raises a `NotFoundError`.\r\n\r\nI believe this is because the libcurl packaged with tensorflow doesn't know where to find the ca-certificates bundle file on OpenSUSE, which is at `\/etc\/ssl\/ca-bundle.pem` rather than `\/etc\/ssl\/certs\/ca-certificates.crt`. Also, I installed through miniconda so there's another equivalent file at `$CONDA_PREFIX\/ssl\/cacert.pem`. Neither of these seems to be found by tensorflow.\r\n\r\n[This code](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/5597c17b6a677be5264ebda7cc31404f0ae8a434\/tensorflow\/core\/platform\/cloud\/curl_http_request.cc#L129-L132) suggests that the bundle file's location can be customized with the `CURL_CA_BUNDLE` env variable. However, this doesn't change the behavior as far as i can tell; the error is still raised.\r\n\r\n**Describe the expected behavior**\r\nIt should list the contents of the bucket.","labels":["type:bug","comp:core","TF 2.11"],"created_at":"2020-06-01T20:05:13Z","comments":18,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/40065"},{"issue_number":476,"repository":"tensorflow\/tensorflow","title":"Unable to log scalar summaries in XLA","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5 Beta\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n\/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.8.2\r\n- Bazel version (if compiling from source): n\/a\r\n- GCC\/Compiler version (if compiling from source): n\/a\r\n- CUDA\/cuDNN version: n\/a\r\n- GPU model and memory: n\/a\r\n\r\n**Describe the current behavior**\r\nForcing XLA compilation via `experimental_compile=True` in a `tf.function` raises `tensorflow.python.framework.errors_impl.InvalidArgumentError` when the function contains `tf.summary.scalar` (I haven't tried other summaries). \r\n\r\n**Describe the expected behavior**\r\nPassing `experimental_compile=True` should log the scalar without raising any errors. \r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.function(\r\n    experimental_compile=True,\r\n)\r\ndef test_summaries():\r\n    tf.summary.scalar('testing', 12.3)\r\n\r\nwith tf.summary.create_file_writer('.\/logs').as_default():\r\n    tf.summary.experimental.set_step(0)\r\n    test_summaries()\r\n```\r\n\r\n**Other info \/ logs**\r\n```\r\n2020-05-22 22:30:57.072264: I tensorflow\/core\/platform\/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2020-05-22 22:30:57.091548: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x7f99eef80060 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\r\n2020-05-22 22:30:57.091590: I tensorflow\/compiler\/xla\/service\/service.cc:176]   StreamExecutor device (0): Host, Default Version\r\nTraceback (most recent call last):\r\n  File \"testsum.py\", line 12, in <module>\r\n    test_summaries()\r\n  File \"\/private\/tmp\/venv\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/def_function.py\", line 576, in __call__\r\n    result = self._call(*args, **kwds)\r\n  File \"\/private\/tmp\/venv\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/def_function.py\", line 650, in _call\r\n    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access\r\n  File \"\/private\/tmp\/venv\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/function.py\", line 1661, in _filtered_call\r\n    return self._call_flat(\r\n  File \"\/private\/tmp\/venv\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/function.py\", line 1745, in _call_flat\r\n    return self._build_call_outputs(self._inference_function.call(\r\n  File \"\/private\/tmp\/venv\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/function.py\", line 593, in call\r\n    outputs = execute.execute(\r\n  File \"\/private\/tmp\/venv\/lib\/python3.8\/site-packages\/tensorflow\/python\/eager\/execute.py\", line 59, in quick_execute\r\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_test_summaries_22}} = __inference_test_summaries_22[_XlaMustCompile=true, config_proto=\"\\n\\007\\n\\003GPU\\020\\000\\n\\007\\n\\003CPU\\020\\0012\\002J\\0008\\001\", executor_type=\"\"](dummy_input).\r\nUncompilable nodes:\r\ntesting\/write_summary\/tag: unsupported op: Const op with type DT_STRING is not supported by XLA.\r\n\tStacktrace:\r\n\t\tNode: __inference_test_summaries_22, function: \r\n\t\tNode: testing\/write_summary\/tag, function: __inference_test_summaries_22\r\n\r\ntesting\/write_summary\/summary_metadata: unsupported op: Const op with type DT_STRING is not supported by XLA.\r\n\tStacktrace:\r\n\t\tNode: __inference_test_summaries_22, function: \r\n\t\tNode: testing\/write_summary\/summary_metadata, function: __inference_test_summaries_22\r\n\r\ntesting\/write_summary: unsupported op: No registered 'WriteSummary' OpKernel for XLA_CPU_JIT devices compatible with node {{node testing\/write_summary}}\r\n\tStacktrace:\r\n\t\tNode: __inference_test_summaries_22, function: \r\n\t\tNode: testing\/write_summary, function: __inference_test_summaries_22\r\n [Op:__inference_test_summaries_22]\r\n```\r\n","labels":["comp:tensorboard","stat:awaiting tensorflower","type:bug","comp:xla","TF 2.11"],"created_at":"2020-05-23T02:34:43Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/39798"},{"issue_number":477,"repository":"tensorflow\/tensorflow","title":"tf.Module.name_scope overrides parent scopes","description":"**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: macOS 10.15.5\r\n- Mobile device if the issue happens on mobile device: n\/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.2.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): n\/a\r\n- GCC\/Compiler version (if compiling from source): n\/a\r\n- CUDA\/cuDNN version: n\/a\r\n- GPU model and memory: n\/a\r\n\r\n**Describe the current behavior**\r\nThe `name_scope` of a `tf.Module` instance has a forward slash appended to the name, which causes the scope to disregard any parent scopes. Using `with tf.Module.name_scope` and `with tf.name_scope(mymod.name_scope.name):` both result in this issue. \r\n\r\n**Describe the expected behavior**\r\nEach of the following should have the same outcome:\r\n- `with tf.Module.name_scope:`\r\n- `with tf.name_scope(tf.Module.name_scope.name):`\r\n- `with tf.name_scope(tf.Module.name):`\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\ndef my_test_mod_scopes():\r\n    mymod = tf.Module(name='mymod')\r\n    with tf.name_scope('first_scope'):\r\n         with mymod.name_scope:\r\n             tf.summary.scalar('scalar1', 84.9)\r\n         with tf.name_scope(mymod.name):\r\n             tf.summary.scalar('scalar2', 74.3)\r\n         with tf.name_scope(mymod.name_scope.name):\r\n             tf.summary.scalar('scalar3', 79.7)\r\n\r\nwith tf.summary.create_file_writer('.\/logs').as_default():\r\n    tf.summary.experimental.set_step(0)\r\n    my_test_mod_scopes()\r\n```\r\n![Screen Shot 2020-05-07 at 21 43 20](https:\/\/user-images.githubusercontent.com\/31281983\/81361269-e7bdbb80-90ab-11ea-84b9-4d41f84250b8.png)\r\n\r\n**Other info \/ logs** Include any logs or source code that would be helpful to\r\ndiagnose the problem. If including tracebacks, please include the full\r\ntraceback. Large logs and files should be attached.\r\n","labels":["comp:tensorboard","stat:awaiting tensorflower","type:bug","TF 2.5"],"created_at":"2020-05-08T01:45:08Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/39289"},{"issue_number":478,"repository":"tensorflow\/tensorflow","title":"tf.io.gfile.listdir is inconsistent between GCS dir and local dir - adds trailing slashes","description":"\r\n**System information**\r\n- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\n\r\nWhen listing the directory items for a GCS dir, listdir returns names with trailing slashes. \r\nWhen listing the directory items for a local dir, listdir returns names without trailing slashes. \r\n\r\n**Describe the expected behavior**\r\n\r\nThe behavior needs to be consistent.\r\n\r\n**Standalone code to reproduce the issue**\r\n\r\n```\r\ntensorflow.io.gfile.listdir('gs:\/\/bucket\/dir')\r\n>>> ['eval\/', 'train\/']\r\n\r\ntensorflow.io.gfile.listdir('..')\r\n>>> ['eval', 'train']\r\n```","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.7"],"created_at":"2020-05-04T21:00:50Z","comments":22,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/39167"},{"issue_number":479,"repository":"tensorflow\/tensorflow","title":"No default summary writer available when using tf.py_function with autograph","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 3.6.8\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5 Beta\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n\/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source): n\/a\r\n- GCC\/Compiler version (if compiling from source): n\/a\r\n- CUDA\/cuDNN version: n\/a\r\n- GPU model and memory: n\/a\r\n\r\n**Describe the current behavior**\r\nUsing `tf.summary` returns `False` inside a `tf.py_function` when using autograph.\r\n\r\n**Describe the expected behavior**\r\n`tf.summary` should return `True`.\r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n@tf.function\r\ndef eager_pyfunc():\r\n    def inner_func():\r\n        bool_out = tf.summary.scalar('myscalar', tf.constant(32.9))\r\n        tf.print(bool_out, name='log-myscalar-success')\r\n\r\n    tf.py_function(inner_func, [], [], name='log-myscalar')\r\n\r\n    bool_out2 = tf.summary.scalar('myscalar2', tf.constant(52.3))\r\n    tf.print(bool_out2, name='log-myscalar2-success')\r\n\r\n\r\nwith tf.summary.create_file_writer('.\/logs').as_default():\r\n    tf.summary.experimental.set_step(0)\r\n    eager_pyfunc()\r\n```\r\noutputs:\r\n```\r\n0\r\n1\r\n```\r\nRemoving the `tf.function` outputs:\r\n```\r\n1\r\n1\r\n```\r\n\r\n**Other info \/ logs** \r\nCould be related to https:\/\/github.com\/tensorflow\/tensorflow\/issues\/26409.","labels":["comp:tensorboard","stat:awaiting tensorflower","type:bug","TF 2.12"],"created_at":"2020-04-21T22:29:48Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/38772"},{"issue_number":480,"repository":"tensorflow\/tensorflow","title":"TPU PyFunction results in UnavailableError: failed to connect to all addresses","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Modified [Colab MNIST guide](https:\/\/www.tensorflow.org\/guide\/tpu)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\r\n- TensorFlow version (use command below): `2.2-rc3`\r\n\r\n**Describe the current behavior**\r\nWhen processing pipeline for `tf.data.Dataset` contains usage of `tf.py_function` the `UnavailableError: failed to connect to all addresses` is thrown on TPU environment.\r\n\r\n**Describe the expected behavior**\r\n`tf.py_function` is working on TPU environments. \r\n\r\n**Standalone code to reproduce the issue**\r\n[Colab notebook](https:\/\/colab.research.google.com\/drive\/1D7qU4f1FZqieYHdyUezUEFPWoVZZJSxi) with simplified example. In my original code the preprocessing function is more complicated. \r\n\r\n**Other info \/ logs**\r\nRelated issue: [34346](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/34346).\r\nStacktrace:\r\n```\r\n---------------------------------------------------------------------------\r\nUnavailableError                          Traceback (most recent call last)\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/context.py in execution_mode(mode)\r\n   1985       ctx.executor = executor_new\r\n-> 1986       yield\r\n   1987     finally:\r\n\r\n14 frames\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py in _next_internal(self)\r\n    660       except AttributeError:\r\n--> 661         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    662 \r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/data\/util\/structure.py in from_compatible_tensor_list(element_spec, tensor_list)\r\n    229       lambda spec, value: spec._from_compatible_tensor_list(value),\r\n--> 230       element_spec, tensor_list)\r\n    231 \r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/data\/util\/structure.py in _from_tensor_list_helper(decode_fn, element_spec, tensor_list)\r\n    204     value = tensor_list[i:i + num_flat_values]\r\n--> 205     flat_ret.append(decode_fn(component_spec, value))\r\n    206     i += num_flat_values\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/data\/util\/structure.py in <lambda>(spec, value)\r\n    228   return _from_tensor_list_helper(\r\n--> 229       lambda spec, value: spec._from_compatible_tensor_list(value),\r\n    230       element_spec, tensor_list)\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/tensor_spec.py in _from_compatible_tensor_list(self, tensor_list)\r\n    176     assert len(tensor_list) == 1\r\n--> 177     tensor_list[0].set_shape(self._shape)\r\n    178     return tensor_list[0]\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/ops.py in set_shape(self, shape)\r\n   1103   def set_shape(self, shape):\r\n-> 1104     if not self.shape.is_compatible_with(shape):\r\n   1105       raise ValueError(\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/framework\/ops.py in shape(self)\r\n   1066       except core._NotOkStatusException as e:\r\n-> 1067         six.raise_from(core._status_to_exception(e.code, e.message), None)\r\n   1068 \r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/six.py in raise_from(value, from_value)\r\n\r\nUnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1587494349.376555159\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party\/grpc\/src\/core\/ext\/filters\/client_channel\/client_channel.cc\",\"file_line\":3959,\"referenced_errors\":[{\"created\":\"@1587494349.376552078\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party\/grpc\/src\/core\/ext\/filters\/client_channel\/lb_policy\/pick_first\/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nUnavailableError                          Traceback (most recent call last)\r\n<ipython-input-8-f9a6a321af70> in <module>()\r\n      1 train_dataset, test_dataset = get_dataset()\r\n----> 2 list(train_dataset.take(1))\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py in __next__(self)\r\n    629 \r\n    630   def __next__(self):  # For Python 3 compatibility\r\n--> 631     return self.next()\r\n    632 \r\n    633   def _next_internal(self):\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py in next(self)\r\n    668     \"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\r\n    669     try:\r\n--> 670       return self._next_internal()\r\n    671     except errors.OutOfRangeError:\r\n    672       raise StopIteration\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/data\/ops\/iterator_ops.py in _next_internal(self)\r\n    659         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access\r\n    660       except AttributeError:\r\n--> 661         return structure.from_compatible_tensor_list(self._element_spec, ret)\r\n    662 \r\n    663   @property\r\n\r\n\/usr\/lib\/python3.6\/contextlib.py in __exit__(self, type, value, traceback)\r\n     97                 value = type()\r\n     98             try:\r\n---> 99                 self.gen.throw(type, value, traceback)\r\n    100             except StopIteration as exc:\r\n    101                 # Suppress StopIteration *unless* it's the same exception that\r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/context.py in execution_mode(mode)\r\n   1987     finally:\r\n   1988       ctx.executor = executor_old\r\n-> 1989       executor_new.wait()\r\n   1990 \r\n   1991 \r\n\r\n\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/eager\/executor.py in wait(self)\r\n     65   def wait(self):\r\n     66     \"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\r\n---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)\r\n     68 \r\n     69   def clear_error(self):\r\n\r\nUnavailableError: failed to connect to all addresses\r\nAdditional GRPC error information:\r\n{\"created\":\"@1587494349.376555159\",\"description\":\"Failed to pick subchannel\",\"file\":\"third_party\/grpc\/src\/core\/ext\/filters\/client_channel\/client_channel.cc\",\"file_line\":3959,\"referenced_errors\":[{\"created\":\"@1587494349.376552078\",\"description\":\"failed to connect to all addresses\",\"file\":\"third_party\/grpc\/src\/core\/ext\/filters\/client_channel\/lb_policy\/pick_first\/pick_first.cc\",\"file_line\":394,\"grpc_status\":14}]}\r\n```","labels":["stat:awaiting tensorflower","type:bug","comp:tpus","TF 2.9"],"created_at":"2020-04-21T18:42:26Z","comments":33,"reactions":20,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/38762"},{"issue_number":481,"repository":"tensorflow\/tensorflow","title":"tf.name_scope with spaces does not raise ValueError in eager execution ","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5 Beta\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n\/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.6.8\r\n- Bazel version (if compiling from source):  n\/a\r\n- GCC\/Compiler version (if compiling from source): n\/a\r\n- CUDA\/cuDNN version: n\/a\r\n- GPU model and memory: n\/a\r\n\r\n**Describe the current behavior**\r\nIn autograph, `tf.name_scope` does not allow spaces in the string argument. In eager execution, `tf.name_scope` *does* allow spaces in the string argument. \r\n\r\n**Describe the expected behavior**\r\nThe constraints on `tf.name_scope` should be the same across eager execution and autograph. \r\n\r\n**Standalone code to reproduce the issue**\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\n# eager execution since there's no @tf.function decorator\r\ndef graphcry():\r\n    myscalar = tf.constant(83.2)  # just a random number\r\n\r\n    with tf.name_scope('scalaragain scope'):  # doesn't work\r\n        tf.summary.scalar('scalaragain', data=myscalar)\r\n\r\n    with tf.name_scope('nospace_scope'):  # works\r\n        tf.summary.scalar('nospace', data=myscalar)\r\n\r\ngraphcry()\r\n```\r\n\r\n**Other info \/ logs**\r\nCC @jvishnuvardhan, also see #38661 \r\n","labels":["stat:awaiting tensorflower","type:bug","comp:eager","comp:ops","TF 2.9"],"created_at":"2020-04-21T14:48:08Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/38754"},{"issue_number":482,"repository":"tensorflow\/tensorflow","title":"TF saved model Assertion Error ( Called a function referencing variables which have been deleted )","description":"I am facing an issue . When returning a ```tf.saved_model.load``` object inside a function and then try to use it, it is not working. \r\n\r\nI am having a file ```sample.py``` \r\n```\r\n#### sample.py\r\n\r\nimport tensorflow as tf\r\ndef load_model(model_dir):\r\n\r\n    # Load Model\r\n    loaded = tf.saved_model.load(model_dir)\r\n    model = loaded.signatures['serving_default']\r\n    print(\"Model Loaded\")\r\n    return model\r\n\r\n```\r\n\r\nWhen I am executing ```main.py```\r\n\r\n```\r\nfrom sample import load_model\r\n\r\nmodel_dir = 'som_path of a saved model'\r\nmodel1 = load_model(model_dir)\r\n```\r\n\r\nIf I print model.variables I am getting following error\r\n\r\n```\r\nAssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.\r\n\r\n```\r\n\r\nBut. If load the model with same code inside the function, but not using the function it works fine\r\n\r\n```\r\n#### main.py\r\nloaded = tf.saved_model.load(model_dir)\r\nmodel = loaded.signatures['serving_default']\r\n```\r\nIf I print model.variables, its working as expected. ","labels":["type:bug","comp:apis","TF 2.11"],"created_at":"2020-03-15T16:25:56Z","comments":26,"reactions":5,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/37615"},{"issue_number":483,"repository":"tensorflow\/tensorflow","title":"tf.data.Dataset unusable with steps_per_epoch standard training loop","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint \r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1.0\r\n- Python version: 3.7.4\r\n\r\n**Describe the current behavior**\r\n\r\nThis is the underlying issue for https:\/\/github.com\/tensorflow\/tensorflow\/issues\/36153\r\n\r\nBasically:\r\n- `Dataset` can be used as\/converted to an iterator\r\n- Once the iterator reaches the end of the dataset it does not restart -> Yields no more samples\r\n- `steps_per_epoch` for the `fit` method of a keras model can be used to specify the number of batches to run per epoch, more exactly: The number of times the iterator is advanced\/dereferenced\r\n- `steps_per_epoch` is required for e.g. if not the full dataset should be traversed. Either due to external requirements or to avoid using the trailing (incomplete) batch.\r\n- `steps_per_epoch` is absolutely required to be set for `MultiWorkerMirroredStrategy`: https:\/\/www.tensorflow.org\/tutorials\/distribute\/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy\r\n- Whether to recreate an iterator for each epoch out of the dataset is determined by `DatasetAdapter.should_recreate_iterator` at https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.1.0\/tensorflow\/python\/keras\/engine\/training_v2.py#L241-L242\r\n- Note that the iterator absolutely must be recreated for all common use cases:\r\n  - Iterate over full dataset\r\n  - Iterate over full dataset except incomplete trailing batch\r\n  - Iterate over a random subset of the full dataset per epoch\r\n  - Not to be recreated for: Infinite datasets. **Maybe** If the full dataset should be consumed over multiple epochs. But why? What should happen if the dataset is exhausted after some epoch? Usually restart, right?\r\n\r\nIn the current TF (2.1.0) the implementation of `DatasetAdapter.should_recreate_iterator` is: `return self.get_size() is not None or steps_per_epoch is None`: https:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.1.0\/tensorflow\/python\/keras\/engine\/data_adapter.py#L208\r\n\r\nThis is **wrong**. For datasets the size is always None (see https:\/\/github.com\/tensorflow\/tensorflow\/issues\/36531 which intends for this to be changed) and as motivated above having `steps_per_epoch` set is a common use case but the iterator should still be recreated on each epoch.\r\n\r\nThis was recently changed to ` (self._user_steps is None or cardinality.cardinality(self._dataset).numpy() == self._user_steps)` https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6be131d0860559954c42685a87c63f16cebb2185#diff-f8dd40712ac721c1b363e1a1ec44c1a3R741-R747\r\n\r\nThis is also **wrong**. Again assume `_user_steps` is set (see above reasoning): First the `cardinality` might be unknown, e.g. for any TFDS dataset (and `TFRecordDataset`) it is unknown. I guess due to use of `interleave` in the dataset reader. Second even if the size was known it may not be equal to the number of steps. Common example: Skipping the last batch.\r\n\r\n**Describe the expected behavior**\r\n\r\nThis is a design issue and hence hard to resolve.\r\n\r\nIn general it would be best to eliminate the UNKNOWN size of a dataset. But when reading data line-by-line from a file it might not be known upfront. So the user has to input the size of the dataset or specify explicitly `AUTO` which would iterate over the whole dataset once to get the number of samples. This can be costly but should not be possible in general (e.g. TFDS knows the number of samples)\r\n\r\nI think the sanest approach would be to default to recreating the iterator on each epoch UNLESS the dataset is known to be infinite. This might still be wrong for cases I can't imagine right now but is correct for all cases I can think of. Maybe even allow the user to specify this, but this default is IMO way better than the current.\r\n\r\nThe other approach would be to recreate the iterator when it runs out of data after starting an epoch. This would partially solve the issue, but fails for:\r\n- Omitting the trailing batch: It would yield the incomplete batch from the last epoch first, but that should be skipped.\r\n- Using only some random samples per batch but wanting to shuffle before each batch: It would happily consume the rest of the samples and not see the samples used in an earlier epoch until it runs out of data\r\n\r\nWith the UNKNOWN size fixed, one could also fix the check at  https:\/\/github.com\/tensorflow\/tensorflow\/commit\/6be131d0860559954c42685a87c63f16cebb2185#diff-f8dd40712ac721c1b363e1a1ec44c1a3R741-R747 to check if `_user_steps` yields 1 epoch (and optional trailing batches) by using `size \/\/ steps == 1` which would be better than the previous approach (recreate iterator when out of data) as the use case with of Omitting the trailing batch is covered. But it would fail for the other.\r\n\r\nSo my suggestion would to\r\n- (optional) avoid UNKNOWN sizes\r\n- recreate iterators unless dataset is infinite by default\r\n- allow the user to overwrite this explicitly, maybe via `with_options` of the dataset\r\n\r\n\r\n**Code to reproduce the issue**\r\nSome reduced example code based on e.g. https:\/\/www.tensorflow.org\/tutorials\/distribute\/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_datasets as tfds\r\nimport tensorflow as tf\r\nfrom tensorflow.python.data.experimental.ops import cardinality\r\nimport numpy as np\r\ntfds.disable_progress_bar()\r\n\r\n# Scaling MNIST data from (0, 255] to (0., 1.]\r\ndef scale(image, label):\r\n    image = tf.cast(image, tf.float32)\r\n    image \/= 255\r\n    return image, label\r\n\r\ndef build_and_compile_cnn_model():\r\n    model = tf.keras.Sequential([\r\n        tf.keras.layers.Conv2D(32,\r\n                               3,\r\n                               activation='relu',\r\n                               input_shape=(28, 28, 1)),\r\n        tf.keras.layers.MaxPooling2D(),\r\n        tf.keras.layers.Flatten(),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(10, activation='softmax')\r\n    ])\r\n    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\r\n                  optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\r\n                  metrics=['accuracy'])\r\n    return model\r\n\r\nBATCH_SIZE = 64\r\nif False:\r\n    examples = np.ones([10*BATCH_SIZE,28,28,1])\r\n    labels = np.ones([examples.shape[0]])\r\n    dataset = tf.data.Dataset.from_tensor_slices((examples, labels))\r\n    num_examples = examples.shape[0]\r\nelse:\r\n    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\r\n    dataset = datasets['test']\r\n    num_examples = info.splits['test'].num_examples\r\n\r\nx = dataset.map(scale).cache().shuffle(10000).batch(BATCH_SIZE)\r\nmodel = build_and_compile_cnn_model()\r\n\r\ncard = cardinality.cardinality(x)\r\nnum_batches = sum(1 for _ in x)\r\nfull_batches = num_examples \/\/ BATCH_SIZE\r\nprint(\"Samples: %s\\nBatches: %s (%s full)\\nCardinality: %s\" % (num_examples, num_batches, full_batches, card))\r\nmodel.fit(x=x, epochs=2, steps_per_epoch=full_batches)\r\n\r\n```\r\n\r\n**Other info \/ logs**\r\nThere is a warning:\r\n> WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches). You may need to use the repeat() function when building your dataset.\r\n\r\nIt seems that adding `repeat()` and hence creating an infinite dataset is a viable option. However the docu also states\r\n> In TF 1.X, the idiomatic way to create epochs was through the `repeat` transformation:\r\n>     In TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it possible to also create epochs through Python iteration:\r\n\r\nSo it seems that it should not be required and as per above explanation the return value of `DatasetAdapter.should_recreate_iterator` is not correct.","labels":["stat:awaiting tensorflower","type:bug","comp:data","TF 2.10"],"created_at":"2020-02-07T13:30:41Z","comments":14,"reactions":2,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/36539"},{"issue_number":484,"repository":"tensorflow\/tensorflow","title":"How can I clear GPU memory in tensorflow 2?","description":"\r\n\r\n### System information\r\n- Custom code; nothing exotic though.\r\n- Ubuntu 18.04\r\n- installed from source (with pip)\r\n- tensorflow version v2.1.0-rc2-17-ge5bf8de\r\n- 3.6\r\n- CUDA 10.1\r\n- Tesla V100, 32GB RAM\r\n\r\nI created a model, nothing especially fancy in it. When I create the model, when using nvidia-smi, I can see that tensorflow takes up nearly all of the memory. When I try to fit the model with a small batch size, it successfully runs. When I fit with a larger batch size, it runs out of memory. Nothing unexpected so far. \r\n\r\nHowever, the only way I can then release the GPU memory is to restart my computer. When I run nvidia-smi I can see the memory is still used, but there is no process using a GPU. Also, If I try to run another model, it fails much sooner. \r\n\r\nNothing in the first five pages of google results works. (and most solutions are for TF1)\r\n\r\nIs there any way to release GPU memory in tensorflow 2?","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.7"],"created_at":"2020-02-04T15:16:15Z","comments":123,"reactions":83,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/36465"},{"issue_number":485,"repository":"tensorflow\/tensorflow","title":"AttributeError: 'Tensor' object has no attribute 'log_prob'","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina (Version: 10.15.2 (19C57))\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.1\r\n- Python version: 3.7.5\r\n- GPU model and memory: Intel Iris Pro 1536 MB\r\n\r\n**Describe the current behavior**\r\n\r\nI get the error\r\n\r\n> AttributeError: 'Tensor' object has no attribute 'log_prob'\r\n\r\nwith TensorFlow Probability 0.9 (and TF 2.1).\r\n\r\n**Describe the expected behavior**\r\n\r\nNo error.\r\n\r\n**Code to reproduce the issue**\r\n\r\nThe following code\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tensorflow_probability as tfp\r\nfrom tensorflow_probability import distributions as tfd\r\n\r\n\r\ndef get_mnist_data(normalize=True):\r\n    img_rows, img_cols = 28, 28\r\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n\r\n    if tf.keras.backend.image_data_format() == 'channels_first':\r\n        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n        input_shape = (1, img_rows, img_cols)\r\n    else:\r\n        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n        input_shape = (img_rows, img_cols, 1)\r\n\r\n    x_train = x_train.astype('float32')\r\n    x_test = x_test.astype('float32')\r\n\r\n    if normalize:\r\n        x_train \/= 255\r\n        x_test \/= 255\r\n\r\n    return x_train, y_train, x_test, y_test, input_shape\r\n\r\n\r\ndef get_bayesian_cnn(input_shape, num_classes=10):\r\n    model_input = tf.keras.layers.Input(shape=input_shape)\r\n\r\n    # kernel_divergence_fn=None to solve a symbolic exception.\r\n    x = tfp.layers.Convolution2DFlipout(6, kernel_size=(5, 5), padding=\"SAME\", activation=tf.nn.relu,\r\n                                        kernel_divergence_fn=None)(model_input)\r\n    x = tf.keras.layers.Flatten()(x)\r\n    x = tfp.layers.DenseFlipout(84, activation=tf.nn.relu)(x)\r\n    x = tfp.layers.DenseFlipout(num_classes)(x)\r\n\r\n    model_output = tfp.layers.DistributionLambda(lambda t: tfd.Categorical(logits=t, validate_args=True))(x)\r\n\r\n    model = tf.keras.Model(model_input, model_output)\r\n\r\n    return model\r\n\r\n\r\ndef neg_log_likelihood(y_true, y_pred):\r\n    return -tf.reduce_mean(y_pred.log_prob(tf.cast(tf.argmax(y_true, axis=-1), tf.int32)))\r\n\r\n\r\ndef train():\r\n    x_train, y_train, x_test, y_test, input_shape = get_mnist_data()\r\n\r\n    model = get_bayesian_cnn(input_shape=input_shape)\r\n\r\n    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=neg_log_likelihood,\r\n                  metrics=[neg_log_likelihood])\r\n\r\n    model.fit(x_train, y_train, batch_size=128, epochs=1, verbose=1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    train()\r\n```\r\n\r\n**Comments**\r\n\r\nThis error seems to be due to the fact that `y_pred` is a tensor when the loss is called, while it should be a distribution. Meanwhile, I found a [question on Stack Overflow related to the third issue I mentioned above](https:\/\/stackoverflow.com\/q\/59743872\/3924118). \r\n\r\n(_This is a duplicate issue of https:\/\/github.com\/tensorflow\/probability\/issues\/742, but, for completeness, I decided to open it here too._)","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.11"],"created_at":"2020-01-24T13:50:40Z","comments":15,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/36181"},{"issue_number":486,"repository":"tensorflow\/tensorflow","title":"tf.io.gfile.glob does not list all files in a Google Cloud Storage bucket","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): ?\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: \/\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 2.0.0\r\n- Python version: 3\r\n- Bazel version (if compiling from source): \/\r\n- GCC\/Compiler version (if compiling from source): \/\r\n- CUDA\/cuDNN version: \/\r\n- GPU model and memory: \/\r\n\r\n**Describe the current behavior**\r\nWhen listing file with `tf.io.gfile.glob` not all images are returned. It seems it is not resolving the folders recursively. \r\nWhen using the same path with gsutils  we get the correct image count.\r\n\r\n\r\n**Describe the expected behavior**\r\nWhen using the same gs:\/\/ path with **gsutils** we get the correct amount of images. \r\n\r\n**Code to reproduce the issue**\r\nIn order to reproduce the behavior I prepared a Google Bucket with the following structure. The bucket is public accessible, please feel free to use it to reproduce the behavior on your end: `gs:\/\/tensorflow-issue-reproduction`\r\n\r\n![level0](https:\/\/user-images.githubusercontent.com\/1991664\/69040135-93375e80-09ed-11ea-927f-da445d2bae10.png)\r\n![level1](https:\/\/user-images.githubusercontent.com\/1991664\/69040139-94688b80-09ed-11ea-9e19-28af47c3bcee.png)\r\n![level2](https:\/\/user-images.githubusercontent.com\/1991664\/69040146-96cae580-09ed-11ea-8894-c69a82acbbaa.png)\r\n![level3](https:\/\/user-images.githubusercontent.com\/1991664\/69040150-9894a900-09ed-11ea-94dc-49fb2d9b3b9b.png)\r\n\r\nIn summary we have 4 jpg images nested in different folder levels.\r\n\r\nTensorFlow 2 code to reproduce\r\n```\r\nfiles = tf.io.gfile.glob('gs:\/\/tensorflow-issue-reproduction\/**\/*.jpg')\r\nprint('file count: ', len(files))\r\n# found files 1\r\n```\r\n\r\ngsutil command which works properly\r\n```\r\ngsutil du gs:\/\/tensorflow-issue-reproduction\/**\/*.jpg | wc -l\r\n# found files 4\r\n```\r\n\r\n**Other info \/ logs**\r\n\/\r\n\r\nBest regards\r\nSascha\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops"],"created_at":"2019-11-18T09:34:42Z","comments":16,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/34371"},{"issue_number":487,"repository":"tensorflow\/tensorflow","title":"Cannot seek on write only tf.gfile.GFile","description":"**System information**\r\n\r\n-  Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0\r\n- Python version: 3.6\r\n\r\n**Describe the current behavior**\r\nCalling `seek()` on a `tf.gfile.GFile` opened in write only mode raises `tensorflow.python.framework.errors_impl.PermissionDeniedError`.\r\n\r\n**Describe the expected behavior**\r\nGFile should support the Python IO semantics that supports seeking on a write only file.\r\n\r\nMore generally it would be preferable if GFile followed the API of Python's [`io.IOBase`](https:\/\/docs.python.org\/3\/library\/io.html#io.IOBase).\r\n\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.io.gfile.GFile('test.txt', 'w') as f:\r\n    f.seek(0)\r\n```\r\n\r\n**Other info \/ logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 2, in <module>\r\n  File \"\/VENV\/lib\/python3.6\/site-packages\/tensorflow\/python\/util\/deprecation.py\", line 507, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"\/VENV\/lib\/python3.6\/site-packages\/tensorflow\/python\/lib\/io\/file_io.py\", line 146, in seek\r\n    self._preread_check()\r\n  File \"\/VENV\/lib\/python3.6\/site-packages\/tensorflow\/python\/lib\/io\/file_io.py\", line 82, in _preread_check\r\n    \"File isn't open for reading\")\r\ntensorflow.python.framework.errors_impl.PermissionDeniedError: File isn't open for reading\r\n```","labels":["stat:awaiting tensorflower","type:bug","comp:apis","TF 2.11"],"created_at":"2019-08-30T19:01:28Z","comments":12,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/32122"},{"issue_number":488,"repository":"tensorflow\/tensorflow","title":"Initial read on nonexistent tf.gfile.GFile in w+ mode crashes","description":"**System information**\r\n- Have I written custom code: Yes\r\n- OS Platform and Distribution: Linux Ubuntu 18.04\r\n- TensorFlow installed from: binary\r\n- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0\r\n- Python version: 3.7\r\n\r\n**Describe the current behavior**\r\nPython raises `tensorflow.python.framework.errors_impl.NotFoundError` when doing a first read (no writes before it) on a nonexistent `tf.gfile.GFile` in `w+` mode.\r\n\r\n**Describe the expected behavior**\r\nRead on an empty `w+` file should return an empty string.\r\nOne problem with the current behaviour is that numpy.savez() crashes when writing to a GFile.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport tensorflow as tf\r\n\r\nwith tf.io.gfile.GFile('test.txt', 'w+') as f:\r\n    f.read()\r\n```\r\n\r\n**Other info \/ logs**\r\n```\r\nTraceback (most recent call last):\r\n  File \"test_gfile.py\", line 5, in <module>\r\n    f.read()\r\n  File \"\/VENV\/lib\/python3.7\/site-packages\/tensorflow\/python\/lib\/io\/file_io.py\", line 122, in read\r\n    self._preread_check()\r\n  File \"\/VENV\/lib\/python3.7\/site-packages\/tensorflow\/python\/lib\/io\/file_io.py\", line 84, in _preread_check\r\n    compat.as_bytes(self.__name), 1024 * 512)\r\ntensorflow.python.framework.errors_impl.NotFoundError: test.txt; No such file or directory\r\n```","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.11"],"created_at":"2019-08-29T18:56:58Z","comments":18,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/32090"},{"issue_number":489,"repository":"tensorflow\/tensorflow","title":"Potential bugs found with static analysis","description":"<em>Please make sure that this is a bug. As per our [GitHub Policy](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/ISSUES.md), we only address code\/doc bugs, performance issues, feature requests and build\/installation issues on GitHub. tag:bug_template<\/em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N\/A\r\n- TensorFlow installed from (source or binary): Source\r\n- TensorFlow version (use command below): 1.14.0\r\n- Python version: 2.7.12\r\n- Bazel version (if compiling from source): 0.26.1\r\n- GCC\/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n- CUDA\/cuDNN version: N\/A\r\n- GPU model and memory: N\/A\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https:\/\/github.com\/tensorflow\/tensorflow\/tree\/master\/tools\/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with: 1. TF 1.0: `python -c \"import\r\ntensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"` 2. TF 2.0: `python -c\r\n\"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n**Describe the current behavior**\r\n\r\n**Issue 1:**\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/1ee51a3b868a3ccd5f80724f6b9389fd0a9aed07\/tensorflow\/compiler\/tf2xla\/functionalize_cond.cc#L445-L447\r\n`dst_copy != nullptr` is checked immediately after `if (dst_copy == nullptr) continue;`. Is one of comparisons supposed to be different, or can the `TF_RET_CHECK` be removed?\r\n\r\n**Issue 2:**\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/9d67841e6c1f852516abf6ff44490b5d5a8331af\/tensorflow\/contrib\/ignite\/kernels\/dataset\/ignite_dataset_iterator.cc#L126-L128\r\nThis code is unreachable, because both branches of `if` above return. Either it should be deleted or some part of `if` modified.\r\n\r\n**Issue 3:**\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/514004a2347058214d1e7b13b9769a2abdd06830\/tensorflow\/core\/profiler\/rpc\/client\/capture_profile.cc#L218-L220\r\nThe condition is always true. My guess is that `==` is intended instead of `!=`.\r\n\r\n**Issue 4:**\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/9590c4c32dd4346ea5c35673336f5912c6072bf2\/tensorflow\/core\/profiler\/internal\/tfprof_tensor.h#L55\r\nShould this be `void` since nothing gets returned in any branch?\r\n\r\n**Issue 5:**\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/9380a41290e8fb8b9ea85f614472deab56dbc481\/tensorflow\/stream_executor\/stream_executor_pimpl.cc#L112-L125\r\n`ScopedTracer` doesn't obey the rule of 3, but this `SCOPED_TRACE` macro expands to calling its copy constructor. This should be safe in practice because of copy elision, I believe, but undesirable to rely on. Unfortunately, changing to `auto tracer{MakeScopedTracer(this, &LOC##Begin, &LOC##Complete, ##__VA_ARGS__)};` will infer `initializer_list<...>` (is that right?).\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info \/ logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n","labels":["stat:awaiting tensorflower","type:bug","TF 1.13","comp:core"],"created_at":"2019-07-23T11:20:49Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/30954"},{"issue_number":490,"repository":"tensorflow\/tensorflow","title":"Many context switches \/ Many threads even if threading is limited","description":"**System information**\r\n- Have I written custom code: No (https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/r2\/tutorials\/quickstart\/beginner.ipynb)\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux OpenSuse Leap 15.0\r\n- TensorFlow installed from (source or binary): pip package tensorflow==2.0.0-beta1\r\n- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213\r\n- Python version: 3.7.3 \r\n- GPU model and memory: CPU only\r\n\r\n**Describe the current behavior**\r\nIf you limit the number of threads with tf.set_inter_op_parallelism_threads(NUM_THREADS) and tf.set_intra_op_parallelism_threads(NUM_THREADS) tensorflow creates a threadpool with more threads than NUM_THREADS and runs maximal NUM_THREADS causing high amount of context switches, which is delaying execution.\r\n\r\n**Describe the expected behavior**\r\nCreate maximal NUM_THREADS and limit context switches.\r\n\r\n**Code to reproduce the issue**\r\n```python\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n!pip install -q tensorflow==2.0.0-beta1\r\nimport tensorflow as tf\r\nNUM_THREADS=2\r\ntf.config.threading.set_inter_op_parallelism_threads(NUM_THREADS)\r\ntf.config.threading.set_intra_op_parallelism_threads(NUM_THREADS)\r\nmnist = tf.keras.datasets.mnist\r\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\nx_train, x_test = x_train \/ 255.0, x_test \/ 255.0\r\nmodel = tf.keras.models.Sequential([\r\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\r\n  tf.keras.layers.Dense(128, activation='relu'),\r\n  tf.keras.layers.Dropout(0.2),\r\n  tf.keras.layers.Dense(10, activation='softmax')\r\n])\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy',\r\n              metrics=['accuracy'])\r\nmodel.fit(x_train, y_train, epochs=100)\r\n```\r\n\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:runtime","TF 2.5","2.6.0"],"created_at":"2019-06-19T16:22:16Z","comments":31,"reactions":2,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/29968"},{"issue_number":491,"repository":"tensorflow\/tensorflow","title":"Unexpected UnicodeDecodeError: invalid continuation byte when reading lines from a file","description":"**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n\/a\r\n- TensorFlow installed from (source or binary): binary\r\n- TensorFlow version (use command below): 1.13.1\r\n- Python version: 3.5.2\r\n- Bazel version (if compiling from source): n\/a\r\n- GCC\/Compiler version (if compiling from source): n\/a\r\n- CUDA\/cuDNN version: n\/a\r\n- GPU model and memory: n\/a\r\n\r\n**Describe the current behavior**\r\nUnexpected and undocumented runtime exception\/error when handling malformed data.\r\n\r\n**Describe the expected behavior**\r\nExpected a \"TypeError\" or an empty list as a result.\r\n\r\n**Code to reproduce the issue**\r\n```\r\nimport csv\r\nimport sys\r\nimport tensorflow as tf\r\n\r\ninput_file_name = sys.argv[1]\r\n\r\nwith tf.gfile.Open(input_file_name, \"r\") as f:\r\n  reader = csv.reader(f, delimiter=\"\\t\", quotechar=None)\r\n  for line in reader:\r\n    print(line)\r\n```\r\nRun with the path to the attached file as a command line argument.\r\n\r\n**Other info \/ logs**\r\n\r\nTraceback (most recent call last):\r\n  File \"tensorflow_bug.py\", line 9, in <module>\r\n    for line in reader:\r\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/tensorflow\/python\/lib\/io\/file_io.py\", line 220, in \\_\\_next\\_\\_\r\n    return self.next()\r\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/tensorflow\/python\/lib\/io\/file_io.py\", line 214, in next\r\n    retval = self.readline()\r\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/tensorflow\/python\/lib\/io\/file_io.py\", line 184, in readline\r\n    return self._prepare_value(self._read_buf.ReadLineAsString())\r\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/tensorflow\/python\/lib\/io\/file_io.py\", line 100, in _prepare_value\r\n    return compat.as_str_any(val)\r\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/tensorflow\/python\/util\/compat.py\", line 107, in as_str_any\r\n    return as_str(value)\r\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/tensorflow\/python\/util\/compat.py\", line 80, in as_text\r\n    return bytes_or_text.decode(encoding)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte\r\n\r\n[corrupted_file1.zip](https:\/\/github.com\/tensorflow\/tensorflow\/files\/3047460\/corrupted_file1.zip)\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.11"],"created_at":"2019-04-05T10:29:22Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/27537"},{"issue_number":492,"repository":"tensorflow\/tensorflow","title":"Optimizing slice of variable not possible","description":"Applying the gradient of a variable slice currently results in a `NotImplemented` error of tf.train.Optimizer.\r\n\r\n**The following two examples are working:**\r\n```python\r\n### WORKING ###\r\nX = tf.Variable(2, dtype=tf.float32)\r\ny = tf.constant(10, dtype=\"float32\")\r\nloss = y - (X*X)\r\n\r\nvariables=[X]\r\ngradient = tf.gradients(loss, variables)\r\ngradient = [(g, v) for g, v in zip(gradient, variables)]\r\ntrain_op = tf.train.AdamOptimizer().apply_gradients(gradient)\r\n```\r\n\r\n```python\r\n### WORKING ###\r\nbig_X = tf.Variable([2,3,4], dtype=tf.float32)\r\nX = big_X[0]\r\ny = tf.constant(10, dtype=\"float32\")\r\nloss = y - (X*X)\r\n\r\ntrain_op = train_op = tf.train.AdamOptimizer().minimize(loss)\r\n```\r\n\r\n**The following example throws an error:**\r\n```python\r\n### NOT WORKING ###\r\nbig_X = tf.Variable([2,3,4], dtype=tf.float32)\r\nX = big_X[0]\r\ny = tf.constant(10, dtype=\"float32\")\r\nloss = y - (X*X)\r\n\r\nvariables=[X]\r\ngradient = tf.gradients(loss, variables)\r\ngradient = [(g, v) for g, v in zip(gradient, variables)]\r\ntrain_op = tf.train.AdamOptimizer().apply_gradients(gradient)\r\n```\r\nThe error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/IPython\/core\/interactiveshell.py\", line 2963, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-22-10282dee2005>\", line 10, in <module>\r\n    train_op = tf.train.AdamOptimizer().apply_gradients(gradient)\r\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/training\/optimizer.py\", line 605, in apply_gradients\r\n    update_ops.append(processor.update_op(self, grad))\r\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/training\/optimizer.py\", line 189, in update_op\r\n    raise NotImplementedError(\"Trying to update a Tensor \", self._v)\r\nNotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'strided_slice_9:0' shape=() dtype=float32>)\r\n```\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC\/Compiler version (if compiling from source)**: NA\r\n- **CUDA\/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.11"],"created_at":"2018-09-04T17:44:33Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/22059"},{"issue_number":493,"repository":"tensorflow\/tensorflow","title":"`tf.dynamic_stitch` gradient is incorrect","description":"### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nOriginal reproduction code (TensorFlow 1.0)\r\n``` python\r\nimport tensorflow as tf\r\n\r\nx = tf.zeros((1, 3))\r\ny = tf.dynamic_stitch([[0], [0]], [x, tf.ones((1, 3))])\r\n\r\nwith tf.Session() as sess:\r\n    print(\"y\")\r\n    print(sess.run(y))\r\n\r\n    analytic, numeric = tf.test.compute_gradient(x, (1, 3), y, (1, 3))\r\n    print(\"analytic\")\r\n    print(analytic)\r\n    print(\"numeric\")\r\n    print(numeric)\r\n```\r\nUpdated reproduction code (TensorFlow 2.16)\r\n``` python\r\nimport tensorflow as tf\r\n\r\nx = tf.zeros((1, 3))\r\n\r\nanalytic, numeric = tf.test.compute_gradient(\r\n    lambda x: tf.dynamic_stitch([[0], [0]], [x, tf.ones((1, 3))]), [x]\r\n)\r\nprint(\"analytic\")\r\nprint(analytic)\r\nprint(\"numeric\")\r\nprint(numeric)\r\n```\r\n\r\ngives output\r\n```\r\ny\r\n[[ 1.  1.  1.]]\r\nanalytic\r\n[[ 1.  0.  0.]\r\n [ 0.  1.  0.]\r\n [ 0.  0.  1.]]\r\nnumeric\r\n[[ 0.  0.  0.]\r\n [ 0.  0.  0.]\r\n [ 0.  0.  0.]]\r\n```\r\n\r\nThe numeric gradient correctly shows that `x` has no impact on `y` (since the value of `x` is completely overwritten by a constant in the `dynamic_stitch`).  The analytic gradient is incorrect; it seems like the gradient calculation in `dynamic_stitch` does not handle the case where there are duplicate indices being merged.\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.9"],"created_at":"2017-02-09T19:09:54Z","comments":15,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/7397"}]