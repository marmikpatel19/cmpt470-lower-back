issue_number,repository,title,description,labels,created_at,comments,reactions,url
1,nomic-ai/gpt4all,Bad UI design for 'clone and remove' button in the settings,"I would expect the ""remove"" button in the model settings page to work for custom OpenAI models, but it seems to only work for clones.","['bug', 'chat', 'chat-ui-ux']",2024-12-09T22:36:39Z,1,0,https://github.com/nomic-ai/gpt4all/issues/3243
2,nomic-ai/gpt4all,Models inside of subfolders can be seen in the models view but not removed,"GPT4All searches subfolders of the data directory for models. However, it also assumes that all models are in the root of the data in many parts of the code, by using the data dir itself instead of the `dirpath` field of ModelInfo. This causes actions such as the ""Remove"" button to not work.

Fix buried somewhere in #3034, but that is on hold due to other priorities.",['bug'],2024-10-09T22:41:47Z,0,0,https://github.com/nomic-ai/gpt4all/issues/3069
3,nomic-ai/gpt4all,"""New Chat"" does not change after switching locale","Strings such as ""Application default"" are updated without issue, but ""New Chat"" does not switch to the English translation after switching locale (in this case, from Romanian to English).

![gpt4all-newchat-tr](https://github.com/user-attachments/assets/95561f23-fc41-45db-b085-765a590ad3a3)","['bug', 'chat', 'chat-ui-ux', 'translations']",2024-08-19T16:17:31Z,2,0,https://github.com/nomic-ai/gpt4all/issues/2892
4,nomic-ai/gpt4all,Vulkan: Meta-Llama-3.1-8b-128k slow generation.,"<!-- Before creating a new issue, please make sure to take a few moments to check the issue tracker for existing issues about the bug. -->
>[!NOTE]
>Until this is fixed the workaround is use the CPU or CUDA instead.
### Bug Report
Vulkan: Meta-Llama-3.1-8b-128k slow generation.
<!-- A clear and concise description of what the bug is. -->
When using release 3.1.1 and Vulkan the Meta-Llama-3.1-8b-128k is extremely slow. (1.5t/s)
This is not a problem on CPU.
### Steps to Reproduce

<!-- List the steps that should be taken to experience this issue. Provide any relevant information about your configuration, and describe anything that was unexpected. -->

1. Using GPT4All 3.1.1 with Vulkan
2. Chat with Meta-Llama-3.1-8b-128k
3. Speed Is immediately slow (1.5t/s)

### Expected Behavior


<!-- In a few words, what did you expect to happen? -->
Using the model with llama.cpp directly reports over 60t/s
Using the model with GPT4All before 3.1.1 I could get about 30t/s.
### Your Environment

- GPT4All version: 3.1.1 (release or web_beta)
- Operating System: Windows
- Chat model used (if applicable): Vulkan & Meta-Llama-3.1-8b-128k

<!-- You can freely edit this text, please remove all the lines you believe are unnecessary. -->
","['bug', 'chat', 'vulkan']",2024-07-29T16:43:21Z,13,2,https://github.com/nomic-ai/gpt4all/issues/2768
5,nomic-ai/gpt4all,[Crash] Assertion `isModelLoaded()` fails in ChatLLM::generateName when switching chats while model is generating,"Steps to reproduce:
- Start generating
- Switch off of the chat
- Switch back onto the chat (or onto a third chat)
- Assertion failure in debug build, here:

https://github.com/nomic-ai/gpt4all/blob/fd005be844d91f821eb4ecd69c40f08afba7ae28/gpt4all-chat/chatllm.cpp#L872-L876","['bug', 'chat', 'crash']",2024-07-09T21:15:53Z,0,0,https://github.com/nomic-ai/gpt4all/issues/2626
6,nomic-ai/gpt4all,Collections with overlapping folder paths cause false negatives in LocalDocs lookup,"We can miss entire documents when indexing a LocalDocs collection, because the documents are already in the database, and document paths are expected to be unique (we assume that we have already processed the document).

This can cause problems in practice, when a document is shared between two `folder_id`s, e.g. because the user has added the parent directory of one collection as another collection in the UI. In that case, the parent will be missing all of the child's documents if the child was indexed first.

Should we even allow the user to add overlapping folders?","['bug', 'chat', 'local-docs', 'chat-ui-ux']",2024-07-08T22:50:54Z,0,0,https://github.com/nomic-ai/gpt4all/issues/2608
7,nomic-ai/gpt4all,Chats loaded from text (the default) do not preserve LocalDocs context and prompt template,"- LocalDocs context is not restored when the chat is loaded&mdash;ChatLLM::promptInternal directly inserts database results into the context, but ChatLLM::processRestoreStateFromText bypasses this (and we don't want fresh results anyway, we want the old ones).
- The system prompt and prompt template are applied to past messages when the chat is *loaded*, which is something you can otherwise never observe (AFAIK) without first closing GPT4All.

Neither of these things should happen with the ""Save Chat Context"" option turned on, which used to be the default. So this can be seen as a regression.","['bug', 'chat']",2024-07-08T21:44:49Z,0,0,https://github.com/nomic-ai/gpt4all/issues/2603
8,nomic-ai/gpt4all,GPT4All can be opened more than once - but this isn't supported,"GPT4All was never designed to be opened more than once&mdash;this creates multiple instances of the application, which isn't supported, and could result in corruption of the LocalDocs database, or the settings file. The local server will only work on one instance because both will try to use the same port number.

Related: #2309","['bug', 'chat']",2024-07-08T15:30:34Z,0,0,https://github.com/nomic-ai/gpt4all/issues/2590
9,nomic-ai/gpt4all,GPT4All can restore its window geometry to somewhere off-screen,"### Bug Report

gpt4all appears to be running in the task manager, but I can't see any Windows. My computer is an amdR7800hCPU laptop.

### Steps to Reproduce

1.Double click run

### Expected Behavior
The program should appear in a window

![PixPin_2024-07-05_16-34-27](https://github.com/nomic-ai/gpt4all/assets/68932312/3c4e0831-37ef-4716-a9f9-ca9e9cbbc564)

### Your Environment

- GPT4All version:3.0.0
- Operating System:Windows 10 Home Chinese Version


","['bug', 'chat']",2024-07-05T08:40:59Z,16,1,https://github.com/nomic-ai/gpt4all/issues/2564
10,nomic-ai/gpt4all,How to solve localdocs database error,"![image](https://github.com/nomic-ai/gpt4all/assets/117589240/c6e67bf9-0040-42c2-8cbc-2777fbd77db7)
","['bug', 'chat']",2024-07-02T16:35:07Z,23,0,https://github.com/nomic-ai/gpt4all/issues/2516
11,nomic-ai/gpt4all,"UI: If you have too many installed models, the list gets cut off and can't be scrolled","

![Image](https://github.com/nomic-ai/gpt4all/assets/14168726/a74df79a-2164-422c-98c5-e8233f07ef8b)

","['bug', 'chat', 'chat-ui-ux']",2024-05-10T21:23:40Z,0,0,https://github.com/nomic-ai/gpt4all/issues/2338
12,nomic-ai/gpt4all,Generation speed indication is scientific notation for speeds >= 100 tokens/s,"### Bug Report
The speed indication for any model is in scientific notation for speeds >= 100 t/sec

<img width=""1559"" alt=""Screenshot 2024-04-30 at 10 01 48"" src=""https://github.com/nomic-ai/gpt4all/assets/42358482/afdd214c-7d48-4788-9893-09ed31cebcb9"">


### Steps to Reproduce
1. Works with any model that is fast enough
2. Speed shows up in bottom right part of the screen.

### Expected Behavior
* Speed should be shown in `%.2f` formatting

### Your Environment
- GPT4All version: 2.7.4
- Operating System: OSX 13.6.3
- Chat model used (if applicable):

","['bug', 'chat', 'chat-ui-ux']",2024-04-30T08:11:29Z,0,0,https://github.com/nomic-ai/gpt4all/issues/2287
13,nomic-ai/gpt4all,local server crashes when a model is already loaded on the GPU,"Your Environment
- GPT4All version: 2.7.3
- Operating System: Windows 11
- Chat model used (if applicable): Nous Hermes 2 Mistral DPO
I have open the port and allow the chat.exe in the windows firewall. I got the 'Error: connect ECONNREFUSED 127.0.0.1:4891' in the same machine. 
Any suggestion? thanks.
![Capturesss](https://github.com/nomic-ai/gpt4all/assets/68432267/6a880114-3391-40ef-afd9-936b203f3bb2)
![Capturesss2](https://github.com/nomic-ai/gpt4all/assets/68432267/afde6884-d500-4bfa-9a1b-a30af7f5b37c)

","['bug', 'chat', 'vulkan', 'local-server']",2024-04-08T20:41:52Z,21,0,https://github.com/nomic-ai/gpt4all/issues/2205
14,nomic-ai/gpt4all,CPU t/s regression from GPT4All 2.6.1 -> 2.6.2,"On my old 4-core computer, I got roughly 2.1 t/s with openorca-chat model. When I upgraded to a 6-core PC, the speed doubled. Other models got the same speed increase. Then two things happened at the same time, I had to use a crappy video card temporarily until I got a new one to replace my broken one, and I upgraded to the latest gpt4all.

My speed dropped to 1.2 t/s for openorca and below 1 for other models. I assumed it was my crappy GPU but I just got my new one that's benchmarked as 3 times faster than my old one and I'm still getting the same slow speed. I downgraded to gpt4all 2.6.1 and my speed is back to 4 t/s for openorca, so it seems the problem was the latest version. Why is this?","['bug', 'backend', 'chat']",2024-04-08T16:13:09Z,11,0,https://github.com/nomic-ai/gpt4all/issues/2204
15,nomic-ai/gpt4all,High CPU usage in chat with Hermes 2 Pro Mistral 7B after generation has finished,"### Bug Report

With Hermes 2 Pro Mistral 7B, in certain situations chat.exe causes high CPU usage even after generation has finished.

### Steps to Reproduce

1. Install [Hermes 2 Pro Mistral 7B](https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B-GGUF/blob/main/Hermes-2-Pro-Mistral-7B.Q5_K_M.gguf) (SHA256 checks out!)

2. Set the context length to 8192. This is OK because the model supports 32k. The device is CPU, with 8 threads. (These settings may or may not be important here.)

3. Open the chat, load the model and enter e.g. the following prompt:
```
Write a fictional story titled ""A Triangle"", with the following synopsis:

""An observer becomes entranced by a seemingly ordinary couple on the street, follows them home, and then
watches them from outside in the rising floodwaters, drawing an eerie connection between the woman and
a discarded, burned chair they’d noticed earlier.""

Make sure the story is at least three pages long.
```
The actual prompt is irrelevant, as long as the response is long enough (c. 700 words is sufficient - not that long actually).

4. The model will respond with a story (or, at any rate, something that resembles it).

5. **Once the generation has finished, the CPU usage stays high as if the generation is still running, instead of dropping to 0. It's due to chat.exe, according to Task Manager.**

6. **Upon closing the chat window, chat.exe stays active in the background and high CPU usage continues, until the process is killed in Task Manager.**

I haven't seen this problem with other models, so it's kind of a narrow issue, but might be worth a look.

### Your Environment

- GPT4All version: 2.7.3
- Operating System: Windows 10
- Chat model used (if applicable): Hermes 2 Pro Mistral 7B

","['bug', 'chat']",2024-03-26T08:53:41Z,7,0,https://github.com/nomic-ai/gpt4all/issues/2167
16,nomic-ai/gpt4all,"Weird download speed, varying between very large negative and very large positive numbers","Hello.

I don't know if this is a Bug per se... looks weird, anyway.

On the Download dialog, when downloading an LLM (the .gguf file in the video, maybe others), a large negative number is being shown at times, as the download speed; it appears once in a while (perhaps random) for ~1 second, then (not always immediately afterwards) another large number appears, but a positive one, say, 25 MB/s. 

### Steps to Reproduce

1. Select an LLM, then Click on the Download button
2. Watch it burn

### Expected Behavior

A more or less reasonable but consistent download speed, over time (in the 10^1 MB/s).

### Your Environment

- GPT4All v2.7.3
- Operating System: Windows 10
- Chat model used (if applicable): (incomplete)


Thank you for considering this.


https://github.com/nomic-ai/gpt4all/assets/158754254/6390e67e-812a-40ea-8874-9f994bd6191e

","['bug', 'chat']",2024-03-21T20:00:56Z,0,0,https://github.com/nomic-ai/gpt4all/issues/2156
17,nomic-ai/gpt4all,"Vulkan: Gibberish output on Quadro (Kepler, Maxwell, Turing)","<!-- Before creating a new issue, please make sure to take a few moments to check the issue tracker for existing issues about the bug. -->

### Bug Report

<!-- A clear and concise description of what the bug is. -->
Does anyone know why GPT4All will respond in gibberish. 
The behavior is  inconsistent  

### Steps to Reproduce
<!-- List the steps that should be taken to experience this issue. Provide any relevant information about your configuration, and describe anything that was unexpected. -->

1.	Start GPT4All selected model “Nous Hermes Mistral DPO”
2. 	Enter the prompt “Sample Java program”.
3.     The response is legible.
4. 	Enter the prompt “Sample R program”.
5. 	The response is gibberish.


### Expected Behavior

<!-- In a few words, what did you expect to happen? -->
The response should not be gibberish it is currently not legible
### Your Environment

- GPT4All version: GPT4All V2.7.3
- Operating System: Win 10 Enterprise 
- Chat model used (if applicable): Nous Hermes Mistral DPO but it does not matter
- Intel(R) Xeon(R) W-10885M CPU @ 2.40GHz, 2400 Mhz, 8 Core(s), 16 Logical Processor(s)
- 128 GB RAM 
- GPU Quadro RTX 3000 6 GB RAM

- See screenshot 
![2024-03-20_11-15-34](https://github.com/nomic-ai/gpt4all/assets/23298288/d117c99a-ac78-4961-943c-f656c536e7dd)


<!-- You can freely edit this text, please remove all the lines you believe are unnecessary. -->

","['bug', 'backend', 'vulkan']",2024-03-20T15:50:24Z,7,0,https://github.com/nomic-ai/gpt4all/issues/2149
18,nomic-ai/gpt4all,"GPT4All 2.7.3: Context ""link"": inactive when clicked to show a snippet; selectable by double-click","Hello.

Sometimes, the ""links"" marked ""Context"" after a response:
- are selectable by double-clicking on them, instead of showing a document snippet
- when clicked, do not show a snippet from a LocalDoc which was presumably used/quoted in the response

Occurrence: unpredictable/unexpected/no repeatability discerned
Location: the entire set of quotes after a response from the currently-selected LLM; not in the set preceding it, not in the set after it.


### Steps to Reproduce

1. click on such a ""link"": no snippet is shown; this is unexpected
2. drag over a ""link"" with the mouse, starting from a character/letter/space before it, from left to right; the ""link"" is selected; this is expected 
3. double-click a ""link"": the ""link"" is selected; this is unexpected


### Expected Behavior

All _visible_ ""links"" marked ""Context"" should show a snippet when clicked.


### Your Environment

- GPT4All version: 2.7.3
- Operating System: Windows 10, x64
- Chat model used (if applicable): Nous Hermes 2 Mistral DPO, Context Length: 30720

As an aside - the mouse pointer over these links should change to hand or some other form indicating their distinct role, like their color does.

Thank you for considering this.

 
![img_gpt4all_q1](https://github.com/nomic-ai/gpt4all/assets/158754254/d27ad0dc-8994-47cc-b829-5d8d78f34200)
","['bug', 'chat']",2024-03-13T22:52:05Z,0,0,https://github.com/nomic-ai/gpt4all/issues/2120
19,nomic-ai/gpt4all,"GPT4All 2.7.1 - searching localdocs even if no category is checked/selected; repeating the reply and the Context fragments, regardless of LLM","<!-- Before creating a new issue, please make sure to take a few moments to check the issue tracker for existing issues about the bug. -->

### Bug Report

<!-- A clear and concise description of what the bug is. -->

### Steps to Reproduce

**1. Loaded the model: mistral-7b-instruct-v.0.1Q4_0.gguf**
1.1. checked a (1, one) LocalDocs collection
1.2. asked the model something
1.3. the model searched the checked LocalDocs collection: ""searching localdocs: [category]..."". 
1.4. the model replied as per the checked LocalDocs collection; following the reply, there were N Certain fragments cited, as Context.
1.5. This was expected.
1.6. unchecked the same LocalDocs collection
1.7. reloaded the model by using the icon next to its name in the combobox
1.8. asked the model the same question as before
1.9. it started searching again the localdocs, but _NOT_ specifying which collection (none was checked, anyway):  ""searching localdocs: ..."". in the end, it 1) _repeated the reply_ from the time when the collection was checked and 2) quoted _the same_ N Certain fragments, as Context, like the previous model
1.10. unloaded the model

**2. Loaded the model: gpt4all-falcon-newbpe-q4_0.gguf**
2.1. checked again - the same LocalDocs collection as in the case of previous model
2.2. asked the current model the same question as the previous model
2.3. the current model searched in the checked LocalDocs collection: ""searching localdocs: [category]..."". 
2.4. the model replied as per the checked LocalDocs collection; following the reply, there were N Certain fragments cited, as Context.
2.5. This was expected; however, the cited Contexts were _exactly the same_ as in the case of the previous model.
2.6. unchecked the same LocalDocs collection
2.7. reloaded the current model by using the icon next to its name in the combobox
2.8. asked the model the same question as before
2.9. it started searching again the localdocs, but _NOT_ specifying which collection (none was checked, anyway):  ""searching localdocs: ..."". in the end, it 1) _repeated the reply_ from the time when the collection was checked and 2) quoted _the same_ N Certain fragments, as Context, like the previous model


### Expected Behavior

Expectations:
- when a (even 1, since _it seems_ like searching the LocalDocs depends on the number>1 of collections, _although not every time_) LocalDocs collection was checked, the reply should have been different to that when no collection was checked.
- moreover, the quoted Contexts at the end of replies should have been 1) present when the reply was built upon checked LocalDocs collections and 2) absent when no such collection was checked.
- moreover, the content of the replies should have been different even in 1 word, regardless if any LocalDocs category was checked.

### Your Environment

- GPT4All 2.7.1
- Windows 10, latest update as of 2024.02.26
- Mistral Instruct 7B, Falcon 7B
- Mistral Instruct: Context Length: 16384, Temperature: 0.1, Top P; 0.6, Prompt Batch Size: 128
- Falcon: the same values as for Mistral, above

It looks like both the reply, the cited Contexts and the LocalDocs categories are being remembered and transferred to the next reply (replies) to the same question, regardless of the currently loaded model.","['bug', 'chat', 'local-docs']",2024-02-26T23:23:36Z,0,0,https://github.com/nomic-ai/gpt4all/issues/2033
20,nomic-ai/gpt4all,Ceased GPT4ALL during indexing by Local Documents plugin,"<!-- Before creating a new issue, please make sure to take a few moments to check the issue tracker for existing issues about the bug. -->

### Bug Report

<!-- A clear and concise description of what the bug is. -->

### Steps to Reproduce

<!-- List the steps that should be taken to experience this issue. Provide any relevant information about your configuration, and describe anything that was unexpected. -->

1. Indexing Local Documents more 20 pdf files.
2. Per a book page are more 500 pages
3. Sometimes didn't indexing or ceased GPT4ALL

### Expected Behavior

<!-- In a few words, what did you expect to happen? -->

### Your Environment

- GPT4All version: 2.7.1
- Operating System: Windows 11
- Chat model used (if applicable): EM German Mistral

<!-- You can freely edit this text, please remove all the lines you believe are unnecessary. -->
I'm testing LLM learning for Local Documents.
I already explained bug above.
Have a limitation of Local Documents plugin?
ex, Pages amount per a file or File size or number?","['bug', 'chat', 'local-docs']",2024-02-26T05:36:51Z,0,0,https://github.com/nomic-ai/gpt4all/issues/2029
21,nomic-ai/gpt4all,Chat UI crashes for long inputs,"I am running the latest version of GPT4all, version 2.7.0, on Windows 11 Pro. Whenever I provide a long input to a model, the program crashes immediately after it finishes responding, and I am unable to use it again. Can you please identify the reason for this error and provide a solution to fix it? Your assistance would be greatly appreciated.","['bug', 'chat']",2024-02-24T14:42:53Z,1,0,https://github.com/nomic-ai/gpt4all/issues/2020
22,nomic-ai/gpt4all,"Online v2.7.1 release doesn't open, offline v2.7.0 release does","### Bug Report

I upgraded the app from 2.70 to 2.71, and first Windows Defender gave me a virus notification and removed some files and then the app stopped working. 

### Steps to Reproduce

Upgrade from 2.70 to 2.71 using in app upgrade option on windows 11


### Expected Behavior

Nothing works. The chat UI doesn't start and the issue persists even after reinstalling until I downgraded to 2.7.0.

### Your Environment

- GPT4All version: 2.7.1
- Operating System: Windows 11
- Chat model used (if applicable): mistral 7b openorca


","['bug', 'chat']",2024-02-22T11:24:58Z,7,0,https://github.com/nomic-ai/gpt4all/issues/1998
23,nomic-ai/gpt4all,"Windows: System menu for window has ""Minimize"" and ""Maximize"" options disabled when opened with keyboard (Alt+Space shortcut)","<!-- Before creating a new issue, please make sure to take a few moments to check the issue tracker for existing issues about the bug. -->

### Bug Report

<!-- A clear and concise description of what the bug is. -->

### Steps to Reproduce

<!-- List the steps that should be taken to experience this issue. Provide any relevant information about your configuration, and describe anything that was unexpected. -->

1. On Windows 11, open the application
2. Press Alt+Space to show the window's system menu
3. Press ""n"" for minimize or ""m"" for maximize
4. Observe that the minimize and maximize options are not enabled
5. Press Esc to close the system menu
6. Open the window's system menu by clicking the window's top left corner. The options are now available.

### Expected Behavior

I expect the ""Minimize"" and ""Maximize"" options to be available (not disabled) and to activate using the ""N"" and ""X"" shortcuts, respectively.

### Actual Behavior:

The options are disabled, and do not activate using the shortcuts.

### Your Environment

- GPT4All version: 2.7.0
- Operating System: Windows 11 
- Chat model used (if applicable):

<!-- You can freely edit this text, please remove all the lines you believe are unnecessary. -->
","['bug', 'chat']",2024-02-18T03:31:56Z,1,0,https://github.com/nomic-ai/gpt4all/issues/1977
24,nomic-ai/gpt4all,Cannot use local server with GPU enabled because it tries to load two models on GPU at once,"### System Info

GPT4All: 2.6.2-2
Python: 3.11.5
OS: Archlinux
Kernel: 6.7.3-arch1-2

### Information

- [X] The official example notebooks/scripts
- [ ] My own modified scripts

### Reproduction

1. Start the GPT4All application and enable the local server
2. Download the mistral-7b-openorca.Q4_0.gguf model
3. Use the code below to test.

```
import openai

openai.api_base = ""http://localhost:4891/v1""

openai.api_key = ""not needed for a local LLM""

# Set up the prompt and other parameters for the API request
prompt = ""Who is Michael Jordan?""

model = ""mistral-7b-openorca.Q4_0.gguf""

# Make the API request
response = openai.Completion.create(
    model=model,
    prompt=prompt,
    max_tokens=50,
    temperature=0.28,
    top_p=0.95,
    n=1,
    echo=True,
    stream=False
)

# Print the generated completion
print(response)
```



### Expected behavior

I would expect the model to return an answer instead of crashing.

Instead I get this log when starting the gpt4all-chat from the shell:

```
[Debug] (Tue Feb 6 16:28:44 2024): deserializing chat ""/home/arael/.local/share/nomic.ai/GPT4All//gpt4all-a93ce28a-a563-4bc0-959f-4e9abcec67f6.chat""
[Debug] (Tue Feb 6 16:28:44 2024): deserializing chat ""/home/arael/.local/share/nomic.ai/GPT4All//gpt4all-3974f69f-fd9f-4e00-80a8-b70c02dd298c.chat""
[Debug] (Tue Feb 6 16:28:44 2024): deserializing chat ""/home/arael/.local/share/nomic.ai/GPT4All//gpt4all-f11fba17-3fe5-476f-8436-48aeae4f10df.chat""
[Debug] (Tue Feb 6 16:28:44 2024): deserializing chats took: 1 ms
llama.cpp: using Vulkan on AMD Radeon RX 6800S (RADV NAVI23)
^NGGML_ASSERT: /home/arael/.cache/yay/gpt4all-chat/src/gpt4all-2.6.2/gpt4all-backend/llama.cpp-mainline/ggml-kompute.cpp:2009: s_kompute_context == nullptr
[1]    28706 IOT instruction (core dumped)  gpt4all-chat
```","['bug', 'chat']",2024-02-06T07:47:44Z,2,0,https://github.com/nomic-ai/gpt4all/issues/1932
25,nomic-ai/gpt4all,No models available - sbert model link broken - GPT4ALL.IO/MODELS/MODELS2.JSON,"### System Info

When attempting to add localdocs plugin, nothing shows up when it attempts to download from:  GPT4ALL.IO/MODELS/MODELS2.JSON

it looks like broken link issue ?

### Information

- [ ] The official example notebooks/scripts
- [ ] My own modified scripts

### Reproduction

When attempting to add localdocs plugin, nothing shows up when it attempts to download from:  GPT4ALL.IO/MODELS/MODELS2.JSON

it looks like broken link issue ?

### Expected behavior

When attempting to add localdocs plugin, nothing shows up when it attempts to download from:  GPT4ALL.IO/MODELS/MODELS2.JSON

it looks like broken link issue ?","['bug', 'chat']",2024-01-22T01:44:42Z,2,0,https://github.com/nomic-ai/gpt4all/issues/1861
26,nomic-ai/gpt4all,macOS: LocalDocs can not add local directory,"### System Info

GPT4All 2.5.4 macOS 14.2.1 (23C71) M1 Macbook 16GB mem 2TB disk

### Information

- [X] The official example notebooks/scripts
- [ ] My own modified scripts

### Reproduction

I installed Mistral OpenOrca and LocalDocs.
Configuring LocalDocs the system asks
- if it can access my Download folder (I do not know why), I answered yes
- I can select the folder with my files, which returns file:///User/MyUsername/..... and the Connection Name turns red
- I try to remove the prefix file:// - so path looks like the path in the  other config menus. connection name stays red
The local path is not parsed.



### Expected behavior

accept the parsed directory name","['bug', 'chat', 'local-docs']",2024-01-09T10:31:00Z,1,0,https://github.com/nomic-ai/gpt4all/issues/1820
27,nomic-ai/gpt4all,Model loaded into GPU ram but 0% GPU usage,"### System Info

GPT4all version 1.0.2
Ubuntu Linux 24 LTS with kernel 5.15.0-91-generic #101-Ubuntu SMP
Nvidia Tesla P100-PCIE-16GB
Nvidia driver v545.29.06
Cuda 12.3

### Information

- [ ] The official example notebooks/scripts
- [X] My own modified scripts

### Reproduction

`
python3 app.py repl --model /model/location/mistral-7b-instruct-v0.2.Q5_K_S.gguf -d nvidia
`

In another console: `nvtop`

### Expected behavior

nvtop should show python3 using several GB of ram upon loading and GPU usage should increase from 0% to/near 100% during inference.

Instead, nvtop shows python3 using several GB of ram as expected, but GPU usage remains at 0% throughout inference.

Separately, the GPU usage does increase to 100% when I perform inference using a non-quantized model via transformers.

Edit: This seems to be model dependent. gpt4all uses the GPU when performing inference using nous-hermes-llama2-13b.Q4_0.gguf and mistral-7b-instruct-v0.1.Q4_0.gguf but not when using mistral-7b-instruct-v0.2.Q5_K_S.gguf or Wizard-Vicuna-13B-Uncensored.Q6_K.gguf. 

I started to think that only 4-bit quantized models are running on the GPU, but then I tried Wizard-Vicuna-13B-Uncensored.Q4_K_M.gguf and found that it used GPU ram but did not run on the GPU.",['bug'],2024-01-03T13:30:13Z,4,0,https://github.com/nomic-ai/gpt4all/issues/1803
28,nomic-ai/gpt4all,temp=0 should use greedy sampling instead of crashing or UB,"### System Info

OS: Windows 11 64 bit
Python 3.10.4
Model: nous-hermes-llama2-13b.Q4_0.gguf

### Information

- [ ] The official example notebooks/scripts
- [X] My own modified scripts

### Reproduction

Step1: The following code produces the error:
```python
from gpt4all import GPT4All
model = GPT4All(""nous-hermes-llama2-13b.Q4_0.gguf"")
with model.chat_session(""You are a cyber secuirty expert who can find vulnerabilities in source code""):
    response = model.generate('who are you?', temp=0)
    print(response)
```
![image](https://github.com/nomic-ai/gpt4all/assets/20457968/931225b3-92af-4d28-a382-a86aa0227c23)


### Expected behavior

1. Without giving system_prompt, the chat session works fine.
2. With system_prompt, the model should respond accordingly without error as mentioned in the doc https://docs.gpt4all.io/gpt4all_python.html#managing-templates","['bug', 'backend', 'chat', 'crash']",2024-01-02T18:35:22Z,2,0,https://github.com/nomic-ai/gpt4all/issues/1799
29,nomic-ai/gpt4all,"""retrieving localdocs: ...""  spinning circle","### System Info

v2.5.4
win 10
Model: Openchat 3.5 -1210

Tried to stop it and didnt work. Pressing ""stop generating"" didnt work.

PS:
Notice most of the time when the generation is finished it sort of hangs as it shows ""stop generating"" button so I assume its still generating....something,
so this will be another recent model after 
- stablelm-zephyr
-  magiccoder
 that doesnt work in gpt4all. 
Seems only the old models work in gpt4all.

### Information

- [ ] The official example notebooks/scripts
- [ ] My own modified scripts

### Reproduction

1. ask to write a code in python to read btc price
2. once finished ask it a question about diference between api key and secret key 
OR ""ok thanks""
4. get the forever lasting spinning circle 

### Expected behavior

to answer question","['bug', 'chat']",2023-12-15T05:11:12Z,2,0,https://github.com/nomic-ai/gpt4all/issues/1754
30,nomic-ai/gpt4all,GPT4all not using my GPU because Models not unloading from VRAM when switching,"### Issue you'd like to raise.

RTX 3060 12 GB is available as a selection, but queries are run through the cpu and are very slow.

Cuda compilation tools, release 12.2, V12.2.128
Build cuda_12.2.r12.2/compiler.33053471_0

### Suggestion:

_No response_","['bug', 'chat']",2023-11-28T23:04:14Z,3,0,https://github.com/nomic-ai/gpt4all/issues/1688
31,nomic-ai/gpt4all,Process finished with exit code 139 (interrupted by signal 11: SIGSEGV) ,"1. I am able to run this model as well as lighter models, but in about 2-4 promts given to the model (in the process of answering) it fails with ""Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)"".  Tried python from 3.9 to 3.11. Also, tried with Jupyter lab (kernel 3.10), PyCharm and terminal. It is all the same. Any ideas where the problem may come from?

2. How much GPU memory do I need to run the model, is 24G of GPU memory enough?","['bug', 'python-bindings']",2023-11-14T10:07:38Z,2,0,https://github.com/nomic-ai/gpt4all/issues/1646
32,nomic-ai/gpt4all,﻿ERROR: The prompt size exceeds the context window size and cannot be processed. (Using GUI),"### System Info

Windows 10, GPT4ALL GUI version 2.5.2.

### Information

- [ ] The official example notebooks/scripts
- [ ] My own modified scripts

### Reproduction

1. Create a localdocs directory and fill it with some .html files
2. Reference that database of context docs in the GUI
3. Ask a fairly small question to the GPT4ALL Falcon model (such as ""Tell me the advantages of Python over other programming languages."")
4. Receive the following error:

```
﻿ERROR: The prompt size exceeds the context window size and cannot be processed.
```

### Expected behavior

A clear response. This issue has been raised and gone stale [here](https://github.com/nomic-ai/gpt4all/issues/664) and was suggested to create a new issue.

I've gone into the settings and increased the input and output token length to as high as 4096, no change. How to fix?","['bug', 'chat']",2023-11-10T16:02:43Z,0,1,https://github.com/nomic-ai/gpt4all/issues/1638
33,nomic-ai/gpt4all,SIGSEGV if Embed4All is called too many times,"### System Info

- Macbook Pro M1 16GB RAM
- Python 3.11.6
- gpt4all==2.0.1

### Information

- [ ] The official example notebooks/scripts
- [X] My own modified scripts

### Reproduction

## Description
I've chunked my document up where each chunk is no more than 500 words with a total of ~300 chunks. I am then looping over each chunk and using the default Embed4All() embedder. However, when it gets to about 30 chunks in, I get a python segmentation fault (which I understand is likely due to running out of memory). I can see my RAM usage increase throughout this process. It seems to me that the C/C++ running under the hood is holding onto the memory even after the embedding is complete? I have ran the exact same using OpenAI without error so I don't think the resultant embeddings are taking up the memory. I also know it's not a specific chunk as I've run the embeddings from different starting points.

I've tried using `device=""gpu""` with no difference and I've also tried using a different embedder model.

## Snippet
```python
def get_embedding(text: str) -> list[float]:
    embedder = Embed4All(device=""cpu"", verbose=True)
    output = embedder.embed(text)
    return output
```

## Error
`18392 segmentation fault`

### Apple error report
```
Crashed Thread:        0  Dispatch queue: com.apple.main-thread

Exception Type:        EXC_BAD_ACCESS (SIGSEGV)
Exception Codes:       KERN_INVALID_ADDRESS at 0x0000000000000011
Exception Codes:       0x0000000000000001, 0x0000000000000011

Termination Reason:    Namespace SIGNAL, Code 11 Segmentation fault: 11
Terminating Process:   exc handler [59316]

VM Region Info: 0x11 is not in any region.  Bytes before following region: 4368498671
      REGION TYPE                    START - END         [ VSIZE] PRT/MAX SHRMOD  REGION DETAIL
      UNUSED SPACE AT START
--->  
      __TEXT                      104620000-104624000    [   16K] r-x/r-x SM=COW  .../MacOS/Python

Thread 0 Crashed::  Dispatch queue: com.apple.main-thread
0   libbert-default.dylib         	       0x127f0cf20 ggml_new_tensor_impl + 296
1   libbert-default.dylib         	       0x127f0d244 ggml_new_tensor_1d + 36
2   libbert-default.dylib         	       0x127ef1118 bert_eval(bert_ctx*, int, int const*, int, float*) + 288
3   libbert-default.dylib         	       0x127ef27d8 bert_load_from_file(char const*) + 4008
4   libbert-default.dylib         	       0x127ef2db8 Bert::loadModel(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) + 36
5   libllmodel.dylib              	       0x127707904 llmodel_loadModel + 148
6   libffi.dylib                  	       0x19bd8f050 ffi_call_SYSV + 80
7   libffi.dylib                  	       0x19bd97adc ffi_call_int + 1208
8   _ctypes.cpython-311-darwin.so 	       0x1064c8874 _ctypes_callproc + 788
9   _ctypes.cpython-311-darwin.so 	       0x1064c34c4 PyCFuncPtr_call + 220
10  Python                        	       0x104f93e8c _PyObject_MakeTpCall + 128
11  Python                        	       0x105071e6c _PyEval_EvalFrameDefault + 42120
12  Python                        	       0x105076554 _PyEval_Vector + 116
13  Python                        	       0x104f941a8 _PyObject_FastCallDictTstate + 208
14  Python                        	       0x104ffe0e8 slot_tp_init + 188
15  Python                        	       0x104ff6598 type_call + 136
16  Python                        	       0x104f94ca4 _PyObject_Call + 124
17  Python                        	       0x105073cac _PyEval_EvalFrameDefault + 49864
18  Python                        	       0x105076554 _PyEval_Vector + 116
19  Python                        	       0x104f941a8 _PyObject_FastCallDictTstate + 208
20  Python                        	       0x104ffe0e8 slot_tp_init + 188
21  Python                        	       0x104ff6598 type_call + 136
22  Python                        	       0x104f93e8c _PyObject_MakeTpCall + 128
23  Python                        	       0x105071e6c _PyEval_EvalFrameDefault + 42120
24  Python                        	       0x105066da0 PyEval_EvalCode + 168
25  Python                        	       0x1050bd7a0 run_eval_code_obj + 84
26  Python                        	       0x1050bd704 run_mod + 112
27  Python                        	       0x1050bd544 pyrun_file + 148
28  Python                        	       0x1050bcf94 _PyRun_SimpleFileObject + 268
29  Python                        	       0x1050bc92c _PyRun_AnyFileObject + 216
30  Python                        	       0x1050d950c pymain_run_file_obj + 220
31  Python                        	       0x1050d8e4c pymain_run_file + 72
32  Python                        	       0x1050d872c Py_RunMain + 704
33  Python                        	       0x1050d9868 Py_BytesMain + 40
34  dyld                          	       0x18af31058 start + 2224
```

### Expected behavior

The embedder can run many times without crashing in a single program.","['bug', 'python-bindings']",2023-10-27T22:01:29Z,2,0,https://github.com/nomic-ai/gpt4all/issues/1583
34,nomic-ai/gpt4all,Vulkan backend can cause desktop UI to freeze/crash,"When attempting to run GPT4All with the vulkan backend on a system where the GPU you're using is also being used by the desktop - this is confirmed on Windows with an integrated GPU - this can result in the desktop GUI freezing and the gpt4all instance not running.

I believe the problem is contention in GPU memory between what the desktop is trying to use and what we are trying to allocate with vulkan.

I can reproduce this on my old Windows 10 laptop with the integrated AMD Vega GPU consistently

Not sure how to fix at this time :(

### Reproduction

Reproduced with old Windows 10 laptop trying to use the integrated AMD Vega gpu. More system details to come.

### Expected behavior

We should detect this case ahead of time and fallback to CPU.","['bug', 'backend', 'chat', 'vulkan']",2023-10-25T14:02:04Z,1,0,https://github.com/nomic-ai/gpt4all/issues/1572
35,nomic-ai/gpt4all,"Deleting chat causes GUI to become unresponsive, loading model never stops.","### System Info

GPT4ALL 2.4.19, Windows 10, Python 3.12

### Information

- [X] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [X] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

1. Open GPT4ALL. Have an existing chat or create a new chat. Let's call it chat **A**.
2. If you only have one chat, send a message, otherwise it is not possible to create a new chat.
3. Create a new chat. Let's call it chat **B**
4. While chat **B** is still loading the language model and you have not written anything in chat **B** yet, left-click on chat **A** and try to delete it.

### Result:

GUI unresponsive, loading model never stops. Have to force close.


### Additional info:

I have save chats to disk enabled. Have not tested, if behaviour changes without it.

### Expected behavior

- chat **A** will be deleted.
- chat **B** continues to load the model
- after chat **A** has been deleted, the GUI should automatically switch back to another model.","['bug', 'chat']",2023-10-05T22:11:31Z,3,2,https://github.com/nomic-ai/gpt4all/issues/1472
36,nomic-ai/gpt4all,Invalid model file / ValueError: Unable to instantiate model on python module 1.0.9+,"### System Info

Ubuntu Server 22.04

### Information

- [X] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [X] python-bindings
- [ ] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

from gpt4all import GPT4All
model = GPT4All(""orca-mini-3b.ggmlv3.q4_0.bin"")
output = model.generate(""The capital of France is "", max_tokens=3)
print(output)

### Expected behavior

Hello! I have a problem. when installing gpt4all 1.0.8 or any other version, it fails. I tried to fix it, but it didn't work out

Model downloaded at:  /root/model/gpt4all/orca-mini-3b.ggmlv3.q4_0.bin
Invalid model file
Traceback (most recent call last):
  File ""/root/test.py"", line 8, in <module>
    model = GPT4All(""orca-mini-3b.ggmlv3.q4_0.bin"")
  File ""/usr/local/lib/python3.10/dist-packages/gpt4all/gpt4all.py"", line 90, in __init__
    self.model.load_model(self.config[""path""])
  File ""/usr/local/lib/python3.10/dist-packages/gpt4all/pyllmodel.py"", line 192, in load_model
    raise ValueError(""Unable to instantiate model"")
ValueError: Unable to instantiate model
","['bug', 'backend']",2023-09-03T14:59:17Z,10,0,https://github.com/nomic-ai/gpt4all/issues/1394
37,nomic-ai/gpt4all,Application crashes on startup if model dir is in Windows network drive,"### System Info

I had changed the target dir for LLM's to a network share drive. Since, gpt4all rejects the setting, exits. Upon opening, rejects, exits. Uninstall/reinstall, no effect. Can't find anywhere to mod any config to change it locally. Attempted building from source but QT whines about compiler identities when everything in the env is intact. Frustrating, really want to get gpt4all working again.

Seen code updates that look like they address this bug. Can we get an updated Windows app release? Thanks!

### Information

- [X] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [X] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

Exactly as above.

### Expected behavior

Not exiting immediately and let me, the user, mod the configs/prefs? Seems reasonable.","['bug', 'chat']",2023-08-22T14:05:41Z,7,0,https://github.com/nomic-ai/gpt4all/issues/1364
38,nomic-ai/gpt4all,Mac Ventura Chat UI opens but shows blank window,"### System Info

GPT4all 2.4.14
![Screenshot 2023-08-22 at 7 04 03 AM](https://github.com/nomic-ai/gpt4all/assets/11655138/8a181516-bdb6-4e3d-8b4a-c937537f1523)
Macbook Air 2014, Inteal HD Graphics 5000, Ventura 13.2.1
![Screenshot 2023-08-22 at 7 05 30 AM](https://github.com/nomic-ai/gpt4all/assets/11655138/20ddc2d8-704d-4719-9f19-3f3e7268cd9b)


### Information

- [X] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [X] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

Download darwin installer from website
Install 
Open gpt4all

### Expected behavior

UI is displayed, but it's not","['bug', 'chat']",2023-08-22T05:09:38Z,3,0,https://github.com/nomic-ai/gpt4all/issues/1363
39,nomic-ai/gpt4all,LocalDocs Plugin no longer working ,"### System Info

GPT4all version v2.4.14

The localdocs plugin is no longer processing or analyzing my pdf files which I place in the referenced folder. I've tried creating new folders and adding them to the folder path, I've reused previously working folders, and I've reinstalled GPT4all a couple times. I've tried this on two pc's (win10 and win11) and with 4-5 models (Falcon, Wizard 1.1 and 1.2, LLama7B and LLama13B). I've also tried different advanced settings. 

Previously, the LocalDocs plugin worked fine. after entering a prompt, gpt4all used to show ""generating response""...""processing localdocsfolder""...""processing"". now it skips the middle step, and I don't get any references or accurate responses based on the provided documents. 

### Information

- [ ] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [ ] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

steps to reproduce the behaviour:

1. create a folder in winexplorer and add to it files (pdf, word) with information
2. reference this localdocs folder in settings
3. ask a specific question of which you know the answer should be clearly formulated in the attached files.

### Expected behavior

a concise answer from the LLM with accurate reference to the local files.","['bug', 'chat']",2023-08-21T15:18:53Z,16,0,https://github.com/nomic-ai/gpt4all/issues/1362
40,nomic-ai/gpt4all,Issue: <LocalDocs Beta Plugin's reference context cannot be click and show again in specific condition >,"### Issue you'd like to raise.

I have try LocalDocs Beta Plugin by some PDF file. After generate the content. It is will show 3 reference source with which page of PDF file and  left the ward  “context” at each end. Click on the ward “context” will show what sentence and wards have been found in PDF file for the reply.


![螢幕擷取畫面 2023-08-08 211819](https://github.com/nomic-ai/gpt4all/assets/141255885/cea157a2-86a7-482f-9d5c-d7aa564bcfb6)

However after generate series of content, when I want to look back the previous content by scroll up for a few column. I find the function of clicking on the ward “context” cannot work again in some early reply. It cannot show the sentence and wards it has found in PDF file again.



### Suggestion:

Please fix the issue in LocalDocs Beta Plugin. 

Beside the bug, I suggest to add the function of forcing LocalDocs Beta Plugin to find the content in PDF file. Not just passively check if the prompt is related to the content in PDF file.

The function of copy the whole conversation is not include the content of 3 reference source generated by LocalDocs Beta Plugin. It is better to add it in the future.","['bug', 'enhancement', 'chat']",2023-08-08T13:48:40Z,1,0,https://github.com/nomic-ai/gpt4all/issues/1325
41,nomic-ai/gpt4all,pydantic.error_wrappers.ValidationError: 3 validation errors for ChatCompletionResponse,"### System Info

Platform : Ubuntu
Python 3.10

### Information

- [X] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [ ] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [X] api

### Reproduction

parameters body to `/v1/chat/completions` endpoint
```
{
  ""model"": ""ggml-llama-2-7b-chat-q4_0.bin"",
  ""messages"": [
    {
      ""role"": ""user"",
      ""content"": ""what is AI?""
    }
  ]
}
```
Throw following error
```
gpt4all_api    | INFO:     172.20.0.1:47342 - ""POST /v1/chat/completions HTTP/1.1"" 500 Internal Server Error
gpt4all_api    | ERROR:    Exception in ASGI application
gpt4all_api    | Traceback (most recent call last):
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py"", line 436, in run_asgi
gpt4all_api    |     result = await app(  # type: ignore[func-returns-value]
gpt4all_api    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py"", line 78, in __call__
gpt4all_api    |     return await self.app(scope, receive, send)
gpt4all_api    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/fastapi/applications.py"", line 289, in __call__
gpt4all_api    |     await super().__call__(scope, receive, send)
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/starlette/applications.py"", line 122, in __call__
gpt4all_api    |     await self.middleware_stack(scope, receive, send)
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py"", line 184, in __call__
gpt4all_api    |     raise exc
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/starlette/middleware/errors.py"", line 162, in __call__
gpt4all_api    |     await self.app(scope, receive, _send)
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py"", line 91, in __call__
gpt4all_api    |     await self.simple_response(scope, receive, send, request_headers=headers)
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/starlette/middleware/cors.py"", line 146, in simple_response
gpt4all_api    |     await self.app(scope, receive, send)
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py"", line 79, in __call__
gpt4all_api    |     raise exc
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py"", line 68, in __call__
gpt4all_api    |     await self.app(scope, receive, sender)
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py"", line 20, in __call__
gpt4all_api    |     raise e
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py"", line 17, in __call__
gpt4all_api    |     await self.app(scope, receive, send)
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/starlette/routing.py"", line 718, in __call__
gpt4all_api    |     await route.handle(scope, receive, send)
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/starlette/routing.py"", line 276, in handle
gpt4all_api    |     await self.app(scope, receive, send)
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/starlette/routing.py"", line 66, in app
gpt4all_api    |     response = await func(request)
gpt4all_api    |                ^^^^^^^^^^^^^^^^^^^
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/fastapi/routing.py"", line 273, in app
gpt4all_api    |     raw_response = await run_endpoint_function(
gpt4all_api    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
gpt4all_api    |   File ""/usr/local/lib/python3.11/site-packages/fastapi/routing.py"", line 190, in run_endpoint_function
gpt4all_api    |     return await dependant.call(**values)
gpt4all_api    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
gpt4all_api    |   File ""/app/api_v1/routes/chat.py"", line 55, in chat_completion
gpt4all_api    |     return ChatCompletionResponse(
gpt4all_api    |            ^^^^^^^^^^^^^^^^^^^^^^^
gpt4all_api    |   File ""pydantic/main.py"", line 341, in pydantic.main.BaseModel.__init__
gpt4all_api    | pydantic.error_wrappers.ValidationError: 3 validation errors for ChatCompletionResponse
gpt4all_api    | choices -> 0 -> message
gpt4all_api    |   field required (type=value_error.missing)
gpt4all_api    | choices -> 0 -> index
gpt4all_api    |   field required (type=value_error.missing)
gpt4all_api    | choices -> 0 -> finish_reason
gpt4all_api    |   field required (type=value_error.missing)
```

where as following parameter to `/v1/completions/` work with very poor results. 
```
{
  ""model"": ""ggml-llama-2-7b-chat-q4_0.bin"",
  ""prompt"": [
    ""What is AI""
  ],
  ""max_tokens"": 100,
  ""temperature"": 0.18,
  ""top_p"": 1,
  ""top_k"": 50,
  ""n"": 1,
  ""stream"": false,
  ""repeat_penalty"": 1.18
}
```

### Expected behavior

Expecting it not give error","['bug', 'chat']",2023-08-05T17:45:41Z,5,6,https://github.com/nomic-ai/gpt4all/issues/1317
42,nomic-ai/gpt4all,"Issue: Broken AVX512 instructions of Xeon Phi KNL 7200 Series Processors, assumes support for ALL AVX512 instructions, and breaks during compilation.","### Issue you'd like to raise.

Runs extremely slow on Xeon Phi 7210 and 7295 CPUs. Doesn't appear to take advantage of the AVX512 instruction support specific to the Knights Landing architecture, and breaks when building with those enabled with -native and -m64 compiler flags.

This breaks the compile, as it's assumed that these processors support the ""other"" AVX512 instructions.

A really good white-paper for this is here: https://colfaxresearch.com/knl-avx512/

""Features common to Knights Landing and future Intel Xeon processors include AVX512-F and
AVX512-CD. If it is desirable to maintain binary compatibility between the Knights Landing processor and a future Intel Xeon processor, these are the subsets to use.
• All AVX-512 features of the Knights Landing processors include, in addition to AVX512-F and
AVX512-CD, the AVX512-ER, and AVX512-PF subsets. If it is not necessary to maintain binary
compatibility between the Knights Landing processor and a future Intel Xeon processor, then the
additional AVX512-ER and AVX512-PF instructions can be leveraged to vectorize previously unvectorizable code.
• All AVX-512 features of the future Intel Xeon processors include, in addition to AVX512-F and
AVX512-CD, the AVX512-BW, AVX512-DQ, AVX512-VL, AVX512-IFMA, and AVX512-VBMI
subsets. Since these instructions are not available on the Knights Landing processors, they are out
of the scope of this document"" -- Colfax Research

If we can remove the non-Xeon Phi KNL AVX512 instructions from the GPT4all source, I believe it should compile properly - these changes should help:

**Ensure only THESE are present:**
`AVX512-F , AVX512-CD, AVX512-ER, and AVX512-PF`

**Remove these:**
`AVX512-BW, AVX512-DQ, AVX512-VL, AVX512-IFMA, and AVX512-VBMI`

If someone wanted to build a really fast and cheap system for running an LLM, getting this fixed work do that, as 7210s are available for around $500-600 USD on eBay all the time.

### Suggestion:

Could you please suggest a better way to compile the chat binary so that it will use all 288 cores correctly without the present slowness?

In Linux this is running extremely slow, even when compiled from source, I've only been able to get a maximum of 10 tickets/sec which is initially that rate, then drags down to less that 0.2 tickets per second.","['bug', 'backend']",2023-08-01T20:05:49Z,0,0,https://github.com/nomic-ai/gpt4all/issues/1305
43,nomic-ai/gpt4all,ThumbsDown feedback window width too small. ,"### System Info

GTP4All v2.4.13  (Linux)
GPT4All v2.4.13  (Windows)



Using Chat GUI,  when I want to provide some feedback to the answer,  I press ""Thumbs Down"" icon.
Then small feedback window is open.  But the width of the input area width is too small.  
It is only 15 characters.  
Even thouth I make main chat GUI window width to wider,  still the ""Thums Down"" feedback window 
width is always fixed width.  
![small-window](https://github.com/nomic-ai/gpt4all/assets/107643/a2846605-b10e-420e-9267-ef8acc1b302a)

This symptom started some versions ago.  
This symptom can be reproduce on Linux and windows vresion of Chat GUI.   (I don't have Mac.)


### Information

- [ ] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [X] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

1. Open a Chat GUI 
2. Post a question,  and wait for the answer.
3. Press ""ThumbsDown"" icon.    
4. See popup feedback widow size.

### Expected behavior

The feedback window input area width should be adjustable in configuration or  automatically resize according to the main chat window width.   Short term solution would be enlarge the  input area width in fixed size.  
","['bug', 'chat']",2023-07-23T05:43:02Z,2,0,https://github.com/nomic-ai/gpt4all/issues/1250
44,nomic-ai/gpt4all,Model download mechanism needs some improvement. Specific suggestions,"### System Info

* GPT4All version: 2.4.13
* OS: Linux Mint  21.1 Cinnamon. Base: Ubuntu 22.04
* Python 3.9.17, installed via PyEnv


### Information

- [ ] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [ ] chat-ui
- [X] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

1. Open the desktop app for GPT4All.
2. Click the hamburger menu icon on the top left.
3. Click the ""downloads"" button.
4. Click the ""install"" button for the ""Wizard v1.1"" model.   
  
I have already seen [this](https://github.com/nomic-ai/gpt4all/issues/1171) issue.  
  
# There are a few suggestions I have:
1. As shown in the screenshot below, please display a message to users about alternative hyperlinks they can use to download the models. Perhaps even via torrents, since the models are quite huge.
2. When the model starts downloading, the temporary download file isn't visible in the folder selected. It might help to ensure that the partially downloaded file is available, so that the user can resume the download even if it gets interrupted.
3. The download UI needs to have an option to pause the download and resume the download if it gets interrupted.
4. If it is possible to download other models from HuggingFace and use them with GPT4All, it helps to mention it on the UI, and provide the users more information on which models they can use. [Oogabooga](https://github.com/oobabooga/text-generation-webui) allows such functionality.
5. If it's difficult to ensure an uninterrupted download from gpt4all's server, you could make models available on other sites like HuggingFace. Providing the links would help users download from there. 
6. If there are users who have a GPU and a lot more RAM, they can download and use much larger models. It helps to provide download access to such models. 
7. It would help to know what the terms like ""Quantization: q4_0"" are. I know what quantization is, but ""q4_0"" needs explanation.
8. In the download progress, it'd help to show how many GB got downloaded. As of now, it only shows a green progress bar, but no number indicating how many GB got downloaded.


### Expected behavior

The model should have downloaded fully. But I tried thrice, and the download fails. Sometimes it fails early, sometimes it downloads only half the file and fails. Sometimes it downloads 3/4th of the file and fails. Screenshot attached.  
![downloadError](https://github.com/nomic-ai/gpt4all/assets/2093933/6d42bc0d-e171-44ab-9c77-db705efa7551)
  
This appears to be a problem with the gpt4all server, because even when I went to GPT4All's website and tried downloading the model using Google Chrome browser, the download started and then failed after a while. I tried in Firefox, and the same problem happened. Surprisingly, even the Firefox extension DownloadThemAll experienced a network error in between and the download could not be resumed. Finally, Turbo Download Manager downloaded it in one shot. I'm sure this isn't a problem with my ISP, since I've downloaded larger models from other websites without the help of extensions like DownloadThemAll. I was also able to use GPT4All's desktop interface to download the GPT4All Falcon model. ","['bug', 'enhancement', 'chat']",2023-07-19T08:58:40Z,1,4,https://github.com/nomic-ai/gpt4all/issues/1228
45,nomic-ai/gpt4all,LocalDocs treats newlines as spaces,"### System Info

GPT4All v2.4.13, Debian 11

### Information

- [ ] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [X] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

1. Create a LocalDocs collection with newline-sensitive code (for example, python)
2. Prompt the model (with LocalDocs) about the code
3. Watch as it treats newlines as spaces

### Expected behavior

The database should contain newlines

### Example

`bot = mineflayer.createBot({'host': 'localhost', 'username': 'bot'}) @On(bot, 'spawn') def handle(*args): print(""I spawned"") @On(bot, 'chat')`","['bug', 'chat']",2023-07-18T13:27:49Z,2,0,https://github.com/nomic-ai/gpt4all/issues/1223
46,nomic-ai/gpt4all,"ChatGPT (GPT3.5 and GPT4) losing context after first answer, make it unsable","### System Info

**GPT4All version: v2.4.13, MacOS Ventura (13.0.1)**

Hello,

ChatGPT is losing context after the first answer it's provide. Exemple with GPT4

```
Prompt: Give me quick todo list to become a better person

Response: 1. Practice Gratitude: Start each day by acknowledging something you're grateful for. This can help you maintain a positive outlook on life.

2. Exercise Regularly: Physical activity is not only good for your body, but also for your mind. It can help reduce stress and improve your mood.

3. Eat Healthy: A balanced diet can improve your physical health and boost your energy levels.

4. Read More: Reading can expand your knowledge and improve your understanding of the world around you.

5. Practice Kindness: Be kind to others. Small acts of kindness can make a big difference in someone's day.

6. Learn Something New: Whether it's a new language, a new skill, or a new hobby, learning something new can stimulate your mind and keep you engaged.

7. Meditate: Meditation can help you stay calm and focused. It's a great way to reduce stress and improve your mental health.

8. Set Goals: Setting goals can give you a sense of purpose and direction. It can also motivate you to work harder and achieve more.

9. Spend Time with Loved Ones: Spending quality time with your family and friends can strengthen your relationships and make you feel loved and appreciated.

10. Volunteer: Volunteering can give you a sense of fulfillment and purpose. It's also a great way to give back to your community.

11. Limit Screen Time: Spending too much time in front of screens can strain your eyes and affect your sleep. Try to limit your screen time, especially before bed.

12. Practice Self-Care: Take time for yourself. Whether it's taking a bath, reading a book, or going for a walk, self-care is important for your mental and physical health.

Remember, becoming a better person is a journey, not a destination. It's about making small, consistent changes that add up over time.


Prompt: Give me more details about point 2

Response: I'm sorry, but I can't provide the information you're looking for without more context. Could you please provide more details or clarify what you're referring to with ""point 2""?
```

Can you please check why it's happening?

Thanks

### Information

- [ ] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [X] chat-ui
- [X] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

Steps to reproduce the behavior

1 - Use ChatGPT3.5 or ChatGPT4 
2 - Ask a question to the model
3 - Ask a question about the response of the model
4 - ChatGPT cannot answer, it has lost the context


### Expected behavior

I expect the model to not lose the context","['bug', 'chat']",2023-07-16T09:02:56Z,0,1,https://github.com/nomic-ai/gpt4all/issues/1214
47,nomic-ai/gpt4all,Hangs (permanent spinning circle) when pasting 88 lines of code,"### System Info

2.4.13, win10
Hangs when pasting 88 lines of code, every time no matter what model i choose, whether its snoozy or snoozy_superhot_8K
Getting permanent, spinning circle and when trying to close the app it crashes AND
doesnt release memory (RAM). so instead of 12Gb its only 7GB or less.
CPU usage shows only 8% - 10% which indicates it doesnt do anything.

### Information

- [ ] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [ ] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

1. paste around 88 lines of code
2. watch the circle spin
3. close tha app and it crashes without releasing RAM

### Expected behavior

When pasted a model should analyze what i pasted and print the analysis.","['bug', 'chat']",2023-07-16T03:45:58Z,1,0,https://github.com/nomic-ai/gpt4all/issues/1211
48,nomic-ai/gpt4all,Changing model download path doesn't clear old models from dropdown list,"### System Info

Changing model download path doesn't clear old models from dropdown list

### Information

- [ ] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [X] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction
change model path, we get a mix of both old and new paths in dropdown list

### Expected behavior
dropdown list should only show models that would still be there if I restart the app

","['bug', 'chat']",2023-07-06T22:32:28Z,2,1,https://github.com/nomic-ai/gpt4all/issues/1148
49,nomic-ai/gpt4all,Error message: The bound address is already in use,"### System Info

OS: Ubuntu Linux 22.04
GPT4All version: 2.4.8


### Information

- [X] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [X] backend
- [ ] bindings
- [ ] python-bindings
- [X] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

I use GPT4All on a Linux system without internet connection. When I start GPT4All I get the following output messages;

> [Critical] (Sun Jun 25 ... 2023): listen failed: The bound address is already in use
> [Warning] (Sun Jun 25 ... 2023): ERROR: Unable to start the server

But I don't want to start a server. There is the following line in the file `$HOME/.config/nomic.ai/GPT4All.ini `:

> serverChat=false

How can I disable the server or avoid the error message?

### Expected behavior

No error message about a bound address or about a server.
","['bug', 'chat']",2023-06-25T13:55:17Z,7,0,https://github.com/nomic-ai/gpt4all/issues/1058
50,nomic-ai/gpt4all,Incompatibility between GPT4All and Vosk speech recognizer.,"### System Info

I am testing GPT4All in a voice chat application. I am using the Vosk speech recognition package to convert speech to text. However, if I import gpt4all, and after that vosk:

```
import gpt4all
from vosk_recognizer import SpeechRecognize
```

my vosk_recognizer module in turn imports:

```
import vosk
```

I get this error:

```
File ""C:\Users\karen\myprojects\simple-chat\ChatGPT.py"", line 21, in <module>
    from vosk_recognizer import SpeechRecognize
  File ""C:\Users\karen\myprojects\simple-chat\vosk_recognizer.py"", line 7, in <module>
    import vosk
  File ""C:\Users\karen\myprojects\simple-chat\chat_env\lib\site-packages\vosk\__init__.py"", line 36, in <module>
    _c = open_dll()
  File ""C:\Users\karen\myprojects\simple-chat\chat_env\lib\site-packages\vosk\__init__.py"", line 28, in open_dll
    return _ffi.dlopen(os.path.join(dlldir, ""libvosk.dll""))
OSError: cannot load library 'C:\Users\karen\myprojects\simple-chat\chat_env\lib\site-packages\vosk\libvosk.dll': error 0x7f  
```

If I reverse the order of imports (import vosk and then gpt4all), I get an error importing gpt4all:

```
File ""C:\Users\karen\myprojects\simple-chat\ChatGPT.py"", line 24, in <module>
    import gpt4all
  File ""C:\Users\karen\myprojects\simple-chat\chat_env\lib\site-packages\gpt4all\__init__.py"", line 1, in <module>
    from .pyllmodel import LLModel # noqa
  File ""C:\Users\karen\myprojects\simple-chat\chat_env\lib\site-packages\gpt4all\pyllmodel.py"", line 49, in <module>
    llmodel = load_llmodel_library()
  File ""C:\Users\karen\myprojects\simple-chat\chat_env\lib\site-packages\gpt4all\pyllmodel.py"", line 45, in load_llmodel_library    llmodel_lib = ctypes.CDLL(llmodel_dir)
  File ""C:\Users\karen\Anaconda3\lib\ctypes\__init__.py"", line 381, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 127] The specified procedure could not be found
```

Name space conflict? This is a show stopper to implement gpt4all in this project. :(


### Information

- [ ] The official example notebooks/scripts
- [X] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [x] python-bindings
- [ ] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

1) import gpt4all
2) import vosk

Or vice versa to produce second error reported above.

### Expected behavior

No conflict. Happy family. ;)","['bug', 'backend']",2023-06-12T21:51:29Z,14,0,https://github.com/nomic-ai/gpt4all/issues/962
51,nomic-ai/gpt4all,Installer attempted to copy the DesktopLauncher to a wrong path,"### System Info

GPT4All 2.4.6, Ubuntu LTS with Gnome, french locale

### Information

- [X] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [X] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

Install GPT4 on a french Desktop.

Installer attempted to copy the DesktopLauncher to a wrong path:
 /home/dev/Desktop. Such a folder doesn't exist and the installer runs into an error.
 
 https://discord.com/channels/1076964370942267462/1090651074358149120/1115391418819690557  

### Expected behavior

Install the Programm without errors.","['bug', 'chat', 'installer']",2023-06-06T11:44:49Z,8,0,https://github.com/nomic-ai/gpt4all/issues/873
52,nomic-ai/gpt4all,Window resize bug,"### System Info

gpt4all 2.4.4, win10

When you resize the window and then you try to press a button or menu and its slightly off the position but definitely not where the symbol of menu or button is.

### Information

- [ ] The official example notebooks/scripts
- [ ] My own modified scripts

### Related Components

- [ ] backend
- [ ] bindings
- [ ] python-bindings
- [ ] chat-ui
- [ ] models
- [ ] circleci
- [ ] docker
- [ ] api

### Reproduction

Steps to reproduce:

1) resize window
2) click a button or menu

### Expected behavior

Button should be where the button is.","['bug', 'chat']",2023-06-01T17:18:23Z,6,0,https://github.com/nomic-ai/gpt4all/issues/795
