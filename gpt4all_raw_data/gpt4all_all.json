[{"issue_number":1,"repository":"nomic-ai\/gpt4all","title":"Bad UI design for 'clone and remove' button in the settings","description":"I would expect the \"remove\" button in the model settings page to work for custom OpenAI models, but it seems to only work for clones.","labels":["bug","chat","chat-ui-ux"],"created_at":"2024-12-09T22:36:39Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/3243"},{"issue_number":2,"repository":"nomic-ai\/gpt4all","title":"Models inside of subfolders can be seen in the models view but not removed","description":"GPT4All searches subfolders of the data directory for models. However, it also assumes that all models are in the root of the data in many parts of the code, by using the data dir itself instead of the `dirpath` field of ModelInfo. This causes actions such as the \"Remove\" button to not work.\r\n\r\nFix buried somewhere in #3034, but that is on hold due to other priorities.","labels":["bug"],"created_at":"2024-10-09T22:41:47Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/3069"},{"issue_number":3,"repository":"nomic-ai\/gpt4all","title":"\"New Chat\" does not change after switching locale","description":"Strings such as \"Application default\" are updated without issue, but \"New Chat\" does not switch to the English translation after switching locale (in this case, from Romanian to English).\r\n\r\n![gpt4all-newchat-tr](https:\/\/github.com\/user-attachments\/assets\/95561f23-fc41-45db-b085-765a590ad3a3)","labels":["bug","chat","chat-ui-ux","translations"],"created_at":"2024-08-19T16:17:31Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2892"},{"issue_number":4,"repository":"nomic-ai\/gpt4all","title":"Vulkan: Meta-Llama-3.1-8b-128k slow generation.","description":"<!-- Before creating a new issue, please make sure to take a few moments to check the issue tracker for existing issues about the bug. -->\r\n>[!NOTE]\r\n>Until this is fixed the workaround is use the CPU or CUDA instead.\r\n### Bug Report\r\nVulkan: Meta-Llama-3.1-8b-128k slow generation.\r\n<!-- A clear and concise description of what the bug is. -->\r\nWhen using release 3.1.1 and Vulkan the Meta-Llama-3.1-8b-128k is extremely slow. (1.5t\/s)\r\nThis is not a problem on CPU.\r\n### Steps to Reproduce\r\n\r\n<!-- List the steps that should be taken to experience this issue. Provide any relevant information about your configuration, and describe anything that was unexpected. -->\r\n\r\n1. Using GPT4All 3.1.1 with Vulkan\r\n2. Chat with Meta-Llama-3.1-8b-128k\r\n3. Speed Is immediately slow (1.5t\/s)\r\n\r\n### Expected Behavior\r\n\r\n\r\n<!-- In a few words, what did you expect to happen? -->\r\nUsing the model with llama.cpp directly reports over 60t\/s\r\nUsing the model with GPT4All before 3.1.1 I could get about 30t\/s.\r\n### Your Environment\r\n\r\n- GPT4All version: 3.1.1 (release or web_beta)\r\n- Operating System: Windows\r\n- Chat model used (if applicable): Vulkan & Meta-Llama-3.1-8b-128k\r\n\r\n<!-- You can freely edit this text, please remove all the lines you believe are unnecessary. -->\r\n","labels":["bug","chat","vulkan"],"created_at":"2024-07-29T16:43:21Z","comments":13,"reactions":2,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2768"},{"issue_number":5,"repository":"nomic-ai\/gpt4all","title":"[Crash] Assertion `isModelLoaded()` fails in ChatLLM::generateName when switching chats while model is generating","description":"Steps to reproduce:\r\n- Start generating\r\n- Switch off of the chat\r\n- Switch back onto the chat (or onto a third chat)\r\n- Assertion failure in debug build, here:\r\n\r\nhttps:\/\/github.com\/nomic-ai\/gpt4all\/blob\/fd005be844d91f821eb4ecd69c40f08afba7ae28\/gpt4all-chat\/chatllm.cpp#L872-L876","labels":["bug","chat","crash"],"created_at":"2024-07-09T21:15:53Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2626"},{"issue_number":6,"repository":"nomic-ai\/gpt4all","title":"Collections with overlapping folder paths cause false negatives in LocalDocs lookup","description":"We can miss entire documents when indexing a LocalDocs collection, because the documents are already in the database, and document paths are expected to be unique (we assume that we have already processed the document).\r\n\r\nThis can cause problems in practice, when a document is shared between two `folder_id`s, e.g. because the user has added the parent directory of one collection as another collection in the UI. In that case, the parent will be missing all of the child's documents if the child was indexed first.\r\n\r\nShould we even allow the user to add overlapping folders?","labels":["bug","chat","local-docs","chat-ui-ux"],"created_at":"2024-07-08T22:50:54Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2608"},{"issue_number":7,"repository":"nomic-ai\/gpt4all","title":"Chats loaded from text (the default) do not preserve LocalDocs context and prompt template","description":"- LocalDocs context is not restored when the chat is loaded&mdash;ChatLLM::promptInternal directly inserts database results into the context, but ChatLLM::processRestoreStateFromText bypasses this (and we don't want fresh results anyway, we want the old ones).\r\n- The system prompt and prompt template are applied to past messages when the chat is *loaded*, which is something you can otherwise never observe (AFAIK) without first closing GPT4All.\r\n\r\nNeither of these things should happen with the \"Save Chat Context\" option turned on, which used to be the default. So this can be seen as a regression.","labels":["bug","chat"],"created_at":"2024-07-08T21:44:49Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2603"},{"issue_number":8,"repository":"nomic-ai\/gpt4all","title":"GPT4All can be opened more than once - but this isn't supported","description":"GPT4All was never designed to be opened more than once&mdash;this creates multiple instances of the application, which isn't supported, and could result in corruption of the LocalDocs database, or the settings file. The local server will only work on one instance because both will try to use the same port number.\r\n\r\nRelated: #2309","labels":["bug","chat"],"created_at":"2024-07-08T15:30:34Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2590"},{"issue_number":9,"repository":"nomic-ai\/gpt4all","title":"GPT4All can restore its window geometry to somewhere off-screen","description":"### Bug Report\r\n\r\ngpt4all appears to be running in the task manager, but I can't see any Windows. My computer is an amdR7800hCPU laptop.\r\n\r\n### Steps to Reproduce\r\n\r\n1.Double click run\r\n\r\n### Expected Behavior\r\nThe program should appear in a window\r\n\r\n![PixPin_2024-07-05_16-34-27](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/68932312\/3c4e0831-37ef-4716-a9f9-ca9e9cbbc564)\r\n\r\n### Your Environment\r\n\r\n- GPT4All version:3.0.0\r\n- Operating System:Windows 10 Home Chinese Version\r\n\r\n\r\n","labels":["bug","chat"],"created_at":"2024-07-05T08:40:59Z","comments":16,"reactions":1,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2564"},{"issue_number":10,"repository":"nomic-ai\/gpt4all","title":"How to solve localdocs database error","description":"![image](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/117589240\/c6e67bf9-0040-42c2-8cbc-2777fbd77db7)\r\n","labels":["bug","chat"],"created_at":"2024-07-02T16:35:07Z","comments":23,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2516"},{"issue_number":11,"repository":"nomic-ai\/gpt4all","title":"UI: If you have too many installed models, the list gets cut off and can't be scrolled","description":"\n\n![Image](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/14168726\/a74df79a-2164-422c-98c5-e8233f07ef8b)\n\n","labels":["bug","chat","chat-ui-ux"],"created_at":"2024-05-10T21:23:40Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2338"},{"issue_number":12,"repository":"nomic-ai\/gpt4all","title":"Generation speed indication is scientific notation for speeds >= 100 tokens\/s","description":"### Bug Report\r\nThe speed indication for any model is in scientific notation for speeds >= 100 t\/sec\r\n\r\n<img width=\"1559\" alt=\"Screenshot 2024-04-30 at 10 01 48\" src=\"https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/42358482\/afdd214c-7d48-4788-9893-09ed31cebcb9\">\r\n\r\n\r\n### Steps to Reproduce\r\n1. Works with any model that is fast enough\r\n2. Speed shows up in bottom right part of the screen.\r\n\r\n### Expected Behavior\r\n* Speed should be shown in `%.2f` formatting\r\n\r\n### Your Environment\r\n- GPT4All version: 2.7.4\r\n- Operating System: OSX 13.6.3\r\n- Chat model used (if applicable):\r\n\r\n","labels":["bug","chat","chat-ui-ux"],"created_at":"2024-04-30T08:11:29Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2287"},{"issue_number":13,"repository":"nomic-ai\/gpt4all","title":"local server crashes when a model is already loaded on the GPU","description":"Your Environment\r\n- GPT4All version: 2.7.3\r\n- Operating System: Windows 11\r\n- Chat model used (if applicable): Nous Hermes 2 Mistral DPO\r\nI have open the port and allow the chat.exe in the windows firewall. I got the 'Error: connect ECONNREFUSED 127.0.0.1:4891' in the same machine. \r\nAny suggestion? thanks.\r\n![Capturesss](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/68432267\/6a880114-3391-40ef-afd9-936b203f3bb2)\r\n![Capturesss2](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/68432267\/afde6884-d500-4bfa-9a1b-a30af7f5b37c)\r\n\r\n","labels":["bug","chat","vulkan","local-server"],"created_at":"2024-04-08T20:41:52Z","comments":21,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2205"},{"issue_number":14,"repository":"nomic-ai\/gpt4all","title":"CPU t\/s regression from GPT4All 2.6.1 -> 2.6.2","description":"On my old 4-core computer, I got roughly 2.1 t\/s with openorca-chat model. When I upgraded to a 6-core PC, the speed doubled. Other models got the same speed increase. Then two things happened at the same time, I had to use a crappy video card temporarily until I got a new one to replace my broken one, and I upgraded to the latest gpt4all.\r\n\r\nMy speed dropped to 1.2 t\/s for openorca and below 1 for other models. I assumed it was my crappy GPU but I just got my new one that's benchmarked as 3 times faster than my old one and I'm still getting the same slow speed. I downgraded to gpt4all 2.6.1 and my speed is back to 4 t\/s for openorca, so it seems the problem was the latest version. Why is this?","labels":["bug","backend","chat"],"created_at":"2024-04-08T16:13:09Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2204"},{"issue_number":15,"repository":"nomic-ai\/gpt4all","title":"High CPU usage in chat with Hermes 2 Pro Mistral 7B after generation has finished","description":"### Bug Report\r\n\r\nWith Hermes 2 Pro Mistral 7B, in certain situations chat.exe causes high CPU usage even after generation has finished.\r\n\r\n### Steps to Reproduce\r\n\r\n1. Install [Hermes 2 Pro Mistral 7B](https:\/\/huggingface.co\/NousResearch\/Hermes-2-Pro-Mistral-7B-GGUF\/blob\/main\/Hermes-2-Pro-Mistral-7B.Q5_K_M.gguf) (SHA256 checks out!)\r\n\r\n2. Set the context length to 8192. This is OK because the model supports 32k. The device is CPU, with 8 threads. (These settings may or may not be important here.)\r\n\r\n3. Open the chat, load the model and enter e.g. the following prompt:\r\n```\r\nWrite a fictional story titled \"A Triangle\", with the following synopsis:\r\n\r\n\"An observer becomes entranced by a seemingly ordinary couple on the street, follows them home, and then\r\nwatches them from outside in the rising floodwaters, drawing an eerie connection between the woman and\r\na discarded, burned chair they\u2019d noticed earlier.\"\r\n\r\nMake sure the story is at least three pages long.\r\n```\r\nThe actual prompt is irrelevant, as long as the response is long enough (c. 700 words is sufficient - not that long actually).\r\n\r\n4. The model will respond with a story (or, at any rate, something that resembles it).\r\n\r\n5. **Once the generation has finished, the CPU usage stays high as if the generation is still running, instead of dropping to 0. It's due to chat.exe, according to Task Manager.**\r\n\r\n6. **Upon closing the chat window, chat.exe stays active in the background and high CPU usage continues, until the process is killed in Task Manager.**\r\n\r\nI haven't seen this problem with other models, so it's kind of a narrow issue, but might be worth a look.\r\n\r\n### Your Environment\r\n\r\n- GPT4All version: 2.7.3\r\n- Operating System: Windows 10\r\n- Chat model used (if applicable): Hermes 2 Pro Mistral 7B\r\n\r\n","labels":["bug","chat"],"created_at":"2024-03-26T08:53:41Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2167"},{"issue_number":16,"repository":"nomic-ai\/gpt4all","title":"Weird download speed, varying between very large negative and very large positive numbers","description":"Hello.\r\n\r\nI don't know if this is a Bug per se... looks weird, anyway.\r\n\r\nOn the Download dialog, when downloading an LLM (the .gguf file in the video, maybe others), a large negative number is being shown at times, as the download speed; it appears once in a while (perhaps random) for ~1 second, then (not always immediately afterwards) another large number appears, but a positive one, say, 25 MB\/s. \r\n\r\n### Steps to Reproduce\r\n\r\n1. Select an LLM, then Click on the Download button\r\n2. Watch it burn\r\n\r\n### Expected Behavior\r\n\r\nA more or less reasonable but consistent download speed, over time (in the 10^1 MB\/s).\r\n\r\n### Your Environment\r\n\r\n- GPT4All v2.7.3\r\n- Operating System: Windows 10\r\n- Chat model used (if applicable): (incomplete)\r\n\r\n\r\nThank you for considering this.\r\n\r\n\r\nhttps:\/\/github.com\/nomic-ai\/gpt4all\/assets\/158754254\/6390e67e-812a-40ea-8874-9f994bd6191e\r\n\r\n","labels":["bug","chat"],"created_at":"2024-03-21T20:00:56Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2156"},{"issue_number":17,"repository":"nomic-ai\/gpt4all","title":"Vulkan: Gibberish output on Quadro (Kepler, Maxwell, Turing)","description":"<!-- Before creating a new issue, please make sure to take a few moments to check the issue tracker for existing issues about the bug. -->\r\n\r\n### Bug Report\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\nDoes anyone know why GPT4All will respond in gibberish. \r\nThe behavior is  inconsistent  \r\n\r\n### Steps to Reproduce\r\n<!-- List the steps that should be taken to experience this issue. Provide any relevant information about your configuration, and describe anything that was unexpected. -->\r\n\r\n1.\tStart GPT4All selected model \u201cNous Hermes Mistral DPO\u201d\r\n2. \tEnter the prompt \u201cSample Java program\u201d.\r\n3.     The response is legible.\r\n4. \tEnter the prompt \u201cSample R program\u201d.\r\n5. \tThe response is gibberish.\r\n\r\n\r\n### Expected Behavior\r\n\r\n<!-- In a few words, what did you expect to happen? -->\r\nThe response should not be gibberish it is currently not legible\r\n### Your Environment\r\n\r\n- GPT4All version: GPT4All V2.7.3\r\n- Operating System: Win 10 Enterprise \r\n- Chat model used (if applicable): Nous Hermes Mistral DPO but it does not matter\r\n- Intel(R) Xeon(R) W-10885M CPU @ 2.40GHz, 2400 Mhz, 8 Core(s), 16 Logical Processor(s)\r\n- 128 GB RAM \r\n- GPU Quadro RTX 3000 6 GB RAM\r\n\r\n- See screenshot \r\n![2024-03-20_11-15-34](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/23298288\/d117c99a-ac78-4961-943c-f656c536e7dd)\r\n\r\n\r\n<!-- You can freely edit this text, please remove all the lines you believe are unnecessary. -->\r\n\r\n","labels":["bug","backend","vulkan"],"created_at":"2024-03-20T15:50:24Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2149"},{"issue_number":18,"repository":"nomic-ai\/gpt4all","title":"GPT4All 2.7.3: Context \"link\": inactive when clicked to show a snippet; selectable by double-click","description":"Hello.\r\n\r\nSometimes, the \"links\" marked \"Context\" after a response:\r\n- are selectable by double-clicking on them, instead of showing a document snippet\r\n- when clicked, do not show a snippet from a LocalDoc which was presumably used\/quoted in the response\r\n\r\nOccurrence: unpredictable\/unexpected\/no repeatability discerned\r\nLocation: the entire set of quotes after a response from the currently-selected LLM; not in the set preceding it, not in the set after it.\r\n\r\n\r\n### Steps to Reproduce\r\n\r\n1. click on such a \"link\": no snippet is shown; this is unexpected\r\n2. drag over a \"link\" with the mouse, starting from a character\/letter\/space before it, from left to right; the \"link\" is selected; this is expected \r\n3. double-click a \"link\": the \"link\" is selected; this is unexpected\r\n\r\n\r\n### Expected Behavior\r\n\r\nAll _visible_ \"links\" marked \"Context\" should show a snippet when clicked.\r\n\r\n\r\n### Your Environment\r\n\r\n- GPT4All version: 2.7.3\r\n- Operating System: Windows 10, x64\r\n- Chat model used (if applicable): Nous Hermes 2 Mistral DPO, Context Length: 30720\r\n\r\nAs an aside - the mouse pointer over these links should change to hand or some other form indicating their distinct role, like their color does.\r\n\r\nThank you for considering this.\r\n\r\n \r\n![img_gpt4all_q1](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/158754254\/d27ad0dc-8994-47cc-b829-5d8d78f34200)\r\n","labels":["bug","chat"],"created_at":"2024-03-13T22:52:05Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2120"},{"issue_number":19,"repository":"nomic-ai\/gpt4all","title":"GPT4All 2.7.1 - searching localdocs even if no category is checked\/selected; repeating the reply and the Context fragments, regardless of LLM","description":"<!-- Before creating a new issue, please make sure to take a few moments to check the issue tracker for existing issues about the bug. -->\r\n\r\n### Bug Report\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### Steps to Reproduce\r\n\r\n**1. Loaded the model: mistral-7b-instruct-v.0.1Q4_0.gguf**\r\n1.1. checked a (1, one) LocalDocs collection\r\n1.2. asked the model something\r\n1.3. the model searched the checked LocalDocs collection: \"searching localdocs: [category]...\". \r\n1.4. the model replied as per the checked LocalDocs collection; following the reply, there were N Certain fragments cited, as Context.\r\n1.5. This was expected.\r\n1.6. unchecked the same LocalDocs collection\r\n1.7. reloaded the model by using the icon next to its name in the combobox\r\n1.8. asked the model the same question as before\r\n1.9. it started searching again the localdocs, but _NOT_ specifying which collection (none was checked, anyway):  \"searching localdocs: ...\". in the end, it 1) _repeated the reply_ from the time when the collection was checked and 2) quoted _the same_ N Certain fragments, as Context, like the previous model\r\n1.10. unloaded the model\r\n\r\n**2. Loaded the model: gpt4all-falcon-newbpe-q4_0.gguf**\r\n2.1. checked again - the same LocalDocs collection as in the case of previous model\r\n2.2. asked the current model the same question as the previous model\r\n2.3. the current model searched in the checked LocalDocs collection: \"searching localdocs: [category]...\". \r\n2.4. the model replied as per the checked LocalDocs collection; following the reply, there were N Certain fragments cited, as Context.\r\n2.5. This was expected; however, the cited Contexts were _exactly the same_ as in the case of the previous model.\r\n2.6. unchecked the same LocalDocs collection\r\n2.7. reloaded the current model by using the icon next to its name in the combobox\r\n2.8. asked the model the same question as before\r\n2.9. it started searching again the localdocs, but _NOT_ specifying which collection (none was checked, anyway):  \"searching localdocs: ...\". in the end, it 1) _repeated the reply_ from the time when the collection was checked and 2) quoted _the same_ N Certain fragments, as Context, like the previous model\r\n\r\n\r\n### Expected Behavior\r\n\r\nExpectations:\r\n- when a (even 1, since _it seems_ like searching the LocalDocs depends on the number>1 of collections, _although not every time_) LocalDocs collection was checked, the reply should have been different to that when no collection was checked.\r\n- moreover, the quoted Contexts at the end of replies should have been 1) present when the reply was built upon checked LocalDocs collections and 2) absent when no such collection was checked.\r\n- moreover, the content of the replies should have been different even in 1 word, regardless if any LocalDocs category was checked.\r\n\r\n### Your Environment\r\n\r\n- GPT4All 2.7.1\r\n- Windows 10, latest update as of 2024.02.26\r\n- Mistral Instruct 7B, Falcon 7B\r\n- Mistral Instruct: Context Length: 16384, Temperature: 0.1, Top P; 0.6, Prompt Batch Size: 128\r\n- Falcon: the same values as for Mistral, above\r\n\r\nIt looks like both the reply, the cited Contexts and the LocalDocs categories are being remembered and transferred to the next reply (replies) to the same question, regardless of the currently loaded model.","labels":["bug","chat","local-docs"],"created_at":"2024-02-26T23:23:36Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2033"},{"issue_number":20,"repository":"nomic-ai\/gpt4all","title":"Ceased GPT4ALL during indexing by Local Documents plugin","description":"<!-- Before creating a new issue, please make sure to take a few moments to check the issue tracker for existing issues about the bug. -->\r\n\r\n### Bug Report\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### Steps to Reproduce\r\n\r\n<!-- List the steps that should be taken to experience this issue. Provide any relevant information about your configuration, and describe anything that was unexpected. -->\r\n\r\n1. Indexing Local Documents more 20 pdf files.\r\n2. Per a book page are more 500 pages\r\n3. Sometimes didn't indexing or ceased GPT4ALL\r\n\r\n### Expected Behavior\r\n\r\n<!-- In a few words, what did you expect to happen? -->\r\n\r\n### Your Environment\r\n\r\n- GPT4All version: 2.7.1\r\n- Operating System: Windows 11\r\n- Chat model used (if applicable): EM German Mistral\r\n\r\n<!-- You can freely edit this text, please remove all the lines you believe are unnecessary. -->\r\nI'm testing LLM learning for Local Documents.\r\nI already explained bug above.\r\nHave a limitation of Local Documents plugin?\r\nex, Pages amount per a file or File size or number?","labels":["bug","chat","local-docs"],"created_at":"2024-02-26T05:36:51Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2029"},{"issue_number":21,"repository":"nomic-ai\/gpt4all","title":"Chat UI crashes for long inputs","description":"I am running the latest version of GPT4all, version 2.7.0, on Windows 11 Pro. Whenever I provide a long input to a model, the program crashes immediately after it finishes responding, and I am unable to use it again. Can you please identify the reason for this error and provide a solution to fix it? Your assistance would be greatly appreciated.","labels":["bug","chat"],"created_at":"2024-02-24T14:42:53Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/2020"},{"issue_number":22,"repository":"nomic-ai\/gpt4all","title":"Online v2.7.1 release doesn't open, offline v2.7.0 release does","description":"### Bug Report\r\n\r\nI upgraded the app from 2.70 to 2.71, and first Windows Defender gave me a virus notification and removed some files and then the app stopped working. \r\n\r\n### Steps to Reproduce\r\n\r\nUpgrade from 2.70 to 2.71 using in app upgrade option on windows 11\r\n\r\n\r\n### Expected Behavior\r\n\r\nNothing works. The chat UI doesn't start and the issue persists even after reinstalling until I downgraded to 2.7.0.\r\n\r\n### Your Environment\r\n\r\n- GPT4All version: 2.7.1\r\n- Operating System: Windows 11\r\n- Chat model used (if applicable): mistral 7b openorca\r\n\r\n\r\n","labels":["bug","chat"],"created_at":"2024-02-22T11:24:58Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1998"},{"issue_number":23,"repository":"nomic-ai\/gpt4all","title":"Windows: System menu for window has \"Minimize\" and \"Maximize\" options disabled when opened with keyboard (Alt+Space shortcut)","description":"<!-- Before creating a new issue, please make sure to take a few moments to check the issue tracker for existing issues about the bug. -->\r\n\r\n### Bug Report\r\n\r\n<!-- A clear and concise description of what the bug is. -->\r\n\r\n### Steps to Reproduce\r\n\r\n<!-- List the steps that should be taken to experience this issue. Provide any relevant information about your configuration, and describe anything that was unexpected. -->\r\n\r\n1. On Windows 11, open the application\r\n2. Press Alt+Space to show the window's system menu\r\n3. Press \"n\" for minimize or \"m\" for maximize\r\n4. Observe that the minimize and maximize options are not enabled\r\n5. Press Esc to close the system menu\r\n6. Open the window's system menu by clicking the window's top left corner. The options are now available.\r\n\r\n### Expected Behavior\r\n\r\nI expect the \"Minimize\" and \"Maximize\" options to be available (not disabled) and to activate using the \"N\" and \"X\" shortcuts, respectively.\r\n\r\n### Actual Behavior:\r\n\r\nThe options are disabled, and do not activate using the shortcuts.\r\n\r\n### Your Environment\r\n\r\n- GPT4All version: 2.7.0\r\n- Operating System: Windows 11 \r\n- Chat model used (if applicable):\r\n\r\n<!-- You can freely edit this text, please remove all the lines you believe are unnecessary. -->\r\n","labels":["bug","chat"],"created_at":"2024-02-18T03:31:56Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1977"},{"issue_number":24,"repository":"nomic-ai\/gpt4all","title":"Cannot use local server with GPU enabled because it tries to load two models on GPU at once","description":"### System Info\r\n\r\nGPT4All: 2.6.2-2\r\nPython: 3.11.5\r\nOS: Archlinux\r\nKernel: 6.7.3-arch1-2\r\n\r\n### Information\r\n\r\n- [X] The official example notebooks\/scripts\r\n- [ ] My own modified scripts\r\n\r\n### Reproduction\r\n\r\n1. Start the GPT4All application and enable the local server\r\n2. Download the mistral-7b-openorca.Q4_0.gguf model\r\n3. Use the code below to test.\r\n\r\n```\r\nimport openai\r\n\r\nopenai.api_base = \"http:\/\/localhost:4891\/v1\"\r\n\r\nopenai.api_key = \"not needed for a local LLM\"\r\n\r\n# Set up the prompt and other parameters for the API request\r\nprompt = \"Who is Michael Jordan?\"\r\n\r\nmodel = \"mistral-7b-openorca.Q4_0.gguf\"\r\n\r\n# Make the API request\r\nresponse = openai.Completion.create(\r\n    model=model,\r\n    prompt=prompt,\r\n    max_tokens=50,\r\n    temperature=0.28,\r\n    top_p=0.95,\r\n    n=1,\r\n    echo=True,\r\n    stream=False\r\n)\r\n\r\n# Print the generated completion\r\nprint(response)\r\n```\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect the model to return an answer instead of crashing.\r\n\r\nInstead I get this log when starting the gpt4all-chat from the shell:\r\n\r\n```\r\n[Debug] (Tue Feb 6 16:28:44 2024): deserializing chat \"\/home\/arael\/.local\/share\/nomic.ai\/GPT4All\/\/gpt4all-a93ce28a-a563-4bc0-959f-4e9abcec67f6.chat\"\r\n[Debug] (Tue Feb 6 16:28:44 2024): deserializing chat \"\/home\/arael\/.local\/share\/nomic.ai\/GPT4All\/\/gpt4all-3974f69f-fd9f-4e00-80a8-b70c02dd298c.chat\"\r\n[Debug] (Tue Feb 6 16:28:44 2024): deserializing chat \"\/home\/arael\/.local\/share\/nomic.ai\/GPT4All\/\/gpt4all-f11fba17-3fe5-476f-8436-48aeae4f10df.chat\"\r\n[Debug] (Tue Feb 6 16:28:44 2024): deserializing chats took: 1 ms\r\nllama.cpp: using Vulkan on AMD Radeon RX 6800S (RADV NAVI23)\r\n^NGGML_ASSERT: \/home\/arael\/.cache\/yay\/gpt4all-chat\/src\/gpt4all-2.6.2\/gpt4all-backend\/llama.cpp-mainline\/ggml-kompute.cpp:2009: s_kompute_context == nullptr\r\n[1]    28706 IOT instruction (core dumped)  gpt4all-chat\r\n```","labels":["bug","chat"],"created_at":"2024-02-06T07:47:44Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1932"},{"issue_number":25,"repository":"nomic-ai\/gpt4all","title":"No models available - sbert model link broken - GPT4ALL.IO\/MODELS\/MODELS2.JSON","description":"### System Info\n\nWhen attempting to add localdocs plugin, nothing shows up when it attempts to download from:  GPT4ALL.IO\/MODELS\/MODELS2.JSON\r\n\r\nit looks like broken link issue ?\n\n### Information\n\n- [ ] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Reproduction\n\nWhen attempting to add localdocs plugin, nothing shows up when it attempts to download from:  GPT4ALL.IO\/MODELS\/MODELS2.JSON\r\n\r\nit looks like broken link issue ?\n\n### Expected behavior\n\nWhen attempting to add localdocs plugin, nothing shows up when it attempts to download from:  GPT4ALL.IO\/MODELS\/MODELS2.JSON\r\n\r\nit looks like broken link issue ?","labels":["bug","chat"],"created_at":"2024-01-22T01:44:42Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1861"},{"issue_number":26,"repository":"nomic-ai\/gpt4all","title":"macOS: LocalDocs can not add local directory","description":"### System Info\n\nGPT4All 2.5.4 macOS 14.2.1 (23C71) M1 Macbook 16GB mem 2TB disk\n\n### Information\n\n- [X] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Reproduction\n\nI installed Mistral OpenOrca and LocalDocs.\r\nConfiguring LocalDocs the system asks\r\n- if it can access my Download folder (I do not know why), I answered yes\r\n- I can select the folder with my files, which returns file:\/\/\/User\/MyUsername\/..... and the Connection Name turns red\r\n- I try to remove the prefix file:\/\/ - so path looks like the path in the  other config menus. connection name stays red\r\nThe local path is not parsed.\r\n\r\n\n\n### Expected behavior\n\naccept the parsed directory name","labels":["bug","chat","local-docs"],"created_at":"2024-01-09T10:31:00Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1820"},{"issue_number":27,"repository":"nomic-ai\/gpt4all","title":"Model loaded into GPU ram but 0% GPU usage","description":"### System Info\r\n\r\nGPT4all version 1.0.2\r\nUbuntu Linux 24 LTS with kernel 5.15.0-91-generic #101-Ubuntu SMP\r\nNvidia Tesla P100-PCIE-16GB\r\nNvidia driver v545.29.06\r\nCuda 12.3\r\n\r\n### Information\r\n\r\n- [ ] The official example notebooks\/scripts\r\n- [X] My own modified scripts\r\n\r\n### Reproduction\r\n\r\n`\r\npython3 app.py repl --model \/model\/location\/mistral-7b-instruct-v0.2.Q5_K_S.gguf -d nvidia\r\n`\r\n\r\nIn another console: `nvtop`\r\n\r\n### Expected behavior\r\n\r\nnvtop should show python3 using several GB of ram upon loading and GPU usage should increase from 0% to\/near 100% during inference.\r\n\r\nInstead, nvtop shows python3 using several GB of ram as expected, but GPU usage remains at 0% throughout inference.\r\n\r\nSeparately, the GPU usage does increase to 100% when I perform inference using a non-quantized model via transformers.\r\n\r\nEdit: This seems to be model dependent. gpt4all uses the GPU when performing inference using nous-hermes-llama2-13b.Q4_0.gguf and mistral-7b-instruct-v0.1.Q4_0.gguf but not when using mistral-7b-instruct-v0.2.Q5_K_S.gguf or Wizard-Vicuna-13B-Uncensored.Q6_K.gguf. \r\n\r\nI started to think that only 4-bit quantized models are running on the GPU, but then I tried Wizard-Vicuna-13B-Uncensored.Q4_K_M.gguf and found that it used GPU ram but did not run on the GPU.","labels":["bug"],"created_at":"2024-01-03T13:30:13Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1803"},{"issue_number":28,"repository":"nomic-ai\/gpt4all","title":"temp=0 should use greedy sampling instead of crashing or UB","description":"### System Info\n\nOS: Windows 11 64 bit\r\nPython 3.10.4\r\nModel: nous-hermes-llama2-13b.Q4_0.gguf\n\n### Information\n\n- [ ] The official example notebooks\/scripts\n- [X] My own modified scripts\n\n### Reproduction\n\nStep1: The following code produces the error:\r\n```python\r\nfrom gpt4all import GPT4All\r\nmodel = GPT4All(\"nous-hermes-llama2-13b.Q4_0.gguf\")\r\nwith model.chat_session(\"You are a cyber secuirty expert who can find vulnerabilities in source code\"):\r\n    response = model.generate('who are you?', temp=0)\r\n    print(response)\r\n```\r\n![image](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/20457968\/931225b3-92af-4d28-a382-a86aa0227c23)\r\n\n\n### Expected behavior\n\n1. Without giving system_prompt, the chat session works fine.\r\n2. With system_prompt, the model should respond accordingly without error as mentioned in the doc https:\/\/docs.gpt4all.io\/gpt4all_python.html#managing-templates","labels":["bug","backend","chat","crash"],"created_at":"2024-01-02T18:35:22Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1799"},{"issue_number":29,"repository":"nomic-ai\/gpt4all","title":"\"retrieving localdocs: ...\"  spinning circle","description":"### System Info\r\n\r\nv2.5.4\r\nwin 10\r\nModel: Openchat 3.5 -1210\r\n\r\nTried to stop it and didnt work. Pressing \"stop generating\" didnt work.\r\n\r\nPS:\r\nNotice most of the time when the generation is finished it sort of hangs as it shows \"stop generating\" button so I assume its still generating....something,\r\nso this will be another recent model after \r\n- stablelm-zephyr\r\n-  magiccoder\r\n that doesnt work in gpt4all. \r\nSeems only the old models work in gpt4all.\r\n\r\n### Information\r\n\r\n- [ ] The official example notebooks\/scripts\r\n- [ ] My own modified scripts\r\n\r\n### Reproduction\r\n\r\n1. ask to write a code in python to read btc price\r\n2. once finished ask it a question about diference between api key and secret key \r\nOR \"ok thanks\"\r\n4. get the forever lasting spinning circle \r\n\r\n### Expected behavior\r\n\r\nto answer question","labels":["bug","chat"],"created_at":"2023-12-15T05:11:12Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1754"},{"issue_number":30,"repository":"nomic-ai\/gpt4all","title":"GPT4all not using my GPU because Models not unloading from VRAM when switching","description":"### Issue you'd like to raise.\n\nRTX 3060 12 GB is available as a selection, but queries are run through the cpu and are very slow.\r\n\r\nCuda compilation tools, release 12.2, V12.2.128\r\nBuild cuda_12.2.r12.2\/compiler.33053471_0\n\n### Suggestion:\n\n_No response_","labels":["bug","chat"],"created_at":"2023-11-28T23:04:14Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1688"},{"issue_number":31,"repository":"nomic-ai\/gpt4all","title":"Process finished with exit code 139 (interrupted by signal 11: SIGSEGV) ","description":"1. I am able to run this model as well as lighter models, but in about 2-4 promts given to the model (in the process of answering) it fails with \"Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)\".  Tried python from 3.9 to 3.11. Also, tried with Jupyter lab (kernel 3.10), PyCharm and terminal. It is all the same. Any ideas where the problem may come from?\r\n\r\n2. How much GPU memory do I need to run the model, is 24G of GPU memory enough?","labels":["bug","python-bindings"],"created_at":"2023-11-14T10:07:38Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1646"},{"issue_number":32,"repository":"nomic-ai\/gpt4all","title":"\ufeffERROR: The prompt size exceeds the context window size and cannot be processed. (Using GUI)","description":"### System Info\n\nWindows 10, GPT4ALL GUI version 2.5.2.\n\n### Information\n\n- [ ] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Reproduction\n\n1. Create a localdocs directory and fill it with some .html files\r\n2. Reference that database of context docs in the GUI\r\n3. Ask a fairly small question to the GPT4ALL Falcon model (such as \"Tell me the advantages of Python over other programming languages.\")\r\n4. Receive the following error:\r\n\r\n```\r\n\ufeffERROR: The prompt size exceeds the context window size and cannot be processed.\r\n```\n\n### Expected behavior\n\nA clear response. This issue has been raised and gone stale [here](https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/664) and was suggested to create a new issue.\r\n\r\nI've gone into the settings and increased the input and output token length to as high as 4096, no change. How to fix?","labels":["bug","chat"],"created_at":"2023-11-10T16:02:43Z","comments":0,"reactions":1,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1638"},{"issue_number":33,"repository":"nomic-ai\/gpt4all","title":"SIGSEGV if Embed4All is called too many times","description":"### System Info\n\n- Macbook Pro M1 16GB RAM\r\n- Python 3.11.6\r\n- gpt4all==2.0.1\n\n### Information\n\n- [ ] The official example notebooks\/scripts\n- [X] My own modified scripts\n\n### Reproduction\n\n## Description\r\nI've chunked my document up where each chunk is no more than 500 words with a total of ~300 chunks. I am then looping over each chunk and using the default Embed4All() embedder. However, when it gets to about 30 chunks in, I get a python segmentation fault (which I understand is likely due to running out of memory). I can see my RAM usage increase throughout this process. It seems to me that the C\/C++ running under the hood is holding onto the memory even after the embedding is complete? I have ran the exact same using OpenAI without error so I don't think the resultant embeddings are taking up the memory. I also know it's not a specific chunk as I've run the embeddings from different starting points.\r\n\r\nI've tried using `device=\"gpu\"` with no difference and I've also tried using a different embedder model.\r\n\r\n## Snippet\r\n```python\r\ndef get_embedding(text: str) -> list[float]:\r\n    embedder = Embed4All(device=\"cpu\", verbose=True)\r\n    output = embedder.embed(text)\r\n    return output\r\n```\r\n\r\n## Error\r\n`18392 segmentation fault`\r\n\r\n### Apple error report\r\n```\r\nCrashed Thread:        0  Dispatch queue: com.apple.main-thread\r\n\r\nException Type:        EXC_BAD_ACCESS (SIGSEGV)\r\nException Codes:       KERN_INVALID_ADDRESS at 0x0000000000000011\r\nException Codes:       0x0000000000000001, 0x0000000000000011\r\n\r\nTermination Reason:    Namespace SIGNAL, Code 11 Segmentation fault: 11\r\nTerminating Process:   exc handler [59316]\r\n\r\nVM Region Info: 0x11 is not in any region.  Bytes before following region: 4368498671\r\n      REGION TYPE                    START - END         [ VSIZE] PRT\/MAX SHRMOD  REGION DETAIL\r\n      UNUSED SPACE AT START\r\n--->  \r\n      __TEXT                      104620000-104624000    [   16K] r-x\/r-x SM=COW  ...\/MacOS\/Python\r\n\r\nThread 0 Crashed::  Dispatch queue: com.apple.main-thread\r\n0   libbert-default.dylib         \t       0x127f0cf20 ggml_new_tensor_impl + 296\r\n1   libbert-default.dylib         \t       0x127f0d244 ggml_new_tensor_1d + 36\r\n2   libbert-default.dylib         \t       0x127ef1118 bert_eval(bert_ctx*, int, int const*, int, float*) + 288\r\n3   libbert-default.dylib         \t       0x127ef27d8 bert_load_from_file(char const*) + 4008\r\n4   libbert-default.dylib         \t       0x127ef2db8 Bert::loadModel(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>> const&) + 36\r\n5   libllmodel.dylib              \t       0x127707904 llmodel_loadModel + 148\r\n6   libffi.dylib                  \t       0x19bd8f050 ffi_call_SYSV + 80\r\n7   libffi.dylib                  \t       0x19bd97adc ffi_call_int + 1208\r\n8   _ctypes.cpython-311-darwin.so \t       0x1064c8874 _ctypes_callproc + 788\r\n9   _ctypes.cpython-311-darwin.so \t       0x1064c34c4 PyCFuncPtr_call + 220\r\n10  Python                        \t       0x104f93e8c _PyObject_MakeTpCall + 128\r\n11  Python                        \t       0x105071e6c _PyEval_EvalFrameDefault + 42120\r\n12  Python                        \t       0x105076554 _PyEval_Vector + 116\r\n13  Python                        \t       0x104f941a8 _PyObject_FastCallDictTstate + 208\r\n14  Python                        \t       0x104ffe0e8 slot_tp_init + 188\r\n15  Python                        \t       0x104ff6598 type_call + 136\r\n16  Python                        \t       0x104f94ca4 _PyObject_Call + 124\r\n17  Python                        \t       0x105073cac _PyEval_EvalFrameDefault + 49864\r\n18  Python                        \t       0x105076554 _PyEval_Vector + 116\r\n19  Python                        \t       0x104f941a8 _PyObject_FastCallDictTstate + 208\r\n20  Python                        \t       0x104ffe0e8 slot_tp_init + 188\r\n21  Python                        \t       0x104ff6598 type_call + 136\r\n22  Python                        \t       0x104f93e8c _PyObject_MakeTpCall + 128\r\n23  Python                        \t       0x105071e6c _PyEval_EvalFrameDefault + 42120\r\n24  Python                        \t       0x105066da0 PyEval_EvalCode + 168\r\n25  Python                        \t       0x1050bd7a0 run_eval_code_obj + 84\r\n26  Python                        \t       0x1050bd704 run_mod + 112\r\n27  Python                        \t       0x1050bd544 pyrun_file + 148\r\n28  Python                        \t       0x1050bcf94 _PyRun_SimpleFileObject + 268\r\n29  Python                        \t       0x1050bc92c _PyRun_AnyFileObject + 216\r\n30  Python                        \t       0x1050d950c pymain_run_file_obj + 220\r\n31  Python                        \t       0x1050d8e4c pymain_run_file + 72\r\n32  Python                        \t       0x1050d872c Py_RunMain + 704\r\n33  Python                        \t       0x1050d9868 Py_BytesMain + 40\r\n34  dyld                          \t       0x18af31058 start + 2224\r\n```\n\n### Expected behavior\n\nThe embedder can run many times without crashing in a single program.","labels":["bug","python-bindings"],"created_at":"2023-10-27T22:01:29Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1583"},{"issue_number":34,"repository":"nomic-ai\/gpt4all","title":"Vulkan backend can cause desktop UI to freeze\/crash","description":"When attempting to run GPT4All with the vulkan backend on a system where the GPU you're using is also being used by the desktop - this is confirmed on Windows with an integrated GPU - this can result in the desktop GUI freezing and the gpt4all instance not running.\r\n\r\nI believe the problem is contention in GPU memory between what the desktop is trying to use and what we are trying to allocate with vulkan.\r\n\r\nI can reproduce this on my old Windows 10 laptop with the integrated AMD Vega GPU consistently\r\n\r\nNot sure how to fix at this time :(\r\n\r\n### Reproduction\r\n\r\nReproduced with old Windows 10 laptop trying to use the integrated AMD Vega gpu. More system details to come.\r\n\r\n### Expected behavior\r\n\r\nWe should detect this case ahead of time and fallback to CPU.","labels":["bug","backend","chat","vulkan"],"created_at":"2023-10-25T14:02:04Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1572"},{"issue_number":35,"repository":"nomic-ai\/gpt4all","title":"Deleting chat causes GUI to become unresponsive, loading model never stops.","description":"### System Info\r\n\r\nGPT4ALL 2.4.19, Windows 10, Python 3.12\r\n\r\n### Information\r\n\r\n- [X] The official example notebooks\/scripts\r\n- [ ] My own modified scripts\r\n\r\n### Related Components\r\n\r\n- [ ] backend\r\n- [ ] bindings\r\n- [ ] python-bindings\r\n- [X] chat-ui\r\n- [ ] models\r\n- [ ] circleci\r\n- [ ] docker\r\n- [ ] api\r\n\r\n### Reproduction\r\n\r\n1. Open GPT4ALL. Have an existing chat or create a new chat. Let's call it chat **A**.\r\n2. If you only have one chat, send a message, otherwise it is not possible to create a new chat.\r\n3. Create a new chat. Let's call it chat **B**\r\n4. While chat **B** is still loading the language model and you have not written anything in chat **B** yet, left-click on chat **A** and try to delete it.\r\n\r\n### Result:\r\n\r\nGUI unresponsive, loading model never stops. Have to force close.\r\n\r\n\r\n### Additional info:\r\n\r\nI have save chats to disk enabled. Have not tested, if behaviour changes without it.\r\n\r\n### Expected behavior\r\n\r\n- chat **A** will be deleted.\r\n- chat **B** continues to load the model\r\n- after chat **A** has been deleted, the GUI should automatically switch back to another model.","labels":["bug","chat"],"created_at":"2023-10-05T22:11:31Z","comments":3,"reactions":2,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1472"},{"issue_number":36,"repository":"nomic-ai\/gpt4all","title":"Invalid model file \/ ValueError: Unable to instantiate model on python module 1.0.9+","description":"### System Info\n\nUbuntu Server 22.04\n\n### Information\n\n- [X] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Related Components\n\n- [ ] backend\n- [ ] bindings\n- [X] python-bindings\n- [ ] chat-ui\n- [ ] models\n- [ ] circleci\n- [ ] docker\n- [ ] api\n\n### Reproduction\n\nfrom gpt4all import GPT4All\r\nmodel = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\r\noutput = model.generate(\"The capital of France is \", max_tokens=3)\r\nprint(output)\n\n### Expected behavior\n\nHello! I have a problem. when installing gpt4all 1.0.8 or any other version, it fails. I tried to fix it, but it didn't work out\r\n\r\nModel downloaded at:  \/root\/model\/gpt4all\/orca-mini-3b.ggmlv3.q4_0.bin\r\nInvalid model file\r\nTraceback (most recent call last):\r\n  File \"\/root\/test.py\", line 8, in <module>\r\n    model = GPT4All(\"orca-mini-3b.ggmlv3.q4_0.bin\")\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/gpt4all\/gpt4all.py\", line 90, in __init__\r\n    self.model.load_model(self.config[\"path\"])\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/gpt4all\/pyllmodel.py\", line 192, in load_model\r\n    raise ValueError(\"Unable to instantiate model\")\r\nValueError: Unable to instantiate model\r\n","labels":["bug","backend"],"created_at":"2023-09-03T14:59:17Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1394"},{"issue_number":37,"repository":"nomic-ai\/gpt4all","title":"Application crashes on startup if model dir is in Windows network drive","description":"### System Info\n\nI had changed the target dir for LLM's to a network share drive. Since, gpt4all rejects the setting, exits. Upon opening, rejects, exits. Uninstall\/reinstall, no effect. Can't find anywhere to mod any config to change it locally. Attempted building from source but QT whines about compiler identities when everything in the env is intact. Frustrating, really want to get gpt4all working again.\r\n\r\nSeen code updates that look like they address this bug. Can we get an updated Windows app release? Thanks!\n\n### Information\n\n- [X] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Related Components\n\n- [ ] backend\n- [ ] bindings\n- [ ] python-bindings\n- [X] chat-ui\n- [ ] models\n- [ ] circleci\n- [ ] docker\n- [ ] api\n\n### Reproduction\n\nExactly as above.\n\n### Expected behavior\n\nNot exiting immediately and let me, the user, mod the configs\/prefs? Seems reasonable.","labels":["bug","chat"],"created_at":"2023-08-22T14:05:41Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1364"},{"issue_number":38,"repository":"nomic-ai\/gpt4all","title":"Mac Ventura Chat UI opens but shows blank window","description":"### System Info\n\nGPT4all 2.4.14\r\n![Screenshot 2023-08-22 at 7 04 03 AM](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/11655138\/8a181516-bdb6-4e3d-8b4a-c937537f1523)\r\nMacbook Air 2014, Inteal HD Graphics 5000, Ventura 13.2.1\r\n![Screenshot 2023-08-22 at 7 05 30 AM](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/11655138\/20ddc2d8-704d-4719-9f19-3f3e7268cd9b)\r\n\n\n### Information\n\n- [X] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Related Components\n\n- [ ] backend\n- [ ] bindings\n- [ ] python-bindings\n- [X] chat-ui\n- [ ] models\n- [ ] circleci\n- [ ] docker\n- [ ] api\n\n### Reproduction\n\nDownload darwin installer from website\r\nInstall \r\nOpen gpt4all\n\n### Expected behavior\n\nUI is displayed, but it's not","labels":["bug","chat"],"created_at":"2023-08-22T05:09:38Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1363"},{"issue_number":39,"repository":"nomic-ai\/gpt4all","title":"LocalDocs Plugin no longer working ","description":"### System Info\n\nGPT4all version v2.4.14\r\n\r\nThe localdocs plugin is no longer processing or analyzing my pdf files which I place in the referenced folder. I've tried creating new folders and adding them to the folder path, I've reused previously working folders, and I've reinstalled GPT4all a couple times. I've tried this on two pc's (win10 and win11) and with 4-5 models (Falcon, Wizard 1.1 and 1.2, LLama7B and LLama13B). I've also tried different advanced settings. \r\n\r\nPreviously, the LocalDocs plugin worked fine. after entering a prompt, gpt4all used to show \"generating response\"...\"processing localdocsfolder\"...\"processing\". now it skips the middle step, and I don't get any references or accurate responses based on the provided documents. \n\n### Information\n\n- [ ] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Related Components\n\n- [ ] backend\n- [ ] bindings\n- [ ] python-bindings\n- [ ] chat-ui\n- [ ] models\n- [ ] circleci\n- [ ] docker\n- [ ] api\n\n### Reproduction\n\nsteps to reproduce the behaviour:\r\n\r\n1. create a folder in winexplorer and add to it files (pdf, word) with information\r\n2. reference this localdocs folder in settings\r\n3. ask a specific question of which you know the answer should be clearly formulated in the attached files.\n\n### Expected behavior\n\na concise answer from the LLM with accurate reference to the local files.","labels":["bug","chat"],"created_at":"2023-08-21T15:18:53Z","comments":16,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1362"},{"issue_number":40,"repository":"nomic-ai\/gpt4all","title":"Issue: <LocalDocs Beta Plugin's reference context cannot be click and show again in specific condition >","description":"### Issue you'd like to raise.\r\n\r\nI have try LocalDocs Beta Plugin by some PDF file. After generate the content. It is will show 3 reference source with which page of PDF file and  left the ward  \u201ccontext\u201d at each end. Click on the ward \u201ccontext\u201d will show what sentence and wards have been found in PDF file for the reply.\r\n\r\n\r\n![\u87a2\u5e55\u64f7\u53d6\u756b\u9762 2023-08-08 211819](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/141255885\/cea157a2-86a7-482f-9d5c-d7aa564bcfb6)\r\n\r\nHowever after generate series of content, when I want to look back the previous content by scroll up for a few column. I find the function of clicking on the ward \u201ccontext\u201d cannot work again in some early reply. It cannot show the sentence and wards it has found in PDF file again.\r\n\r\n\r\n\r\n### Suggestion:\r\n\r\nPlease fix the issue in LocalDocs Beta Plugin. \r\n\r\nBeside the bug, I suggest to add the function of forcing LocalDocs Beta Plugin to find the content in PDF file. Not just passively check if the prompt is related to the content in PDF file.\r\n\r\nThe function of copy the whole conversation is not include the content of 3 reference source generated by LocalDocs Beta Plugin. It is better to add it in the future.","labels":["bug","enhancement","chat"],"created_at":"2023-08-08T13:48:40Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1325"},{"issue_number":41,"repository":"nomic-ai\/gpt4all","title":"pydantic.error_wrappers.ValidationError: 3 validation errors for ChatCompletionResponse","description":"### System Info\n\nPlatform : Ubuntu\r\nPython 3.10\n\n### Information\n\n- [X] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Related Components\n\n- [ ] backend\n- [ ] bindings\n- [ ] python-bindings\n- [ ] chat-ui\n- [ ] models\n- [ ] circleci\n- [ ] docker\n- [X] api\n\n### Reproduction\n\nparameters body to `\/v1\/chat\/completions` endpoint\r\n```\r\n{\r\n  \"model\": \"ggml-llama-2-7b-chat-q4_0.bin\",\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"what is AI?\"\r\n    }\r\n  ]\r\n}\r\n```\r\nThrow following error\r\n```\r\ngpt4all_api    | INFO:     172.20.0.1:47342 - \"POST \/v1\/chat\/completions HTTP\/1.1\" 500 Internal Server Error\r\ngpt4all_api    | ERROR:    Exception in ASGI application\r\ngpt4all_api    | Traceback (most recent call last):\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/uvicorn\/protocols\/http\/httptools_impl.py\", line 436, in run_asgi\r\ngpt4all_api    |     result = await app(  # type: ignore[func-returns-value]\r\ngpt4all_api    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/uvicorn\/middleware\/proxy_headers.py\", line 78, in __call__\r\ngpt4all_api    |     return await self.app(scope, receive, send)\r\ngpt4all_api    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/fastapi\/applications.py\", line 289, in __call__\r\ngpt4all_api    |     await super().__call__(scope, receive, send)\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/starlette\/applications.py\", line 122, in __call__\r\ngpt4all_api    |     await self.middleware_stack(scope, receive, send)\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/starlette\/middleware\/errors.py\", line 184, in __call__\r\ngpt4all_api    |     raise exc\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/starlette\/middleware\/errors.py\", line 162, in __call__\r\ngpt4all_api    |     await self.app(scope, receive, _send)\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/starlette\/middleware\/cors.py\", line 91, in __call__\r\ngpt4all_api    |     await self.simple_response(scope, receive, send, request_headers=headers)\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/starlette\/middleware\/cors.py\", line 146, in simple_response\r\ngpt4all_api    |     await self.app(scope, receive, send)\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/starlette\/middleware\/exceptions.py\", line 79, in __call__\r\ngpt4all_api    |     raise exc\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/starlette\/middleware\/exceptions.py\", line 68, in __call__\r\ngpt4all_api    |     await self.app(scope, receive, sender)\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/fastapi\/middleware\/asyncexitstack.py\", line 20, in __call__\r\ngpt4all_api    |     raise e\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/fastapi\/middleware\/asyncexitstack.py\", line 17, in __call__\r\ngpt4all_api    |     await self.app(scope, receive, send)\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/starlette\/routing.py\", line 718, in __call__\r\ngpt4all_api    |     await route.handle(scope, receive, send)\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/starlette\/routing.py\", line 276, in handle\r\ngpt4all_api    |     await self.app(scope, receive, send)\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/starlette\/routing.py\", line 66, in app\r\ngpt4all_api    |     response = await func(request)\r\ngpt4all_api    |                ^^^^^^^^^^^^^^^^^^^\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/fastapi\/routing.py\", line 273, in app\r\ngpt4all_api    |     raw_response = await run_endpoint_function(\r\ngpt4all_api    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ngpt4all_api    |   File \"\/usr\/local\/lib\/python3.11\/site-packages\/fastapi\/routing.py\", line 190, in run_endpoint_function\r\ngpt4all_api    |     return await dependant.call(**values)\r\ngpt4all_api    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ngpt4all_api    |   File \"\/app\/api_v1\/routes\/chat.py\", line 55, in chat_completion\r\ngpt4all_api    |     return ChatCompletionResponse(\r\ngpt4all_api    |            ^^^^^^^^^^^^^^^^^^^^^^^\r\ngpt4all_api    |   File \"pydantic\/main.py\", line 341, in pydantic.main.BaseModel.__init__\r\ngpt4all_api    | pydantic.error_wrappers.ValidationError: 3 validation errors for ChatCompletionResponse\r\ngpt4all_api    | choices -> 0 -> message\r\ngpt4all_api    |   field required (type=value_error.missing)\r\ngpt4all_api    | choices -> 0 -> index\r\ngpt4all_api    |   field required (type=value_error.missing)\r\ngpt4all_api    | choices -> 0 -> finish_reason\r\ngpt4all_api    |   field required (type=value_error.missing)\r\n```\r\n\r\nwhere as following parameter to `\/v1\/completions\/` work with very poor results. \r\n```\r\n{\r\n  \"model\": \"ggml-llama-2-7b-chat-q4_0.bin\",\r\n  \"prompt\": [\r\n    \"What is AI\"\r\n  ],\r\n  \"max_tokens\": 100,\r\n  \"temperature\": 0.18,\r\n  \"top_p\": 1,\r\n  \"top_k\": 50,\r\n  \"n\": 1,\r\n  \"stream\": false,\r\n  \"repeat_penalty\": 1.18\r\n}\r\n```\n\n### Expected behavior\n\nExpecting it not give error","labels":["bug","chat"],"created_at":"2023-08-05T17:45:41Z","comments":5,"reactions":6,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1317"},{"issue_number":42,"repository":"nomic-ai\/gpt4all","title":"Issue: Broken AVX512 instructions of Xeon Phi KNL 7200 Series Processors, assumes support for ALL AVX512 instructions, and breaks during compilation.","description":"### Issue you'd like to raise.\r\n\r\nRuns extremely slow on Xeon Phi 7210 and 7295 CPUs. Doesn't appear to take advantage of the AVX512 instruction support specific to the Knights Landing architecture, and breaks when building with those enabled with -native and -m64 compiler flags.\r\n\r\nThis breaks the compile, as it's assumed that these processors support the \"other\" AVX512 instructions.\r\n\r\nA really good white-paper for this is here: https:\/\/colfaxresearch.com\/knl-avx512\/\r\n\r\n\"Features common to Knights Landing and future Intel Xeon processors include AVX512-F and\r\nAVX512-CD. If it is desirable to maintain binary compatibility between the Knights Landing pro\u0002cessor and a future Intel Xeon processor, these are the subsets to use.\r\n\u2022 All AVX-512 features of the Knights Landing processors include, in addition to AVX512-F and\r\nAVX512-CD, the AVX512-ER, and AVX512-PF subsets. If it is not necessary to maintain binary\r\ncompatibility between the Knights Landing processor and a future Intel Xeon processor, then the\r\nadditional AVX512-ER and AVX512-PF instructions can be leveraged to vectorize previously un\u0002vectorizable code.\r\n\u2022 All AVX-512 features of the future Intel Xeon processors include, in addition to AVX512-F and\r\nAVX512-CD, the AVX512-BW, AVX512-DQ, AVX512-VL, AVX512-IFMA, and AVX512-VBMI\r\nsubsets. Since these instructions are not available on the Knights Landing processors, they are out\r\nof the scope of this document\" -- Colfax Research\r\n\r\nIf we can remove the non-Xeon Phi KNL AVX512 instructions from the GPT4all source, I believe it should compile properly - these changes should help:\r\n\r\n**Ensure only THESE are present:**\r\n`AVX512-F , AVX512-CD, AVX512-ER, and AVX512-PF`\r\n\r\n**Remove these:**\r\n`AVX512-BW, AVX512-DQ, AVX512-VL, AVX512-IFMA, and AVX512-VBMI`\r\n\r\nIf someone wanted to build a really fast and cheap system for running an LLM, getting this fixed work do that, as 7210s are available for around $500-600 USD on eBay all the time.\r\n\r\n### Suggestion:\r\n\r\nCould you please suggest a better way to compile the chat binary so that it will use all 288 cores correctly without the present slowness?\r\n\r\nIn Linux this is running extremely slow, even when compiled from source, I've only been able to get a maximum of 10 tickets\/sec which is initially that rate, then drags down to less that 0.2 tickets per second.","labels":["bug","backend"],"created_at":"2023-08-01T20:05:49Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1305"},{"issue_number":43,"repository":"nomic-ai\/gpt4all","title":"ThumbsDown feedback window width too small. ","description":"### System Info\n\nGTP4All v2.4.13  (Linux)\r\nGPT4All v2.4.13  (Windows)\r\n\r\n\r\n\r\nUsing Chat GUI,  when I want to provide some feedback to the answer,  I press \"Thumbs Down\" icon.\r\nThen small feedback window is open.  But the width of the input area width is too small.  \r\nIt is only 15 characters.  \r\nEven thouth I make main chat GUI window width to wider,  still the \"Thums Down\" feedback window \r\nwidth is always fixed width.  \r\n![small-window](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/107643\/a2846605-b10e-420e-9267-ef8acc1b302a)\r\n\r\nThis symptom started some versions ago.  \r\nThis symptom can be reproduce on Linux and windows vresion of Chat GUI.   (I don't have Mac.)\r\n\n\n### Information\n\n- [ ] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Related Components\n\n- [ ] backend\n- [ ] bindings\n- [ ] python-bindings\n- [X] chat-ui\n- [ ] models\n- [ ] circleci\n- [ ] docker\n- [ ] api\n\n### Reproduction\n\n1. Open a Chat GUI \r\n2. Post a question,  and wait for the answer.\r\n3. Press \"ThumbsDown\" icon.    \r\n4. See popup feedback widow size.\n\n### Expected behavior\n\nThe feedback window input area width should be adjustable in configuration or  automatically resize according to the main chat window width.   Short term solution would be enlarge the  input area width in fixed size.  \r\n","labels":["bug","chat"],"created_at":"2023-07-23T05:43:02Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1250"},{"issue_number":44,"repository":"nomic-ai\/gpt4all","title":"Model download mechanism needs some improvement. Specific suggestions","description":"### System Info\r\n\r\n* GPT4All version: 2.4.13\r\n* OS: Linux Mint  21.1 Cinnamon. Base: Ubuntu 22.04\r\n* Python 3.9.17, installed via PyEnv\r\n\r\n\r\n### Information\r\n\r\n- [ ] The official example notebooks\/scripts\r\n- [ ] My own modified scripts\r\n\r\n### Related Components\r\n\r\n- [ ] backend\r\n- [ ] bindings\r\n- [ ] python-bindings\r\n- [ ] chat-ui\r\n- [X] models\r\n- [ ] circleci\r\n- [ ] docker\r\n- [ ] api\r\n\r\n### Reproduction\r\n\r\n1. Open the desktop app for GPT4All.\r\n2. Click the hamburger menu icon on the top left.\r\n3. Click the \"downloads\" button.\r\n4. Click the \"install\" button for the \"Wizard v1.1\" model.   \r\n  \r\nI have already seen [this](https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1171) issue.  \r\n  \r\n# There are a few suggestions I have:\r\n1. As shown in the screenshot below, please display a message to users about alternative hyperlinks they can use to download the models. Perhaps even via torrents, since the models are quite huge.\r\n2. When the model starts downloading, the temporary download file isn't visible in the folder selected. It might help to ensure that the partially downloaded file is available, so that the user can resume the download even if it gets interrupted.\r\n3. The download UI needs to have an option to pause the download and resume the download if it gets interrupted.\r\n4. If it is possible to download other models from HuggingFace and use them with GPT4All, it helps to mention it on the UI, and provide the users more information on which models they can use. [Oogabooga](https:\/\/github.com\/oobabooga\/text-generation-webui) allows such functionality.\r\n5. If it's difficult to ensure an uninterrupted download from gpt4all's server, you could make models available on other sites like HuggingFace. Providing the links would help users download from there. \r\n6. If there are users who have a GPU and a lot more RAM, they can download and use much larger models. It helps to provide download access to such models. \r\n7. It would help to know what the terms like \"Quantization: q4_0\" are. I know what quantization is, but \"q4_0\" needs explanation.\r\n8. In the download progress, it'd help to show how many GB got downloaded. As of now, it only shows a green progress bar, but no number indicating how many GB got downloaded.\r\n\r\n\r\n### Expected behavior\r\n\r\nThe model should have downloaded fully. But I tried thrice, and the download fails. Sometimes it fails early, sometimes it downloads only half the file and fails. Sometimes it downloads 3\/4th of the file and fails. Screenshot attached.  \r\n![downloadError](https:\/\/github.com\/nomic-ai\/gpt4all\/assets\/2093933\/6d42bc0d-e171-44ab-9c77-db705efa7551)\r\n  \r\nThis appears to be a problem with the gpt4all server, because even when I went to GPT4All's website and tried downloading the model using Google Chrome browser, the download started and then failed after a while. I tried in Firefox, and the same problem happened. Surprisingly, even the Firefox extension DownloadThemAll experienced a network error in between and the download could not be resumed. Finally, Turbo Download Manager downloaded it in one shot. I'm sure this isn't a problem with my ISP, since I've downloaded larger models from other websites without the help of extensions like DownloadThemAll. I was also able to use GPT4All's desktop interface to download the GPT4All Falcon model. ","labels":["bug","enhancement","chat"],"created_at":"2023-07-19T08:58:40Z","comments":1,"reactions":4,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1228"},{"issue_number":45,"repository":"nomic-ai\/gpt4all","title":"LocalDocs treats newlines as spaces","description":"### System Info\r\n\r\nGPT4All v2.4.13, Debian 11\r\n\r\n### Information\r\n\r\n- [ ] The official example notebooks\/scripts\r\n- [ ] My own modified scripts\r\n\r\n### Related Components\r\n\r\n- [ ] backend\r\n- [ ] bindings\r\n- [ ] python-bindings\r\n- [X] chat-ui\r\n- [ ] models\r\n- [ ] circleci\r\n- [ ] docker\r\n- [ ] api\r\n\r\n### Reproduction\r\n\r\n1. Create a LocalDocs collection with newline-sensitive code (for example, python)\r\n2. Prompt the model (with LocalDocs) about the code\r\n3. Watch as it treats newlines as spaces\r\n\r\n### Expected behavior\r\n\r\nThe database should contain newlines\r\n\r\n### Example\r\n\r\n`bot = mineflayer.createBot({'host': 'localhost', 'username': 'bot'}) @On(bot, 'spawn') def handle(*args): print(\"I spawned\") @On(bot, 'chat')`","labels":["bug","chat"],"created_at":"2023-07-18T13:27:49Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1223"},{"issue_number":46,"repository":"nomic-ai\/gpt4all","title":"ChatGPT (GPT3.5 and GPT4) losing context after first answer, make it unsable","description":"### System Info\n\n**GPT4All version: v2.4.13, MacOS Ventura (13.0.1)**\r\n\r\nHello,\r\n\r\nChatGPT is losing context after the first answer it's provide. Exemple with GPT4\r\n\r\n```\r\nPrompt: Give me quick todo list to become a better person\r\n\r\nResponse: 1. Practice Gratitude: Start each day by acknowledging something you're grateful for. This can help you maintain a positive outlook on life.\r\n\r\n2. Exercise Regularly: Physical activity is not only good for your body, but also for your mind. It can help reduce stress and improve your mood.\r\n\r\n3. Eat Healthy: A balanced diet can improve your physical health and boost your energy levels.\r\n\r\n4. Read More: Reading can expand your knowledge and improve your understanding of the world around you.\r\n\r\n5. Practice Kindness: Be kind to others. Small acts of kindness can make a big difference in someone's day.\r\n\r\n6. Learn Something New: Whether it's a new language, a new skill, or a new hobby, learning something new can stimulate your mind and keep you engaged.\r\n\r\n7. Meditate: Meditation can help you stay calm and focused. It's a great way to reduce stress and improve your mental health.\r\n\r\n8. Set Goals: Setting goals can give you a sense of purpose and direction. It can also motivate you to work harder and achieve more.\r\n\r\n9. Spend Time with Loved Ones: Spending quality time with your family and friends can strengthen your relationships and make you feel loved and appreciated.\r\n\r\n10. Volunteer: Volunteering can give you a sense of fulfillment and purpose. It's also a great way to give back to your community.\r\n\r\n11. Limit Screen Time: Spending too much time in front of screens can strain your eyes and affect your sleep. Try to limit your screen time, especially before bed.\r\n\r\n12. Practice Self-Care: Take time for yourself. Whether it's taking a bath, reading a book, or going for a walk, self-care is important for your mental and physical health.\r\n\r\nRemember, becoming a better person is a journey, not a destination. It's about making small, consistent changes that add up over time.\r\n\r\n\r\nPrompt: Give me more details about point 2\r\n\r\nResponse: I'm sorry, but I can't provide the information you're looking for without more context. Could you please provide more details or clarify what you're referring to with \"point 2\"?\r\n```\r\n\r\nCan you please check why it's happening?\r\n\r\nThanks\n\n### Information\n\n- [ ] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Related Components\n\n- [ ] backend\n- [ ] bindings\n- [ ] python-bindings\n- [X] chat-ui\n- [X] models\n- [ ] circleci\n- [ ] docker\n- [ ] api\n\n### Reproduction\n\nSteps to reproduce the behavior\r\n\r\n1 - Use ChatGPT3.5 or ChatGPT4 \r\n2 - Ask a question to the model\r\n3 - Ask a question about the response of the model\r\n4 - ChatGPT cannot answer, it has lost the context\r\n\n\n### Expected behavior\n\nI expect the model to not lose the context","labels":["bug","chat"],"created_at":"2023-07-16T09:02:56Z","comments":0,"reactions":1,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1214"},{"issue_number":47,"repository":"nomic-ai\/gpt4all","title":"Hangs (permanent spinning circle) when pasting 88 lines of code","description":"### System Info\r\n\r\n2.4.13, win10\r\nHangs when pasting 88 lines of code, every time no matter what model i choose, whether its snoozy or snoozy_superhot_8K\r\nGetting permanent, spinning circle and when trying to close the app it crashes AND\r\ndoesnt release memory (RAM). so instead of 12Gb its only 7GB or less.\r\nCPU usage shows only 8% - 10% which indicates it doesnt do anything.\r\n\r\n### Information\r\n\r\n- [ ] The official example notebooks\/scripts\r\n- [ ] My own modified scripts\r\n\r\n### Related Components\r\n\r\n- [ ] backend\r\n- [ ] bindings\r\n- [ ] python-bindings\r\n- [ ] chat-ui\r\n- [ ] models\r\n- [ ] circleci\r\n- [ ] docker\r\n- [ ] api\r\n\r\n### Reproduction\r\n\r\n1. paste around 88 lines of code\r\n2. watch the circle spin\r\n3. close tha app and it crashes without releasing RAM\r\n\r\n### Expected behavior\r\n\r\nWhen pasted a model should analyze what i pasted and print the analysis.","labels":["bug","chat"],"created_at":"2023-07-16T03:45:58Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1211"},{"issue_number":48,"repository":"nomic-ai\/gpt4all","title":"Changing model download path doesn't clear old models from dropdown list","description":"### System Info\r\n\r\nChanging model download path doesn't clear old models from dropdown list\r\n\r\n### Information\r\n\r\n- [ ] The official example notebooks\/scripts\r\n- [ ] My own modified scripts\r\n\r\n### Related Components\r\n\r\n- [ ] backend\r\n- [ ] bindings\r\n- [ ] python-bindings\r\n- [X] chat-ui\r\n- [ ] models\r\n- [ ] circleci\r\n- [ ] docker\r\n- [ ] api\r\n\r\n### Reproduction\r\nchange model path, we get a mix of both old and new paths in dropdown list\r\n\r\n### Expected behavior\r\ndropdown list should only show models that would still be there if I restart the app\r\n\r\n","labels":["bug","chat"],"created_at":"2023-07-06T22:32:28Z","comments":2,"reactions":1,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1148"},{"issue_number":49,"repository":"nomic-ai\/gpt4all","title":"Error message: The bound address is already in use","description":"### System Info\n\nOS: Ubuntu Linux 22.04\r\nGPT4All version: 2.4.8\r\n\n\n### Information\n\n- [X] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Related Components\n\n- [X] backend\n- [ ] bindings\n- [ ] python-bindings\n- [X] chat-ui\n- [ ] models\n- [ ] circleci\n- [ ] docker\n- [ ] api\n\n### Reproduction\n\nI use GPT4All on a Linux system without internet connection. When I start GPT4All I get the following output messages;\r\n\r\n> [Critical] (Sun Jun 25 ... 2023): listen failed: The bound address is already in use\r\n> [Warning] (Sun Jun 25 ... 2023): ERROR: Unable to start the server\r\n\r\nBut I don't want to start a server. There is the following line in the file `$HOME\/.config\/nomic.ai\/GPT4All.ini `:\r\n\r\n> serverChat=false\r\n\r\nHow can I disable the server or avoid the error message?\n\n### Expected behavior\n\nNo error message about a bound address or about a server.\r\n","labels":["bug","chat"],"created_at":"2023-06-25T13:55:17Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/1058"},{"issue_number":50,"repository":"nomic-ai\/gpt4all","title":"Incompatibility between GPT4All and Vosk speech recognizer.","description":"### System Info\r\n\r\nI am testing GPT4All in a voice chat application. I am using the Vosk speech recognition package to convert speech to text. However, if I import gpt4all, and after that vosk:\r\n\r\n```\r\nimport gpt4all\r\nfrom vosk_recognizer import SpeechRecognize\r\n```\r\n\r\nmy vosk_recognizer module in turn imports:\r\n\r\n```\r\nimport vosk\r\n```\r\n\r\nI get this error:\r\n\r\n```\r\nFile \"C:\\Users\\karen\\myprojects\\simple-chat\\ChatGPT.py\", line 21, in <module>\r\n    from vosk_recognizer import SpeechRecognize\r\n  File \"C:\\Users\\karen\\myprojects\\simple-chat\\vosk_recognizer.py\", line 7, in <module>\r\n    import vosk\r\n  File \"C:\\Users\\karen\\myprojects\\simple-chat\\chat_env\\lib\\site-packages\\vosk\\__init__.py\", line 36, in <module>\r\n    _c = open_dll()\r\n  File \"C:\\Users\\karen\\myprojects\\simple-chat\\chat_env\\lib\\site-packages\\vosk\\__init__.py\", line 28, in open_dll\r\n    return _ffi.dlopen(os.path.join(dlldir, \"libvosk.dll\"))\r\nOSError: cannot load library 'C:\\Users\\karen\\myprojects\\simple-chat\\chat_env\\lib\\site-packages\\vosk\\libvosk.dll': error 0x7f  \r\n```\r\n\r\nIf I reverse the order of imports (import vosk and then gpt4all), I get an error importing gpt4all:\r\n\r\n```\r\nFile \"C:\\Users\\karen\\myprojects\\simple-chat\\ChatGPT.py\", line 24, in <module>\r\n    import gpt4all\r\n  File \"C:\\Users\\karen\\myprojects\\simple-chat\\chat_env\\lib\\site-packages\\gpt4all\\__init__.py\", line 1, in <module>\r\n    from .pyllmodel import LLModel # noqa\r\n  File \"C:\\Users\\karen\\myprojects\\simple-chat\\chat_env\\lib\\site-packages\\gpt4all\\pyllmodel.py\", line 49, in <module>\r\n    llmodel = load_llmodel_library()\r\n  File \"C:\\Users\\karen\\myprojects\\simple-chat\\chat_env\\lib\\site-packages\\gpt4all\\pyllmodel.py\", line 45, in load_llmodel_library    llmodel_lib = ctypes.CDLL(llmodel_dir)\r\n  File \"C:\\Users\\karen\\Anaconda3\\lib\\ctypes\\__init__.py\", line 381, in __init__\r\n    self._handle = _dlopen(self._name, mode)\r\nOSError: [WinError 127] The specified procedure could not be found\r\n```\r\n\r\nName space conflict? This is a show stopper to implement gpt4all in this project. :(\r\n\r\n\r\n### Information\r\n\r\n- [ ] The official example notebooks\/scripts\r\n- [X] My own modified scripts\r\n\r\n### Related Components\r\n\r\n- [ ] backend\r\n- [ ] bindings\r\n- [x] python-bindings\r\n- [ ] chat-ui\r\n- [ ] models\r\n- [ ] circleci\r\n- [ ] docker\r\n- [ ] api\r\n\r\n### Reproduction\r\n\r\n1) import gpt4all\r\n2) import vosk\r\n\r\nOr vice versa to produce second error reported above.\r\n\r\n### Expected behavior\r\n\r\nNo conflict. Happy family. ;)","labels":["bug","backend"],"created_at":"2023-06-12T21:51:29Z","comments":14,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/962"},{"issue_number":51,"repository":"nomic-ai\/gpt4all","title":"Installer attempted to copy the DesktopLauncher to a wrong path","description":"### System Info\n\nGPT4All 2.4.6, Ubuntu LTS with Gnome, french locale\n\n### Information\n\n- [X] The official example notebooks\/scripts\n- [ ] My own modified scripts\n\n### Related Components\n\n- [ ] backend\n- [ ] bindings\n- [ ] python-bindings\n- [X] chat-ui\n- [ ] models\n- [ ] circleci\n- [ ] docker\n- [ ] api\n\n### Reproduction\n\nInstall GPT4 on a french Desktop.\r\n\r\nInstaller attempted to copy the DesktopLauncher to a wrong path:\r\n \/home\/dev\/Desktop. Such a folder doesn't exist and the installer runs into an error.\r\n \r\n https:\/\/discord.com\/channels\/1076964370942267462\/1090651074358149120\/1115391418819690557  \n\n### Expected behavior\n\nInstall the Programm without errors.","labels":["bug","chat","installer"],"created_at":"2023-06-06T11:44:49Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/873"},{"issue_number":52,"repository":"nomic-ai\/gpt4all","title":"Window resize bug","description":"### System Info\r\n\r\ngpt4all 2.4.4, win10\r\n\r\nWhen you resize the window and then you try to press a button or menu and its slightly off the position but definitely not where the symbol of menu or button is.\r\n\r\n### Information\r\n\r\n- [ ] The official example notebooks\/scripts\r\n- [ ] My own modified scripts\r\n\r\n### Related Components\r\n\r\n- [ ] backend\r\n- [ ] bindings\r\n- [ ] python-bindings\r\n- [ ] chat-ui\r\n- [ ] models\r\n- [ ] circleci\r\n- [ ] docker\r\n- [ ] api\r\n\r\n### Reproduction\r\n\r\nSteps to reproduce:\r\n\r\n1) resize window\r\n2) click a button or menu\r\n\r\n### Expected behavior\r\n\r\nButton should be where the button is.","labels":["bug","chat"],"created_at":"2023-06-01T17:18:23Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/nomic-ai\/gpt4all\/issues\/795"}]