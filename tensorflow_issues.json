[{"repository":"tensorflow\/tensorflow","title":"`gradient_checker.compute_gradient` can cause a crash","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20241025\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `Floating point exception` issue in TensorFlow when I used API `gradient_checker.compute_gradient`. I have confirmed that below code would crash on tf-nightly 2.19.0-dev20241025 (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1Ow6DQI7g-s7LpKUM1fy8OXbNcfw1ZH7r?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport math\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import nn_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.python.util.compat import collections_abc\nfrom tensorflow.python.eager import context\ndef DtypesToTest(use_gpu):\n  # double datatype is currently not supported for convolution ops\n  # on the ROCm platform\n  optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]\n  if use_gpu:\n    if not test_util.GpuSupportsHalfMatMulAndConv():\n      return optional_float64 + [dtypes.float32]\n    else:\n      # It is important that float32 comes before float16 here,\n      # as we will be using its gradients as reference for fp16 gradients.\n      return optional_float64 + [dtypes.float32, dtypes.float16]\n  else:\n    return optional_float64 + [dtypes.float32, dtypes.float16, dtypes.bfloat16]\ndef _ConstructAndTestGradientForConfig(\n    batch, input_shape, filter_shape, in_depth, out_depth, stride,\n    padding, test_input, data_format, use_gpu):\n  input_planes, input_rows, input_cols = input_shape\n  filter_planes, filter_rows, filter_cols = filter_shape\n  input_shape = [batch, input_planes, input_rows, input_cols, in_depth]\n  filter_shape = [\n      filter_planes, filter_rows, filter_cols, in_depth, out_depth\n  ]\n  if isinstance(stride, collections_abc.Iterable):\n    strides = [1] + list(stride) + [1]\n  else:\n    strides = [1, stride, stride, stride, 1]\n  if padding == \"VALID\":\n    output_planes = int(\n        math.ceil((input_planes - filter_planes + 1.0) \/ strides[1]))\n    output_rows = int(\n        math.ceil((input_rows - filter_rows + 1.0) \/ strides[2]))\n    output_cols = int(\n        math.ceil((input_cols - filter_cols + 1.0) \/ strides[3]))\n  else:\n    output_planes = int(math.ceil(float(input_planes) \/ strides[1]))\n    output_rows = int(math.ceil(float(input_rows) \/ strides[2]))\n    output_cols = int(math.ceil(float(input_cols) \/ strides[3]))\n  output_shape = [batch, output_planes, output_rows, output_cols, out_depth]\n  input_size = 1\n  for x in input_shape:\n    input_size *= x\n  filter_size = 1\n  for x in filter_shape:\n    filter_size *= x\n  input_data = [x * 1.0 \/ input_size for x in range(0, input_size)]\n  filter_data = [x * 1.0 \/ filter_size for x in range(0, filter_size)]\n  for data_type in DtypesToTest(use_gpu=use_gpu):\n    # TODO(mjanusz): Modify gradient_checker to also provide max relative\n    # error and synchronize the tolerance levels between the tests for forward\n    # and backward computations.\n    if data_type == dtypes.float64:\n      tolerance = 1e-8\n    elif data_type == dtypes.float32:\n      tolerance = 5e-3\n    elif data_type == dtypes.float16:\n      tolerance = 5e-3 if test.is_built_with_rocm() else 1e-3\n    elif data_type == dtypes.bfloat16:\n      tolerance = 1e-2\n    sess = tf.compat.v1.Session()\n    with sess.as_default():\n      orig_input_tensor = constant_op.constant(\n          input_data, shape=input_shape, dtype=data_type, name=\"input\")\n      filter_tensor = constant_op.constant(\n          filter_data, shape=filter_shape, dtype=data_type, name=\"filter\")\n      if data_format == \"NCDHW\":\n        input_tensor = test_util.NHWCToNCHW(orig_input_tensor)\n        new_strides = test_util.NHWCToNCHW(strides)\n      else:\n        input_tensor = orig_input_tensor\n        new_strides = strides\n      conv = nn_ops.conv3d(\n          input_tensor,\n          filter_tensor,\n          new_strides,\n          padding,\n          data_format=data_format,\n          name=\"conv\")\n      jacob_t, jacob_n = gradient_checker.compute_gradient(\n          orig_input_tensor, input_shape, conv, output_shape)\n\nwith context.graph_mode():\n  _ConstructAndTestGradientForConfig(data_format=\"NDHWC\",use_gpu=False,batch=2, input_shape=(3, 7, 6), filter_shape=(3, 3, 3), in_depth=2, out_depth=0, stride=3, padding='VALID', test_input=True)\n```\n\n### Relevant log output\n\n```shell\nFatal Python error: Floating point exception\n```","labels":["type:bug","type:support"],"created_at":"2025-02-11T16:38:56Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/87063"},{"repository":"tensorflow\/tensorflow","title":"TPU nan issue","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux (google colab default)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIn the 2.18.0 Tensorflow version, occur only when using TPU:\n- At the middle of any epoch, loss turns out to be nan for the rest of the epoch \n\n### Standalone code to reproduce the issue\n\n```shell\nShortest way: connect and connect to the colab tutorial on TPU, i.e. https:\/\/colab.research.google.com\/github\/tensorflow\/docs\/blob\/master\/site\/en\/guide\/tpu.ipynb#scrollTo=Tce3stUlHN0L\nIf the issue hasn't been fixed, the training loop will produce nan loss\n```\n\n### Relevant log output\n\n```shell\nEpoch 1\/5\n300\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 22s 47ms\/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan\nEpoch 2\/5\n300\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 30ms\/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan\nEpoch 3\/5\n300\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 30ms\/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan\nEpoch 4\/5\n231\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 23ms\/step - loss: nan - sparse_categorical_accuracy: nan\n```","labels":["type:bug","comp:tpus","TF 2.18"],"created_at":"2025-02-10T15:03:57Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86953"},{"repository":"tensorflow\/tensorflow","title":"`tf.summary_ops.write` aborts with \"Check failed: 1 == NumElements() (1 vs. 4)\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20250207\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.write` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/13pz-hZNK_BaOnbYo_9CT2YMdoncPXEA3?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops import summary_ops_v2 as summary_ops\nfrom tensorflow.python.ops import variables\nwriter = summary_ops.create_file_writer_v2(\"\/tmp\")\nmystep = variables.Variable(1, dtype=dtypes.int64)\nwith writer.as_default(step=[3, 0, 0, 2]):\n    summary_ops.write('tag', 1.0)\n```\n\n### Relevant log output\n\n```shell\n2025-02-09 04:47:35.482562: F tensorflow\/core\/framework\/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor\nAborted (core dumped)\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-09T04:49:39Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86918"},{"repository":"tensorflow\/tensorflow","title":"`tf.summary_ops.run_metadata_graphs` aborts with \"Check failed: 1 == NumElements() (1 vs. 4)\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20250207\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.run_metadata_graphs` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1qCZb2sPj2l79IecGA1tz9CywtNlevAjf?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.ops import summary_ops_v2 as summary_ops\nfrom tensorflow.core.protobuf import config_pb2\nwriter = summary_ops.create_file_writer_v2(\"\/tmp\")\nmeta = config_pb2.RunMetadata()\nwith writer.as_default([3, 0, 0, 2]):\n    summary_ops.run_metadata_graphs(name='my_name', data=meta)\n```\n\n### Relevant log output\n\n```shell\n2025-02-09 04:39:36.666278: F tensorflow\/core\/framework\/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor\nAborted (core dumped)\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-09T04:42:49Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86917"},{"repository":"tensorflow\/tensorflow","title":"`io_ops.restore_v2` aborts with \"Check failed: size >= 0 (0 vs. -3) \"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.19.0-dev20250207\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `aborted issue` in TensorFlow when I used API `tf.io_ops.restore_v2` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20250207` (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/17XwNF4WI3HqVJkLwk2VQjcTDAV42-6Bl?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import io_ops\n\ndtype = dtypes.uint4\nwith ops.Graph().as_default():\n    op = io_ops.restore_v2('model', ['var1', 'var2'], ['', '-3 4 0,1:-'], [dtype, dtype])\n```\n\n### Relevant log output\n\n```shell\n2025-02-09 04:25:19.857968: F tensorflow\/core\/framework\/tensor_shape.cc:413] Check failed: size >= 0 (0 vs. -3) \nAborted (core dumped)\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-09T04:34:07Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86916"},{"repository":"tensorflow\/tensorflow","title":"Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMacos 15.3 (worker 0) Macos 12.7.6(worker 1)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.20\n\n### Bazel version\n\n...\n\n### GCC\/compiler version\n\n16.0.0 (apple M3) 14.0.0 (intel iris)\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nApple M3 and Intel Iris Graphics 6100\n\n### Current behavior?\n\nWhen I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  #Build the model under the strategy\n\nNo error message is displayed, but the process no longer progresses after\n\n\u2022\t I followed the recommendations of the official documentation, but the problem persists.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport json\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nTF_CONFIG = {\n    \"cluster\": {\n        \"worker\": [\"192.168.0.68:12345\", \"192.168.0.68:12346\"]\n    },\n    \"task\": {\"type\": \"worker\", \"index\": 0}  # Modifier index pour chaque worker\n}\n\nos.environ[\"TF_CONFIG\"] = json.dumps(TF_CONFIG)\n\n# Manually Load the MNIST dataset\ndata = np.load(\"mnist.npz\")\nx_train, y_train = data[\"x_train\"], data[\"y_train\"]\nx_test, y_test = data[\"x_test\"], data[\"y_test\"]\n\n# Normalize images\nx_train, x_test = x_train \/ 255.0, x_test \/ 255.0\n\n# Add a dimension to match TensorFlow's expectations\nx_train = x_train[..., np.newaxis]\nx_test = x_test[..., np.newaxis]\n\n# Define the distribution strategy\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\n\n# Build the model under the strategy\nwith strategy.scope():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n        tf.keras.layers.MaxPooling2D((2, 2)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f\"Test accuracy: {test_acc:.4f}\")\n```\n\n### Relevant log output\n\n```shell\n2025-02-08 12:29:20.630247: I metal_plugin\/src\/device\/metal_device.cc:1154] Metal device set to: Apple M3\n2025-02-08 12:29:20.630286: I metal_plugin\/src\/device\/metal_device.cc:296] systemMemory: 16.00 GB\n2025-02-08 12:29:20.630293: I metal_plugin\/src\/device\/metal_device.cc:313] maxCacheSize: 5.33 GB\n2025-02-08 12:29:20.630324: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2025-02-08 12:29:20.630338: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:269] Created TensorFlow device (\/job:localhost\/replica:0\/task:0\/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n2025-02-08 12:29:20.631249: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2025-02-08 12:29:20.631259: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:269] Created TensorFlow device (\/job:worker\/replica:0\/task:0\/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n2025-02-08 12:29:20.632347: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:449] Started server with target: grpc:\/\/192.168.0.68:12345\n2025-02-08 12:29:20.637550: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:535] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 14287335759644278642\n2025-02-08 12:29:20.637654: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service_agent.cc:298] Coordination agent has successfully connected.\n2025-02-08 12:29:37.728182: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:535] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 15227140312468372989\n```","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.13"],"created_at":"2025-02-08T11:40:33Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86897"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.Rsqrt``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 (google colab)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\nresult of ``tf.raw_ops.Rsqrt``` is inconsistent between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nx_0 = tf.constant([[[1.2031, 0.2168], [0.1177, 0.6758]], \n                    [[1.8203, 0.9688], [0.8828, 0.0767]]], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.Rsqrt(x=x_0)\n  print(\"Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.Rsqrt(x=x_0)\n  print(\"Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nOutput on CPU: tf.Tensor(\n[[[0.910156 2.14062]\n  [2.92188 1.21875]]\n\n [[0.742188 1.01562]\n  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)\n\nOutput on GPU: tf.Tensor(\n[[[0.914062 2.15625]\n  [2.90625 1.21875]]\n\n [[0.738281 1.01562]\n  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-05T06:12:29Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86607"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.Tan``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\n```tf.raw_ops.Tan``` produces inconsistent results between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nreal = tf.constant([1.5634333], dtype=tf.float32)\nimag = tf.constant([0.020735], dtype=tf.float32)\n\ncomplex_tensor = tf.complex(real, imag)\n\nwith tf.device('\/CPU:0'):\n    result_cpu = tf.raw_ops.Tan(x=complex_tensor)\n    print(result_cpu)\n\nwith tf.device('\/GPU:0'):\n    result_gpu = tf.raw_ops.Tan(x=complex_tensor)\n    print(result_gpu)\n\n##Comparing whole complex numbers\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_cons = tf.experimental.numpy.allclose(result_cpu, result_gpu, rtol=1e-6,  atol=1e-5)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:\", is_cons.numpy())\n\n##Comparing by parts\nreal_part_cpu = tf.math.real(result_cpu)\nreal_part_gpu = tf.math.real(result_gpu)\nreal_part_diff = tf.reduce_max(tf.abs(real_part_cpu - real_part_gpu)).numpy()\nreal_part_cons = tf.experimental.numpy.allclose(real_part_cpu, real_part_gpu, rtol=1e-6,  atol=1e-5)\n\nimag_part_cpu = tf.math.imag(result_cpu)\nimag_part_gpu = tf.math.imag(result_gpu)\nimag_part_diff = tf.reduce_max(tf.abs(imag_part_cpu - imag_part_gpu)).numpy()\nimag_part_cons = tf.experimental.numpy.allclose(imag_part_cpu, imag_part_gpu, rtol=1e-6,  atol=1e-5)\n\nprint(\"Real parts absolute difference:\", real_part_diff)\nprint(\"Real parts Consistency check with atol=1e-5 and rtol=1e-6:\", real_part_cons.numpy())\n\nprint(\"Imag parts absolute difference:\", imag_part_diff)\nprint(\"Imag parts Consistency check with atol=1e-5 and rtol=1e-6:\", imag_part_cons.numpy())\n```\n\n### Relevant log output\n\n```shell\ntf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64)\ntf.Tensor([15.205605+42.834225j], shape=(1,), dtype=complex64)\n\nMax absolute difference: 8.5064334e-05\nConsistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False\n\nReal parts absolute difference: 2.861023e-05\nReal parts Consistency check with atol=1e-5 and rtol=1e-6: False\n\nImag parts absolute difference: 8.010864e-05\nImag parts Consistency check with atol=1e-5 and rtol=1e-6: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-04T03:18:15Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86506"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.LogSoftmax``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 (google colab)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\nresult of ``tf.raw_ops.LogSoftmax``` is inconsistent between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nlogits = tf.constant([[0.0664, -2.3906]], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.LogSoftmax(logits=logits)\n  print(\"Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.LogSoftmax(logits=logits)\n  print(\"Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nOutput on CPU: tf.Tensor([[-0.0825195 -2.53125]], shape=(1, 2), dtype=bfloat16)\n\nOutput on GPU: tf.Tensor([[-0.0825195 -2.54688]], shape=(1, 2), dtype=bfloat16)\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-03T03:30:58Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86434"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.L2Loss``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 (google colab)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\nresult of ```tw.raw_ops.L2Loss``` is inconsistent between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nt = tf.constant([\n    [[0.9922, -1.4922], \n     [0.0376,  0.1504], \n     [0.6172,  1.2266]],\n\n    [[-0.1387,  1.3047], \n     [0.3535, -0.0471], \n     [0.0437,  0.2637]]\n], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.L2Loss(t=t)\n  print(\"Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.L2Loss(t=t)\n  print(\"Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nOutput on CPU: tf.Tensor(3.51562, shape=(), dtype=bfloat16)\n\nOutput on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16)\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-02T07:05:17Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86406"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.BiasAddGrad``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\n```tf.raw_ops.BiasAddGrad``` produces inconsistent results between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nout_backprop = tf.constant([\n    [\n        [\n            [[ 0.2207,  2.1094], [-0.3730, -1.0625], [ 1.7031,  0.7148]], \n            [[ 1.5078, -0.6719], [-0.6367,  0.5039], [-2.3281,  0.5078]]\n        ],\n        [\n            [[-0.3574,  0.0461], [ 2.3750, -2.9688], [-0.5703, -2.0156]],\n            [[ 0.8125,  1.7656], [-0.9570,  0.6250], [-0.6914, -0.4746]]\n        ],\n        [\n            [[-0.3750, -0.7383], [ 0.3691,  0.4570], [ 1.1641,  0.2715]],\n            [[-1.2969, -0.9844], [-0.4863,  1.0938], [-1.4297,  0.8086]]\n        ]\n    ],\n    [\n        [\n            [[ 0.3730,  0.8477], [-0.3887,  1.2266], [ 0.0859, -0.5742]],\n            [[-0.7383, -0.2432], [-0.7578, -0.8281], [-0.1660, -0.9336]]\n        ],\n        [\n            [[ 1.4297,  0.6797], [-1.6172,  0.4941], [-0.3047, -0.3711]],\n            [[-0.6250, -0.7617], [ 0.9453,  0.1064], [ 1.4062, -2.9531]]\n        ],\n        [\n            [[-1.4297, -0.1387], [ 0.0625,  1.0469], [-0.1953,  1.6406]],\n            [[-0.3047,  0.5117], [ 1.8125,  1.1797], [-0.8789, -0.4688]]\n        ]\n    ]\n], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=\"NCHW\")\n  print(\"BiasAddGrad Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=\"NCHW\")\n  print(\"BiasAddGrad Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nBiasAddGrad Output on CPU: tf.Tensor([0.09375 -3.96875 1.70312], shape=(3,), dtype=bfloat16)\n\nBiasAddGrad Output on GPU: tf.Tensor([0.078125 -4 1.70312], shape=(3,), dtype=bfloat16)\n\nMax absolute difference: 0.03125\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-01T10:43:06Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86378"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.BatchMatMulV2``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\ngetting inconsistent results of ```tf.raw_ops.BatchMatMulV2``` between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nx_0 = tf.constant([\n    [[[[ -1.3594, -0.3027], [-1.4141,  0.2969]],\n      [[ -0.9141,  1.7812], [ 1.2266,  0.8594]]],\n\n     [[[  0.8359, -0.9414], [-1.7969, -0.7461]],\n      [[  0.3164,  0.3691], [ 0.7656,  0.2354]]]],\n\n    [[[[ -0.5898,  1.3516], [ 0.4902, -0.1045]],\n      [[ -0.1099,  1.5078], [ 0.2852, -0.0957]]],\n\n     [[[-0.9883,  1.3203], [-0.2715, -1.7578]],\n      [[ -0.1602, -0.4336], [-0.6875, -0.4492]]]]\n], dtype=tf.bfloat16)\n\ny = tf.constant([\n    [[[  0.6836, -0.6562], [-0.5508, -0.8438]], \n     [[  1.6094, -0.9883], [-0.1318,  1.1094]]],\n\n    [[[  0.4062, -1.1094], [-0.7188, -1.7578]], \n     [[ -1.0391, -0.6602], [ 0.8359, -0.6562]]]\n], dtype=tf.bfloat16) \n\nwith tf.device('CPU:0'):\n    result_cpu = tf.raw_ops.BatchMatMulV2(\n        x=x_0, \n        y=y,\n    )\n    print(result_cpu)\n\nwith tf.device('GPU:0'):\n    result_gpu = tf.raw_ops.BatchMatMulV2(\n        x=x_0, \n        y=y,\n    )\n    print(result_gpu)\n\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\ntf.Tensor(\n[[[[[-0.761719 1.14062]\n    [-1.125 0.675781]]\n\n   [[-1.70312 2.875]\n    [1.85938 -0.257812]]]\n\n\n  [[[1.01562 0.726562]\n    [-0.193359 3.29688]]\n\n   [[-0.0201416 -0.449219]\n    [-0.597656 -0.65625]]]]\n\n\n\n [[[[-1.14062 -0.75]\n    [0.392578 -0.233398]]\n\n   [[-0.375 1.78125]\n    [0.470703 -0.386719]]]\n\n\n  [[[-1.34375 -1.21875]\n    [1.14844 3.39062]]\n\n   [[-0.195312 0.388672]\n    [0.337891 0.746094]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)\n\ntf.Tensor(\n[[[[[-0.761719 1.14844]\n    [-1.13281 0.675781]]\n\n   [[-1.70312 2.875]\n    [1.85938 -0.259766]]]\n\n\n  [[[1.01562 0.726562]\n    [-0.193359 3.3125]]\n\n   [[-0.0201416 -0.451172]\n    [-0.597656 -0.660156]]]]\n\n\n\n [[[[-1.14844 -0.753906]\n    [0.392578 -0.233398]]\n\n   [[-0.375 1.78125]\n    [0.470703 -0.388672]]]\n\n\n  [[[-1.35156 -1.22656]\n    [1.15625 3.39062]]\n\n   [[-0.196289 0.390625]\n    [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)\n\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-01T07:13:36Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86350"},{"repository":"tensorflow\/tensorflow","title":"Stateful LSTM bug with batch size","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nWindows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid. \nBasically the error is the same as this https:\/\/github.com\/tensorflow\/tensorflow\/issues\/64061\n\nBelow is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\n\n# Set a fixed batch size\nbatch_size = 32\n\n# Create some random training data\n# We'll have sequences of length 5, with 1 feature per time step\nsequence_length = 5\nnum_features = 1\nnum_samples = 100  # Total number of samples (must be divisible by batch_size)\n\n# Ensure num_samples is a multiple of batch_size\nnum_samples = (num_samples \/\/ batch_size) * batch_size\n\nX_train = np.random.rand(num_samples, sequence_length, num_features)\ny_train = np.random.rand(num_samples, 1)  # Example target values\n\n# Reshape y_train to match expected output shape if needed\ny_train = y_train.reshape(-1,1)\n\n# Create the stateful LSTM model\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.LSTM(units=64,  # Number of LSTM units\n                               batch_input_shape=(batch_size, sequence_length, num_features),\n                               stateful=True,\n                               return_sequences=False)) #often false for a final prediction\n\nmodel.add(tf.keras.layers.Dense(units=1)) # Output layer with 1 unit\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')\n\n# Train the model\nepochs = 10\n\nfor epoch in range(epochs):\n    # Shuffle data indices for each epoch (important for stateful LSTMs)\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    X_train = X_train[indices]\n    y_train = y_train[indices]\n\n    model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False) # Shuffle must be false\n\n    # Reset states after each epoch (essential for stateful LSTMs)\n    model.reset_states()\n```\n\n### Relevant log output\n\n```shell\n\n```","labels":["stat:awaiting response","type:bug","comp:keras","TF 2.18"],"created_at":"2025-01-31T18:07:07Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86310"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\ngetting inconsistent results of ```tf.image.adjust_hue``` between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nimages = tf.constant([\n    [[ 1.9720840,  2.1302242, -0.1902120],\n     [ 0.6557856, -1.3016001,  1.1452782]],\n    \n    [[-2.2193234,  0.3198028,  0.9568117],\n     [-0.3937407, -0.0503466, -0.3693791]]\n], dtype=tf.float32)\n\ndelta = tf.constant(-0.7441734, dtype=tf.float32)\n\nwith tf.device('CPU:0'):\n    adjusted_cpu = tf.image.adjust_hue(images, delta)\n    print(\"Adjusted Hue on CPU:\\n\", adjusted_cpu)\n\nwith tf.device('GPU:0'):\n    adjusted_gpu = tf.image.adjust_hue(images, delta)\n    print(\"Adjusted Hue on GPU:\\n\", adjusted_gpu)\n\n\nis_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e-5, rtol=1e-6)\n\nmax_abs_diff = tf.reduce_max(tf.abs(adjusted_cpu - adjusted_gpu)).numpy()\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nAdjusted Hue on CPU:\n tf.Tensor(\n[[[-0.190212    2.1302242   1.2092681 ]\n  [ 1.1452782  -0.48211157 -1.3016001 ]]\n\n [[ 0.11679006 -2.2193234   0.9568117 ]\n  [-0.3937407  -0.25841027 -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)\n\nAdjusted Hue on GPU:\n tf.Tensor(\n[[[-0.19021209  2.1302242   1.209268  ]\n  [ 1.1452781  -0.48211193 -1.3016001 ]]\n\n [[ 0.11678863 -2.2193234   0.95681167]\n  [-0.0503466  -0.0503466  -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)\n\nMax absolute difference: 0.3433941\nConsistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-01-31T07:40:34Z","comments":4,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86256"},{"repository":"tensorflow\/tensorflow","title":"Buffer allocation error in Tensorflow Lite with OpenCL backend on certain platforms","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nmacOS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI noticed a memory allocation error in clCreateBuffer. The issue seems to be caused by this:\n\n1) TFlite tries to alloca 0xa2000000 bytes of memory (value stored as size_t)\n\n2) The call ends up in this function (tensorflow\/lite\/experimental\/litert\/runtime\/opencl\/buffer.cc):\n\n```\nabsl::Status CreateClBuffer(cl_context context, int size_in_bytes,\n                            bool read_only, void* data, cl_mem* result) \n```\n\nwhere the size is now int (i.e. 32 bit signed integer... so 0xa2000000 is interpreted as a negative value).\n\n3) This function then calls clCreateBuffer, which takes the size argument as size_t again, and thus receives 0xffffffffa2000000, i.e. the signed 32 bit integer first sign extended to 64bit and then interpreted as unsigned, and thus resulting in a huge size.\n\nThe issue doesn't seem to appear with the same model on Android, probably because: The max buffer allocation size on macOS (M1) seems to be 9GB (according to clinfo), but on that android device it's only 1GB (so on android tflite never tries to allocate such a huge chunk of memory).\n\n### Standalone code to reproduce the issue\n\n```shell\nUnfortunately I'm not allowed to share the code\/model, but looking at the function signatures one can see the issue.\n```\n\n### Relevant log output\n\n```shell\nERROR: Failed to allocate device memory (clCreateBuffer): Invalid buffer size\n```","labels":["type:bug","comp:lite","TF 2.13"],"created_at":"2025-01-29T12:11:28Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86048"},{"repository":"tensorflow\/tensorflow","title":"TensorFlow warning shows whenever importing it","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.18.0-rc2-4-g6550e4bd802\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 24.10 x86_64\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nCUDA:12.6\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`cuFFT`, `cuDNN` and `cuBLAS` warnings showing after importing `TensorFlow`\n\n- OS: Ubuntu 24.10 x86_64\n- Host: G5 5590\n- Kernel: 6.11.0-13-generic\n- CPU: Intel i7-9750H (12) @ 4.500GHz\n- GPU: NVIDIA GeForce GTX 1650 Mobile \/ Max-Q\n- GPU: Intel CoffeeLake-H GT2 [UHD Graphics 630]\n> whenever running the following code it gives that warning also it outputs the predicted output but after the warning:\n```python\nimport tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))\n```\n> output:\n```\n2025-01-23 21:08:06.468437: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1737659286.484845  763412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1737659286.489647  763412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-23 21:08:06.505984: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n[PhysicalDevice(name='\/physical_device:GPU:0', device_type='GPU')]\n```\n> also i know that for that warning (`cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`) i must rebuild the tensor-flow from binaries enabling the AVX2 and FMA instructions but what about the others?\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))\n```\n\n### Relevant log output\n\n```shell\n\n```","labels":["type:bug","2.18.rc"],"created_at":"2025-01-23T21:56:39Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85604"},{"repository":"tensorflow\/tensorflow","title":"Tutorial \"Multi-worker training with Keras\" fails to complete","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv1.12.1-120353-gc5bd67bc56f 2.19.0-dev20250107\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nDebian 6.1.123-1 (2025-01-02) x86_64 GNU\/Linux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython 3.12.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nFollowing the tutorial everything goes well until you start the second worker. Then the below failure occures.\n\n2025-01-20 07:19:35.283801: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-01-20 07:19:35.290192: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-20 07:19:35.307721: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-01-20 07:19:36.510476: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2025-01-20 07:19:36.510494: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=\"-1\"\n2025-01-20 07:19:36.510499: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n2025-01-20 07:19:36.510501: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n2025-01-20 07:19:36.510505: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael\n2025-01-20 07:19:36.510507: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:190] hostname: michael\n2025-01-20 07:19:36.510562: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0\n2025-01-20 07:19:36.510572: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0\n2025-01-20 07:19:36.510574: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0\n2025-01-20 07:19:36.519175: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:637] Initializing CoordinationService\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc:\/\/localhost:12345\n2025-01-20 07:19:36.524874: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 4677280066871850635\n2025-01-20 07:19:36.524894: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 1\/2 tasks to connect.\n2025-01-20 07:19:36.524898: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:819] Example stragglers:\n\/job:worker\/replica:0\/task:1\nI0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.\n2025-01-20 07:22:27.996664: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 13530699364709055870\n2025-01-20 07:22:27.996686: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 0\/2 tasks to connect.\n\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/layers\/core\/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n  warnings.warn(\n2025-01-20 07:22:28.461733: W tensorflow\/core\/framework\/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\nTraceback (most recent call last):\n  File \"\/home\/chad\/Documents\/McCueFiles\/NeuralNetworks\/TensorFlowProject\/TensorFlowDocExample\/main.py\", line 21, in <module>\n    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 108, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: Attempt to convert a value (PerReplica:{\n  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=\narray([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>\n}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.\n\n### Standalone code to reproduce the issue\n\n```shell\npython main.py &> job_1.log\n```\n\n### Relevant log output\n\n```shell\n2025-01-20 07:19:35.283801: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-01-20 07:19:35.290192: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-20 07:19:35.307721: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-01-20 07:19:36.510476: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2025-01-20 07:19:36.510494: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=\"-1\"\n2025-01-20 07:19:36.510499: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n2025-01-20 07:19:36.510501: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n2025-01-20 07:19:36.510505: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael\n2025-01-20 07:19:36.510507: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:190] hostname: michael\n2025-01-20 07:19:36.510562: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0\n2025-01-20 07:19:36.510572: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0\n2025-01-20 07:19:36.510574: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0\n2025-01-20 07:19:36.519175: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:637] Initializing CoordinationService\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc:\/\/localhost:12345\n2025-01-20 07:19:36.524874: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 4677280066871850635\n2025-01-20 07:19:36.524894: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 1\/2 tasks to connect.\n2025-01-20 07:19:36.524898: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:819] Example stragglers:\n\/job:worker\/replica:0\/task:1\nI0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.\n2025-01-20 07:22:27.996664: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 13530699364709055870\n2025-01-20 07:22:27.996686: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 0\/2 tasks to connect.\n\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/layers\/core\/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n  warnings.warn(\n2025-01-20 07:22:28.461733: W tensorflow\/core\/framework\/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\nTraceback (most recent call last):\n  File \"\/home\/chad\/Documents\/McCueFiles\/NeuralNetworks\/TensorFlowProject\/TensorFlowDocExample\/main.py\", line 21, in <module>\n    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 108, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: Attempt to convert a value (PerReplica:{\n  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=\narray([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>\n}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.\n```","labels":["type:bug","TF 2.18"],"created_at":"2025-01-20T14:03:18Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85351"},{"repository":"tensorflow\/tensorflow","title":"Aborted  in `tf.raw_ops.RaggedGather`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific inputs, `tf.raw_ops.RaggedGather` triggers crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nparams_nested_splits = tf.constant(0, shape=[3], dtype=tf.int64)\nparams_dense_values = tf.constant(1, shape=[0], dtype=tf.float32)\nindices = tf.constant(0, shape=[], dtype=tf.int64)\nOUTPUT_RAGGED_RANK = 1\nPARAMS_RAGGED_RANK = 1\n\ntf.raw_ops.RaggedGather(\n    params_nested_splits=[params_nested_splits],\n    params_dense_values=params_dense_values,\n    indices=indices,\n    OUTPUT_RAGGED_RANK=1,\n    name=None\n)\n```\n\n### Relevant log output\n\n```shell\n2025-01-18 09:30:00.549762: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64\nAborted (core dumped)\n```","labels":["type:bug","comp:ops","2.17"],"created_at":"2025-01-18T09:32:16Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85242"},{"repository":"tensorflow\/tensorflow","title":"Segmentation fault (core dumped) in `RaggedTensorToTensor`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nshape = tf.constant(-1, shape=[], dtype=tf.int64)\nvalues = tf.constant(0, shape=[0], dtype=tf.int32)\ndefault_value = tf.constant(0, shape=[], dtype=tf.int32)\nrow_partition_tensors = tf.constant([0, 1, 6], shape=[3], dtype=tf.int64)\nrow_partition_types = [\"ROW_SPLITS\"]\n\ntf.raw_ops.RaggedTensorToTensor(\n    shape=shape,\n    values=values,\n    default_value=default_value,\n    row_partition_tensors=[row_partition_tensors],\n    row_partition_types=row_partition_types)\n```\n\n### Relevant log output\n\n```shell\nSegmentation fault (core dumped)\n```","labels":["type:bug","comp:ops","2.17"],"created_at":"2025-01-18T09:27:19Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85240"},{"repository":"tensorflow\/tensorflow","title":"Seg Fault when iterate dataset created from data service","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSegfault when trying to iterate dataset get from data service.\n\n### Standalone code to reproduce the issue\n\n```shell\n# start the data service file start_dataservice.py\n\nimport tensorflow as tf\n\ndispatcher = tf.data.experimental.service.DispatchServer(\n    tf.data.experimental.service.DispatcherConfig(port=50050), start=True\n)\ndispatcher_address = dispatcher.target.split(\":\/\/\")[1]\nworker = tf.data.experimental.service.WorkerServer(\n    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True\n)\nprint(\"Starting Worker\")\nworker.join()\n\n# test file test_dataset_service.py\nimport tensorflow as tf\nimport numpy as np\n\n\nflags = tf.compat.v1.app.flags\n\nflags.DEFINE_bool(\"local\", False, \"Run data service in process\")\nflags.DEFINE_bool(\"distribute\", False, \"Run data service in distributed_epoch mode\")\nFLAGS = flags.FLAGS\n\n\ndef local_service():\n    print(\"Starting Local Service\")\n    dispatcher = tf.data.experimental.service.DispatchServer(\n        tf.data.experimental.service.DispatcherConfig(port=50050), start=True\n    )\n    dispatcher_address = dispatcher.target.split(\":\/\/\")[1]\n    worker = tf.data.experimental.service.WorkerServer(\n        tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True\n    )\n    print(\"Dispatcher target is \", dispatcher.target)\n    return dispatcher, worker, dispatcher.target\n\n\ndef apply_transformations(ds_train):\n    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    ds_train = ds_train.cache()\n    ds_train = ds_train.shuffle(60000)\n    ds_train = ds_train.batch(128)\n    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n    return ds_train\n\n\n(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\nx_train = x_train \/ np.float32(255)\ny_train = y_train.astype(np.int64)\nds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n\n\ndef normalize_img(image, label):\n    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n    return tf.cast(image, tf.float32) \/ 255.0, label\n\n\nds_train = apply_transformations(ds_train)\n# Create dataset however you were before using the tf.data service.\ndataset = ds_train\nif FLAGS.local:\n    dispatcher, worker, service = local_service()\nelse:\n    dispatcher_address = \"localhost\"\n    dispatcher_port = \"50050\"\n    service = \"grpc:\/\/{}:{}\".format(dispatcher_address, dispatcher_port)\nif FLAGS.distribute:\n    processing_mode = \"distributed_epoch\"\nelse:\n    processing_mode = \"parallel_epochs\"\n\n# This will register the dataset with the tf.data service cluster so that\n# tf.data workers can run the dataset to produce elements. The dataset returned\n# from applying `distribute` will fetch elements produced by tf.data workers.\ndataset = dataset.apply(\n    tf.data.experimental.service.distribute(processing_mode=processing_mode, service=service)\n)\n\nfor (x1, y1), (x2, y2) in zip(dataset, ds_train):\n    np.allclose(x1, x2)\n    np.allclose(y1, y2)\n\nprint(\"verified mnist dataset locally vs over service\")\n\n# script to run \npython -m pip install --upgrade pip\npython -m pip install tensorflow==2.18.0\npython -m pip install 'protobuf<4'\nscreen -d -m python start_dataservice.py\npython3 test_dataset_service.py --local=False\n```\n\n### Relevant log output\n\n```shell\n2025-01-14 21:56:19.778399: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1736891779.795141    9168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1736891779.800177    9168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-14 21:56:19.815971: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nI0000 00:00:1736891783.518634    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 889 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.520395    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 37945 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1d.0, compute capability: 8.0\nI0000 00:00:1736891783.522012    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:2 with 37945 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.523626    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:3 with 37945 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1d.0, compute capability: 8.0\nI0000 00:00:1736891783.525222    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:4 with 37945 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.526807    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:5 with 37945 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1d.0, compute capability: 8.0\nI0000 00:00:1736891783.528377    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:6 with 37945 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.529933    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:7 with 37945 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1d.0, compute capability: 8.0\n\/test\/bin\/testDataservice: line 5:  9168 Segmentation fault      (core dumped) python ${BIN_DIR}\/test_dataset_service.py --local=False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-01-14T21:57:20Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84897"},{"repository":"tensorflow\/tensorflow","title":"GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\nnightly\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 22.04\r\n\r\n### Python version\r\n\r\nPython 3.12\r\n\r\n### CUDA\/cuDNN version\r\n\r\nCUDA 12.4\r\n\r\n### GPU model and memory\r\n\r\nA100 80GB\r\n\r\n### Current behavior?\r\n\r\nStart a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace. \r\nNo any memory profile events or OP profiler, but only trace view.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n**tf_allreduce.py**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.collective_ops import all_reduce, all_reduce_v2\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python.distribute import cluster_resolver as cluster_resolver_lib\r\n\r\ncluster_resolver = cluster_resolver_lib.TFConfigClusterResolver()\r\ncluster = cluster_resolver.cluster_spec()\r\ntask_type = cluster_resolver.task_type\r\ntask_id = cluster_resolver.task_id\r\n\r\nexperimental_config = config_pb2.ConfigProto.Experimental(\r\n    share_cluster_devices_in_session=False,\r\n    share_session_state_in_clusterspec_propagation=False\r\n)\r\nconfig = config_pb2.ConfigProto(experimental=experimental_config)\r\nconfig.experimental.collective_group_leader = '\/job:worker\/replica:0\/task:0'\r\nserver = tf.distribute.Server(cluster,\r\n                              job_name=task_type,\r\n                              task_index=task_id,\r\n                              protocol=\"grpc\", # \"grpc+verbs\"\r\n                              config=config)\r\nrun_options = config_pb2.RunOptions()\r\n\r\nwith tf.compat.v1.Session(target=server.target, config=config) as sess:\r\n    tensor = tf.Variable(tf.ones([2, 2]), dtype=tf.float32)\r\n    init = tf.compat.v1.global_variables_initializer()\r\n    sess.run(init)\r\n    sess.run(tf.print([\"tensor:\",tensor]))\r\n\r\n    reduced_tensor = all_reduce(tensor, group_size=2, group_key=4321, instance_key=1234, merge_op='Add', final_op='Id', communication_hint='auto')\r\n    run_options.experimental.collective_graph_key = 6\r\n    while True:\r\n        sess.run(tf.print([\"reduced_tensor:\",reduced_tensor]), options=run_options)\r\n```\r\n\r\nRun script to start server.\r\n```bash\r\nCUDA_VISIBLE_DEVICES=0 TF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2223\",\"localhost:2224\"]},\"task\":{\"type\":\"worker\",\"index\":0}}' python tf_allreduce.py&\r\nCUDA_VISIBLE_DEVICES=1 TF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2223\",\"localhost:2224\"]},\"task\":{\"type\":\"worker\",\"index\":1}}' python tf_allreduce.py&\r\n```\r\n\r\n use capture_profile in Tensorboard or tf.profiler.experimental.client.trace.\r\n```python\r\ntf.profiler.experimental.client.trace(\r\n  'grpc:\/\/localhost:2223,grpc:\/\/localhost:2224',\r\n   '\/tmp\/my_tb_dir',\r\n   2000,\r\n)\r\n```\r\n\r\nTry to convert xplane.pb to memory_profile, nothing show.\r\n```python\r\nfrom tensorflow.python.profiler.internal import _pywrap_profiler as profiler_wrapper\r\njson = profiler_wrapper.xspace_to_tools_data([\"xxx.xplane\"], \"memory_profile\")\r\n```\r\n\r\n**Relevant log output**\r\n```\r\n{\"memoryProfilePerAllocator\":{},\"numHosts\":1,\"memoryIds\":[]}\r\n```\r\n\r\nRelative issue: #48146 ","labels":["type:bug","comp:gpu","TF 2.18"],"created_at":"2025-01-09T09:26:20Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84460"},{"repository":"tensorflow\/tensorflow","title":"Unable to connect to TPU through Cloud VM (metadata issue?)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.18.0-rc2-4-g6550e4bd802 2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\ntpu-ubuntu2204-base\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.2\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI am on a VM instance trying to connect to a tpu v4-32 using a test script. I installed tensorflow-tpu on both the VM (in venv) and TPU (globally) as per the instructions from the google website.\r\n\r\nIt seems like there is an issue with getting TPU metadata.\r\n\r\nIt is able to connect to the metadata server when I request manually from the VM:\r\n\r\n```\r\n$ curl http:\/\/169.254.169.254\/computeMetadata\/v1\/ -H \"Metadata-Flavor: Google\"\r\ninstance\/\r\noslogin\/\r\nproject\/\r\n```\r\n\r\nAny help would be appreciated!\n\n### Standalone code to reproduce the issue\n\n```shell\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name)\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntry:\r\n    tf.tpu.experimental.initialize_tpu_system(resolver)\r\n    print(\"TPU initialized:\", resolver.master())\r\nexcept Exception as e:\r\n    print(\"Failed to initialize TPU:\", e)\n```\n\n\n### Relevant log output\n\n```shell\n$ python hello.py\r\n2025-01-08 23:49:33.189260: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2025-01-08 23:49:33.221197: I tensorflow\/core\/tpu\/tpu_api_dlsym_initializer.cc:95] Opening library: \/home\/ucsdwanglab\/test_tpu\/.venv\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2\r\n2025-01-08 23:49:33.221290: I tensorflow\/core\/tpu\/tpu_api_dlsym_initializer.cc:121] Libtpu path is: \/home\/ucsdwanglab\/test_tpu\/.venv\/lib\/python3.11\/site-packages\/libtpu\/libtpu.so\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable CHIPS_PER_HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable ALT: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable WRAP: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (accelerator-type) from instance metadata for variable TPU_ACCELERATOR_TYPE: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\n\r\nFailed to find host bounds for accelerator type: WARNING: could not determine TPU accelerator type, please set env var `TPU_ACCELERATOR_TYPE` manually, otherwise libtpu.so may not properly initialize.\r\nFailed to get TPU metadata (agent-worker-number) from instance metadata for variable TPU_WORKER_ID: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\n\r\nFailed to get TPU metadata (worker-network-endpoints) from instance metadata for variable TPU_WORKER_HOSTNAMES: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\n\r\nWARNING: Logging before InitGoogle() is written to STDERR\r\nE0000 00:00:1736380405.363400    3192 common_lib.cc:511] INVALID_ARGUMENT: Error: unexpected worker hostname 'WARNING: could not determine TPU worker hostnames or IP addresses' from env var TPU_WORKER_HOSTNAMES. Expecting a valid hostname or IP address without port number. (Full TPU workers' addr string: WARNING: could not determine TPU worker hostnames or IP addresses, please set env var `TPU_WORKER_HOSTNAMES` manually, otherwise libtpu.so may not properly initialize.)\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/libtpu_init_utils.cc:173\r\n2025-01-08 23:56:48.526584: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1736380609.730442    3192 context_distributed_manager.cc:762] unknown service tensorflow.WorkerService\r\nAdditional GRPC error information from remote target \/job:worker\/replica:0\/task:0 while calling \/tensorflow.WorkerService\/GetStatus:\r\n:{\"created\":\"@1736380609.730372913\",\"description\":\"Error received from peer ipv4:10.130.0.3:8470\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/surface\/call.cc\",\"file_line\":1056,\"grpc_message\":\"unknown service tensorflow.WorkerService\",\"grpc_status\":12}\r\nE0108 23:56:49.730822322    3192 completion_queue.cc:244]    assertion failed: queue.num_items() == 0\r\nhttps:\/\/symbolize.stripped_domain\/r\/?trace=7f1ccaf5cebc,7f1ccaf0e04f&map= \r\n*** SIGABRT received by PID 3192 (TID 3192) on cpu 4 from PID 3192; stack trace: ***\r\nPC: @     0x7f1ccaf5cebc  (unknown)  (unknown)\r\n    @     0x7f1caa302841       1888  (unknown)\r\n    @     0x7f1ccaf0e050   18460496  (unknown)\r\n    @     0x7f1ccaed1c60  (unknown)  (unknown)\r\nhttps:\/\/symbolize.stripped_domain\/r\/?trace=7f1ccaf5cebc,7f1caa302840,7f1ccaf0e04f,7f1ccaed1c5f&map= \r\nE0108 23:56:49.732558    3192 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked.\r\nE0108 23:56:49.732569    3192 coredump_hook.cc:355] RAW: Skipping coredump since rlimit was 0 at process start.\r\nE0108 23:56:49.732575    3192 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.\r\nE0108 23:56:49.732580    3192 coredump_hook.cc:411] RAW: Sending fingerprint to remote end.\r\nE0108 23:56:49.732595    3192 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket \/var\/google\/services\/logmanagerd\/remote_coredump.socket (Is the listener running?): No such file or directory\r\nE0108 23:56:49.732601    3192 coredump_hook.cc:472] RAW: Dumping core locally.\r\nE0108 23:56:49.745981    3192 process_state.cc:805] RAW: Raising signal 6 with default behavior\r\nAborted\n```\n","labels":["type:bug","comp:tpus","TF 2.18"],"created_at":"2025-01-09T00:04:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84413"},{"repository":"tensorflow\/tensorflow","title":"dictionaries in fit method of model load data in wrong order","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17; tf 2.18\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 22.04.3 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.12\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nthe code is running in google collab.\r\nThe code below is an example of a model with multiple inputs and multiple outputs.\r\nNOT working code with using **dictionaries** in method **fit** of model.\r\n\r\nthe link to collab:  https:\/\/colab.research.google.com\/drive\/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing\r\nthe link to gist: https:\/\/gist.github.com\/moprules\/def9b2bda642a064b35e51b8914a28dd\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\n# collab:  https:\/\/colab.research.google.com\/drive\/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing\r\n# gist:      https:\/\/gist.github.com\/moprules\/def9b2bda642a064b35e51b8914a28dd\r\n\r\n# fast code\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\nvocabulary_size = 10000\r\nnum_tags = 100\r\nnum_departments = 4\r\n\r\n# define three model inputs\r\ntitle = keras.Input(shape=(vocabulary_size,), name=\"title\")\r\ntext_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\r\ntags = keras.Input(shape=(num_tags,), name=\"tags\")\r\n\r\nfeatures = layers.Concatenate()([title, text_body, tags])\r\n# one intermediate layer\r\nfeatures = layers.Dense(64, activation=\"relu\")(features)\r\n\r\n# Define two model outputs\r\npriority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\r\ndepartment = layers.Dense(num_departments, activation=\"softmax\", name=\"department\")(features)\r\n\r\n# set the model\r\nmodel = keras.Model(inputs=[title, text_body, tags],\r\n                    outputs=[priority, department])\r\n# prepare data\r\nnum_samples = 1280\r\n# The data is filled in with zeros and ones\r\ntitle_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\r\ntext_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\r\ntags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\r\n\r\n# priority: [0., 1.]\r\npriority_data = np.random.random(size=(num_samples, 1))\r\n# class of 4 labels\r\ndepartment_data = np.random.randint(0, 2, size=(num_samples, num_departments))\r\n\r\n# compile model\r\nmodel.compile(optimizer=\"rmsprop\",\r\n              loss={\"priority\": \"mean_squared_error\",\r\n                    \"department\": \"categorical_crossentropy\"},\r\n              metrics={\"priority\": [\"mean_absolute_error\"],\r\n                       \"department\": [\"accuracy\"]})\r\n\r\n# It doesn't matter how the model is compiled\r\n# model.compile(optimizer=\"rmsprop\",\r\n#               loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\r\n#               metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\r\n\r\n\r\n# NOT WORKING\r\n# TRAIN MODEL WITH transferring the DICTIONARY to the method\r\nmodel.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\r\n          {\"priority\": priority_data, \"department\": department_data},\r\n          epochs=1\r\n)\r\n\r\n# WORK\r\n# TRAIN MODEL WITHOUT transferring the DICTIONARY to the method\r\nmodel.fit([title_data, text_body_data, tags_data],\r\n          [priority_data, department_data],\r\n          epochs=1\r\n)\r\n\r\n# ALSO WORK\r\n# TRAIN MODEL WITH transferring the DICTIONARY to the method\r\n# REPLACE priority and department\r\nmodel.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\r\n          {\"priority\": department_data, \"department\": priority_data},\r\n          epochs=1\r\n)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting response","type:bug","stale","comp:keras","TF 2.18"],"created_at":"2025-01-07T13:08:06Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84278"},{"repository":"tensorflow\/tensorflow","title":"keras model.save does not respect `include_optimizer=False`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.19.0-dev20250105\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSaving a model using keras with `include_optizer = False` results in a model being saved with optimizer\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1x5NJs9nFxmExhuy8_f_fOehHmIOmk-CZ?usp=sharing\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","comp:keras","TF 2.18"],"created_at":"2025-01-07T10:33:38Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84268"},{"repository":"tensorflow\/tensorflow","title":"Encountered unresolved custom op: XlaDynamicSlice","description":"Hi, i am doing a task in converting T5 model to TFLite for Android. Currently, i am using T5ModelForConditionalGeneration from Huggingface to convert. The conversion is done with some below logging but when load the `generator` from Interpretor, and run an inference example, i have faced this error. You can reproduce with the colab provided below. AFAIK, this XlaDynamicSlice is in TF ops but why this op cannot be resolved in this cases. \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): PyPI 24.0 on Python 3.11.5\r\n- TensorFlow version (or github SHA if from source): 2.18.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nIn colab version, tflite_convert doesn't log anything, below log is in my local version\r\n```\r\nINFO:tensorflow:Assets written to: \/tmp\/tmpaxxybw9x\/assets\r\nINFO:tensorflow:Assets written to: \/tmp\/tmpaxxybw9x\/assets\r\nW0000 00:00:1736157114.568747 1061359 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\r\nW0000 00:00:1736157114.568765 1061359 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\r\n2025-01-06 16:51:54.568997: I tensorflow\/cc\/saved_model\/reader.cc:83] Reading SavedModel from: \/tmp\/tmpaxxybw9x\r\n2025-01-06 16:51:54.645325: I tensorflow\/cc\/saved_model\/reader.cc:52] Reading meta graph with tags { serve }\r\n2025-01-06 16:51:54.645352: I tensorflow\/cc\/saved_model\/reader.cc:147] Reading SavedModel debug info (if present) from: \/tmp\/tmpaxxybw9x\r\n2025-01-06 16:51:55.085153: I tensorflow\/cc\/saved_model\/loader.cc:236] Restoring SavedModel bundle.\r\n2025-01-06 16:51:56.061632: I tensorflow\/cc\/saved_model\/loader.cc:220] Running initialization op on SavedModel bundle at path: \/tmp\/tmpaxxybw9x\r\n2025-01-06 16:51:56.517300: I tensorflow\/cc\/saved_model\/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 1948307 microseconds.\r\n2025-01-06 16:52:30.233639: W tensorflow\/compiler\/mlir\/lite\/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\r\nFlex ops: FlexStridedSlice\r\nDetails:\r\n\ttf.StridedSlice(tensor<?x?x?xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<?x1x?x?xf32>) : {begin_mask = 13 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 13 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}\r\nSee instructions: https:\/\/www.tensorflow.org\/lite\/guide\/ops_select\r\n2025-01-06 16:52:30.233666: W tensorflow\/compiler\/mlir\/lite\/flatbuffer_export.cc:3836] The following operation(s) need TFLite custom op implementation(s):\r\nCustom ops: XlaDynamicSlice\r\nDetails:\r\n\ttf.XlaDynamicSlice(tensor<1x12x?x?xf32>, tensor<4xi64>, tensor<4xi64>) -> (tensor<1x12x1x?xf32>) : {device = \"\"}\r\nSee instructions: https:\/\/www.tensorflow.org\/lite\/guide\/ops_custom\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab\/Jupyter\/any notebook.\r\nMy reproduce code in Colab: https:\/\/colab.research.google.com\/drive\/1Rmhc_vpJSa7M1Vt-4ugV5uORaEfRJMOw?usp=sharing\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info \/ logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n","labels":["stat:awaiting response","type:bug","comp:lite","TFLiteConverter","TF 2.18"],"created_at":"2025-01-06T10:46:54Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84203"},{"repository":"tensorflow\/tensorflow","title":"MFCC-Example-Model converted from TF to TFlite fails with IsPowerOfTwo-RuntimeError inside rfft2d","description":"### 1. System information\r\n\r\n- OS Platform and Distribution: Linux Mint 6.2.9\r\n- TensorFlow installation: pip\r\n- TensorFlow library: 2.18.0 (latest)\r\n\r\n### 2. Code\r\n\r\nBelow is a minimum example which triggers the rfft2d IsPowerOfTwo RuntimeError.\r\nThe MFCC-Calculation was directly taken from the tutorial from [tensorflow.org](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/signal\/mfccs_from_log_mel_spectrograms#for_example)\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass MFCCLayer(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        super(MFCCLayer, self).__init__(**kwargs)\r\n\r\n    def call(self, pcm):\r\n        # A 1024-point STFT with frames of 64 ms and 75% overlap.\r\n        stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256, fft_length=1024)\r\n        spectrograms = tf.abs(stfts)\r\n\r\n        # Warp the linear scale spectrograms into the mel-scale.\r\n        num_spectrogram_bins = stfts.shape[-1]\r\n        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\r\n        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\r\n            num_mel_bins,\r\n            num_spectrogram_bins,\r\n            sample_rate,\r\n            lower_edge_hertz,\r\n            upper_edge_hertz,\r\n        )\r\n        mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\r\n        mel_spectrograms.set_shape(\r\n            spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:])\r\n        )\r\n\r\n        # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\r\n        log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\r\n\r\n        # Compute MFCCs from log_mel_spectrograms and take the first 13.\r\n        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[\r\n            ..., :13\r\n        ]\r\n        print(\"mfccs.shape: \", mfccs.shape)\r\n        return mfccs\r\n\r\n\r\ndef build_model(input_shape):\r\n    input_layer = tf.keras.layers.Input(shape=input_shape)\r\n    output_layer = MFCCLayer()(input_layer)\r\n    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    batch_size, num_samples, sample_rate = 32, 32000, 16000.0\r\n    # A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].\r\n    pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)\r\n    print(\"pcm.shape: \", pcm.shape)\r\n\r\n    model = build_model(pcm.shape)\r\n    model.summary()\r\n\r\n    # Convert to TensorFlow Lite and Save\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    tflite_model = converter.convert()\r\n\r\n    with open(\"mfcc.tflite\", \"wb\") as f:\r\n        f.write(tflite_model)\r\n\r\n    # Load the model and run inference\r\n    with open(\"mfcc.tflite\", \"rb\") as f:\r\n        tflite_model = f.read()\r\n\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    pcm = tf.expand_dims(pcm, axis=0)  # Add batch dimension\r\n\r\n    interpreter.set_tensor(input_details[0][\"index\"], pcm)\r\n    interpreter.invoke()  # <-- RuntimeError: tensorflow\/lite\/kernels\/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.\r\n    mfccs = interpreter.get_tensor(output_details[0][\"index\"])\r\n    print(\"mfccs.shape: \", mfccs.shape)\r\n```\r\n\r\n\r\n### 3. Failure after conversion\r\nAs far as I know, the RuntimeError should't happen, as all supplied stft-function arguments are power of two's?\r\n\r\nI am unsure if this is just a user error from myself or this is a bug.\r\nI couldn't find any info online, hence i ask here.\r\n\r\nIs a MFCC-calculation model possible with TFlite?\r\n\r\nThanks for all help\r\n\r\n","labels":["stat:awaiting response","type:bug","stale","comp:lite","TFLiteConverter","TF 2.18"],"created_at":"2025-01-05T20:45:45Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84171"},{"repository":"tensorflow\/tensorflow","title":"Broken compatibility with tensorflow-metal in 2.18","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMacOS 15.2\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nApple M2 Max GPU 38-cores\n\n### Current behavior?\n\nApple silicone GPU with tensorflow-metal==1.1.0  and python 3.11 works fine with tensorboard==2.17.0\r\n\r\nThis is normal output:\r\n```\r\n\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/bin\/python \/Users\/mspanchenko\/VSCode\/cryptoNN\/ml\/core_second_window\/test_tensorflow_gpus.py \r\n[PhysicalDevice(name='\/physical_device:GPU:0', device_type='GPU')]\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\nBut if I upgrade tensorflow to 2.18 I'll have error, attached in \"Relevant log output\" issue section\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nif __name__ == '__main__':\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    print(gpus)\n```\n\n\n### Relevant log output\n\n```shell\n\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/bin\/python \/Users\/mspanchenko\/VSCode\/cryptoNN\/ml\/core_second_window\/test_tensorflow_gpus.py \r\nTraceback (most recent call last):\r\n  File \"\/Users\/mspanchenko\/VSCode\/cryptoNN\/ml\/core_second_window\/test_tensorflow_gpus.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow\/__init__.py\", line 437, in <module>\r\n    _ll.load_library(_plugin_dir)\r\n  File \"\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/load_library.py\", line 151, in load_library\r\n    py_tf.TF_LoadLibrary(lib)\r\ntensorflow.python.framework.errors_impl.NotFoundError: dlopen(\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow-plugins\/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii\r\n  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> \/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow-plugins\/libmetal_plugin.dylib\r\n  Expected in:     <2814A58E-D752-317B-8040-131217E2F9AA> \/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tensorflow_internal.so\r\n\r\nProcess finished with exit code 1\n```\n","labels":["stat:awaiting response","type:bug","stale","comp:gpu","TF 2.18"],"created_at":"2025-01-05T17:26:17Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84167"},{"repository":"tensorflow\/tensorflow","title":"The test case label_image .py of tensorflow2.4.1 source code fails to be execued.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.4.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.7.12\n\n### Bazel version\n\n3.7\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe test case label_image.py fails to be executed,and the message \"module 'tensorfle' has no attribute 'GrapDef'\" is displayed.\r\n![image](https:\/\/github.com\/user-attachments\/assets\/e4b7b56d-2589-41fe-8395-1743c941dd49)\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ngraph_def = tf.GraphDef()\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting response","type:bug","stale","TF 2.4"],"created_at":"2025-01-03T03:03:30Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84039"},{"repository":"tensorflow\/tensorflow","title":"Failing to convert MobileNetV3Large to TFLite w\/ Integer q","description":"### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Windows 10 WSL\r\n- TensorFlow installation (pip package or built from source):  2.10 (on Win 10) and 2.16.2 (on WSL)\r\n\r\n### 2. Code\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.applications import MobileNetV3Large\r\nfrom tensorflow.keras.applications.mobilenet_v3 import preprocess_input\r\nimport matplotlib.pyplot as plt\r\nfrom scipy.stats import pearsonr\r\n\r\n# Generate one sample image for testing\r\ntest_image = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))\r\ntest_image = np.clip(test_image, 0, 255).astype(np.float32)\r\npreprocessed_image = preprocess_input(test_image.copy())\r\n\r\n# Load model\r\nmodel = MobileNetV3Large(\r\n    weights='imagenet',\r\n    include_top=True,\r\n    input_shape=(224, 224, 3)\r\n)\r\n\r\n# Get original prediction\r\noriginal_pred = model.predict(preprocessed_image, verbose=0)\r\n\r\n# Convert to TFLite\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable dynamic range quantization\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\nconverter._experimental_disable_per_channel = True\r\nconverter.experimental_new_converter = True\r\n\r\n# Convert\r\ntflite_model = converter.convert()\r\n\r\n# Get TFLite prediction\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], preprocessed_image)\r\ninterpreter.invoke()\r\ntflite_pred = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n# Calculate correlation\r\ncorrelation, _ = pearsonr(original_pred.flatten(), tflite_pred.flatten())\r\n\r\n# Visualize\r\nplt.figure(figsize=(10, 5))\r\n\r\n# Scatter plot\r\nplt.subplot(1, 2, 1)\r\nplt.scatter(original_pred.flatten(), tflite_pred.flatten(), alpha=0.5)\r\nplt.plot([original_pred.min(), original_pred.max()],\r\n         [original_pred.min(), original_pred.max()],\r\n         'r--', label=f'Perfect Correlation\\nActual: {correlation:.4f}')\r\nplt.title('Original vs Quantized Predictions')\r\nplt.xlabel('Original Model')\r\nplt.ylabel('Quantized Model')\r\nplt.legend()\r\n\r\n# Distribution plot\r\nplt.subplot(1, 2, 2)\r\nplt.hist(np.abs(original_pred.flatten() - tflite_pred.flatten()),\r\n         bins=50, alpha=0.75, label='Prediction Differences')\r\nplt.title('Distribution of Prediction Differences')\r\nplt.xlabel('|Original - Quantized|')\r\nplt.ylabel('Count')\r\nplt.legend()\r\n\r\nplt.tight_layout()\r\nplt.show()\r\n\r\nprint(f\"\\nResults:\")\r\nprint(f\"Prediction correlation: {correlation:.4f}\")\r\nprint(f\"Original model size: {len(model.get_weights()) \/ 1024 \/ 1024:.2f} MB\")\r\nprint(f\"Quantized model size: {len(tflite_model) \/ 1024 \/ 1024:.2f} MB\")\r\nprint(f\"Size reduction: {(1 - len(tflite_model) \/ len(model.get_weights())) * 100:.1f}%\")\r\n```\r\n\r\n### 3. Failure after conversion\r\n1. TF 2.10 in Win10 Log:\r\nModel produces wrong results. See plot made from code:\r\n![image](https:\/\/github.com\/user-attachments\/assets\/8823cbfc-88d9-4e2d-9ef7-c8a2adc3ef0a)\r\n\r\n2. TF2.16 in WSL:\r\nModel fails to convert. Gets error: `LLVM ERROR: Failed to infer result type(s).` (see log)\r\n\r\n\r\n### 5. (optional) Any other info \/ logs\r\nI ran this on 2 systems:\r\n\r\n1. TF 2.10 in Win10 Log:\r\n```\r\nimport sys; print('Python %s on %s' % (sys.version, sys.platform))\r\nD:\\code\\ai_dev\\venv\\Scripts\\python.exe \"C:\/Program Files\/JetBrains\/PyCharm 2023.2.4\/plugins\/python\/helpers\/pydev\/pydevd.py\" --multiprocess --qt-support=auto --client 127.0.0.1 --port 54366 --file C:\\Users\\Administrator\\AppData\\Roaming\\JetBrains\\PyCharm2023.2\\scratches\\tfmodel_tflite.py \r\nConnected to pydev debugger (build 232.10203.26)\r\n2024-12-26 15:17:07.215039: I tensorflow\/core\/platform\/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-12-26 15:17:07.670016: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1616] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 7423 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6\r\n2024-12-26 15:17:11.111912: I tensorflow\/stream_executor\/cuda\/cuda_dnn.cc:384] Loaded cuDNN version 8906\r\n2024-12-26 15:17:12.036555: I tensorflow\/stream_executor\/cuda\/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 64). These functions will not be directly callable after loading.\r\n2024-12-26 15:17:44.746406: W tensorflow\/compiler\/mlir\/lite\/python\/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\r\n2024-12-26 15:17:44.746529: W tensorflow\/compiler\/mlir\/lite\/python\/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\r\n2024-12-26 15:17:44.747230: I tensorflow\/cc\/saved_model\/reader.cc:45] Reading SavedModel from: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprjeve5nr\r\n2024-12-26 15:17:44.771028: I tensorflow\/cc\/saved_model\/reader.cc:89] Reading meta graph with tags { serve }\r\n2024-12-26 15:17:44.771129: I tensorflow\/cc\/saved_model\/reader.cc:130] Reading SavedModel debug info (if present) from: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprjeve5nr\r\n2024-12-26 15:17:44.886049: I tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\r\n2024-12-26 15:17:44.904668: I tensorflow\/cc\/saved_model\/loader.cc:229] Restoring SavedModel bundle.\r\n2024-12-26 15:17:45.275249: I tensorflow\/cc\/saved_model\/loader.cc:213] Running initialization op on SavedModel bundle at path: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprjeve5nr\r\n2024-12-26 15:17:45.402632: I tensorflow\/cc\/saved_model\/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 655396 microseconds.\r\n2024-12-26 15:17:45.811466: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n```\r\n\r\n3. TF 2.16.2 in WSL log:\r\n```\r\n\/root\/ai_dev\/.venv\/bin\/python \/root\/.pycharm_helpers\/pydev\/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 55955 --file \/mnt\/c\/Users\/Administrator\/AppData\/Roaming\/JetBrains\/PyCharm2023.2\/scratches\/tfmodel_tflite.py \r\nConnected to pydev debugger (build 232.10203.26)\r\n2024-12-26 15:18:56.434366: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-26 15:18:57.803991: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-26 15:18:58.473972: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-12-26 15:18:58.972456: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-12-26 15:18:58.975123: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-12-26 15:18:59.910805: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-12-26 15:19:06.124533: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-12-26 15:19:15.989425: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:984] could not open file to read NUMA node: \/sys\/bus\/pci\/devices\/0000:0a:00.0\/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2024-12-26 15:19:17.013008: W tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https:\/\/www.tensorflow.org\/install\/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nW0000 00:00:1735255170.947341  469943 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\r\nW0000 00:00:1735255170.947401  469943 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\r\n2024-12-26 15:19:30.948060: I tensorflow\/cc\/saved_model\/reader.cc:83] Reading SavedModel from: \/tmp\/tmp9lkkwnp9\r\n2024-12-26 15:19:30.953309: I tensorflow\/cc\/saved_model\/reader.cc:51] Reading meta graph with tags { serve }\r\n2024-12-26 15:19:30.953334: I tensorflow\/cc\/saved_model\/reader.cc:146] Reading SavedModel debug info (if present) from: \/tmp\/tmp9lkkwnp9\r\n2024-12-26 15:19:31.011901: I tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\r\n2024-12-26 15:19:31.020594: I tensorflow\/cc\/saved_model\/loader.cc:234] Restoring SavedModel bundle.\r\n2024-12-26 15:19:31.231606: I tensorflow\/cc\/saved_model\/loader.cc:218] Running initialization op on SavedModel bundle at path: \/tmp\/tmp9lkkwnp9\r\n2024-12-26 15:19:31.297302: I tensorflow\/cc\/saved_model\/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 349244 microseconds.\r\n2024-12-26 15:19:31.779723: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nloc(fused[\"ReadVariableOp:\", callsite(\"MobileNetV3Large_1\/conv_1\/convolution\/ReadVariableOp@__inference_serving_default_5035\"(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":2199:1) at callsite(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":2181:1 at callsite(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":1493:1 at callsite(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":1500:1 at callsite(\"\/root\/.pycharm_helpers\/pydev\/_pydev_imps\/_pydev_execfile.py\":18:1 at callsite(\"\/mnt\/c\/Users\/Administrator\/AppData\/Roaming\/JetBrains\/PyCharm2023.2\/scratches\/tfmodel_tflite.py\":34:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1175:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1129:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1636:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1614:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/convert_phase.py\":205:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1537:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/layer.py\":58:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/layer.py\":112:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/layer.py\":899:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/operation.py\":46:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":156:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\":182:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/function.py\":171:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\":632:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/layer.py\":899:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/operation.py\":46:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":156:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/convolutional\/base_conv.py\":243:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/convolutional\/base_conv.py\":233:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/nn.py\":1183:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/nn.py\":301:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/nn.py\":274:1 at \"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/core.py\":85:1))))))))))))))))))))))))))))))))]): error: missing attribute 'value'\r\nLLVM ERROR: Failed to infer result type(s).\r\n\r\nProcess finished with exit code 134\r\n```\r\n\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:lite","TFLiteConverter","TF 2.16"],"created_at":"2024-12-26T23:21:09Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83754"},{"repository":"tensorflow\/tensorflow","title":"How to run TFLite benchmark with QNN delegate in Android","description":"### Issue type\r\n\r\nFeature Request\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.15.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nmacOS 15.2\r\n\r\n### Mobile device\r\n\r\nOne Plus 7 Pro, Android 11\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI have built\/installed\/run TFLite benchmark following this [instruction](https:\/\/github.com\/tensorflow\/tensorflow\/tree\/master\/tensorflow\/lite\/tools\/benchmark#on-android) for Android, and used TensorFlow 2.15.0 according to [issue#66015](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66015). I test the benchmark via the following commands and the output result seems correct.\r\n```shell\r\nadb push \/Users\/handleychen\/Github\/tensorflow\/tensorflow\/bazel-bin\/tensorflow\/lite\/tools\/benchmark\/benchmark_model \/data\/local\/tmp\r\nadb shell chmod +x \/data\/local\/tmp\/benchmark_model\r\nadb shell \"mkdir \/data\/local\/tmp\/models\"\r\nadb push \/Users\/handleychen\/Github\/tensorflow\/models\/*.tflite \/data\/local\/tmp\/models\r\nadb shell \/data\/local\/tmp\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224.tflite --num_threads=4 --enable_op_profiling=true\r\nadb shell \/data\/local\/tmp\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224.tflite --use_gpu=true --enable_op_profiling=true\r\nadb shell \/data\/local\/tmp\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224.tflite --use_nnapi=true --enable_op_profiling=true\r\n``` \r\n[benchmark result.txt](https:\/\/github.com\/user-attachments\/files\/18197819\/benchmark.result.txt)\r\n\r\nNow I want to run the benchmark with QNN delegate. I [setup the on device environment](https:\/\/docs.qualcomm.com\/bundle\/publicresource\/topics\/80-63442-50\/TfLite-Delegate_setup.html#on-device-environment-setup) and [run a QNN delegate using an external delegate](https:\/\/docs.qualcomm.com\/bundle\/publicresource\/topics\/80-70015-54\/sample-applications.html#run-a-qnn-delegate-using-an-external-delegate). The [model](https:\/\/storage.googleapis.com\/download.tensorflow.org\/models\/tflite\/task_library\/image_classification\/android_java\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite) being tested comes from tflite example [image_classification](https:\/\/github.com\/tensorflow\/examples\/tree\/master\/lite\/examples\/image_classification).  I tested the benchmark using the following commands, but the result was a failure.\r\n```shell\r\nadb shell \"mkdir \/data\/local\/tmp\/qnn_delegate\"\r\nadb push \/Users\/handleychen\/Github\/quic\/SDK\/qairt\/2.26.0.240828\/lib\/aarch64-android\/* \/data\/local\/tmp\/qnn_delegate\r\nadb shell\r\ncd \/data\/local\/tmp\r\nexport LD_LIBRARY_PATH=\/data\/local\/tmp\/qnn_delegate\r\nexport ADSP_LIBRARY_PATH=\"\/data\/local\/tmp\/qnn_delegate\"\r\n.\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:gpu;'\r\n.\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;'\r\n# I also tried setting htp_precision:1, but the result was the same.\r\n.\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;htp_precision:1'\r\n``` \r\n```shell\r\n# for gpu delegate\r\n\u2026\u2026\r\nINFO: Though EXTERNAL delegate is explicitly applied, the model graph will not be executed by the delegate.\r\n\u2026\u2026\r\n\r\n# for npu delegate\r\nINFO: STARTING!\r\nINFO: Log parameter values verbosely: [0]\r\nINFO: Graph: [\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]\r\nINFO: External delegate path: [\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so]\r\nINFO: External delegate options: [backend_type:htp;htp_precision:1]\r\nINFO: Loaded model \/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: EXTERNAL delegate created.\r\nERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008\r\nERROR: Restored original execution plan after delegate application failure.\r\nERROR: Failed to apply EXTERNAL delegate.\r\nERROR: Benchmarking failed.\r\n``` \r\nThe full output is attached. [benchmarkQNN result.txt](https:\/\/github.com\/user-attachments\/files\/18206356\/benchmarkQNN.result.txt)\r\n\r\nI have also tested it on Android phones equipped with Snapdragon 855 and Gen 3 chips, and the results were the same.\r\n\r\nCould anyone tell me how to deal with this?\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nas described above\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:lite"],"created_at":"2024-12-19T12:40:25Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83344"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `LearnedUnigramCandidateSampler`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific inputs,`tf.raw_ops.LearnedUnigramCandidateSampler` triggers crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntrue_classes = tf.constant([], dtype=tf.int64)\r\nnum_true = 3590707793247644003\r\nnum_sampled = 126\r\nunique = False\r\nrange_max = 186785497093039093\r\nseed = 8997\r\nseed2 = 0\r\n\r\ntf.raw_ops.LearnedUnigramCandidateSampler(\r\n    true_classes=true_classes,\r\n    num_true=num_true,\r\n    num_sampled=num_sampled,\r\n    unique=unique,\r\n    range_max=range_max,\r\n    seed=seed,\r\n    seed2=seed2,\r\n    name=None\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-12-17 11:36:29.305345: W tensorflow\/core\/framework\/op_kernel.cc:1816] OP_REQUIRES failed at candidate_sampler_ops.cc:37 : INVALID_ARGUMENT: Attr num_true has value 3590707793247644003 out of range for an int32\r\n2024-12-17 11:36:29.305378: F external\/local_tsl\/tsl\/lib\/random\/weighted_picker.cc:28] Check failed: N >= 0 (0 vs. -2090015755)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-12-17T12:05:50Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83164"},{"repository":"tensorflow\/tensorflow","title":"[XLA] `tf.keras.layers.LSTM` behaves differently on GPU","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nnightly\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen executing LSTM on **XLA**, it fails.\r\nHowever, when executing it without XLA, it passes.\r\nThe above failure is on GPU.\r\nIf I use CPU as backend, with or without XLA both pass the check.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport os\r\nimport tensorflow\r\nimport tensorflow as tf\r\ntf.random.set_seed(42)\r\nclass RecurrentModel(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(RecurrentModel, self).__init__()\r\n        self.lstm = tf.keras.layers.LSTM(units=64, return_sequences=True)\r\n\r\n    @tf.function(jit_compile=True)\r\n    def call(self, x):\r\n        return self.lstm(x)\r\n\r\n\r\nmodel = RecurrentModel()\r\n\r\n\r\ninput_shape = (10, 20, 1)\r\nx = tf.random.normal(shape=input_shape)\r\n\r\ninputs = [x]\r\n\r\noutput = model(*inputs)\r\nprint(output)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-0938fdccd1fa> in <cell line: 24>()\r\n     22 inputs = [x]\r\n     23 \r\n---> 24 output = model(*inputs)\r\n     25 print(output)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51   try:\r\n     52     ctx.ensure_initialized()\r\n---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                         inputs, attrs, num_outputs)\r\n     55   except core._NotOkStatusException as e:\r\n\r\nInvalidArgumentError: Exception encountered when calling RecurrentModel.call().\r\n\r\nDetected unsupported operations when trying to compile graph __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: CudnnRNNV3 (No registered 'CudnnRNNV3' OpKernel for XLA_GPU_JIT devices compatible with node {{node lstm_3_1\/CudnnRNNV3}}){{node lstm_3_1\/CudnnRNNV3}}\r\nThe op is created at: \r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/colab_kernel_launcher.py\", line 37, in <module>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/traitlets\/config\/application.py\", line 992, in launch_instance\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelapp.py\", line 619, in start\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/platform\/asyncio.py\", line 195, in start\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 685, in <lambda>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 738, in _run_callback\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 825, in inner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 361, in process_one\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 261, in dispatch_shell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 539, in execute_request\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 2975, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3030, in _run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/async_helpers.py\", line 78, in _pseudo_sync_runner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3257, in run_cell_async\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3473, in run_ast_nodes\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3553, in run_code\r\nFile \"<ipython-input-4-0938fdccd1fa>\", line 24, in <cell line: 24>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/layer.py\", line 826, in __call__\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/layer.py\", line 1376, in _maybe_build\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/core.py\", line 212, in compute_output_spec\r\nFile \"<ipython-input-1-0938fdccd1fa>\", line 13, in call\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/layer.py\", line 901, in __call__\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/ops\/operation.py\", line 46, in __call__\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 156, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/rnn\/lstm.py\", line 570, in call\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/rnn\/rnn.py\", line 406, in call\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/rnn\/lstm.py\", line 537, in inner_loop\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/rnn.py\", line 841, in lstm\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/rnn.py\", line 933, in _cudnn_lstm\r\n\ttf2xla conversion failed while converting __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=\/path\/to\/dump\/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_877]\r\n\r\nArguments received by RecurrentModel.call():\r\n  \u2022 x=tf.Tensor(shape=(10, 20, 1), dtype=float32)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","comp:xla","TF 2.18"],"created_at":"2024-12-16T13:57:22Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83063"},{"repository":"tensorflow\/tensorflow","title":"`tensorflow.experimental.numpy.kron` not working with multidimensional Arrays","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThis is a new issue in replacement for https:\/\/github.com\/tensorflow\/tensorflow\/issues\/59761 as suggested by @tilakrayal\r\n\r\nI tested the function against numpy and it throws an error when the `ndim` of the input tensors is greater than 2.\r\nI run the code on the latest TensorFlow version on PyPI and the nightly version, and I get the same failures.\r\n\r\nAlso, I am not getting as much debug information only this error\r\n\r\n`UnimplementedError: {{function_node __wrapped__Mul_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name: `\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\ntry:\r\n    a = tf.constant(np.arange(100).reshape(2, 5, 2, 5))\r\n    b = tf.constant(np.arange(24).reshape(2, 3, 4))\r\n    print(a.ndim) # 4\r\n    print(b.ndim) # 3\r\n\r\n    y = tf.experimental.numpy.kron(a, b)\r\n\r\n    print(y.shape)\r\nexcept:\r\n    print(\"Can't use tf.experimental.numpy.kron on multi-dimensional arrays\")\r\n\r\nx = np.arange(100).reshape(2, 5, 2, 5)\r\ny = np.arange(24).reshape(2, 3, 4)\r\n\r\nprint(x.ndim) # 4\r\nprint(y.ndim) # 3\r\n\r\nz = np.kron(x, y)\r\n\r\nprint(z.shape) # (2, 10, 6, 20)\n```\n\n\n### Relevant log output\n\n```shell\nUnimplementedError: {{function_node __wrapped__Mul_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-12-16T07:22:01Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83037"},{"repository":"tensorflow\/tensorflow","title":"error message is inconsistent with documentation in `tf.raw_ops.MaxPoolGradWithArgmax`","description":"### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nAccording to [Doc tf.raw_ops.MaxPoolGradWithArgmax](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/MaxPoolGradWithArgmax), the argument `argmax` can be `int32` or `int64`. However, after actual testing, the parameter `argmax` can only support tensor input of data type `int64`.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput_tensor = tf.constant(1, shape=[1, 2, 2, 1], dtype=tf.float32)\r\ngrad_tensor = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.float32)\r\nargmax_indices = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.int32)\r\nksize = [1, 2, 2, 1]\r\nstrides = [1, 1, 1, 1]\r\npadding = \"VALID\"\r\n\r\noutput_grad = tf.raw_ops.MaxPoolGradWithArgmax(\r\n    input=input_tensor,\r\n    grad=grad_tensor,\r\n    argmax=argmax_indices,\r\n    ksize=ksize,\r\n    strides=strides,\r\n    padding=padding,\r\n    include_batch_in_index=False\r\n)\n```\n\n\n### Relevant log output\n\n```shell\nNotFoundError: Could not find device for node: {{node MaxPoolGradWithArgmax}} = MaxPoolGradWithArgmax[T=DT_FLOAT, Targmax=DT_INT32, include_batch_in_index=false, ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 1, 1, 1]]\r\nAll kernels registered for op MaxPoolGradWithArgmax:\r\n  device='CPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT32]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT64]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_HALF]; Targmax in [DT_INT64]\r\n [Op:MaxPoolGradWithArgmax] name:\n```\n","labels":["type:docs-bug","stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-12-12T13:14:53Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82837"},{"repository":"tensorflow\/tensorflow","title":"Update Python in Docker images to 3.11.x and ditch `3.11.0rc1`","description":"### Issue type\n\nBuild\/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nDocker\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe Python version of the docker images is outdated and should be updated\n\n### Standalone code to reproduce the issue\n\n```shell\ndocker run -it tensorflow\/tensorflow python\n```\n\n\n### Relevant log output\n\n```shell\nPython 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","TF 2.18"],"created_at":"2024-12-09T13:35:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82529"},{"repository":"tensorflow\/tensorflow","title":"The warning \"The structure of `inputs` doesn't match the expected structure\" when training a functional model","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.13.1-0-gf841394b1b7 2.13.1 (Nightly: v1.12.1-119104-gf8fd6f53fa3 2.19.0-dev20241204)\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWindows 11 23H2 22631.4460\n\n### Mobile device\n\nWindows 11 23H2 22631.4460\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the model is functional, not Sequential, the warning has occured:\r\n\r\n```\r\nEpoch 1\/5\r\n<path-to-python>\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*\r\n  warnings.warn(\r\n```\r\n\r\nYes, the warning message has interrupted on parenthesis. When I've run the same code in Nightly, the warning message is:\r\n\r\n```\r\nEpoch 1\/5\r\n<path-to-python>\\lib\\site-packages\\keras\\src\\models\\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\r\nExpected: ['keras_tensor']\r\nReceived: inputs=Tensor(shape=(None, 10))\r\n  warnings.warn(msg)\r\n```\r\n\r\nAfter the warning, the training continues normally, but because of this warning, I can't be sure that the model works as I expect.\r\n\r\nI've traced the source and found that in `Lib\\site-packages\\keras\\src\\tree\\optree_impl.py` on line 95 comparasion of expected and actual structure failed. Now I place the traced variables here:\r\n\r\n```\r\n>>> a\r\n<tf.Tensor 'data:0' shape=(None, 10) dtype=float64>\r\n>>> b\r\n[<KerasTensor shape=(None, 10), dtype=float32, sparse=False, name=keras_tensor>]\r\n>>> a_structure\r\nPyTreeSpec(*, NoneIsLeaf)\r\n>>> b_structure\r\nPyTreeSpec([*], NoneIsLeaf)\r\n```\r\n\r\nThe data passed to the `fit` function fully corresponds to the [documentation](https:\/\/keras.io\/api\/models\/model_training_apis\/#fit-method). The warning appears independently of whether I use numpy array or PyDataset as dataset of `fit` function.\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom keras.models import Model\r\nfrom keras.layers import Dense, Input, Flatten, Concatenate\r\nfrom keras import utils\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass SamplesSet(utils.PyDataset):\r\n    \r\n    def __init__(self, batch_size, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.batch_size = batch_size\r\n        \r\n    def __len__(self):\r\n        return 1\r\n    \r\n    def __getitem__(self, idx):\r\n        x1 = np.random.uniform(size=10*self.batch_size).reshape((self.batch_size, 10))\r\n        y = np.arange(self.batch_size)\r\n        return x1, y\r\n    \r\ntrain = SamplesSet(100)\r\nx1_train = np.random.uniform(size=10*100).reshape((100, 10))\r\ny_train = np.arange(100)\r\n\r\ninput1 = Input(shape=(10,))\r\nl1 = Dense(1)(input1)\r\nd2 = Dense(1, activation='sigmoid')(l1)\r\nmodel = Model(inputs=[input1], outputs=[d2])\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\nprint(model.summary())\r\n\r\nhistory = model.fit(x1_train, y_train, epochs=5, verbose=1)\r\n# In the all cases below warning occures too\r\n# history = Model.fit(train, epochs=5, verbose=1) \r\n# ret = model.predict(np.arange(10)[np.newaxis,:])\r\n# ret = model.predict(tf.constant([[0,1,2,3,4,5,6,7,8,9]]))\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","comp:keras","TF 2.13"],"created_at":"2024-12-06T03:47:02Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82372"},{"repository":"tensorflow\/tensorflow","title":"[XLA] TF XLA outputs abnormal value when compiling `Embedding`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nnightly\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nFor `Embedding` operator, when I set the `input_dim=1` (which means it indexed from 0 to 0), the output always returns **0 without XLA**.\r\n\r\nAfter compilation, the outputs are usually some random tensors.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\ntf.random.set_seed(42)\r\nx = tf.constant([1])\r\n\r\n\r\n# uncompiled model\r\nclass Model(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.embedding = tf.keras.layers.Embedding(1, 1)\r\n\r\n    def call(self, x):\r\n        output = self.embedding(x)\r\n        return output\r\n\r\n\r\nm = Model()\r\n\r\noutput1 = m(x)\r\n\r\n\r\n# compiled model\r\nclass Model(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.embedding = tf.keras.layers.Embedding(1, 1)\r\n\r\n    @tf.function(jit_compile=True)\r\n    def call(self, x):\r\n        output = self.embedding(x)\r\n        return output\r\n\r\n\r\nm = Model()\r\noutput2 = m(x)\r\n\r\nprint(output1)\r\nprint(output2)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\ntf.Tensor([[0.]], shape=(1, 1), dtype=float32)\r\ntf.Tensor([[-0.00567592]], shape=(1, 1), dtype=float32)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.18"],"created_at":"2024-12-05T13:58:40Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82317"},{"repository":"tensorflow\/tensorflow","title":"Are checkpoints broken in >= 2.16?","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.16, 2.17\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe example given in https:\/\/www.tensorflow.org\/guide\/checkpoint does not seem to work as expected in 2.16 and 2.17, while working fine in 2.15. After restoring and restarting the training process, it starts training from the very beginning.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1n76Mu5BhdBJBSXc7cXYJMr0lMDER2JRa?usp=sharing\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:apis","2.17"],"created_at":"2024-12-04T15:11:53Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82209"},{"repository":"tensorflow\/tensorflow","title":"TPU not support TensorFlow 2.18 and 2.17.1","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.18 and tf. 2.17.1\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`import tensorflow as tf` results `segmentation fault core dumped`\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:tpus","TF 2.18"],"created_at":"2024-12-04T14:56:53Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82208"},{"repository":"tensorflow\/tensorflow","title":"Clarify the `constant_op.constant(2)` statement","description":"It would be helpful to clarify the `constant_op.constant(2)` statement by explaining the corresponding import statement.\r\n\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/5bc9d26649cca274750ad3625bd93422617eed4b\/tensorflow\/python\/ops\/summary_ops_v2.py#L1062-L1066","labels":["type:bug","comp:ops"],"created_at":"2024-12-02T18:05:41Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/81954"},{"repository":"tensorflow\/tensorflow","title":"MixedPrecision + XLA: Seen floating point types of different precisions","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nGoogle Colab\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\nGoogle Colab default\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen using bilinear interpolation + XLA + mixed_float16 policy issue raises during compilation.\r\nWithout bilinear interpolation or without XLA or without mixed_float16 there is no issue.\r\n\r\nIn Google Colab i have this issue only on CPU with TF 2.17 and both CPU & GPU with TF 2.18.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nhttps:\/\/colab.research.google.com\/drive\/1joSiScbM7Stc9bn1C4R_4sFkDzakrTsJ?usp=sharing\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-4-9cc273be2d5a> in <cell line: 28>()\r\n     26 model.compile(loss='mse', optimizer='adam', run_eagerly=False, jit_compile=True)\r\n     27 \r\n---> 28 model.fit(dataset)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51   try:\r\n     52     ctx.ensure_initialized()\r\n---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                         inputs, attrs, num_outputs)\r\n     55   except core._NotOkStatusException as e:\r\n\r\nInternalError: Graph execution error:\r\n\r\nDetected at node StatefulPartitionedCall defined at (most recent call last):\r\n  File \"\/usr\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\n\r\n  File \"\/usr\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/colab_kernel_launcher.py\", line 37, in <module>\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/traitlets\/config\/application.py\", line 992, in launch_instance\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelapp.py\", line 619, in start\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/platform\/asyncio.py\", line 195, in start\r\n\r\n  File \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\n\r\n  File \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\n\r\n  File \"\/usr\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 685, in <lambda>\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 738, in _run_callback\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 825, in inner\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 377, in dispatch_queue\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 250, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 748, in __init__\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 361, in process_one\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 261, in dispatch_shell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 539, in execute_request\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 2975, in run_cell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3030, in _run_cell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/async_helpers.py\", line 78, in _pseudo_sync_runner\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3257, in run_cell_async\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3473, in run_ast_nodes\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3553, in run_code\r\n\r\n  File \"<ipython-input-4-9cc273be2d5a>\", line 28, in <cell line: 28>\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 368, in fit\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 216, in function\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 129, in multi_step_on_iterator\r\n\r\nduring context [Unknown]: Seen floating point types of different precisions in %multiply.43589 = f32[2,8,8,1280]{3,2,1,0} multiply(f32[2,8,8,1280]{3,2,1,0} %add.43539, f16[2,8,8,1280]{3,2,1,0} %multiply.43588), metadata={op_type=\"Mul\" op_name=\"mul_9\" source_file=\"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/framework\/ops.py\" source_line=1196}, but mixed precision is disallowed.\r\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_124474]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.18"],"created_at":"2024-11-29T07:11:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/81273"},{"repository":"tensorflow\/tensorflow","title":"Some operators give different results on CPU and GPU when dealing with complex numbers that include `inf`.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04.3\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe outputs of TensorFlow mathematical APIs (`sin, cos, tan, sinh, cosh, exp, and reduce_mean`) are inconsistent between the CPU and GPU when applied to complex inputs containing `inf`. \n\n### Standalone code to reproduce the issue\n\n```shell\npython\r\nimport tensorflow as tf\r\n\r\ntest_inputs = [\r\n    tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128),\r\n]\r\n\r\ntest_apis = [\r\n    tf.math.sin, tf.math.cos, tf.math.tan,\r\n    tf.math.sinh, tf.math.cosh, tf.math.exp, tf.math.reduce_mean\r\n]\r\n\r\nfor api in test_apis:\r\n    print(f\"Testing {api.__name__}\")\r\n    for x in test_inputs:\r\n        try:\r\n            with tf.device('\/CPU'):\r\n              cpu_out = api(x)\r\n              print(f\"CPU Output: {cpu_out}\")\r\n            with tf.device('\/GPU:0'):\r\n              gpu_out = api(x)\r\n              print(f\"GPU Output: {gpu_out}\")\r\n        except Exception as e:\r\n            print(f\"Error in {api.__name__}: {e}\")\n```\n\n\n### Relevant log output\n\n```shell\nTesting sin\r\nCPU Output: [nan +0.j  0.+infj nan+infj]\r\nGPU Output: [nan+nanj nan+infj nan+nanj]\r\nTesting cos\r\nCPU Output: [nan +0.j inf -0.j inf+nanj]\r\nGPU Output: [nan+nanj inf+nanj nan+nanj]\r\nTesting tan\r\nCPU Output: [nan+0.j  0.+1.j  0.+1.j]\r\nGPU Output: [nan+nanj  0. +1.j  0. +1.j]\r\nTesting sinh\r\nCPU Output: [inf +0.j  0.+nanj inf+nanj]\r\nGPU Output: [inf+nanj nan+nanj nan+nanj]\r\nTesting cosh\r\nCPU Output: [inf +0.j nan +0.j inf+nanj]\r\nGPU Output: [inf+nanj nan+nanj nan+nanj]\r\nTesting exp\r\nCPU Output: [inf +0.j nan+nanj inf+nanj]\r\nGPU Output: [inf+nanj nan+nanj nan+nanj]\r\nTesting reduce_mean\r\nCPU Output: (inf+infj)\r\nGPU Output: (nan+nanj)\n```\n","labels":["stat:awaiting response","type:bug","stale","comp:ops","2.17"],"created_at":"2024-11-27T13:13:05Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80947"},{"repository":"tensorflow\/tensorflow","title":"When using `tf.math.log1p` and NumPy's `np.log1p` with the same complex input, the outputs are inconsistent.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04.3\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow's `tf.math.log1p` produces inconsistent results with NumPy's `np.log1p` for complex inputs containing `inf`, such as `[inf+0.j, 0+inf.j, inf+inf.j]`. TensorFlow outputs `[inf+0.j, nan+nanj, nan+nanj]`, while NumPy returns `[inf+0.j, inf+1.57079633j, inf+0.78539816j]`.\n\n### Standalone code to reproduce the issue\n\n```shell\npython\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntest_input = tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128)\r\n\r\n# TensorFlow computation\r\nwith tf.device('\/CPU:0'):\r\n    cpu_out = tf.math.log1p(test_input)\r\n\r\n# NumPy computation\r\nnumpy_out = np.log1p(test_input.numpy())\r\n\r\nprint(f\"CPU Output: {cpu_out}\")\r\nprint(f\"NumPy Output: {numpy_out}\")\n```\n\n\n### Relevant log output\n\n```shell\nCPU Output: [inf +0.j nan+nanj nan+nanj]\r\nNumPy Output: [inf+0.j         inf+1.57079633j inf+0.78539816j]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-26T13:36:51Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80850"},{"repository":"tensorflow\/tensorflow","title":"Heap-buffer-overflow in `SparseMatrixSparseCholesky`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen batch_index is scalar, `SparseMatrixSparseCholesky` triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nindices = tf.constant([[0, 0], [1, 1], [2, 1], [2, 2], [3, 3]], dtype=tf.int64)\r\nvalues = tf.constant([1.0, 2.0, 1.0, 3.0, 4.0], tf.float32)\r\ndense_shape = tf.constant([4, 4], dtype=tf.int64)\r\ninput = tf.raw_ops.SparseTensorToCSRSparseMatrix(\r\n    indices=indices, values=values, dense_shape=dense_shape, name=None\r\n)\r\npermutation = tf.constant([4,1,1,1], dtype=tf.int32)\r\n\r\ntf.raw_ops.SparseMatrixSparseCholesky(\r\n  input=input, permutation=permutation, type=tf.float32\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n=================================================================\r\n==3331846==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60700049d1d0 at pc 0x7faa202d6d87 bp 0x7ffd09bf8fe0 sp 0x7ffd09bf8fd0\r\nWRITE of size 4 at 0x60700049d1d0 thread T0\r\n    #0 0x7faa202d6d86 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2a2d86)\r\n    #1 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f373a)\r\n    #2 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f84e9)\r\n    #3 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x4201d41)\r\n    #4 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x26d8d71)\r\n    #5 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2adab6)\r\n    #6 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1cdc14a)\r\n    #7 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1862fe6)\r\n    #8 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x173f306)\r\n    #9 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x179cf74)\r\n    #10 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x17aa8b1)\r\n    #11 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c582eb)\r\n    #12 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aedc2c)\r\n    #13 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31af066a)\r\n    #14 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c2a939)\r\n    #15 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31addde4)\r\n    #16 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31ae1dd4)\r\n    #17 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aebd26)\r\n    #18 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x20ad4c33)\r\n    #19 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c1fe5e)\r\n    #20 0x7fa9f268645b in TFE_Execute (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x1065245b)\r\n    #21 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/_pywrap_tensorflow_internal.so+0x3c2274)\r\n    #22 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0xb7ccb)\r\n    #23 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0x1ce899)\r\n    #24 0x51ad66  (\/usr\/bin\/python3.11+0x51ad66)\r\n    #25 0x4e75db in _PyObject_MakeTpCall (\/usr\/bin\/python3.11+0x4e75db)\r\n    #26 0x4fb151 in _PyEval_EvalFrameDefault (\/usr\/bin\/python3.11+0x4fb151)\r\n    #27 0x531822 in _PyFunction_Vectorcall (\/usr\/bin\/python3.11+0x531822)\r\n    #28 0x541194 in PyObject_Call (\/usr\/bin\/python3.11+0x541194)\r\n    #29 0x4fefe0 in _PyEval_EvalFrameDefault (\/usr\/bin\/python3.11+0x4fefe0)\r\n    #30 0x62e1b3  (\/usr\/bin\/python3.11+0x62e1b3)\r\n    #31 0x4f3a66 in PyEval_EvalCode (\/usr\/bin\/python3.11+0x4f3a66)\r\n    #32 0x647c36  (\/usr\/bin\/python3.11+0x647c36)\r\n    #33 0x64534f  (\/usr\/bin\/python3.11+0x64534f)\r\n    #34 0x650d14  (\/usr\/bin\/python3.11+0x650d14)\r\n    #35 0x650a63 in _PyRun_SimpleFileObject (\/usr\/bin\/python3.11+0x650a63)\r\n    #36 0x650832 in _PyRun_AnyFileObject (\/usr\/bin\/python3.11+0x650832)\r\n    #37 0x64f786 in Py_RunMain (\/usr\/bin\/python3.11+0x64f786)\r\n    #38 0x61ee0c in Py_BytesMain (\/usr\/bin\/python3.11+0x61ee0c)\r\n    #39 0x7faaf80d1d8f  (\/lib\/x86_64-linux-gnu\/libc.so.6+0x29d8f)\r\n    #40 0x7faaf80d1e3f in __libc_start_main (\/lib\/x86_64-linux-gnu\/libc.so.6+0x29e3f)\r\n    #41 0x61ec94 in _start (\/usr\/bin\/python3.11+0x61ec94)\r\n\r\n0x60700049d1d0 is located 0 bytes to the right of 80-byte region [0x60700049d180,0x60700049d1d0)\r\nallocated by thread T0 here:\r\n    #0 0x7faaf84bf887 in __interceptor_malloc ..\/..\/..\/..\/src\/libsanitizer\/asan\/asan_malloc_linux.cpp:145\r\n    #1 0x7faa202d6803 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2a2803)\r\n    #2 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f373a)\r\n    #3 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f84e9)\r\n    #4 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x4201d41)\r\n    #5 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x26d8d71)\r\n    #6 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2adab6)\r\n    #7 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1cdc14a)\r\n    #8 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1862fe6)\r\n    #9 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x173f306)\r\n    #10 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x179cf74)\r\n    #11 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x17aa8b1)\r\n    #12 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c582eb)\r\n    #13 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aedc2c)\r\n    #14 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31af066a)\r\n    #15 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c2a939)\r\n    #16 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31addde4)\r\n    #17 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31ae1dd4)\r\n    #18 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aebd26)\r\n    #19 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x20ad4c33)\r\n    #20 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c1fe5e)\r\n    #21 0x7fa9f268645b in TFE_Execute (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x1065245b)\r\n    #22 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/_pywrap_tensorflow_internal.so+0x3c2274)\r\n    #23 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0xb7ccb)\r\n    #24 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0x1ce899)\r\n    #25 0x51ad66  (\/usr\/bin\/python3.11+0x51ad66)\r\n\r\nSUMMARY: AddressSanitizer: heap-buffer-overflow (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2a2d86) in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const\r\nShadow bytes around the buggy address:\r\n  0x0c0e8008b9e0: 00 00 00 00 00 00 fa fa fa fa fd fd fd fd fd fd\r\n  0x0c0e8008b9f0: fd fd fd fd fa fa fa fa 00 00 00 00 00 00 00 00\r\n  0x0c0e8008ba00: 00 fa fa fa fa fa 00 00 00 00 00 00 00 00 04 fa\r\n  0x0c0e8008ba10: fa fa fa fa 00 00 00 00 00 00 00 00 04 fa fa fa\r\n  0x0c0e8008ba20: fa fa 00 00 00 00 00 00 00 00 04 fa fa fa fa fa\r\n=>0x0c0e8008ba30: 00 00 00 00 00 00 00 00 00 00[fa]fa fa fa fa fa\r\n  0x0c0e8008ba40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba50: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba60: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba70: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\nShadow byte legend (one shadow byte represents 8 application bytes):\r\n  Addressable:           00\r\n  Partially addressable: 01 02 03 04 05 06 07\r\n  Heap left redzone:       fa\r\n  Freed heap region:       fd\r\n  Stack left redzone:      f1\r\n  Stack mid redzone:       f2\r\n  Stack right redzone:     f3\r\n  Stack after return:      f5\r\n  Stack use after scope:   f8\r\n  Global redzone:          f9\r\n  Global init order:       f6\r\n  Poisoned by user:        f7\r\n  Container overflow:      fc\r\n  Array cookie:            ac\r\n  Intra object redzone:    bb\r\n  ASan internal:           fe\r\n  Left alloca redzone:     ca\r\n  Right alloca redzone:    cb\r\n  Shadow gap:              cc\r\n==3331846==ABORTING\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-26T12:42:50Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80847"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `RaggedBincount`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the value of size is close to the maximum value for the dtype, an integer overflow error will occur in following multiplication calculations.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nsplits = tf.constant([0, 3, 5, 9], dtype=tf.int64)\r\nvalues = tf.constant(1, shape=[3,3], dtype=tf.int64)\r\nsize = tf.constant(6522107765268123892, dtype=tf.int64)\r\nweights = tf.constant(1, shape=[3,3], dtype=tf.float32)\r\ncounts = tf.raw_ops.RaggedBincount(splits=splits, values=values, size=size, weights=weights)\n```\n\n\n### Relevant log output\n\n```shell\nStatus: INVALID_ARGUMENT: Encountered overflow when multiplying 3 with 6522107765268123892, result: -1\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-26T03:18:31Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80812"},{"repository":"tensorflow\/tensorflow","title":"This method creates a model with a 100% memory leak loop using model. fit()","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nubuntu 2.2 or mac m1\r\n\r\n### Mobile device\r\n\r\nubuntu 2.2 or mac m1\r\n\r\n### Python version\r\n\r\n3.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport gc\r\n\r\nimport keras\r\nimport numpy as np\r\nimport psutil\r\nfrom keras.optimizers import Adam\r\nfrom keras.layers import Dense, Dropout, Input, LSTM\r\nimport tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom tensorflow.keras.models import Sequential, load_model\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\nimport time\r\nimport json\r\n\r\n\r\nnum_samples = 6\r\nnum_features = 3\r\nnum_classes = 4\r\nepochs = 50\r\nbatch_size = 2\r\nidentifier = \"test_model\"\r\nnum_iterations = 500  \r\n\r\ndef build_model(X, num_classes):\r\n    model = Sequential()\r\n    model.add(Input(shape=(X.shape[1], X.shape[2])))\r\n    model.add(LSTM(16, return_sequences=True))\r\n    model.add(LSTM(16))\r\n    model.add(Dropout(0.4))\r\n    model.add(Dense(8, activation='tanh'))\r\n    model.add(Dense(num_classes, activation='softmax'))\r\n\r\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])\r\n    return model\r\n\r\n\r\ndata_X = np.random.rand(num_samples, num_features)\r\ndata_Y = np.random.randint(0, num_classes, size=(num_samples, 1))  \r\n\r\n\r\ndata_Y = np.eye(num_classes)[data_Y.flatten()]  \r\nprint(type(data_X))\r\n\r\nscaler = MinMaxScaler()\r\ndata_X_scaled = scaler.fit_transform(data_X)\r\n\r\n\r\ntrain_X, test_X, train_Y, test_Y = train_test_split(data_X_scaled, data_Y, train_size=0.6, random_state=42)\r\n\r\n\r\ntrain_X = np.expand_dims(train_X, axis=1)\r\ntest_X = np.expand_dims(test_X, axis=1)\r\n\r\nbest_loss = np.inf\r\nbest_model_data = None\r\nfor iteration in range(num_iterations):\r\n   \r\n    tf.keras.backend.clear_session()\r\n    tf.compat.v1.reset_default_graph()\r\n    model = build_model(train_X, num_classes)\r\n \r\n    model_name = f\"model_{iteration}\"\r\n    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=0, restore_best_weights=True)\r\n    print(f\"Iteration {iteration + 1}\/{num_iterations}\")\r\n    process = psutil.Process()\r\n    mem_info = process.memory_info()\r\n    print(f\"start Current memory usage: {mem_info.rss \/ (1024 * 1024):.2f} MB\")  # RSS - Resident Set Size\r\n    try:\r\n        history = model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, shuffle=True,\r\n                            validation_data=(test_X, test_Y), verbose=0)\r\n        current_loss = history.history['loss'][-1]\r\n        print(f\"Training model: {model.name}\")\r\n    \r\n        del model\r\n        tf.keras.backend.clear_session()\r\n        gc.collect()\r\n    except Exception as e:\r\n        print(\"err:\", e)\r\n    finally:\r\n        process = psutil.Process()\r\n        mem_info = process.memory_info()\r\n        print(f\"end Current memory usage: {mem_info.rss \/ (1024 * 1024):.2f} MB\")  # RSS - Resident Set Size\r\n\r\nprint(\"end\uff01\")\r\n\r\n\r\nif best_model_data:\r\n    model_json = best_model_data[\"model_architecture\"]\r\n    model_weights = json.loads(best_model_data[\"model_weights\"], object_hook=lambda d: np.array(d))\r\n    model = tf.keras.models.model_from_json(model_json)\r\n    model.set_weights(model_weights)\r\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])\r\n    print(\"ok\")\r\nelse:\r\n    print(\"not found\")\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nIteration 1\/500\r\nstart Current memory usage: 450.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 524.41 MB\r\nIteration 2\/500\r\nstart Current memory usage: 524.52 MB\r\nTraining model: sequential\r\nend Current memory usage: 564.97 MB\r\nIteration 3\/500\r\nstart Current memory usage: 564.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 598.00 MB\r\nIteration 4\/500\r\nstart Current memory usage: 598.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 624.69 MB\r\nIteration 5\/500\r\nstart Current memory usage: 624.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 653.89 MB\r\nIteration 6\/500\r\nstart Current memory usage: 653.91 MB\r\nTraining model: sequential\r\nend Current memory usage: 679.45 MB\r\nIteration 7\/500\r\nstart Current memory usage: 679.45 MB\r\nTraining model: sequential\r\nend Current memory usage: 701.59 MB\r\nIteration 8\/500\r\nstart Current memory usage: 701.59 MB\r\nTraining model: sequential\r\nend Current memory usage: 726.83 MB\r\nIteration 9\/500\r\nstart Current memory usage: 726.84 MB\r\nTraining model: sequential\r\nend Current memory usage: 749.56 MB\r\nIteration 10\/500\r\nstart Current memory usage: 749.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 782.56 MB\r\nIteration 11\/500\r\nstart Current memory usage: 782.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 805.92 MB\r\nIteration 12\/500\r\nstart Current memory usage: 805.92 MB\r\nTraining model: sequential\r\nend Current memory usage: 833.17 MB\r\nIteration 13\/500\r\nstart Current memory usage: 833.17 MB\r\nTraining model: sequential\r\nend Current memory usage: 852.84 MB\r\nIteration 14\/500\r\nstart Current memory usage: 852.84 MB\r\nTraining model: sequential\r\nend Current memory usage: 875.05 MB\r\nIteration 15\/500\r\nstart Current memory usage: 875.06 MB\r\nTraining model: sequential\r\nend Current memory usage: 901.56 MB\r\nIteration 16\/500\r\nstart Current memory usage: 901.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 930.62 MB\r\nIteration 17\/500\r\nstart Current memory usage: 705.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 762.64 MB\r\nIteration 18\/500\r\nstart Current memory usage: 762.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 798.06 MB\r\nIteration 19\/500\r\nstart Current memory usage: 798.17 MB\r\nTraining model: sequential\r\nend Current memory usage: 824.98 MB\r\nIteration 20\/500\r\nstart Current memory usage: 824.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 850.34 MB\r\nIteration 21\/500\r\nstart Current memory usage: 850.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 876.81 MB\r\nIteration 22\/500\r\nstart Current memory usage: 876.81 MB\r\nTraining model: sequential\r\nend Current memory usage: 904.02 MB\r\nIteration 23\/500\r\nstart Current memory usage: 904.08 MB\r\nTraining model: sequential\r\nend Current memory usage: 929.70 MB\r\nIteration 24\/500\r\nstart Current memory usage: 929.73 MB\r\nTraining model: sequential\r\nend Current memory usage: 952.33 MB\r\nIteration 25\/500\r\nstart Current memory usage: 952.34 MB\r\nTraining model: sequential\r\nend Current memory usage: 952.28 MB\r\nIteration 26\/500\r\nstart Current memory usage: 952.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 980.39 MB\r\nIteration 27\/500\r\nstart Current memory usage: 978.78 MB\r\nTraining model: sequential\r\nend Current memory usage: 999.02 MB\r\nIteration 28\/500\r\nstart Current memory usage: 999.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1023.50 MB\r\nIteration 29\/500\r\nstart Current memory usage: 1023.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1047.80 MB\r\nIteration 30\/500\r\nstart Current memory usage: 1047.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1068.88 MB\r\nIteration 31\/500\r\nstart Current memory usage: 1068.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1095.78 MB\r\nIteration 32\/500\r\nstart Current memory usage: 1095.78 MB\r\nTraining model: sequential\r\nend Current memory usage: 1119.03 MB\r\nIteration 33\/500\r\nstart Current memory usage: 1119.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 1039.41 MB\r\nIteration 34\/500\r\nstart Current memory usage: 1022.78 MB\r\nTraining model: sequential\r\nend Current memory usage: 1040.88 MB\r\nIteration 35\/500\r\nstart Current memory usage: 1040.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 1054.58 MB\r\nIteration 36\/500\r\nstart Current memory usage: 1054.58 MB\r\nTraining model: sequential\r\nend Current memory usage: 1076.16 MB\r\nIteration 37\/500\r\nstart Current memory usage: 1076.19 MB\r\nTraining model: sequential\r\nend Current memory usage: 1097.02 MB\r\nIteration 38\/500\r\nstart Current memory usage: 1097.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 1113.70 MB\r\nIteration 39\/500\r\nstart Current memory usage: 1114.12 MB\r\nTraining model: sequential\r\nend Current memory usage: 1140.30 MB\r\nIteration 40\/500\r\nstart Current memory usage: 1140.33 MB\r\nTraining model: sequential\r\nend Current memory usage: 1163.81 MB\r\nIteration 41\/500\r\nstart Current memory usage: 1163.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 1195.83 MB\r\nIteration 42\/500\r\nstart Current memory usage: 1195.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1221.53 MB\r\nIteration 43\/500\r\nstart Current memory usage: 1221.55 MB\r\nTraining model: sequential\r\nend Current memory usage: 1231.09 MB\r\nIteration 44\/500\r\nstart Current memory usage: 1231.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1245.78 MB\r\nIteration 45\/500\r\nstart Current memory usage: 1199.55 MB\r\nTraining model: sequential\r\nend Current memory usage: 1221.59 MB\r\nIteration 46\/500\r\nstart Current memory usage: 1221.59 MB\r\nTraining model: sequential\r\nend Current memory usage: 1249.11 MB\r\nIteration 47\/500\r\nstart Current memory usage: 1249.22 MB\r\nTraining model: sequential\r\nend Current memory usage: 1275.50 MB\r\nIteration 48\/500\r\nstart Current memory usage: 1259.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1290.91 MB\r\nIteration 49\/500\r\nstart Current memory usage: 1285.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1296.75 MB\r\nIteration 50\/500\r\nstart Current memory usage: 1296.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1306.59 MB\r\nIteration 51\/500\r\nstart Current memory usage: 1306.59 MB\r\nTraining model: sequential\r\nend Current memory usage: 1287.53 MB\r\nIteration 52\/500\r\nstart Current memory usage: 1287.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1297.23 MB\r\nIteration 53\/500\r\nstart Current memory usage: 1297.25 MB\r\nTraining model: sequential\r\nend Current memory usage: 1285.45 MB\r\nIteration 54\/500\r\nstart Current memory usage: 1285.45 MB\r\nTraining model: sequential\r\nend Current memory usage: 1290.36 MB\r\nIteration 55\/500\r\nstart Current memory usage: 1282.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1302.14 MB\r\nIteration 56\/500\r\nstart Current memory usage: 1302.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1287.70 MB\r\nIteration 57\/500\r\nstart Current memory usage: 1287.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1282.77 MB\r\nIteration 58\/500\r\nstart Current memory usage: 1271.38 MB\r\nTraining model: sequential\r\nend Current memory usage: 1232.14 MB\r\nIteration 59\/500\r\nstart Current memory usage: 1212.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 1201.16 MB\r\nIteration 60\/500\r\nstart Current memory usage: 1200.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1169.45 MB\r\nIteration 61\/500\r\nstart Current memory usage: 1169.45 MB\r\nTraining model: sequential\r\nend Current memory usage: 1209.73 MB\r\nIteration 62\/500\r\nstart Current memory usage: 1207.19 MB\r\nTraining model: sequential\r\nend Current memory usage: 1226.28 MB\r\nIteration 63\/500\r\nstart Current memory usage: 1226.28 MB\r\nTraining model: sequential\r\nend Current memory usage: 1231.45 MB\r\nIteration 64\/500\r\nstart Current memory usage: 1210.11 MB\r\nTraining model: sequential\r\nend Current memory usage: 1176.00 MB\r\nIteration 65\/500\r\nstart Current memory usage: 1173.97 MB\r\nTraining model: sequential\r\nend Current memory usage: 1201.42 MB\r\nIteration 66\/500\r\nstart Current memory usage: 1201.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 1223.94 MB\r\nIteration 67\/500\r\nstart Current memory usage: 1222.50 MB\r\nTraining model: sequential\r\nend Current memory usage: 1229.80 MB\r\nIteration 68\/500\r\nstart Current memory usage: 1227.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1219.02 MB\r\nIteration 69\/500\r\nstart Current memory usage: 1210.48 MB\r\nTraining model: sequential\r\nend Current memory usage: 1247.17 MB\r\nIteration 70\/500\r\nstart Current memory usage: 1245.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1259.84 MB\r\nIteration 71\/500\r\nstart Current memory usage: 1259.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 1286.39 MB\r\nIteration 72\/500\r\nstart Current memory usage: 1286.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1316.52 MB\r\nIteration 73\/500\r\nstart Current memory usage: 1311.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1338.72 MB\r\nIteration 74\/500\r\nstart Current memory usage: 1338.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1348.45 MB\r\nIteration 75\/500\r\nstart Current memory usage: 1338.30 MB\r\nTraining model: sequential\r\nend Current memory usage: 1354.97 MB\r\nIteration 76\/500\r\nstart Current memory usage: 1353.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1385.67 MB\r\nIteration 77\/500\r\nstart Current memory usage: 1385.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1408.83 MB\r\nIteration 78\/500\r\nstart Current memory usage: 1408.88 MB\r\nTraining model: sequential\r\nend Current memory usage: 1430.91 MB\r\nIteration 79\/500\r\nstart Current memory usage: 1430.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1443.62 MB\r\nIteration 80\/500\r\nstart Current memory usage: 1428.00 MB\r\nTraining model: sequential\r\nend Current memory usage: 1436.50 MB\r\nIteration 81\/500\r\nstart Current memory usage: 1436.64 MB\r\nTraining model: sequential\r\nend Current memory usage: 1454.66 MB\r\nIteration 82\/500\r\nstart Current memory usage: 1440.91 MB\r\nTraining model: sequential\r\nend Current memory usage: 1461.81 MB\r\nIteration 83\/500\r\nstart Current memory usage: 1460.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 1481.19 MB\r\nIteration 84\/500\r\nstart Current memory usage: 1481.19 MB\r\nTraining model: sequential\r\nend Current memory usage: 1477.84 MB\r\nIteration 85\/500\r\nstart Current memory usage: 1477.84 MB\r\nTraining model: sequential\r\nend Current memory usage: 1493.55 MB\r\nIteration 86\/500\r\nstart Current memory usage: 1493.58 MB\r\nTraining model: sequential\r\nend Current memory usage: 1509.50 MB\r\nIteration 87\/500\r\nstart Current memory usage: 1509.50 MB\r\nTraining model: sequential\r\nend Current memory usage: 1543.94 MB\r\nIteration 88\/500\r\nstart Current memory usage: 1542.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1516.17 MB\r\nIteration 89\/500\r\nstart Current memory usage: 1516.20 MB\r\nTraining model: sequential\r\nend Current memory usage: 1470.17 MB\r\nIteration 90\/500\r\nstart Current memory usage: 1470.22 MB\r\nTraining model: sequential\r\nend Current memory usage: 1443.72 MB\r\nIteration 91\/500\r\nstart Current memory usage: 1444.36 MB\r\nTraining model: sequential\r\nend Current memory usage: 1486.23 MB\r\nIteration 92\/500\r\nstart Current memory usage: 1476.41 MB\r\nTraining model: sequential\r\nend Current memory usage: 1524.97 MB\r\nIteration 93\/500\r\nstart Current memory usage: 1524.97 MB\r\nTraining model: sequential\r\nend Current memory usage: 1534.94 MB\r\nIteration 94\/500\r\nstart Current memory usage: 1551.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1853.48 MB\r\nIteration 95\/500\r\nstart Current memory usage: 1853.48 MB\r\nTraining model: sequential\r\nend Current memory usage: 1790.12 MB\r\nIteration 96\/500\r\nstart Current memory usage: 1792.27 MB\r\nTraining model: sequential\r\nend Current memory usage: 1883.20 MB\r\nIteration 97\/500\r\nstart Current memory usage: 1879.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1759.69 MB\r\nIteration 98\/500\r\nstart Current memory usage: 1669.66 MB\r\nTraining model: sequential\r\nend Current memory usage: 1596.77 MB\r\nIteration 99\/500\r\nstart Current memory usage: 1597.12 MB\r\nTraining model: sequential\r\nend Current memory usage: 1568.83 MB\r\nIteration 100\/500\r\nstart Current memory usage: 1532.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1516.75 MB\r\nIteration 101\/500\r\nstart Current memory usage: 1465.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1486.66 MB\r\nIteration 102\/500\r\nstart Current memory usage: 1483.34 MB\r\nTraining model: sequential\r\nend Current memory usage: 1523.19 MB\r\nIteration 103\/500\r\nstart Current memory usage: 1523.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1532.77 MB\r\nIteration 104\/500\r\nstart Current memory usage: 1531.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1561.78 MB\r\nIteration 105\/500\r\nstart Current memory usage: 1555.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1586.70 MB\r\nIteration 106\/500\r\nstart Current memory usage: 1586.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1608.41 MB\r\nIteration 107\/500\r\nstart Current memory usage: 1603.81 MB\r\nTraining model: sequential\r\nend Current memory usage: 1629.00 MB\r\nIteration 108\/500\r\nstart Current memory usage: 1629.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1609.25 MB\r\nIteration 109\/500\r\nstart Current memory usage: 1609.31 MB\r\nTraining model: sequential\r\nend Current memory usage: 1630.09 MB\r\nIteration 110\/500\r\nstart Current memory usage: 1629.20 MB\r\nTraining model: sequential\r\nend Current memory usage: 1638.66 MB\r\nIteration 111\/500\r\nstart Current memory usage: 1620.30 MB\r\nTraining model: sequential\r\nend Current memory usage: 1642.81 MB\r\nIteration 112\/500\r\nstart Current memory usage: 1642.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1659.45 MB\r\nIteration 113\/500\r\nstart Current memory usage: 1655.17 MB\r\nTraining model: sequential\r\nend Current memory usage: 1687.80 MB\r\nIteration 114\/500\r\nstart Current memory usage: 1673.33 MB\r\nTraining model: sequential\r\nend Current memory usage: 1705.94 MB\r\nIteration 115\/500\r\nstart Current memory usage: 1699.95 MB\r\nTraining model: sequential\r\nend Current memory usage: 1708.22 MB\r\nIteration 116\/500\r\nstart Current memory usage: 1707.88 MB\r\nTraining model: sequential\r\nend Current memory usage: 1648.23 MB\r\nIteration 117\/500\r\nstart Current memory usage: 1634.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 1670.97 MB\r\nIteration 118\/500\r\nstart Current memory usage: 1671.97 MB\r\nTraining model: sequential\r\nend Current memory usage: 1649.69 MB\r\nIteration 119\/500\r\nstart Current memory usage: 1645.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1698.64 MB\r\nIteration 120\/500\r\nstart Current memory usage: 1699.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1737.67 MB\r\nIteration 121\/500\r\nstart Current memory usage: 1737.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1738.05 MB\r\nIteration 122\/500\r\nstart Current memory usage: 1721.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 1730.64 MB\r\nIteration 123\/500\r\nstart Current memory usage: 1729.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1766.12 MB\r\nIteration 124\/500\r\nstart Current memory usage: 1761.22 MB\r\nTraining model: sequential\r\nend Current memory usage: 1796.58 MB\r\nIteration 125\/500\r\nstart Current memory usage: 1796.73 MB\r\nTraining model: sequential\r\nend Current memory usage: 1709.02 MB\r\nIteration 126\/500\r\nstart Current memory usage: 1721.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1771.50 MB\r\nIteration 127\/500\r\nstart Current memory usage: 1771.50 MB\r\nTraining model: sequential\r\nend Current memory usage: 1777.38 MB\r\nIteration 128\/500\r\nstart Current memory usage: 1757.58 MB\r\nTraining model: sequential\r\nend Current memory usage: 1806.50 MB\r\nIteration 129\/500\r\nstart Current memory usage: 1758.81 MB\r\nTraining model: sequential\r\nend Current memory usage: 1812.45 MB\r\nIteration 130\/500\r\nstart Current memory usage: 1812.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 1811.14 MB\r\nIteration 131\/500\r\nstart Current memory usage: 1799.61 MB\r\nTraining model: sequential\r\nend Current memory usage: 1835.33 MB\r\nIteration 132\/500\r\nstart Current memory usage: 1716.38 MB\r\nTraining model: sequential\r\nend Current memory usage: 1759.75 MB\r\nIteration 133\/500\r\nstart Current memory usage: 1752.44 MB\r\nTraining model: sequential\r\nend Current memory usage: 1818.41 MB\r\nIteration 134\/500\r\nstart Current memory usage: 1811.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 1853.58 MB\r\nIteration 135\/500\r\nstart Current memory usage: 1853.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 1858.50 MB\r\nIteration 136\/500\r\nstart Current memory usage: 1858.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 1874.84 MB\r\nIteration 137\/500\r\nstart Current memory usage: 1862.92 MB\r\nTraining model: sequential\r\nend Current memory usage: 1768.23 MB\r\nIteration 138\/500\r\nstart Current memory usage: 1762.73 MB\r\nTraining model: sequential\r\nend Current memory usage: 1843.39 MB\r\nIteration 139\/500\r\nstart Current memory usage: 1843.52 MB\r\nTraining model: sequential\r\nend Current memory usage: 1885.88 MB\r\nIteration 140\/500\r\nstart Current memory usage: 1885.95 MB\r\nTraining model: sequential\r\nend Current memory usage: 1924.86 MB\r\nIteration 141\/500\r\nstart Current memory usage: 1925.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1946.80 MB\r\nIteration 142\/500\r\nstart Current memory usage: 1946.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1977.53 MB\r\nIteration 143\/500\r\nstart Current memory usage: 1974.27 MB\r\nTraining model: sequential\r\nend Current memory usage: 1995.17 MB\r\nIteration 144\/500\r\nstart Current memory usage: 1992.41 MB\r\nTraining model: sequential\r\nend Current memory usage: 1984.45 MB\r\nIteration 145\/500\r\nstart Current memory usage: 1963.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 1947.31 MB\r\nIteration 146\/500\r\nstart Current memory usage: 1944.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 1996.00 MB\r\nIteration 147\/500\r\nstart Current memory usage: 1996.08 MB\r\nTraining model: sequential\r\nend Current memory usage: 2008.41 MB\r\nIteration 148\/500\r\nstart Current memory usage: 1999.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1951.30 MB\r\nIteration 149\/500\r\nstart Current memory usage: 1942.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1992.28 MB\r\nIteration 150\/500\r\nstart Current memory usage: 1982.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 2008.83 MB\r\nIteration 151\/500\r\nstart Current memory usage: 2008.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1946.42 MB\r\nIteration 152\/500\r\nstart Current memory usage: 1946.92 MB\r\nTraining model: sequential\r\nend Current memory usage: 1992.48 MB\r\nIteration 153\/500\r\nstart Current memory usage: 1979.52 MB\r\nTraining model: sequential\r\nend Current memory usage: 2035.66 MB\r\nIteration 154\/500\r\nstart Current memory usage: 2023.91 MB\r\nTraining model: sequential\r\nend Current memory usage: 2030.31 MB\r\nIteration 155\/500\r\nstart Current memory usage: 1974.39 MB\r\nTraining model: sequential\r\nend Current memory usage: 2029.30 MB\r\nIteration 156\/500\r\nstart Current memory usage: 1997.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 2000.31 MB\r\nIteration 157\/500\r\nstart Current memory usage: 1964.38 MB\r\nTraining model: sequential\r\nend Current memory usage: 1979.45 MB\r\nIteration 158\/500\r\nstart Current memory usage: 1973.12 MB\r\nTraining model: sequential\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF 2.18"],"created_at":"2024-11-25T12:12:55Z","comments":7,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80753"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.SparseConcat`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific inputs, `SparseConcat` triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nindices1 = tf.constant(2, shape=[3,3], dtype=tf.int64)\r\nvalues1 = tf.constant(\"aaaabaaacaaadaaaeaaafaaagaaahaaaiaaajaaakaaalaaamaaanaaaoaaapaaaqaaaraaasaaataaauaaavaaawaaaxaaayaaazaabbaabcaabdaabeaabfaabgaabhaabiaabjaabkaablaabmaabnaaboaabpaabqaabraabsaabtaabuaabvaabwaabxaabyaabzaacbaaccaacdaaceaacfaacgaachaaciaacjaackaaclaacmaacnaacoaacpaacqaacraacsaactaacuaacvaacwaacxaacyaac\",\r\n    shape=[3], dtype=tf.string)\r\nshapes1 = tf.constant([5, 2, 2147483647], dtype=tf.int64)\r\n\r\nindices2 = tf.constant(-2, shape=[4,3], dtype=tf.int64)\r\nvalues2 = tf.constant(\" \", shape=[4], dtype=tf.string)\r\nshapes2 = tf.constant([5,1879048192,536870912], dtype=tf.int64)\r\n\r\nconcat_dim = 1\r\ntf.raw_ops.SparseConcat(\r\n    indices=[indices1, indices2], values=[values1, values2], shapes=[shapes1, shapes2], concat_dim=concat_dim, name=None\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-11-24 06:36:10.994508: F tensorflow\/core\/framework\/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements() status: INVALID_ARGUMENT: Shape [5,1879048194,2147483647] results in overflow when computing number of elements\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-24T06:40:09Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80669"},{"repository":"tensorflow\/tensorflow","title":"Floating point exception (core dumped) in `tf.raw_ops.Reshape`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific inputs,  `tf.raw_ops.Reshape` triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntensor = tf.constant(-3.5e+35, shape=[5], dtype=tf.float32)\r\nshape = tf.constant([0, 1879048192, 100000000, 1610612736, -1], dtype=tf.int32)\r\n\r\ntf.raw_ops.Reshape(tensor=tensor, shape=shape)\n```\n\n\n### Relevant log output\n\n```shell\nFloating point exception (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-22T02:58:01Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80529"},{"repository":"tensorflow\/tensorflow","title":"The documentation in `data_performance.ipynb` uses `py_function()` without an explanation","description":"### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nTF 2.18\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIn the [\"Better performance with the tf.data API\" guide](https:\/\/www.tensorflow.org\/guide\/data_performance), `tf.py_function()` is used several times in the mapping function. There are various performance boosts demonstrated by the guide with said mapping functions. However, there does not seem to be any practical reason why `tf.py_function()` is used inside the mapping functions. In fact, if you remove those, the behavior is the same; in other words, there doesn't seem to be a need for them at all.\r\n\r\nCuriously, if you remove them from the examples and then perform the time measurements, the speedup goes away. For example, consider the following mapping function from [the guide](https:\/\/github.com\/tensorflow\/docs\/blob\/bbc0b9c70fc0bd4411793d1b0bcc56ef1dbc2405\/site\/en\/guide\/data_performance.ipynb#L447-L450):\r\n\r\n```python\r\ndef mapped_function(s):\r\n    # Do some hard pre-processing\r\n    tf.py_function(lambda: time.sleep(0.03), [], ())\r\n    return s\r\n```\r\n\r\nIn the sequential case, it is shown that the following mapping code results in a time measurement of 0.49222556600034295:\r\n\r\n```python\r\nbenchmark(\r\n    ArtificialDataset()\r\n    .map(mapped_function)\r\n)\r\n```\r\n\r\nAnd, the optimized (parallel) version as follows results in a time measurement of 0.36392719900049997:\r\n\r\n```python\r\nbenchmark(\r\n    ArtificialDataset()\r\n    .map(\r\n        mapped_function,\r\n        num_parallel_calls=tf.data.AUTOTUNE\r\n    )\r\n)\r\n```\r\n\r\nBut, if I remove the `tf.py_function()` from the mapping function, I get comparable measurements from both examples, namely, 0.22448736599994845 and 0.2266392660001202:\r\n\r\n```python\r\ndef mapped_function(s):\r\n    # Do some hard pre-processing\r\n    lambda: time.sleep(0.03), [], ()\r\n    return s\r\n```\r\n\r\nIn fact, that measurement is even better, which makes me believe that this example is contrived to show a performance benefit by using `num_parallel_calls` when in fact TF is already optimizing the code without it. Frivolously wrapping the function in `tf.py_function()` is most likely  causing TensorFlow *not* to optimize the function. Thus, is `num_parallel_calls` even needed to achieve better performance?\n\n### Standalone code to reproduce the issue\n\n```shell\nMentioned in the above description.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:docs-bug","stat:awaiting tensorflower","type:bug"],"created_at":"2024-11-20T16:01:24Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80365"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.MatrixSolve`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04.3 LTS (x86_64)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen the shape of the matrix argument is empty and the gpu is available, tf.raw_ops.MatrixSolve triggers a crash.\r\nIt can be reproduced on tf-nightly when the gpu is available.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\ntf.raw_ops.MatrixSolve(matrix=tf.random.uniform([], dtype=tf.dtypes.double, maxval=1000000000), rhs=tf.random.uniform([1, 2], dtype=tf.dtypes.double, maxval=1000000000), adjoint=True)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-11-20 14:51:08.714846: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-11-20 14:51:08.775383: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-11-20 14:51:08.852267: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-11-20 14:51:08.876168: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-20 14:51:08.931206: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-11-20 14:51:16.385650: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-11-20 14:51:16.387914: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-11-20 14:51:16.542383: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-20T06:53:21Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80331"}]