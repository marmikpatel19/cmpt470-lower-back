[{"repository":"tensorflow\/tensorflow","title":"`gradient_checker.compute_gradient` can cause a crash","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20241025\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `Floating point exception` issue in TensorFlow when I used API `gradient_checker.compute_gradient`. I have confirmed that below code would crash on tf-nightly 2.19.0-dev20241025 (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1Ow6DQI7g-s7LpKUM1fy8OXbNcfw1ZH7r?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport math\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import errors_impl\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.framework import test_util\nfrom tensorflow.python.ops import gen_nn_ops\nfrom tensorflow.python.ops import gradient_checker\nfrom tensorflow.python.ops import gradients_impl\nfrom tensorflow.python.ops import nn_ops\nimport tensorflow.python.ops.nn_grad  # pylint: disable=unused-import\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging\nfrom tensorflow.python.util.compat import collections_abc\nfrom tensorflow.python.eager import context\ndef DtypesToTest(use_gpu):\n  # double datatype is currently not supported for convolution ops\n  # on the ROCm platform\n  optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]\n  if use_gpu:\n    if not test_util.GpuSupportsHalfMatMulAndConv():\n      return optional_float64 + [dtypes.float32]\n    else:\n      # It is important that float32 comes before float16 here,\n      # as we will be using its gradients as reference for fp16 gradients.\n      return optional_float64 + [dtypes.float32, dtypes.float16]\n  else:\n    return optional_float64 + [dtypes.float32, dtypes.float16, dtypes.bfloat16]\ndef _ConstructAndTestGradientForConfig(\n    batch, input_shape, filter_shape, in_depth, out_depth, stride,\n    padding, test_input, data_format, use_gpu):\n  input_planes, input_rows, input_cols = input_shape\n  filter_planes, filter_rows, filter_cols = filter_shape\n  input_shape = [batch, input_planes, input_rows, input_cols, in_depth]\n  filter_shape = [\n      filter_planes, filter_rows, filter_cols, in_depth, out_depth\n  ]\n  if isinstance(stride, collections_abc.Iterable):\n    strides = [1] + list(stride) + [1]\n  else:\n    strides = [1, stride, stride, stride, 1]\n  if padding == \"VALID\":\n    output_planes = int(\n        math.ceil((input_planes - filter_planes + 1.0) \/ strides[1]))\n    output_rows = int(\n        math.ceil((input_rows - filter_rows + 1.0) \/ strides[2]))\n    output_cols = int(\n        math.ceil((input_cols - filter_cols + 1.0) \/ strides[3]))\n  else:\n    output_planes = int(math.ceil(float(input_planes) \/ strides[1]))\n    output_rows = int(math.ceil(float(input_rows) \/ strides[2]))\n    output_cols = int(math.ceil(float(input_cols) \/ strides[3]))\n  output_shape = [batch, output_planes, output_rows, output_cols, out_depth]\n  input_size = 1\n  for x in input_shape:\n    input_size *= x\n  filter_size = 1\n  for x in filter_shape:\n    filter_size *= x\n  input_data = [x * 1.0 \/ input_size for x in range(0, input_size)]\n  filter_data = [x * 1.0 \/ filter_size for x in range(0, filter_size)]\n  for data_type in DtypesToTest(use_gpu=use_gpu):\n    # TODO(mjanusz): Modify gradient_checker to also provide max relative\n    # error and synchronize the tolerance levels between the tests for forward\n    # and backward computations.\n    if data_type == dtypes.float64:\n      tolerance = 1e-8\n    elif data_type == dtypes.float32:\n      tolerance = 5e-3\n    elif data_type == dtypes.float16:\n      tolerance = 5e-3 if test.is_built_with_rocm() else 1e-3\n    elif data_type == dtypes.bfloat16:\n      tolerance = 1e-2\n    sess = tf.compat.v1.Session()\n    with sess.as_default():\n      orig_input_tensor = constant_op.constant(\n          input_data, shape=input_shape, dtype=data_type, name=\"input\")\n      filter_tensor = constant_op.constant(\n          filter_data, shape=filter_shape, dtype=data_type, name=\"filter\")\n      if data_format == \"NCDHW\":\n        input_tensor = test_util.NHWCToNCHW(orig_input_tensor)\n        new_strides = test_util.NHWCToNCHW(strides)\n      else:\n        input_tensor = orig_input_tensor\n        new_strides = strides\n      conv = nn_ops.conv3d(\n          input_tensor,\n          filter_tensor,\n          new_strides,\n          padding,\n          data_format=data_format,\n          name=\"conv\")\n      jacob_t, jacob_n = gradient_checker.compute_gradient(\n          orig_input_tensor, input_shape, conv, output_shape)\n\nwith context.graph_mode():\n  _ConstructAndTestGradientForConfig(data_format=\"NDHWC\",use_gpu=False,batch=2, input_shape=(3, 7, 6), filter_shape=(3, 3, 3), in_depth=2, out_depth=0, stride=3, padding='VALID', test_input=True)\n```\n\n### Relevant log output\n\n```shell\nFatal Python error: Floating point exception\n```","labels":["type:bug","type:support"],"created_at":"2025-02-11T16:38:56Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/87063"},{"repository":"tensorflow\/tensorflow","title":"TPU nan issue","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux (google colab default)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIn the 2.18.0 Tensorflow version, occur only when using TPU:\n- At the middle of any epoch, loss turns out to be nan for the rest of the epoch \n\n### Standalone code to reproduce the issue\n\n```shell\nShortest way: connect and connect to the colab tutorial on TPU, i.e. https:\/\/colab.research.google.com\/github\/tensorflow\/docs\/blob\/master\/site\/en\/guide\/tpu.ipynb#scrollTo=Tce3stUlHN0L\nIf the issue hasn't been fixed, the training loop will produce nan loss\n```\n\n### Relevant log output\n\n```shell\nEpoch 1\/5\n300\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 22s 47ms\/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan\nEpoch 2\/5\n300\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 30ms\/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan\nEpoch 3\/5\n300\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 30ms\/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan\nEpoch 4\/5\n231\/300 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 23ms\/step - loss: nan - sparse_categorical_accuracy: nan\n```","labels":["type:bug","comp:tpus","TF 2.18"],"created_at":"2025-02-10T15:03:57Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86953"},{"repository":"tensorflow\/tensorflow","title":"`tf.summary_ops.write` aborts with \"Check failed: 1 == NumElements() (1 vs. 4)\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20250207\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.write` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/13pz-hZNK_BaOnbYo_9CT2YMdoncPXEA3?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops import summary_ops_v2 as summary_ops\nfrom tensorflow.python.ops import variables\nwriter = summary_ops.create_file_writer_v2(\"\/tmp\")\nmystep = variables.Variable(1, dtype=dtypes.int64)\nwith writer.as_default(step=[3, 0, 0, 2]):\n    summary_ops.write('tag', 1.0)\n```\n\n### Relevant log output\n\n```shell\n2025-02-09 04:47:35.482562: F tensorflow\/core\/framework\/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor\nAborted (core dumped)\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-09T04:49:39Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86918"},{"repository":"tensorflow\/tensorflow","title":"`tf.summary_ops.run_metadata_graphs` aborts with \"Check failed: 1 == NumElements() (1 vs. 4)\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20250207\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.run_metadata_graphs` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1qCZb2sPj2l79IecGA1tz9CywtNlevAjf?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.ops import summary_ops_v2 as summary_ops\nfrom tensorflow.core.protobuf import config_pb2\nwriter = summary_ops.create_file_writer_v2(\"\/tmp\")\nmeta = config_pb2.RunMetadata()\nwith writer.as_default([3, 0, 0, 2]):\n    summary_ops.run_metadata_graphs(name='my_name', data=meta)\n```\n\n### Relevant log output\n\n```shell\n2025-02-09 04:39:36.666278: F tensorflow\/core\/framework\/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor\nAborted (core dumped)\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-09T04:42:49Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86917"},{"repository":"tensorflow\/tensorflow","title":"`io_ops.restore_v2` aborts with \"Check failed: size >= 0 (0 vs. -3) \"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.19.0-dev20250207\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI encountered an `aborted issue` in TensorFlow when I used API `tf.io_ops.restore_v2` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20250207` (nightly-build).\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/17XwNF4WI3HqVJkLwk2VQjcTDAV42-6Bl?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import io_ops\n\ndtype = dtypes.uint4\nwith ops.Graph().as_default():\n    op = io_ops.restore_v2('model', ['var1', 'var2'], ['', '-3 4 0,1:-'], [dtype, dtype])\n```\n\n### Relevant log output\n\n```shell\n2025-02-09 04:25:19.857968: F tensorflow\/core\/framework\/tensor_shape.cc:413] Check failed: size >= 0 (0 vs. -3) \nAborted (core dumped)\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-09T04:34:07Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86916"},{"repository":"tensorflow\/tensorflow","title":"Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMacos 15.3 (worker 0) Macos 12.7.6(worker 1)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8.20\n\n### Bazel version\n\n...\n\n### GCC\/compiler version\n\n16.0.0 (apple M3) 14.0.0 (intel iris)\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nApple M3 and Intel Iris Graphics 6100\n\n### Current behavior?\n\nWhen I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  #Build the model under the strategy\n\nNo error message is displayed, but the process no longer progresses after\n\n\u2022\t I followed the recommendations of the official documentation, but the problem persists.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport json\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nTF_CONFIG = {\n    \"cluster\": {\n        \"worker\": [\"192.168.0.68:12345\", \"192.168.0.68:12346\"]\n    },\n    \"task\": {\"type\": \"worker\", \"index\": 0}  # Modifier index pour chaque worker\n}\n\nos.environ[\"TF_CONFIG\"] = json.dumps(TF_CONFIG)\n\n# Manually Load the MNIST dataset\ndata = np.load(\"mnist.npz\")\nx_train, y_train = data[\"x_train\"], data[\"y_train\"]\nx_test, y_test = data[\"x_test\"], data[\"y_test\"]\n\n# Normalize images\nx_train, x_test = x_train \/ 255.0, x_test \/ 255.0\n\n# Add a dimension to match TensorFlow's expectations\nx_train = x_train[..., np.newaxis]\nx_test = x_test[..., np.newaxis]\n\n# Define the distribution strategy\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\n\n# Build the model under the strategy\nwith strategy.scope():\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n        tf.keras.layers.MaxPooling2D((2, 2)),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n\ntest_loss, test_acc = model.evaluate(x_test, y_test)\nprint(f\"Test accuracy: {test_acc:.4f}\")\n```\n\n### Relevant log output\n\n```shell\n2025-02-08 12:29:20.630247: I metal_plugin\/src\/device\/metal_device.cc:1154] Metal device set to: Apple M3\n2025-02-08 12:29:20.630286: I metal_plugin\/src\/device\/metal_device.cc:296] systemMemory: 16.00 GB\n2025-02-08 12:29:20.630293: I metal_plugin\/src\/device\/metal_device.cc:313] maxCacheSize: 5.33 GB\n2025-02-08 12:29:20.630324: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2025-02-08 12:29:20.630338: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:269] Created TensorFlow device (\/job:localhost\/replica:0\/task:0\/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n2025-02-08 12:29:20.631249: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2025-02-08 12:29:20.631259: I tensorflow\/core\/common_runtime\/pluggable_device\/pluggable_device_factory.cc:269] Created TensorFlow device (\/job:worker\/replica:0\/task:0\/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n2025-02-08 12:29:20.632347: I tensorflow\/core\/distributed_runtime\/rpc\/grpc_server_lib.cc:449] Started server with target: grpc:\/\/192.168.0.68:12345\n2025-02-08 12:29:20.637550: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:535] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 14287335759644278642\n2025-02-08 12:29:20.637654: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service_agent.cc:298] Coordination agent has successfully connected.\n2025-02-08 12:29:37.728182: I tensorflow\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:535] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 15227140312468372989\n```","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.13"],"created_at":"2025-02-08T11:40:33Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86897"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.Rsqrt``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 (google colab)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\nresult of ``tf.raw_ops.Rsqrt``` is inconsistent between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nx_0 = tf.constant([[[1.2031, 0.2168], [0.1177, 0.6758]], \n                    [[1.8203, 0.9688], [0.8828, 0.0767]]], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.Rsqrt(x=x_0)\n  print(\"Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.Rsqrt(x=x_0)\n  print(\"Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nOutput on CPU: tf.Tensor(\n[[[0.910156 2.14062]\n  [2.92188 1.21875]]\n\n [[0.742188 1.01562]\n  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)\n\nOutput on GPU: tf.Tensor(\n[[[0.914062 2.15625]\n  [2.90625 1.21875]]\n\n [[0.738281 1.01562]\n  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-05T06:12:29Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86607"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.Tan``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\n```tf.raw_ops.Tan``` produces inconsistent results between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nreal = tf.constant([1.5634333], dtype=tf.float32)\nimag = tf.constant([0.020735], dtype=tf.float32)\n\ncomplex_tensor = tf.complex(real, imag)\n\nwith tf.device('\/CPU:0'):\n    result_cpu = tf.raw_ops.Tan(x=complex_tensor)\n    print(result_cpu)\n\nwith tf.device('\/GPU:0'):\n    result_gpu = tf.raw_ops.Tan(x=complex_tensor)\n    print(result_gpu)\n\n##Comparing whole complex numbers\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_cons = tf.experimental.numpy.allclose(result_cpu, result_gpu, rtol=1e-6,  atol=1e-5)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:\", is_cons.numpy())\n\n##Comparing by parts\nreal_part_cpu = tf.math.real(result_cpu)\nreal_part_gpu = tf.math.real(result_gpu)\nreal_part_diff = tf.reduce_max(tf.abs(real_part_cpu - real_part_gpu)).numpy()\nreal_part_cons = tf.experimental.numpy.allclose(real_part_cpu, real_part_gpu, rtol=1e-6,  atol=1e-5)\n\nimag_part_cpu = tf.math.imag(result_cpu)\nimag_part_gpu = tf.math.imag(result_gpu)\nimag_part_diff = tf.reduce_max(tf.abs(imag_part_cpu - imag_part_gpu)).numpy()\nimag_part_cons = tf.experimental.numpy.allclose(imag_part_cpu, imag_part_gpu, rtol=1e-6,  atol=1e-5)\n\nprint(\"Real parts absolute difference:\", real_part_diff)\nprint(\"Real parts Consistency check with atol=1e-5 and rtol=1e-6:\", real_part_cons.numpy())\n\nprint(\"Imag parts absolute difference:\", imag_part_diff)\nprint(\"Imag parts Consistency check with atol=1e-5 and rtol=1e-6:\", imag_part_cons.numpy())\n```\n\n### Relevant log output\n\n```shell\ntf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64)\ntf.Tensor([15.205605+42.834225j], shape=(1,), dtype=complex64)\n\nMax absolute difference: 8.5064334e-05\nConsistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False\n\nReal parts absolute difference: 2.861023e-05\nReal parts Consistency check with atol=1e-5 and rtol=1e-6: False\n\nImag parts absolute difference: 8.010864e-05\nImag parts Consistency check with atol=1e-5 and rtol=1e-6: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-04T03:18:15Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86506"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.LogSoftmax``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 (google colab)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\nresult of ``tf.raw_ops.LogSoftmax``` is inconsistent between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nlogits = tf.constant([[0.0664, -2.3906]], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.LogSoftmax(logits=logits)\n  print(\"Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.LogSoftmax(logits=logits)\n  print(\"Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nOutput on CPU: tf.Tensor([[-0.0825195 -2.53125]], shape=(1, 2), dtype=bfloat16)\n\nOutput on GPU: tf.Tensor([[-0.0825195 -2.54688]], shape=(1, 2), dtype=bfloat16)\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-03T03:30:58Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86434"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.L2Loss``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04 (google colab)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\nresult of ```tw.raw_ops.L2Loss``` is inconsistent between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nt = tf.constant([\n    [[0.9922, -1.4922], \n     [0.0376,  0.1504], \n     [0.6172,  1.2266]],\n\n    [[-0.1387,  1.3047], \n     [0.3535, -0.0471], \n     [0.0437,  0.2637]]\n], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.L2Loss(t=t)\n  print(\"Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.L2Loss(t=t)\n  print(\"Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nOutput on CPU: tf.Tensor(3.51562, shape=(), dtype=bfloat16)\n\nOutput on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16)\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-02T07:05:17Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86406"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.BiasAddGrad``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\n```tf.raw_ops.BiasAddGrad``` produces inconsistent results between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nout_backprop = tf.constant([\n    [\n        [\n            [[ 0.2207,  2.1094], [-0.3730, -1.0625], [ 1.7031,  0.7148]], \n            [[ 1.5078, -0.6719], [-0.6367,  0.5039], [-2.3281,  0.5078]]\n        ],\n        [\n            [[-0.3574,  0.0461], [ 2.3750, -2.9688], [-0.5703, -2.0156]],\n            [[ 0.8125,  1.7656], [-0.9570,  0.6250], [-0.6914, -0.4746]]\n        ],\n        [\n            [[-0.3750, -0.7383], [ 0.3691,  0.4570], [ 1.1641,  0.2715]],\n            [[-1.2969, -0.9844], [-0.4863,  1.0938], [-1.4297,  0.8086]]\n        ]\n    ],\n    [\n        [\n            [[ 0.3730,  0.8477], [-0.3887,  1.2266], [ 0.0859, -0.5742]],\n            [[-0.7383, -0.2432], [-0.7578, -0.8281], [-0.1660, -0.9336]]\n        ],\n        [\n            [[ 1.4297,  0.6797], [-1.6172,  0.4941], [-0.3047, -0.3711]],\n            [[-0.6250, -0.7617], [ 0.9453,  0.1064], [ 1.4062, -2.9531]]\n        ],\n        [\n            [[-1.4297, -0.1387], [ 0.0625,  1.0469], [-0.1953,  1.6406]],\n            [[-0.3047,  0.5117], [ 1.8125,  1.1797], [-0.8789, -0.4688]]\n        ]\n    ]\n], dtype=tf.bfloat16)\n\nwith tf.device('CPU:0'):\n  result_cpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=\"NCHW\")\n  print(\"BiasAddGrad Output on CPU:\", result_cpu)\n\nwith tf.device('GPU:0'):\n  result_gpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=\"NCHW\")\n  print(\"BiasAddGrad Output on GPU:\", result_gpu)\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nBiasAddGrad Output on CPU: tf.Tensor([0.09375 -3.96875 1.70312], shape=(3,), dtype=bfloat16)\n\nBiasAddGrad Output on GPU: tf.Tensor([0.078125 -4 1.70312], shape=(3,), dtype=bfloat16)\n\nMax absolute difference: 0.03125\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-01T10:43:06Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86378"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.raw_ops.BatchMatMulV2``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\ngetting inconsistent results of ```tf.raw_ops.BatchMatMulV2``` between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nx_0 = tf.constant([\n    [[[[ -1.3594, -0.3027], [-1.4141,  0.2969]],\n      [[ -0.9141,  1.7812], [ 1.2266,  0.8594]]],\n\n     [[[  0.8359, -0.9414], [-1.7969, -0.7461]],\n      [[  0.3164,  0.3691], [ 0.7656,  0.2354]]]],\n\n    [[[[ -0.5898,  1.3516], [ 0.4902, -0.1045]],\n      [[ -0.1099,  1.5078], [ 0.2852, -0.0957]]],\n\n     [[[-0.9883,  1.3203], [-0.2715, -1.7578]],\n      [[ -0.1602, -0.4336], [-0.6875, -0.4492]]]]\n], dtype=tf.bfloat16)\n\ny = tf.constant([\n    [[[  0.6836, -0.6562], [-0.5508, -0.8438]], \n     [[  1.6094, -0.9883], [-0.1318,  1.1094]]],\n\n    [[[  0.4062, -1.1094], [-0.7188, -1.7578]], \n     [[ -1.0391, -0.6602], [ 0.8359, -0.6562]]]\n], dtype=tf.bfloat16) \n\nwith tf.device('CPU:0'):\n    result_cpu = tf.raw_ops.BatchMatMulV2(\n        x=x_0, \n        y=y,\n    )\n    print(result_cpu)\n\nwith tf.device('GPU:0'):\n    result_gpu = tf.raw_ops.BatchMatMulV2(\n        x=x_0, \n        y=y,\n    )\n    print(result_gpu)\n\n\nmax_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()\n\nis_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\ntf.Tensor(\n[[[[[-0.761719 1.14062]\n    [-1.125 0.675781]]\n\n   [[-1.70312 2.875]\n    [1.85938 -0.257812]]]\n\n\n  [[[1.01562 0.726562]\n    [-0.193359 3.29688]]\n\n   [[-0.0201416 -0.449219]\n    [-0.597656 -0.65625]]]]\n\n\n\n [[[[-1.14062 -0.75]\n    [0.392578 -0.233398]]\n\n   [[-0.375 1.78125]\n    [0.470703 -0.386719]]]\n\n\n  [[[-1.34375 -1.21875]\n    [1.14844 3.39062]]\n\n   [[-0.195312 0.388672]\n    [0.337891 0.746094]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)\n\ntf.Tensor(\n[[[[[-0.761719 1.14844]\n    [-1.13281 0.675781]]\n\n   [[-1.70312 2.875]\n    [1.85938 -0.259766]]]\n\n\n  [[[1.01562 0.726562]\n    [-0.193359 3.3125]]\n\n   [[-0.0201416 -0.451172]\n    [-0.597656 -0.660156]]]]\n\n\n\n [[[[-1.14844 -0.753906]\n    [0.392578 -0.233398]]\n\n   [[-0.375 1.78125]\n    [0.470703 -0.388672]]]\n\n\n  [[[-1.35156 -1.22656]\n    [1.15625 3.39062]]\n\n   [[-0.196289 0.390625]\n    [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)\n\n\nMax absolute difference: 0.015625\n\nConsistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-02-01T07:13:36Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86350"},{"repository":"tensorflow\/tensorflow","title":"Stateful LSTM bug with batch size","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.18\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nWindows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid. \nBasically the error is the same as this https:\/\/github.com\/tensorflow\/tensorflow\/issues\/64061\n\nBelow is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\nimport tensorflow as tf\n\n# Set a fixed batch size\nbatch_size = 32\n\n# Create some random training data\n# We'll have sequences of length 5, with 1 feature per time step\nsequence_length = 5\nnum_features = 1\nnum_samples = 100  # Total number of samples (must be divisible by batch_size)\n\n# Ensure num_samples is a multiple of batch_size\nnum_samples = (num_samples \/\/ batch_size) * batch_size\n\nX_train = np.random.rand(num_samples, sequence_length, num_features)\ny_train = np.random.rand(num_samples, 1)  # Example target values\n\n# Reshape y_train to match expected output shape if needed\ny_train = y_train.reshape(-1,1)\n\n# Create the stateful LSTM model\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.LSTM(units=64,  # Number of LSTM units\n                               batch_input_shape=(batch_size, sequence_length, num_features),\n                               stateful=True,\n                               return_sequences=False)) #often false for a final prediction\n\nmodel.add(tf.keras.layers.Dense(units=1)) # Output layer with 1 unit\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mse')\n\n# Train the model\nepochs = 10\n\nfor epoch in range(epochs):\n    # Shuffle data indices for each epoch (important for stateful LSTMs)\n    indices = np.arange(num_samples)\n    np.random.shuffle(indices)\n    X_train = X_train[indices]\n    y_train = y_train[indices]\n\n    model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False) # Shuffle must be false\n\n    # Reset states after each epoch (essential for stateful LSTMs)\n    model.reset_states()\n```\n\n### Relevant log output\n\n```shell\n\n```","labels":["stat:awaiting response","type:bug","comp:keras","TF 2.18"],"created_at":"2025-01-31T18:07:07Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86310"},{"repository":"tensorflow\/tensorflow","title":"inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.5\/9\n\n### GPU model and memory\n\nTesla T4\n\n### Current behavior?\n\ngetting inconsistent results of ```tf.image.adjust_hue``` between CPU and GPU\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nimages = tf.constant([\n    [[ 1.9720840,  2.1302242, -0.1902120],\n     [ 0.6557856, -1.3016001,  1.1452782]],\n    \n    [[-2.2193234,  0.3198028,  0.9568117],\n     [-0.3937407, -0.0503466, -0.3693791]]\n], dtype=tf.float32)\n\ndelta = tf.constant(-0.7441734, dtype=tf.float32)\n\nwith tf.device('CPU:0'):\n    adjusted_cpu = tf.image.adjust_hue(images, delta)\n    print(\"Adjusted Hue on CPU:\\n\", adjusted_cpu)\n\nwith tf.device('GPU:0'):\n    adjusted_gpu = tf.image.adjust_hue(images, delta)\n    print(\"Adjusted Hue on GPU:\\n\", adjusted_gpu)\n\n\nis_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e-5, rtol=1e-6)\n\nmax_abs_diff = tf.reduce_max(tf.abs(adjusted_cpu - adjusted_gpu)).numpy()\n\nprint(\"Max absolute difference:\", max_abs_diff)\nprint(\"Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:\", is_consistent.numpy())\n```\n\n### Relevant log output\n\n```shell\nAdjusted Hue on CPU:\n tf.Tensor(\n[[[-0.190212    2.1302242   1.2092681 ]\n  [ 1.1452782  -0.48211157 -1.3016001 ]]\n\n [[ 0.11679006 -2.2193234   0.9568117 ]\n  [-0.3937407  -0.25841027 -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)\n\nAdjusted Hue on GPU:\n tf.Tensor(\n[[[-0.19021209  2.1302242   1.209268  ]\n  [ 1.1452781  -0.48211193 -1.3016001 ]]\n\n [[ 0.11678863 -2.2193234   0.95681167]\n  [-0.0503466  -0.0503466  -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)\n\nMax absolute difference: 0.3433941\nConsistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-01-31T07:40:34Z","comments":4,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86256"},{"repository":"tensorflow\/tensorflow","title":"Buffer allocation error in Tensorflow Lite with OpenCL backend on certain platforms","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.13\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nmacOS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI noticed a memory allocation error in clCreateBuffer. The issue seems to be caused by this:\n\n1) TFlite tries to alloca 0xa2000000 bytes of memory (value stored as size_t)\n\n2) The call ends up in this function (tensorflow\/lite\/experimental\/litert\/runtime\/opencl\/buffer.cc):\n\n```\nabsl::Status CreateClBuffer(cl_context context, int size_in_bytes,\n                            bool read_only, void* data, cl_mem* result) \n```\n\nwhere the size is now int (i.e. 32 bit signed integer... so 0xa2000000 is interpreted as a negative value).\n\n3) This function then calls clCreateBuffer, which takes the size argument as size_t again, and thus receives 0xffffffffa2000000, i.e. the signed 32 bit integer first sign extended to 64bit and then interpreted as unsigned, and thus resulting in a huge size.\n\nThe issue doesn't seem to appear with the same model on Android, probably because: The max buffer allocation size on macOS (M1) seems to be 9GB (according to clinfo), but on that android device it's only 1GB (so on android tflite never tries to allocate such a huge chunk of memory).\n\n### Standalone code to reproduce the issue\n\n```shell\nUnfortunately I'm not allowed to share the code\/model, but looking at the function signatures one can see the issue.\n```\n\n### Relevant log output\n\n```shell\nERROR: Failed to allocate device memory (clCreateBuffer): Invalid buffer size\n```","labels":["type:bug","comp:lite","TF 2.13"],"created_at":"2025-01-29T12:11:28Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/86048"},{"repository":"tensorflow\/tensorflow","title":"TensorFlow warning shows whenever importing it","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.18.0-rc2-4-g6550e4bd802\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 24.10 x86_64\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\nCUDA:12.6\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n`cuFFT`, `cuDNN` and `cuBLAS` warnings showing after importing `TensorFlow`\n\n- OS: Ubuntu 24.10 x86_64\n- Host: G5 5590\n- Kernel: 6.11.0-13-generic\n- CPU: Intel i7-9750H (12) @ 4.500GHz\n- GPU: NVIDIA GeForce GTX 1650 Mobile \/ Max-Q\n- GPU: Intel CoffeeLake-H GT2 [UHD Graphics 630]\n> whenever running the following code it gives that warning also it outputs the predicted output but after the warning:\n```python\nimport tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))\n```\n> output:\n```\n2025-01-23 21:08:06.468437: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1737659286.484845  763412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1737659286.489647  763412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-23 21:08:06.505984: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n[PhysicalDevice(name='\/physical_device:GPU:0', device_type='GPU')]\n```\n> also i know that for that warning (`cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`) i must rebuild the tensor-flow from binaries enabling the AVX2 and FMA instructions but what about the others?\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\nprint(tf.config.list_physical_devices('GPU'))\n```\n\n### Relevant log output\n\n```shell\n\n```","labels":["type:bug","2.18.rc"],"created_at":"2025-01-23T21:56:39Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85604"},{"repository":"tensorflow\/tensorflow","title":"Tutorial \"Multi-worker training with Keras\" fails to complete","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv1.12.1-120353-gc5bd67bc56f 2.19.0-dev20250107\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nDebian 6.1.123-1 (2025-01-02) x86_64 GNU\/Linux\n\n### Mobile device\n\n_No response_\n\n### Python version\n\nPython 3.12.8\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nFollowing the tutorial everything goes well until you start the second worker. Then the below failure occures.\n\n2025-01-20 07:19:35.283801: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-01-20 07:19:35.290192: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-20 07:19:35.307721: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-01-20 07:19:36.510476: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2025-01-20 07:19:36.510494: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=\"-1\"\n2025-01-20 07:19:36.510499: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n2025-01-20 07:19:36.510501: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n2025-01-20 07:19:36.510505: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael\n2025-01-20 07:19:36.510507: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:190] hostname: michael\n2025-01-20 07:19:36.510562: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0\n2025-01-20 07:19:36.510572: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0\n2025-01-20 07:19:36.510574: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0\n2025-01-20 07:19:36.519175: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:637] Initializing CoordinationService\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc:\/\/localhost:12345\n2025-01-20 07:19:36.524874: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 4677280066871850635\n2025-01-20 07:19:36.524894: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 1\/2 tasks to connect.\n2025-01-20 07:19:36.524898: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:819] Example stragglers:\n\/job:worker\/replica:0\/task:1\nI0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.\n2025-01-20 07:22:27.996664: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 13530699364709055870\n2025-01-20 07:22:27.996686: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 0\/2 tasks to connect.\n\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/layers\/core\/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n  warnings.warn(\n2025-01-20 07:22:28.461733: W tensorflow\/core\/framework\/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\nTraceback (most recent call last):\n  File \"\/home\/chad\/Documents\/McCueFiles\/NeuralNetworks\/TensorFlowProject\/TensorFlowDocExample\/main.py\", line 21, in <module>\n    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 108, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: Attempt to convert a value (PerReplica:{\n  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=\narray([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>\n}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.\n\n### Standalone code to reproduce the issue\n\n```shell\npython main.py &> job_1.log\n```\n\n### Relevant log output\n\n```shell\n2025-01-20 07:19:35.283801: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-01-20 07:19:35.290192: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-20 07:19:35.307721: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-01-20 07:19:36.510476: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n2025-01-20 07:19:36.510494: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=\"-1\"\n2025-01-20 07:19:36.510499: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA\n2025-01-20 07:19:36.510501: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module\n2025-01-20 07:19:36.510505: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael\n2025-01-20 07:19:36.510507: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:190] hostname: michael\n2025-01-20 07:19:36.510562: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0\n2025-01-20 07:19:36.510572: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0\n2025-01-20 07:19:36.510574: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0\n2025-01-20 07:19:36.519175: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:637] Initializing CoordinationService\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc:\/\/localhost:12345\n2025-01-20 07:19:36.524874: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:0 has connected to coordination service. Incarnation: 4677280066871850635\n2025-01-20 07:19:36.524894: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 1\/2 tasks to connect.\n2025-01-20 07:19:36.524898: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:819] Example stragglers:\n\/job:worker\/replica:0\/task:1\nI0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.\n2025-01-20 07:22:27.996664: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:378] \/job:worker\/replica:0\/task:1 has connected to coordination service. Incarnation: 13530699364709055870\n2025-01-20 07:22:27.996686: I external\/local_xla\/xla\/tsl\/distributed_runtime\/coordination\/coordination_service.cc:816] Waiting for 0\/2 tasks to connect.\n\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/layers\/core\/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n  warnings.warn(\n2025-01-20 07:22:28.461733: W tensorflow\/core\/framework\/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\nTraceback (most recent call last):\n  File \"\/home\/chad\/Documents\/McCueFiles\/NeuralNetworks\/TensorFlowProject\/TensorFlowDocExample\/main.py\", line 21, in <module>\n    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"\/home\/chad\/anaconda3\/lib\/python3.12\/site-packages\/tensorflow\/python\/framework\/constant_op.py\", line 108, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: Attempt to convert a value (PerReplica:{\n  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=\narray([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>\n}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.\n```","labels":["type:bug","TF 2.18"],"created_at":"2025-01-20T14:03:18Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85351"},{"repository":"tensorflow\/tensorflow","title":"Aborted  in `tf.raw_ops.RaggedGather`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific inputs, `tf.raw_ops.RaggedGather` triggers crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nparams_nested_splits = tf.constant(0, shape=[3], dtype=tf.int64)\nparams_dense_values = tf.constant(1, shape=[0], dtype=tf.float32)\nindices = tf.constant(0, shape=[], dtype=tf.int64)\nOUTPUT_RAGGED_RANK = 1\nPARAMS_RAGGED_RANK = 1\n\ntf.raw_ops.RaggedGather(\n    params_nested_splits=[params_nested_splits],\n    params_dense_values=params_dense_values,\n    indices=indices,\n    OUTPUT_RAGGED_RANK=1,\n    name=None\n)\n```\n\n### Relevant log output\n\n```shell\n2025-01-18 09:30:00.549762: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64\nAborted (core dumped)\n```","labels":["type:bug","comp:ops","2.17"],"created_at":"2025-01-18T09:32:16Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85242"},{"repository":"tensorflow\/tensorflow","title":"Segmentation fault (core dumped) in `RaggedTensorToTensor`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\n\nshape = tf.constant(-1, shape=[], dtype=tf.int64)\nvalues = tf.constant(0, shape=[0], dtype=tf.int32)\ndefault_value = tf.constant(0, shape=[], dtype=tf.int32)\nrow_partition_tensors = tf.constant([0, 1, 6], shape=[3], dtype=tf.int64)\nrow_partition_types = [\"ROW_SPLITS\"]\n\ntf.raw_ops.RaggedTensorToTensor(\n    shape=shape,\n    values=values,\n    default_value=default_value,\n    row_partition_tensors=[row_partition_tensors],\n    row_partition_types=row_partition_types)\n```\n\n### Relevant log output\n\n```shell\nSegmentation fault (core dumped)\n```","labels":["type:bug","comp:ops","2.17"],"created_at":"2025-01-18T09:27:19Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/85240"},{"repository":"tensorflow\/tensorflow","title":"Seg Fault when iterate dataset created from data service","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSegfault when trying to iterate dataset get from data service.\n\n### Standalone code to reproduce the issue\n\n```shell\n# start the data service file start_dataservice.py\n\nimport tensorflow as tf\n\ndispatcher = tf.data.experimental.service.DispatchServer(\n    tf.data.experimental.service.DispatcherConfig(port=50050), start=True\n)\ndispatcher_address = dispatcher.target.split(\":\/\/\")[1]\nworker = tf.data.experimental.service.WorkerServer(\n    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True\n)\nprint(\"Starting Worker\")\nworker.join()\n\n# test file test_dataset_service.py\nimport tensorflow as tf\nimport numpy as np\n\n\nflags = tf.compat.v1.app.flags\n\nflags.DEFINE_bool(\"local\", False, \"Run data service in process\")\nflags.DEFINE_bool(\"distribute\", False, \"Run data service in distributed_epoch mode\")\nFLAGS = flags.FLAGS\n\n\ndef local_service():\n    print(\"Starting Local Service\")\n    dispatcher = tf.data.experimental.service.DispatchServer(\n        tf.data.experimental.service.DispatcherConfig(port=50050), start=True\n    )\n    dispatcher_address = dispatcher.target.split(\":\/\/\")[1]\n    worker = tf.data.experimental.service.WorkerServer(\n        tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True\n    )\n    print(\"Dispatcher target is \", dispatcher.target)\n    return dispatcher, worker, dispatcher.target\n\n\ndef apply_transformations(ds_train):\n    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    ds_train = ds_train.cache()\n    ds_train = ds_train.shuffle(60000)\n    ds_train = ds_train.batch(128)\n    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n    return ds_train\n\n\n(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\nx_train = x_train \/ np.float32(255)\ny_train = y_train.astype(np.int64)\nds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n\n\ndef normalize_img(image, label):\n    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n    return tf.cast(image, tf.float32) \/ 255.0, label\n\n\nds_train = apply_transformations(ds_train)\n# Create dataset however you were before using the tf.data service.\ndataset = ds_train\nif FLAGS.local:\n    dispatcher, worker, service = local_service()\nelse:\n    dispatcher_address = \"localhost\"\n    dispatcher_port = \"50050\"\n    service = \"grpc:\/\/{}:{}\".format(dispatcher_address, dispatcher_port)\nif FLAGS.distribute:\n    processing_mode = \"distributed_epoch\"\nelse:\n    processing_mode = \"parallel_epochs\"\n\n# This will register the dataset with the tf.data service cluster so that\n# tf.data workers can run the dataset to produce elements. The dataset returned\n# from applying `distribute` will fetch elements produced by tf.data workers.\ndataset = dataset.apply(\n    tf.data.experimental.service.distribute(processing_mode=processing_mode, service=service)\n)\n\nfor (x1, y1), (x2, y2) in zip(dataset, ds_train):\n    np.allclose(x1, x2)\n    np.allclose(y1, y2)\n\nprint(\"verified mnist dataset locally vs over service\")\n\n# script to run \npython -m pip install --upgrade pip\npython -m pip install tensorflow==2.18.0\npython -m pip install 'protobuf<4'\nscreen -d -m python start_dataservice.py\npython3 test_dataset_service.py --local=False\n```\n\n### Relevant log output\n\n```shell\n2025-01-14 21:56:19.778399: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1736891779.795141    9168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1736891779.800177    9168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-01-14 21:56:19.815971: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nI0000 00:00:1736891783.518634    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 889 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.520395    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 37945 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1d.0, compute capability: 8.0\nI0000 00:00:1736891783.522012    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:2 with 37945 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.523626    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:3 with 37945 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1d.0, compute capability: 8.0\nI0000 00:00:1736891783.525222    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:4 with 37945 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.526807    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:5 with 37945 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1d.0, compute capability: 8.0\nI0000 00:00:1736891783.528377    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:6 with 37945 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1c.0, compute capability: 8.0\nI0000 00:00:1736891783.529933    9168 gpu_device.cc:2022] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:7 with 37945 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1d.0, compute capability: 8.0\n\/test\/bin\/testDataservice: line 5:  9168 Segmentation fault      (core dumped) python ${BIN_DIR}\/test_dataset_service.py --local=False\n```","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2025-01-14T21:57:20Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84897"},{"repository":"tensorflow\/tensorflow","title":"GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\nnightly\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 22.04\r\n\r\n### Python version\r\n\r\nPython 3.12\r\n\r\n### CUDA\/cuDNN version\r\n\r\nCUDA 12.4\r\n\r\n### GPU model and memory\r\n\r\nA100 80GB\r\n\r\n### Current behavior?\r\n\r\nStart a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace. \r\nNo any memory profile events or OP profiler, but only trace view.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n**tf_allreduce.py**\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.python.ops.collective_ops import all_reduce, all_reduce_v2\r\nfrom tensorflow.python.eager import context\r\nfrom tensorflow.core.protobuf import config_pb2\r\nfrom tensorflow.python.distribute import cluster_resolver as cluster_resolver_lib\r\n\r\ncluster_resolver = cluster_resolver_lib.TFConfigClusterResolver()\r\ncluster = cluster_resolver.cluster_spec()\r\ntask_type = cluster_resolver.task_type\r\ntask_id = cluster_resolver.task_id\r\n\r\nexperimental_config = config_pb2.ConfigProto.Experimental(\r\n    share_cluster_devices_in_session=False,\r\n    share_session_state_in_clusterspec_propagation=False\r\n)\r\nconfig = config_pb2.ConfigProto(experimental=experimental_config)\r\nconfig.experimental.collective_group_leader = '\/job:worker\/replica:0\/task:0'\r\nserver = tf.distribute.Server(cluster,\r\n                              job_name=task_type,\r\n                              task_index=task_id,\r\n                              protocol=\"grpc\", # \"grpc+verbs\"\r\n                              config=config)\r\nrun_options = config_pb2.RunOptions()\r\n\r\nwith tf.compat.v1.Session(target=server.target, config=config) as sess:\r\n    tensor = tf.Variable(tf.ones([2, 2]), dtype=tf.float32)\r\n    init = tf.compat.v1.global_variables_initializer()\r\n    sess.run(init)\r\n    sess.run(tf.print([\"tensor:\",tensor]))\r\n\r\n    reduced_tensor = all_reduce(tensor, group_size=2, group_key=4321, instance_key=1234, merge_op='Add', final_op='Id', communication_hint='auto')\r\n    run_options.experimental.collective_graph_key = 6\r\n    while True:\r\n        sess.run(tf.print([\"reduced_tensor:\",reduced_tensor]), options=run_options)\r\n```\r\n\r\nRun script to start server.\r\n```bash\r\nCUDA_VISIBLE_DEVICES=0 TF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2223\",\"localhost:2224\"]},\"task\":{\"type\":\"worker\",\"index\":0}}' python tf_allreduce.py&\r\nCUDA_VISIBLE_DEVICES=1 TF_CONFIG='{\"cluster\":{\"worker\":[\"localhost:2223\",\"localhost:2224\"]},\"task\":{\"type\":\"worker\",\"index\":1}}' python tf_allreduce.py&\r\n```\r\n\r\n use capture_profile in Tensorboard or tf.profiler.experimental.client.trace.\r\n```python\r\ntf.profiler.experimental.client.trace(\r\n  'grpc:\/\/localhost:2223,grpc:\/\/localhost:2224',\r\n   '\/tmp\/my_tb_dir',\r\n   2000,\r\n)\r\n```\r\n\r\nTry to convert xplane.pb to memory_profile, nothing show.\r\n```python\r\nfrom tensorflow.python.profiler.internal import _pywrap_profiler as profiler_wrapper\r\njson = profiler_wrapper.xspace_to_tools_data([\"xxx.xplane\"], \"memory_profile\")\r\n```\r\n\r\n**Relevant log output**\r\n```\r\n{\"memoryProfilePerAllocator\":{},\"numHosts\":1,\"memoryIds\":[]}\r\n```\r\n\r\nRelative issue: #48146 ","labels":["type:bug","comp:gpu","TF 2.18"],"created_at":"2025-01-09T09:26:20Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84460"},{"repository":"tensorflow\/tensorflow","title":"Unable to connect to TPU through Cloud VM (metadata issue?)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.18.0-rc2-4-g6550e4bd802 2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\ntpu-ubuntu2204-base\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.2\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI am on a VM instance trying to connect to a tpu v4-32 using a test script. I installed tensorflow-tpu on both the VM (in venv) and TPU (globally) as per the instructions from the google website.\r\n\r\nIt seems like there is an issue with getting TPU metadata.\r\n\r\nIt is able to connect to the metadata server when I request manually from the VM:\r\n\r\n```\r\n$ curl http:\/\/169.254.169.254\/computeMetadata\/v1\/ -H \"Metadata-Flavor: Google\"\r\ninstance\/\r\noslogin\/\r\nproject\/\r\n```\r\n\r\nAny help would be appreciated!\n\n### Standalone code to reproduce the issue\n\n```shell\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name)\r\ntf.config.experimental_connect_to_cluster(resolver)\r\ntry:\r\n    tf.tpu.experimental.initialize_tpu_system(resolver)\r\n    print(\"TPU initialized:\", resolver.master())\r\nexcept Exception as e:\r\n    print(\"Failed to initialize TPU:\", e)\n```\n\n\n### Relevant log output\n\n```shell\n$ python hello.py\r\n2025-01-08 23:49:33.189260: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2025-01-08 23:49:33.221197: I tensorflow\/core\/tpu\/tpu_api_dlsym_initializer.cc:95] Opening library: \/home\/ucsdwanglab\/test_tpu\/.venv\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2\r\n2025-01-08 23:49:33.221290: I tensorflow\/core\/tpu\/tpu_api_dlsym_initializer.cc:121] Libtpu path is: \/home\/ucsdwanglab\/test_tpu\/.venv\/lib\/python3.11\/site-packages\/libtpu\/libtpu.so\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable CHIPS_PER_HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable ALT: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (tpu-env) from instance metadata for variable WRAP: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\nlearning\/45eac\/tfrc\/runtime\/env_var_utils.cc:50\r\n\r\nFailed to get TPU metadata (accelerator-type) from instance metadata for variable TPU_ACCELERATOR_TYPE: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\n\r\nFailed to find host bounds for accelerator type: WARNING: could not determine TPU accelerator type, please set env var `TPU_ACCELERATOR_TYPE` manually, otherwise libtpu.so may not properly initialize.\r\nFailed to get TPU metadata (agent-worker-number) from instance metadata for variable TPU_WORKER_ID: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\n\r\nFailed to get TPU metadata (worker-network-endpoints) from instance metadata for variable TPU_WORKER_HOSTNAMES: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/gcp_metadata_utils.cc:93\r\n\r\nWARNING: Logging before InitGoogle() is written to STDERR\r\nE0000 00:00:1736380405.363400    3192 common_lib.cc:511] INVALID_ARGUMENT: Error: unexpected worker hostname 'WARNING: could not determine TPU worker hostnames or IP addresses' from env var TPU_WORKER_HOSTNAMES. Expecting a valid hostname or IP address without port number. (Full TPU workers' addr string: WARNING: could not determine TPU worker hostnames or IP addresses, please set env var `TPU_WORKER_HOSTNAMES` manually, otherwise libtpu.so may not properly initialize.)\r\n=== Source Location Trace: === \r\nlearning\/45eac\/tfrc\/runtime\/libtpu_init_utils.cc:173\r\n2025-01-08 23:56:48.526584: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1736380609.730442    3192 context_distributed_manager.cc:762] unknown service tensorflow.WorkerService\r\nAdditional GRPC error information from remote target \/job:worker\/replica:0\/task:0 while calling \/tensorflow.WorkerService\/GetStatus:\r\n:{\"created\":\"@1736380609.730372913\",\"description\":\"Error received from peer ipv4:10.130.0.3:8470\",\"file\":\"external\/com_github_grpc_grpc\/src\/core\/lib\/surface\/call.cc\",\"file_line\":1056,\"grpc_message\":\"unknown service tensorflow.WorkerService\",\"grpc_status\":12}\r\nE0108 23:56:49.730822322    3192 completion_queue.cc:244]    assertion failed: queue.num_items() == 0\r\nhttps:\/\/symbolize.stripped_domain\/r\/?trace=7f1ccaf5cebc,7f1ccaf0e04f&map= \r\n*** SIGABRT received by PID 3192 (TID 3192) on cpu 4 from PID 3192; stack trace: ***\r\nPC: @     0x7f1ccaf5cebc  (unknown)  (unknown)\r\n    @     0x7f1caa302841       1888  (unknown)\r\n    @     0x7f1ccaf0e050   18460496  (unknown)\r\n    @     0x7f1ccaed1c60  (unknown)  (unknown)\r\nhttps:\/\/symbolize.stripped_domain\/r\/?trace=7f1ccaf5cebc,7f1caa302840,7f1ccaf0e04f,7f1ccaed1c5f&map= \r\nE0108 23:56:49.732558    3192 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked.\r\nE0108 23:56:49.732569    3192 coredump_hook.cc:355] RAW: Skipping coredump since rlimit was 0 at process start.\r\nE0108 23:56:49.732575    3192 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.\r\nE0108 23:56:49.732580    3192 coredump_hook.cc:411] RAW: Sending fingerprint to remote end.\r\nE0108 23:56:49.732595    3192 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket \/var\/google\/services\/logmanagerd\/remote_coredump.socket (Is the listener running?): No such file or directory\r\nE0108 23:56:49.732601    3192 coredump_hook.cc:472] RAW: Dumping core locally.\r\nE0108 23:56:49.745981    3192 process_state.cc:805] RAW: Raising signal 6 with default behavior\r\nAborted\n```\n","labels":["type:bug","comp:tpus","TF 2.18"],"created_at":"2025-01-09T00:04:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84413"},{"repository":"tensorflow\/tensorflow","title":"dictionaries in fit method of model load data in wrong order","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17; tf 2.18\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 22.04.3 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.12\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nthe code is running in google collab.\r\nThe code below is an example of a model with multiple inputs and multiple outputs.\r\nNOT working code with using **dictionaries** in method **fit** of model.\r\n\r\nthe link to collab:  https:\/\/colab.research.google.com\/drive\/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing\r\nthe link to gist: https:\/\/gist.github.com\/moprules\/def9b2bda642a064b35e51b8914a28dd\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\n# collab:  https:\/\/colab.research.google.com\/drive\/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing\r\n# gist:      https:\/\/gist.github.com\/moprules\/def9b2bda642a064b35e51b8914a28dd\r\n\r\n# fast code\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nimport numpy as np\r\n\r\nvocabulary_size = 10000\r\nnum_tags = 100\r\nnum_departments = 4\r\n\r\n# define three model inputs\r\ntitle = keras.Input(shape=(vocabulary_size,), name=\"title\")\r\ntext_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\r\ntags = keras.Input(shape=(num_tags,), name=\"tags\")\r\n\r\nfeatures = layers.Concatenate()([title, text_body, tags])\r\n# one intermediate layer\r\nfeatures = layers.Dense(64, activation=\"relu\")(features)\r\n\r\n# Define two model outputs\r\npriority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\r\ndepartment = layers.Dense(num_departments, activation=\"softmax\", name=\"department\")(features)\r\n\r\n# set the model\r\nmodel = keras.Model(inputs=[title, text_body, tags],\r\n                    outputs=[priority, department])\r\n# prepare data\r\nnum_samples = 1280\r\n# The data is filled in with zeros and ones\r\ntitle_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\r\ntext_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\r\ntags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\r\n\r\n# priority: [0., 1.]\r\npriority_data = np.random.random(size=(num_samples, 1))\r\n# class of 4 labels\r\ndepartment_data = np.random.randint(0, 2, size=(num_samples, num_departments))\r\n\r\n# compile model\r\nmodel.compile(optimizer=\"rmsprop\",\r\n              loss={\"priority\": \"mean_squared_error\",\r\n                    \"department\": \"categorical_crossentropy\"},\r\n              metrics={\"priority\": [\"mean_absolute_error\"],\r\n                       \"department\": [\"accuracy\"]})\r\n\r\n# It doesn't matter how the model is compiled\r\n# model.compile(optimizer=\"rmsprop\",\r\n#               loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\r\n#               metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\r\n\r\n\r\n# NOT WORKING\r\n# TRAIN MODEL WITH transferring the DICTIONARY to the method\r\nmodel.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\r\n          {\"priority\": priority_data, \"department\": department_data},\r\n          epochs=1\r\n)\r\n\r\n# WORK\r\n# TRAIN MODEL WITHOUT transferring the DICTIONARY to the method\r\nmodel.fit([title_data, text_body_data, tags_data],\r\n          [priority_data, department_data],\r\n          epochs=1\r\n)\r\n\r\n# ALSO WORK\r\n# TRAIN MODEL WITH transferring the DICTIONARY to the method\r\n# REPLACE priority and department\r\nmodel.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\r\n          {\"priority\": department_data, \"department\": priority_data},\r\n          epochs=1\r\n)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting response","type:bug","stale","comp:keras","TF 2.18"],"created_at":"2025-01-07T13:08:06Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84278"},{"repository":"tensorflow\/tensorflow","title":"keras model.save does not respect `include_optimizer=False`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.19.0-dev20250105\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSaving a model using keras with `include_optizer = False` results in a model being saved with optimizer\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1x5NJs9nFxmExhuy8_f_fOehHmIOmk-CZ?usp=sharing\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","comp:keras","TF 2.18"],"created_at":"2025-01-07T10:33:38Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84268"},{"repository":"tensorflow\/tensorflow","title":"Encountered unresolved custom op: XlaDynamicSlice","description":"Hi, i am doing a task in converting T5 model to TFLite for Android. Currently, i am using T5ModelForConditionalGeneration from Huggingface to convert. The conversion is done with some below logging but when load the `generator` from Interpretor, and run an inference example, i have faced this error. You can reproduce with the colab provided below. AFAIK, this XlaDynamicSlice is in TF ops but why this op cannot be resolved in this cases. \r\n\r\n**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04\r\n- TensorFlow installed from (source or binary): PyPI 24.0 on Python 3.11.5\r\n- TensorFlow version (or github SHA if from source): 2.18.0\r\n\r\n\r\n**Provide the text output from tflite_convert**\r\nIn colab version, tflite_convert doesn't log anything, below log is in my local version\r\n```\r\nINFO:tensorflow:Assets written to: \/tmp\/tmpaxxybw9x\/assets\r\nINFO:tensorflow:Assets written to: \/tmp\/tmpaxxybw9x\/assets\r\nW0000 00:00:1736157114.568747 1061359 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\r\nW0000 00:00:1736157114.568765 1061359 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\r\n2025-01-06 16:51:54.568997: I tensorflow\/cc\/saved_model\/reader.cc:83] Reading SavedModel from: \/tmp\/tmpaxxybw9x\r\n2025-01-06 16:51:54.645325: I tensorflow\/cc\/saved_model\/reader.cc:52] Reading meta graph with tags { serve }\r\n2025-01-06 16:51:54.645352: I tensorflow\/cc\/saved_model\/reader.cc:147] Reading SavedModel debug info (if present) from: \/tmp\/tmpaxxybw9x\r\n2025-01-06 16:51:55.085153: I tensorflow\/cc\/saved_model\/loader.cc:236] Restoring SavedModel bundle.\r\n2025-01-06 16:51:56.061632: I tensorflow\/cc\/saved_model\/loader.cc:220] Running initialization op on SavedModel bundle at path: \/tmp\/tmpaxxybw9x\r\n2025-01-06 16:51:56.517300: I tensorflow\/cc\/saved_model\/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 1948307 microseconds.\r\n2025-01-06 16:52:30.233639: W tensorflow\/compiler\/mlir\/lite\/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\r\nFlex ops: FlexStridedSlice\r\nDetails:\r\n\ttf.StridedSlice(tensor<?x?x?xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<?x1x?x?xf32>) : {begin_mask = 13 : i64, device = \"\", ellipsis_mask = 0 : i64, end_mask = 13 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}\r\nSee instructions: https:\/\/www.tensorflow.org\/lite\/guide\/ops_select\r\n2025-01-06 16:52:30.233666: W tensorflow\/compiler\/mlir\/lite\/flatbuffer_export.cc:3836] The following operation(s) need TFLite custom op implementation(s):\r\nCustom ops: XlaDynamicSlice\r\nDetails:\r\n\ttf.XlaDynamicSlice(tensor<1x12x?x?xf32>, tensor<4xi64>, tensor<4xi64>) -> (tensor<1x12x1x?xf32>) : {device = \"\"}\r\nSee instructions: https:\/\/www.tensorflow.org\/lite\/guide\/ops_custom\r\n```\r\n\r\n**Standalone code to reproduce the issue** \r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab\/Jupyter\/any notebook.\r\nMy reproduce code in Colab: https:\/\/colab.research.google.com\/drive\/1Rmhc_vpJSa7M1Vt-4ugV5uORaEfRJMOw?usp=sharing\r\nAlso, please include a link to a GraphDef or the model if possible.\r\n\r\n**Any other info \/ logs**\r\n\r\nInclude any logs or source code that would be helpful to diagnose the problem.\r\nIf including tracebacks, please include the full traceback. Large logs and files\r\nshould be attached.\r\n","labels":["stat:awaiting response","type:bug","comp:lite","TFLiteConverter","TF 2.18"],"created_at":"2025-01-06T10:46:54Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84203"},{"repository":"tensorflow\/tensorflow","title":"MFCC-Example-Model converted from TF to TFlite fails with IsPowerOfTwo-RuntimeError inside rfft2d","description":"### 1. System information\r\n\r\n- OS Platform and Distribution: Linux Mint 6.2.9\r\n- TensorFlow installation: pip\r\n- TensorFlow library: 2.18.0 (latest)\r\n\r\n### 2. Code\r\n\r\nBelow is a minimum example which triggers the rfft2d IsPowerOfTwo RuntimeError.\r\nThe MFCC-Calculation was directly taken from the tutorial from [tensorflow.org](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/signal\/mfccs_from_log_mel_spectrograms#for_example)\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nclass MFCCLayer(tf.keras.layers.Layer):\r\n    def __init__(self, **kwargs):\r\n        super(MFCCLayer, self).__init__(**kwargs)\r\n\r\n    def call(self, pcm):\r\n        # A 1024-point STFT with frames of 64 ms and 75% overlap.\r\n        stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256, fft_length=1024)\r\n        spectrograms = tf.abs(stfts)\r\n\r\n        # Warp the linear scale spectrograms into the mel-scale.\r\n        num_spectrogram_bins = stfts.shape[-1]\r\n        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80\r\n        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\r\n            num_mel_bins,\r\n            num_spectrogram_bins,\r\n            sample_rate,\r\n            lower_edge_hertz,\r\n            upper_edge_hertz,\r\n        )\r\n        mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)\r\n        mel_spectrograms.set_shape(\r\n            spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:])\r\n        )\r\n\r\n        # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\r\n        log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\r\n\r\n        # Compute MFCCs from log_mel_spectrograms and take the first 13.\r\n        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[\r\n            ..., :13\r\n        ]\r\n        print(\"mfccs.shape: \", mfccs.shape)\r\n        return mfccs\r\n\r\n\r\ndef build_model(input_shape):\r\n    input_layer = tf.keras.layers.Input(shape=input_shape)\r\n    output_layer = MFCCLayer()(input_layer)\r\n    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    batch_size, num_samples, sample_rate = 32, 32000, 16000.0\r\n    # A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].\r\n    pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)\r\n    print(\"pcm.shape: \", pcm.shape)\r\n\r\n    model = build_model(pcm.shape)\r\n    model.summary()\r\n\r\n    # Convert to TensorFlow Lite and Save\r\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\r\n    converter.target_spec.supported_ops = [\r\n        tf.lite.OpsSet.TFLITE_BUILTINS,\r\n        tf.lite.OpsSet.SELECT_TF_OPS,\r\n    ]\r\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n    tflite_model = converter.convert()\r\n\r\n    with open(\"mfcc.tflite\", \"wb\") as f:\r\n        f.write(tflite_model)\r\n\r\n    # Load the model and run inference\r\n    with open(\"mfcc.tflite\", \"rb\") as f:\r\n        tflite_model = f.read()\r\n\r\n    interpreter = tf.lite.Interpreter(model_content=tflite_model)\r\n    interpreter.allocate_tensors()\r\n    input_details = interpreter.get_input_details()\r\n    output_details = interpreter.get_output_details()\r\n\r\n    pcm = tf.expand_dims(pcm, axis=0)  # Add batch dimension\r\n\r\n    interpreter.set_tensor(input_details[0][\"index\"], pcm)\r\n    interpreter.invoke()  # <-- RuntimeError: tensorflow\/lite\/kernels\/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.\r\n    mfccs = interpreter.get_tensor(output_details[0][\"index\"])\r\n    print(\"mfccs.shape: \", mfccs.shape)\r\n```\r\n\r\n\r\n### 3. Failure after conversion\r\nAs far as I know, the RuntimeError should't happen, as all supplied stft-function arguments are power of two's?\r\n\r\nI am unsure if this is just a user error from myself or this is a bug.\r\nI couldn't find any info online, hence i ask here.\r\n\r\nIs a MFCC-calculation model possible with TFlite?\r\n\r\nThanks for all help\r\n\r\n","labels":["stat:awaiting response","type:bug","stale","comp:lite","TFLiteConverter","TF 2.18"],"created_at":"2025-01-05T20:45:45Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84171"},{"repository":"tensorflow\/tensorflow","title":"Broken compatibility with tensorflow-metal in 2.18","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMacOS 15.2\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\nApple M2 Max GPU 38-cores\n\n### Current behavior?\n\nApple silicone GPU with tensorflow-metal==1.1.0  and python 3.11 works fine with tensorboard==2.17.0\r\n\r\nThis is normal output:\r\n```\r\n\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/bin\/python \/Users\/mspanchenko\/VSCode\/cryptoNN\/ml\/core_second_window\/test_tensorflow_gpus.py \r\n[PhysicalDevice(name='\/physical_device:GPU:0', device_type='GPU')]\r\n\r\nProcess finished with exit code 0\r\n```\r\n\r\nBut if I upgrade tensorflow to 2.18 I'll have error, attached in \"Relevant log output\" issue section\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nif __name__ == '__main__':\r\n    gpus = tf.config.experimental.list_physical_devices('GPU')\r\n    print(gpus)\n```\n\n\n### Relevant log output\n\n```shell\n\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/bin\/python \/Users\/mspanchenko\/VSCode\/cryptoNN\/ml\/core_second_window\/test_tensorflow_gpus.py \r\nTraceback (most recent call last):\r\n  File \"\/Users\/mspanchenko\/VSCode\/cryptoNN\/ml\/core_second_window\/test_tensorflow_gpus.py\", line 1, in <module>\r\n    import tensorflow as tf\r\n  File \"\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow\/__init__.py\", line 437, in <module>\r\n    _ll.load_library(_plugin_dir)\r\n  File \"\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/load_library.py\", line 151, in load_library\r\n    py_tf.TF_LoadLibrary(lib)\r\ntensorflow.python.framework.errors_impl.NotFoundError: dlopen(\/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow-plugins\/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii\r\n  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> \/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow-plugins\/libmetal_plugin.dylib\r\n  Expected in:     <2814A58E-D752-317B-8040-131217E2F9AA> \/Users\/mspanchenko\/anaconda3\/envs\/cryptoNN_ml_core\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tensorflow_internal.so\r\n\r\nProcess finished with exit code 1\n```\n","labels":["stat:awaiting response","type:bug","stale","comp:gpu","TF 2.18"],"created_at":"2025-01-05T17:26:17Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84167"},{"repository":"tensorflow\/tensorflow","title":"The test case label_image .py of tensorflow2.4.1 source code fails to be execued.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.4.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.7.12\n\n### Bazel version\n\n3.7\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe test case label_image.py fails to be executed,and the message \"module 'tensorfle' has no attribute 'GrapDef'\" is displayed.\r\n![image](https:\/\/github.com\/user-attachments\/assets\/e4b7b56d-2589-41fe-8395-1743c941dd49)\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ngraph_def = tf.GraphDef()\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting response","type:bug","stale","TF 2.4"],"created_at":"2025-01-03T03:03:30Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/84039"},{"repository":"tensorflow\/tensorflow","title":"Failing to convert MobileNetV3Large to TFLite w\/ Integer q","description":"### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Windows 10 WSL\r\n- TensorFlow installation (pip package or built from source):  2.10 (on Win 10) and 2.16.2 (on WSL)\r\n\r\n### 2. Code\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom tensorflow.keras.applications import MobileNetV3Large\r\nfrom tensorflow.keras.applications.mobilenet_v3 import preprocess_input\r\nimport matplotlib.pyplot as plt\r\nfrom scipy.stats import pearsonr\r\n\r\n# Generate one sample image for testing\r\ntest_image = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))\r\ntest_image = np.clip(test_image, 0, 255).astype(np.float32)\r\npreprocessed_image = preprocess_input(test_image.copy())\r\n\r\n# Load model\r\nmodel = MobileNetV3Large(\r\n    weights='imagenet',\r\n    include_top=True,\r\n    input_shape=(224, 224, 3)\r\n)\r\n\r\n# Get original prediction\r\noriginal_pred = model.predict(preprocessed_image, verbose=0)\r\n\r\n# Convert to TFLite\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable dynamic range quantization\r\nconverter.target_spec.supported_ops = [\r\n    tf.lite.OpsSet.TFLITE_BUILTINS,\r\n    tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\nconverter._experimental_disable_per_channel = True\r\nconverter.experimental_new_converter = True\r\n\r\n# Convert\r\ntflite_model = converter.convert()\r\n\r\n# Get TFLite prediction\r\ninterpreter = tf.lite.Interpreter(model_content=tflite_model)\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninterpreter.set_tensor(input_details[0]['index'], preprocessed_image)\r\ninterpreter.invoke()\r\ntflite_pred = interpreter.get_tensor(output_details[0]['index'])\r\n\r\n# Calculate correlation\r\ncorrelation, _ = pearsonr(original_pred.flatten(), tflite_pred.flatten())\r\n\r\n# Visualize\r\nplt.figure(figsize=(10, 5))\r\n\r\n# Scatter plot\r\nplt.subplot(1, 2, 1)\r\nplt.scatter(original_pred.flatten(), tflite_pred.flatten(), alpha=0.5)\r\nplt.plot([original_pred.min(), original_pred.max()],\r\n         [original_pred.min(), original_pred.max()],\r\n         'r--', label=f'Perfect Correlation\\nActual: {correlation:.4f}')\r\nplt.title('Original vs Quantized Predictions')\r\nplt.xlabel('Original Model')\r\nplt.ylabel('Quantized Model')\r\nplt.legend()\r\n\r\n# Distribution plot\r\nplt.subplot(1, 2, 2)\r\nplt.hist(np.abs(original_pred.flatten() - tflite_pred.flatten()),\r\n         bins=50, alpha=0.75, label='Prediction Differences')\r\nplt.title('Distribution of Prediction Differences')\r\nplt.xlabel('|Original - Quantized|')\r\nplt.ylabel('Count')\r\nplt.legend()\r\n\r\nplt.tight_layout()\r\nplt.show()\r\n\r\nprint(f\"\\nResults:\")\r\nprint(f\"Prediction correlation: {correlation:.4f}\")\r\nprint(f\"Original model size: {len(model.get_weights()) \/ 1024 \/ 1024:.2f} MB\")\r\nprint(f\"Quantized model size: {len(tflite_model) \/ 1024 \/ 1024:.2f} MB\")\r\nprint(f\"Size reduction: {(1 - len(tflite_model) \/ len(model.get_weights())) * 100:.1f}%\")\r\n```\r\n\r\n### 3. Failure after conversion\r\n1. TF 2.10 in Win10 Log:\r\nModel produces wrong results. See plot made from code:\r\n![image](https:\/\/github.com\/user-attachments\/assets\/8823cbfc-88d9-4e2d-9ef7-c8a2adc3ef0a)\r\n\r\n2. TF2.16 in WSL:\r\nModel fails to convert. Gets error: `LLVM ERROR: Failed to infer result type(s).` (see log)\r\n\r\n\r\n### 5. (optional) Any other info \/ logs\r\nI ran this on 2 systems:\r\n\r\n1. TF 2.10 in Win10 Log:\r\n```\r\nimport sys; print('Python %s on %s' % (sys.version, sys.platform))\r\nD:\\code\\ai_dev\\venv\\Scripts\\python.exe \"C:\/Program Files\/JetBrains\/PyCharm 2023.2.4\/plugins\/python\/helpers\/pydev\/pydevd.py\" --multiprocess --qt-support=auto --client 127.0.0.1 --port 54366 --file C:\\Users\\Administrator\\AppData\\Roaming\\JetBrains\\PyCharm2023.2\\scratches\\tfmodel_tflite.py \r\nConnected to pydev debugger (build 232.10203.26)\r\n2024-12-26 15:17:07.215039: I tensorflow\/core\/platform\/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\r\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-12-26 15:17:07.670016: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:1616] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 7423 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6\r\n2024-12-26 15:17:11.111912: I tensorflow\/stream_executor\/cuda\/cuda_dnn.cc:384] Loaded cuDNN version 8906\r\n2024-12-26 15:17:12.036555: I tensorflow\/stream_executor\/cuda\/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 64). These functions will not be directly callable after loading.\r\n2024-12-26 15:17:44.746406: W tensorflow\/compiler\/mlir\/lite\/python\/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\r\n2024-12-26 15:17:44.746529: W tensorflow\/compiler\/mlir\/lite\/python\/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\r\n2024-12-26 15:17:44.747230: I tensorflow\/cc\/saved_model\/reader.cc:45] Reading SavedModel from: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprjeve5nr\r\n2024-12-26 15:17:44.771028: I tensorflow\/cc\/saved_model\/reader.cc:89] Reading meta graph with tags { serve }\r\n2024-12-26 15:17:44.771129: I tensorflow\/cc\/saved_model\/reader.cc:130] Reading SavedModel debug info (if present) from: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprjeve5nr\r\n2024-12-26 15:17:44.886049: I tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\r\n2024-12-26 15:17:44.904668: I tensorflow\/cc\/saved_model\/loader.cc:229] Restoring SavedModel bundle.\r\n2024-12-26 15:17:45.275249: I tensorflow\/cc\/saved_model\/loader.cc:213] Running initialization op on SavedModel bundle at path: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmprjeve5nr\r\n2024-12-26 15:17:45.402632: I tensorflow\/cc\/saved_model\/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 655396 microseconds.\r\n2024-12-26 15:17:45.811466: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\r\n```\r\n\r\n3. TF 2.16.2 in WSL log:\r\n```\r\n\/root\/ai_dev\/.venv\/bin\/python \/root\/.pycharm_helpers\/pydev\/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 55955 --file \/mnt\/c\/Users\/Administrator\/AppData\/Roaming\/JetBrains\/PyCharm2023.2\/scratches\/tfmodel_tflite.py \r\nConnected to pydev debugger (build 232.10203.26)\r\n2024-12-26 15:18:56.434366: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-26 15:18:57.803991: I external\/local_tsl\/tsl\/cuda\/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\r\n2024-12-26 15:18:58.473972: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-12-26 15:18:58.972456: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-12-26 15:18:58.975123: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-12-26 15:18:59.910805: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-12-26 15:19:06.124533: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\n2024-12-26 15:19:15.989425: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_executor.cc:984] could not open file to read NUMA node: \/sys\/bus\/pci\/devices\/0000:0a:00.0\/numa_node\r\nYour kernel may have been built without NUMA support.\r\n2024-12-26 15:19:17.013008: W tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https:\/\/www.tensorflow.org\/install\/gpu for how to download and setup the required libraries for your platform.\r\nSkipping registering GPU devices...\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nW0000 00:00:1735255170.947341  469943 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\r\nW0000 00:00:1735255170.947401  469943 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\r\n2024-12-26 15:19:30.948060: I tensorflow\/cc\/saved_model\/reader.cc:83] Reading SavedModel from: \/tmp\/tmp9lkkwnp9\r\n2024-12-26 15:19:30.953309: I tensorflow\/cc\/saved_model\/reader.cc:51] Reading meta graph with tags { serve }\r\n2024-12-26 15:19:30.953334: I tensorflow\/cc\/saved_model\/reader.cc:146] Reading SavedModel debug info (if present) from: \/tmp\/tmp9lkkwnp9\r\n2024-12-26 15:19:31.011901: I tensorflow\/compiler\/mlir\/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\r\n2024-12-26 15:19:31.020594: I tensorflow\/cc\/saved_model\/loader.cc:234] Restoring SavedModel bundle.\r\n2024-12-26 15:19:31.231606: I tensorflow\/cc\/saved_model\/loader.cc:218] Running initialization op on SavedModel bundle at path: \/tmp\/tmp9lkkwnp9\r\n2024-12-26 15:19:31.297302: I tensorflow\/cc\/saved_model\/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 349244 microseconds.\r\n2024-12-26 15:19:31.779723: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\nloc(fused[\"ReadVariableOp:\", callsite(\"MobileNetV3Large_1\/conv_1\/convolution\/ReadVariableOp@__inference_serving_default_5035\"(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":2199:1) at callsite(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":2181:1 at callsite(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":1493:1 at callsite(\"\/root\/.pycharm_helpers\/pydev\/pydevd.py\":1500:1 at callsite(\"\/root\/.pycharm_helpers\/pydev\/_pydev_imps\/_pydev_execfile.py\":18:1 at callsite(\"\/mnt\/c\/Users\/Administrator\/AppData\/Roaming\/JetBrains\/PyCharm2023.2\/scratches\/tfmodel_tflite.py\":34:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1175:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1129:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1636:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1614:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/convert_phase.py\":205:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/tensorflow\/lite\/python\/lite.py\":1537:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/layer.py\":58:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/layer.py\":112:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/layer.py\":899:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/operation.py\":46:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":156:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\":182:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/function.py\":171:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\":632:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/layer.py\":899:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":117:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/operation.py\":46:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\":156:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/convolutional\/base_conv.py\":243:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/layers\/convolutional\/base_conv.py\":233:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/ops\/nn.py\":1183:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/nn.py\":301:1 at callsite(\"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/nn.py\":274:1 at \"\/root\/ai_dev\/.venv\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/core.py\":85:1))))))))))))))))))))))))))))))))]): error: missing attribute 'value'\r\nLLVM ERROR: Failed to infer result type(s).\r\n\r\nProcess finished with exit code 134\r\n```\r\n\r\n\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:lite","TFLiteConverter","TF 2.16"],"created_at":"2024-12-26T23:21:09Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83754"},{"repository":"tensorflow\/tensorflow","title":"How to run TFLite benchmark with QNN delegate in Android","description":"### Issue type\r\n\r\nFeature Request\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.15.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nmacOS 15.2\r\n\r\n### Mobile device\r\n\r\nOne Plus 7 Pro, Android 11\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI have built\/installed\/run TFLite benchmark following this [instruction](https:\/\/github.com\/tensorflow\/tensorflow\/tree\/master\/tensorflow\/lite\/tools\/benchmark#on-android) for Android, and used TensorFlow 2.15.0 according to [issue#66015](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/66015). I test the benchmark via the following commands and the output result seems correct.\r\n```shell\r\nadb push \/Users\/handleychen\/Github\/tensorflow\/tensorflow\/bazel-bin\/tensorflow\/lite\/tools\/benchmark\/benchmark_model \/data\/local\/tmp\r\nadb shell chmod +x \/data\/local\/tmp\/benchmark_model\r\nadb shell \"mkdir \/data\/local\/tmp\/models\"\r\nadb push \/Users\/handleychen\/Github\/tensorflow\/models\/*.tflite \/data\/local\/tmp\/models\r\nadb shell \/data\/local\/tmp\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224.tflite --num_threads=4 --enable_op_profiling=true\r\nadb shell \/data\/local\/tmp\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224.tflite --use_gpu=true --enable_op_profiling=true\r\nadb shell \/data\/local\/tmp\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224.tflite --use_nnapi=true --enable_op_profiling=true\r\n``` \r\n[benchmark result.txt](https:\/\/github.com\/user-attachments\/files\/18197819\/benchmark.result.txt)\r\n\r\nNow I want to run the benchmark with QNN delegate. I [setup the on device environment](https:\/\/docs.qualcomm.com\/bundle\/publicresource\/topics\/80-63442-50\/TfLite-Delegate_setup.html#on-device-environment-setup) and [run a QNN delegate using an external delegate](https:\/\/docs.qualcomm.com\/bundle\/publicresource\/topics\/80-70015-54\/sample-applications.html#run-a-qnn-delegate-using-an-external-delegate). The [model](https:\/\/storage.googleapis.com\/download.tensorflow.org\/models\/tflite\/task_library\/image_classification\/android_java\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite) being tested comes from tflite example [image_classification](https:\/\/github.com\/tensorflow\/examples\/tree\/master\/lite\/examples\/image_classification).  I tested the benchmark using the following commands, but the result was a failure.\r\n```shell\r\nadb shell \"mkdir \/data\/local\/tmp\/qnn_delegate\"\r\nadb push \/Users\/handleychen\/Github\/quic\/SDK\/qairt\/2.26.0.240828\/lib\/aarch64-android\/* \/data\/local\/tmp\/qnn_delegate\r\nadb shell\r\ncd \/data\/local\/tmp\r\nexport LD_LIBRARY_PATH=\/data\/local\/tmp\/qnn_delegate\r\nexport ADSP_LIBRARY_PATH=\"\/data\/local\/tmp\/qnn_delegate\"\r\n.\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:gpu;'\r\n.\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;'\r\n# I also tried setting htp_precision:1, but the result was the same.\r\n.\/benchmark_model --graph=\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;htp_precision:1'\r\n``` \r\n```shell\r\n# for gpu delegate\r\n\u2026\u2026\r\nINFO: Though EXTERNAL delegate is explicitly applied, the model graph will not be executed by the delegate.\r\n\u2026\u2026\r\n\r\n# for npu delegate\r\nINFO: STARTING!\r\nINFO: Log parameter values verbosely: [0]\r\nINFO: Graph: [\/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]\r\nINFO: External delegate path: [\/data\/local\/tmp\/qnn_delegate\/libQnnTFLiteDelegate.so]\r\nINFO: External delegate options: [backend_type:htp;htp_precision:1]\r\nINFO: Loaded model \/data\/local\/tmp\/models\/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite\r\nINFO: Initialized TensorFlow Lite runtime.\r\nINFO: EXTERNAL delegate created.\r\nERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008\r\nERROR: Restored original execution plan after delegate application failure.\r\nERROR: Failed to apply EXTERNAL delegate.\r\nERROR: Benchmarking failed.\r\n``` \r\nThe full output is attached. [benchmarkQNN result.txt](https:\/\/github.com\/user-attachments\/files\/18206356\/benchmarkQNN.result.txt)\r\n\r\nI have also tested it on Android phones equipped with Snapdragon 855 and Gen 3 chips, and the results were the same.\r\n\r\nCould anyone tell me how to deal with this?\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nas described above\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:lite"],"created_at":"2024-12-19T12:40:25Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83344"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `LearnedUnigramCandidateSampler`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOn specific inputs,`tf.raw_ops.LearnedUnigramCandidateSampler` triggers crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntrue_classes = tf.constant([], dtype=tf.int64)\r\nnum_true = 3590707793247644003\r\nnum_sampled = 126\r\nunique = False\r\nrange_max = 186785497093039093\r\nseed = 8997\r\nseed2 = 0\r\n\r\ntf.raw_ops.LearnedUnigramCandidateSampler(\r\n    true_classes=true_classes,\r\n    num_true=num_true,\r\n    num_sampled=num_sampled,\r\n    unique=unique,\r\n    range_max=range_max,\r\n    seed=seed,\r\n    seed2=seed2,\r\n    name=None\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-12-17 11:36:29.305345: W tensorflow\/core\/framework\/op_kernel.cc:1816] OP_REQUIRES failed at candidate_sampler_ops.cc:37 : INVALID_ARGUMENT: Attr num_true has value 3590707793247644003 out of range for an int32\r\n2024-12-17 11:36:29.305378: F external\/local_tsl\/tsl\/lib\/random\/weighted_picker.cc:28] Check failed: N >= 0 (0 vs. -2090015755)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-12-17T12:05:50Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83164"},{"repository":"tensorflow\/tensorflow","title":"[XLA] `tf.keras.layers.LSTM` behaves differently on GPU","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nnightly\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen executing LSTM on **XLA**, it fails.\r\nHowever, when executing it without XLA, it passes.\r\nThe above failure is on GPU.\r\nIf I use CPU as backend, with or without XLA both pass the check.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```python\r\nimport os\r\nimport tensorflow\r\nimport tensorflow as tf\r\ntf.random.set_seed(42)\r\nclass RecurrentModel(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(RecurrentModel, self).__init__()\r\n        self.lstm = tf.keras.layers.LSTM(units=64, return_sequences=True)\r\n\r\n    @tf.function(jit_compile=True)\r\n    def call(self, x):\r\n        return self.lstm(x)\r\n\r\n\r\nmodel = RecurrentModel()\r\n\r\n\r\ninput_shape = (10, 20, 1)\r\nx = tf.random.normal(shape=input_shape)\r\n\r\ninputs = [x]\r\n\r\noutput = model(*inputs)\r\nprint(output)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-0938fdccd1fa> in <cell line: 24>()\r\n     22 inputs = [x]\r\n     23 \r\n---> 24 output = model(*inputs)\r\n     25 print(output)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51   try:\r\n     52     ctx.ensure_initialized()\r\n---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                         inputs, attrs, num_outputs)\r\n     55   except core._NotOkStatusException as e:\r\n\r\nInvalidArgumentError: Exception encountered when calling RecurrentModel.call().\r\n\r\nDetected unsupported operations when trying to compile graph __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: CudnnRNNV3 (No registered 'CudnnRNNV3' OpKernel for XLA_GPU_JIT devices compatible with node {{node lstm_3_1\/CudnnRNNV3}}){{node lstm_3_1\/CudnnRNNV3}}\r\nThe op is created at: \r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\nFile \"\/usr\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/colab_kernel_launcher.py\", line 37, in <module>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/traitlets\/config\/application.py\", line 992, in launch_instance\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelapp.py\", line 619, in start\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/platform\/asyncio.py\", line 195, in start\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\nFile \"\/usr\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 685, in <lambda>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 738, in _run_callback\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 825, in inner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 361, in process_one\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 261, in dispatch_shell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 539, in execute_request\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 2975, in run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3030, in _run_cell\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/async_helpers.py\", line 78, in _pseudo_sync_runner\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3257, in run_cell_async\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3473, in run_ast_nodes\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3553, in run_code\r\nFile \"<ipython-input-4-0938fdccd1fa>\", line 24, in <cell line: 24>\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/layer.py\", line 826, in __call__\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/layer.py\", line 1376, in _maybe_build\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/core.py\", line 212, in compute_output_spec\r\nFile \"<ipython-input-1-0938fdccd1fa>\", line 13, in call\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/layer.py\", line 901, in __call__\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/ops\/operation.py\", line 46, in __call__\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 156, in error_handler\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/rnn\/lstm.py\", line 570, in call\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/rnn\/rnn.py\", line 406, in call\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/layers\/rnn\/lstm.py\", line 537, in inner_loop\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/rnn.py\", line 841, in lstm\r\nFile \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/rnn.py\", line 933, in _cudnn_lstm\r\n\ttf2xla conversion failed while converting __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=\/path\/to\/dump\/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_877]\r\n\r\nArguments received by RecurrentModel.call():\r\n  \u2022 x=tf.Tensor(shape=(10, 20, 1), dtype=float32)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","comp:xla","TF 2.18"],"created_at":"2024-12-16T13:57:22Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83063"},{"repository":"tensorflow\/tensorflow","title":"`tensorflow.experimental.numpy.kron` not working with multidimensional Arrays","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThis is a new issue in replacement for https:\/\/github.com\/tensorflow\/tensorflow\/issues\/59761 as suggested by @tilakrayal\r\n\r\nI tested the function against numpy and it throws an error when the `ndim` of the input tensors is greater than 2.\r\nI run the code on the latest TensorFlow version on PyPI and the nightly version, and I get the same failures.\r\n\r\nAlso, I am not getting as much debug information only this error\r\n\r\n`UnimplementedError: {{function_node __wrapped__Mul_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name: `\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\ntry:\r\n    a = tf.constant(np.arange(100).reshape(2, 5, 2, 5))\r\n    b = tf.constant(np.arange(24).reshape(2, 3, 4))\r\n    print(a.ndim) # 4\r\n    print(b.ndim) # 3\r\n\r\n    y = tf.experimental.numpy.kron(a, b)\r\n\r\n    print(y.shape)\r\nexcept:\r\n    print(\"Can't use tf.experimental.numpy.kron on multi-dimensional arrays\")\r\n\r\nx = np.arange(100).reshape(2, 5, 2, 5)\r\ny = np.arange(24).reshape(2, 3, 4)\r\n\r\nprint(x.ndim) # 4\r\nprint(y.ndim) # 3\r\n\r\nz = np.kron(x, y)\r\n\r\nprint(z.shape) # (2, 10, 6, 20)\n```\n\n\n### Relevant log output\n\n```shell\nUnimplementedError: {{function_node __wrapped__Mul_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-12-16T07:22:01Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/83037"},{"repository":"tensorflow\/tensorflow","title":"error message is inconsistent with documentation in `tf.raw_ops.MaxPoolGradWithArgmax`","description":"### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nAccording to [Doc tf.raw_ops.MaxPoolGradWithArgmax](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/MaxPoolGradWithArgmax), the argument `argmax` can be `int32` or `int64`. However, after actual testing, the parameter `argmax` can only support tensor input of data type `int64`.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput_tensor = tf.constant(1, shape=[1, 2, 2, 1], dtype=tf.float32)\r\ngrad_tensor = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.float32)\r\nargmax_indices = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.int32)\r\nksize = [1, 2, 2, 1]\r\nstrides = [1, 1, 1, 1]\r\npadding = \"VALID\"\r\n\r\noutput_grad = tf.raw_ops.MaxPoolGradWithArgmax(\r\n    input=input_tensor,\r\n    grad=grad_tensor,\r\n    argmax=argmax_indices,\r\n    ksize=ksize,\r\n    strides=strides,\r\n    padding=padding,\r\n    include_batch_in_index=False\r\n)\n```\n\n\n### Relevant log output\n\n```shell\nNotFoundError: Could not find device for node: {{node MaxPoolGradWithArgmax}} = MaxPoolGradWithArgmax[T=DT_FLOAT, Targmax=DT_INT32, include_batch_in_index=false, ksize=[1, 2, 2, 1], padding=\"VALID\", strides=[1, 1, 1, 1]]\r\nAll kernels registered for op MaxPoolGradWithArgmax:\r\n  device='CPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_HALF]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT32]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT8]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT8]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT16]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT16]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT32]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_INT64]; Targmax in [DT_INT64]\r\n  device='CPU'; T in [DT_UINT64]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]\r\n  device='GPU'; T in [DT_HALF]; Targmax in [DT_INT64]\r\n [Op:MaxPoolGradWithArgmax] name:\n```\n","labels":["type:docs-bug","stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-12-12T13:14:53Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82837"},{"repository":"tensorflow\/tensorflow","title":"Update Python in Docker images to 3.11.x and ditch `3.11.0rc1`","description":"### Issue type\n\nBuild\/Install\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nDocker\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe Python version of the docker images is outdated and should be updated\n\n### Standalone code to reproduce the issue\n\n```shell\ndocker run -it tensorflow\/tensorflow python\n```\n\n\n### Relevant log output\n\n```shell\nPython 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>>\n```\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","TF 2.18"],"created_at":"2024-12-09T13:35:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82529"},{"repository":"tensorflow\/tensorflow","title":"The warning \"The structure of `inputs` doesn't match the expected structure\" when training a functional model","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.13.1-0-gf841394b1b7 2.13.1 (Nightly: v1.12.1-119104-gf8fd6f53fa3 2.19.0-dev20241204)\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nWindows 11 23H2 22631.4460\n\n### Mobile device\n\nWindows 11 23H2 22631.4460\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the model is functional, not Sequential, the warning has occured:\r\n\r\n```\r\nEpoch 1\/5\r\n<path-to-python>\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*\r\n  warnings.warn(\r\n```\r\n\r\nYes, the warning message has interrupted on parenthesis. When I've run the same code in Nightly, the warning message is:\r\n\r\n```\r\nEpoch 1\/5\r\n<path-to-python>\\lib\\site-packages\\keras\\src\\models\\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\r\nExpected: ['keras_tensor']\r\nReceived: inputs=Tensor(shape=(None, 10))\r\n  warnings.warn(msg)\r\n```\r\n\r\nAfter the warning, the training continues normally, but because of this warning, I can't be sure that the model works as I expect.\r\n\r\nI've traced the source and found that in `Lib\\site-packages\\keras\\src\\tree\\optree_impl.py` on line 95 comparasion of expected and actual structure failed. Now I place the traced variables here:\r\n\r\n```\r\n>>> a\r\n<tf.Tensor 'data:0' shape=(None, 10) dtype=float64>\r\n>>> b\r\n[<KerasTensor shape=(None, 10), dtype=float32, sparse=False, name=keras_tensor>]\r\n>>> a_structure\r\nPyTreeSpec(*, NoneIsLeaf)\r\n>>> b_structure\r\nPyTreeSpec([*], NoneIsLeaf)\r\n```\r\n\r\nThe data passed to the `fit` function fully corresponds to the [documentation](https:\/\/keras.io\/api\/models\/model_training_apis\/#fit-method). The warning appears independently of whether I use numpy array or PyDataset as dataset of `fit` function.\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom keras.models import Model\r\nfrom keras.layers import Dense, Input, Flatten, Concatenate\r\nfrom keras import utils\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclass SamplesSet(utils.PyDataset):\r\n    \r\n    def __init__(self, batch_size, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.batch_size = batch_size\r\n        \r\n    def __len__(self):\r\n        return 1\r\n    \r\n    def __getitem__(self, idx):\r\n        x1 = np.random.uniform(size=10*self.batch_size).reshape((self.batch_size, 10))\r\n        y = np.arange(self.batch_size)\r\n        return x1, y\r\n    \r\ntrain = SamplesSet(100)\r\nx1_train = np.random.uniform(size=10*100).reshape((100, 10))\r\ny_train = np.arange(100)\r\n\r\ninput1 = Input(shape=(10,))\r\nl1 = Dense(1)(input1)\r\nd2 = Dense(1, activation='sigmoid')(l1)\r\nmodel = Model(inputs=[input1], outputs=[d2])\r\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\nprint(model.summary())\r\n\r\nhistory = model.fit(x1_train, y_train, epochs=5, verbose=1)\r\n# In the all cases below warning occures too\r\n# history = Model.fit(train, epochs=5, verbose=1) \r\n# ret = model.predict(np.arange(10)[np.newaxis,:])\r\n# ret = model.predict(tf.constant([[0,1,2,3,4,5,6,7,8,9]]))\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","comp:keras","TF 2.13"],"created_at":"2024-12-06T03:47:02Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82372"},{"repository":"tensorflow\/tensorflow","title":"[XLA] TF XLA outputs abnormal value when compiling `Embedding`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\nnightly\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nFor `Embedding` operator, when I set the `input_dim=1` (which means it indexed from 0 to 0), the output always returns **0 without XLA**.\r\n\r\nAfter compilation, the outputs are usually some random tensors.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\ntf.random.set_seed(42)\r\nx = tf.constant([1])\r\n\r\n\r\n# uncompiled model\r\nclass Model(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.embedding = tf.keras.layers.Embedding(1, 1)\r\n\r\n    def call(self, x):\r\n        output = self.embedding(x)\r\n        return output\r\n\r\n\r\nm = Model()\r\n\r\noutput1 = m(x)\r\n\r\n\r\n# compiled model\r\nclass Model(tf.keras.Model):\r\n\r\n    def __init__(self):\r\n        super(Model, self).__init__()\r\n        self.embedding = tf.keras.layers.Embedding(1, 1)\r\n\r\n    @tf.function(jit_compile=True)\r\n    def call(self, x):\r\n        output = self.embedding(x)\r\n        return output\r\n\r\n\r\nm = Model()\r\noutput2 = m(x)\r\n\r\nprint(output1)\r\nprint(output2)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\ntf.Tensor([[0.]], shape=(1, 1), dtype=float32)\r\ntf.Tensor([[-0.00567592]], shape=(1, 1), dtype=float32)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.18"],"created_at":"2024-12-05T13:58:40Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82317"},{"repository":"tensorflow\/tensorflow","title":"Are checkpoints broken in >= 2.16?","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.16, 2.17\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe example given in https:\/\/www.tensorflow.org\/guide\/checkpoint does not seem to work as expected in 2.16 and 2.17, while working fine in 2.15. After restoring and restarting the training process, it starts training from the very beginning.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1n76Mu5BhdBJBSXc7cXYJMr0lMDER2JRa?usp=sharing\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:apis","2.17"],"created_at":"2024-12-04T15:11:53Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82209"},{"repository":"tensorflow\/tensorflow","title":"TPU not support TensorFlow 2.18 and 2.17.1","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.18 and tf. 2.17.1\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`import tensorflow as tf` results `segmentation fault core dumped`\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:tpus","TF 2.18"],"created_at":"2024-12-04T14:56:53Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/82208"},{"repository":"tensorflow\/tensorflow","title":"Clarify the `constant_op.constant(2)` statement","description":"It would be helpful to clarify the `constant_op.constant(2)` statement by explaining the corresponding import statement.\r\n\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/5bc9d26649cca274750ad3625bd93422617eed4b\/tensorflow\/python\/ops\/summary_ops_v2.py#L1062-L1066","labels":["type:bug","comp:ops"],"created_at":"2024-12-02T18:05:41Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/81954"},{"repository":"tensorflow\/tensorflow","title":"MixedPrecision + XLA: Seen floating point types of different precisions","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nGoogle Colab\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\nGoogle Colab default\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen using bilinear interpolation + XLA + mixed_float16 policy issue raises during compilation.\r\nWithout bilinear interpolation or without XLA or without mixed_float16 there is no issue.\r\n\r\nIn Google Colab i have this issue only on CPU with TF 2.17 and both CPU & GPU with TF 2.18.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nhttps:\/\/colab.research.google.com\/drive\/1joSiScbM7Stc9bn1C4R_4sFkDzakrTsJ?usp=sharing\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n---------------------------------------------------------------------------\r\nInternalError                             Traceback (most recent call last)\r\n<ipython-input-4-9cc273be2d5a> in <cell line: 28>()\r\n     26 model.compile(loss='mse', optimizer='adam', run_eagerly=False, jit_compile=True)\r\n     27 \r\n---> 28 model.fit(dataset)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51   try:\r\n     52     ctx.ensure_initialized()\r\n---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                         inputs, attrs, num_outputs)\r\n     55   except core._NotOkStatusException as e:\r\n\r\nInternalError: Graph execution error:\r\n\r\nDetected at node StatefulPartitionedCall defined at (most recent call last):\r\n  File \"\/usr\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\n\r\n  File \"\/usr\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/colab_kernel_launcher.py\", line 37, in <module>\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/traitlets\/config\/application.py\", line 992, in launch_instance\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelapp.py\", line 619, in start\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/platform\/asyncio.py\", line 195, in start\r\n\r\n  File \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\n\r\n  File \"\/usr\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\n\r\n  File \"\/usr\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 685, in <lambda>\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/ioloop.py\", line 738, in _run_callback\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 825, in inner\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 377, in dispatch_queue\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 250, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 748, in __init__\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 786, in run\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 361, in process_one\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 261, in dispatch_shell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/kernelbase.py\", line 539, in execute_request\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/tornado\/gen.py\", line 234, in wrapper\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/ipkernel.py\", line 302, in do_execute\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/ipykernel\/zmqshell.py\", line 539, in run_cell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 2975, in run_cell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3030, in _run_cell\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/async_helpers.py\", line 78, in _pseudo_sync_runner\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3257, in run_cell_async\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3473, in run_ast_nodes\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/interactiveshell.py\", line 3553, in run_code\r\n\r\n  File \"<ipython-input-4-9cc273be2d5a>\", line 28, in <cell line: 28>\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 368, in fit\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 216, in function\r\n\r\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 129, in multi_step_on_iterator\r\n\r\nduring context [Unknown]: Seen floating point types of different precisions in %multiply.43589 = f32[2,8,8,1280]{3,2,1,0} multiply(f32[2,8,8,1280]{3,2,1,0} %add.43539, f16[2,8,8,1280]{3,2,1,0} %multiply.43588), metadata={op_type=\"Mul\" op_name=\"mul_9\" source_file=\"\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/framework\/ops.py\" source_line=1196}, but mixed precision is disallowed.\r\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_124474]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:xla","TF 2.18"],"created_at":"2024-11-29T07:11:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/81273"},{"repository":"tensorflow\/tensorflow","title":"Some operators give different results on CPU and GPU when dealing with complex numbers that include `inf`.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04.3\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe outputs of TensorFlow mathematical APIs (`sin, cos, tan, sinh, cosh, exp, and reduce_mean`) are inconsistent between the CPU and GPU when applied to complex inputs containing `inf`. \n\n### Standalone code to reproduce the issue\n\n```shell\npython\r\nimport tensorflow as tf\r\n\r\ntest_inputs = [\r\n    tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128),\r\n]\r\n\r\ntest_apis = [\r\n    tf.math.sin, tf.math.cos, tf.math.tan,\r\n    tf.math.sinh, tf.math.cosh, tf.math.exp, tf.math.reduce_mean\r\n]\r\n\r\nfor api in test_apis:\r\n    print(f\"Testing {api.__name__}\")\r\n    for x in test_inputs:\r\n        try:\r\n            with tf.device('\/CPU'):\r\n              cpu_out = api(x)\r\n              print(f\"CPU Output: {cpu_out}\")\r\n            with tf.device('\/GPU:0'):\r\n              gpu_out = api(x)\r\n              print(f\"GPU Output: {gpu_out}\")\r\n        except Exception as e:\r\n            print(f\"Error in {api.__name__}: {e}\")\n```\n\n\n### Relevant log output\n\n```shell\nTesting sin\r\nCPU Output: [nan +0.j  0.+infj nan+infj]\r\nGPU Output: [nan+nanj nan+infj nan+nanj]\r\nTesting cos\r\nCPU Output: [nan +0.j inf -0.j inf+nanj]\r\nGPU Output: [nan+nanj inf+nanj nan+nanj]\r\nTesting tan\r\nCPU Output: [nan+0.j  0.+1.j  0.+1.j]\r\nGPU Output: [nan+nanj  0. +1.j  0. +1.j]\r\nTesting sinh\r\nCPU Output: [inf +0.j  0.+nanj inf+nanj]\r\nGPU Output: [inf+nanj nan+nanj nan+nanj]\r\nTesting cosh\r\nCPU Output: [inf +0.j nan +0.j inf+nanj]\r\nGPU Output: [inf+nanj nan+nanj nan+nanj]\r\nTesting exp\r\nCPU Output: [inf +0.j nan+nanj inf+nanj]\r\nGPU Output: [inf+nanj nan+nanj nan+nanj]\r\nTesting reduce_mean\r\nCPU Output: (inf+infj)\r\nGPU Output: (nan+nanj)\n```\n","labels":["stat:awaiting response","type:bug","stale","comp:ops","2.17"],"created_at":"2024-11-27T13:13:05Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80947"},{"repository":"tensorflow\/tensorflow","title":"When using `tf.math.log1p` and NumPy's `np.log1p` with the same complex input, the outputs are inconsistent.","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 22.04.3\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow's `tf.math.log1p` produces inconsistent results with NumPy's `np.log1p` for complex inputs containing `inf`, such as `[inf+0.j, 0+inf.j, inf+inf.j]`. TensorFlow outputs `[inf+0.j, nan+nanj, nan+nanj]`, while NumPy returns `[inf+0.j, inf+1.57079633j, inf+0.78539816j]`.\n\n### Standalone code to reproduce the issue\n\n```shell\npython\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ntest_input = tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128)\r\n\r\n# TensorFlow computation\r\nwith tf.device('\/CPU:0'):\r\n    cpu_out = tf.math.log1p(test_input)\r\n\r\n# NumPy computation\r\nnumpy_out = np.log1p(test_input.numpy())\r\n\r\nprint(f\"CPU Output: {cpu_out}\")\r\nprint(f\"NumPy Output: {numpy_out}\")\n```\n\n\n### Relevant log output\n\n```shell\nCPU Output: [inf +0.j nan+nanj nan+nanj]\r\nNumPy Output: [inf+0.j         inf+1.57079633j inf+0.78539816j]\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-26T13:36:51Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80850"},{"repository":"tensorflow\/tensorflow","title":"Heap-buffer-overflow in `SparseMatrixSparseCholesky`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen batch_index is scalar, `SparseMatrixSparseCholesky` triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nindices = tf.constant([[0, 0], [1, 1], [2, 1], [2, 2], [3, 3]], dtype=tf.int64)\r\nvalues = tf.constant([1.0, 2.0, 1.0, 3.0, 4.0], tf.float32)\r\ndense_shape = tf.constant([4, 4], dtype=tf.int64)\r\ninput = tf.raw_ops.SparseTensorToCSRSparseMatrix(\r\n    indices=indices, values=values, dense_shape=dense_shape, name=None\r\n)\r\npermutation = tf.constant([4,1,1,1], dtype=tf.int32)\r\n\r\ntf.raw_ops.SparseMatrixSparseCholesky(\r\n  input=input, permutation=permutation, type=tf.float32\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n=================================================================\r\n==3331846==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60700049d1d0 at pc 0x7faa202d6d87 bp 0x7ffd09bf8fe0 sp 0x7ffd09bf8fd0\r\nWRITE of size 4 at 0x60700049d1d0 thread T0\r\n    #0 0x7faa202d6d86 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2a2d86)\r\n    #1 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f373a)\r\n    #2 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f84e9)\r\n    #3 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x4201d41)\r\n    #4 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x26d8d71)\r\n    #5 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2adab6)\r\n    #6 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1cdc14a)\r\n    #7 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1862fe6)\r\n    #8 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x173f306)\r\n    #9 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x179cf74)\r\n    #10 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x17aa8b1)\r\n    #11 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c582eb)\r\n    #12 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aedc2c)\r\n    #13 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31af066a)\r\n    #14 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c2a939)\r\n    #15 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31addde4)\r\n    #16 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31ae1dd4)\r\n    #17 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aebd26)\r\n    #18 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x20ad4c33)\r\n    #19 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c1fe5e)\r\n    #20 0x7fa9f268645b in TFE_Execute (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x1065245b)\r\n    #21 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/_pywrap_tensorflow_internal.so+0x3c2274)\r\n    #22 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0xb7ccb)\r\n    #23 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0x1ce899)\r\n    #24 0x51ad66  (\/usr\/bin\/python3.11+0x51ad66)\r\n    #25 0x4e75db in _PyObject_MakeTpCall (\/usr\/bin\/python3.11+0x4e75db)\r\n    #26 0x4fb151 in _PyEval_EvalFrameDefault (\/usr\/bin\/python3.11+0x4fb151)\r\n    #27 0x531822 in _PyFunction_Vectorcall (\/usr\/bin\/python3.11+0x531822)\r\n    #28 0x541194 in PyObject_Call (\/usr\/bin\/python3.11+0x541194)\r\n    #29 0x4fefe0 in _PyEval_EvalFrameDefault (\/usr\/bin\/python3.11+0x4fefe0)\r\n    #30 0x62e1b3  (\/usr\/bin\/python3.11+0x62e1b3)\r\n    #31 0x4f3a66 in PyEval_EvalCode (\/usr\/bin\/python3.11+0x4f3a66)\r\n    #32 0x647c36  (\/usr\/bin\/python3.11+0x647c36)\r\n    #33 0x64534f  (\/usr\/bin\/python3.11+0x64534f)\r\n    #34 0x650d14  (\/usr\/bin\/python3.11+0x650d14)\r\n    #35 0x650a63 in _PyRun_SimpleFileObject (\/usr\/bin\/python3.11+0x650a63)\r\n    #36 0x650832 in _PyRun_AnyFileObject (\/usr\/bin\/python3.11+0x650832)\r\n    #37 0x64f786 in Py_RunMain (\/usr\/bin\/python3.11+0x64f786)\r\n    #38 0x61ee0c in Py_BytesMain (\/usr\/bin\/python3.11+0x61ee0c)\r\n    #39 0x7faaf80d1d8f  (\/lib\/x86_64-linux-gnu\/libc.so.6+0x29d8f)\r\n    #40 0x7faaf80d1e3f in __libc_start_main (\/lib\/x86_64-linux-gnu\/libc.so.6+0x29e3f)\r\n    #41 0x61ec94 in _start (\/usr\/bin\/python3.11+0x61ec94)\r\n\r\n0x60700049d1d0 is located 0 bytes to the right of 80-byte region [0x60700049d180,0x60700049d1d0)\r\nallocated by thread T0 here:\r\n    #0 0x7faaf84bf887 in __interceptor_malloc ..\/..\/..\/..\/src\/libsanitizer\/asan\/asan_malloc_linux.cpp:145\r\n    #1 0x7faa202d6803 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2a2803)\r\n    #2 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f373a)\r\n    #3 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x41f84e9)\r\n    #4 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x4201d41)\r\n    #5 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x26d8d71)\r\n    #6 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2adab6)\r\n    #7 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1cdc14a)\r\n    #8 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x1862fe6)\r\n    #9 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x173f306)\r\n    #10 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x179cf74)\r\n    #11 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_framework.so.2+0x17aa8b1)\r\n    #12 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c582eb)\r\n    #13 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aedc2c)\r\n    #14 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31af066a)\r\n    #15 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c2a939)\r\n    #16 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31addde4)\r\n    #17 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31ae1dd4)\r\n    #18 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31aebd26)\r\n    #19 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x20ad4c33)\r\n    #20 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x31c1fe5e)\r\n    #21 0x7fa9f268645b in TFE_Execute (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x1065245b)\r\n    #22 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/_pywrap_tensorflow_internal.so+0x3c2274)\r\n    #23 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0xb7ccb)\r\n    #24 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/_pywrap_tfe.so+0x1ce899)\r\n    #25 0x51ad66  (\/usr\/bin\/python3.11+0x51ad66)\r\n\r\nSUMMARY: AddressSanitizer: heap-buffer-overflow (\/mnt\/venv\/tensorflow-2.16.1-asan\/lib\/python3.11\/site-packages\/tensorflow\/python\/platform\/..\/..\/libtensorflow_cc.so.2+0x3e2a2d86) in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const\r\nShadow bytes around the buggy address:\r\n  0x0c0e8008b9e0: 00 00 00 00 00 00 fa fa fa fa fd fd fd fd fd fd\r\n  0x0c0e8008b9f0: fd fd fd fd fa fa fa fa 00 00 00 00 00 00 00 00\r\n  0x0c0e8008ba00: 00 fa fa fa fa fa 00 00 00 00 00 00 00 00 04 fa\r\n  0x0c0e8008ba10: fa fa fa fa 00 00 00 00 00 00 00 00 04 fa fa fa\r\n  0x0c0e8008ba20: fa fa 00 00 00 00 00 00 00 00 04 fa fa fa fa fa\r\n=>0x0c0e8008ba30: 00 00 00 00 00 00 00 00 00 00[fa]fa fa fa fa fa\r\n  0x0c0e8008ba40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba50: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba60: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba70: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\n  0x0c0e8008ba80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa\r\nShadow byte legend (one shadow byte represents 8 application bytes):\r\n  Addressable:           00\r\n  Partially addressable: 01 02 03 04 05 06 07\r\n  Heap left redzone:       fa\r\n  Freed heap region:       fd\r\n  Stack left redzone:      f1\r\n  Stack mid redzone:       f2\r\n  Stack right redzone:     f3\r\n  Stack after return:      f5\r\n  Stack use after scope:   f8\r\n  Global redzone:          f9\r\n  Global init order:       f6\r\n  Poisoned by user:        f7\r\n  Container overflow:      fc\r\n  Array cookie:            ac\r\n  Intra object redzone:    bb\r\n  ASan internal:           fe\r\n  Left alloca redzone:     ca\r\n  Right alloca redzone:    cb\r\n  Shadow gap:              cc\r\n==3331846==ABORTING\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-26T12:42:50Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80847"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `RaggedBincount`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the value of size is close to the maximum value for the dtype, an integer overflow error will occur in following multiplication calculations.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nsplits = tf.constant([0, 3, 5, 9], dtype=tf.int64)\r\nvalues = tf.constant(1, shape=[3,3], dtype=tf.int64)\r\nsize = tf.constant(6522107765268123892, dtype=tf.int64)\r\nweights = tf.constant(1, shape=[3,3], dtype=tf.float32)\r\ncounts = tf.raw_ops.RaggedBincount(splits=splits, values=values, size=size, weights=weights)\n```\n\n\n### Relevant log output\n\n```shell\nStatus: INVALID_ARGUMENT: Encountered overflow when multiplying 3 with 6522107765268123892, result: -1\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-26T03:18:31Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80812"},{"repository":"tensorflow\/tensorflow","title":"This method creates a model with a 100% memory leak loop using model. fit()","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nubuntu 2.2 or mac m1\r\n\r\n### Mobile device\r\n\r\nubuntu 2.2 or mac m1\r\n\r\n### Python version\r\n\r\n3.11\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport gc\r\n\r\nimport keras\r\nimport numpy as np\r\nimport psutil\r\nfrom keras.optimizers import Adam\r\nfrom keras.layers import Dense, Dropout, Input, LSTM\r\nimport tensorflow as tf\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom tensorflow.keras.models import Sequential, load_model\r\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\nimport time\r\nimport json\r\n\r\n\r\nnum_samples = 6\r\nnum_features = 3\r\nnum_classes = 4\r\nepochs = 50\r\nbatch_size = 2\r\nidentifier = \"test_model\"\r\nnum_iterations = 500  \r\n\r\ndef build_model(X, num_classes):\r\n    model = Sequential()\r\n    model.add(Input(shape=(X.shape[1], X.shape[2])))\r\n    model.add(LSTM(16, return_sequences=True))\r\n    model.add(LSTM(16))\r\n    model.add(Dropout(0.4))\r\n    model.add(Dense(8, activation='tanh'))\r\n    model.add(Dense(num_classes, activation='softmax'))\r\n\r\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])\r\n    return model\r\n\r\n\r\ndata_X = np.random.rand(num_samples, num_features)\r\ndata_Y = np.random.randint(0, num_classes, size=(num_samples, 1))  \r\n\r\n\r\ndata_Y = np.eye(num_classes)[data_Y.flatten()]  \r\nprint(type(data_X))\r\n\r\nscaler = MinMaxScaler()\r\ndata_X_scaled = scaler.fit_transform(data_X)\r\n\r\n\r\ntrain_X, test_X, train_Y, test_Y = train_test_split(data_X_scaled, data_Y, train_size=0.6, random_state=42)\r\n\r\n\r\ntrain_X = np.expand_dims(train_X, axis=1)\r\ntest_X = np.expand_dims(test_X, axis=1)\r\n\r\nbest_loss = np.inf\r\nbest_model_data = None\r\nfor iteration in range(num_iterations):\r\n   \r\n    tf.keras.backend.clear_session()\r\n    tf.compat.v1.reset_default_graph()\r\n    model = build_model(train_X, num_classes)\r\n \r\n    model_name = f\"model_{iteration}\"\r\n    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=0, restore_best_weights=True)\r\n    print(f\"Iteration {iteration + 1}\/{num_iterations}\")\r\n    process = psutil.Process()\r\n    mem_info = process.memory_info()\r\n    print(f\"start Current memory usage: {mem_info.rss \/ (1024 * 1024):.2f} MB\")  # RSS - Resident Set Size\r\n    try:\r\n        history = model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, shuffle=True,\r\n                            validation_data=(test_X, test_Y), verbose=0)\r\n        current_loss = history.history['loss'][-1]\r\n        print(f\"Training model: {model.name}\")\r\n    \r\n        del model\r\n        tf.keras.backend.clear_session()\r\n        gc.collect()\r\n    except Exception as e:\r\n        print(\"err:\", e)\r\n    finally:\r\n        process = psutil.Process()\r\n        mem_info = process.memory_info()\r\n        print(f\"end Current memory usage: {mem_info.rss \/ (1024 * 1024):.2f} MB\")  # RSS - Resident Set Size\r\n\r\nprint(\"end\uff01\")\r\n\r\n\r\nif best_model_data:\r\n    model_json = best_model_data[\"model_architecture\"]\r\n    model_weights = json.loads(best_model_data[\"model_weights\"], object_hook=lambda d: np.array(d))\r\n    model = tf.keras.models.model_from_json(model_json)\r\n    model.set_weights(model_weights)\r\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])\r\n    print(\"ok\")\r\nelse:\r\n    print(\"not found\")\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nIteration 1\/500\r\nstart Current memory usage: 450.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 524.41 MB\r\nIteration 2\/500\r\nstart Current memory usage: 524.52 MB\r\nTraining model: sequential\r\nend Current memory usage: 564.97 MB\r\nIteration 3\/500\r\nstart Current memory usage: 564.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 598.00 MB\r\nIteration 4\/500\r\nstart Current memory usage: 598.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 624.69 MB\r\nIteration 5\/500\r\nstart Current memory usage: 624.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 653.89 MB\r\nIteration 6\/500\r\nstart Current memory usage: 653.91 MB\r\nTraining model: sequential\r\nend Current memory usage: 679.45 MB\r\nIteration 7\/500\r\nstart Current memory usage: 679.45 MB\r\nTraining model: sequential\r\nend Current memory usage: 701.59 MB\r\nIteration 8\/500\r\nstart Current memory usage: 701.59 MB\r\nTraining model: sequential\r\nend Current memory usage: 726.83 MB\r\nIteration 9\/500\r\nstart Current memory usage: 726.84 MB\r\nTraining model: sequential\r\nend Current memory usage: 749.56 MB\r\nIteration 10\/500\r\nstart Current memory usage: 749.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 782.56 MB\r\nIteration 11\/500\r\nstart Current memory usage: 782.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 805.92 MB\r\nIteration 12\/500\r\nstart Current memory usage: 805.92 MB\r\nTraining model: sequential\r\nend Current memory usage: 833.17 MB\r\nIteration 13\/500\r\nstart Current memory usage: 833.17 MB\r\nTraining model: sequential\r\nend Current memory usage: 852.84 MB\r\nIteration 14\/500\r\nstart Current memory usage: 852.84 MB\r\nTraining model: sequential\r\nend Current memory usage: 875.05 MB\r\nIteration 15\/500\r\nstart Current memory usage: 875.06 MB\r\nTraining model: sequential\r\nend Current memory usage: 901.56 MB\r\nIteration 16\/500\r\nstart Current memory usage: 901.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 930.62 MB\r\nIteration 17\/500\r\nstart Current memory usage: 705.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 762.64 MB\r\nIteration 18\/500\r\nstart Current memory usage: 762.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 798.06 MB\r\nIteration 19\/500\r\nstart Current memory usage: 798.17 MB\r\nTraining model: sequential\r\nend Current memory usage: 824.98 MB\r\nIteration 20\/500\r\nstart Current memory usage: 824.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 850.34 MB\r\nIteration 21\/500\r\nstart Current memory usage: 850.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 876.81 MB\r\nIteration 22\/500\r\nstart Current memory usage: 876.81 MB\r\nTraining model: sequential\r\nend Current memory usage: 904.02 MB\r\nIteration 23\/500\r\nstart Current memory usage: 904.08 MB\r\nTraining model: sequential\r\nend Current memory usage: 929.70 MB\r\nIteration 24\/500\r\nstart Current memory usage: 929.73 MB\r\nTraining model: sequential\r\nend Current memory usage: 952.33 MB\r\nIteration 25\/500\r\nstart Current memory usage: 952.34 MB\r\nTraining model: sequential\r\nend Current memory usage: 952.28 MB\r\nIteration 26\/500\r\nstart Current memory usage: 952.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 980.39 MB\r\nIteration 27\/500\r\nstart Current memory usage: 978.78 MB\r\nTraining model: sequential\r\nend Current memory usage: 999.02 MB\r\nIteration 28\/500\r\nstart Current memory usage: 999.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1023.50 MB\r\nIteration 29\/500\r\nstart Current memory usage: 1023.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1047.80 MB\r\nIteration 30\/500\r\nstart Current memory usage: 1047.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1068.88 MB\r\nIteration 31\/500\r\nstart Current memory usage: 1068.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1095.78 MB\r\nIteration 32\/500\r\nstart Current memory usage: 1095.78 MB\r\nTraining model: sequential\r\nend Current memory usage: 1119.03 MB\r\nIteration 33\/500\r\nstart Current memory usage: 1119.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 1039.41 MB\r\nIteration 34\/500\r\nstart Current memory usage: 1022.78 MB\r\nTraining model: sequential\r\nend Current memory usage: 1040.88 MB\r\nIteration 35\/500\r\nstart Current memory usage: 1040.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 1054.58 MB\r\nIteration 36\/500\r\nstart Current memory usage: 1054.58 MB\r\nTraining model: sequential\r\nend Current memory usage: 1076.16 MB\r\nIteration 37\/500\r\nstart Current memory usage: 1076.19 MB\r\nTraining model: sequential\r\nend Current memory usage: 1097.02 MB\r\nIteration 38\/500\r\nstart Current memory usage: 1097.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 1113.70 MB\r\nIteration 39\/500\r\nstart Current memory usage: 1114.12 MB\r\nTraining model: sequential\r\nend Current memory usage: 1140.30 MB\r\nIteration 40\/500\r\nstart Current memory usage: 1140.33 MB\r\nTraining model: sequential\r\nend Current memory usage: 1163.81 MB\r\nIteration 41\/500\r\nstart Current memory usage: 1163.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 1195.83 MB\r\nIteration 42\/500\r\nstart Current memory usage: 1195.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1221.53 MB\r\nIteration 43\/500\r\nstart Current memory usage: 1221.55 MB\r\nTraining model: sequential\r\nend Current memory usage: 1231.09 MB\r\nIteration 44\/500\r\nstart Current memory usage: 1231.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1245.78 MB\r\nIteration 45\/500\r\nstart Current memory usage: 1199.55 MB\r\nTraining model: sequential\r\nend Current memory usage: 1221.59 MB\r\nIteration 46\/500\r\nstart Current memory usage: 1221.59 MB\r\nTraining model: sequential\r\nend Current memory usage: 1249.11 MB\r\nIteration 47\/500\r\nstart Current memory usage: 1249.22 MB\r\nTraining model: sequential\r\nend Current memory usage: 1275.50 MB\r\nIteration 48\/500\r\nstart Current memory usage: 1259.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1290.91 MB\r\nIteration 49\/500\r\nstart Current memory usage: 1285.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1296.75 MB\r\nIteration 50\/500\r\nstart Current memory usage: 1296.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1306.59 MB\r\nIteration 51\/500\r\nstart Current memory usage: 1306.59 MB\r\nTraining model: sequential\r\nend Current memory usage: 1287.53 MB\r\nIteration 52\/500\r\nstart Current memory usage: 1287.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1297.23 MB\r\nIteration 53\/500\r\nstart Current memory usage: 1297.25 MB\r\nTraining model: sequential\r\nend Current memory usage: 1285.45 MB\r\nIteration 54\/500\r\nstart Current memory usage: 1285.45 MB\r\nTraining model: sequential\r\nend Current memory usage: 1290.36 MB\r\nIteration 55\/500\r\nstart Current memory usage: 1282.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1302.14 MB\r\nIteration 56\/500\r\nstart Current memory usage: 1302.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1287.70 MB\r\nIteration 57\/500\r\nstart Current memory usage: 1287.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1282.77 MB\r\nIteration 58\/500\r\nstart Current memory usage: 1271.38 MB\r\nTraining model: sequential\r\nend Current memory usage: 1232.14 MB\r\nIteration 59\/500\r\nstart Current memory usage: 1212.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 1201.16 MB\r\nIteration 60\/500\r\nstart Current memory usage: 1200.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1169.45 MB\r\nIteration 61\/500\r\nstart Current memory usage: 1169.45 MB\r\nTraining model: sequential\r\nend Current memory usage: 1209.73 MB\r\nIteration 62\/500\r\nstart Current memory usage: 1207.19 MB\r\nTraining model: sequential\r\nend Current memory usage: 1226.28 MB\r\nIteration 63\/500\r\nstart Current memory usage: 1226.28 MB\r\nTraining model: sequential\r\nend Current memory usage: 1231.45 MB\r\nIteration 64\/500\r\nstart Current memory usage: 1210.11 MB\r\nTraining model: sequential\r\nend Current memory usage: 1176.00 MB\r\nIteration 65\/500\r\nstart Current memory usage: 1173.97 MB\r\nTraining model: sequential\r\nend Current memory usage: 1201.42 MB\r\nIteration 66\/500\r\nstart Current memory usage: 1201.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 1223.94 MB\r\nIteration 67\/500\r\nstart Current memory usage: 1222.50 MB\r\nTraining model: sequential\r\nend Current memory usage: 1229.80 MB\r\nIteration 68\/500\r\nstart Current memory usage: 1227.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1219.02 MB\r\nIteration 69\/500\r\nstart Current memory usage: 1210.48 MB\r\nTraining model: sequential\r\nend Current memory usage: 1247.17 MB\r\nIteration 70\/500\r\nstart Current memory usage: 1245.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1259.84 MB\r\nIteration 71\/500\r\nstart Current memory usage: 1259.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 1286.39 MB\r\nIteration 72\/500\r\nstart Current memory usage: 1286.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1316.52 MB\r\nIteration 73\/500\r\nstart Current memory usage: 1311.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1338.72 MB\r\nIteration 74\/500\r\nstart Current memory usage: 1338.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1348.45 MB\r\nIteration 75\/500\r\nstart Current memory usage: 1338.30 MB\r\nTraining model: sequential\r\nend Current memory usage: 1354.97 MB\r\nIteration 76\/500\r\nstart Current memory usage: 1353.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1385.67 MB\r\nIteration 77\/500\r\nstart Current memory usage: 1385.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1408.83 MB\r\nIteration 78\/500\r\nstart Current memory usage: 1408.88 MB\r\nTraining model: sequential\r\nend Current memory usage: 1430.91 MB\r\nIteration 79\/500\r\nstart Current memory usage: 1430.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1443.62 MB\r\nIteration 80\/500\r\nstart Current memory usage: 1428.00 MB\r\nTraining model: sequential\r\nend Current memory usage: 1436.50 MB\r\nIteration 81\/500\r\nstart Current memory usage: 1436.64 MB\r\nTraining model: sequential\r\nend Current memory usage: 1454.66 MB\r\nIteration 82\/500\r\nstart Current memory usage: 1440.91 MB\r\nTraining model: sequential\r\nend Current memory usage: 1461.81 MB\r\nIteration 83\/500\r\nstart Current memory usage: 1460.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 1481.19 MB\r\nIteration 84\/500\r\nstart Current memory usage: 1481.19 MB\r\nTraining model: sequential\r\nend Current memory usage: 1477.84 MB\r\nIteration 85\/500\r\nstart Current memory usage: 1477.84 MB\r\nTraining model: sequential\r\nend Current memory usage: 1493.55 MB\r\nIteration 86\/500\r\nstart Current memory usage: 1493.58 MB\r\nTraining model: sequential\r\nend Current memory usage: 1509.50 MB\r\nIteration 87\/500\r\nstart Current memory usage: 1509.50 MB\r\nTraining model: sequential\r\nend Current memory usage: 1543.94 MB\r\nIteration 88\/500\r\nstart Current memory usage: 1542.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1516.17 MB\r\nIteration 89\/500\r\nstart Current memory usage: 1516.20 MB\r\nTraining model: sequential\r\nend Current memory usage: 1470.17 MB\r\nIteration 90\/500\r\nstart Current memory usage: 1470.22 MB\r\nTraining model: sequential\r\nend Current memory usage: 1443.72 MB\r\nIteration 91\/500\r\nstart Current memory usage: 1444.36 MB\r\nTraining model: sequential\r\nend Current memory usage: 1486.23 MB\r\nIteration 92\/500\r\nstart Current memory usage: 1476.41 MB\r\nTraining model: sequential\r\nend Current memory usage: 1524.97 MB\r\nIteration 93\/500\r\nstart Current memory usage: 1524.97 MB\r\nTraining model: sequential\r\nend Current memory usage: 1534.94 MB\r\nIteration 94\/500\r\nstart Current memory usage: 1551.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1853.48 MB\r\nIteration 95\/500\r\nstart Current memory usage: 1853.48 MB\r\nTraining model: sequential\r\nend Current memory usage: 1790.12 MB\r\nIteration 96\/500\r\nstart Current memory usage: 1792.27 MB\r\nTraining model: sequential\r\nend Current memory usage: 1883.20 MB\r\nIteration 97\/500\r\nstart Current memory usage: 1879.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1759.69 MB\r\nIteration 98\/500\r\nstart Current memory usage: 1669.66 MB\r\nTraining model: sequential\r\nend Current memory usage: 1596.77 MB\r\nIteration 99\/500\r\nstart Current memory usage: 1597.12 MB\r\nTraining model: sequential\r\nend Current memory usage: 1568.83 MB\r\nIteration 100\/500\r\nstart Current memory usage: 1532.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1516.75 MB\r\nIteration 101\/500\r\nstart Current memory usage: 1465.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1486.66 MB\r\nIteration 102\/500\r\nstart Current memory usage: 1483.34 MB\r\nTraining model: sequential\r\nend Current memory usage: 1523.19 MB\r\nIteration 103\/500\r\nstart Current memory usage: 1523.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1532.77 MB\r\nIteration 104\/500\r\nstart Current memory usage: 1531.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1561.78 MB\r\nIteration 105\/500\r\nstart Current memory usage: 1555.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1586.70 MB\r\nIteration 106\/500\r\nstart Current memory usage: 1586.75 MB\r\nTraining model: sequential\r\nend Current memory usage: 1608.41 MB\r\nIteration 107\/500\r\nstart Current memory usage: 1603.81 MB\r\nTraining model: sequential\r\nend Current memory usage: 1629.00 MB\r\nIteration 108\/500\r\nstart Current memory usage: 1629.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1609.25 MB\r\nIteration 109\/500\r\nstart Current memory usage: 1609.31 MB\r\nTraining model: sequential\r\nend Current memory usage: 1630.09 MB\r\nIteration 110\/500\r\nstart Current memory usage: 1629.20 MB\r\nTraining model: sequential\r\nend Current memory usage: 1638.66 MB\r\nIteration 111\/500\r\nstart Current memory usage: 1620.30 MB\r\nTraining model: sequential\r\nend Current memory usage: 1642.81 MB\r\nIteration 112\/500\r\nstart Current memory usage: 1642.94 MB\r\nTraining model: sequential\r\nend Current memory usage: 1659.45 MB\r\nIteration 113\/500\r\nstart Current memory usage: 1655.17 MB\r\nTraining model: sequential\r\nend Current memory usage: 1687.80 MB\r\nIteration 114\/500\r\nstart Current memory usage: 1673.33 MB\r\nTraining model: sequential\r\nend Current memory usage: 1705.94 MB\r\nIteration 115\/500\r\nstart Current memory usage: 1699.95 MB\r\nTraining model: sequential\r\nend Current memory usage: 1708.22 MB\r\nIteration 116\/500\r\nstart Current memory usage: 1707.88 MB\r\nTraining model: sequential\r\nend Current memory usage: 1648.23 MB\r\nIteration 117\/500\r\nstart Current memory usage: 1634.03 MB\r\nTraining model: sequential\r\nend Current memory usage: 1670.97 MB\r\nIteration 118\/500\r\nstart Current memory usage: 1671.97 MB\r\nTraining model: sequential\r\nend Current memory usage: 1649.69 MB\r\nIteration 119\/500\r\nstart Current memory usage: 1645.14 MB\r\nTraining model: sequential\r\nend Current memory usage: 1698.64 MB\r\nIteration 120\/500\r\nstart Current memory usage: 1699.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1737.67 MB\r\nIteration 121\/500\r\nstart Current memory usage: 1737.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1738.05 MB\r\nIteration 122\/500\r\nstart Current memory usage: 1721.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 1730.64 MB\r\nIteration 123\/500\r\nstart Current memory usage: 1729.53 MB\r\nTraining model: sequential\r\nend Current memory usage: 1766.12 MB\r\nIteration 124\/500\r\nstart Current memory usage: 1761.22 MB\r\nTraining model: sequential\r\nend Current memory usage: 1796.58 MB\r\nIteration 125\/500\r\nstart Current memory usage: 1796.73 MB\r\nTraining model: sequential\r\nend Current memory usage: 1709.02 MB\r\nIteration 126\/500\r\nstart Current memory usage: 1721.67 MB\r\nTraining model: sequential\r\nend Current memory usage: 1771.50 MB\r\nIteration 127\/500\r\nstart Current memory usage: 1771.50 MB\r\nTraining model: sequential\r\nend Current memory usage: 1777.38 MB\r\nIteration 128\/500\r\nstart Current memory usage: 1757.58 MB\r\nTraining model: sequential\r\nend Current memory usage: 1806.50 MB\r\nIteration 129\/500\r\nstart Current memory usage: 1758.81 MB\r\nTraining model: sequential\r\nend Current memory usage: 1812.45 MB\r\nIteration 130\/500\r\nstart Current memory usage: 1812.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 1811.14 MB\r\nIteration 131\/500\r\nstart Current memory usage: 1799.61 MB\r\nTraining model: sequential\r\nend Current memory usage: 1835.33 MB\r\nIteration 132\/500\r\nstart Current memory usage: 1716.38 MB\r\nTraining model: sequential\r\nend Current memory usage: 1759.75 MB\r\nIteration 133\/500\r\nstart Current memory usage: 1752.44 MB\r\nTraining model: sequential\r\nend Current memory usage: 1818.41 MB\r\nIteration 134\/500\r\nstart Current memory usage: 1811.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 1853.58 MB\r\nIteration 135\/500\r\nstart Current memory usage: 1853.70 MB\r\nTraining model: sequential\r\nend Current memory usage: 1858.50 MB\r\nIteration 136\/500\r\nstart Current memory usage: 1858.56 MB\r\nTraining model: sequential\r\nend Current memory usage: 1874.84 MB\r\nIteration 137\/500\r\nstart Current memory usage: 1862.92 MB\r\nTraining model: sequential\r\nend Current memory usage: 1768.23 MB\r\nIteration 138\/500\r\nstart Current memory usage: 1762.73 MB\r\nTraining model: sequential\r\nend Current memory usage: 1843.39 MB\r\nIteration 139\/500\r\nstart Current memory usage: 1843.52 MB\r\nTraining model: sequential\r\nend Current memory usage: 1885.88 MB\r\nIteration 140\/500\r\nstart Current memory usage: 1885.95 MB\r\nTraining model: sequential\r\nend Current memory usage: 1924.86 MB\r\nIteration 141\/500\r\nstart Current memory usage: 1925.05 MB\r\nTraining model: sequential\r\nend Current memory usage: 1946.80 MB\r\nIteration 142\/500\r\nstart Current memory usage: 1946.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1977.53 MB\r\nIteration 143\/500\r\nstart Current memory usage: 1974.27 MB\r\nTraining model: sequential\r\nend Current memory usage: 1995.17 MB\r\nIteration 144\/500\r\nstart Current memory usage: 1992.41 MB\r\nTraining model: sequential\r\nend Current memory usage: 1984.45 MB\r\nIteration 145\/500\r\nstart Current memory usage: 1963.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 1947.31 MB\r\nIteration 146\/500\r\nstart Current memory usage: 1944.47 MB\r\nTraining model: sequential\r\nend Current memory usage: 1996.00 MB\r\nIteration 147\/500\r\nstart Current memory usage: 1996.08 MB\r\nTraining model: sequential\r\nend Current memory usage: 2008.41 MB\r\nIteration 148\/500\r\nstart Current memory usage: 1999.69 MB\r\nTraining model: sequential\r\nend Current memory usage: 1951.30 MB\r\nIteration 149\/500\r\nstart Current memory usage: 1942.98 MB\r\nTraining model: sequential\r\nend Current memory usage: 1992.28 MB\r\nIteration 150\/500\r\nstart Current memory usage: 1982.86 MB\r\nTraining model: sequential\r\nend Current memory usage: 2008.83 MB\r\nIteration 151\/500\r\nstart Current memory usage: 2008.83 MB\r\nTraining model: sequential\r\nend Current memory usage: 1946.42 MB\r\nIteration 152\/500\r\nstart Current memory usage: 1946.92 MB\r\nTraining model: sequential\r\nend Current memory usage: 1992.48 MB\r\nIteration 153\/500\r\nstart Current memory usage: 1979.52 MB\r\nTraining model: sequential\r\nend Current memory usage: 2035.66 MB\r\nIteration 154\/500\r\nstart Current memory usage: 2023.91 MB\r\nTraining model: sequential\r\nend Current memory usage: 2030.31 MB\r\nIteration 155\/500\r\nstart Current memory usage: 1974.39 MB\r\nTraining model: sequential\r\nend Current memory usage: 2029.30 MB\r\nIteration 156\/500\r\nstart Current memory usage: 1997.42 MB\r\nTraining model: sequential\r\nend Current memory usage: 2000.31 MB\r\nIteration 157\/500\r\nstart Current memory usage: 1964.38 MB\r\nTraining model: sequential\r\nend Current memory usage: 1979.45 MB\r\nIteration 158\/500\r\nstart Current memory usage: 1973.12 MB\r\nTraining model: sequential\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:keras","TF 2.18"],"created_at":"2024-11-25T12:12:55Z","comments":7,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80753"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.SparseConcat`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific inputs, `SparseConcat` triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nindices1 = tf.constant(2, shape=[3,3], dtype=tf.int64)\r\nvalues1 = tf.constant(\"aaaabaaacaaadaaaeaaafaaagaaahaaaiaaajaaakaaalaaamaaanaaaoaaapaaaqaaaraaasaaataaauaaavaaawaaaxaaayaaazaabbaabcaabdaabeaabfaabgaabhaabiaabjaabkaablaabmaabnaaboaabpaabqaabraabsaabtaabuaabvaabwaabxaabyaabzaacbaaccaacdaaceaacfaacgaachaaciaacjaackaaclaacmaacnaacoaacpaacqaacraacsaactaacuaacvaacwaacxaacyaac\",\r\n    shape=[3], dtype=tf.string)\r\nshapes1 = tf.constant([5, 2, 2147483647], dtype=tf.int64)\r\n\r\nindices2 = tf.constant(-2, shape=[4,3], dtype=tf.int64)\r\nvalues2 = tf.constant(\" \", shape=[4], dtype=tf.string)\r\nshapes2 = tf.constant([5,1879048192,536870912], dtype=tf.int64)\r\n\r\nconcat_dim = 1\r\ntf.raw_ops.SparseConcat(\r\n    indices=[indices1, indices2], values=[values1, values2], shapes=[shapes1, shapes2], concat_dim=concat_dim, name=None\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-11-24 06:36:10.994508: F tensorflow\/core\/framework\/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements() status: INVALID_ARGUMENT: Shape [5,1879048194,2147483647] results in overflow when computing number of elements\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-24T06:40:09Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80669"},{"repository":"tensorflow\/tensorflow","title":"Floating point exception (core dumped) in `tf.raw_ops.Reshape`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific inputs,  `tf.raw_ops.Reshape` triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntensor = tf.constant(-3.5e+35, shape=[5], dtype=tf.float32)\r\nshape = tf.constant([0, 1879048192, 100000000, 1610612736, -1], dtype=tf.int32)\r\n\r\ntf.raw_ops.Reshape(tensor=tensor, shape=shape)\n```\n\n\n### Relevant log output\n\n```shell\nFloating point exception (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-22T02:58:01Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80529"},{"repository":"tensorflow\/tensorflow","title":"The documentation in `data_performance.ipynb` uses `py_function()` without an explanation","description":"### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nTF 2.18\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIn the [\"Better performance with the tf.data API\" guide](https:\/\/www.tensorflow.org\/guide\/data_performance), `tf.py_function()` is used several times in the mapping function. There are various performance boosts demonstrated by the guide with said mapping functions. However, there does not seem to be any practical reason why `tf.py_function()` is used inside the mapping functions. In fact, if you remove those, the behavior is the same; in other words, there doesn't seem to be a need for them at all.\r\n\r\nCuriously, if you remove them from the examples and then perform the time measurements, the speedup goes away. For example, consider the following mapping function from [the guide](https:\/\/github.com\/tensorflow\/docs\/blob\/bbc0b9c70fc0bd4411793d1b0bcc56ef1dbc2405\/site\/en\/guide\/data_performance.ipynb#L447-L450):\r\n\r\n```python\r\ndef mapped_function(s):\r\n    # Do some hard pre-processing\r\n    tf.py_function(lambda: time.sleep(0.03), [], ())\r\n    return s\r\n```\r\n\r\nIn the sequential case, it is shown that the following mapping code results in a time measurement of 0.49222556600034295:\r\n\r\n```python\r\nbenchmark(\r\n    ArtificialDataset()\r\n    .map(mapped_function)\r\n)\r\n```\r\n\r\nAnd, the optimized (parallel) version as follows results in a time measurement of 0.36392719900049997:\r\n\r\n```python\r\nbenchmark(\r\n    ArtificialDataset()\r\n    .map(\r\n        mapped_function,\r\n        num_parallel_calls=tf.data.AUTOTUNE\r\n    )\r\n)\r\n```\r\n\r\nBut, if I remove the `tf.py_function()` from the mapping function, I get comparable measurements from both examples, namely, 0.22448736599994845 and 0.2266392660001202:\r\n\r\n```python\r\ndef mapped_function(s):\r\n    # Do some hard pre-processing\r\n    lambda: time.sleep(0.03), [], ()\r\n    return s\r\n```\r\n\r\nIn fact, that measurement is even better, which makes me believe that this example is contrived to show a performance benefit by using `num_parallel_calls` when in fact TF is already optimizing the code without it. Frivolously wrapping the function in `tf.py_function()` is most likely  causing TensorFlow *not* to optimize the function. Thus, is `num_parallel_calls` even needed to achieve better performance?\n\n### Standalone code to reproduce the issue\n\n```shell\nMentioned in the above description.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:docs-bug","stat:awaiting tensorflower","type:bug"],"created_at":"2024-11-20T16:01:24Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80365"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.MatrixSolve`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04.3 LTS (x86_64)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen the shape of the matrix argument is empty and the gpu is available, tf.raw_ops.MatrixSolve triggers a crash.\r\nIt can be reproduced on tf-nightly when the gpu is available.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\ntf.raw_ops.MatrixSolve(matrix=tf.random.uniform([], dtype=tf.dtypes.double, maxval=1000000000), rhs=tf.random.uniform([1, 2], dtype=tf.dtypes.double, maxval=1000000000), adjoint=True)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-11-20 14:51:08.714846: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-11-20 14:51:08.775383: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-11-20 14:51:08.852267: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-11-20 14:51:08.876168: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-20 14:51:08.931206: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-11-20 14:51:16.385650: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-11-20 14:51:16.387914: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-11-20 14:51:16.542383: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-20T06:53:21Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80331"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.MatrixInverse`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf 2.17\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the shape of the input argument is empty and the gpu is available, tf.raw_ops.MatrixInverse triggers a crash.\r\nIt can be reproduced on tf-nightly when the gpu is available.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ntf.raw_ops.MatrixInverse(input=tf.cast(tf.random.uniform([], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex128),adjoint=True)\n```\n\n\n### Relevant log output\n\n```shell\n2024-11-20 10:46:15.940818: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-11-20 10:46:16.001155: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-11-20 10:46:16.076386: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-11-20 10:46:16.100080: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-20 10:46:16.154057: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-11-20 10:46:23.889652: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 21903 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-11-20 10:46:23.891964: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-11-20 10:46:24.756574: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-11-20T02:47:58Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80316"},{"repository":"tensorflow\/tensorflow","title":"model.fit fails when the number of rows exceeds Int32.MaxValue","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.19.0-dev20241117\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nMacOS 15.1.0\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI would expect model.fit to handle training on extremely large NumPy arrays without limitations.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\r\nfrom keras import Sequential\r\nfrom keras.layers import Dense\r\n\r\nn = 2_147_483_648\r\nx = np.zeros(n).astype(np.float32)\r\ny = x\r\n\r\nmodel = Sequential([\r\n    Dense(64, activation=\"relu\", input_shape=(1,)),\r\n    Dense(1, activation=\"sigmoid\")\r\n])\r\nmodel.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\r\nmodel.fit(x=x,y=y, epochs=1, batch_size=1024, verbose=1)\n```\n\n\n### Relevant log output\n\n```shell\nValueError: Invalid value in tensor used for shape: -2147483648\n```\n","labels":["type:bug","TF 2.18"],"created_at":"2024-11-19T09:24:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80241"},{"repository":"tensorflow\/tensorflow","title":"tf.range still miss some dtypes support","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\nv2.18.0-rc2-4-g6550e4bd802 2.18.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nGoogle Colab\n\n### Mobile device\n\nNo\n\n### Python version\n\nGoogle Colab default\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSame issue as in https:\/\/github.com\/tensorflow\/tensorflow\/issues\/72365 but now with unsigned dtypes\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ntf.range(10, delta=1, dtype='uint8')\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-1-7b6ccd0e0a16> in <cell line: 3>()\r\n      1 import tensorflow as tf\r\n      2 \r\n----> 3 tf.range(10, delta=1, dtype='uint8')\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/framework\/ops.py in raise_from_not_ok_status(e, name)\r\n   6000 def raise_from_not_ok_status(e, name) -> NoReturn:\r\n   6001   e.message += (\" name: \" + str(name if name is not None else \"\"))\r\n-> 6002   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n   6003 \r\n   6004 \r\n\r\nInvalidArgumentError: Value for attr 'Tidx' of uint8 is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64, uint16, uint32\r\n\t; NodeDef: {{node Range}}; Op<name=Range; signature=start:Tidx, limit:Tidx, delta:Tidx -> output:Tidx; attr=Tidx:type,default=DT_INT32,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT16, DT_UINT32]> [Op:Range] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-11-14T17:40:39Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/80039"},{"repository":"tensorflow\/tensorflow","title":"tf.cast to int8 produce wrong number","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.15 - 2.17\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\ninput is a long list include random numbers.\r\ntf.cast to int8 output is  different from numpy cast.\r\n\r\nThe strange thing is if you truncate the long input list to a short list then things works fine.\r\n\r\nThe code is straight forward, you can reproduce it in the colab\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/18dYjvY6JQk79hVq8JsjcWEwIG5KBuF1v?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nNumPy Casted values (int8): \r\n [   0    0    0    0    1   -1    0    3   -3    2   -2    0   50   21\r\n -125   62   25   31  127   -9  117   61  123   47  -20   28   52   36\r\n  -43  -45  -84  118   37  -17   -6  -79    1   75  -45  -60 -103  -63\r\n   85 -112   76   96  -56   86  -32 -108 -105 -121   -2  121   86  -54\r\n   91  -55   36 -119   -3  -36   95  127 -105  -60   37   -9 -106    7\r\n  -31  105   13 -103 -123   79   17  -48 -108  -56  -87 -128   35  -94\r\n  -45  118  -91   86  -63  -43   77    1 -127  -16  -71  -73  -76  -15\r\n  -11]\r\nTensorFlow Casted values (int8): \r\n [   0 -128    0    0    1   -1    0    3   -3    2   -2    0  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  127  127\r\n  127  127  127  127  127  127  127  127  127  127  127  127  -76  -15\r\n  -11]\r\n```\n```\n","labels":["type:bug","comp:apis","2.17"],"created_at":"2024-11-08T11:33:35Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79689"},{"repository":"tensorflow\/tensorflow","title":"Unable to register CUDA plug-ins runnung docker image latest-gpu-jypyter","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.18.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\ndocker desktop 4.35.1 , ubuntu 24.04.1 LTS, WSL 2.3.24.0\n\n### Mobile device\n\nNone\n\n### Python version\n\nPython 3.11.0rc1 (provided by tensorflow docker image)\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.7\n\n### GPU model and memory\n\nNVIDIA GeForce RTX 4060 Ti 16G\n\n### Current behavior?\n\nI Strictly followed the instructions provided in:\r\nhttps:\/\/www.tensorflow.org\/install\/docker \r\nhttps:\/\/docs.nvidia.com\/datacenter\/cloud-native\/container-toolkit\/latest\/install-guide.html\r\nGot correct results running a sample workload (as suggested in nvidia contaner toolkit installation manual)\r\n\r\n![image](https:\/\/github.com\/user-attachments\/assets\/6f8c74e6-e23d-4395-8ffc-314f69af5bc7)\r\n\r\nDownloaded tensorflow\/tensorflow latest-gpu-jupyter image and ran the container.\r\nOpend a new jupyter notebook (http:\/\/127.0.0.1:8888\/tree?token=...)\r\nImporting tensorflow I wanted to check the GPU support.\r\nGot error messages and empy available gpu list.\r\n`2024-11-06 10:31:50.143673: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-06 10:31:50.712357: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`\r\n\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ntf.__version__\r\n\r\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n```\n\n\n### Relevant log output\n\n```shell\n2024-11-06 10:31:50.143673: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nE0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\nE0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-11-06 10:31:50.712357: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n'2.18.0'\r\n\r\nNum GPUs Available:  0\r\n2024-11-06 10:31:54.748888: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (34)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:gpu","TF 2.18"],"created_at":"2024-11-06T11:15:27Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79520"},{"repository":"tensorflow\/tensorflow","title":"`tf.math.floormod` not throwing `Integer division by zero` error on GPU for tensor of int64 dtype","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.3\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nRunning `tf.math.floormod` with a tensor containing zeroes in the denominator tensor on GPU does not throw a division by zero error when the tensor has a dtype of `int64`.\r\n\r\n[colab](https:\/\/colab.research.google.com\/drive\/1e053_hcmu0sHQcMPop-_WZR6ya1bw6dy?usp=sharing)\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nA = tf.constant([[2,2],[2,2]], dtype=tf.int64)\r\nB = tf.constant([[0,0],[0,0]], dtype=tf.int64)\r\n\r\nwith tf.device(\"\/gpu:0\"):\r\n    output_gpu = tf.math.floormod(A, B) # No error\r\n    print(f\"\\nGPU: {output_gpu}\\n\") # GPU: [[2 2] [2 2]]\r\n\r\nwith tf.device(\"\/cpu:0\"):\r\n    output_cpu = tf.math.floormod(A, B) \r\n    # InvalidArgumentError: Integer division by zero\n```\n\n\n### Relevant log output\n\n```shell\nGPU: [[2 2]\r\n [2 2]]\r\n\r\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-2-ba17e88e7095> in <cell line: 9>()\r\n      8 \r\n      9 with tf.device(\"\/cpu:0\"):\r\n---> 10     output_cpu = tf.math.floormod(A, B)\r\n     11     # InvalidArgumentError: Integer division by zero\r\n\r\n2 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/framework\/ops.py in raise_from_not_ok_status(e, name)\r\n   5981 def raise_from_not_ok_status(e, name) -> NoReturn:\r\n   5982   e.message += (\" name: \" + str(name if name is not None else \"\"))\r\n-> 5983   raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n   5984 \r\n   5985 \r\n\r\nInvalidArgumentError: {{function_node __wrapped__FloorMod_device_\/job:localhost\/replica:0\/task:0\/device:CPU:0}} Integer division by zero [Op:FloorMod] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-11-01T00:42:37Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79162"},{"repository":"tensorflow\/tensorflow","title":"`tf.linalg.lstsq` producing outputs with large inconsistencies between CPU and GPU with `float32` tensors","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n12.3\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nPassing tensors of `float32` to 'tf.linalg.lstsq' is producing very different output from CPU and GPU.\r\n\r\n### Standalone code to reproduce the issue\r\n[colab](https:\/\/colab.research.google.com\/drive\/1Fyh3HQs39NhGlOLEdzpqPKVkznBe6Fe9?usp=sharing)\r\n```shell\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nA = tf.constant([[[[0.37454012, 0.9507143, 0.7319939, 0.5986585, 0.15601864], [0.15599452, 0.05808361, 0.8661761, 0.601115, 0.7080726,], [0.02058449, 0.96990985, 0.83244264,\r\n                0.21233912, 0.18182497], [0.1834045, 0.30424225, 0.52475643, 0.43194503, 0.29122913], [0.6118529, 0.13949387, 0.29214466, 0.36636186, 0.45606998]]]], dtype=tf.float32)\r\n\r\nB = tf.constant([[[[0.59241456, 0.04645041], [0.94888556, 0.965632,], [0.684233, 0.4401525,], [\r\n                0.9093204, 0.25877997], [0.54671025, 0.18485446]]]], dtype=tf.float32)\r\n\r\nwith tf.device(\"\/gpu:0\"):\r\n    output_gpu = tf.linalg.lstsq(A, B)\r\n    \r\nwith tf.device( \"\/cpu:0\"):\r\n    output_cpu = tf.linalg.lstsq(A, B)\r\n\r\nnp.testing.assert_allclose(output_cpu, output_gpu.cpu(), atol=100) # AssertionError\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=100\r\n\r\nMismatched elements: 2 \/ 10 (20%)\r\nMax absolute difference: 112.95715\r\nMax relative difference: 0.2963447\r\n x: array([[[[-170.69518 ,   28.379017],\r\n         [ 164.85085 ,  -28.04222 ],\r\n         [-273.4264  ,   46.952198],...\r\n y: array([[[[-241.07059 ,   40.25023 ],\r\n         [ 232.80626 ,  -39.505226],\r\n         [-386.38354 ,   66.00629 ],...\r\n```\r\n","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2024-10-31T22:52:23Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79157"},{"repository":"tensorflow\/tensorflow","title":"Thread ID in TensorBoard Profiler Trace Viewer Could Be Negative","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Mint 21.2\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n6.5.0\n\n### GCC\/compiler version\n\nclang 14.0.0\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhile using TensorFlow's profiler, I found that some thread IDs displayed in the Trace Viewer are negative. As shown in the image below, thread names such as `tf_Compute\/-1030865605`, `tf_data_private_threadpool\/-102013...`, etc., have negative thread IDs.\r\n\r\n![image](https:\/\/github.com\/user-attachments\/assets\/71d32241-d617-4e52-bcbf-d3140ebef440)\r\n\r\nMy log file is also attached.\r\n\r\n[20241030-161104.zip](https:\/\/github.com\/user-attachments\/files\/17587809\/20241030-161104.zip)\n\n### Standalone code to reproduce the issue\n\n```shell\n(Just the profiler demo code) https:\/\/www.tensorflow.org\/tensorboard\/tensorboard_profiling_keras\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","awaiting PR merge","2.17"],"created_at":"2024-10-31T13:36:36Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79128"},{"repository":"tensorflow\/tensorflow","title":"Significant Discrepancy in `tf.linalg.triangular_solve` Results Between CPU and GPU","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\nv2.18.0-rc2-4-g6550e4bd802 2.18.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 22.04.4 LTS x86_64\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.0\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nWhen using `tf.linalg.triangular_solve` with large matrices or specific triangular matrix conditions (e.g., `upper=True`, `transpose=True`, `unitriangular=True`), the GPU results significantly differ from the CPU results. \r\n\r\nThe discrepancy includes extremely large Mean Absolute Error (MAE) values and infinite Mean Squared Error (MSE) values of the results, indicating a possible issue in the GPU implementation of the function.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n[colab](https:\/\/colab.research.google.com\/drive\/1zXcOAkxHV6snaMyWGMeKIukf5IVBd0Bh?usp=sharing)\r\n[Safe Tensors](https:\/\/drive.google.com\/file\/d\/1n1JyugXNAS9M2wmnk9u9vCVuHz0uE3R6\/view?usp=sharing)\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nfrom safetensors.torch import load_file\r\n\r\ndef set_seed(seed_value=42):\r\n    \"\"\"Sets the random seed for reproducibility.\"\"\"\r\n    np.random.seed(seed_value)\r\n    tf.random.set_seed(seed_value)\r\n\r\ndef tensorflow_version(input, cpu=True):\r\n    set_seed()\r\n    if cpu:\r\n        device_string = \"\/cpu:0\"\r\n    else:\r\n        device_string = \"\/gpu:0\"\r\n\r\n    with tf.device(device_string):\r\n        b_tensor = tf.constant(input[\"b\"])\r\n        A_tensor = tf.constant(input[\"A\"])\r\n\r\n        upper = input.get(\"upper\", True)\r\n        transpose = input.get(\"transpose\", False)\r\n        unitriangular = input.get(\"unitriangular\", False)\r\n\r\n        solution = tf.linalg.triangular_solve(\r\n            A_tensor, b_tensor, lower=not upper, adjoint=transpose\r\n        )\r\n\r\n        return {\"triangular_solve_solution\": solution.numpy()}\r\n\r\ndef load_safe_tensor(file_path):\r\n    safe_tensor_data = load_file(file_path)\r\n    A_tensor = safe_tensor_data[\"A\"]\r\n    b_tensor = safe_tensor_data[\"b\"]\r\n\r\n    return {\r\n        \"A\": tf.convert_to_tensor(A_tensor.numpy()),\r\n        \"b\": tf.convert_to_tensor(b_tensor.numpy()),\r\n    }\r\n\r\ndef calculate_differences(cpu_result, gpu_result):\r\n    diff = np.abs(cpu_result - gpu_result)\r\n    mae = np.mean(diff)\r\n    mse = np.mean(diff**2)\r\n    rmse = np.sqrt(mse)\r\n    max_diff = np.max(diff)\r\n    mean_relative_diff = np.mean(diff \/ (np.abs(cpu_result) + 1e-10))\r\n\r\n    return {\r\n        \"Mean Absolute Error\": mae,\r\n        \"Mean Squared Error\": mse,\r\n        \"Root Mean Squared Error\": rmse,\r\n        \"Maximum Absolute Difference\": max_diff,\r\n        \"Mean Relative Difference\": mean_relative_diff,\r\n    }\r\n\r\ndef main():\r\n    file_path = \"tensorflow_triangular_solve_3.safetensors\"\r\n    input_data = load_safe_tensor(file_path)\r\n    input_data[\"upper\"] = True\r\n    input_data[\"transpose\"] = True\r\n    input_data[\"unitriangular\"] = True\r\n\r\n    result_cpu = tensorflow_version(input_data, cpu=True)\r\n    print(\"CPU Result:\")\r\n    print(result_cpu)\r\n\r\n    if tf.config.list_physical_devices(\"GPU\"):\r\n        result_gpu = tensorflow_version(input_data, cpu=False)\r\n        print(\"GPU Result:\")\r\n        print(result_gpu)\r\n\r\n        if result_gpu:\r\n            cpu_solution = result_cpu[\"triangular_solve_solution\"]\r\n            gpu_solution = result_gpu[\"triangular_solve_solution\"]\r\n            differences = calculate_differences(cpu_solution, gpu_solution)\r\n            for key, value in differences.items():\r\n                print(f\"{key}: {value}\")\r\n    else:\r\n        print(\"GPU not available.\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n#### **Issue Summary:**\r\n- The `tf.linalg.triangular_solve` function produces vastly different results on the GPU compared to the CPU. The differences include an enormous Mean Absolute Error (MAE) and an infinite Mean Squared Error (MSE), indicating a severe discrepancy between the CPU and GPU results.\r\n- The issue appears to be related to the use of specific arguments like `upper=True`, `transpose=True`, and `unitriangular=True`, suggesting that there might be a numerical precision or stability issue in the GPU implementation.\r\n- It is unclear why these large discrepancies occur, but they point to a critical inconsistency that could impact numerical reliability for users depending on TensorFlow\u2019s triangular solve functionality.\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nMean Absolute Error: 1.566790629150662e+21\r\nMean Squared Error: inf\r\nRoot Mean Squared Error: inf\r\nMaximum Absolute Difference: 1.4265324671452624e+26\r\n```\r\n","labels":["type:bug","comp:ops","TF 2.18"],"created_at":"2024-10-31T04:34:40Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/79090"},{"repository":"tensorflow\/tensorflow","title":"`lookup_ops.StaticVocabularyTable` and `lookup_ops.StaticVocabularyTableV1`  can cause a crash","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf-nightly  2.19.0-dev20241025\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04.3 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI encountered an `segmentation fault issue` in TensorFlow when I used API `lookup_ops.StaticVocabularyTable` or `lookup_ops.StaticVocabularyTableV1` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).\r\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1NGnFQqFl6Jy_Xt7wBL3ynmSVs9AiFw_l?usp=sharing) to reproduce the issue.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport os\r\nfrom tensorflow.python.ops import lookup_ops\r\n\r\ndef _createVocabFile(basename, values=('brain', 'salad', 'surgery')):\r\n    vocabulary_file = os.path.join(\"\/tmp\", basename)\r\n    with open(vocabulary_file, 'w') as f:\r\n        f.write('\\n'.join(values) + '\\n')\r\n    return vocabulary_file\r\n\r\nvocab_file = _createVocabFile('feat_to_id_1.txt')\r\nvocab_size = 9223372036854775807\r\noov_buckets = 1\r\ntable = lookup_ops.StaticVocabularyTable(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)\r\n#lookup_ops.StaticVocabularyTableV1(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nSegmentation fault (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-10-26T07:19:51Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78831"},{"repository":"tensorflow\/tensorflow","title":"`gen_list_ops.tensor_list_concat_v2` aborts with \"Check failed: size >= 0 (0 vs. -1)\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly  2.19.0-dev20241025\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\nLinux Ubuntu 20.04.3 LTS\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)\r\n\r\nPlease find the [[gist](https:\/\/colab.research.google.com\/drive\/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing)](https:\/\/colab.research.google.com\/drive\/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import gen_list_ops\r\nfrom tensorflow.python.ops import list_ops\r\n\r\nl = list_ops.tensor_list_reserve(element_dtype=dtypes.float32, element_shape=None, num_elements=3)\r\nt = gen_list_ops.tensor_list_concat_v2(l, element_dtype=dtypes.float32, element_shape=list_ops._build_element_shape((None, 3)), leading_dims=[-1, 3, 5])\n```\n\n\n### Relevant log output\n\n```shell\nF tensorflow\/core\/framework\/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-10-26T07:15:16Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78830"},{"repository":"tensorflow\/tensorflow","title":"`data_flow_ops.Barrier` aborts with \"Check failed: i >= 0 (0 vs. -100)\" ","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly  2.19.0-dev20241025\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)\r\n\r\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1OOOIvZ7brRDjRqshv36CA_55BlPbgI4e?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import data_flow_ops\r\nfrom tensorflow.python.eager import context\r\nimport tensorflow as tf\r\n\r\nwith context.graph_mode():\r\n    sess = tf.compat.v1.Session()\r\n    with sess.as_default():\r\n        b = data_flow_ops.Barrier((dtypes.float32, dtypes.float32), shapes=((), ()), name='B')\r\n        keys = [b'a', b'b', b'c', b'd']\r\n        values_0 = [10.0, 20.0, 30.0, 40.0]\r\n        values_1 = [100.0, 200.0, 300.0, 400.0]\r\n        insert_1_1_op = b.insert_many(-100, keys[0:2], values_1[0:2]) \r\n        insert_1_1_op.run()\n```\n\n\n### Relevant log output\n\n```shell\nF tensorflow\/core\/kernels\/barrier_ops.cc:286] Check failed: i >= 0 (0 vs. -100)\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","TF 2.18"],"created_at":"2024-10-26T07:04:16Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78828"},{"repository":"tensorflow\/tensorflow","title":"Does TFLite dequantize opertor support constant buffer input","description":"**System information**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04\r\n- TensorFlow installed from (source or binary): source \r\n- TensorFlow version (or github SHA if from source): the latest version\r\n\r\n**Standalone code to reproduce the issue** \r\n<img width=\"724\" alt=\"image\" src=\"https:\/\/github.com\/user-attachments\/assets\/849a0952-90ce-42c4-b2dc-9a9e7333fb0b\">\r\nThe [tflite model for user input](https:\/\/github.com\/fujunwei\/tensorflow\/blob\/tflite_label_image\/tensorflow\/lite\/examples\/python\/dequantize_input.tflite) can work as expected\r\n\r\n<img width=\"713\" alt=\"image\" src=\"https:\/\/github.com\/user-attachments\/assets\/43cafc84-0ea6-4a06-84b4-4772c6cea8e8\">\r\n\r\nThe [tflite model with constant buffer](https:\/\/github.com\/fujunwei\/tensorflow\/blob\/tflite_label_image\/tensorflow\/lite\/examples\/python\/dequantize_constant.tflite) can't compute, so does [dequantize](https:\/\/www.tensorflow.org\/mlir\/tfl_ops#tfldequantize_tfldequantizeop) support constant input?\r\n\r\n**Any other info \/ logs**\r\n\r\nYou can also modify [dequantize_tester in the Repo](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/lite\/delegates\/xnnpack\/dequantize_tester.cc#L110) to reproduce this issue like below code:\r\n\r\n1, Create a constant buffer\r\n```\r\n  const auto buffer_data = builder.CreateVector( reinterpret_cast<const uint8_t*>(constant_buffer.data()),\r\n                                                                              constant_buffer.size());\r\n  const auto buffer_index = base::checked_cast<uint32_t>(buffers.size());\r\n  buffers.emplace_back(::tflite::CreateBuffer(builder, buffer_data));\r\n```\r\n\r\n2, Use the `buffer_index ` when creating tensor:\r\n```\r\nconst std::array<flatbuffers::Offset<::tflite::Tensor>, 2> tensors{{\r\n      ::tflite::CreateTensor(\r\n          builder,\r\n          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),\r\n          Unsigned() ? ::tflite::TensorType_UINT8 : ::tflite::TensorType_INT8,\r\n          \/*buffer=*\/buffer_index , \/*name=*\/0,\r\n          ::tflite::CreateQuantizationParameters(\r\n              builder, \/*min=*\/0, \/*max=*\/0,\r\n              builder.CreateVector<float>({InputScale()}),\r\n              builder.CreateVector<int64_t>({InputZeroPoint()}))),\r\n      ::tflite::CreateTensor(\r\n          builder,\r\n          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),\r\n          ::tflite::TensorType_FLOAT32,\r\n          \/*buffer=*\/0, \/*name=*\/builder.CreateString(\"dequantizeLinearOutput\")),\r\n  }};\r\n\r\n```","labels":["stat:awaiting tensorflower","type:bug","comp:lite","ModelOptimizationToolkit","2.17"],"created_at":"2024-10-25T08:28:42Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78748"},{"repository":"tensorflow\/tensorflow","title":"No kernels registered for op `Conv2DBackpropInputV2`","description":"### Issue type\n\nDocumentation Bug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf 2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nThe existence of operator `Conv2DBackpropInputV2` is described in the official website document.https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/raw_ops\/Conv2DBackpropInputV2.\r\nHowever, during my actual execution, the following error message appears:\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput_sizes = tf.constant(1, shape=[4], dtype=tf.int32)\r\nfilter_tensor = tf.constant(2, shape=[4], dtype=tf.float32)\r\nout_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)\r\n\r\nstrides = [1, 1, 1, 1]\r\npadding = \"SAME\"\r\n\r\ntf.raw_ops.Conv2DBackpropInputV2(\r\n    input=input_sizes,\r\n    filter=filter_tensor,\r\n    out_backprop=out_backprop,\r\n    strides=strides,\r\n    padding=padding\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-10-21 12:30:18.492624: W tensorflow\/compiler\/tf2tensorrt\/utils\/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\r\nTraceback (most recent call last):\r\n  File \"\/mnt\/tests\/Conv2DBackpropInput.py\", line 10, in <module>\r\n    tf.raw_ops.Conv2DBackpropInputV2(\r\n  File \"\/mnt\/origin\/venv\/tensorflow-nightly\/lib\/python3.11\/site-packages\/tensorflow\/python\/util\/tf_export.py\", line 377, in wrapper\r\n    return f(**kwargs)\r\n           ^^^^^^^^^^^\r\n  File \"\/mnt\/origin\/venv\/tensorflow-nightly\/lib\/python3.11\/site-packages\/tensorflow\/python\/ops\/gen_nn_ops.py\", line 2030, in conv2d_backprop_input_v2\r\n    _ops.raise_from_not_ok_status(e, name)\r\n  File \"\/mnt\/origin\/venv\/tensorflow-nightly\/lib\/python3.11\/site-packages\/tensorflow\/python\/framework\/ops.py\", line 5983, in raise_from_not_ok_status\r\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Conv2DBackpropInputV2}} = Conv2DBackpropInputV2[T=DT_INT32, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true]\r\nAll kernels registered for op Conv2DBackpropInputV2:\r\n  <no registered kernels>\r\n [Op:Conv2DBackpropInputV2] name:\n```\n","labels":["type:docs-bug","stat:awaiting tensorflower","type:bug","comp:ops","TF 2.16"],"created_at":"2024-10-21T12:34:35Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78431"},{"repository":"tensorflow\/tensorflow","title":"Overflow and Check fail in `tf.raw_ops.Conv2DBackpropInput`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOverflow : `input_sizes` is a one-dimensional tensor and contains maximum values\r\nCheck fail\uff1a`input_sizes`'s shape is [2] and The dimension of `filter` is less than 2\n\n### Standalone code to reproduce the issue\n\n```shell\nOverflow:\r\n\r\nimport tensorflow as tf\r\n\r\ninput_sizes = tf.constant([1, 5, 5, 999999999999], shape=[4], dtype=tf.int32)\r\nfilter_tensor = tf.constant(3, shape=[3, 3, 3, 2], dtype=tf.float32)\r\nout_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)\r\n\r\nstrides = [1, 1, 1, 1]\r\npadding = \"SAME\"\r\n\r\ntf.raw_ops.Conv2DBackpropInput(\r\n    input_sizes=input_sizes,\r\n    filter=filter_tensor,\r\n    out_backprop=out_backprop,\r\n    strides=strides,\r\n    padding=padding\r\n)\r\n\r\nCheck fail:\r\n```python\r\nimport tensorflow as tf\r\n\r\ninput_sizes = tf.constant(1, shape=[2], dtype=tf.int32)\r\nfilter_tensor = tf.constant(2, shape=[1], dtype=tf.float32)\r\nout_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)\r\n\r\nstrides = [1, 1, 1, 1]\r\npadding = \"SAME\"\r\n\r\ntf.raw_ops.Conv2DBackpropInput(\r\n    input_sizes=input_sizes,\r\n    filter=filter_tensor,\r\n    out_backprop=out_backprop,\r\n    strides=strides,\r\n    padding=padding\r\n)\r\n```\n```\n\n\n### Relevant log output\n\n```shell\nOverflow:\r\n\r\n2024-10-21 11:40:49.417611: F tensorflow\/core\/kernels\/mkl\/mkl_conv_grad_input_ops.cc:578] Non-OK-status: tensor::MakeShape(input_tensor, &input_tf_shape)\r\nStatus: INVALID_ARGUMENT: Dimension -727379969 must be >= 0\r\nAborted (core dumped)\r\n```\r\n\r\nCheck fail:\r\n```\r\n2024-10-21 11:16:29.937265: F tensorflow\/core\/framework\/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 1)\r\nAborted (core dumped)\r\n```\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-21T12:17:20Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78428"},{"repository":"tensorflow\/tensorflow","title":"Overflow in `tf.raw_ops.Fill`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nOverflow in `tf.raw_ops.Fill` when there are too large values in `dims`.\n\n### Standalone code to reproduce the issue\n\n```shell\nhttps:\/\/colab.research.google.com\/drive\/1GDBN4lheNUIW704hsXVt3lW55UJue97S?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\nKill\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-21T11:10:34Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78427"},{"repository":"tensorflow\/tensorflow","title":"Multi-threaded execution throws an exception (using GPU).","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.19.0-dev20241018\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 24.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nMulti-threaded execution throws an exception (using GPU).\n\n### Standalone code to reproduce the issue\n\n```shell\nimport concurrent\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nprint(tf.__version__)\r\n\r\nexecutor = concurrent.futures.ThreadPoolExecutor()\r\n\r\n\r\ndef sum(x, axis):\r\n    return tf.reduce_sum(x, axis=axis)\r\n\r\n\r\nfutures = []\r\n\r\nfor i in range(1000):\r\n    futures.clear()\r\n    for _ in range(4):\r\n        x = tf.convert_to_tensor(np.random.rand(100, 100))\r\n        futures.append(executor.submit(sum, x, 1))\r\n        x = tf.convert_to_tensor(np.random.rand(100))\r\n        futures.append(executor.submit(sum, x, 0))\r\n    concurrent.futures.wait(\r\n        futures, return_when=concurrent.futures.ALL_COMPLETED\r\n    )\r\n    [future.result() for future in futures]\n```\n\n\n### Relevant log output\n\n```shell\nW tensorflow\/core\/framework\/op_kernel.cc:1840] OP_REQUIRES failed at reduction_ops_common.h:147 : INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)\r\nI tensorflow\/core\/framework\/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)\r\n\r\n```\n```\n","labels":["type:bug","comp:gpu"],"created_at":"2024-10-19T13:02:32Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78338"},{"repository":"tensorflow\/tensorflow","title":"[Incorrect Result] `tf.math.reciprocal` returns `NaN` on `inf` input on Linux.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17.0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nAlmaLinux 9.4\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n`tf.math.reciprocal` returns `NaN` on Linux when input is `inf` or `-inf` and has dtype=complex128, shape >= 2.\r\nThe output is expected to be 0, since:\r\n1. This behavior is not consistent with dtype=float64, where the output will be 0.\r\n2. When input tensor contains only one value, the output will be 0.\r\n3. The same code snippet will return different result on macOS, where the output is also 0.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ninput = tf.constant(np.inf, dtype=tf.float64)\r\nout = tf.math.reciprocal(input)\r\n# tf.Tensor(0.0, shape=(), dtype=float64)\r\nprint(out)\r\n\r\ninput = tf.constant(np.inf, dtype=tf.complex128)\r\nout = tf.math.reciprocal(input)\r\n# tf.Tensor(0j, shape=(), dtype=complex128)\r\nprint(out)\r\n\r\ninput = tf.constant([np.inf, np.inf], dtype=tf.complex128)\r\nout = tf.math.reciprocal(input)\r\n# On Linux: tf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)\r\n# On macOS: tf.Tensor([0.+0.j 0.+0.j], shape=(2,), dtype=complex128)\r\nprint(out)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nAttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'\r\ntf.Tensor(0.0, shape=(), dtype=float64)\r\ntf.Tensor(0j, shape=(), dtype=complex128)\r\ntf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-19T09:41:36Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78298"},{"repository":"tensorflow\/tensorflow","title":"tf.linalg.expm fails to support half\/float16 data type, which is inconsistent with doc","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nAccording to the documentation: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/linalg\/expm\r\ntf.linalg.expm is expected to accept float16 input, but it fails on float16 when actually running the following code.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\ninput = tf.constant(np.random.randn(1,1), dtype='float16')\r\nout = tf.linalg.expm(input)\r\n```\n```\n\n\n### Relevant log output\n\n```shell\ntensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node MatrixSolve}} = MatrixSolve[T=DT_HALF, adjoint=false]\r\nAll kernels registered for op MatrixSolve:\r\n  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\r\n  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\r\n  device='GPU'; T in [DT_COMPLEX128]\r\n  device='GPU'; T in [DT_COMPLEX64]\r\n  device='GPU'; T in [DT_DOUBLE]\r\n  device='GPU'; T in [DT_FLOAT]\r\n  device='CPU'; T in [DT_COMPLEX128]\r\n  device='CPU'; T in [DT_COMPLEX64]\r\n  device='CPU'; T in [DT_DOUBLE]\r\n  device='CPU'; T in [DT_FLOAT]\r\n [Op:MatrixSolve] name:\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-17T13:09:08Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78107"},{"repository":"tensorflow\/tensorflow","title":"DLPack with Int32 tensor on the GPU: inconsistent eager mode \/ graph mode \/ XLA","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv1.12.1-117097-gecf05620570 2.19.0-dev20241016\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nHello,\r\n\r\nI realize that `int32` is a special dtype in TensorFlow for historical reasons. It seems that the handling of GPU int32-typed tensors has evolved over time.\r\n\r\nCurrently, the `device` field of a tensor created with:\r\n```py\r\nwith tf.device('gpu'):\r\n    x = tf.constant([0,1,2], tf.int32)\r\n```\r\n*does* indicate it's a GPU tensor: `\/job:localhost\/replica:0\/task:0\/device:GPU:0`.\r\n\r\nHowever, when exporting and re-importing it via DLPack, it comes back as a CPU tensor.\r\nThere even seems to be a unit test validating this:\r\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/d3de971a7348ecaefdbb920e580c37ebde10d780\/tensorflow\/python\/dlpack\/dlpack_test.py#L75-L78\r\n\r\n\r\nHowever, @jhoydis found that this is *not* consistent between modes. In particular, if the tensor goes through an XLA-compiled function, it will correctly live on the GPU even after a round-trip through DLPack. (See reproducer below).\r\n\r\nWould it please be possible to revisit this behavior, so that **exporting an int32 GPU tensor via DLPack does result in a GPU DLPack capsule in all modes, not just XLA?**\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n\r\ndef f_eager(x):\r\n    return x\r\nf_graph = tf.function()(f_eager)\r\nf_xla = tf.function(jit_compile=True)(f_eager)\r\n\r\n\r\nwith tf.device('gpu'):\r\n    x = tf.constant([0,1,2], tf.int32)\r\n    print(\"Original tensor:\", x.device)\r\n\r\n    dlcapsule = tf.experimental.dlpack.to_dlpack(x)\r\n    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)\r\n    print(\"Default:\", x_.device)\r\n\r\n    dlcapsule = tf.experimental.dlpack.to_dlpack(f_eager(x))\r\n    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)\r\n    print(\"Eager:\", x_.device)\r\n\r\n    dlcapsule = tf.experimental.dlpack.to_dlpack(f_graph(x))\r\n    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)\r\n    print(\"Graph:\", x_.device)\r\n\r\n    dlcapsule = tf.experimental.dlpack.to_dlpack(f_xla(x))\r\n    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)\r\n    print(\"XLA:\", x_.device)\n```\n\n\n### Relevant log output\n\n```shell\nOriginal tensor: \/job:localhost\/replica:0\/task:0\/device:GPU:0\r\nDefault: \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nEager: \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nGraph: \/job:localhost\/replica:0\/task:0\/device:CPU:0\r\nXLA: \/job:localhost\/replica:0\/task:0\/device:GPU:0\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","comp:gpu"],"created_at":"2024-10-17T08:59:51Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/78091"},{"repository":"tensorflow\/tensorflow","title":"tf.math.special.bessel_* has inconsistent result with scipy","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nBased on the documentation, special function such as bessel_y0 should have consistent result with scipy. However, when receiving `-inf`, it has inconsistent results with scipy. Please check the reproducible for details.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport scipy\r\nimport numpy as np\r\nimport tensorflow as tf\r\nx = tf.constant(-np.inf, dtype='float64')\r\nprint(\"TF:\", tf.math.special.bessel_y1(x))\r\nprint(\"Scipy: \", scipy.special.y1(x))\r\nprint(\"TF:\", tf.math.special.bessel_y0(x))\r\nprint(\"Scipy: \", scipy.special.y0(x))\r\nprint(\"TF:\", tf.math.special.bessel_k0(x))\r\nprint(\"Scipy: \", scipy.special.k0(x))\r\nprint(\"TF:\", tf.math.special.bessel_k1(x))\r\nprint(\"Scipy: \", scipy.special.k1(x))\n```\n\n\n### Relevant log output\n\n```shell\nTF: tf.Tensor(-inf, shape=(), dtype=float64)\r\nScipy:  nan\r\nTF: tf.Tensor(-inf, shape=(), dtype=float64)\r\nScipy:  nan\r\nTF: tf.Tensor(inf, shape=(), dtype=float64)\r\nScipy:  nan\r\nTF: tf.Tensor(inf, shape=(), dtype=float64)\r\nScipy:  nan\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-14T15:45:19Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77864"},{"repository":"tensorflow\/tensorflow","title":"tf.math.is_strictly_increasing's behavior is not clear on a (2,2) matrix","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen receiving this input:\r\n```\r\nx = tf.constant([[1,2],[2,3]])\r\n```\r\n`tf.math.is_strictly_increasing` outputs `False` instead of `True`.\r\nAlso, for a tensor with shape (1,3,3):\r\n```\r\nx = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],\r\n  [-0.5704009,  -0.2167283,   0.2548743 ],\r\n  [-0.14944994,  2.0107825,  -0.09678416]]])\r\n```\r\nIt's output is still `False` instead of `True` even when x's first dimension only has one element.\r\nBased on the description \"Elements of x are compared in row-major order.\", it seems that elements in x are compared along row (i.e., the first dimension).\r\nTherefore, to my understanding, if the first dimension contains only one element (such as 1x3x3 shape tensor), the output should be True. If the input is [[1,2],[3,4]], the output should also be `True` since the value in the first dimension is increasing (from `[1,2]` to `[3,4]`)\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nx = tf.constant([[1,2],[2,3]])\r\nprint(tf.math.is_strictly_increasing(x))  # False\r\nx = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],\r\n  [-0.5704009,  -0.2167283,   0.2548743 ],\r\n  [-0.14944994,  2.0107825,  -0.09678416]]])\r\nprint(tf.math.is_strictly_increasing(x))  # False\r\n```\n```\n\n\n### Relevant log output\n\n_No response_","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-14T15:28:21Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77863"},{"repository":"tensorflow\/tensorflow","title":"argmax returns incorrect result for input containing Minimum number (TensorFlow 2.x)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nCurrent Behavior:\r\nWhen using tf.math.argmax on an input array that contains -0.0, the result is incorrect. Specifically, the function returns 1 (the index of -0.0) as the position of the maximum value, while the actual maximum value is 1.401298464324817e-45 at index 2.\r\n\r\nThe same behavior is observed in Keras and JAX, as both use TensorFlow internally for the argmax function.\r\n\r\nExpected Behavior:\r\ntf.math.argmax should return 2, as the value at index 2 (1.401298464324817e-45) is greater than both -1.0 and -0.0.\r\n```\r\nimport numpy as np\r\nimport torch\r\nimport tensorflow as tf\r\nimport jax.numpy as jnp\r\nfrom tensorflow import keras\r\n\r\ndef test_argmax():\r\n    # Input data\r\n    input_data = np.array([-1.0, -0.0, 1.401298464324817e-45], dtype=np.float32)\r\n\r\n    # PyTorch argmax\r\n    pytorch_result = torch.argmax(torch.tensor(input_data, dtype=torch.float32)).item()\r\n    print(f\"PyTorch argmax result: {pytorch_result}\")\r\n\r\n    # TensorFlow argmax\r\n    tensorflow_result = tf.math.argmax(input_data).numpy()\r\n    print(f\"TensorFlow argmax result: {tensorflow_result}\")\r\n\r\n    # Keras argmax (Keras internally uses TensorFlow, so should be the same)\r\n    keras_result = keras.backend.argmax(input_data).numpy()\r\n    print(f\"Keras argmax result: {keras_result}\")\r\n\r\n    # JAX argmax\r\n    jax_result = jnp.argmax(input_data)\r\n    print(f\"JAX argmax result: {jax_result}\")\r\n\r\nif __name__ == \"__main__\":\r\n    test_argmax()\r\n\r\n```\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\nPyTorch argmax result: 2\r\nTensorFlow argmax result: 1\r\nKeras argmax result: 1\r\nJAX argmax result: 1\r\n\r\n```\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","comp:ops","TF 2.16"],"created_at":"2024-10-14T10:07:59Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77853"},{"repository":"tensorflow\/tensorflow","title":"argsort incorrectly handles very small floating-point numbers and -0.0 compared to other libraries (PyTorch and JAX)","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nwindows\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen using TensorFlow's argsort function on an array containing small floating-point numbers and both 0.0 and -0.0, the sort order is incorrect compared to other deep learning libraries such as PyTorch and JAX. TensorFlow incorrectly places 1.401298464324817e-45 (a very small positive number) before 0.0 and -0.0.\r\n\r\nExpected behavior is that both 0.0 and -0.0 should be treated as equivalent and placed before any positive number, including very small ones like 1.401298464324817e-45. However, TensorFlow does not follow this behavior, whereas PyTorch correctly handles this.\r\n```\r\nimport numpy as np\r\nimport torch\r\nimport tensorflow as tf\r\nimport jax.numpy as jnp\r\n\r\ndef test_argsort():\r\n    # Input data, hardcoded as float32\r\n    input_data = np.array([\r\n        -0.0, 1.401298464324817e-45, 1.100000023841858, -0.0,\r\n        5.960464477539063e-08, -2.0000100135803223, 1000000.0,\r\n        722801.375, 0.0, -1.100000023841858\r\n    ], dtype=np.float32)\r\n\r\n    # PyTorch argsort\r\n    pytorch_result = torch.argsort(torch.tensor(input_data, dtype=torch.float32)).numpy()\r\n    print(f\"PyTorch argsort result: {pytorch_result}\")\r\n\r\n    # TensorFlow argsort\r\n    tensorflow_result = tf.argsort(input_data).numpy().astype(np.int32)\r\n    print(f\"TensorFlow argsort result: {tensorflow_result}\")\r\n\r\n    # JAX argsort\r\n    jax_result = jnp.argsort(input_data).astype(np.int32)\r\n    print(f\"JAX argsort result: {jax_result}\")\r\n\r\nif __name__ == \"__main__\":\r\n    test_argsort()\r\n\r\n```\n\n### Standalone code to reproduce the issue\n\n```shell\nPyTorch argsort result: [5 9 0 3 8 1 4 2 7 6]\r\nTensorFlow argsort result: [5 9 0 1 3 8 4 2 7 6]\r\nJAX argsort result: [5 9 0 1 3 8 4 2 7 6]\r\n```\r\nExpected Behavior:\r\nTensorFlow's argsort should place 0.0 and -0.0 before any positive number, including very small values like 1.401298464324817e-45. PyTorch demonstrates the correct behavior by treating 0.0 and -0.0 as equal and placing them in the correct order relative to other values.\r\n\r\nStandalone Code to Reproduce the Issue:\r\nThe above Python code demonstrates the issue. It uses the same input data for PyTorch, TensorFlow, and JAX to show the difference in behavior. TensorFlow and JAX produce incorrect results by misplacing the small positive value before 0.0, while PyTorch produces the correct order.\r\n\r\nRelevant Log Output:\r\nNo error logs are generated, but the incorrect behavior is clearly shown in the sorting results.\n```\n\n\n### Relevant log output\n\n_No response_","labels":["type:bug","TF 2.16"],"created_at":"2024-10-14T09:02:22Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77849"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT3D`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nIf the value contained in fft_length is the maximum value, it will cause an abort\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput = tf.constant(0, shape=[2,0,0,0] ,dtype=tf.complex64)\r\nfft_length = tf.constant(1879048192, shape=[3], dtype=tf.int32)\r\n\r\ntf.raw_ops.IRFFT3D(input=input, fft_length=fft_length)\n```\n\n\n### Relevant log output\n\n```shell\n2024-10-13 13:04:53.308156: F tensorflow\/core\/framework\/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()\r\nStatus: INVALID_ARGUMENT: Shape [2,1879048192,1879048192,1879048192] results in overflow when computing number of elements\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-13T13:07:08Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77824"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT2D`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf2.17.0 tf2.16.1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nSince the value in fft_length is a maximum value, it will cause abort\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ninput = tf.constant(0, shape=[1,4,10,0,0] ,dtype=tf.complex64)\r\nfft_length = tf.constant(2147483647, shape=[2], dtype=tf.int32)\r\n\r\ntf.raw_ops.IRFFT2D(input=input, fft_length=fft_length)\n```\n\n\n### Relevant log output\n\n```shell\n2024-10-13 12:59:11.295197: F tensorflow\/core\/framework\/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()\r\nStatus: INVALID_ARGUMENT: Shape [1,4,10,2147483647,2147483647] results in overflow when computing number of elements\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-13T13:04:05Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77823"},{"repository":"tensorflow\/tensorflow","title":"Gradients of tf.linalg.expm not supported with JIT compilation","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntested 2.17 and 2.10, both have the issue\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu\n\n### Mobile device\n\n_No response_\n\n### Python version\n\ntested 3.9 and 3.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nGradients of `tf.linalg.expm` can not be computed with JIT compilation. \r\n\r\nThis is an issue, because tf 2.17 seems to have activated jit compilation for compiled models per default whereas earlier versions did not, breaking existing code.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nA = tf.Variable([[.4, 1.5], [.6, .1]], dtype=tf.float32)\r\n\r\n@tf.function(jit_compile=True) #set jit_compile=False to make it work\r\ndef f(A):\r\n    with tf.GradientTape() as tape:\r\n        B = tf.linalg.expm(A)\r\n    return tape.gradient(B, A)\r\n\r\nf(A)\n```\n\n\n### Relevant log output\n\n```shell\n2024-10-11 11:17:27.281304: W tensorflow\/core\/framework\/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: XLA compilation requires a fixed tensor list size. Set the max number of elements. This could also happen if you're using a TensorArray in a while loop that does not have its maximum_iteration set, you can fix this by setting maximum_iteration to a suitable value.\r\n\r\nStack trace for op definition: \r\nFile \"<frozen runpy>\", line 198, in _run_module_as_main\r\nFile \"<frozen runpy>\", line 88, in _run_code\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel_launcher.py\", line 18, in <module>\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/traitlets\/config\/application.py\", line 1075, in launch_instance\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/kernelapp.py\", line 739, in start\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/tornado\/platform\/asyncio.py\", line 205, in start\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/asyncio\/base_events.py\", line 641, in run_forever\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/asyncio\/base_events.py\", line 1986, in _run_once\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/asyncio\/events.py\", line 88, in _run\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/kernelbase.py\", line 545, in dispatch_queue\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/kernelbase.py\", line 534, in process_one\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/kernelbase.py\", line 437, in dispatch_shell\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/ipkernel.py\", line 362, in execute_request\r\nFile \"\/home\/beckerf\/mambaforge\/envs\/learnMSAdev2\/lib\/python3.12\/site-packages\/ipykernel\/kernelbase.py\",\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-11T12:31:35Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77693"},{"repository":"tensorflow\/tensorflow","title":"tf.custom_gradient for function with kwarg shows unexpected behavior","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nNo\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nNo\n\n### OS platform and distribution\n\nUbuntu 22.04.5 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.12.7\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n12.3\/8\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have a function that takes two tensors as inputs, one as argument and one as keyword argument.\r\nThe function has a custom gradient.\r\n\r\nWhen ``tape.gradient`` for both input tensors with respect to the output of the function is called, TensorFlow throws an error, saying that only one gradient is expect and not two.\r\n\r\nWhen the function is called with both inputs as arguments (and not one of them as kwarg), no error is thrown.\r\n\n\n### Standalone code to reproduce the issue\n\n```shell\n@tf.custom_gradient\r\ndef func(x, y=0):\r\n    z = 2*x + y\r\n    def grad(dz):\r\n        dx = 2*dz\r\n        dy = dz\r\n        return dx, dy\r\n    return z, grad\r\nx = tf.constant(2.)\r\ny = tf.constant(3.)\r\nwith tf.GradientTape() as tape:\r\n    tape.watch([x, y])\r\n    z = func(x, y=y) #func(x, y) does not generate the error\r\ngrads = tape.gradient(z, [x, y])\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[129], line 14\r\n     12     tape.watch([x, y])\r\n     13     z = func(x, y=y)\r\n---> 14 grads = tape.gradient(z, [x, y])\r\n\r\nFile ~\/.local\/lib\/python3.12\/site-packages\/tensorflow\/python\/eager\/backprop.py:1066, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients)\r\n   1060   output_gradients = (\r\n   1061       composite_tensor_gradient.get_flat_tensors_for_gradients(\r\n   1062           output_gradients))\r\n   1063   output_gradients = [None if x is None else ops.convert_to_tensor(x)\r\n   1064                       for x in output_gradients]\r\n-> 1066 flat_grad = imperative_grad.imperative_grad(\r\n   1067     self._tape,\r\n   1068     flat_targets,\r\n   1069     flat_sources,\r\n   1070     output_gradients=output_gradients,\r\n   1071     sources_raw=flat_sources_raw,\r\n   1072     unconnected_gradients=unconnected_gradients)\r\n   1074 if not self._persistent:\r\n   1075   # Keep track of watched variables before setting tape to None\r\n   1076   self._watched_variables = self._tape.watched_variables()\r\n\r\nFile ~\/.local\/lib\/python3.12\/site-packages\/tensorflow\/python\/eager\/imperative_grad.py:67, in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\r\n     63 except ValueError:\r\n     64   raise ValueError(\r\n     65       \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\r\n---> 67 return pywrap_tfe.TFE_Py_TapeGradient(\r\n     68     tape._tape,  # pylint: disable=protected-access\r\n     69     target,\r\n     70     sources,\r\n     71     output_gradients,\r\n     72     sources_raw,\r\n     73     compat.as_str(unconnected_gradients.value))\r\n\r\nFile ~\/.local\/lib\/python3.12\/site-packages\/tensorflow\/python\/ops\/custom_gradient.py:588, in _eager_mode_decorator.<locals>.actual_grad_fn(*result_grad_components)\r\n    585 flat_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(\r\n    586     nest.flatten(input_grads))\r\n    587 if len(flat_grads) != arg_count:\r\n--> 588   raise ValueError(\r\n    589       f\"custom_gradient function expected to return {arg_count} \"\r\n    590       f\"gradients, but returned {len(flat_grads)} instead.\")\r\n    591 return flat_grads + variable_grads\r\n\r\nValueError: custom_gradient function expected to return 1 gradients, but returned 2 instead.\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-10T10:18:54Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77559"},{"repository":"tensorflow\/tensorflow","title":"Backward compatibility issue: failure to load models saved in TensorFlow format (Keras 2) in TensorFlow 2.17","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.9.1 (model saved), 2.17.0 (model loaded)\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n(Official Docker Image) Ubuntu 22.04.4 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.8.10 (model saved),  3.11.0rc1 (model loaded)\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n## Description\r\n\r\nI have encountered a backward compatibility issue when loading models saved with Keras 2 in TensorFlow 2.9 into TensorFlow 2.17, which now uses Keras 3 API. This issue impacts various loading methods, and there does not appear to be a straightforward solution to resolve the errors.\r\n\r\n## Steps to reproduce\r\n\r\n1. **Train and export a model in TensorFlow 2.9 with Keras 2 API**:\r\n   - A simple Keras sequential model is created and trained on random data.\r\n   - The model is saved using both `tf.saved_model.save` and `tf.keras.models.save_model` with `tf` save format (which is unsupported in Keras 3).\r\n   \r\n2. **Attempt to load the models in TensorFlow 2.17 with Keras 3 API**:\r\n   - The models are loaded using TensorFlow\u2019s `tf.saved_model.load`, `keras.layers.TFSMLayer`, and `tf.keras.models.load_model`.\r\n\r\n3. **Observe the errors**:\r\n   - When loading using `tf.saved_model.load`, the error `'_UserObject' object has no attribute 'add_slot'` occurs.\r\n   - When loading using `keras.layers.TFSMLayer`, the same `'_UserObject' object has no attribute 'add_slot'` error is triggered.\r\n   - When loading using `tf.keras.models.load_model`, a different error appears: `File format not supported.` Because Keras 3 has dropped support for the default `tf` save format in version 2!\r\n\r\n## Expected behavior\r\n\r\nWhile I understand that issues related to loading legacy Keras models saved with the `tf` save format using Keras are out of scope for TensorFlow and should be addressed by the Keras team, the functionality surrounding TensorFlow's `tf.saved_model`, which uses the `SavedModel` bundle, is part of TensorFlow Core. Since this format is shared across different runtimes, it should remain backward compatible. Therefore, models saved in earlier versions of TensorFlow using the `SavedModel` format should load seamlessly in newer TensorFlow versions, without requiring users to rebuild their models or encountering compatibility errors.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n## Minimal example to reproduce the issue\r\n\r\nA minimal code example to reproduce the issue is available in this repository: [Reproduce TF Model Compatibility Issue.](https:\/\/github.com\/arianmaghsoudnia\/reproduce-tf-model-compat-issue)\r\n\r\nPlease follow the steps in the README file.\r\n\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_","labels":["type:bug","comp:keras","2.17"],"created_at":"2024-10-09T10:50:39Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77356"},{"repository":"tensorflow\/tensorflow","title":"tf.nn.conv2d terminates process with invalid input shape instead of raising an exception","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\n2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.11.9\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nTensorFlow terminates the process when passing an invalid input shape to `tf.nn.conv2d`. Instead of raising a Python exception that can be caught with a try-except block.\r\n\r\nI expected TensorFlow to raise a catchable Python exception indicating that the input tensor shape is invalid. This would allow the error to be handled in a try-except block, instead of terminating the process. The error message should clearly explain the shape mismatch issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\n# Define invalid input tensor and kernel\r\ninput_tensor = [[1.0, 2.0, 3.0]]\r\nkernel = [[0.5, 0.5], [0.5, 0.5]]\r\n\r\ntry:\r\n    # Create TensorFlow constants\r\n    input_tf = tf.constant(input_tensor, dtype=tf.float32)\r\n    kernel_tf = tf.constant(kernel, dtype=tf.float32)\r\n    \r\n    # Attempt to perform convolution, expecting an error\r\n    output_tf = tf.nn.conv2d(\r\n        tf.expand_dims(input_tf, axis=0), \r\n        tf.expand_dims(kernel_tf, axis=0), \r\n        strides=[1, 1, 1, 1], \r\n        padding='VALID'\r\n    )\r\n    \r\n    print(\"TensorFlow Output:\", output_tf.numpy())\r\nexcept Exception as e:\r\n    print(\"TensorFlow Error:\", e)\n```\n\n\n### Relevant log output\n\n```shell\n2024-10-09 14:53:31.118589: F .\/tensorflow\/core\/util\/tensor_format.h:427] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-09T07:52:38Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77336"},{"repository":"tensorflow\/tensorflow","title":"RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.","description":"### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nGoogle Colab with Python 3.10.12\r\n- TensorFlow installation (pip package or built from source):\r\npip package\r\n- TensorFlow library (version, if pip package or github SHA, if built from source):\r\nv2.17.0\r\n\r\n### 2. Code\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nsaved_model_dir = '\/content\/saved_model'\r\n\r\nnum_calibration_steps = 100\r\n\r\ninput = tf.cast(tf.random.normal((1, 640, 640, 3)), tf.float32)\r\ndummy_input = tf.cast(tf.random.normal((1, 2)), tf.int64)\r\n\r\ndef representative_dataset_gen():\r\n    for _ in range(num_calibration_steps):\r\n        yield [dummy_input, input] #model has 2 input tensors\r\n\r\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\r\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\r\nconverter.representative_dataset = representative_dataset_gen\r\nconverter.target_spec.supported_ops = [\r\n  tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\r\n  tf.lite.OpsSet.SELECT_TF_OPS\r\n]\r\nconverter.inference_input_type = tf.int8\r\nconverter.inference_output_type = tf.int8\r\n\r\ntflite_quant_model = converter.convert()\r\n\r\n# Save the quantized model to a local file\r\nwith open('quantized_model.tflite', 'wb') as f:\r\n    f.write(tflite_quant_model)\r\n```\r\n\r\n### 3. Failure after conversion\r\n\r\nAfter converting the model from ONNX using onnx2tf, I got saved_model, which I tried to convert to int8 quantized model using the code above. When trying to inference the model, after reading the model by the interpreter and calling the function `allocate_tensors()` \r\n```\r\ninterpreter = tf.lite.Interpreter(model_path=\"\/content\/quantized_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n```\r\nI get the following error:\r\n\r\n```\r\nRuntimeError                              Traceback (most recent call last)\r\n[<ipython-input-7-b6b80a3bdf94>](https:\/\/localhost:8080\/#) in <cell line: 6>()\r\n      4 interpreter = tf.lite.Interpreter(model_path=\"\/content\/quantized_model.tflite\")\r\n      5 print(interpreter.get_input_details())\r\n----> 6 interpreter.allocate_tensors()\r\n\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/lite\/python\/interpreter.py](https:\/\/localhost:8080\/#) in allocate_tensors(self)\r\n    535   def allocate_tensors(self):\r\n    536     self._ensure_safe()\r\n--> 537     return self._interpreter.AllocateTensors()\r\n    538 \r\n    539   def _safe_to_run(self):\r\n\r\nRuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.\r\n```\r\n\r\nCould someone give me some advice, suggestions on how to solve this error? I couldn't even find that anyone has solved the same problem. \r\nThe closest to this error is this [issue](https:\/\/github.com\/tensorflow\/tensorflow\/issues\/61395), but the workaround is to convert the ONNX model to Keras and for my complex model it is not possible to fix.\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:lite","TFLiteConverter","2.17"],"created_at":"2024-10-08T22:02:42Z","comments":12,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77293"},{"repository":"tensorflow\/tensorflow","title":"tensorflow.python.ops.signal.dct_ops.dct aborts with \"Assertion failure no zero-sized FFTs\"","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20241007\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)\r\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/1oBjZoqp6WZn_VU-CTxZ3bspUc6v9D51s?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\r\nfrom tensorflow.python.eager import def_function\r\nfrom tensorflow.python.framework import tensor_spec\r\nfrom tensorflow.python.ops.signal import dct_ops\r\n\r\ndef test_with_dynamic_dimensions(dct_type, norm, shape, dtype):\r\n    @def_function.function\r\n    def func(signals):\r\n        return dct_ops.dct(signals, n=norm, type=dct_type, norm=None)\r\n    signals_spec = tensor_spec.TensorSpec([None] * len(shape), dtype)\r\n    f = func.get_concrete_function(signals_spec)\r\n    f(np.zeros([0], dtype=dtype))\r\ntest_with_dynamic_dimensions(3, None, [3], np.float32)\n```\n\n\n### Relevant log output\n\n```shell\nDUCC FFT c2r failed: \r\nbazel-out\/k8-opt\/bin\/external\/ducc\/_virtual_includes\/fft\/ducc\/src\/ducc0\/fft\/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):\r\n\r\nAssertion failure\r\nno zero-sized FFTs\r\n\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-10-08T06:57:49Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77211"},{"repository":"tensorflow\/tensorflow","title":"tensorflow.python.ops.parsing_ops.parse_single_sequence_example can cause a crash","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\ntf-nightly 2.19.0-dev20241007\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 20.04.3 LTS\n\n### Mobile device\n\nLinux Ubuntu 20.04.3 LTS\n\n### Python version\n\n3.10.14\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)\r\n\r\nPlease find the [gist](https:\/\/colab.research.google.com\/drive\/17PzKxkDEr3N8E9D9Kk_mT2A1LyPpoZZe?usp=sharing) to reproduce the issue.\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.core.example import example_pb2\r\nfrom tensorflow.core.example import feature_pb2\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.framework import ops\r\nfrom tensorflow.python.ops import parsing_ops\r\nexample = example_pb2.Example\r\nfeature = feature_pb2.Feature\r\nfeatures = lambda d: feature_pb2.Features(feature=d)\r\nbytes_feature = lambda v: feature(bytes_list=feature_pb2.BytesList(value=v))\r\nint64_feature = lambda v: feature(int64_list=feature_pb2.Int64List(value=v))\r\nfloat_feature = lambda v: feature(float_list=feature_pb2.FloatList(value=v))\r\nfeature_list = lambda l: feature_pb2.FeatureList(feature=l)\r\nfeature_lists = lambda d: feature_pb2.FeatureLists(feature_list=d)\r\nsequence_example = example_pb2.SequenceExample\r\n\r\ndef testSequenceExampleListWithWrongShapeFails():\r\n    original = sequence_example(feature_lists=feature_lists({'a': feature_list([int64_feature([2, 3]), int64_feature([2, 3, 4])])}))\r\n    serialized = original.SerializeToString()\r\n    parsing_ops.parse_single_sequence_example(**\r\n        ({\r\n            'example_name': 'in1',\r\n            'serialized': ops.convert_to_tensor(serialized),\r\n            'sequence_features': {'a': parsing_ops.FixedLenSequenceFeature((0, 0), dtypes.int64)}\r\n        }))\r\ntestSequenceExampleListWithWrongShapeFails()\n```\n\n\n### Relevant log output\n\n```shell\nFloating point exception (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops"],"created_at":"2024-10-08T06:52:07Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77210"},{"repository":"tensorflow\/tensorflow","title":"`SimpleDynamicBuffer::AddString` is calling `memcpy` with null data","description":"I've noticed this hitting on our ubsan builds recently:\r\n\r\n```\r\n..\/..\/third_party\/tflite\/src\/tensorflow\/compiler\/mlir\/lite\/utils\/string_utils.cc:32:10: runtime error: null pointer passed as argument 1, which is declared to never be null\r\n..\/..\/build\/linux\/debian_bullseye_amd64-sysroot\/usr\/include\/string.h:44:28: note: nonnull attribute specified here\r\n    #0 0x5a36de826450 in mlir::TFL::SimpleDynamicBuffer::AddString(char const*, unsigned long) third_party\/tflite\/src\/tensorflow\/compiler\/mlir\/lite\/utils\/string_utils.cc:32:3\r\n    #1 0x5a36de825d3e in tflite::DynamicBuffer::AddString(char const*, unsigned long) third_party\/tflite\/src\/tensorflow\/lite\/string_util.cc:37:28\r\n    #2 0x5a36de82924d in PopulateTensor<std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char> > > third_party\/tflite_support\/src\/tensorflow_lite_support\/cc\/task\/core\/task_utils.h:125:13\r\n    #3 0x5a36de82924d in tflite::task::processor::UniversalSentenceEncoderPreprocessor::Preprocess(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party\/tflite_support\/src\/tensorflow_lite_support\/cc\/task\/processor\/universal_sentence_encoder_preprocessor.cc:58:3\r\n    #4 0x5a36de81d3f7 in tflite::task::text::TextEmbedder::Preprocess(std::__Cr::vector<TfLiteTensor*, std::__Cr::allocator<TfLiteTensor*>> const&, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party\/tflite_support\/src\/tensorflow_lite_support\/cc\/task\/text\/text_embedder.cc:174:25\r\n    #5 0x5a36de81cd8c in tflite::task::core::BaseTaskApi<tflite::task::processor::EmbeddingResult, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&>::InferWithFallback(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party\/tflite_support\/src\/tensorflow_lite_support\/cc\/task\/core\/base_task_api.h:146:5\r\n    #6 0x5a36de81cc40 in tflite::task::text::TextEmbedder::Embed(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party\/tflite_support\/src\/tensorflow_lite_support\/cc\/task\/text\/text_embedder.cc:169:10\r\n    #7 0x5a36d2728c24 in ai_chat::TextEmbedder::EmbedText(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&, tflite::task::processor::EmbeddingResult&) brave\/components\/ai_chat\/core\/browser\/text_embedder.cc:271:49\r\n    #8 0x5a36d2728073 in ai_chat::TextEmbedder::EmbedSegments() brave\/components\/ai_chat\/core\/browser\/text_embedder.cc:287:19\r\n    #9 0x5a36c8c67059 in ai_chat::TextEmbedderUnitTest::EmbedSegments(ai_chat::TextEmbedder*)::'lambda'()::operator()() const brave\/components\/ai_chat\/core\/browser\/text_embedder_unittest.cc:67:58\r\n    #10 0x5a36c6762969 in base::OnceCallback<void ()>::Run() && base\/functional\/callback.h:156:12\r\n    #11 0x5a36d4977df2 in base::TaskAnnotator::RunTaskImpl(base::PendingTask&) base\/task\/common\/task_annotator.cc:202:34\r\n    #12 0x5a36d49dcfe9 in RunTask<(lambda at ..\/..\/base\/task\/thread_pool\/task_tracker.cc:678:35)> base\/task\/common\/task_annotator.h:90:5\r\n    #13 0x5a36d49dcfe9 in base::internal::TaskTracker::RunTaskImpl(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base\/task\/thread_pool\/task_tracker.cc:677:19\r\n    #14 0x5a36d49dd0f1 in base::internal::TaskTracker::RunSkipOnShutdown(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base\/task\/thread_pool\/task_tracker.cc:662:3\r\n    #15 0x5a36d49dc1f5 in base::internal::TaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base\/task\/thread_pool\/task_tracker.cc:520:5\r\n    #16 0x5a36d4af81fb in base::test::TaskEnvironment::TestTaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base\/test\/task_environment.cc:1028:46\r\n    #17 0x5a36d49db5a5 in base::internal::TaskTracker::RunAndPopNextTask(base::internal::RegisteredTaskSource) base\/task\/thread_pool\/task_tracker.cc:415:5\r\n    #18 0x5a36d4a0cabd in base::internal::WorkerThread::RunWorker() base\/task\/thread_pool\/worker_thread.cc:493:36\r\n    #19 0x5a36d4a0c100 in base::internal::WorkerThread::RunPooledWorker() base\/task\/thread_pool\/worker_thread.cc:379:3\r\n    #20 0x5a36d4a0bc86 in base::internal::WorkerThread::ThreadMain() base\/task\/thread_pool\/worker_thread.cc:359:7\r\n    #21 0x5a36d4a3e1ec in base::(anonymous namespace)::ThreadFunc(void*) base\/threading\/platform_thread_posix.cc:101:13\r\n    #22 0x7695b109ca93 in start_thread nptl\/pthread_create.c:447:8\r\n    #23 0x7695b1129c3b in clone3 misc\/..\/sysdeps\/unix\/sysv\/linux\/x86_64\/clone3.S:78\r\n```","labels":["type:bug","comp:lite","TFLiteConverter","awaiting PR merge"],"created_at":"2024-10-07T18:57:35Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77168"},{"repository":"tensorflow\/tensorflow","title":"NotImplementedError from tf.constant in trivial case","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\n2.16.1\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.12\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nTrying to make a tensor that has the same value for all items in the batch, see the following bare minimum code. \r\nI get `NotImplementedError: cannot convert a symbolic tf.Tensor (custom_model_5_1\/strided_slice:0) to a numpy array.`\r\nI am not trying to use numpy, this is an internal error.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\nimport keras\r\nimport numpy as np\r\n\r\nclass CustomModel(keras.models.Model):\r\n    def call(self, inputs):\r\n        inputs_shape = tf.shape(inputs)\r\n        return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # NotImplementedError\r\n        #return 3.0 * tf.ones(shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # OK\r\n\r\nmodel = CustomModel()\r\nmodel.compile(run_eagerly=False, loss=\"mse\")  # OK if run_eagerly=True\r\nmodel.fit(np.array([[0.0]]), np.array([[0.0]]))\r\n```\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n{\r\n\t\"name\": \"NotImplementedError\",\r\n\t\"message\": \"Exception encountered when calling CustomModel.call().\r\n\r\nCannot convert a symbolic tf.Tensor (custom_model_5_1\/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\r\n\r\nArguments received by CustomModel.call():\r\n  \u2022 inputs=tf.Tensor(shape=(None, 1), dtype=float32)\",\r\n\t\"stack\": \"---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\nCell In[6], line 13\r\n     11 model = CustomModel()\r\n     12 model.compile(run_eagerly=False, loss=\\\"mse\\\")  # OK if run_eagerly=True\r\n---> 13 model.fit(np.array([[0.0]]), np.array([[0.0]]))\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/keras\/src\/utils\/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n    119     filtered_tb = _process_traceback_frames(e.__traceback__)\r\n    120     # To get the full stack trace, call:\r\n    121     # `keras.config.disable_traceback_filtering()`\r\n--> 122     raise e.with_traceback(filtered_tb) from None\r\n    123 finally:\r\n    124     del filtered_tb\r\n\r\nCell In[6], line 8, in CustomModel.call(self, inputs)\r\n      6 def call(self, inputs):\r\n      7     inputs_shape = tf.shape(inputs)\r\n----> 8     return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/numpy\/core\/fromnumeric.py:3100, in prod(a, axis, dtype, out, keepdims, initial, where)\r\n   2979 @array_function_dispatch(_prod_dispatcher)\r\n   2980 def prod(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\r\n   2981          initial=np._NoValue, where=np._NoValue):\r\n   2982     \\\"\\\"\\\"\r\n   2983     Return the product of array elements over a given axis.\r\n   2984 \r\n   (...)\r\n   3098     10\r\n   3099     \\\"\\\"\\\"\r\n-> 3100     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\r\n   3101                           keepdims=keepdims, initial=initial, where=where)\r\n\r\nFile \/usr\/local\/lib\/python3.10\/dist-packages\/numpy\/core\/fromnumeric.py:88, in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)\r\n     85         else:\r\n     86             return reduction(axis=axis, out=out, **passkwargs)\r\n---> 88 return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n\r\nNotImplementedError: Exception encountered when calling CustomModel.call().\r\n\r\nCannot convert a symbolic tf.Tensor (custom_model_5_1\/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.\r\n\r\nArguments received by CustomModel.call():\r\n  \u2022 inputs=tf.Tensor(shape=(None, 1), dtype=float32)\"\r\n}\r\n```\r\n","labels":["type:bug","TF 2.16"],"created_at":"2024-10-04T13:38:25Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/77045"},{"repository":"tensorflow\/tensorflow","title":"TFlite compilation crashes on MacOS (error: _Float16 is not supported on this target)","description":"### Issue type\r\n\r\nBuild\/Install\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18.0-rc0\r\n\r\n### Custom code\r\n\r\nNo\r\n\r\n### OS platform and distribution\r\n\r\nMacOS 15.0\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.12\r\n\r\n### Bazel version\r\n\r\n6.5\r\n\r\n### GCC\/compiler version\r\n\r\nApple clang version 16.0.0 (clang-1600.0.26.3)\r\nXCode 16.0\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nCompiling TF_lite v2.18.0-rc0 from source, using the command suggested in the documentation leads to crash (log below):\r\n\r\n```PYTHON=python3 tensorflow\/lite\/tools\/pip_package\/build_pip_package_with_bazel.sh native```\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nPYTHON=python3 tensorflow\/lite\/tools\/pip_package\/build_pip_package_with_bazel.sh native\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nTF2_BEHAVIOR=1 \\\r\n    XCODE_VERSION_OVERRIDE=16.0.0.16A242d \\\r\n    ZERO_AR_DATE=1 \\\r\n  external\/local_config_cc\/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' 'DEBUG_PREFIX_MAP_PWD=.' -iquote external\/XNNPACK -iquote bazel-out\/darwin-opt\/bin\/external\/XNNPACK -iquote external\/pthreadpool -iquote bazel-out\/darwin-opt\/bin\/external\/pthreadpool -iquote external\/FXdiv -iquote bazel-out\/darwin-opt\/bin\/external\/FXdiv -iquote external\/cpuinfo -iquote bazel-out\/darwin-opt\/bin\/external\/cpuinfo -iquote external\/FP16 -iquote bazel-out\/darwin-opt\/bin\/external\/FP16 -Ibazel-out\/darwin-opt\/bin\/external\/pthreadpool\/_virtual_includes\/pthreadpool -Ibazel-out\/darwin-opt\/bin\/external\/FXdiv\/_virtual_includes\/FXdiv -Ibazel-out\/darwin-opt\/bin\/external\/cpuinfo\/_virtual_includes\/cpuinfo -Ibazel-out\/darwin-opt\/bin\/external\/FP16\/_virtual_includes\/FP16 -isystem external\/XNNPACK\/include -isystem bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/include -isystem external\/XNNPACK\/src -isystem bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/src -isystem external\/pthreadpool\/include -isystem bazel-out\/darwin-opt\/bin\/external\/pthreadpool\/include -isystem external\/FXdiv\/include -isystem bazel-out\/darwin-opt\/bin\/external\/FXdiv\/include -isystem external\/cpuinfo\/include -isystem bazel-out\/darwin-opt\/bin\/external\/cpuinfo\/include -isystem external\/cpuinfo\/src -isystem bazel-out\/darwin-opt\/bin\/external\/cpuinfo\/src -isystem external\/FP16\/include -isystem bazel-out\/darwin-opt\/bin\/external\/FP16\/include -MD -MF bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/_objs\/microkernel_configs\/cmul-config.d -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_LOG_LEVEL=0' '-DXNN_ENABLE_CPUINFO=1' '-DXNN_ENABLE_MEMOPT=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=1' '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_SPARSE=1' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_ARM_I8MM=0' '-DXNN_ENABLE_RISCV_FP16_VECTOR=0' '-DXNN_ENABLE_AVX512VNNIGFNI=1' '-DXNN_ENABLE_AVX512AMX=1' '-DXNN_ENABLE_AVX512FP16=1' '-DXNN_ENABLE_AVXVNNI=1' '-DXNN_ENABLE_AVXVNNIINT8=1' '-DXNN_ENABLE_AVX256SKX=1' '-DXNN_ENABLE_AVX256VNNI=1' '-DXNN_ENABLE_AVX256VNNIGFNI=1' '-DXNN_ENABLE_HVX=0' '-DXNN_ENABLE_KLEIDIAI=0' '-DBAZEL_CURRENT_REPOSITORY=\"XNNPACK\"' '-frandom-seed=bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/_objs\/microkernel_configs\/cmul-config.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__\/System\/Library\/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__\/Platforms\/MacOSX.platform\/Developer\/Library\/Frameworks -no-canonical-prefixes -pthread -DGRPC_BAZEL_BUILD -w -O3 '-march=native' -Iinclude -Isrc '-DXNN_ENABLE_CPUINFO=1' '-std=c99' -O2 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=\"redacted\"' '-D__TIMESTAMP__=\"redacted\"' '-D__TIME__=\"redacted\"' -target x86_64-apple-macosx15.0 -c external\/XNNPACK\/src\/configs\/cmul-config.c -o bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/_objs\/microkernel_configs\/cmul-config.o)\r\n# Configuration: 7ffafbfebd31d6f3229fc3b4603178937ffcc0387347731c7b47fcb97e2cd76d\r\n# Execution platform: @local_execution_config_platform\/\/:platform\r\nERROR: \/private\/var\/tmp\/_bazel_feranick\/50b852099a3bf3aaa184abce166f8e34\/external\/XNNPACK\/BUILD.bazel:803:36: Compiling external\/XNNPACK\/sse_prod_microkernels.c failed: (Exit 1): wrapped_clang failed: error executing command (from target @XNNPACK\/\/:sse_prod_microkernels) external\/local_config_cc\/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' ... (remaining 109 arguments skipped)\r\nIn file included from bazel-out\/darwin-opt\/bin\/external\/XNNPACK\/sse_prod_microkernels.c:1:\r\nIn file included from external\/XNNPACK\/src\/xnnpack\/avgpool.h:15:\r\nIn file included from external\/XNNPACK\/src\/xnnpack\/microparams.h:12:\r\nIn file included from external\/XNNPACK\/src\/xnnpack\/math.h:21:\r\nIn file included from bazel-out\/darwin-opt\/bin\/external\/FP16\/_virtual_includes\/FP16\/fp16\/fp16.h:10:\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:614:27: error: _Float16 is not supported on this target\r\n  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:614:8: error: _Float16 is not supported on this target\r\n  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:615:28: error: _Float16 is not supported on this target\r\n  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:615:38: error: _Float16 is not supported on this target\r\n  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                                      ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:615:8: error: _Float16 is not supported on this target\r\n  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:616:27: error: _Float16 is not supported on this target\r\n  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:616:8: error: _Float16 is not supported on this target\r\n  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:617:27: error: _Float16 is not supported on this target\r\n  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:617:8: error: _Float16 is not supported on this target\r\n  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:618:28: error: _Float16 is not supported on this target\r\n  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:618:8: error: _Float16 is not supported on this target\r\n  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:619:27: error: _Float16 is not supported on this target\r\n  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                           ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:619:8: error: _Float16 is not supported on this target\r\n  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:620:28: error: _Float16 is not supported on this target\r\n  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:620:8: error: _Float16 is not supported on this target\r\n  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:621:28: error: _Float16 is not supported on this target\r\n  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                            ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:621:8: error: _Float16 is not supported on this target\r\n  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |        ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:622:31: error: _Float16 is not supported on this target\r\n  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                               ^\r\n\/Applications\/Xcode.app\/Contents\/Developer\/Platforms\/MacOSX.platform\/Developer\/SDKs\/MacOSX15.0.sdk\/usr\/include\/math.h:622:41: error: _Float16 is not supported on this target\r\n  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));\r\n      |                                         ^\r\nfatal error: too many errors emitted, stopping now [-ferror-limit=]\r\n20 errors generated.\r\nError in child process '\/usr\/bin\/xcrun'. 1\r\nTarget \/\/tensorflow\/lite\/python\/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nINFO: Elapsed time: 119.893s, Critical Path: 3.17s\r\nINFO: 342 processes: 313 internal, 29 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","type:build\/install","comp:lite","2.17"],"created_at":"2024-10-02T18:20:47Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76976"},{"repository":"tensorflow\/tensorflow","title":"Jit-compiling `tf.while_loop` inside `tf.vectorized_map` raises `InvalidArgumentError`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nbinary\n\n### TensorFlow version\n\nv2.17.0\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nColab\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nMRE\r\n-------\r\nThe following mock-up of `cumsum` attempts to JIT compile a `tf.vectorized_map`ped function containing a `tf.scan`:\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef cumsum(xs):\r\n    return tf.scan(\r\n        lambda a, x: a + x, elems=xs\r\n    )\r\n\r\n@tf.function(jit_compile=True)\r\ndef vec_cumsum(xs):\r\n    return tf.vectorized_map(cumsum, elems=xs)\r\n\r\nxs_batched = tf.reshape(tf.range(30), (3, 10))\r\nvec_cumsum(xs_batched)\r\n```\r\n\r\n__Expected behaviour__: `vec_cumsum(xs_batched)` returns a batch of cumulative sums.\r\n\r\n__Actual behaviour__: Even though all data structures are known statically at JIT compile time, an InvalidArgumentError is raised with \"No registered 'TensorListReserve'\".  The fault is clearly related to `tf.scan`'s use of `tf.while_loop`, as a (longer) example using naked `tf.while_loop(..., max_iterations=n)` will confirm.\r\n\r\nIn JAX, it is possible to jit-compile a `vmap`ped function containing a `lax.while_loop` indicating that this is possible in HLO.  It seems the `tf.function(jit_compile=True)` machinery may be mis-transpiling to HLO somehow.\r\n\r\nMay be related to #73367 also involving `tf.vectorized_map` and `tf.while_loop` (albeit with reversed scope)?\n\n### Standalone code to reproduce the issue\n\n```shell\nColab MRE: https:\/\/colab.research.google.com\/drive\/1bmq1t3PdtebCSlNd0t-iEFrXX7Q0qqZp?usp=sharing\n```\n\n\n### Relevant log output\n\n```shell\n---------------------------------------------------------------------------\r\nInvalidArgumentError                      Traceback (most recent call last)\r\n<ipython-input-4-26ea491cd046> in <cell line: 13>()\r\n     11 \r\n     12 # Fails with \"No registered 'TensorListReserve'\"\"\r\n---> 13 vec_cumsum(xs_batched)\r\n\r\n1 frames\r\n\/usr\/local\/lib\/python3.10\/dist-packages\/tensorflow\/python\/eager\/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51   try:\r\n     52     ctx.ensure_initialized()\r\n---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                         inputs, attrs, num_outputs)\r\n     55   except core._NotOkStatusException as e:\r\n\r\nInvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_vec_cumsum_462[_XlaMustCompile=true,config_proto=13561319589895757934,executor_type=11160318154034397263] on XLA_CPU_JIT: TensorListReserve (No registered 'TensorListReserve' OpKernel for XLA_CPU_JIT devices compatible with node {{function_node __inference_while_fn_428}}{{node while_init\/TensorArrayV2_4}}\r\n\t (OpKernel was found, but attributes didn't match) Requested Attributes: element_dtype=DT_VARIANT, shape_type=DT_INT32){{function_node __inference_while_fn_428}}{{node while_init\/TensorArrayV2_4}}\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:apis","comp:ops","2.17"],"created_at":"2024-10-01T17:01:36Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76891"},{"repository":"tensorflow\/tensorflow","title":"Multithreading is not working with teansorflow","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntensorflow==2.15.0.post1\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\npython:3.10.12\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nI am using bert model for classification and serving the model with gunicorn worker_class=gthreads, tf.config.threading.set_intra_op_parallelism_threads(1)\r\ntf.config.threading.set_inter_op_parallelism_threads(1)\r\n\r\nwhen I using above two line of code the code is working fine as expected and if increase the number to more than 1, the code getting blocked at the below line of code\r\n\r\n# Make predictions\r\noutputs = model_obj(inputs)\r\n\r\nand also inorder to reduce the docker image size I am using \r\nRUN pip3 install torch==2.0.0+cpu -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\r\n\r\n\r\nbefore installing all the dependencies\r\nFlask==2.2.5\r\ng2p-en==2.1.0\r\ngunicorn==21.2.0\r\njellyfish==1.0.3\r\nkenlm==0.2.0\r\nnltk==3.8.1\r\nnumpy==1.26.3\r\npandas==2.2.0\r\npython-dotenv==1.0.1\r\nrequests==2.31.0\r\nscikit-learn==1.4.0\r\nsemantic-router==0.0.17\r\nsemantic-router[fastembed]\r\nsentence-transformers==2.3.1\r\ntensorflow==2.15.0.post1\r\ntensorflow-hub==0.16.0\r\ntheano==1.0.5\r\ntransformers==4.37.2\r\nWerkzeug==2.2.2\r\n\r\n\r\nplease tell me why is my code is getting blocked if I use more than 1 thread.\n\n### Standalone code to reproduce the issue\n\n```shell\ndef intent_prediction(self, sentence, thresold_score):\r\n        logger.info(f\"Threshold Score: {thresold_score}\")\r\n        try:\r\n            model_obj = intent_object_dict[self.model]\r\n        except KeyError:\r\n            logger.error(\"Model not found in intent_object_dict\")\r\n            model_obj = self.load_model()\r\n\r\n        if INTENT_MODEL == \"cohere\":\r\n            score, intent = self.cohere_intent_prediction(sentence, model_obj)\r\n        else:\r\n            score, intent = self.Bert_intent_prediction(\r\n                sentence, model_obj, thresold_score\r\n            )\r\n        return score, intent\r\n\r\ndef Bert_intent_prediction(self, sentence, model_obj, thresold_score):\r\n        inputs = self.load_BERT_tokenizer(sentence)\r\n        # Make predictions\r\n        outputs = model_obj(inputs)\r\n        # Get predicted class\r\n        probabilities = tf.nn.softmax(outputs.logits, axis=1)\r\n        predicted_class = tf.argmax(probabilities, axis=1).numpy()[0]\r\n        matching_score = probabilities[0][predicted_class].numpy()\r\n        try:\r\n            intent_data = intent_label_dict[self.model]\r\n        except Exception as e:\r\n            logger.error(f\"error while getting label: {e}\")\r\n            intent_data = self.get_intent_labels()\r\n\r\n        if matching_score >= thresold_score:\r\n            logger.info(\r\n                f\"Matched Main Intent:\\\r\n    {intent_data[predicted_class]},\\\r\n    SCORE :{matching_score}\"\r\n            )\r\n            logger.info(f\"Matched Sentence: {intent_data[predicted_class]}\")\r\n        else:\r\n            logger.info(f\"Intent not matched, score is {matching_score}\")\r\n\r\n        intent = intent_data[predicted_class]\r\n\r\n        return str(matching_score), intent\n```\n\n\n### Relevant log output\n\n```shell\n30-Sep-2024 15:50:42.761|INFO    |__init__|I want my sofa get cleaned|\r\n    __init__.py:171|Enter into PUNC for intent...\r\n30-Sep-2024 15:50:42.761|INFO    |phrase_sim|I want my sofa get cleaned|\r\n    phrase_sim.py:72|Threshold Score: 0.7\r\n\r\n\r\nafter this the code is blocked\n```\n","labels":["stat:awaiting tensorflower","type:bug","TF 2.15"],"created_at":"2024-09-30T10:37:06Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76794"},{"repository":"tensorflow\/tensorflow","title":"Segmentation fault (core dumped) in `tf.data.experimental.SqlDataset`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0-dev20240925\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the illegal input to tf.data.experimental.SqlDataset triggered when a crash, and will only come when iteration data.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\ndata_source_name = \"sqlite:\/\/\/path\/to\/correct_database.db\"\r\n\r\nquery = \"SELECT id, name FROM my_table\"\r\noutput_types = (tf.int64, tf.string)\r\ndataset = tf.data.experimental.SqlDataset(\r\n    'sqlite', data_source_name, query, output_types)\r\n\r\nfor element in dataset:\r\n    print(element)\n```\n\n\n### Relevant log output\n\n```shell\n2024-09-28 21:18:33.844482: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 21:18:33.907260: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 21:18:33.986019: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 21:18:34.009755: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 21:18:34.068897: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-28 21:18:38.768599: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-09-28 21:18:38.769172: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-09-28 21:18:39.132534: W tensorflow\/core\/kernels\/data\/experimental\/sql_dataset_op.cc:209] Failed to connect to database: INVALID_ARGUMENT: Sqlite::Open(sqlite:\/\/\/path\/to\/correct_database.db) failed: unable to open database file\r\nSegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T13:20:59Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76731"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.linalg.det\/slogdet\/logdet\/cholesky\/inv`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18.0-dev20240925\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04.3 LTS (x86_64)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\ntf.linalg.det\/slogdet\/logdet\/cholesky\/inv triggered a crash when the input is empty. Note that this will only be triggered if the gpu is available.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\ninvalid_input = tf.zeros([])\r\ntf.linalg.det(invalid_input)    # crash\r\ntf.linalg.slogdet(invalid_input)  # crash\r\ntf.linalg.cholesky(invalid_input)  # crash\r\ntf.linalg.logdet(invalid_input)  # crash\r\ntf.linalg.inv(invalid_input)  # crash\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-09-28 21:11:10.188752: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 21:11:10.199880: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 21:11:10.213635: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 21:11:10.221654: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 21:11:10.279720: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-28 21:11:17.015480: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-09-28 21:11:17.015957: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-09-28 21:11:17.154391: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T13:15:48Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76730"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.raw_ops.ResourceScatterNdop`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0-dev20240925\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nWhen the type of resource_handle is inconsistent with that of updates,tf.raw_ops.ResourceScatterNdop triggers the crash. As follows:\r\ntf.raw_ops.ResourceScatterNdUpdate\r\ntf.raw_ops.ResourceScatterNdAdd\r\ntf.raw_ops.ResourceScatterNdSub\r\ntf.raw_ops.ResourceScatterNdMax\r\ntf.raw_ops.ResourceScatterNdMin\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nresource_var = tf.Variable(initial_value=tf.zeros([2, 2], dtype=tf.int32), trainable=False)\r\nresource_handle = resource_var.handle\r\n\r\nindices = np.array([[2, 1], [1, 2]], dtype=np.int32)\r\nupdates = np.array([10, 20], dtype=np.float32)\r\ntf.raw_ops.ResourceScatterNdUpdate(  # crash\r\n    ref=resource_handle,\r\n    indices=indices,\r\n    updates=updates,\r\n    use_locking=True\r\n)\r\n\r\ntf.raw_ops.ResourceScatterNdAdd(  # crash\r\n    ref=resource_handle,\r\n    indices=indices,\r\n    updates=updates,\r\n    use_locking=True\r\n)\r\ntf.raw_ops.ResourceScatterNdSub(  # crash\r\n    ref=resource_handle,\r\n    indices=indices,\r\n    updates=updates,\r\n    use_locking=True\r\n)\r\ntf.raw_ops.ResourceScatterNdMax(  # crash\r\n    ref=resource_handle,\r\n    indices=indices,\r\n    updates=updates,\r\n    use_locking=True\r\n)\r\ntf.raw_ops.ResourceScatterNdMin(  # crash\r\n    ref=resource_handle,\r\n    indices=indices,\r\n    updates=updates,\r\n    use_locking=True\r\n)\n```\n\n\n### Relevant log output\n\n```shell\n2024-09-28 21:06:23.445185: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 21:06:23.508056: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 21:06:23.583640: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 21:06:23.607538: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 21:06:23.664877: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-28 21:06:31.527466: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-09-28 21:06:31.527985: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-09-28 21:06:31.782114: F tensorflow\/core\/framework\/tensor.cc:844] Check failed: dtype() == expected_dtype (3 vs. 1) float expected, got int32\r\nAborted (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T13:09:06Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76729"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.io.encode_png`\/`tf.compat.v1.image.encode_png`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.18.0-dev20240925\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n_No response_\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nThe crash was triggered when an illegal image was passed to tf.io.encode_png\/tf.compat.v1.image.encode_png\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\nimage = tf.cast(tf.tile([[[0, 0, 0, 1]], [[0, 0, 1, 0]]], [0, 0, 1]), tf.uint8)\r\n\r\nencoded_image = tf.compat.v1.image.encode_png(image) # crash\r\ntf.io.encode_png(image, compression=-1, name=None) #crash\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-09-28 20:48:36.270008: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 20:48:36.332972: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 20:48:36.411391: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 20:48:36.428306: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 20:48:36.438336: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-09-28 20:48:41.296886: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.92024-09-28 20:48:41.297450: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-09-28 20:48:41.475588: F tensorflow\/core\/lib\/png\/png_io.cc:350] 'image' Must be non NULL\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T12:49:42Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76726"},{"repository":"tensorflow\/tensorflow","title":"Floating point exception (core dumped) in `tf.nn.depth_to_space`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0-dev20240925\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific inputs, tf.nn.depth_to_space triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\ntry:\r\n    # Create an empty tensor\r\n    arg_0_tensor = tf.zeros([0, 2, 3, 12], dtype=tf.float32)\r\n    # arg_0 = tf.identity(arg_0_tensor)\r\n    arg_1 = 536870912\r\n    out = tf.nn.depth_to_space(arg_0_tensor, arg_1)\r\nexcept Exception as e:\r\n    print(\"Error:\", str(e))\n```\n\n\n### Relevant log output\n\n```shell\n2024-09-28 20:41:05.888017: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 20:41:05.950498: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 20:41:06.028236: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 20:41:06.052072: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 20:41:06.111011: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-28 20:41:11.970896: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 2704 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-09-28 20:41:11.973176: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\nFloating point exception (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T12:41:49Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76724"},{"repository":"tensorflow\/tensorflow","title":"Aborted (core dumped) in `tf.nn.max_pool\/tf.nn.max_pool1d`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 22.04.3 LTS (x86_64)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nUnder specific inputs, tf.nn.max_pool triggered a crash.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\n\r\ninvalid_kernel_size = -1\r\ninvalid_operation = tf.nn.max_pool(\r\n    tf.random.normal([1, 32, 32, 3]),\r\n    ksize=[1, invalid_kernel_size, invalid_kernel_size, 1],\r\n    strides=[1, 2, 2, 1],\r\n    padding='SAME'\r\n)\r\n```\r\n\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\n\r\nksize = sys.maxsize + 100  # Set to a value larger than sys.maxsize\r\ninput_tensor = tf.random.normal(shape=(2, 10, 4))\r\nresult = tf.nn.max_pool1d(input=input_tensor, ksize=ksize, strides=1, padding='SAME')\r\n\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-09-28 20:26:47.491907: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\r\n2024-09-28 20:26:47.554171: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 20:26:47.606570: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 20:26:47.610539: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n2024-09-28 20:26:47.639739: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\r\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\n2024-09-28 20:26:54.579839: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:0 with 21471 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9\r\n2024-09-28 20:26:54.582099: I tensorflow\/core\/common_runtime\/gpu\/gpu_device.cc:2021] Created device \/job:localhost\/replica:0\/task:0\/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9\r\n2024-09-28 20:26:55.563477: I external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:531] Loaded cuDNN version 8907\r\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\nF0000 00:00:1727526415.563805  147227 cuda_dnn.cc:1107] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0) \r\n*** Check failure stack trace: ***\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T12:28:21Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76722"},{"repository":"tensorflow\/tensorflow","title":"Segmentation fault (core dumped) in `tf.profiler.experimental.Profile`","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\n2.18.0-dev20240925\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nLinux Ubuntu 22.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9.13\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\nUnder specific inputs, tf.profiler.experimental.Profile triggered a crash.\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\n\r\nprofiler_options = tf.profiler.experimental.ProfilerOptions(\r\n    host_tracer_level=999,\r\n    python_tracer_level=-1,\r\n    device_tracer_level=10,\r\n    delay_ms=None\r\n)\r\n\r\nwith tf.profiler.experimental.Profile(None, options=profiler_options):\r\n    a = tf.constant(1)\r\n    b = tf.constant(2)\r\n    c = a + b\r\n    print(c.numpy())\n```\n\n\n### Relevant log output\n\n```shell\n2024-09-28 20:07:36.902909: I tensorflow\/core\/util\/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-09-28 20:07:36.966049: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n2024-09-28 20:07:36.998027: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n2024-09-28 20:07:37.002984: E external\/local_xla\/xla\/stream_executor\/cuda\/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered2024-09-28 20:07:37.055864: I tensorflow\/core\/platform\/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\r\nSegmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-28T12:11:00Z","comments":2,"reactions":1,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76718"},{"repository":"tensorflow\/tensorflow","title":"TensorFlow keeps creating threads when multi-GPU training \uff08thread leak\uff09","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\n2.11.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.9.13\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n11.4\r\n\r\n### GPU model and memory\r\n\r\nNvidia A800 \r\n\r\n### Current behavior?\r\n\r\nI was using this machine to train a GPT-2 example from (the data I used can also be found in this link, also here https:\/\/github.com\/chinese-poetry\/chinese-poetry.git) https:\/\/keras.io\/examples\/generative\/gpt2_text_generation_with_kerasnlp\/\r\n\r\nBefore we start, I would give a baseline amount of this machine's threads :\r\n![10](https:\/\/github.com\/user-attachments\/assets\/53e16381-e50f-4054-8c2a-b790fe2e077b)\r\n\r\nWhen starting with the multi-GPU training of tensorflow by calling the tf.distribute.MirroredStrategy, the training process worked fine as usual. \r\n\r\nBut with the time went by, I found the amount of threads increased with the training process going, here is the evidence of thread increasing when processing 3354th batch (I used cat \/proc\/\"this programs' pid\"\/status to check the number of threads):\r\n![3](https:\/\/github.com\/user-attachments\/assets\/56bbfaf0-f59c-48f1-ae19-a98d563ad000)\r\n![tf](https:\/\/github.com\/user-attachments\/assets\/c546ea4b-855a-44e5-84c1-95d6d3f3aba4)\r\n![2](https:\/\/github.com\/user-attachments\/assets\/0d07b24d-e93e-451e-9e1e-51a26f7db60d)\r\n\r\nThen, the evidence of thread increasing when processing 3791st batch (the amount of threads reached 22178):\r\n![6](https:\/\/github.com\/user-attachments\/assets\/508fed5c-8aec-4deb-8e68-74dbf6aa5613)\r\n![4](https:\/\/github.com\/user-attachments\/assets\/77507790-70fd-4abf-b72c-e9d831565879)\r\n![5](https:\/\/github.com\/user-attachments\/assets\/d8945e53-23d9-4fb7-b50d-37962b93a692)\r\n\r\nWhen calculating  the 5054th batch, the training program got an error, and I captured a count of threads before the error (achieved around 31120 threads):\r\n![8](https:\/\/github.com\/user-attachments\/assets\/15b433cc-eefb-451f-a31c-2a286e17afec)\r\n![8](https:\/\/github.com\/user-attachments\/assets\/813835d2-3834-400a-b8fe-ec27f15d89ad)\r\n\r\nI checked an similar issue in https:\/\/github.com\/tensorflow\/tensorflow\/issues\/62466, but I cannot find a solution, moreover, I have run other examples like diffusion model using this machine and the same tensorflow env with multi-GPU training, which worked fine and no any problems. So, could you please give me a help for this problem, very appreciated.\r\n\r\n\r\n\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nHere is my code\r\n\r\nimport os\r\n\r\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"tensorflow\" or \"torch\"\r\n\r\nimport keras_nlp\r\nimport keras\r\nimport tensorflow as tf\r\nimport time\r\n\r\nkeras.mixed_precision.set_global_policy(\"mixed_float16\")\r\n\r\nimport os\r\nimport json\r\nimport datetime\r\n\r\ntrain_ds = (\r\n    tf.data.Dataset.from_tensor_slices(paragraphs)\r\n    .batch(36)\r\n    .cache()\r\n    .prefetch(tf.data.AUTOTUNE)\r\n)\r\n\r\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\", \"GPU:4\", \"GPU:5\"])\r\nprint(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\r\n\r\npreprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\r\n    \"gpt2_base_en\",\r\n    sequence_length=128,\r\n)\r\n\r\n# Open a strategy scope.\r\nwith strategy.scope():\r\n# To speed up training and generation, we use preprocessor of length 128\r\n# instead of full length 1024.\r\n    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\r\n        \"gpt2_base_en\", preprocessor=preprocessor\r\n    )\r\n    num_epochs = 5\r\n\r\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n        filepath=checkpoint_path,\r\n        save_weights_only=True,\r\n        monitor=\"accuracy\",\r\n        # monitor=\"i_loss\",\r\n        mode=\"min\",\r\n        save_best_only=True,\r\n        save_freq=\"epoch\"\r\n    )\r\n    learning_rate = 5e-4\r\n    \r\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n    gpt2_lm.compile(\r\n        optimizer=keras.optimizers.Adam(learning_rate),\r\n        loss=loss,\r\n        weighted_metrics=[\"accuracy\"],\r\n    )\r\n\r\n    gpt2_lm.fit(train_ds, epochs=num_epochs, callbacks=[\r\n        checkpoint_callback,\r\n        tensorboard_callback,\r\n    ],\r\n                )\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-09-21 00:24:25.840848: W tensorflow\/compiler\/tf2xla\/kernels\/random_ops.cc:57] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. gpt2_causal_lm\/gpt2_backbone\/embeddings_dropout\/dropout\/random_uniform\/RandomUniform\r\n2024-09-21 00:24:25.846135: I tensorflow\/compiler\/mlir\/tensorflow\/utils\/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\r\n2024-09-21 00:24:26.559075: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n2024-09-21 00:24:26.572007: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n2024-09-21 00:24:26.573183: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n2024-09-21 00:24:26.581128: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n2024-09-21 00:24:26.581197: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n2024-09-21 00:24:26.588190: W tensorflow\/compiler\/tf2xla\/kernels\/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy\/SparseSoftmaxCrossEntropyWithLogits\/assert_equal_1\/Assert\/Assert\r\n12024-09-21 00:25:01.986918: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n2024-09-21 00:25:02.215951: I tensorflow\/compiler\/jit\/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n2024-09-21 00:25:02.420703: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n2024-09-21 00:25:02.542130: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n2024-09-21 00:25:03.595774: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n12024-09-21 00:25:04.071526: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n2024-09-21 00:25:04.130504: I tensorflow\/compiler\/xla\/stream_executor\/gpu\/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1225'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1111'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1220'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1124'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1175'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1128'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1006'\r\nptxas warning : Registers are spilled to local memory in function 'fusion_1015'\r\n\r\n2024-09-21 00:25:05.333297: I tensorflow\/compiler\/xla\/stream_executor\/cuda\/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\r\n5054\/8663 [================>.............] - ETA: 9:14 - loss: 11.8715 - accuracy: 2.2702terminate called after throwing an instance of 'std::system_error'\r\nterminate called recursively\r\n  what():  Resource temporarily unavailable\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:dist-strat","TF 2.11"],"created_at":"2024-09-20T17:14:03Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76157"},{"repository":"tensorflow\/tensorflow","title":"tf.python.ops.array_ops.transpose aborts with \"Check failed: d >= 0 (0 vs. -1)\"","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### TensorFlow version\r\n\r\ntf-nightly 2.18.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nLinux Ubuntu 20.04.3 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.14\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nI encountered an `aborted issue` in TensorFlow when I used API `array_ops.transpose`\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport numpy as np\r\nfrom tensorflow.python.ops import array_ops\r\n\r\nx =np.arange(0, 8).reshape([2, 4]).astype(np.float32)\r\ny = np.array([-1, 0]).astype(np.int32)\r\narray_ops.transpose(x, y,conjugate = False)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n2024-09-19 16:16:30.137164: F tensorflow\/core\/framework\/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)\r\nAborted (core dumped)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-19T08:31:38Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/76036"},{"repository":"tensorflow\/tensorflow","title":"Code error when feature name has multiple `_`","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nNo\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf 2.17.0\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\n5.15.149-99.162.amzn2.x86_64\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\nPython 3.10.14\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\nExpect there shouldn't be errors just by changing feature name. \r\n\r\n### Standalone code to reproduce the issue\r\nThis is a code sample that will work normally\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Input, Dense, Concatenate\r\nfrom tensorflow.keras import Model\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame()\r\nnumeric_feature_name = 'a' * 27\r\ncategorical_feature_name = 'b' * 11\r\ndf[numeric_feature_name] = range(1000)\r\ndf[categorical_feature_name] = 'a'\r\ndf['label'] = 1\r\n\r\nnumeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')\r\ncategorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=\"string\")\r\nencoding_layer = get_category_encoding_layer(vocab=['a'])\r\nencoded_categorical_feature = encoding_layer(categorical_feature_layer)\r\n\r\nall_inputs = [numeric_feature_layer, categorical_feature_layer]\r\nencoded_features = [numeric_feature_layer, encoded_categorical_feature]\r\nconcat_features = Concatenate()(encoded_features)\r\noutput = Dense(units=1, activation='sigmoid')(concat_features)\r\nmodel = Model(inputs=all_inputs, outputs=output)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\ndataframe_x = df[[numeric_feature_name, categorical_feature_name]]\r\ndataframe_y = df['label']\r\ndf2 = ((dict(dataframe_x), dataframe_y))\r\nds = tf.data.Dataset.from_tensor_slices(df2)\r\nds = ds.batch(32)\r\nds_train = ds\r\n\r\nmodel.fit(\r\n    ds_train,\r\n    epochs=10,\r\n    batch_size=300,\r\n    verbose=1\r\n)\r\n```\r\n\r\nHowever, if I change the feature name, the same code will throw error\r\n\r\n```\r\ndf = pd.DataFrame()\r\n## Just change the feature name here\r\nnumeric_feature_name = 'a_b_c_d_e_f_g' \r\ncategorical_feature_name = 'a_b_c_d_e_f'\r\ndf[numeric_feature_name] = range(1000)\r\ndf[categorical_feature_name] = 'a'\r\ndf['label'] = 1\r\n\r\nnumeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')\r\ncategorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=\"string\")\r\nencoding_layer = get_category_encoding_layer(vocab=['a'])\r\nencoded_categorical_feature = encoding_layer(categorical_feature_layer)\r\n\r\nall_inputs = [numeric_feature_layer, categorical_feature_layer]\r\nencoded_features = [numeric_feature_layer, encoded_categorical_feature]\r\nconcat_features = Concatenate()(encoded_features)\r\noutput = Dense(units=1, activation='sigmoid')(concat_features)\r\nmodel = Model(inputs=all_inputs, outputs=output)\r\n\r\nmodel.compile(optimizer='adam',\r\n              loss='binary_crossentropy',\r\n              metrics=['accuracy'])\r\n\r\ndataframe_x = df[[numeric_feature_name, categorical_feature_name]]\r\ndataframe_y = df['label']\r\ndf2 = ((dict(dataframe_x), dataframe_y))\r\nds = tf.data.Dataset.from_tensor_slices(df2)\r\nds = ds.batch(32)\r\nds_train = ds\r\n\r\nmodel.fit(\r\n    ds_train,\r\n    epochs=10,\r\n    batch_size=300,\r\n    verbose=1\r\n)\r\n```\r\n\r\nWe have tested that this error is on 2.17.0 and if we are using 2.15 tensorflow, both codes will run smoothly.\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nEpoch 1\/10\r\n2024-09-18 05:28:05.240962: W tensorflow\/core\/framework\/op_kernel.cc:1817] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported\r\n---------------------------------------------------------------------------\r\nUnimplementedError                        Traceback (most recent call last)\r\nCell In[14], line 8\r\n      5 ds = ds.batch(32)\r\n      6 ds_train = ds\r\n----> 8 model.fit(\r\n      9     ds_train,\r\n     10     epochs=10,\r\n     11     batch_size=300,\r\n     12     verbose=1\r\n     13 )\r\n\r\nFile ~\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)\r\n    119     filtered_tb = _process_traceback_frames(e.__traceback__)\r\n    120     # To get the full stack trace, call:\r\n    121     # `keras.config.disable_traceback_filtering()`\r\n--> 122     raise e.with_traceback(filtered_tb) from None\r\n    123 finally:\r\n    124     del filtered_tb\r\n\r\nFile ~\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/tensorflow\/python\/eager\/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\r\n     51 try:\r\n     52   ctx.ensure_initialized()\r\n---> 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\r\n     54                                       inputs, attrs, num_outputs)\r\n     55 except core._NotOkStatusException as e:\r\n     56   if name is not None:\r\n\r\nUnimplementedError: Graph execution error:\r\n\r\nDetected at node functional_5_1\/Cast defined at (most recent call last):\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/runpy.py\", line 196, in _run_module_as_main\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/runpy.py\", line 86, in _run_code\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel_launcher.py\", line 18, in <module>\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/traitlets\/config\/application.py\", line 1075, in launch_instance\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/kernelapp.py\", line 739, in start\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/tornado\/platform\/asyncio.py\", line 205, in start\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/asyncio\/base_events.py\", line 603, in run_forever\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/asyncio\/base_events.py\", line 1909, in _run_once\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/asyncio\/events.py\", line 80, in _run\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 545, in dispatch_queue\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 534, in process_one\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 437, in dispatch_shell\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/ipkernel.py\", line 362, in execute_request\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/kernelbase.py\", line 778, in execute_request\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/ipkernel.py\", line 449, in do_execute\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/ipykernel\/zmqshell.py\", line 549, in run_cell\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3075, in run_cell\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3130, in _run_cell\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/async_helpers.py\", line 128, in _pseudo_sync_runner\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3334, in run_cell_async\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3517, in run_ast_nodes\r\n\r\n  File \"\/home\/jinqi_shen\/.local\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3577, in run_code\r\n\r\n  File \"\/tmp\/ipykernel_37779\/4021243845.py\", line 8, in <module>\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 320, in fit\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 121, in one_step_on_iterator\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 108, in one_step_on_data\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/trainer.py\", line 51, in train_step\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/layers\/layer.py\", line 901, in __call__\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 117, in error_handler\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/ops\/operation.py\", line 46, in __call__\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/utils\/traceback_utils.py\", line 156, in error_handler\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\", line 167, in call\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\", line 258, in _standardize_inputs\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/models\/functional.py\", line 218, in _convert_inputs_to_tensors\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/ops\/core.py\", line 822, in convert_to_tensor\r\n\r\n  File \"\/home\/jinqi_shen\/.airconda-environments\/production--payments--tensorflow--ray_tf215--v0.0.1\/lib\/python3.10\/site-packages\/keras\/src\/backend\/tensorflow\/core.py\", line 132, in convert_to_tensor\r\n\r\nCast string to float is not supported\r\n\t [[{{node functional_5_1\/Cast}}]] [Op:__inference_one_step_on_iterator_6125]\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:keras","2.17"],"created_at":"2024-09-18T17:06:56Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75996"},{"repository":"tensorflow\/tensorflow","title":"`resource_create_op` operation can cause TensorFlow to crash.","description":"### Issue type\r\n\r\nBug\r\n\r\n### Have you reproduced the bug with TensorFlow Nightly?\r\n\r\nYes\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### TensorFlow version\r\n\r\ntf-nightly 2.18.0.dev20240817\r\n\r\n### Custom code\r\n\r\nYes\r\n\r\n### OS platform and distribution\r\n\r\nUbuntu 20.04.3 LTS (x86_64)\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC\/compiler version\r\n\r\n_No response_\r\n\r\n### CUDA\/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current behavior?\r\n\r\n A `Segmentation fault`  could be raised in TensorFlow when using `test_ops.resource_create_op` . The code is as follows:\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import test_ops\r\nfrom tensorflow.python.ops import array_ops\r\nfrom tensorflow.python.ops import array_ops_stack\r\n\r\n\r\nsess = tf.compat.v1.Session()\r\n\r\n@tf.function\r\ndef func():\r\n    r1 = test_ops.stub_resource_handle_op(container='a', shared_name='b')\r\n    r2 = test_ops.stub_resource_handle_op(container='a', shared_name='c')\r\n    c = array_ops_stack.stack([r1, r2])\r\n    s = array_ops.strided_slice(c, [1], [2 ** 32])\r\n    with sess.as_default():\r\n        test_ops.resource_create_op(s)\r\n\r\nfunc()\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n> Segmentation fault (core dumped)\r\n\r\nThe above code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)\r\n```\r\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-16T01:45:51Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75825"},{"repository":"tensorflow\/tensorflow","title":"Encountering a `Segmentation fault` when using `data_flow_ops.FIFOQueue` in TensorFlow","description":"### Issue type\n\nBug\n\n### Have you reproduced the bug with TensorFlow Nightly?\n\nYes\n\n### Source\n\nsource\n\n### TensorFlow version\n\ntf-nightly 2.18.0.dev20240817\n\n### Custom code\n\nYes\n\n### OS platform and distribution\n\nUbuntu 20.04.3 LTS (x86_64)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC\/compiler version\n\n_No response_\n\n### CUDA\/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current behavior?\n\n A `Segmentation fault`  could be raised in TensorFlow when using `data_flow_ops.FIFOQueue` . The following code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)\n\n### Standalone code to reproduce the issue\n\n```shell\nfrom tensorflow.python.framework import dtypes as dtypes_lib\r\nfrom tensorflow.python.ops import data_flow_ops\r\nimport tensorflow as tf\r\n\r\nq = data_flow_ops.FIFOQueue(10, dtypes_lib.float32, ())\r\nelems = [10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\r\ntmp_var64 = elems[4:8]\r\n\r\nsess = tf.compat.v1.Session()\r\nwith sess.as_default():\r\n    q.dequeue_up_to([])\n```\n\n\n### Relevant log output\n\n```shell\n> Segmentation fault (core dumped)\n```\n","labels":["stat:awaiting tensorflower","type:bug","comp:ops","2.17"],"created_at":"2024-09-15T04:17:25Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/tensorflow\/tensorflow\/issues\/75814"}]