{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZVO91_qZfdb"
      },
      "source": [
        "# üöÄ Hands-On Lecture: Exploring the GitHub API in Python with Google Colab üêç\n",
        "\n",
        "Welcome to the Hands-on lecture on GitHub API magic!  With the power of Python, we‚Äôll unlock new skills to interact with GitHub repositories like true software engineering pros.\n",
        "\n",
        "**What‚Äôs on the Map?**\n",
        "\n",
        "Here‚Äôs what we‚Äôll uncover in this session:\n",
        "\n",
        "  üõ†Ô∏è Building API Superpowers: Dive into exciting use cases, including:\n",
        "\n",
        "*  Fetching and analyzing issues and comments.\n",
        "*  Accessing code and repositories programmatically.\n",
        "*  Exploring advanced operations to automate your workflows and many more.\n",
        "\n",
        "\n",
        "üí° **Why This Matters**\n",
        "\n",
        "Imagine automating tedious tasks, analyzing repository data like a detective, or building tools that integrate directly with GitHub. The GitHub API opens up limitless possibilities for innovation in software engineering. By the end of this session, you‚Äôll have the tools to transform your ideas into powerful automations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF5bbIfn48PL"
      },
      "source": [
        "## Let's retrieve some trending projects from GitHub\n",
        "\n",
        "GitHub does not allow to collect trending projects through GitHub API. So, we will do web scrapping. Let's see how it is done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Eas26Ir46fD",
        "outputId": "d97af178-cff0-4e6f-9f00-5d7b93728b78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available time periods: 'daily', 'weekly', 'monthly'\n",
            "https://github.com/trending/python?since=daily&spoken_language_code=en\n",
            "Top 5 Trending Python Projects on GitHub (Daily, Spoken Language: English):\n",
            "\n",
            "Name: benbusby/whoogle-search\n",
            "Description: A self-hosted, ad-free, privacy-respecting metasearch engine\n",
            "Stars: 10,276\n",
            "Repo URL: https://github.com/benbusby/whoogle-search\n",
            "--------------------------------------------------\n",
            "Name: OpenBB-finance/OpenBB\n",
            "Description: Investment Research for Everyone, Everywhere.\n",
            "Stars: 35,438\n",
            "Repo URL: https://github.com/OpenBB-finance/OpenBB\n",
            "--------------------------------------------------\n",
            "Name: AUTOMATIC1111/stable-diffusion-webui\n",
            "Description: Stable Diffusion web UI\n",
            "Stars: 146,149\n",
            "Repo URL: https://github.com/AUTOMATIC1111/stable-diffusion-webui\n",
            "--------------------------------------------------\n",
            "Name: make-all/tuya-local\n",
            "Description: Local support for Tuya devices in Home Assistant\n",
            "Stars: 1,583\n",
            "Repo URL: https://github.com/make-all/tuya-local\n",
            "--------------------------------------------------\n",
            "Name: ArchipelagoMW/Archipelago\n",
            "Description: Archipelago Multi-Game Randomizer and Server\n",
            "Stars: 605\n",
            "Repo URL: https://github.com/ArchipelagoMW/Archipelago\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def fetch_trending_python_projects(time_period=\"daily\", spoken_language=\"en\", limit=5):\n",
        "    # Validate the time period\n",
        "    if time_period not in [\"daily\", \"weekly\", \"monthly\"]:\n",
        "        print(\"Invalid time period. Please choose from 'daily', 'weekly', or 'monthly'.\")\n",
        "        return\n",
        "\n",
        "    # Construct the URL with time period and spoken language\n",
        "    url = f\"https://github.com/trending/python?since={time_period}&spoken_language_code={spoken_language}\"\n",
        "    print(url)\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse the HTML content\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Find repository entries\n",
        "        projects = soup.find_all(\"article\", class_=\"Box-row\")\n",
        "\n",
        "        print(\n",
        "            f\"Top {limit} Trending Python Projects on GitHub ({time_period.capitalize()}, Spoken Language: English):\\n\"\n",
        "        )\n",
        "        for i, project in enumerate(projects[:limit]):  # Limit to the top 'limit' projects\n",
        "            # Extract repository name\n",
        "            repo_name_tag = project.find(\"h2\", class_=\"h3 lh-condensed\").find(\"a\")\n",
        "            repo_name = repo_name_tag.text.strip().replace(\"\\n\", \"\").replace(\" \", \"\")\n",
        "\n",
        "            # Extract repository URL\n",
        "            repo_url = f\"https://github.com{repo_name_tag['href']}\"\n",
        "\n",
        "            # Extract description\n",
        "            description_tag = project.find(\"p\", class_=\"col-9 color-fg-muted my-1 pr-4\")\n",
        "            description = description_tag.text.strip() if description_tag else \"No description provided\"\n",
        "\n",
        "            # Extract stars\n",
        "            stars_tag = project.find(\"a\", href=lambda x: x and x.endswith(\"/stargazers\"))\n",
        "            stars = stars_tag.text.strip() if stars_tag else \"0\"\n",
        "\n",
        "            print(f\"Name: {repo_name}\")\n",
        "            print(f\"Description: {description}\")\n",
        "            print(f\"Stars: {stars}\")\n",
        "            print(f\"Repo URL: {repo_url}\")\n",
        "            print(\"-\" * 50)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Available time periods: 'daily', 'weekly', 'monthly'\")\n",
        "    time_period = \"daily\"\n",
        "    fetch_trending_python_projects(time_period=time_period, spoken_language=\"en\", limit=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVXGsopnbQDB"
      },
      "source": [
        "## üîë Creating a GitHub Classic API Token\n",
        "\n",
        "To interact with GitHub's API, you need a **Personal Access Token (PAT)**, which acts as a secure key for authentication. Here's how you can generate one:\n",
        "\n",
        "### ‚ú® Step-by-Step Guide:  \n",
        "\n",
        "1. **Login to GitHub**: Start by logging into your GitHub account.  \n",
        "2. **Navigate to Settings**:  \n",
        "   - Click on your profile picture in the top-right corner.  \n",
        "   - Select **Settings** from the dropdown menu.  \n",
        "\n",
        "3. **Go to Developer Settings**:  \n",
        "   - Scroll to the bottom of the left-hand menu in the **Settings** page.  \n",
        "   - Click on **Developer Settings**.  \n",
        "\n",
        "4. **Access Personal Access Tokens**:  \n",
        "   - In the **Developer Settings** menu, find the **Personal Access Tokens** section.  \n",
        "   - Select **Token (classic)** from the dropdown.  \n",
        "\n",
        "5. **Generate New Token**:  \n",
        "   - On the top-right corner, click **Generate new token**.  \n",
        "   - From the dropdown, select **Generate new token (classic)**.  \n",
        "\n",
        "6. **Fill Out the Token Form**:  \n",
        "   - **Note**: Add a descriptive name for the token (e.g., *CMPT470 API Token*) to remember why it was created.  \n",
        "   - **Expiration**: Choose a suitable expiration period (e.g., 7 days, 30 days, or custom).  \n",
        "   - **Scopes**: Select the permissions the token will have. For this lecture, I chose following scopes:  \n",
        "     - `repo`  \n",
        "     - `workflow`  \n",
        "     - `user`  \n",
        "     - `audit_log`  \n",
        "     - `project`  \n",
        "\n",
        "7. **Generate and Save Your Token**:  \n",
        "   - Scroll to the bottom and click **Generate Token**.  \n",
        "   - Once generated, **copy the token immediately**. GitHub will not show it again, and you'll need to create a new token it if lost.\n",
        "\n",
        "---\n",
        "\n",
        "Now that you have your GitHub API token, you‚Äôre ready to connect to GitHub programmatically!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40vSNJJTzi6p"
      },
      "source": [
        "## Technique 1: Let's fetch issues from GitHub API and save them in CSV or JSON file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIItnlNXzFrO",
        "outputId": "aaced6b5-a28e-41af-b46f-e3e697a89cca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing issues repository: psf/black\n",
            "Finished issues processing: psf/black\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import requests\n",
        "import json\n",
        "\n",
        "REPOSITORIES = [\"psf/black\"]  # targeted repo\n",
        "TOKEN = \"\"  # GitHub Personal Access Token\n",
        "HEADERS = {\"Authorization\": f\"token {TOKEN}\"}  # header for the request\n",
        "# additional params to access specific types of data\n",
        "PARAMS = {\"state\": \"closed\", \"since\": \"2022-01-01T00:00:00Z\", \"sort\": \"updated\"}\n",
        "\n",
        "\n",
        "def fetch_issues(repo, max_pages=2):\n",
        "    csv_file = f'{repo.replace(\"/\", \"-\")}-issues.csv'\n",
        "    json_file = f'{repo.replace(\"/\", \"-\")}-issues.json'\n",
        "    issues_data = []\n",
        "    page_count = 0\n",
        "    url = f\"https://api.github.com/repos/{repo}/issues\"\n",
        "\n",
        "    with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"ID\", \"State\", \"Title\", \"Body\", \"Labels\", \"Created At\", \"Closed At\"])\n",
        "\n",
        "        while url and page_count < max_pages:\n",
        "            response = requests.get(url, params=PARAMS, headers=HEADERS)\n",
        "            if response.status_code != 200:\n",
        "                raise Exception(f\"Error {response.status_code}: {response.json()}\")\n",
        "\n",
        "            # looping through each issue\n",
        "            for issue in response.json():\n",
        "                if \"pull_request\" not in issue:  # Skip pull requests\n",
        "                    labels = \",\".join(label[\"name\"] for label in issue.get(\"labels\", []))\n",
        "                    body = issue.get(\"body\", \"\")[:10000]\n",
        "                    row = [\n",
        "                        issue[\"number\"],\n",
        "                        issue[\"state\"],\n",
        "                        issue[\"title\"],\n",
        "                        body,\n",
        "                        labels,\n",
        "                        issue[\"created_at\"],\n",
        "                        issue[\"closed_at\"],\n",
        "                    ]\n",
        "                    # saving to csv\n",
        "                    writer.writerow(row)\n",
        "                    issues_data.append(\n",
        "                        {\n",
        "                            \"ID\": issue[\"number\"],\n",
        "                            \"State\": issue[\"state\"],\n",
        "                            \"Title\": issue[\"title\"],\n",
        "                            \"Body\": body,\n",
        "                            \"Labels\": labels,\n",
        "                            \"Created At\": issue[\"created_at\"],\n",
        "                            \"Closed At\": issue[\"closed_at\"],\n",
        "                        }\n",
        "                    )\n",
        "            # fetching all the issues links so that we can go one by one\n",
        "            links = {\n",
        "                rel.split(\"=\")[1]: url.strip(\"<>\")\n",
        "                for link in response.headers.get(\"Link\", \"\").split(\",\")\n",
        "                for url, rel in [link.split(\";\")]\n",
        "            }\n",
        "            url = links.get(\"next\")  # getting the next url to fetch\n",
        "            page_count += 1\n",
        "\n",
        "    # saving it to json\n",
        "    with open(json_file, \"w\", encoding=\"utf-8\") as jf:\n",
        "        json.dump(issues_data, jf, indent=4)\n",
        "\n",
        "\n",
        "for repository in REPOSITORIES:\n",
        "    print(f\"Processing issues repository: {repository}\")\n",
        "    fetch_issues(repository)\n",
        "    print(f\"Finished issues processing: {repository}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Technique 2: Let's fetch issues from GitHub API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetched 100 issues from tensorflow/tensorflow\n",
            "Issues data saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# import requests\n",
        "# import json\n",
        "\n",
        "# # GitHub API endpoint for fetching issues from a public repository\n",
        "# repo_owner = \"tensorflow\"  # Change to any repo you want\n",
        "# repo_name = \"tensorflow\"\n",
        "# api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/issues\"\n",
        "\n",
        "# # Define parameters to filter issues\n",
        "# TOKEN = \"\"  # GitHub Personal Access Token\n",
        "# params = {\"state\": \"open\", \"labels\": \"type:bug\", \"per_page\": 5}  # Number of issues to fetch\n",
        "\n",
        "# # Add headers (GitHub API requires a user-agent)\n",
        "# headers = {\"Accept\": \"application/vnd.github.v3+json\", \"Authorization\": f\"token {TOKEN}\"}  # add toke\n",
        "\n",
        "# # Send request to GitHub API\n",
        "# response = requests.get(api_url, headers=headers, params=params)\n",
        "\n",
        "# # Process the response\n",
        "# if response.status_code == 200:\n",
        "#     issues = response.json()\n",
        "#     print(f\"Fetched {len(issues)} issues from {repo_owner}/{repo_name}\")\n",
        "\n",
        "#     # Extract relevant fields\n",
        "#     for issue in issues:\n",
        "#         print(f\"Issue ID: {issue.get('id')}\")\n",
        "#         print(f\"Title: {issue.get('title')}\")\n",
        "#         print(f\"Description: {issue.get('body', 'No description provided')}\")\n",
        "#         print(f\"Labels: {[label['name'] for label in issue.get('labels', [])]}\")\n",
        "#         print(f\"Created At: {issue.get('created_at')}\")\n",
        "#         print(f\"Comments: {issue.get('comments')}\")\n",
        "#         print(\"=\" * 80)\n",
        "\n",
        "# else:\n",
        "#     print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# GitHub API Configuration\n",
        "TOKEN = \"\"  # Add your GitHub Personal Access Token here (if needed)\n",
        "headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n",
        "if TOKEN:\n",
        "    headers[\"Authorization\"] = f\"token {TOKEN}\"\n",
        "\n",
        "# Define repository to fetch issues from (TensorFlow)\n",
        "repos = [\"tensorflow/tensorflow\"]\n",
        "\n",
        "# Define parameters to filter issues (fetch only open issues with \"type:bug\" label)\n",
        "params = {\"state\": \"open\", \"labels\": \"type:bug\", \"per_page\": 493}\n",
        "\n",
        "# List to store issues data\n",
        "issues_list = []\n",
        "\n",
        "# Fetch issues\n",
        "for repo in repos:\n",
        "    api_url = f\"https://api.github.com/repos/{repo}/issues\"\n",
        "    response = requests.get(api_url, headers=headers, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        issues = response.json()\n",
        "        print(f\"Fetched {len(issues)} issues from {repo}\")\n",
        "\n",
        "        # Extract relevant fields\n",
        "        for issue in issues:\n",
        "            issues_list.append({\n",
        "                \"repository\": repo,\n",
        "                \"title\": issue.get(\"title\"),\n",
        "                \"description\": issue.get(\"body\", \"No description provided\"),\n",
        "                \"labels\": [label[\"name\"] for label in issue.get(\"labels\", [])],\n",
        "                \"created_at\": issue.get(\"created_at\"),\n",
        "                \"comments\": issue.get(\"comments\"),\n",
        "                \"reactions\": issue.get(\"reactions\", {}).get(\"total_count\", 0),\n",
        "                \"url\": issue.get(\"html_url\")\n",
        "            })\n",
        "    else:\n",
        "        print(f\"Failed to fetch data from {repo}: {response.status_code}, {response.text}\")\n",
        "\n",
        "# Save the data to CSV and JSON\n",
        "df = pd.DataFrame(issues_list)\n",
        "df.to_csv(\"tensorflow_issues.csv\", index=False)\n",
        "df.to_json(\"tensorflow_issues.json\", orient=\"records\")\n",
        "\n",
        "print(\"Issues data saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEbqYqBRmLCx"
      },
      "source": [
        "## Resources used for this lecture:\n",
        "\n",
        "1.   https://docs.github.com/en/rest?apiVersion=2022-11-28\n",
        "2.   https://github.com/psf/black\n",
        "3.   https://github.com/vercel/vercel\n",
        "4.   https://github.com/trending/\n",
        "5.   https://medium.com/analytics-vidhya/getting-started-with-github-api-dc7057e2834d\n",
        "6.   https://seart-ghs.si.usi.ch/\n",
        "https://blog.apify.com/python-github-api/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3GQYd-_3lcRw"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
