input_text,actual_label,predicted_label
"textarea does not show warning when switching from uncontrolled to controlled like inputs do <!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

Bug

**What is the current behavior?**

While things like `<input>` correctly get a warning when switching from uncrontrolled to controlled, I'm noticing `<textarea>` does not

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

Here's a codesandbox. Type in the input field, we see error (correct), change to textarea and start over, type in field and we don't see the error (incorrect I think) https://codesandbox.io/s/recursing-dawn-jls8i

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

16.8
",Logical Bug,Logical Bug
"Program files /app/out/ folder being deleted nightly. 
Type: <b>Bug</b>

C:\Program Files\Microsoft VS Code\resources\app\out directory being deleted nightly for about 2 weeks now. After this happens, open VS Code windows are partially functional but new windows will not open, triggering this error instead.

A JavaScript error occurred in the main process
Uncaught Exception:
Error [ERR_MODULE_NOT_FOUND]: Cannot find module 'C:\Program Files\Microsoft VS
Code\resources\app\out\main.js' imported from C:\WINDOWS\system32\
at finalizeResolution (node:internal/modules/esm/resolve:265:11)
at moduleResolve (node:internal/modules/esm/resolve:940:10)
continued...

To fix it, I end all vscode processes in windows task manager, then re-run installer. I've noticed when closing the processes, there are a few vscode install/uninstall processes running when stuck in this state, so I'm not sure if this is due to some self-update going wrong.

Not sure if this is being caused by a VS code bug or my company's antivirus software. Any help would be appreciated.

VS Code version: Code 1.95.0 (912bb683695358a54ae0c670461738984cbb5b95, 2024-10-28T20:16:24.561Z)
OS version: Windows_NT x64 10.0.26100
Modes:
Remote OS version: Linux x64 6.8.0-48-generic

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i7-1185G7 @ 3.00GHz (8 x 2995)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.74GB (1.81GB free)|
|Process Argv|--crash-reporter-id 81949ced-f38a-4b7b-a6fd-0494dce958d4|
|Screen Reader|no|
|VM|0%|

|Item|Value|
|---|---|
|Remote|SSH: netbox-dev.jsinoc.com|
|OS|Linux x64 6.8.0-48-generic|
|CPUs|Intel(R) Xeon(R) Gold 6208U CPU @ 2.90GHz (4 x 0)|
|Memory (System)|7.67GB (2.34GB free)|
|VM|0%|
</details><details><summary>Extensions (14)</summary>

Extension|Author (truncated)|Version
---|---|---
codespaces|Git|1.17.3
cisco|jam|1.9.1
remote-containers|ms-|0.388.0
remote-ssh|ms-|0.115.0
remote-ssh-edit|ms-|0.87.0
remote-wsl|ms-|0.88.5
remote-explorer|ms-|0.4.3
iosxr|phi|1.0.4
pdf|tom|1.2.2
vscode-docker|ms-|1.29.3
debugpy|ms-|2024.12.0
python|ms-|2024.18.1
vscode-pylance|ms-|2024.11.1
save-as-root|yy0|1.8.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
pythongtdpath:30769146
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
945dj816:31013170
dvdeprecation:31068756
dwnewjupytercf:31046870
newcmakeconfigv2:31071590
impr_priority:31102340
nativerepl2:31139839
refactort:31108082
pythonrstrctxt:31112756
cf971741:31144450
iacca1:31171482
notype1:31157159
5fd0e150:31155592
dwcopilot:31170013
j44ff735:31179530

```

</details>

<!-- generated by issue reporter -->",Security Vulnerability,Security Vulnerability
"`Segmentation Fault` in `tf.raw_ops.TensorListScatter` and `tf.raw_ops.TensorListScatterV2` when the value of `indices` is too large. ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value of `indices` is too large,  `tf.raw_ops.TensorListScatter` and `tf.raw_ops.TensorListScatterV2` triggers `Segmentation Fault` due to RAM being full.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1BOVVbofLSB2x_chg_WuFIdTWZXS9hiw0?usp=sharing
```


### Relevant log output

```shell
segmentation fault (core dumped)
```
",Runtime Error,Runtime Error
"Tensorflow 2.15 Docker image cannot find the GPU drivers, but nvidia-smi can. ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

TF 2.15.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.2

### GPU model and memory

NVIDIA TITAN V

### Current behavior?

Running TensorFlow 2.15.0 from within Docker image does not find GPU drivers.

nvidia-smi reports the GPUs as available.

This works as intended with a TF 2.14 image in the same machine.


### Standalone code to reproduce the issue

```shell
docker run --gpus all -it tensorflow/tensorflow:2.15.0-gpu-jupyter bash

<within the container>

# nvidia-smi

# python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```


### Relevant log output

```shell
# nvidia-smi

Thu Nov 16 16:47:22 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA TITAN V                 Off | 00000000:65:00.0 Off |                  N/A |
| 29%   46C    P8              27W / 250W |   1693MiB / 12288MiB |      2%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

# python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""

2023-11-16 16:46:54.131081: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2023-11-16 16:46:54.255566: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-11-16 16:46:54.255623: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-11-16 16:46:54.276648: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-11-16 16:46:54.327586: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2023-11-16 16:46:54.328299: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-16 16:46:56.486851: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]
```
",Dependency Issue,Dependency Issue
"Linux: writing elevated fails with high `ulimit` values <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.96.2 (code_1.96.2-1734607745_amd64.deb)
- OS Version: Ubuntu 24.10

Steps to Reproduce:

1. Open a file that is not writable by the user
2. Edit the file
3. Try to save it
4. Error pops up stating that not enough permissions
5. Retry as Sudo option appears
6. Enter password
7. Saving process running forever

Same problem has been discussed here: https://github.com/microsoft/vscode/issues/234311. 
Is there any progress or plan? Or any workaround ?",Security Vulnerability,Security Vulnerability
"[DevTools Bug]: Profiling not supported error with up-to-date React. ### Website or app

https://vercel.com/login

### Repro steps

When I open React Developer Tools profiler page, I get the following error:

```
Profiling not supported.
Profiling support requires either a development or profiling build of React v16.5+.

Learn more at [reactjs.org/link/profiling](https://fb.me/react-devtools-profiling).
```

I first saw the problem on my own app, which is running React 18 and NextJS 15, but was able to reproduce the problem on every other NextJS react page I could find.

I am running Google Chrome 131.0.6778.204 on Fedora 40. My version of React Developer Tools is 6.0.1 (10/15/2024). 

If I test the same conditions, but on Firefox, profiling works, suggesting it is a problem with React Dev Tools.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Syntax Error,Syntax Error
"A check fail can be triggered in tf.raw_ops.ResourceSparseApplyKerasMomentum ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the latest version of TensorFlow, the following code can trigger a crash in `tf.raw_ops.ResourceSparseApplyKerasMomentum` due to check-fail.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.complex64)  
accum = tf.Variable([[0.1, 0.2], [0.3, 0.4]])
lr = 0.01
grad = tf.constant([[0.1, 0.2], [0.3, 0.4]])
indices = tf.constant([0, 1])  
momentum = 0.9

result = tf.raw_ops.ResourceSparseApplyKerasMomentum(var=var.handle, accum=accum.handle, lr=lr, grad=grad, indices=indices, momentum=momentum)

print(result)
```


### Relevant log output

```shell
2024-03-14 15:57:08.452454: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
",Runtime Error,Runtime Error
"[DevTools Bug]: Source map error: URL: react_devtools_backend_compact.js.map ### Website or app

https://github.com/avenida714/alec-synth/blob/dccf984d89ae44558c70ff93dfa03b1227d5df5b/src/App.jsx#L63-L64C17

Link found on the vite getting started page: https://stackblitz.com/edit/vitejs-vite-zff6zx?file=src%2FApp.jsx&terminal=dev

### Repro steps

â ï¸ This is not my code (not public) â ï¸. While I searched for this error I stumble upon this repo. I had this issue when adding the plugin for first time.

1. Open the website
2. Open console
3. See the error

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Dependency Issue,Dependency Issue
"BUG: StandardScaler partial_fit overflows The recent implementation of `partial_fit` for `StandardScaler` can overflow. A use case there is to transform indefinitely long stream of data, but that is problematic with the current implementation. The reason is that to compute the running mean, [we keep track](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py#L788) of the sample sum.

Here the code to reproduce the behavior. To simulate long stream of data would take long time; instead, I use samples with very large norm but the effect is the same. The same batch is presented to the transformer many times. The mean should be same.

```python
from sklearn.preprocessing import StandardScaler
import numpy as np

rng = np.random.RandomState(0)

def gen_1d_uniform_batch(min_, max_, n):
    return rng.uniform(min_, max_, size=(n, 1))

max_f = np.finfo(np.float64).max / 1e5
min_f = max_f / 1e2
stream_dim = 100
batch_dim = 500000
print(""mean overflow: batch vs online on %d repetitions"" % stream_dim)

X = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)

scaler = StandardScaler(with_std=False).fit(X)
print(scaler.mean_)
[  1.79769313e+301]

iscaler = StandardScaler(with_std=False)
batch = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)
for _ in range(stream_dim):
    iscaler = iscaler.partial_fit(batch)
RuntimeWarning: overflow encountered in add
  updated_mean = (last_sum + new_sum) / updated_sample_count

print(iscaler.mean_)
[ inf]
```
",Security Vulnerability,Security Vulnerability
"TreeError [Chat] Tree element not found: [object Object] ```javascript
Error: TreeError [Chat] Tree element not found: [object Object]
at Ose.o in src/vs/base/browser/ui/tree/objectTreeModel.ts:313:10
at Ose.expandTo in src/vs/base/browser/ui/tree/objectTreeModel.ts:261:25
at jk.reveal in out-vscode/vs/base/browser/ui/tree/vs/base/browser/ui/tree/abstractTree.ts:3037:14
at PD.sb in out-vscode/vs/workbench/contrib/chat/browser/vs/workbench/contrib/chat/browser/chatWidget.ts:455:14
at PD.layout in out-vscode/vs/workbench/contrib/chat/browser/vs/workbench/contrib/chat/browser/chatWidget.ts:1140:9
at Ije.x in out-vscode/vs/workbench/contrib/inlineChat/browser/vs/workbench/contrib/inlineChat/browser/inlineChatWidget.ts:338:20
at Ije.x in out-vscode/vs/workbench/contrib/inlineChat/browser/vs/workbench/contrib/inlineChat/browser/inlineChatWidget.ts:553:9
at Ije.layout in out-vscode/vs/workbench/contrib/inlineChat/browser/vs/workbench/contrib/inlineChat/browser/inlineChatWidget.ts:319:9
at Wje.E in out-vscode/vs/workbench/contrib/inlineChat/browser/vs/workbench/contrib/inlineChat/browser/inlineChatZoneWidget.ts:161:15
at Wje.w in src/vs/editor/contrib/zoneWidget/browser/zoneWidget.ts:298:9
```
[Go to Errors Site](https://errors.code.visualstudio.com/card?ch=90868576241dd25c6c5da64adadc0a09de91a9fe&bH=f99e8fe7-7324-faa4-723c-d10b0e8be5e4)",Runtime Error,Runtime Error
"Devtools v4 does not work with Firefox's private window <!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

* bug
* This issue has been reported in https://github.com/facebook/react-devtools/issues/1383

**What is the current behavior?**


Steps to Reproduce is here:

1. Environments are:
2. Open the page which uses react with a private window.
3. Open Firefox's devtools.

Actual Result is:

* react devtools' _component_ pane show `Unable to find React on the page.`
* From about:debugging, we can see the below messsage:

```
SecurityError: Permission denied to access property ""container"" on cross-origin object main.js:51:305877
    Kl moz-extension://56db142d-3d36-b04e-91ca-a7504c7708a5/build/main.js:51
    apply self-hosted:4417
    applySafeWithoutClone resource://gre/modules/ExtensionCommon.jsm:588
    asyncWithoutClone resource://gre/modules/ExtensionCommon.jsm:2400
```



**What is the expected behavior?**

react devtools work

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

* Firefox 68
* react devtools v4.0.5
* react v16.9",Security Vulnerability,Security Vulnerability
"Unicode highlight: No explanations nor setting-adjustment shortcuts anymore when hovering highlighted characters <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version:
Version: 1.94.1
Commit: e10f2369d0d9614a452462f2e01cdc4aa9486296
Date: 2024-10-05T05:44:32.189Z
Electron: 30.5.1
ElectronBuildId: 10262041
Chromium: 124.0.6367.243
Node.js: 20.16.0
V8: 12.4.254.20-electron.0

- OS Version:
OS: Linux x64 5.15.0-122-generic
Xubuntu 20

Steps to Reproduce:

1. Type characters subjected to the Unicode Highlight rules.
2. Check that they are indeed highlighted (check or reset your relevant settings otherwise).
3. Hover over the highlighted characters with the mouse cursor.

**Expected result:** The explanations that there used to be there (I think?), giving the U+xxxx code of the character and what it can be mistaken with (if ambiguous) and Adjust settings options, etc.

**Actual result:** Nothing appears on hover, and no code actions are available.

Tried completely resetting my `settings.json` and disabling all extensions, but still nothing.
Tried in multiple filetypes (AsciiDoc, Shell script).

Maybe my memories about how this worked in the past are slightly mistaken, but still: in many old issues regarding Unicode highlight, I see people mentioning the possibility of hovering over them to get info and to adjust settings.

Other, unrelated hover-based features still seem OK.

Heres a GIF in which I type and try to inspect a capital ?, a capital ?, and U+00a0?/?U+202f no-break spaces:

![Image](https://github.com/user-attachments/assets/550d9019-36f8-4b0f-a8ea-fefaa8847b77)",UI/UX Bug,UI/UX Bug
"[DevTools Bug]: Component flashing many times but profiler says it was rendered once ### Website or app

https://berrotools.vercel.app/signin

### Repro steps

- Access the website https://berrotools.vercel.app/signin, you'll see a sign in form.
- Open the DevTools, go to the Profiler tab and start profiling.
- Focus the e-mail input and start typing any letter, should not be a valid e-mail so the error label will show. You'll see the error message flashing as long as you type.
- Stop the profiling.
- Check the profiling results, you'll see that the error message was rendered once, if you have typed random letters. The problem is, why did it flash many times?

If the website doesn't work try cloning and running the project located at https://github.com/rhberro/berrotools.

Thank you in advance!

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

6.0.1-c7c68ef842

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Syntax Error,Syntax Error
"getEventKey implementation inconsistent with DOM3 spec / Firefox implementation There are some inconsistencies between `getEventKey` and the DOM3 keyboard event spec (as well as what Firefox has implemented):
- `key` property is not correctly set for printable characters on `keydown` and `keyup` events. This works properly in Firefox, and my interpretation of the standard is that Firefox's behaviour is correct here. From the spec: _If the key generates a printable character, and there exists an appropriate Unicode code point, then the KeyboardEvent.key attribute must be a string consisting of the char value of that character._
- Enter key only fires keydown in Firefox, but fires both keydown and keypress in Chrome. This should be consistent across browsers
- CapsLock key only fires keydown when it is toggled on. When caps lock is toggled from on to off, no keydown event is fired (this may be a browser limitation in Chrome)

Repro: Test this page in Firefox and compare the result to Chrome: http://jsfiddle.net/63ycmLhe/1/
",UI/UX Bug,UI/UX Bug
"deleted files don't go to trash 
Type: <b>Bug</b>

I delete files in the explorer using `del` key of keyboard, files are deleted and cannot be found in the trash bin

VS Code version: Code 1.95.2 (e8653663e8840adaf45af01eab5c627a5af81807, 2024-11-07T11:07:22.054Z)
OS version: Linux x64 6.8.0-48-generic snap
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-1365U (12 x 3700)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 1|
|Memory (System)|31.00GB (22.61GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 8447ec3e-1827-4960-bb59-12316efae9e7|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|wayland|
</details><details><summary>Extensions (14)</summary>

Extension|Author (truncated)|Version
---|---|---
gitlens|eam|15.6.3
ftp-simple|hum|0.7.6
git-graph|mhu|1.30.0
autopep8|ms-|2024.0.0
debugpy|ms-|2024.12.0
python|ms-|2024.18.1
vscode-pylance|ms-|2024.11.1
jupyter|ms-|2024.10.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.21
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.388.0
LiveServer|rit|5.7.9


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
pythongtdpath:30769146
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
bdiig495:31013172
dvdeprecation:31068756
dwnewjupyter:31046869
impr_priority:31102340
nativerepl2:31139839
refactort:31108082
pythonrstrctxt:31112756
cf971741:31144450
iacca1:31171482
notype1cf:31157160
5fd0e150:31155592
dwcopilot:31170013
j44ff735:31177056

```

</details>

<!-- generated by issue reporter -->",Security Vulnerability,Security Vulnerability
" Test_on_batch() gives the same loss output on different batches in a single run ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes


### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

windows 10

### Python version

3.10.12

### Current behavior?

I noticed the problem when I got a straight horizontal line on plotting the test results on the trained network. I used the sequential models.

I use train_on_batch(), which gives converging losses. When I switch to test_on_batch(), the losses remain the same for different batches. When I restart the test with different test files, it will give a different loss value, which remains the same for all the batches. In other words, the loss from test_on_batch() remains the some for all batches in a single run.

It's a sequential model.

Here is the code of the section:

    print('mfccs3 value = ', tf.keras.backend.eval(mfccs3[1,:]) )               
    #logs = vadModel.train_on_batch(mfccs3, vadLabel)
    logs = vadModel.test_on_batch(mfccs3, vadLabel) 
    print('string logs = ', str(logs))
The result is:
index = 1

```
mfccs3 value = [[-8.2793800e+01 -5.9538417e+00 9.8302096e-01 -3.5255635e-01
3.0392697e-01 -6.4597696e-01 2.2358397e-02 2.5344249e-02
-6.8171650e-01 -3.7053981e-01 -3.4044239e-01 -8.1056818e-02]]
```

string logs = 0.2398043

index = 2

```
mfccs3 value = [[-69.159195 -2.2269542 4.2501264 -1.3486748 0.62957734
-3.2606528 -3.253118 -3.5308673 -1.1313365 -1.1839466
-2.330786 -1.6313086 ]]
```

string logs = 0.2398043

index = 3

```
mfccs3 value = [[-64.894104 -1.892648 0.11392474 -0.81098145 -1.4640433
-1.1901256 -1.7744782 -0.85753983 -0.9694403 -0.8149232
-1.0680746 -1.0442001 ]]
```

string logs = 0.2398043

You can see that the inputs for test_on_batch() have changed. However, the loss remains the same. I use the same code for train_on_batch(), which gives converging losses.



### Standalone code to reproduce the issue

```shell
logs = vadModel.train_on_batch(mfccs3, vadLabel)
"""""" vs.  """"""
    logs = vadModel.test_on_batch(mfccs3, vadLabel) 

it's just these two lines for a sequential model.
```


### Relevant log output

```shell
I even tried latest Tensorflow version. It has the same problem.

tensorflow version: 2.15.0-dev20230817
listOfFiles 1681
2023-08-17 15:42:12.560549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
model length =  7
Model: ""sequential""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 multiple                  2640      
                                                                 
 dense (Dense)               multiple                  210       
                                                                 
 dense_1 (Dense)             multiple                  11        
                                                                 
=================================================================
Total params: 2861 (11.18 KB)
Trainable params: 2861 (11.18 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```
",Logical Bug,Logical Bug
"Respect Keras layer names for output operations in Concrete Function <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When converting a Keras model to concrete function, you can preserve the input name by creating a named TensorSpec, but the outputs are always created for you by just slapping `tf.identity` on top of whatever you had there, even if it was a custom named `tf.identity` operation. Since many converters rely on concrete functions to make their own representation (TFLite, ONNX, CoreML, etc), this behavior messes up the output operation names, often making them inconsistent with each other. 
There's currently no workaround for that. You *can* access previous graph nodes by calling a layer  named like {model_name}/{output_layer_name} when doing inference on frozen graph itself, but it won't help you in any way to convert the model.

So I'd be happy to see one of those things as a solution to that:
1) Add an option to explicitly specify the TensorSpec for outputs, just the way we do it for inputs. This would be the most obvious and convenient way of doing it from a user standpoint
2) Don't add new identity operations on top of existing ones. More of a kludge, but would get the job done
3) Add an option to rename operations in concrete function post factum.
4) Add an option to cut off the operations in concrete function past a certain node. 
5) Add an option to convert a graph into a concrete function. Since you can directly modify graphs, this could work as well


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Build simple model
inputs = tf.keras.Input((224, 224, 3), name='custom_input_layer')
x = tf.keras.layers.Flatten()(inputs)
x = tf.keras.layers.Dense(512, activation='relu')(x)
x = tf.keras.layers.Dense(256, activation='relu')(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dense(1, activation='sigmoid', name='custom_output_layer')(x)

model = tf.keras.Model(inputs=inputs, outputs=x, name='my_custom_model_name')
model.summary()

input_tensors = [tf.TensorSpec(shape=inp.shape, dtype=tf.float32, name=inp.name) for inp in model.inputs]
concrete_function = tf.function(lambda x: model(x)).get_concrete_function(x=input_tensors)

print(concrete_function.inputs)  # we can see 'custom_input_layer:0' is there. So is the 'true' output 'my_custom_model_name/custom_output_layer/BiasAdd/ReadVariableOp/resource:0'
print(concrete_function.outputs)  # pesky Identity node gets inserted
```


### Relevant log output

```shell
Model: ""my_custom_model_name""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 custom_input_layer (InputLa  [(None, 224, 224, 3)]    0         
 yer)                                                            
                                                                 
 flatten (Flatten)           (None, 150528)            0         
                                                                 
 dense (Dense)               (None, 512)               77070848  
                                                                 
 dense_1 (Dense)             (None, 256)               131328    
                                                                 
 dense_2 (Dense)             (None, 128)               32896     
                                                                 
 custom_output_layer (Dense)  (None, 1)                129       
                                                                 
=================================================================
Total params: 77,235,201
Trainable params: 77,235,201
Non-trainable params: 0
_________________________________________________________________
[<tf.Tensor 'custom_input_layer:0' shape=(None, 224, 224, 3) dtype=float32>, <tf.Tensor 'my_custom_model_name/dense/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_1/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_1/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_2/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_2/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/custom_output_layer/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/custom_output_layer/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>]
[<tf.Tensor 'Identity:0' shape=(None, 1) dtype=float32>]
```
</details>",UI/UX Bug,Logical Bug
"File comparer freezes in the background 
Type: <b>Performance Issue</b>

When you open the comparison for two files, if these are big and totally different the comparison will start processing but takes long, if you close it and then open another comparison with small files or similar files (that doesn't take long) it will hang, it will take several seconds up to one minute before it finally loads, apparently, the first comparison is frozen in the background still loading even though I already closed it, so it freezes every other comparison until it finishes.

VS Code version: Code 1.95.1 (65edc4939843c90c34d61f4ce11704f09d3e5cb6, 2024-10-31T05:14:54.222Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 5800H with Radeon Graphics          (16 x 3194)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.40GB (15.70GB free)|
|Process Argv|--crash-reporter-id d820b89a-04e6-46b5-8816-8090ebf19a94|
|Screen Reader|no|
|VM|0%|
</details>
<!-- generated by issue reporter -->",Performance Issue,Performance Issue
"Some sidebar items, such as Copilot Edits, select colors from unrelated elements <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.96.2
- OS Version: Windows 10, Windows 11

Steps to Reproduce:

1. Activate any color theme with sidebars that use a different color scheme than the main editor window.
2. Open Copilot Edits.

Items in the sidebar may be invisible or difficult to see depending on the source elements use color. In this example, the main editor uses a light background with dark text, while the sidebars use a dark background with light text. The Copilot Edits file list uses the background of the main editor and the text color of the sidebars, causing its text to have almost no contrast against its background.

![Image](https://github.com/user-attachments/assets/49abdad6-3dea-4ea3-9855-ce891d0c7e08)

I suggest that sidebar widgets should select colors from a single source section that would use paired background/foreground. I.e., all from the editor settings, all from sidebar settings, etc.",UI/UX Bug,UI/UX Bug
"tf.raw_ops.ExperimentalDatasetToTFRecord: Aborted (core dumped) ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.ExperimentalDatasetToTFRecord` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = tf.data.Dataset.range(10).map(lambda x: tf.strings.as_string(x))
input_data = input_data.batch(2)  

filename = tf.constant(""output.tfrecord"")
compression_type = tf.constant(""GZIP"")
tf.raw_ops.ExperimentalDatasetToTFRecord(input_dataset=input_data._variant_tensor, filename=filename, compression_type=compression_type)
```


### Relevant log output

```shell
2024-03-14 05:47:55.415453: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
Aborted (core dumped)
```
",Runtime Error,Runtime Error
"Terminal Command Truncation For a long time, my Visual Studio Code setup, along with its extensions, functioned flawlessly and significantly improved my workflow. However, at some point, I began experiencing issues with certain extensions. Notably, when debugging C++ code with CodeLLDB or CMake Tools, the commands sent to the terminal would occasionally be truncated, preventing the debugger from launching properly.

Recently, I discovered that setting `""terminal.integrated.defaultProfile.osx"": ""bash""` resolved the issue, while switching back to zsh caused it to resurface. Later, after transferring some conda and jenv initialization settings from `.zshrc` to `.bash_profile`, I found that the problem began to intermittently occur in bash as well. Interestingly, the more configuration scripts I added, the more consistently the issue appeared.

Recognizing a pattern, I decided to investigate further by reviewing discussions and documentation in the projects GitHub repository.

## Historical Discussions

An issue similar to what I am currently experiencing was first raised back in 2017 as [#38137](https://github.com/microsoft/vscode/issues/38137). Although it did not provide a direct solution to my problem and was eventually closed because it could not be reproduced, I would like to acknowledge [@Tyriar](https://github.com/Tyriar) for his extensive contributions in that discussion, where he referenced several related issues that offered valuable insights.

One notable comment came from [@fabiospampinato](https://github.com/microsoft/vscode/issues/38137#issuecomment-352450960), who mentioned that the command truncation only happens the first time text is sent to the terminal, but works properly afterwards.

The character limit at which commands are truncated seems to vary across different operating systems. According to [#63613](https://github.com/microsoft/vscode/issues/63613), commands exceeding 1568 characters are truncated on Windows. Meanwhile, issues such as [#59135](https://github.com/microsoft/vscode/issues/59135), [#87183](https://github.com/microsoft/vscode/issues/87183), [#130736](https://github.com/microsoft/vscode/issues/130736), and [#134324](https://github.com/microsoft/vscode/issues/134324) indicate that on macOS, this limit is 1024 characters, which aligns with my observations.

Additionally, issues [#96973](https://github.com/microsoft/vscode/issues/96973) and [#61999](https://github.com/microsoft/vscode/issues/61999) provided effective testing methods for the ""Run selected text in active terminal"" functionality. Additionally, [#136587](https://github.com/microsoft/vscode/issues/136587#issuecomment-966510277) offered an approach for testing using `launch.json`, which significantly simplified my process of reproducing this issue.

I also found the [enable trace logging](https://github.com/microsoft/vscode/wiki/Terminal-Issues#enabling-trace-logging) guide in the projects wiki, which helped me expedite the process of identifying the cause of the problem.

## Steps to Reproduce

1.  Modify `~/.zshrc`. The configurations for `oh-my-zsh`, `jenv`, and `conda` can introduce delays during zsh initialization. To simulate this, add a `sleep` command with timestamps for tracking:
```bash
gdate ""+%Y-%m-%d %H:%M:%S.%3N""
sleep 3
gdate ""+%Y-%m-%d %H:%M:%S.%3N""
```

2. Configure `settings.json`. Set the terminal settings to use zsh and disable environment inheritance:
```json
{
  ""terminal.external.osxExec"": ""Terminal.app"",
  ""terminal.integrated.defaultProfile.osx"": ""zsh"",
  ""terminal.integrated.inheritEnv"": false
}
```

3. Set Log Level to Trace.
4. Verify that all Terminal instances are closed to ensure the next session undergoes full `.zshrc` initialization.
5. Create a text file containing a single line exceeding 1024 characters. Below is an example, where the space after `256`marks the 1024-character boundary:
```txt
001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257
```

6. Select the entire text and use **Terminal: Run Selected Text in Active Terminal**. Observe that commands beyond 1024 characters, starting from `257`, are truncated.
7. Examine the logs for details on the truncation and potential causes.
![Image](https://github.com/user-attachments/assets/bb8cdd15-4b1b-44be-86d8-d119b38844b7)

## Cross-referencing the code and logs

In the [ptyhost.log](https://github.com/user-attachments/files/17680208/ptyhost.log), I observed the following two log entries. The line:
```log
2024-11-06 22:28:02.211 [trace] node-pty.IPty#write 251 252 253 254 255 256 257
```
is the last time the text I entered appeared in full. This log entry was generated by [this._logService.trace('node-pty.IPty#write', object.data);](https://github.com/microsoft/vscode/blob/024999d114e2d9dccd8472a03a17fb0f97c1349e/src/vs/platform/terminal/node/terminalProcess.ts#L522). At this point, the input remains intact. The data is then passed through [this._ptyProcess!.write(object.data);](https://github.com/microsoft/vscode/blob/024999d114e2d9dccd8472a03a17fb0f97c1349e/src/vs/platform/terminal/node/terminalProcess.ts#L526C4-L526C41) into the `node-pty` module, where it is further processed by [this._socket.write(data);](https://github.com/microsoft/node-pty/blob/8bdbd712f40acb939dbab1def8a1c3a815254f74/src/unixTerminal.ts#L178). This method facilitates communication with the C++ layer, which writes the data to the master side of the pseudo-terminal created via [int ret = openpty(&master, &slave, nullptr, NULL, static_cast<winsize*>(&winp));](https://github.com/microsoft/node-pty/blob/8bdbd712f40acb939dbab1def8a1c3a815254f74/src/unix/pty.cc#L478).

The line:
```log
2024-11-06 22:28:02.211 [trace] node-pty.IPty#onData 251 252 253 254 255 256 
```
marks the first instance of text truncation. In the `node-pty` project, when the master side of the pseudo-terminal receives data, it triggers the [public get onData(): IEvent<string> { return this._onData.event; }](https://github.com/microsoft/node-pty/blob/8bdbd712f40acb939dbab1def8a1c3a815254f74/src/terminal.ts#L44) event, allowing subscribed listeners to capture the incoming data. In `vscode`, the `onData` event is subscribed to, and the captured data is logged via [this._logService.trace('node-pty.IPty#onData', data);](https://github.com/microsoft/vscode/blob/5cae08d2afa91f2703d1f5f3e4dd8ad358501424/src/vs/platform/terminal/node/terminalProcess.ts#L320).

Notably, I observed two occurrences of the `gdate` timestamp output from `.zshrc` in the `onData` logs:

```log
2024-11-06 22:28:02.153 [trace] node-pty.IPty#onData 2024-11-06 22:28:02.152
2024-11-06 22:28:05.175 [trace] node-pty.IPty#onData 2024-11-06 22:28:05.174
```
Additionally, after the shell finished loading `.zshrc`, there was another instance of truncated output:
```log
2024-11-06 22:28:05.229 [trace] node-pty.IPty#onData 2
2024-11-06 22:28:05.229 [trace] node-pty.IPty#onData 5
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData 5
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData 2
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData 5
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData 6
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData
```
Since the pseudo-terminals slave side in the `node-pty` project is configured with `ECHO` mode enabled ([`term->c_lflag = ICANON | ISIG | IEXTEN | ECHO | ECHOE | ECHOK | ECHOKE | ECHOCTL;`](https://github.com/microsoft/node-pty/blob/8bdbd712f40acb939dbab1def8a1c3a815254f74/src/unix/pty.cc#L321)), the echo back at `2024-11-06 22:28:02.211` corresponds to the data being written to the slave's buffer by the master. At this point, the shell on the slave side is still initializing `.zshrc` and has not consumed any data from the buffer.

Once the shell finishes initialization and begins consuming data from the slave's buffer, this data is echoed back to the slave and subsequently captured by the master, triggering the corresponding `onData` events.

## Conclusion

In summary, on my macOS system, the issue of commands exceeding 1024 characters being truncated in the terminal stems from how Visual Studio Code interacts with the pseudo-terminal via `node-pty`. During shell initialization, which involves blocking processes, VSCode continues to write data to the slave side of the pseudo-terminal. Once the buffer reaches its maximum capacity, any additional data is discarded. When the shell resumes, it can only read as much data as fits within the buffer's limit, resulting in truncated commands.",Dependency Issue,Dependency Issue
"Perplexity not monotonically decreasing for batch Latent Dirichlet Allocation When using the batch method, the perplexity in LDA should be non-increasing in every iteration, right?
I have cases where it does increase. If this is indeed a bug, I'll investigate.
",UI/UX Bug,Logical Bug
"FIX TunedThresholdClassifierCV error or warn with informative message on invalid metrics This PR fixes two usability problems with the new `TunedThresholdClassifierCV` when using it with invalid values for the `scoring` parameter:

- the first case, is passing a scoring name or scorer object that expects metrics defined for unthresholded predictions (e.g. ROC-AUC). This is clearly invalid and we can raise a `ValueError` with a meaning error message.
- the second case is passing an under-specified scoring function that would return a constant prediction on a given dataset. In this case I chose to warn the user but leave the dummy threshold value that results from this case.

For the second point we could instead warn the user and keep on using 0.5 as the threshold which is probably less pathological/arbitrary.

Alternatively we could also raise a `ValueError` but I am worried that this error could be triggered for bad reasons when doing a grid search or an other resampling procedure around the `TunedThresholdClassifierCV` instance, hence I thought that a warning would be less disruptive.

/cc @glemaitre @lorentzenchr ",Syntax Error,Syntax Error
"UnsetMetadataPassedError can point towards the wrong method ### Describe the bug

When `enable_metadata_routing=True`, for a missing `set_score_request`, `UnsetMetadataPassedError` message states that a `set_fit_request` is missing.

### Steps/Code to Reproduce

```python
from sklearn import set_config
from sklearn.exceptions import UnsetMetadataPassedError
from sklearn.model_selection import cross_validate
from sklearn.linear_model import LogisticRegression
import numpy as np

rng = np.random.RandomState(22)
n_samples, n_features = 10, 4
X = rng.rand(n_samples, n_features)
y = rng.randint(0, 2, size=n_samples)
sw = rng.randint(0, 5, size=n_samples)

set_config(enable_metadata_routing=True)
# missing set_score_request
logreg = LogisticRegression().set_fit_request(sample_weight=True)
try:
    cross_validate(
        logreg, X, y, 
        params={""sample_weight"":sw}, 
        error_score='raise'
    )
except UnsetMetadataPassedError as e:
    print(e)
```

### Expected Results

I would expect an error message pointing towards the missing `set_score_request`, and perhaps a less verbose message when only one metadata is passed. Something like:


'sample_weight' are passed to cross validation but are not explicitly set as requested or not requested for cross_validate's estimator: LogisticRegression. Call `.set_score_request(sample_weight=True)` on the estimator for using 'sample_weight' or `sample_weight=False` for not using it. See the Metadata Routing User guide...

### Actual Results

['sample_weight'] are passed to cross validation but are not explicitly set as requested or not requested for cross_validate's estimator: LogisticRegression. Call `.set_fit_request({{metadata}}=True)` on the estimator for each metadata in ['sample_weight'] that you want to use and `metadata=False` for not using it. See the Metadata Routing User guide...

### Versions

```shell
sklearn: 1.7.dev0
```",Syntax Error,Syntax Error
"BUG: StandardScaler partial_fit overflows The recent implementation of `partial_fit` for `StandardScaler` can overflow. A use case there is to transform indefinitely long stream of data, but that is problematic with the current implementation. The reason is that to compute the running mean, [we keep track](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py#L788) of the sample sum.

Here the code to reproduce the behavior. To simulate long stream of data would take long time; instead, I use samples with very large norm but the effect is the same. The same batch is presented to the transformer many times. The mean should be same.

```python
from sklearn.preprocessing import StandardScaler
import numpy as np

rng = np.random.RandomState(0)

def gen_1d_uniform_batch(min_, max_, n):
    return rng.uniform(min_, max_, size=(n, 1))

max_f = np.finfo(np.float64).max / 1e5
min_f = max_f / 1e2
stream_dim = 100
batch_dim = 500000
print(""mean overflow: batch vs online on %d repetitions"" % stream_dim)

X = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)

scaler = StandardScaler(with_std=False).fit(X)
print(scaler.mean_)
[  1.79769313e+301]

iscaler = StandardScaler(with_std=False)
batch = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)
for _ in range(stream_dim):
    iscaler = iscaler.partial_fit(batch)
RuntimeWarning: overflow encountered in add
  updated_mean = (last_sum + new_sum) / updated_sample_count

print(iscaler.mean_)
[ inf]
```
",Security Vulnerability,Security Vulnerability
"toggle diff for edit command does not keep cursor location consistent @jooyoungseo tested the edits experience, including the diff editor toggling keybinding.

He noticed that the cursor location shifted when toggling back and forth, making it impractical to use.",Performance Issue,Performance Issue
"Chat: Picked model is not preserved 
Type: <b>Bug</b>

Once Claude Sonnet is selected in the Pick Model option  within Chat, it goes back to gpt4o after I restart my computer or reload my session.

VS Code version: Code - Insiders 1.96.0-insider (f87f8a56f3a30238076bee3db39c245bd69be264, 2024-11-05T05:04:15.310Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|11th Gen Intel(R) Core(TM) i5-1145G7 @ 2.60GHz (8 x 2611)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.71GB (5.46GB free)|
|Process Argv|--crash-reporter-id b05b88e5-8894-4031-ae34-fa034ebddea9|
|Screen Reader|yes|
|VM|0%|
</details><details><summary>Extensions (133)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-openapi|42C|4.29.2
zotenote|A-W|1.0.1
android-dev-ext|ade|1.4.0
aiprm-lang|AIP|0.0.2
Bookmarks|ale|13.5.0
openscad|Ant|1.2.2
spellright|ban|3.0.140
mermaid-markdown-syntax-highlighting|bpr|1.6.0
external-pdf|cha|1.2.0
doxdocgen|csc|1.4.0
vscode-markdownlint|Dav|0.53.0
vscode-eslint|dba|3.0.10
vscode-quick-select|dba|0.2.9
vscode-deno|den|3.42.0
gitlens|eam|14.6.1
EditorConfig|Edi|0.16.4
prettier-vscode|esb|10.1.0
figma-vscode-extension|fig|0.4.0
vscode-firefox-debug|fir|2.9.10
shell-format|fox|7.2.5
vscode-google-translate|fun|1.4.13
codespaces|Git|1.17.3
copilot|Git|1.243.1196
copilot-chat|Git|0.23.2024110501
remotehub|Git|0.64.0
vscode-github-actions|git|0.26.2
vscode-pull-request-github|Git|0.101.2024110504
cloudcode|goo|2.20.0
overleaf-workshop|iam|0.14.1
cslpreview|igo|0.2.2
path-autocomplete|ion|1.25.0
latex-workshop|Jam|10.5.6
lilypond-syntax|jea|0.1.1
scheme|jea|0.2.0
better-cpp-syntax|jef|1.17.2
commitlint|jos|2.6.0
language-julia|jul|1.127.2
google-search|kam|0.0.1
vscode-lua-format|Koi|1.3.8
lilypond-formatter|lhl|0.2.3
lilypond-pdf-preview|lhl|0.2.8
lilypond-snippets|lhl|0.1.1
vslilypond|lhl|1.7.3
language-matlab|Mat|1.2.6
git-graph|mhu|1.30.0
mongodb-vscode|mon|1.9.3
azure-dev|ms-|0.8.4
vscode-azureappservice|ms-|0.25.4
vscode-azurecontainerapps|ms-|0.6.1
vscode-azurefunctions|ms-|1.15.4
vscode-azureresourcegroups|ms-|0.8.3
vscode-azurestaticwebapps|ms-|0.12.2
vscode-azurestorage|ms-|0.16.1
vscode-azurevirtualmachines|ms-|0.6.6
vscode-cosmosdb|ms-|0.23.0
vscode-docker|ms-|1.29.3
vscode-edge-devtools|ms-|2.1.6
black-formatter|ms-|2024.5.12841012
debugpy|ms-|2024.13.2024103001
flake8|ms-|2023.13.12291011
isort|ms-|2023.13.12321012
python|ms-|2024.19.2024103101
vscode-pylance|ms-|2024.11.1
jupyter|ms-|2024.11.2024103101
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.21
vscode-jupyter-cell-tags|ms-|0.1.8
vscode-jupyter-slideshow|ms-|0.1.5
remote-containers|ms-|0.390.0
remote-ssh|ms-|0.116.2024102915
remote-ssh-edit|ms-|0.87.0
remote-wsl|ms-|0.81.8
vscode-remote-extensionpack|ms-|0.25.0
azure-account|ms-|0.12.0
azure-repos|ms-|0.40.0
cmake-tools|ms-|1.19.52
cpptools|ms-|1.23.0
cpptools-extension-pack|ms-|1.3.0
js-debug-nightly|ms-|2024.11.417
live-server|ms-|0.5.2024091601
remote-explorer|ms-|0.5.2024011009
remote-repositories|ms-|0.42.0
remote-server|ms-|1.6.2024011109
vscode-commander|ms-|0.2.0
vscode-copilot-data-analysis|ms-|0.2.2
vscode-copilot-vision|ms-|0.2.2024110422
vscode-github-issue-notebooks|ms-|0.0.130
vscode-node-azure-pack|ms-|1.2.0
vscode-selfhost-test-provider|ms-|0.3.25
vscode-serial-monitor|ms-|0.13.1
vscode-speech|ms-|0.12.1
vscode-speech-language-pack-en-ca|ms-|0.5.0
vscode-speech-language-pack-en-gb|ms-|0.5.0
vscode-speech-language-pack-ko-kr|ms-|0.5.0
vscode-websearchforcopilot|ms-|0.1.2024110502
vsliveshare|ms-|1.0.5941
windows-ai-studio|ms-|0.5.2024102408
autodocstring|njp|0.6.1
pandocciter|not|0.10.4
typst-lsp|nva|0.13.0
publisher|pos|1.2.1
shiny|Pos|1.1.0
shinyuieditor|pos|0.5.0
quarto|qua|1.116.0
r-debugger|RDe|0.5.5
java|red|1.36.0
vscode-xml|red|0.27.1
vscode-yaml|red|1.14.0
r|REd|2.8.4
multi-command|ryu|1.6.0
AudioQ|Seh|0.0.2
vscode-deepl|soe|1.1.1
abc-music|sof|0.4.0
lua|sum|3.12.0
latex-utilities|tec|0.4.14
cmake|twx|0.0.17
msft-todo-unofficial|tyl|0.0.18
vscode-terminal-here|Tyr|0.2.4
windows-terminal|Tyr|0.7.0
errorlens|use|3.16.0
intellicode-api-usage-examples|Vis|0.2.9
vscodeintellicode|Vis|1.2.30
vscode-conventional-commits|viv|1.26.0
vscode-arduino|vsc|0.7.1
vscode-gradle|vsc|3.16.4
vscode-java-debug|vsc|0.58.1
vscode-java-dependency|vsc|0.24.0
vscode-java-pack|vsc|0.29.0
vscode-java-test|vsc|0.42.0
vscode-maven|vsc|0.44.0
zoterobackwardsearcher|vsz|0.0.1
markdown-all-in-one|yzh|3.6.2
grammarly|znc|0.25.0

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
vsaa593:30376534
py29gd2263:31024238
c4g48928:30535728
a9j8j154:30646983
962ge761:30841072
pythongtdpath:30726887
pythonnoceb:30776497
asynctok:30898717
dsvsc014:30777825
dsvsc015:30821418
pythonmypyd1:30859725
h48ei257:31000450
pythontbext0:30879054
cppperfnew:30980852
pythonait:30973460
01bff139:31013167
dvdeprecation:31040973
dwnewjupyter:31046869
impr_priority:31057980
nativerepl1:31134653
refactort:31084545
pythonrstrctxt:31093868
nativeloc1:31118317
cf971741:31144450
e80f6927:31120813
12bdf347:31141542
iacca1:31150324
notype1:31143044
dwcopilot:31158714
g7688163:31155431

```

</details>

<!-- generated by issue reporter -->",Dependency Issue,Dependency Issue
"VSCode opens automatically when I plug in a mass storage device or click on ""open with File Manager"" (dolphin) <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No
yes
<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- 1.93
- OS Version: 
- Kubuntu 23.10

Steps to Reproduce:

1. plug in msc
2. click on ""open with file manager""


I think vscode helped itself to auto-handle some mime-type that I did not ask it to, which usually gets handled by dolphin. Please don't do this microsoft. undo it. really lame mcro d*** move. vscode is awesome, but disguisting to consider itself anywhere close to a sufficient replacement for dolphin.",Dependency Issue,Dependency Issue
"Use after free in propagator_state.cc <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.12

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Pointer `next_iter` from function `PropagatorState::FrameState::IncrementIteration` is passed 
as the 1st parameter into `ActivateLoopInvs` where it is passed as the 1st parameter into 
`AdjustOutstandingOpsLocked`, then inside this function it is passed into `CleanupIterations` 
where it is deleted.

Then in `PropagatorState::FrameState::IncrementIteration` this possibly freed pointer is used 
in `return`-statement.

This behavior was introduced by https://github.com/tensorflow/tensorflow/commit/ae2a0e5c473f2a575767262021c26852d22886f8. 
Before this commit, no `return` was performed on the possibly freed pointer.
```


### Standalone code to reproduce the issue

```shell
Bug was found by Svace static analysis tool.
```


### Relevant log output

_No response_</details>",Logical Bug,Logical Bug
"Is the check of strict convergence in KMeans too expensive for the benefits ? ### Describe the bug

In `KMeans` scikit-learn defines [`strict_convergence`](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/cluster/_kmeans.py#L701) as the event of producing the same label assignments at two successive iterations.

When this happens, it means convergence for both labels and centroids (set aside possible oscillations due to numerical instability, were the iterations to continue).

But checking for strict convergence seems to be somewhat expensive (one loop over the last two label assignments of each sample per iteration), and if the user properly set `tol` it doesn't seem necessary at all ?

Checking for strict convergence seems to really help when `tol==0`. With `tol==0` I've seen cases of endless oscillations around 0 because of numerical instability, but never reaching 0, and finally terminating at `max_iter` iterations.

For the general case, isn't it detrimental to performance though ? one can expect the performance cost to be significant for small dimensions of data, for which an additional pass on a column is marginally more expensive.

So I would maybe suggest the following improvements:

- [ ] enable automatically the strict convergence checks only if `tol==0` (or if `tol` is ""very small"")
- [ ] maybe expose to the user the choice of enabling strict convergence at each iteration ?


### Versions

```shell
1.3
```
",Performance Issue,Performance Issue
"Bug: MouseEnter does not fire when coming _from_ disabled elements <!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

React version: 16.13.1

## Steps To Reproduce

https://codesandbox.io/s/eager-euler-c72fw?file=/src/App.js

1. move from a disabled element to an element with a `mouseenter` listener.
2. `mouseenter` does not fire

Link to code example: https://codesandbox.io/s/eager-euler-c72fw?file=/src/App.js


## The current behavior

`mouseenter` does not fire because React calculates mouseenter/leave based on the relative element of the opposing event.
e.g. `mouseenter` calculated from the `mouseout` on the button. Browsers do not fire pointer events on disabled elements


## The expected behavior

`mouseenter` should fire


Previous issue: https://github.com/facebook/react/issues/4251",UI/UX Bug,UI/UX Bug
"Field type=""email"" with multiple attribute cursor jumps to start <!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**
Bug

**What is the current behavior?**
While adding multiple comma separated email addresses the cursor will jump to te beginning the input field. First i thought it was a [Formik](https://github.com/jaredpalmer/formik/issues/1428) issue, but it seems to be a React one.

Example pure React: https://codesandbox.io/embed/0y06zo7l8p
Example React with Formik: https://codesandbox.io/embed/k57zw7wz5
MDN docs: https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input/email#multiple

**What is the expected behavior?**
Cursor stays on current position.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**
Chrome 73, Safari 12.1, Opera 60.0
React 16.7.0

Maybe #14551 related?
",UI/UX Bug,UI/UX Bug
"[DevTools Bug]: DevTools causes hook callback behavior inconstency ### Website or app

https://gist.github.com/nhusher/33981014bb69318ead012c11a73eff52

### Repro steps

Either clone the gist and run with static-server, or check out [this equivalent codesandbox link](https://5lu8ks.csb.app/) (Full sandbox IDE [here](https://codesandbox.io/s/new-snowflake-5lu8ks?file=/src/index.tsx))

1. Have React devtools installed 
2. Visit the URL indicated by static-server
3. Click both increment buttons, ensuring that they work
4. Open the developer tools
5. Navigate to the DevTools Component tree and highlight `<App />`
6. Click both the increment buttons
7. Observe that the normal in-React increment button works
8. Observe that the ""increment (outside react)"" button no longer works

Verification of the issue:

1. Open an incognito window
2. Visit the URL indicated by static-server
3. Click the two buttons and verify that both work
4. Open the developer tools (no React Devtools available)
5. Verify the two buttons continue to work

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Logical Bug,Syntax Error
"Segmentation fault (core dumped) in `RaggedTensorToTensor` ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

shape = tf.constant(-1, shape=[], dtype=tf.int64)
values = tf.constant(0, shape=[0], dtype=tf.int32)
default_value = tf.constant(0, shape=[], dtype=tf.int32)
row_partition_tensors = tf.constant([0, 1, 6], shape=[3], dtype=tf.int64)
row_partition_types = [""ROW_SPLITS""]

tf.raw_ops.RaggedTensorToTensor(
    shape=shape,
    values=values,
    default_value=default_value,
    row_partition_tensors=[row_partition_tensors],
    row_partition_types=row_partition_types)
```

### Relevant log output

```shell
Segmentation fault (core dumped)
```",Runtime Error,Runtime Error
"Report issue fails with HTTP 401 # Behaviour

After running the ""Python: Report Issue..."" command and filling everything out, the ""Create on GitHub"" button appears to do nothing.

The extension is making a request using `Authorization: Bearer gho_xxx` and gets a HTTP 401 ""Bad credentials"" response from the GitHub API.

## Steps to reproduce:

1. Follow https://github.com/microsoft/vscode-python/wiki/Reporting-a-bug#the-extension-loads or https://github.com/microsoft/vscode-python/wiki/Reporting-a-bug#the-extension-wont-even-start
2. Press ""Create on GitHub""
3. Toggle Developer Tools and check the console logs and network requests

# Diagnostic data

No relevant output for `Python` in the `Output` panel",Security Vulnerability,Security Vulnerability
"MaxListenersExceededWarning: Possible EventEmitter memory leak detected. <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.95.3
- OS Version: Ubuntu 22.04.5 LTS

Steps to Reproduce:

1. Close VSCode with 8 projects opened (most of them DevContainer projects)
2. Open VSCode again from terminal ( running `code --disable-extensions --trace-warnings` to disable extensions and to show warning details) without specifying any folder or file (so last working session is used opening the 8 projects)
3. Following log is printed in the terminal:
```bash
Warning: 'trace-warnings' is not in the list of known options, but still passed to Electron/Chromium.
[main 2024-11-19T10:24:29.689Z] update#setState idle
(node:471892) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 child-process-gone listeners added to [App]. MaxListeners is 10. Use emitter.setMaxListeners() to increase limit
    at genericNodeError (node:internal/errors:984:15)
    at wrappedFn (node:internal/errors:538:14)
    at _addListener (node:events:593:17)
    at App.addListener (node:events:611:10)
    at Object.X [as onWillAddFirstListener] (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:7366)
    at q (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:31:1282)
    at Na.H (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:13550)
    at Na.F (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:11857)
    at Na.start (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:15161)
    at vh.start (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:29191)
    at Object.call (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:36:4563)
    at df.s (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:19676)
    at df.q (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:19199)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:18601)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.C (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:816)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:1033)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:4908)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:964)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:5092)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:964)
    at T (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:7343)
    at IpcMainImpl.i (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:36:21224)
    at IpcMainImpl.emit (node:events:531:35)
    at WebContents.<anonymous> (node:electron/js2c/browser_init:2:85782)
    at WebContents.emit (node:events:519:28)
[main 2024-11-19T10:24:59.835Z] update#setState checking for updates
[main 2024-11-19T10:25:00.551Z] update#setState idle
```
",Security Vulnerability,Security Vulnerability
"Task API for `ShellExecutionOptions.env` type does not support `undefined` values 
Type: <b>Bug</b>

The `TerminalOptions.env` API supports environment variable values that include both `string | undefined`.

The ask is for this to match the TerminalOptions API.

Usage:
We plan on using this in the Python extension where users can contribute environment variables via `.env` files. We want to make sure the experience is same for the env variable merge in both cases.

VS Code version: Code - Insiders 1.96.0-insider (69acde7458f428f0e6869de8915c9dd995cdda1a, 2024-11-21T05:04:38.064Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-1065G7 CPU @ 1.30GHz (8 x 1498)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.60GB (12.74GB free)|
|Process Argv|--log trace --log ms-python.python=info --crash-reporter-id 4fb1ebc1-cf4c-4880-a88a-47738ec3768d|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (18)</summary>

Extension|Author (truncated)|Version
---|---|---
tsl-problem-matcher|amo|0.6.2
ruff|cha|2024.54.0
esbuild-problem-matchers|con|0.0.3
vscode-eslint|dba|3.0.10
gitlens|eam|16.0.2
EditorConfig|Edi|0.16.4
prettier-vscode|esb|11.0.0
copilot|Git|1.245.1221
copilot-chat|Git|0.23.2024112103
vscode-github-actions|git|0.27.0
vscode-pull-request-github|Git|0.101.2024112104
debugpy|ms-|2024.13.2024111901
python|ms-|2024.21.0-dev
vscode-pylance|ms-|2024.11.101
remote-containers|ms-|0.388.0
remote-wsl|ms-|0.88.5
extension-test-runner|ms-|0.0.12
code-spell-checker|str|4.0.21


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vsc_aacf:30263846
pythonvspyt551:31179976
vscod805cf:30301675
vsaa593cf:30376535
py29gd2263:31024238
c4g48928:30535728
vscrpc:30624061
a9j8j154:30646983
962ge761:30841072
pythonnoceb:30776497
asynctok:30898717
dsvsc014:30777825
dsvsc015:30821418
pythonmypyd1:30859725
h48ei257:31000450
pythontbext0:30879054
cppperfnew:30980852
pythonait:30973460
0ee40948:31013168
dvdeprecation:31040973
dwnewjupytercf:31046870
newcmakeconfigv2:31071590
nativerepl1:31134653
pythonrstrctxt:31093868
nativeloc1:31118317
cf971741:31144450
notreesitter:31116712
e80f6927:31120813
i21gd607:31141543
iacca1:31150324
notype1:31143044
dwcopilot:31158714
h409b430:31177054
5b1c1929:31184661

```

</details>

<!-- generated by issue reporter -->",Dependency Issue,Dependency Issue
"HistGradientBoostingClassifier slow in prediction mode While HistGradientBoostingClassifier is 100 faster than GradientBoostingClassifier when fitting the model, I found it to be very slow in case of predicting the class probabilities, in my case about 100 times slower :-(

For example: 
GradientBoostingClassifier: 3.2 min for training for 1 million examples. 32 ms for 1000 predictions.
HistGradientBoostingClassifier: 7s for training. 1s for 1000 predictions",Performance Issue,Performance Issue
"`tf.raw_ops.ArgMax`: Heap buffer overflow ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.ArgMax` can lead to heap buffer overflow.
[Error location](https://github.com/tensorflow/tensorflow/blob/774b0c3e97b5ef60bfc9c54961347dd0bc3660a8/tensorflow/core/kernels/argmax_op.cc#L59):
```C++
    const int32_t dim = internal::SubtleMustCopy(dimension.scalar<int32>()());
```
It copies scalar value of `dimension` without checking exact type. Therefore when `dimension` is an `int16` it reads over the bound.
Note that `int16` is an allowed type for `dimension` according to [opdef](https://github.com/tensorflow/tensorflow/blob/41b93a8b310086f69aab6b6369d2af9d5178881d/tensorflow/core/ops/math_ops.cc#L1153-L1160):
```C++
REGISTER_OP(""ArgMax"")
    .Input(""input: T"")
    .Input(""dimension: Tidx"")
    .Output(""output: output_type"")
    .Attr(""T: {realnumbertype, quantizedtype, bool}"")
    .Attr(""Tidx: {int16, int32, int64} = DT_INT32"")
    .Attr(""output_type: {int16, uint16, int32, int64} = DT_INT64"")
    .SetShapeFn(ArgOpShape);
```

### Standalone code to reproduce the issue

```python
import tensorflow as tf

tf.raw_ops.ArgMax(
    input=tf.random.normal([1,1,1,1]),
    dimension=tf.constant(1,shape=[],dtype=tf.int16),
    output_type=tf.dtypes.int64,
    name=None
)
```


### Relevant log output

The below log needs ASAN build.
```shell
=================================================================
==4008222==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x609000000400 at pc 0x7fa6a0dca809 bp 0x7ffe63b29d90 sp 0x7ffe63b29d88
READ of size 4 at 0x609000000400 thread T0
    #0 0x7fa6a0dca808 in int const tensorflow::internal::SubtleMustCopy<int>(int const&) /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10
    #1 0x7fa6a0dca808 in tensorflow::ArgOp<Eigen::ThreadPoolDevice, float, long, tensorflow::functor::ArgMax<Eigen::ThreadPoolDevice, float, long>>::Compute(tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/kernels/argmax_op.cc:59:25
    #2 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #3 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #4 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #5 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #6 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #7 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #8 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #9 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #10 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #11 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #12 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #13 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #14 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #15 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #16 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #17 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #18 0x7fa6b989418d in TFE_Py_FastPathExecute_C(_object*) /proc/self/cwd/tensorflow/python/eager/pywrap_tfe_src.cc:3979:3
    #19 0x7fa667d2683e in pybind11_init__pywrap_tfe(pybind11::module_&)::$_60::operator()(pybind11::args) const /proc/self/cwd/tensorflow/python/tfe_wrapper.cc:1276:35
    #20 0x7fa667d2683e in pybind11::object pybind11::detail::argument_loader<pybind11::args>::call_impl<pybind11::object, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, 0ul, pybind11::detail::void_type>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, std::integer_sequence<unsigned long, 0ul>, pybind11::detail::void_type&&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1443:16
    #21 0x7fa667d2683e in std::enable_if<!std::is_void<pybind11::object>::value, pybind11::object>::type pybind11::detail::argument_loader<pybind11::args>::call<pybind11::object, pybind11::detail::void_type, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1411:42
    #22 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:248:69
    #23 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:223:21
    #24 0x7fa667d67a59 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:939:30
    #25 0x528186 in cfunction_call /usr/local/src/conda/python-3.11.7/Objects/methodobject.c:542:18
    #26 0x503a0b in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18
    #27 0x510f32 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:4769:23
    #28 0x538732 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #29 0x538732 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #30 0x538732 in _PyFunction_Vectorcall /usr/local/src/conda/python-3.11.7/Objects/call.c:393:16
    #31 0x5426bb in _PyVectorcall_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:257:24
    #32 0x5426bb in _PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:328:16
    #33 0x5426bb in PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:355:12
    #34 0x514ff0 in do_call_core /usr/local/src/conda/python-3.11.7/Python/ceval.c:7352:12
    #35 0x514ff0 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:5376:22
    #36 0x5cb559 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #37 0x5cb559 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #38 0x5cac2e in PyEval_EvalCode /usr/local/src/conda/python-3.11.7/Python/ceval.c:1148:21
    #39 0x5ebcf6 in run_eval_code_obj /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1710:9
    #40 0x5e788f in run_mod /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1731:19
    #41 0x5fc831 in pyrun_file /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1626:15
    #42 0x5fbbfe in _PyRun_SimpleFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:440:13
    #43 0x5fb922 in _PyRun_AnyFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:79:15
    #44 0x5f65cd in pymain_run_file_obj /usr/local/src/conda/python-3.11.7/Modules/main.c:360:15
    #45 0x5f65cd in pymain_run_file /usr/local/src/conda/python-3.11.7/Modules/main.c:379:15
    #46 0x5f65cd in pymain_run_python /usr/local/src/conda/python-3.11.7/Modules/main.c:601:21
    #47 0x5f65cd in Py_RunMain /usr/local/src/conda/python-3.11.7/Modules/main.c:680:5
    #48 0x5bb3d8 in Py_BytesMain /usr/local/src/conda/python-3.11.7/Modules/main.c:734:12
    #49 0x7fa791408d8f in __libc_start_call_main csu/../sysdeps/nptl/libc_start_call_main.h:58:16
    #50 0x7fa791408e3f in __libc_start_main csu/../csu/libc-start.c:392:3
    #51 0x5bb222 in _start (/home/loft/anaconda3/envs/tf-latest-asan/bin/python3.11+0x5bb222)

0x609000000402 is located 0 bytes after 2-byte region [0x609000000400,0x609000000402)
allocated by thread T0 here:
    #0 0x7fa7917ed617 in __interceptor_posix_memalign /home/runner/work/llvm-project/llvm-project/final/llvm-project/compiler-rt/lib/asan/asan_malloc_linux.cpp:145:3
    #1 0x7fa6be4f6902 in tsl::port::AlignedMalloc(unsigned long, int) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4a18902)
    #2 0x7fa6bc2d3bcc in tsl::(anonymous namespace)::CPUAllocator::AllocateRaw(unsigned long, unsigned long) cpu_allocator_impl.cc
    #3 0x7fa6bc4883bc in short* tensorflow::TypedAllocator::Allocate<short>(tsl::Allocator*, unsigned long, tsl::AllocationAttributes const&) /proc/self/cwd/./tensorflow/core/framework/typed_allocator.h:47:24
    #4 0x7fa6bc4883bc in tensorflow::(anonymous namespace)::Buffer<short>::Buffer(tsl::Allocator*, long, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:574:21
    #5 0x7fa6bc4883bc in tensorflow::Tensor::Tensor(tsl::Allocator*, tensorflow::DataType, tensorflow::TensorShape const&, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:986:5
    #6 0x7fa6bbef871d in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:764:10
    #7 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes) /proc/self/cwd/./tensorflow/core/framework/op_kernel.h:1270:12
    #8 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**, tsl::AllocatorAttributes) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:822:14
    #9 0x7fa6bbef4368 in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:728:10
    #10 0x7fa6a9bf0207 in tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e9fe207)
    #11 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #12 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #13 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #14 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #15 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #16 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #17 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #18 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #19 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #20 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #21 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #22 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #23 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #24 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #25 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #26 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #27 0x7fa6b985dcbf in tensorflow::EagerCast(TFE_Context*, TFE_TensorHandle*, TF_DataType, TF_DataType, TSL_Status*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:259:3
    #28 0x7fa6b985e67b in tensorflow::ConvertToEagerTensorUncached(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:317:11
    #29 0x7fa6b985f8a1 in tensorflow::ConvertToEagerTensor(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:405:14
    #30 0x7fa6b9860088 in EagerTensor_init /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:529:18
    #31 0x5039d2 in type_call /usr/local/src/conda/python-3.11.7/Objects/typeobject.c:1103:19
    #32 0x5039d2 in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18

SUMMARY: AddressSanitizer: heap-buffer-overflow /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10 in int const tensorflow::internal::SubtleMustCopy<int>(int const&)
Shadow bytes around the buggy address:
  0x609000000180: fa fa fa fa fa fa fa fa fd fa fa fa fa fa fa fa
  0x609000000200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000280: fd fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000300: 04 fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000380: fd fd fd fd fa fa fa fa fa fa fa fa fa fa fa fa
=>0x609000000400:[02]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000480: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000500: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000580: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000600: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000680: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
==4008222==ABORTING
```
",Security Vulnerability,Security Vulnerability
"Poor/pathological multi-cursor performance 
Type: <b>Performance Issue</b>

Using Windows-Alt-down shortcut (Command-Option?) on Mac to extend multi-cursor, after a dozen or so lines vs code totally freezes and must be killed (dialog prompts to reopen window), even though cursor limit is in the thousands (like, 10000).

Debugging, it seems that vs/editor/common/cursor/cursorMoveCommands.ts addCursorDown is generating duplicate cursors for each existing cursor for each down command. E.g. 1 -> 2 -> 4 -> 8 -> 16...after extending down to 4 cursors. I imagine this is easily reaching the cursor limit. It makes performing simple edits (delete indents) for more than just a handful of lines impossible.

https://github.com/microsoft/vscode/blob/8eb7fac5658846e35a0399dc65e9a0580d4e4ed7/src/vs/editor/common/cursor/cursorMoveCommands.ts#L17

```typescript
public static addCursorDown(viewModel: IViewModel, cursors: CursorState[], useLogicalLine: boolean): PartialCursorState[] {
		const result: PartialCursorState[] = [];
		let resultLen = 0;
                 // multiple duplicate cursors added for each action?
		for (let i = 0, len = cursors.length; i < len; i++) {
			const cursor = cursors[i];
			result[resultLen++] = new CursorState(cursor.modelState, cursor.viewState);
			if (useLogicalLine) {
				result[resultLen++] = CursorState.fromModelState(MoveOperations.translateDown(viewModel.cursorConfig, viewModel.model, cursor.modelState));
			} else {
				result[resultLen++] = CursorState.fromViewState(MoveOperations.translateDown(viewModel.cursorConfig, viewModel, cursor.viewState));
			}
		}
		return result;
	}
```

![Image](https://github.com/user-attachments/assets/1ca92396-614f-4c72-a7df-41db39c065fd)

![Image](https://github.com/user-attachments/assets/1f249530-d583-435f-ba02-dc6b5f6efa07)

![Image](https://github.com/user-attachments/assets/2b2ec748-6c18-4841-8c50-9d32bd50e942)

(the leaf calls here are to function `removeChild`)

VS Code version: Code 1.95.3 (Universal) (f1a4fb101478ce6ec82fe9627c43efbf9e98c813, 2024-11-13T14:50:04.152Z)
OS version: Darwin x64 23.4.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz (12 x 2600)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 3, 4|
|Memory (System)|32.00GB (1.84GB free)|
|Process Argv|--crash-reporter-id 129e9867-ec36-439b-a1f1-7c366a524771|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    7	   197	 22632	code main
    0	    66	 22635	   gpu-process
    0	    33	 22636	   utility-network-service
    0	   492	 22638	window [1] (realm.ts  web)
    0	   131	 22900	shared-process
    0	     0	 23751	     /bin/ps -ax -o pid=,ppid=,pcpu=,pmem=,command=
    0	    66	 22901	fileWatcher [1]
    0	   295	 23481	extensionHost [1]
    0	    98	 23604	     electron-nodejs (tsserver.js )
    0	   164	 23605	     electron-nodejs (tsserver.js )
    0	    66	 23608	       electron-nodejs (typingsInstaller.js typesMap.js )
    0	   131	 23606	     electron-nodejs (server.js )
    0	    66	 23642	     electron-nodejs (languageserver.js )
    0	    66	 23643	     electron-nodejs (languageserver.js )
    0	   131	 23645	     electron-nodejs (index.js )
    0	    33	 23656	     /Applications/Visual Studio Code.app/Contents/Frameworks/Code Helper (Plugin).app/Contents/MacOS/Code Helper (Plugin) /Applications/Visual Studio Code.app/Contents/Resources/app/extensions/json-language-features/server/dist/node/jsonServerMain --node-ipc --clientProcessId=23481
    0	   131	 23679	     electron-nodejs (eslintServer.js )
    0	   131	 23724	     electron-nodejs (main-bundle.js )
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (realm.ts  web)
|    Folder (web): 13737 files
|      File types: json(4490) txt(2175) js(514) ts(404) properties(248)
|                  png(220) svg(200) dmg(180) map(179) sql(171)
|      Conf files: dockerfile(41) package.json(31) tsconfig.json(17)
|                  makefile(6) settings.json(4) launch.json(1)
|                  gulp.js(1)
|      Launch Configs: pwa-node(4);
```

</details>
<details><summary>Extensions (49)</summary>

Extension|Author (truncated)|Version
---|---|---
rust-bundle|1Yi|1.0.0
Bookmarks|ale|13.5.0
tsl-problem-matcher|amo|0.6.2
atlascode|atl|3.0.13
markdown-mermaid|bie|1.27.0
vscode-svgviewer|css|2.0.0
vscode-eslint|dba|3.0.10
javascript-ejs-support|Dig|1.3.3
rust-syntax|dus|0.6.1
gitlens|eam|16.0.3
shell-format|fox|7.2.5
gitlab-workflow|Git|5.18.1
vscode-graphql|Gra|0.12.1
vscode-graphql-execution|Gra|0.3.1
vscode-graphql-syntax|Gra|1.3.8
vscode-mocha-test-adapter|hbe|2.14.1
vscode-test-explorer|hbe|2.22.1
rainbow-csv|mec|3.13.0
vscode-docker|ms-|1.29.3
debugpy|ms-|2024.12.0
isort|ms-|2023.10.1
python|ms-|2024.20.0
vscode-pylance|ms-|2024.11.3
jupyter|ms-|2024.10.0
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-ssh|ms-|0.115.1
remote-ssh-edit|ms-|0.87.0
live-server|ms-|0.4.15
makefile-tools|ms-|0.11.13
remote-explorer|ms-|0.4.3
test-adapter-converter|ms-|0.2.1
vscode-js-profile-flame|ms-|1.0.9
sqltools|mtx|0.28.3
sqltools-driver-pg|mtx|0.5.4
java|red|1.36.0
vscode-xml|red|0.27.1
vscode-yaml|red|1.15.0
vscode-coverage-gutters|rya|2.12.0
semanticdiff|sem|0.9.0
intellicode-api-usage-examples|Vis|0.2.9
vscodeintellicode|Vis|1.3.2
vscode-gradle|vsc|3.16.4
vscode-java-debug|vsc|0.58.1
vscode-java-dependency|vsc|0.24.1
vscode-java-pack|vsc|0.29.0
vscode-java-test|vsc|0.43.0
vscode-maven|vsc|0.44.0
quokka-vscode|Wal|1.0.667


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
9c06g630:31013171
dvdeprecation:31068756
dwnewjupyter:31046869
newcmakeconfigv2:31071590
nativerepl1:31139838
pythonrstrctxt:31112756
nativeloc1:31185841
cf971741:31144450
iacca1:31171482
notype1cf:31157160
5fd0e150:31155592
dwcopilot:31170013
stablechunks:31184530

```

</details>

<!-- generated by issue reporter -->",Performance Issue,Performance Issue
"MaxListenersExceededWarning: Possible EventEmitter memory leak detected. <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.95.3
- OS Version: Ubuntu 22.04.5 LTS

Steps to Reproduce:

1. Close VSCode with 8 projects opened (most of them DevContainer projects)
2. Open VSCode again from terminal ( running `code --disable-extensions --trace-warnings` to disable extensions and to show warning details) without specifying any folder or file (so last working session is used opening the 8 projects)
3. Following log is printed in the terminal:
```bash
Warning: 'trace-warnings' is not in the list of known options, but still passed to Electron/Chromium.
[main 2024-11-19T10:24:29.689Z] update#setState idle
(node:471892) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 child-process-gone listeners added to [App]. MaxListeners is 10. Use emitter.setMaxListeners() to increase limit
    at genericNodeError (node:internal/errors:984:15)
    at wrappedFn (node:internal/errors:538:14)
    at _addListener (node:events:593:17)
    at App.addListener (node:events:611:10)
    at Object.X [as onWillAddFirstListener] (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:7366)
    at q (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:31:1282)
    at Na.H (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:13550)
    at Na.F (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:11857)
    at Na.start (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:15161)
    at vh.start (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:29191)
    at Object.call (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:36:4563)
    at df.s (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:19676)
    at df.q (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:19199)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:18601)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.C (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:816)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:1033)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:4908)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:964)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:5092)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:964)
    at T (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:7343)
    at IpcMainImpl.i (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:36:21224)
    at IpcMainImpl.emit (node:events:531:35)
    at WebContents.<anonymous> (node:electron/js2c/browser_init:2:85782)
    at WebContents.emit (node:events:519:28)
[main 2024-11-19T10:24:59.835Z] update#setState checking for updates
[main 2024-11-19T10:25:00.551Z] update#setState idle
```
",Security Vulnerability,Security Vulnerability
"Interactive Imputer cannot accept PLSRegression() as an estimator due to ""shape mismatch"" <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Describe the bug
When setting the estimator as PLSRegression(), a ValueError is triggered by module '_iteractive.py' in line 348, caused by ""shape mismatch""

#### Steps/Code to Reproduce

Example:
```python
import numpy as np

from sklearn.datasets import fetch_california_housing
from sklearn.cross_decomposition import PLSRegression
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer

rng = np.random.RandomState(42)

X_california, y_california = fetch_california_housing(return_X_y=True)
X_california = X_california[:400]
y_california = y_california[:400]

def add_missing_values(X_full, y_full):
    n_samples, n_features = X_full.shape

    # Add missing values in 75% of the lines
    missing_rate = 0.75
    n_missing_samples = int(n_samples * missing_rate)

    missing_samples = np.zeros(n_samples, dtype=bool)
    missing_samples[: n_missing_samples] = True

    rng.shuffle(missing_samples)
    missing_features = rng.randint(0, n_features, n_missing_samples)
    X_missing = X_full.copy()
    X_missing[missing_samples, missing_features] = np.nan
    y_missing = y_full.copy()

    return X_missing, y_missing

X_miss_california, y_miss_california = add_missing_values(
    X_california, y_california)

imputer = IterativeImputer(estimator=PLSRegression(n_components=2))

X_imputed = imputer.fit_transform(X_miss_california)
print(X_imputed)
```


#### Expected Results: after applying the workaround below:
```python
[[   8.3252       41.            6.98412698 ...    2.55555556
    37.88       -122.25930206]
 [   8.3014       21.            6.23813708 ...    2.10984183
    37.86       -122.22      ]
 [   7.2574       52.            8.28813559 ...    2.80225989
    37.85       -122.24      ]
 ...
 [   3.60438721   50.            5.33480176 ...    2.30396476
    37.88       -122.29      ]
 [   5.1675       52.            6.39869281 ...    2.44444444
    37.89       -122.29      ]
 [   5.1696       52.            6.11590296 ...    2.70619946
    37.8709526  -122.29      ]]
```

#### Actual Results
```python
File ""/home/hushsh/py3/lib/python3.6/site-packages/sklearn/impute/_iterative.py"", line 348, in _impute_one_feature
    X_filled[missing_row_mask, feat_idx] = imputed_values
ValueError: shape mismatch: value array of shape (27,1) could not be broadcast to indexing result of shape (27,) 
```

#### Versions
System:
    python: 3.6.9 (default, Oct  8 2020, 12:12:24)  [GCC 8.4.0]
executable: /home/hushsh/raid_data/py3/bin/python
   machine: Linux-5.4.0-60-generic-x86_64-with-LinuxMint-19.3-tricia

Python dependencies:
          pip: 21.0.1
   setuptools: 47.3.1
      sklearn: 0.24.1
        numpy: 1.18.1
        scipy: 1.4.1
       Cython: 0.29.15
       pandas: 1.1.3
   matplotlib: 3.1.2
       joblib: 0.14.1
threadpoolctl: 2.1.0

Built with OpenMP: True

#### My Workaround that fixed the bug: Insert the following three lines before line 348
```python
shape_imputed_values = imputed_values.shape
if len(shape_imputed_values)>1:
    # convert 2D array to 1D array fixes the bug:
    imputed_values = imputed_values.reshape(shape_imputed_values[0])
```

<!-- Thanks for contributing! -->
",Logical Bug,Logical Bug
"HTML display rendering poorly in vscode ""Dark High Contrast"" color theme ### Describe the bug

When I use vscode, I use the ""Dark High Contrast"" theme, as my eyes are tired. In this mode, some of the estimator names are not visible in the HTML display

### Steps/Code to Reproduce

Execute the following code in a vscode (for instance a cell)
```python
# %%
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import PCA
from sklearn.ensemble import HistGradientBoostingRegressor

pipe = make_pipeline(PCA(), HistGradientBoostingRegressor())
pipe
```

### Expected Results

With the ""Dark (Visual Studio)"" theme, the result is:
![image](https://github.com/user-attachments/assets/1c8d52d4-ce8c-4e8a-a217-fc68be2f2f70)


### Actual Results

However, with the ""Dark High Contrast"", the result is
![image](https://github.com/user-attachments/assets/a229f0dd-c71f-4744-9733-00a82d5258c0)

Note that the title of the enclosing meta-estimator, here ""Pipeline"", is not visible

### Versions

```shell
git main of today (last commit: 426e6be923e34f68bc720ae625c8ca258f473265, merge of #30347)

System:
    python: 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]
executable: /bin/python3
   machine: Linux-6.8.0-49-generic-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.7.dev0
          pip: 24.0
   setuptools: 68.1.2
        numpy: 1.26.4
        scipy: 1.11.4
       Cython: 3.0.11
       pandas: 2.1.4+dfsg
   matplotlib: 3.6.3
       joblib: 1.3.2
threadpoolctl: 3.1.0
```
```
",Logical Bug,Logical Bug
"ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part. Hello Scikit-learn team,

I am encountering an issue while running inference VotingClassifier model with `voting=""hard""` argument, I found that this issue may related to [NEP 34](https://numpy.org/neps/nep-0034-infer-dtype-is-object.html) restriction of `dtype=object` in numpy and the solution is downgrading to numpy `1.23.1`. However, it doesn't work in my case due to dependency conflicts with pandas and other packages. I'd appreciate if you could analyze this issue and provide an update when possible.

```
Traceback (most recent call last):
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 135, in <module>
    ensemble_model, trained_models, model_results, ensemble_results = main(sparse=False)
                                                                      ^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 127, in main
    trained_ensemble, ensemble_results = train_ensemble_model(
                                         ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 89, in train_ensemble_model
    ensemble_results, trained_ensemble = train_and_evaluate_ensemble(voting_clf, X_train, X_test, y_train, y_test)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training/ensemble_trainer.py"", line 33, in train_and_evaluate_ensemble
    y_pred_ensemble = voting_clf.predict(X_test)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py"", line 443, in predict
    predictions = self._predict(X)
                  ^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py"", line 80, in _predict
    return np.asarray([est.predict(X) for est in self.estimators_]).T
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part.
```

### Steps/Code to Reproduce

```
try:
  main_logger.info(""Training ensemble"")
  voting_clf.fit(X_train, y_train)
  
  main_logger.info(""Evaluating ensemble"")
  y_pred_ensemble = voting_clf.predict(X_test)
  results = classification_report(y_test, y_pred_ensemble, output_dict=True)
  main_logger.info(f""Ensemble Results:\n{classification_report(y_test, y_pred_ensemble)}"")
  
  return results, voting_clf

except Exception as e:
    main_logger.error(f""Error in ensemble training: {str(e)}"")
    raise
```

### Expected Results

```Finish training```

### Actual Results

```
Traceback (most recent call last):
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 135, in <module>
    ensemble_model, trained_models, model_results, ensemble_results = main(sparse=False)
                                                                      ^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 127, in main
    trained_ensemble, ensemble_results = train_ensemble_model(
                                         ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 89, in train_ensemble_model
    ensemble_results, trained_ensemble = train_and_evaluate_ensemble(voting_clf, X_train, X_test, y_train, y_test)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training/ensemble_trainer.py"", line 33, in train_and_evaluate_ensemble
    y_pred_ensemble = voting_clf.predict(X_test)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py"", line 443, in predict
    predictions = self._predict(X)
                  ^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py"", line 80, in _predict
    return np.asarray([est.predict(X) for est in self.estimators_]).T
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part.
```

### Versions

```shell
1.5.2
```
",Dependency Issue,Dependency Issue
"""inline suggestions"" spinner on the language server status item I've seen this every day in the vscode repo for the past few days at least

<img width=""305"" alt=""Image"" src=""https://github.com/user-attachments/assets/fcfb8b95-9dd3-4228-a4ae-36867fa2cfd8"" />",Syntax Error,Syntax Error
"LocallyLinearEmbedding : n_neighbors <= n_samples ### Describe the bug

Minor bug in `LocallyLinearEmbedding`'s parameter validation:

https://github.com/scikit-learn/scikit-learn/blob/70fdc843a4b8182d97a3508c1a426acc5e87e980/sklearn/manifold/_locally_linear.py#L226-L230

The `if` condition contradicts the error message in the case that `n_neighbors == N`. So you get a message like

```python-traceback
ValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3""
```

which doesn't make sense.

### Steps/Code to Reproduce

```python
import numpy as np
import sklearn.manifold

X = np.random.randn(3, 5)

embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])

embedder.fit_transform(X)
```

### Expected Results

n/a

### Actual Results

```python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1119], line 8
      4 X = np.random.randn(3, 5)
      6 embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])
----> 8 embedder.fit_transform(X)

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/utils/_set_output.py:313, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)
    311 @wraps(f)
    312 def wrapped(self, X, *args, **kwargs):
--> 313     data_to_wrap = f(self, X, *args, **kwargs)
    314     if isinstance(data_to_wrap, tuple):
    315         # only wrap the first output for cross decomposition
    316         return_tuple = (
    317             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    318             *data_to_wrap[1:],
    319         )

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:848, in LocallyLinearEmbedding.fit_transform(self, X, y)
    831 @_fit_context(prefer_skip_nested_validation=True)
    832 def fit_transform(self, X, y=None):
    833     \""\""\""Compute the embedding vectors for data X and transform X.
    834 
    835     Parameters
   (...)
    846         Returns the instance itself.
    847     \""\""\""
--> 848     self._fit_transform(X)
    849     return self.embedding_

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:795, in LocallyLinearEmbedding._fit_transform(self, X)
    793 X = self._validate_data(X, dtype=float)
    794 self.nbrs_.fit(X)
--> 795 self.embedding_, self.reconstruction_error_ = _locally_linear_embedding(
    796     X=self.nbrs_,
    797     n_neighbors=self.n_neighbors,
    798     n_components=self.n_components,
    799     eigen_solver=self.eigen_solver,
    800     tol=self.tol,
    801     max_iter=self.max_iter,
    802     method=self.method,
    803     hessian_tol=self.hessian_tol,
    804     modified_tol=self.modified_tol,
    805     random_state=random_state,
    806     reg=self.reg,
    807     n_jobs=self.n_jobs,
    808 )
    809 self._n_features_out = self.embedding_.shape[1]

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:227, in _locally_linear_embedding(X, n_neighbors, n_components, reg, eigen_solver, tol, max_iter, method, hessian_tol, modified_tol, random_state, n_jobs)
    223     raise ValueError(
    224         \""output dimension must be less than or equal to input dimension\""
    225     )
    226 if n_neighbors >= N:
--> 227     raise ValueError(
    228         \""Expected n_neighbors <= n_samples,  but n_samples = %d, n_neighbors = %d\""
    229         % (N, n_neighbors)
    230     )
    232 M_sparse = eigen_solver != \""dense\""
    234 if method == \""standard\"":

ValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3""
```

### Versions

```shell
System:
    python: 3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:47) [Clang 13.0.0 (clang-1300.0.29.30)]
executable: /usr/local/bin/python3
   machine: macOS-14.5-arm64-arm-64bit

Python dependencies:
      sklearn: 1.5.0
          pip: 24.0
   setuptools: 70.0.0
        numpy: 1.26.4
        scipy: 1.13.0
       Cython: 3.0.10
       pandas: 2.2.2
   matplotlib: 3.8.4
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 11
         prefix: libopenblas
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: blas
   internal_api: openblas
    num_threads: 11
         prefix: libopenblas
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.26.dev
threading_layer: pthreads
   architecture: neoversen1

       user_api: openmp
   internal_api: openmp
    num_threads: 11
         prefix: libomp
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```
```
",Syntax Error,Syntax Error
"SCM - Last row in these hovers are not consistent Testing #238852

Last row in these hovers are not consistent. One has background and other does not. They should either both have background or both should not

<img width=""966"" alt=""Image"" src=""https://github.com/user-attachments/assets/db8ec76e-db88-484d-916f-11742afbaacb"" />

<img width=""504"" alt=""Image"" src=""https://github.com/user-attachments/assets/2fbb4fa2-d7ef-4d8e-8b4c-22787e39f53b"" />

",Performance Issue,Performance Issue
"`tf.raw_ops.Transpose` aborts with negative `perm` value ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.Transpose` aborts with negative `perm` value. [gist](https://colab.research.google.com/drive/1r0oUxDcu-uWHgjQGOU8qvcSzD1z2fv-a?usp=sharing)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.raw_ops.Transpose(
    x=tf.random.normal([2,1]),
    perm=[-1,0],
    name=None
)
```


### Relevant log output

```shell
2024-02-20 19:35:25.981306: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
",Runtime Error,Runtime Error
"Interactive Imputer cannot accept PLSRegression() as an estimator due to ""shape mismatch"" <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Describe the bug
When setting the estimator as PLSRegression(), a ValueError is triggered by module '_iteractive.py' in line 348, caused by ""shape mismatch""

#### Steps/Code to Reproduce

Example:
```python
import numpy as np

from sklearn.datasets import fetch_california_housing
from sklearn.cross_decomposition import PLSRegression
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer

rng = np.random.RandomState(42)

X_california, y_california = fetch_california_housing(return_X_y=True)
X_california = X_california[:400]
y_california = y_california[:400]

def add_missing_values(X_full, y_full):
    n_samples, n_features = X_full.shape

    # Add missing values in 75% of the lines
    missing_rate = 0.75
    n_missing_samples = int(n_samples * missing_rate)

    missing_samples = np.zeros(n_samples, dtype=bool)
    missing_samples[: n_missing_samples] = True

    rng.shuffle(missing_samples)
    missing_features = rng.randint(0, n_features, n_missing_samples)
    X_missing = X_full.copy()
    X_missing[missing_samples, missing_features] = np.nan
    y_missing = y_full.copy()

    return X_missing, y_missing

X_miss_california, y_miss_california = add_missing_values(
    X_california, y_california)

imputer = IterativeImputer(estimator=PLSRegression(n_components=2))

X_imputed = imputer.fit_transform(X_miss_california)
print(X_imputed)
```


#### Expected Results: after applying the workaround below:
```python
[[   8.3252       41.            6.98412698 ...    2.55555556
    37.88       -122.25930206]
 [   8.3014       21.            6.23813708 ...    2.10984183
    37.86       -122.22      ]
 [   7.2574       52.            8.28813559 ...    2.80225989
    37.85       -122.24      ]
 ...
 [   3.60438721   50.            5.33480176 ...    2.30396476
    37.88       -122.29      ]
 [   5.1675       52.            6.39869281 ...    2.44444444
    37.89       -122.29      ]
 [   5.1696       52.            6.11590296 ...    2.70619946
    37.8709526  -122.29      ]]
```

#### Actual Results
```python
File ""/home/hushsh/py3/lib/python3.6/site-packages/sklearn/impute/_iterative.py"", line 348, in _impute_one_feature
    X_filled[missing_row_mask, feat_idx] = imputed_values
ValueError: shape mismatch: value array of shape (27,1) could not be broadcast to indexing result of shape (27,) 
```

#### Versions
System:
    python: 3.6.9 (default, Oct  8 2020, 12:12:24)  [GCC 8.4.0]
executable: /home/hushsh/raid_data/py3/bin/python
   machine: Linux-5.4.0-60-generic-x86_64-with-LinuxMint-19.3-tricia

Python dependencies:
          pip: 21.0.1
   setuptools: 47.3.1
      sklearn: 0.24.1
        numpy: 1.18.1
        scipy: 1.4.1
       Cython: 0.29.15
       pandas: 1.1.3
   matplotlib: 3.1.2
       joblib: 0.14.1
threadpoolctl: 2.1.0

Built with OpenMP: True

#### My Workaround that fixed the bug: Insert the following three lines before line 348
```python
shape_imputed_values = imputed_values.shape
if len(shape_imputed_values)>1:
    # convert 2D array to 1D array fixes the bug:
    imputed_values = imputed_values.reshape(shape_imputed_values[0])
```

<!-- Thanks for contributing! -->
",Logical Bug,Logical Bug
"GitHub Copilot Reference Links Disrupt NVDA Screen Reader 
Type: <b>Bug</b>

When there is a reference to GitHub in the Copilot explanation, it is difficult to understand the link to the reference, and it disrupts reading with NVDA.
See an example below.
Explanation
Context:
This code snippet appears to be part of a method for constructing or configuring an object, possibly using a design pattern like Builder.
Field [`tpHrio`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%7B%22scheme%22%3A%22file%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Ft102315%2FDocuments%2Fmdl-commons-service%2Fsrc%2Fmain%2Fjava%2Fbr%2Fcom%2Fnuclea%2Fcommonsservice%2Ftimetableconsult%2Fadapter%2Fhandler%2FStr0001RequestHandler.java%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22%22%7D%2C%22pos%22%3A%7B%22line%22%3A56%2C%22character%22%3A13%7D%7D%5D%2C%22301af23e-8e14-4623-a218-5c859776c80a%22%5D ""Go to definition""):
`.tpHrio(...)`: It is configuring the field [`tpHrio`](command:_github.copilot.openSymbolFromReferences?%5B%22%22%2C%5B%7B%22uri%22%3A%22file%22%2C%22authority%22%3A%22%22%2C%22path%22%3A%22%2Fhome%2Ft102315%2FDocuments%2Fmdl-commons-service%2Fsrc%2Fmain%2Fjava%2Fbr%2Fcom%2Fnuclea%2Fcommonsservice%2Ftimetableconsult%2Fadapter%2Fhandler%2FStr0001RequestHandler.java%22%2C%22query%22%3A%22%22%2C%22fragment%22%3A%22%22%7D%2C%22pos%22%3A%7B%22line%22%3A56%2C%22character%22%3A13%7D%7D%5D%2C%22301af23e-8e14-4623-a218-5c859776c80a%22%5D ""Go to definition"") of the object being constructed.

VS Code version: Code 1.94.2 (384ff7382de624fb94dbaf6da11977bba1ecd427, 2024-10-09T16:08:44.566Z)
OS version: Windows_NT x64 10.0.22631
Modes: Restricted

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i5-8365U CPU @ 1.60GHz (8 x 1896)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.72GB (20.14GB free)|
|Process Argv|--crash-reporter-id 5391933d-2a30-4ca5-a5e4-b4571630ea33|
|Screen Reader|yes|
|VM|0%|
</details><details><summary>Extensions (11)</summary>

Extension|Author (truncated)|Version
---|---|---
gitlens|eam|15.6.1
gc-excelviewer|Gra|4.2.62
azure-pipelines|ms-|1.247.2
python|ms-|2024.16.1
vscode-pylance|ms-|2024.10.1
remote-containers|ms-|0.389.0
remote-wsl|ms-|0.88.4
java|red|1.35.1
vscode-yaml|red|1.15.0
vscodeintellicode|Vis|1.3.1
vscode-maven|vsc|0.44.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
vscaat:30438848
c4g48928:30535728
azure-dev_surveyonecf:30548226
962ge761:30959799
pythongtdpath:30769146
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
accentitlementsc:30995553
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
724cj586:31013169
a69g1124:31058053
dvdeprecation:31068756
dwnewjupytercf:31046870
newcmakeconfigv2:31071590
impr_priority:31102340
nativerepl2:31139839
refactort:31108082
pythonrstrctxt:31112756
wkspc-onlycs-t:31132770
wkspc-ranged-t:31151552
cf971741:31144450
autoexpandse:31146404
iacca1:31156133
notype1:31157159
5fd0e150:31155592
iconenabled:31158251

```

</details>

<!-- generated by issue reporter -->",Dependency Issue,Dependency Issue
"model.fit fails when the number of rows exceeds Int32.MaxValue ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19.0-dev20241117

### Custom code

Yes

### OS platform and distribution

MacOS 15.1.0

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I would expect model.fit to handle training on extremely large NumPy arrays without limitations.

### Standalone code to reproduce the issue

```shell
import numpy as np
from keras import Sequential
from keras.layers import Dense

n = 2_147_483_648
x = np.zeros(n).astype(np.float32)
y = x

model = Sequential([
    Dense(64, activation=""relu"", input_shape=(1,)),
    Dense(1, activation=""sigmoid"")
])
model.compile(optimizer=""adam"", loss=""binary_crossentropy"")
model.fit(x=x,y=y, epochs=1, batch_size=1024, verbose=1)
```


### Relevant log output

```shell
ValueError: Invalid value in tensor used for shape: -2147483648
```
",Logical Bug,Logical Bug
"`tf.raw_ops.ConjugateTranspose`: negative value of `perm` can lead to out-of-bounds read ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In `tf.raw_ops.ConjugateTranspose`, negative value of `perm` can lead to out-of-bounds read.
[Here](https://github.com/tensorflow/tensorflow/blob/617b5e97d6a9a71e1972dfe6fead5bf460094658/tensorflow/core/kernels/transpose_functor_cpu.cc#L52),
```C++
        i_idx += ratio * in_strides[perm[i]];
```
as there is no guard which checks validity of `perm`, when its value is -1 `in_strides[perm[i]]` can be an out-of-bounds reading(I guess -1 would be interpreted as an `SIZE_T_MAX` or something).
Note that the below code ends up with absl assertion failure in debug build.

### Standalone code to reproduce the issue

```Python
import tensorflow as tf

tf.raw_ops.ConjugateTranspose(
    x=tf.random.normal([2]),
    perm=[-1])
```


### Relevant log output

Release Build:
```shell
    Outputs nothing
```

Debug Build:
```shell
python: external/com_google_absl/absl/container/inlined_vector.h:363: auto absl::InlinedVector<long, 8>::operator[](size_type)::(anonymous class)::operator()() const [T = long, N = 8, A = std::allocator<long>]: Assertion `false && ""i < size()""' failed.
Aborted (core dumped)
```
",Security Vulnerability,Security Vulnerability
"deleted files don't go to trash 
Type: <b>Bug</b>

I delete files in the explorer using `del` key of keyboard, files are deleted and cannot be found in the trash bin

VS Code version: Code 1.95.2 (e8653663e8840adaf45af01eab5c627a5af81807, 2024-11-07T11:07:22.054Z)
OS version: Linux x64 6.8.0-48-generic snap
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-1365U (12 x 3700)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 1|
|Memory (System)|31.00GB (22.61GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 8447ec3e-1827-4960-bb59-12316efae9e7|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|wayland|
</details><details><summary>Extensions (14)</summary>

Extension|Author (truncated)|Version
---|---|---
gitlens|eam|15.6.3
ftp-simple|hum|0.7.6
git-graph|mhu|1.30.0
autopep8|ms-|2024.0.0
debugpy|ms-|2024.12.0
python|ms-|2024.18.1
vscode-pylance|ms-|2024.11.1
jupyter|ms-|2024.10.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.21
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.388.0
LiveServer|rit|5.7.9


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
pythongtdpath:30769146
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
bdiig495:31013172
dvdeprecation:31068756
dwnewjupyter:31046869
impr_priority:31102340
nativerepl2:31139839
refactort:31108082
pythonrstrctxt:31112756
cf971741:31144450
iacca1:31171482
notype1cf:31157160
5fd0e150:31155592
dwcopilot:31170013
j44ff735:31177056

```

</details>

<!-- generated by issue reporter -->",Security Vulnerability,Security Vulnerability
"Invalid `Conv2d` can be executed without compilation ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

An invalid `Conv2d` can be executed without compilation. By contrast, after using `@tf.function(jit_compile=True)`, it will raise error `Negative dimension size caused by subtracting 2 from 1 for '{{node conv2d_6/Conv2D}} ...`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

""""""
Don't use @tf.function(jit_compile=True)
""""""

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.conv2d = tf.keras.layers.Conv2D(5, 2, activation=tf.nn.relu)

  def call(self, x):
    conv2d = self.conv2d(x)
    return conv2d

# Initializing the model
m = Model()

# Inputs to the model
# This input results in a Conv2D kernel that has the shape (1, 2, 3, 5).
x1 = tf.constant([[[[1., 2., 3.], [4., 5., 6.]]]], shape=[1, 1, 2, 3])

y = m(x1)
print(y.shape)
# (1, 0, 1, 5)

""""""
Using @tf.function(jit_compile=True)
""""""
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.conv2d = tf.keras.layers.Conv2D(5, 2, activation=tf.nn.relu)
  @tf.function(jit_compile=True)
  def call(self, x):
    conv2d = self.conv2d(x)
    return conv2d
# Initializing the model
m = Model()

# Inputs to the model
# This input results in a Conv2D kernel that has the shape (1, 2, 3, 5).
x1 = tf.constant([[[[1., 2., 3.], [4., 5., 6.]]]], shape=[1, 1, 2, 3])

y = m(x1)
""""""

    ValueError: Exception encountered when calling layer 'conv2d_6' (type Conv2D).
    
    Negative dimension size caused by subtracting 2 from 1 for '{{node conv2d_6/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x, conv2d_6/Conv2D/ReadVariableOp)' with input shapes: [1,1,2,3], [2,2,3,5].
    
    Call arguments received by layer 'conv2d_6' (type Conv2D):
      â¢ inputs=tf.Tensor(shape=(1, 1, 2, 3), dtype=float32)


Call arguments received by layer 'model_10' (type Model):
  â¢ x=tf.Tensor(shape=(1, 1, 2, 3), dtype=float32)
""""""
```


### Relevant log output

_No response_",Runtime Error,Runtime Error
"arguments suggested for mkdir command seems to be invalid Testing #238901

![Image](https://github.com/user-attachments/assets/7f63dc56-bb49-4376-b41e-5d83fb6674cf)

",Logical Bug,Logical Bug
"[DevTools Bug] getCommitTree(): Invalid commit ""7"" for root ""1"". There are only ""7"" commits. ### Website or app

private

### Repro steps

profile re-renders click on individual re-renders. Clicked on the last re-render and it crashed instead of displaying info.

### How often does this bug happen?

Only once

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

6.0.0-d66fa02a30

### Error message (automated)

getCommitTree(): Invalid commit ""7"" for root ""1"". There are only ""7"" commits.

### Error call stack (automated)

```text
ve/<@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1159236
CommitFlamegraphAutoSizer@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1404914
renderWithHooks@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:52244
updateFunctionComponent@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:81718
beginWork@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:95937
performUnitOfWork@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:154622
workLoopSync@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:154498
renderRootSync@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:154249
performWorkOnRoot@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:149824
performSyncWorkOnRoot@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:164884
flushSyncWorkAcrossRoots_impl@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:163205
processRootScheduleInMicrotask@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:163662
1519/ensureRootIsScheduled/<@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:162805
```


### Error component stack (automated)

```text
CommitFlamegraphAutoSizer@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1404715
div@unknown:0:0
div@unknown:0:0
div@unknown:0:0
SettingsModalContextController@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1297733
wl<@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1536567
fa@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1315530
div@unknown:0:0
div@unknown:0:0
ThemeProvider@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1318230
portaledContent/<@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1318420
div@unknown:0:0
div@unknown:0:0
div@unknown:0:0
ThemeProvider@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1318230
TimelineContextController@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1395885
ProfilerContextController@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1387636
TreeContextController@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1209953
SettingsContextController@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1238133
ModalDialogContextController@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1375334
DevTools_DevTools@moz-extension://beb769c0-41a5-4f80-ba39-12fab791919f/build/main.js:1:1544294
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=getCommitTree(): Invalid commit  for root . There are only  commits. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
",Runtime Error,Runtime Error
"Differences in scalar vs vectorized predictions with `GaussianProcessRegressor` ### Describe the bug

I would expect that calling `GaussianProcessRegressor.predict(X)` with a single X matrix, or with repeated scalar evaluations rows of X should would give nearly the same result, but they don't.

An example is given below.

### Steps/Code to Reproduce

```python
import numpy as np
import pandas as pd
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import DotProduct, RBF
from sklearn.model_selection import train_test_split


data_url = ""http://lib.stat.cmu.edu/datasets/boston""
raw_df = pd.read_csv(data_url, sep=""\s+"", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]


def evaluate_gpr_vec_and_scalar(gpr, X):
    res_vector = gpr.predict(X).squeeze()
    res_scalar = np.array([gpr.predict(xi.reshape(1, -1)) for xi in X]).squeeze()
    return res_scalar, res_vector

# load/split data
X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=0)

# train
gpr = GaussianProcessRegressor(DotProduct() + RBF(), alpha=1.)
gpr.fit(X_train, y_train)

# predict scalar and vector
pred_scalar, pred_vector = evaluate_gpr_vec_and_scalar(gpr, X_test)
err = pred_scalar - pred_vector
print(err.max())
```

### Expected Results

I would expect the error to be 0 or small - perhaps around machine `eps`, if anything.

### Actual Results

`2.1609594114124775e-08`


### Versions

```shell
System:
    python: 3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:54) [Clang 13.0.1 ]
executable: /Users/phil/miniconda3/envs/skl/bin/python
   machine: macOS-12.5.1-x86_64-i386-64bit
Python dependencies:
      sklearn: 1.1.1
          pip: 22.3
   setuptools: 65.5.0
        numpy: 1.23.4
        scipy: 1.9.3
       Cython: None
       pandas: 1.5.1
   matplotlib: 3.6.1
       joblib: 1.2.0
threadpoolctl: 3.1.0
Built with OpenMP: True
threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/phil/miniconda3/envs/skl/lib/libopenblasp-r0.3.21.dylib
        version: 0.3.21
threading_layer: openmp
   architecture: Nehalem
    num_threads: 8
       user_api: openmp
   internal_api: openmp
         prefix: libomp
       filepath: /Users/phil/miniconda3/envs/skl/lib/libomp.dylib
        version: None
    num_threads: 8
```
",Syntax Error,Syntax Error
"process.report.getReport is very slow when called in Extension Host, blocks most functionality <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->

Version: 1.97.0-insider (user setup)
Commit: cc14c75c962284dfd2e2493dd487e3b81fb15d23
Date: 2025-01-23T10:37:55.126Z
Electron: 32.2.7
ElectronBuildId: 10660205
Chromium: 128.0.6613.186
Node.js: 20.18.1
V8: 12.8.374.38-electron.0
OS: Windows_NT x64 10.0.26100

Steps to Reproduce:

1. Clone https://github.com/jakebailey/vscode-getreport-issue
2. Run the extension
3. Run the ""Hello World"" command
4. Attempt to do anything with the editor

![Image](https://github.com/user-attachments/assets/8204f381-14e1-4088-bb33-74208c27ee9f)

I discovered this as part of https://github.com/dprint/dprint-vscode/pull/99; for months my editor has been unresponsive at startup, unable to hover or even copy and paste now that there are paste providers, and this particular call is the culprit.

If I run Node directly, or call VS Code ""as node"", it does not exhibit these problems. My impression is that this has to do with VS Code patching all of the networking calls, since unfortunately `getReport` calls to the network (even though effectively all users use it to check if libc is musl or not).

Related: https://github.com/nodejs/node/issues/55576
",Performance Issue,Performance Issue
"Segmentation fault (core dumped) in `tf.config.threading.set_inter_op_parallelism_threads` ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Crash triggered when input boundary values into tf.config.threading.set_intra_op_parallelism_threads

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/12s6D2GuBFEWAdvFdCvjbs4nK888vLGvm?usp=sharing
```


### Relevant log output

```shell
2024-08-10 21:36:47.636016: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-10 21:36:47.703371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-10 21:36:47.791020: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-10 21:36:47.817978: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-10 21:36:47.883418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-10 21:36:56.611712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-08-10 21:36:56.617085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1717 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
Segmentation fault (core dumped)
```
",Runtime Error,Runtime Error
"[Compiler Bug]: Memoizing a debounced function complains about ref access ### What kind of issue is this?

- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)
- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)
- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)
- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)

### Link to repro

https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBQhAC2AUUoAlUvgC8+NWUYA6KGARqSQ8fknSClJjIDyUXAhgnN2hLtwGjJsxPOWdDL4APpgcDAQlJQAKhAAQhC4uApuhggAwmTRAEa6ANb8QpoAfCIBFnJKqqR6cLAwCHRe4ZHRcfxiEhYWKZjI+Db2js4mdQ1NLRFRlAASCEwA5gAWuAA0FT05CMtkAG4sMAOi4PIQScsnG934AL6CANwBt2v4ANoAuo8BUkEErTM4olkqktOl2AgzvxNsUNGViDloAwEPwwtN2gkkil5K8AIwABnxgmuPXeaLasUxIPkHxJ+G+dHEAUauFgbAAPIQmHsSvNohB8AB1HCUQjsgD0XJ5Tzot3EIFuQA

### Repro steps

Attempting to memoize a debounced function that access a ref gives me this react compiler lint violation:
> Ref values (the `current` property) may not be accessed during render. 

However, the underlying function isn't called during render, it's simply referenced, and only called when the memoized, debounced function is called. In my case, it's called in an event handler, which is safe.

### How often does this bug happen?

Every time

### What version of React are you using?

18.3.1",Logical Bug,Logical Bug
"search.action.focusFilesToExclude and Include not working <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.95.3
- OS Version: linux mint mate 21.3

Version: 1.95.3
Commit: https://github.com/microsoft/vscode/commit/f1a4fb101478ce6ec82fe9627c43efbf9e98c813
Date: 2024-11-13T14:50:04.152Z
Electron: 32.2.1
ElectronBuildId: 10427718
Chromium: 128.0.6613.186
Node.js: 20.18.0
V8: 12.8.374.38-electron.0
OS: Linux x64 5.15.0-126-generic

__Issue__
search.action.focusFilesToExclude and search.action.focusFilesToInclude  not working 
( the relevant inputboxes are not getting highlighted )
checked by key binding these action items 

__Steps to Reproduce:__

1. Assign keybindings alt + I and alt + O to search.action.focusFilesToInclude and search.action.focusFilesToExclude
2. set when expression for both as 'searchInputBoxFocus || searchViewletFocus'
3. now in search side bar pane focused, either focus in inputbox or the result area, trigger the newly set keybindings to focus the include or exclude input boxes. 
We see no transfer of focus to these input boxes.

When checked with developer: keyboard shortcuts troubleshooting
the keybindings do get triggered, but relevant action is not executed.


__Additionally,__
only action item is present to focus these 2 include & exclude boxes
there is __NO when clause__ items to check while triggering keybindings
focusFilesToInclude & focusFilesToExclude







",UI/UX Bug,UI/UX Bug
"LocallyLinearEmbedding : n_neighbors <= n_samples ### Describe the bug

Minor bug in `LocallyLinearEmbedding`'s parameter validation:

https://github.com/scikit-learn/scikit-learn/blob/70fdc843a4b8182d97a3508c1a426acc5e87e980/sklearn/manifold/_locally_linear.py#L226-L230

The `if` condition contradicts the error message in the case that `n_neighbors == N`. So you get a message like

```python-traceback
ValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3""
```

which doesn't make sense.

### Steps/Code to Reproduce

```python
import numpy as np
import sklearn.manifold

X = np.random.randn(3, 5)

embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])

embedder.fit_transform(X)
```

### Expected Results

n/a

### Actual Results

```python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1119], line 8
      4 X = np.random.randn(3, 5)
      6 embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])
----> 8 embedder.fit_transform(X)

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/utils/_set_output.py:313, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)
    311 @wraps(f)
    312 def wrapped(self, X, *args, **kwargs):
--> 313     data_to_wrap = f(self, X, *args, **kwargs)
    314     if isinstance(data_to_wrap, tuple):
    315         # only wrap the first output for cross decomposition
    316         return_tuple = (
    317             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    318             *data_to_wrap[1:],
    319         )

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:848, in LocallyLinearEmbedding.fit_transform(self, X, y)
    831 @_fit_context(prefer_skip_nested_validation=True)
    832 def fit_transform(self, X, y=None):
    833     \""\""\""Compute the embedding vectors for data X and transform X.
    834 
    835     Parameters
   (...)
    846         Returns the instance itself.
    847     \""\""\""
--> 848     self._fit_transform(X)
    849     return self.embedding_

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:795, in LocallyLinearEmbedding._fit_transform(self, X)
    793 X = self._validate_data(X, dtype=float)
    794 self.nbrs_.fit(X)
--> 795 self.embedding_, self.reconstruction_error_ = _locally_linear_embedding(
    796     X=self.nbrs_,
    797     n_neighbors=self.n_neighbors,
    798     n_components=self.n_components,
    799     eigen_solver=self.eigen_solver,
    800     tol=self.tol,
    801     max_iter=self.max_iter,
    802     method=self.method,
    803     hessian_tol=self.hessian_tol,
    804     modified_tol=self.modified_tol,
    805     random_state=random_state,
    806     reg=self.reg,
    807     n_jobs=self.n_jobs,
    808 )
    809 self._n_features_out = self.embedding_.shape[1]

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:227, in _locally_linear_embedding(X, n_neighbors, n_components, reg, eigen_solver, tol, max_iter, method, hessian_tol, modified_tol, random_state, n_jobs)
    223     raise ValueError(
    224         \""output dimension must be less than or equal to input dimension\""
    225     )
    226 if n_neighbors >= N:
--> 227     raise ValueError(
    228         \""Expected n_neighbors <= n_samples,  but n_samples = %d, n_neighbors = %d\""
    229         % (N, n_neighbors)
    230     )
    232 M_sparse = eigen_solver != \""dense\""
    234 if method == \""standard\"":

ValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3""
```

### Versions

```shell
System:
    python: 3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:47) [Clang 13.0.0 (clang-1300.0.29.30)]
executable: /usr/local/bin/python3
   machine: macOS-14.5-arm64-arm-64bit

Python dependencies:
      sklearn: 1.5.0
          pip: 24.0
   setuptools: 70.0.0
        numpy: 1.26.4
        scipy: 1.13.0
       Cython: 3.0.10
       pandas: 2.2.2
   matplotlib: 3.8.4
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 11
         prefix: libopenblas
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: blas
   internal_api: openblas
    num_threads: 11
         prefix: libopenblas
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.26.dev
threading_layer: pthreads
   architecture: neoversen1

       user_api: openmp
   internal_api: openmp
    num_threads: 11
         prefix: libomp
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```
```
",Syntax Error,Syntax Error
"Hang when fitting `SVC` to a specific dataset ### Describe the bug

I am trying to fit an [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) to a specific dataset. The training process gets stuck, never finishing.

scikit-learn uses a fork of LIBSVM [version 3.10.0](https://github.com/scikit-learn/scikit-learn/blame/caaa1f52a0632294bf951a9283d015f7b5dd5dd5/sklearn/svm/src/libsvm/svm.h#L4) from [2011](https://github.com/cjlin1/libsvm/releases/tag/v310). The equivalent code using a newer version of LIBSVM succeeds, suggesting that there is an upstream bug fix that scikit-learn could merge in.

### Steps/Code to Reproduce

[libsvm_problematic_dataset.csv](https://github.com/user-attachments/files/17927924/libsvm_problematic_dataset.csv)

```python
import logging

from polars import read_csv
from sklearn.svm import SVC

_logger = logging.getLogger(__name__)


def main():
    dataset = read_csv(
        source='libsvm_problematic_dataset.csv'
    )

    x = dataset.select('feature').to_numpy()
    y = dataset['label'].to_numpy()

    _logger.info(""Attempting to reproduce issue. If reproduced, the program will not exit."")

    SVC(
        C=100,
        kernel='poly',
        degree=4,
        gamma=0.9597420397825849,
        tol=0.01,
        cache_size=1000,
        class_weight={
            0: 1.04884106,
            1: 0.95550528
        },
        verbose=True
    ).fit(X=x, y=y)

    _logger.error(""The issue was not reproduced."")


if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)

    main()
```

### Expected Results

```
INFO:__main__:Attempting to reproduce issue. If reproduced, the program will not exit.
.................................................................................................
WARNING: using -h 0 may be faster
*..............................
WARNING: using -h 0 may be faster
*.............
WARNING: using -h 0 may be faster
*..................................................................
WARNING: using -h 0 may be faster
*..........................
WARNING: using -h 0 may be faster
*..........
WARNING: using -h 0 may be faster
*..............
WARNING: using -h 0 may be faster
*................
WARNING: using -h 0 may be faster
*................
WARNING: using -h 0 may be faster
*...........................
WARNING: using -h 0 may be faster
*..............
WARNING: using -h 0 may be faster
*.............
WARNING: using -h 0 may be faster
*.............
WARNING: using -h 0 may be faster
*.............................
WARNING: using -h 0 may be faster
*............................................
WARNING: using -h 0 may be faster
*.......
WARNING: using -h 0 may be faster
*......................
WARNING: using -h 0 may be faster
*.............
WARNING: using -h 0 may be faster
*.
WARNING: using -h 0 may be faster
*
optimization finished, #iter = 460766
obj = -245114.248664, rho = 1.000020
nSV = 2452, nBSV = 2450
Total nSV = 2452
ERROR:__main__:The issue was not reproduced.
```

This expected result was generated using LIBSVM [version 3.30.0](https://pypi.org/project/libsvm-official/3.30.0/) with the following code:
```python
import logging

from libsvm.svmutil import svm_train
from polars import read_csv

_logger = logging.getLogger(__name__)


def main():
    dataset = read_csv(
        source='libsvm_problematic_dataset.csv'
    )

    x = dataset.select('feature').to_numpy()
    y = dataset['label'].to_numpy()

    _logger.info(""Attempting to reproduce issue. If reproduced, the program will not exit."")

    svm_train(
        y, x,
        [
            '-s', 0,
            '-t', 1,
            '-d', 4,
            '-g', 0.9597420397825849,
            '-c', 100,
            '-m', 1000,
            '-e', 0.01,
            '-w0', 1.04884106,
            '-w1', 0.95550528
        ]
    )

    _logger.error(""The issue was not reproduced."")


if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)

    main()
```

### Actual Results

```
INFO:__main__:Attempting to reproduce issue. If reproduced, the program will not exit.
[LibSVM].....................................................
Warning: using -h 0 may be faster
*............
Warning: using -h 0 may be faster
*.....
Warning: using -h 0 may be faster
*.................
Warning: using -h 0 may be faster
*.....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
```

The program never exits.

### Versions

```shell
System:
    python: 3.11.10 | packaged by conda-forge | (main, Oct 16 2024, 01:26:25) [Clang 17.0.6 ]
executable: /opt/homebrew/Caskroom/miniforge/base/envs/libsvm-debugging/bin/python
   machine: macOS-14.7.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.5.2
          pip: 24.3.1
   setuptools: 75.6.0
        numpy: 2.1.3
        scipy: 1.14.1
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 10
         prefix: libopenblas
       filepath: /opt/homebrew/Caskroom/miniforge/base/envs/libsvm-debugging/lib/libopenblas.0.dylib
        version: 0.3.28
threading_layer: openmp
   architecture: VORTEX

       user_api: openmp
   internal_api: openmp
    num_threads: 10
         prefix: libomp
       filepath: /opt/homebrew/Caskroom/miniforge/base/envs/libsvm-debugging/lib/libomp.dylib
        version: None
```
",Syntax Error,Syntax Error
"Terminal Command Truncation For a long time, my Visual Studio Code setup, along with its extensions, functioned flawlessly and significantly improved my workflow. However, at some point, I began experiencing issues with certain extensions. Notably, when debugging C++ code with CodeLLDB or CMake Tools, the commands sent to the terminal would occasionally be truncated, preventing the debugger from launching properly.

Recently, I discovered that setting `""terminal.integrated.defaultProfile.osx"": ""bash""` resolved the issue, while switching back to zsh caused it to resurface. Later, after transferring some conda and jenv initialization settings from `.zshrc` to `.bash_profile`, I found that the problem began to intermittently occur in bash as well. Interestingly, the more configuration scripts I added, the more consistently the issue appeared.

Recognizing a pattern, I decided to investigate further by reviewing discussions and documentation in the projects GitHub repository.

## Historical Discussions

An issue similar to what I am currently experiencing was first raised back in 2017 as [#38137](https://github.com/microsoft/vscode/issues/38137). Although it did not provide a direct solution to my problem and was eventually closed because it could not be reproduced, I would like to acknowledge [@Tyriar](https://github.com/Tyriar) for his extensive contributions in that discussion, where he referenced several related issues that offered valuable insights.

One notable comment came from [@fabiospampinato](https://github.com/microsoft/vscode/issues/38137#issuecomment-352450960), who mentioned that the command truncation only happens the first time text is sent to the terminal, but works properly afterwards.

The character limit at which commands are truncated seems to vary across different operating systems. According to [#63613](https://github.com/microsoft/vscode/issues/63613), commands exceeding 1568 characters are truncated on Windows. Meanwhile, issues such as [#59135](https://github.com/microsoft/vscode/issues/59135), [#87183](https://github.com/microsoft/vscode/issues/87183), [#130736](https://github.com/microsoft/vscode/issues/130736), and [#134324](https://github.com/microsoft/vscode/issues/134324) indicate that on macOS, this limit is 1024 characters, which aligns with my observations.

Additionally, issues [#96973](https://github.com/microsoft/vscode/issues/96973) and [#61999](https://github.com/microsoft/vscode/issues/61999) provided effective testing methods for the ""Run selected text in active terminal"" functionality. Additionally, [#136587](https://github.com/microsoft/vscode/issues/136587#issuecomment-966510277) offered an approach for testing using `launch.json`, which significantly simplified my process of reproducing this issue.

I also found the [enable trace logging](https://github.com/microsoft/vscode/wiki/Terminal-Issues#enabling-trace-logging) guide in the projects wiki, which helped me expedite the process of identifying the cause of the problem.

## Steps to Reproduce

1.  Modify `~/.zshrc`. The configurations for `oh-my-zsh`, `jenv`, and `conda` can introduce delays during zsh initialization. To simulate this, add a `sleep` command with timestamps for tracking:
```bash
gdate ""+%Y-%m-%d %H:%M:%S.%3N""
sleep 3
gdate ""+%Y-%m-%d %H:%M:%S.%3N""
```

2. Configure `settings.json`. Set the terminal settings to use zsh and disable environment inheritance:
```json
{
  ""terminal.external.osxExec"": ""Terminal.app"",
  ""terminal.integrated.defaultProfile.osx"": ""zsh"",
  ""terminal.integrated.inheritEnv"": false
}
```

3. Set Log Level to Trace.
4. Verify that all Terminal instances are closed to ensure the next session undergoes full `.zshrc` initialization.
5. Create a text file containing a single line exceeding 1024 characters. Below is an example, where the space after `256`marks the 1024-character boundary:
```txt
001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257
```

6. Select the entire text and use **Terminal: Run Selected Text in Active Terminal**. Observe that commands beyond 1024 characters, starting from `257`, are truncated.
7. Examine the logs for details on the truncation and potential causes.
![Image](https://github.com/user-attachments/assets/bb8cdd15-4b1b-44be-86d8-d119b38844b7)

## Cross-referencing the code and logs

In the [ptyhost.log](https://github.com/user-attachments/files/17680208/ptyhost.log), I observed the following two log entries. The line:
```log
2024-11-06 22:28:02.211 [trace] node-pty.IPty#write 251 252 253 254 255 256 257
```
is the last time the text I entered appeared in full. This log entry was generated by [this._logService.trace('node-pty.IPty#write', object.data);](https://github.com/microsoft/vscode/blob/024999d114e2d9dccd8472a03a17fb0f97c1349e/src/vs/platform/terminal/node/terminalProcess.ts#L522). At this point, the input remains intact. The data is then passed through [this._ptyProcess!.write(object.data);](https://github.com/microsoft/vscode/blob/024999d114e2d9dccd8472a03a17fb0f97c1349e/src/vs/platform/terminal/node/terminalProcess.ts#L526C4-L526C41) into the `node-pty` module, where it is further processed by [this._socket.write(data);](https://github.com/microsoft/node-pty/blob/8bdbd712f40acb939dbab1def8a1c3a815254f74/src/unixTerminal.ts#L178). This method facilitates communication with the C++ layer, which writes the data to the master side of the pseudo-terminal created via [int ret = openpty(&master, &slave, nullptr, NULL, static_cast<winsize*>(&winp));](https://github.com/microsoft/node-pty/blob/8bdbd712f40acb939dbab1def8a1c3a815254f74/src/unix/pty.cc#L478).

The line:
```log
2024-11-06 22:28:02.211 [trace] node-pty.IPty#onData 251 252 253 254 255 256 
```
marks the first instance of text truncation. In the `node-pty` project, when the master side of the pseudo-terminal receives data, it triggers the [public get onData(): IEvent<string> { return this._onData.event; }](https://github.com/microsoft/node-pty/blob/8bdbd712f40acb939dbab1def8a1c3a815254f74/src/terminal.ts#L44) event, allowing subscribed listeners to capture the incoming data. In `vscode`, the `onData` event is subscribed to, and the captured data is logged via [this._logService.trace('node-pty.IPty#onData', data);](https://github.com/microsoft/vscode/blob/5cae08d2afa91f2703d1f5f3e4dd8ad358501424/src/vs/platform/terminal/node/terminalProcess.ts#L320).

Notably, I observed two occurrences of the `gdate` timestamp output from `.zshrc` in the `onData` logs:

```log
2024-11-06 22:28:02.153 [trace] node-pty.IPty#onData 2024-11-06 22:28:02.152
2024-11-06 22:28:05.175 [trace] node-pty.IPty#onData 2024-11-06 22:28:05.174
```
Additionally, after the shell finished loading `.zshrc`, there was another instance of truncated output:
```log
2024-11-06 22:28:05.229 [trace] node-pty.IPty#onData 2
2024-11-06 22:28:05.229 [trace] node-pty.IPty#onData 5
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData 5
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData 2
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData 5
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData 6
2024-11-06 22:28:05.230 [trace] node-pty.IPty#onData
```
Since the pseudo-terminals slave side in the `node-pty` project is configured with `ECHO` mode enabled ([`term->c_lflag = ICANON | ISIG | IEXTEN | ECHO | ECHOE | ECHOK | ECHOKE | ECHOCTL;`](https://github.com/microsoft/node-pty/blob/8bdbd712f40acb939dbab1def8a1c3a815254f74/src/unix/pty.cc#L321)), the echo back at `2024-11-06 22:28:02.211` corresponds to the data being written to the slave's buffer by the master. At this point, the shell on the slave side is still initializing `.zshrc` and has not consumed any data from the buffer.

Once the shell finishes initialization and begins consuming data from the slave's buffer, this data is echoed back to the slave and subsequently captured by the master, triggering the corresponding `onData` events.

## Conclusion

In summary, on my macOS system, the issue of commands exceeding 1024 characters being truncated in the terminal stems from how Visual Studio Code interacts with the pseudo-terminal via `node-pty`. During shell initialization, which involves blocking processes, VSCode continues to write data to the slave side of the pseudo-terminal. Once the buffer reaches its maximum capacity, any additional data is discarded. When the shell resumes, it can only read as much data as fits within the buffer's limit, resulting in truncated commands.",Dependency Issue,Dependency Issue
"Updates not being installed automatically 
Type: <b>Bug</b>

I am having manually install all the updates.

VS Code version: Code 1.95.1 (65edc4939843c90c34d61f4ce11704f09d3e5cb6, 2024-10-31T05:14:54.222Z)
OS version: Linux x64 6.8.0-48-generic
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 4700U with Radeon Graphics (8 x 3891)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|4, 4, 4|
|Memory (System)|30.75GB (10.95GB free)|
|Process Argv|--crash-reporter-id c456e7cc-403b-4b0c-be5f-c2666a60695d|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|
</details><details><summary>Extensions (9)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-intelephense-client|bme|1.12.6
browserstack-vscode|bro|1.2.4
githistory|don|0.6.20
gitlens|eam|16.0.1
vscode-mysql|for|0.5.0
mysql-syntax|jak|1.3.1
php-cs-fixer|jun|0.3.21
vscode-docker|ms-|1.29.3
remote-containers|ms-|0.388.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
724cj586:31013169
dvdeprecation:31068756
dwnewjupyter:31046869
impr_priority:31102340
nativerepl2:31139839
pythonrstrctxt:31112756
cf971741:31144450
iacca1:31171482
notype1cf:31157160
5fd0e150:31155592
dwcopilot:31170013
stablechunks:31181875

```

</details>

<!-- generated by issue reporter -->",Dependency Issue,Dependency Issue
"VS Code re-installing extensions immediately after uninstalling <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No
Probably not since they need to be present
<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- Version: 1.95.3 (user setup)
Commit: f1a4fb101478ce6ec82fe9627c43efbf9e98c813
Date: 2024-11-13T14:50:04.152Z
Electron: 32.2.1
ElectronBuildId: 10427718
Chromium: 128.0.6613.186
Node.js: 20.18.0
V8: 12.8.374.38-electron.0
OS: Windows_NT x64 10.0.22631
- OS Version: 

Steps to Reproduce:

1. Install VS Code
2. Enable syncing of profiles
3. Try to uninstall an extension you have installed in your profile

Expected:
The extension to remain uninstalled

Actual:
The extension is re-installed within seconds of having uninstalled it.  I tried to upload a video to show the issue but it keeps failing.  

### Workaround
Thankfully I have the old computer still handy that originated the sync data.  If I uninstall the extension from the original computer, ~~the change is then propogated to my new computer~~ (edited because my observation was wrong) I can then uninstall the extension on the other computer and it remains uninstalled.  

",Security Vulnerability,Security Vulnerability
"BaggingClassifier uses Class Label as Index to Array when Voting <!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->
BaggingClassifier uses Class Label as Index to Array when Voting

#### Steps/Code to Reproduce

Provide a base estimator to _BaggingClassifier_ that does not define the function _predict_proba_. This results in _BaggingClassifier_ resorting to voting. It appears the code for performing voting uses class labels as array indices instead of looking up the index of the class label in the _classes__ member.

Example:
```python
import numpy as np
from sklearn.ensemble import BaggingClassifier

class Foo:
    
    def __init__(self):
        pass
    
    def fit(self, X, Y, W=None):
        return self
    
    def predict(self, X):
        return np.full(X.shape[0], True, np.bool)
    
    def score(self, X, Y):
        YH = self.predict(X)
        return (Y == YH).mean()
    
    def get_params(self, deep=True):
        return {}
    
    def set_params(self, **params):
        for k, v in params:
            setattr(self, k, v)
        return self
    
# %%
A = np.random.rand(10, 4)
Y = np.random.randint(2, size=10, dtype=np.bool)
bc = BaggingClassifier(Foo())
bc.fit(A, Y)
YH = bc.predict(A)
print('BaggingClassifier Voting Result: ')
print(YH)
print('Ensemble Member Predictions: ')
for Ei in bc.estimators_:
    print(Ei.predict(A))
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
In the above code snippet, _BaggingClassifier_ should return an array of _True_ since it is the majority prediction of all ensemble members.

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->
_BaggingClassifier_ returns an array of False. This issue only occurs when the base estimator does not define the function _predict_proba_.

The issue appears to be due to lines 137 and 140 in ensemble/bagging.py.

https://github.com/scikit-learn/scikit-learn/blob/e14ac6d36d9dd069cc7fc2e51e4973514a003591/sklearn/ensemble/bagging.py#L137

https://github.com/scikit-learn/scikit-learn/blob/e14ac6d36d9dd069cc7fc2e51e4973514a003591/sklearn/ensemble/bagging.py#L140

The predictions of the ensemble members are directly used as indices into the original array. I'm guessing the prediction labels need to be converted into class labels using _estimator.classes__.

#### Versions
System:
    python: 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]
executable: C:\Users\XXXXXX\Anaconda3\pythonw.exe
   machine: Windows 2012 ServerR2

BLAS:
    macros: 
  lib_dirs: 
cblas_libs: cblas

Python deps:
       pip: 18.1
setuptools: 40.6.3
   sklearn: 0.20.1
     numpy: 1.15.4
     scipy: 1.1.0
    Cython: 0.29.2
    pandas: 0.23.4

<!-- Thanks for contributing! -->
",Logical Bug,Logical Bug
"`_BaseEncoder` with boolean `categories_` that include `nan` fails on `transform` when `X` is boolean ### Describe the bug

An `Encoder` that was fitted on a `DataFrame` with boolean columns that include `NaN` will fail when transforming a boolean `X` due to a mismatch in the `dtype`s when calling `_check_unknown`. Since `X` has no `object` `dtype`, there is an attempt to call `np.isnan(known_values)`, which fails because `known_values` _does_ have an `object` `dtype`.

As far as I can tell, this can be fixed by casting the `dtype` of `values` in [`_check_unknown`](https://github.com/scikit-learn/scikit-learn/blob/7e8ad632ff/sklearn/utils/_encode.py#L243) to the `dtype` of `known_values`:
```python
if values.dtype != known_values.dtype:
     values = values.astype(known_values.dtype)
```

### Steps/Code to Reproduce

```python
import pandas as pd

from sklearn.preprocessing import OrdinalEncoder

x = pd.DataFrame({'a': [True, False, np.nan]})
o = OrdinalEncoder()
o.fit_transform(x)

y = pd.DataFrame({'a': [True, True, False]})
o.transform(y)
```

### Expected Results

I expect the array to be transformed according to the known classes:
```python
array([[1.],
       [1.],
       [0.]])
```

### Actual Results

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[1], line 10
      7 o.fit_transform(x)
      9 y = pd.DataFrame({'a': [True, True, False]})
---> 10 o.transform(y)

File ~/miniconda3/envs/analytics-models-v2/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)
    293 @wraps(f)
    294 def wrapped(self, X, *args, **kwargs):
--> 295     data_to_wrap = f(self, X, *args, **kwargs)
    296     if isinstance(data_to_wrap, tuple):
    297         # only wrap the first output for cross decomposition
    298         return_tuple = (
    299             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    300             *data_to_wrap[1:],
    301         )

File ~/miniconda3/envs/analytics-models-v2/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:1578, in OrdinalEncoder.transform(self, X)
   1564 """"""
   1565 Transform X to ordinal codes.
   1566 
   (...)
   1575     Transformed input.
   1576 """"""
   1577 check_is_fitted(self, ""categories_"")
-> 1578 X_int, X_mask = self._transform(
   1579     X,
   1580     handle_unknown=self.handle_unknown,
   1581     force_all_finite=""allow-nan"",
   1582     ignore_category_indices=self._missing_indices,
   1583 )
   1584 X_trans = X_int.astype(self.dtype, copy=False)
   1586 for cat_idx, missing_idx in self._missing_indices.items():

File ~/miniconda3/envs/analytics-models-v2/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:206, in _BaseEncoder._transform(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)
    204 Xi = X_list[i]
    205 breakpoint()
--> 206 diff, valid_mask = _check_unknown(Xi, self.categories_[i], return_mask=True)
    208 if not np.all(valid_mask):
    209     if handle_unknown == ""error"":

File ~/miniconda3/envs/analytics-models-v2/lib/python3.11/site-packages/sklearn/utils/_encode.py:307, in _check_unknown(values, known_values, return_mask)
    304         valid_mask = np.ones(len(values), dtype=bool)
    306 # check for nans in the known_values
--> 307 if np.isnan(known_values).any():
    308     diff_is_nan = np.isnan(diff)
    309     if diff_is_nan.any():
    310         # removes nan from valid_mask

TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```

### Versions

```shell
System:
    python: 3.11.8 (main, Feb 26 2024, 15:43:17) [Clang 14.0.6 ]
executable: ~/miniconda3/envs/analytics-models-v2/bin/python
   machine: macOS-10.16-x86_64-i386-64bit

Python dependencies:
      sklearn: 1.4.1.post1
          pip: 23.3.1
   setuptools: 68.2.2
        numpy: 1.26.4
        scipy: 1.13.0
       Cython: None
       pandas: 2.1.4
   matplotlib: 3.8.4
       joblib: 1.4.0
threadpoolctl: 3.4.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 6
         prefix: libopenblas
       filepath: ~/miniconda3/envs/analytics-models-v2/lib/libopenblasp-r0.3.21.dylib
        version: 0.3.21
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 12
         prefix: libomp
       filepath: ~/miniconda3/envs/analytics-models-v2/lib/python3.11/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```
",Runtime Error,Syntax Error
"All <span> tags in hover have a 4px bottom margin which causes nested spans to have stacking bottom margins Type: <b>Bug</b>

When an extension provides a hover using the `HoverProvider.provideHover` API, and the hover contains Markdown with HTML in it, every span gets a 4px bottom margin. This was added in https://github.com/microsoft/vscode/pull/107442 due to  https://github.com/microsoft/vscode-pull-request-github/issues/1937 

This means that nested spans will get stacking margins, and vertically stacked spans will have gaps between them which may be undesirable depending on how the author of the extension wants to style the hover.

Because of the (understandably) restrictive CSS allowed in `markdownRenderer.ts`, we cannot remove this bottom margin. 

I would consider the stacking margins with nested spans to be a bug, because nested spans may be required to do certain types of styling of text and it does not seem correct for that to cause the margin to expand with each level of nesting.

Demonstration of problem:
![Image](https://github.com/user-attachments/assets/56ef4a6b-6951-4c61-87af-a8d34d261abc)

To reproduce this:
1. build and install the extension at https://github.com/navtej-ac/vscode-hover-sample-extension ... It is based on the Hello World Minimal Sample from the `vscode-extension-samples` repo.
2. With the extension installed, activate it by running the ""Hello World"" command 
3. Then hover on any line the text editor. On every other line we show the hover with background colors and demonstrate both the gap between spans and the background color and the stacking of margins. On the remaining lines we try to show a hover with `margin:0` added to the spans' styling, which causes the HTML sanitizer to strip the styles

Expected: spans with background colors can nest without the outer span increasing in height and spans with backgrounds do not have a vertical gap between them.

Actual: nested spans all have 4px margins which accumulate and vertically stacked spans have a gap between them.

Proposed Fix: Add `margin:0` to the list of allowed styles in `markdownRenderer.ts`.

VS Code version: Code 1.94.2 (Universal) (384ff7382de624fb94dbaf6da11977bba1ecd427, 2024-10-09T16:08:44.566Z)
OS version: Darwin arm64 23.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|11, 9, 9|
|Memory (System)|16.00GB (0.40GB free)|
|Process Argv|--disable-extensions --crash-reporter-id 05b8a457-1897-4b62-8507-c739f0441e0b|
|Screen Reader|no|
|VM|0%|
</details>Extensions disabled<details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
vscaac:30438847
c4g48928:30535728
azure-dev_surveyone:30548225
vscrpc:30673769
2i9eh265:30646982
962ge761:30959799
pythongtdpath:30769146
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
f3je6385:31013174
dvdeprecation:31068756
dwnewjupyter:31046869
impr_priority:31102340
nativerepl1:31139838
refactort:31108082
pythonrstrctxt:31112756
wkspc-onlycs-t:31132770
wkspc-ranged-t:31151552
cf971741:31144450
defaultse:31146405
iacca2:31156134
notype1:31157159
5fd0e150:31155592
dwcopilot:31164048
iconenabled:31158251

```

</details>

<!-- generated by issue reporter -->",UI/UX Bug,UI/UX Bug
"Recursive variable font is rendered with wrong space width <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.96.2
- OS Version: Windows 11 24H2

Steps to Reproduce:

1. Install the VF version of Recursive font (Recursive_VF_1.085.ttf) from www.recursive.design
2. Change the editor font to ""Recursive Mono Linear""

    `settings.json`:
    ```json
    {
        ""editor.fontFamily"": ""'Recursive Mono Linear'""
    }
    ```

3. Observe that spaces are too narrow:
    ![Image](https://github.com/user-attachments/assets/d8fdcf39-384c-434d-870e-5e1c748cbeda)
    compared to e.g. Consolas:
    ![Image](https://github.com/user-attachments/assets/59bf142f-d0ff-466c-b9c3-1832700169d6)
    The bug is not in the font itself: it renders just fine in Zed for Windows.
    ![Image](https://github.com/user-attachments/assets/06b5485d-816e-4fb2-8e79-ac5a1c331d65)


",UI/UX Bug,UI/UX Bug
"Could not find device for node GenerateBoundingBoxProposals ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using tf.image.generate_bounding_box_proposals in CPU, it raises the following error: 

```
NotFoundError: Could not find device for node: {{node GenerateBoundingBoxProposals}} = GenerateBoundingBoxProposals[post_nms_topn=300]
All kernels registered for op GenerateBoundingBoxProposals:
  device='GPU'
 [Op:GenerateBoundingBoxProposals] name: 
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

scores = tf.constant([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4]])
bbox_deltas = tf.constant([[1, 1, 1, 1], [2, 2, 2, 2]])
image_info = tf.constant([100, 100, 1])
anchors = tf.constant([[10, 10, 20, 20], [30, 30, 40, 40], [50, 50, 60, 60]])

result = tf.image.generate_bounding_box_proposals(scores, bbox_deltas, image_info, anchors)

print(result)
```


### Relevant log output

_No response_",Dependency Issue,Dependency Issue
"SpectralClustering with assign_labels='discretize' returns less clusters than specified in n_clusters I'm not sure if this is a bug or an undocumented feature (or at least I couldn't find any info about it).

Using sklearn version 0.22.1.

The following code is producing the behavior mentioned in the title:

```python
clustering = SpectralClustering(n_clusters=4, affinity='rbf', assign_labels='discretize', random_state=0).fit(X)
np.unique(clustering.labels_) shows just 3 labels. 
```

X is a np.array of shape (258, 257).
This happens only with some random seeds. I was not able to reproduce it with assign_labels='kmeans' (running multiple times).
Also the labels of the clusters are not consecutive as if less than n_clusters where picked out of the n_components.",Logical Bug,Logical Bug
"Bug: Input minLength not working after type changed I have an input with type password with minLength 8, and i have a toggle to change the type to text (password toggle).
when i type with length < 8 and press submit, it prevent to submit. but, when i toggle the password so it change the type to text and submit the form, it can submitted. btw i using useRef()

React version:
react: ^17.0.1
react-dom: ^17.0.1

## Steps To Reproduce

1. type character < minLength
2. submit form
3. toggle form type
4. submit form

Link to code example: [DEMO](https://codesandbox.io/s/young-shape-4tciw)

## The current behavior
The form get submitted even when it under minLength

## The expected behavior
The form not get submitted when it under minLength
",Security Vulnerability,Security Vulnerability
"[DevTools Bug]: devtools does not works on null origin(sandbox) ### Website or app

none

### Repro steps

## How to reproduce?
Using the development version of React on a local server with sandbox specified in the Content-Security-Policy header reproduces this problem.
For example, in webpack-dev-server, apply the following configuration: 
```ts
  devServer: {
    headers: {
      ""Content-Security-Policy"": ""sandbox allow-scripts"",
      ""Access-Control-Allow-Origin"": ""*"",
    },
  },
```


### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Syntax Error,Syntax Error
"Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Macos 15.3 (worker 0) Macos 12.7.6(worker 1)

### Mobile device

_No response_

### Python version

3.8.20

### Bazel version

...

### GCC/compiler version

16.0.0 (apple M3) 14.0.0 (intel iris)

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M3 and Intel Iris Graphics 6100

### Current behavior?

When I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  #Build the model under the strategy

No error message is displayed, but the process no longer progresses after

â¢	 I followed the recommendations of the official documentation, but the problem persists.

### Standalone code to reproduce the issue

```shell
import json
import os
import numpy as np
import tensorflow as tf

TF_CONFIG = {
    ""cluster"": {
        ""worker"": [""192.168.0.68:12345"", ""192.168.0.68:12346""]
    },
    ""task"": {""type"": ""worker"", ""index"": 0}  # Modifier index pour chaque worker
}

os.environ[""TF_CONFIG""] = json.dumps(TF_CONFIG)

# Manually Load the MNIST dataset
data = np.load(""mnist.npz"")
x_train, y_train = data[""x_train""], data[""y_train""]
x_test, y_test = data[""x_test""], data[""y_test""]

# Normalize images
x_train, x_test = x_train / 255.0, x_test / 255.0

# Add a dimension to match TensorFlow's expectations
x_train = x_train[..., np.newaxis]
x_test = x_test[..., np.newaxis]

# Define the distribution strategy
strategy = tf.distribute.MultiWorkerMirroredStrategy()

# Build the model under the strategy
with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))

test_loss, test_acc = model.evaluate(x_test, y_test)
print(f""Test accuracy: {test_acc:.4f}"")
```

### Relevant log output

```shell
2025-02-08 12:29:20.630247: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3
2025-02-08 12:29:20.630286: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB
2025-02-08 12:29:20.630293: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB
2025-02-08 12:29:20.630324: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-02-08 12:29:20.630338: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-02-08 12:29:20.631249: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-02-08 12:29:20.631259: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-02-08 12:29:20.632347: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:449] Started server with target: grpc://192.168.0.68:12345
2025-02-08 12:29:20.637550: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 14287335759644278642
2025-02-08 12:29:20.637654: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:298] Coordination agent has successfully connected.
2025-02-08 12:29:37.728182: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 15227140312468372989
```",Runtime Error,Runtime Error
"ImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.13.0

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.1.1

### GCC/Compiler version

9.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I build the lastest TensorFlow code from source successfully with
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

Then I generate a TensorFlow whl successfully with
`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`

But when I pip install this whl, import tensorflow got ""ImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory"". 

I found the whl file generated is only 80.64M. But I think it should be about 200M.

But there has libtensorflow_cc.so.2 file under path tensorflow/bazel-bin/tensorflow. I don't know why it wasn't packed into the whl file.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/__init__.py"", line 38, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 26, in <module>
    self_check.preload_check()
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/python/platform/self_check.py"", line 63, in preload_check
    from tensorflow.python.platform import _pywrap_cpu_feature_guard
ImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory
```
</details>",Dependency Issue,Dependency Issue
"data leak in GBDT due to warm start (This is about the non-histogram-based version of GBDTs)

X is split into train and validation data with `train_test_split(random_state=self.random_state)`.

As @johannfaouzi noted, in a warm starting context, this will produce a leak if If `self.random_state` is a `RandomState` instance: some samples that were used for training in a previous `fit` might be used for validation now.

~~I think the right fix would be to raise a `ValueError` if the provided random state isn't a number and early-stopping is activated~~",Security Vulnerability,Security Vulnerability
"Terminal size is wrong I see this from time to time but I don't know how to reproduce... This time I was able to record at least and it shows that somehow the terminal layed out with wrong dimensions, its container is higher than the terminal itself leading to the gap at the bottom

https://github.com/user-attachments/assets/73fcdc90-4646-40ce-931c-5287545da89c
",UI/UX Bug,UI/UX Bug
"""Accept all changes from .."" button in merge editor duplicates non-conflicting changes <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: This requires the git extension which, I think, is enabled by default.

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
```sh
$ code --version
1.95.3
f1a4fb101478ce6ec82fe9627c43efbf9e98c813
x64
```
- OS Version: `6.11.3-1-siduction-amd64`

Steps to Reproduce:

1. Extract the reproducing example in [vscodebug.tar.gz](https://github.com/user-attachments/files/17791046/vscodebug.tar.gz)
2. The archive contains one git repository called `vscodebug` and the checked out branch should be `feature`. Check out that branch if this is not the case.
3. Open `vscodebug/` in vscode and trust the contents to enable git functionality.
4. Instruct vscode to rebase the current branch (`feature`) onto `main`.
5. There should be a single conflict in `myfile`.
6. Open `myfile` (without using the merge editor) to observe one (and not two) conflicting changes at the beginning of the file: 2x `preamble` versus 1x `preamble`.
7. Now open `myfile` in the merge editor and observe that it still only counts a single conflict (top right status bar of the bottom file view) but also that it highlights the non-conflicting addition of ""new text"" in a similar style to that of the actual conflict.  
8. Click ""Accept all changes from Right"".
9. Observe the addition of ""new text"" being duplicated despite it being exactly the same in both branches and despite it not even being counted as a conflict by vscode itself.

This behavior is highly surprising. I don't see any use for a button that performs the action that vscode performs here. Git users would likely be familiar with `--ours` or `--theirs` but those are exclusive, not additive. I propose that the button be replaced by ""Resolve all conflicts with changes from Right"" which would leave alone those parts that vscode correctly identified as non-conflicting.

There is an additional, related behavior of the merge editor that I have not been able to generate a minimal reproducing example for: it is sometimes the case that in situations such as in `myfile` where `preamble` was accidentally added twice in the rebase's target branch and only once in the current branch, clicking ""Accept all changes from Right"" (i.e. the single addition) actually generates two additions of the line. It seems to me that vscode tries to do something smart by finding the commonalities between the branches (a single addition) but when the button is pressed it interprets it as the user's which to add the line again even though it apparently already part of the code that vscode managed to merge all by itself. I was unable to reproduce this in my example so I cannot demonstrate this easily.",UI/UX Bug,UI/UX Bug
"Bug: Input minLength not working after type changed I have an input with type password with minLength 8, and i have a toggle to change the type to text (password toggle).
when i type with length < 8 and press submit, it prevent to submit. but, when i toggle the password so it change the type to text and submit the form, it can submitted. btw i using useRef()

React version:
react: ^17.0.1
react-dom: ^17.0.1

## Steps To Reproduce

1. type character < minLength
2. submit form
3. toggle form type
4. submit form

Link to code example: [DEMO](https://codesandbox.io/s/young-shape-4tciw)

## The current behavior
The form get submitted even when it under minLength

## The expected behavior
The form not get submitted when it under minLength
",Security Vulnerability,Security Vulnerability
"Asset creation in cosmos db can fail after publishing Build link https://monacotools.visualstudio.com/Monaco/_build/results?buildId=314603&view=logs&j=43b54640-f671-5524-8f7b-714d77229de9&t=7f630ceb-a7eb-52db-66b5-c361c37b9f4e

`vscode_client_win32_x64_archive` asset creation failed with

```
[vscode_client_win32_x64_archive] Deleting blob https://vscodeesrp.blob.core.windows.net/staging/6b601934-aa74-40f2-a610-a13b99cb8c0f
[vscode_client_win32_x64_archive] Deleted blob successfully
TypeError: fetch failed
    at node:internal/deps/undici/undici:13392:13
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async ESRPReleaseService.getReleaseStatus (D:\a\_work\1\s\build\azure-pipelines\common\publish.js:225:21)
    at async ESRPReleaseService.createRelease (D:\a\_work\1\s\build\azure-pipelines\common\publish.js:137:39)
    at async D:\a\_work\1\s\build\azure-pipelines\common\publish.js:543:13
    at async withLease (D:\a\_work\1\s\build\azure-pipelines\common\publish.js:500:32)
    at async processArtifact (D:\a\_work\1\s\build\azure-pipelines\common\publish.js:532:5)
    at async main (D:\a\_work\1\s\build\azure-pipelines\common\publish.js:578:9) {
  [cause]: ConnectTimeoutError: Connect Timeout Error (attempted address: api.esrp.microsoft.com:443, timeout: 10000ms)
      at onConnectTimeout (node:internal/deps/undici/undici:2599:28)
      at Immediate._onImmediate (node:internal/deps/undici/undici:2568:35)
      at process.processImmediate (node:internal/timers:483:21) {
    code: 'UND_ERR_CONNECT_TIMEOUT'
  }
}
[vscode_client_win32_x64_archive] Error: [vscode_client_win32_x64_archive] Worker stopped with exit code 1
    at Worker.<anonymous> (D:\a\_work\1\s\build\azure-pipelines\common\publish.js:647:32)
    at Worker.emit (node:events:518:28)
    at [kOnExit] (node:internal/worker:315:10)
    at Worker.<computed>.onexit (node:internal/worker:229:20)
Error: Some artifacts failed to publish
    at main (D:\a\_work\1\s\build\azure-pipelines\common\publish.js:674:15)

```",UI/UX Bug,UI/UX Bug
"Possible bug in sklearn 1.6.1 PartialDependenceDisplay.from_estimator when target and feature are both binary ### Describe the bug

PartialDependenceDisplay.from_estimator does not seem able to handle dummy variables when the response variable is binary. See example below. The example works fine in 1.5.2 but returns `ValueError: cannot reshape array of size 1 into shape (2)` in 1.6.1

### Steps/Code to Reproduce

```py
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import PartialDependenceDisplay

np.random.seed(42)
n_samples = 1000
age = np.random.normal(35, 10, n_samples)
smoker = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
prob_disease = 1 / (1 + np.exp(-(age - 35) / 10 - 2 * smoker))
heart_disease = (np.random.random(n_samples) < prob_disease).astype(int)
df = pd.DataFrame({""age"": age, ""smoker"": smoker, ""heart_disease"": heart_disease})
X = df[[""age"", ""smoker""]]
y = df[""heart_disease""]

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X, y)

pdp_age = PartialDependenceDisplay.from_estimator(rf_model, X, features=[0, 1])
```

### Expected Results

PDP plots for age and smoker.

### Actual Results

```tb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1], [line 19](vscode-notebook-cell:?execution_count=1&line=19)
     [16](vscode-notebook-cell:?execution_count=1&line=16) rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
     [17](vscode-notebook-cell:?execution_count=1&line=17) rf_model.fit(X, y)
---> [19](vscode-notebook-cell:?execution_count=1&line=19) pdp_age = PartialDependenceDisplay.from_estimator(rf_model, X, features=[0, 1])

File ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:707, in PartialDependenceDisplay.from_estimator(cls, estimator, X, features, sample_weight, categorical_features, feature_names, target, response_method, n_cols, grid_resolution, percentiles, method, n_jobs, verbose, line_kw, ice_lines_kw, pd_line_kw, contour_kw, ax, kind, centered, subsample, random_state)
    [701](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:701)         raise ValueError(
    [702](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:702)             f""When a floating-point, subsample={subsample} should be in ""
    [703](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:703)             ""the (0, 1) range.""
    [704](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:704)         )
    [706](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:706) # compute predictions and/or averaged predictions
--> [707](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:707) pd_results = Parallel(n_jobs=n_jobs, verbose=verbose)(
    [708](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:708)     delayed(partial_dependence)(
    [709](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:709)         estimator,
    [710](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:710)         X,
    [711](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:711)         fxs,
    [712](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:712)         sample_weight=sample_weight,
    [713](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:713)         feature_names=feature_names,
    [714](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:714)         categorical_features=categorical_features,
    [715](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:715)         response_method=response_method,
    [716](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:716)         method=method,
    [717](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:717)         grid_resolution=grid_resolution,
    [718](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:718)         percentiles=percentiles,
    [719](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:719)         kind=kind_plot,
    [720](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:720)     )
    [721](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:721)     for kind_plot, fxs in zip(kind_, features)
    [722](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:722) )
    [724](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:724) # For multioutput regression, we can only check the validity of target
    [725](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:725) # now that we have the predictions.
    [726](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:726) # Also note: as multiclass-multioutput classifiers are not supported,
    [727](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:727) # multiclass and multioutput scenario are mutually exclusive. So there is
    [728](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:728) # no risk of overwriting target_idx here.
    [729](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:729) pd_result = pd_results[0]  # checking the first result is enough

File ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:77, in Parallel.__call__(self, iterable)
     [72](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:72) config = get_config()
     [73](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:73) iterable_with_config = (
     [74](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:74)     (_with_config(delayed_func, config), args, kwargs)
     [75](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:75)     for delayed_func, args, kwargs in iterable
     [76](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:76) )
---> [77](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:77) return super().__call__(iterable_with_config)

File ~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1918, in Parallel.__call__(self, iterable)
   [1916](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1916)     output = self._get_sequential_output(iterable)
   [1917](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1917)     next(output)
-> [1918](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1918)     return output if self.return_generator else list(output)
   [1920](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1920) # Let's create an ID that uniquely identifies the current call. If the
   [1921](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1921) # call is interrupted early and that the same instance is immediately
   [1922](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1922) # re-used, this id will be used to prevent workers that were
   [1923](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1923) # concurrently finalizing a task from the previous call to run the
   [1924](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1924) # callback.
   [1925](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1925) with self._lock:

File ~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1847, in Parallel._get_sequential_output(self, iterable)
   [1845](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1845) self.n_dispatched_batches += 1
   [1846](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1846) self.n_dispatched_tasks += 1
-> [1847](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1847) res = func(*args, **kwargs)
   [1848](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1848) self.n_completed_tasks += 1
   [1849](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1849) self.print_progress()

File ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:139, in _FuncWrapper.__call__(self, *args, **kwargs)
    [137](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:137)     config = {}
    [138](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:138) with config_context(**config):
--> [139](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:139)     return self.function(*args, **kwargs)

File ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)
    [210](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:210) try:
    [211](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:211)     with config_context(
    [212](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:212)         skip_parameter_validation=(
    [213](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213)             prefer_skip_nested_validation or global_skip_validation
    [214](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:214)         )
    [215](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:215)     ):
--> [216](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216)         return func(*args, **kwargs)
    [217](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:217) except InvalidParameterError as e:
    [218](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218)     # When the function is just a wrapper around an estimator, we allow
    [219](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:219)     # the function to delegate validation to the estimator, but we replace
    [220](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:220)     # the name of the estimator by the name of the function in the error
    [221](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:221)     # message to avoid confusion.
    [222](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:222)     msg = re.sub(
    [223](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:223)         r""parameter of \w+ must be"",
    [224](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:224)         f""parameter of {func.__qualname__} must be"",
    [225](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:225)         str(e),
    [226](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:226)     )

File ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:682, in partial_dependence(estimator, X, features, sample_weight, categorical_features, feature_names, response_method, percentiles, grid_resolution, method, kind)
    [676](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:676)     averaged_predictions = _partial_dependence_recursion(
    [677](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:677)         estimator, grid, features_indices
    [678](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:678)     )
    [680](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:680) # reshape averaged_predictions to
    [681](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:681) # (n_outputs, n_values_feature_0, n_values_feature_1, ...)
--> [682](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:682) averaged_predictions = averaged_predictions.reshape(
    [683](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:683)     -1, *[val.shape[0] for val in values]
    [684](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:684) )
    [685](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:685) pdp_results = Bunch(grid_values=values)
    [687](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:687) if kind == ""average"":

ValueError: cannot reshape array of size 1 into shape (2)
```
### Versions

```shell
System:
    python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]
executable: /Users/vnijs/miniconda/envs/msba/bin/python
   machine: macOS-14.2.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.6.1
          pip: 24.3.1
   setuptools: 75.1.0
        numpy: 2.2.1
        scipy: 1.15.1
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.10.0
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
    num_threads: 16
         prefix: libomp
       filepath: /Users/vnijs/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```",Syntax Error,Syntax Error
"UnsetMetadataPassedError can point towards the wrong method ### Describe the bug

When `enable_metadata_routing=True`, for a missing `set_score_request`, `UnsetMetadataPassedError` message states that a `set_fit_request` is missing.

### Steps/Code to Reproduce

```python
from sklearn import set_config
from sklearn.exceptions import UnsetMetadataPassedError
from sklearn.model_selection import cross_validate
from sklearn.linear_model import LogisticRegression
import numpy as np

rng = np.random.RandomState(22)
n_samples, n_features = 10, 4
X = rng.rand(n_samples, n_features)
y = rng.randint(0, 2, size=n_samples)
sw = rng.randint(0, 5, size=n_samples)

set_config(enable_metadata_routing=True)
# missing set_score_request
logreg = LogisticRegression().set_fit_request(sample_weight=True)
try:
    cross_validate(
        logreg, X, y, 
        params={""sample_weight"":sw}, 
        error_score='raise'
    )
except UnsetMetadataPassedError as e:
    print(e)
```

### Expected Results

I would expect an error message pointing towards the missing `set_score_request`, and perhaps a less verbose message when only one metadata is passed. Something like:


'sample_weight' are passed to cross validation but are not explicitly set as requested or not requested for cross_validate's estimator: LogisticRegression. Call `.set_score_request(sample_weight=True)` on the estimator for using 'sample_weight' or `sample_weight=False` for not using it. See the Metadata Routing User guide...

### Actual Results

['sample_weight'] are passed to cross validation but are not explicitly set as requested or not requested for cross_validate's estimator: LogisticRegression. Call `.set_fit_request({{metadata}}=True)` on the estimator for each metadata in ['sample_weight'] that you want to use and `metadata=False` for not using it. See the Metadata Routing User guide...

### Versions

```shell
sklearn: 1.7.dev0
```",Syntax Error,Syntax Error
"BaggingClassifier uses Class Label as Index to Array when Voting <!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->
BaggingClassifier uses Class Label as Index to Array when Voting

#### Steps/Code to Reproduce

Provide a base estimator to _BaggingClassifier_ that does not define the function _predict_proba_. This results in _BaggingClassifier_ resorting to voting. It appears the code for performing voting uses class labels as array indices instead of looking up the index of the class label in the _classes__ member.

Example:
```python
import numpy as np
from sklearn.ensemble import BaggingClassifier

class Foo:
    
    def __init__(self):
        pass
    
    def fit(self, X, Y, W=None):
        return self
    
    def predict(self, X):
        return np.full(X.shape[0], True, np.bool)
    
    def score(self, X, Y):
        YH = self.predict(X)
        return (Y == YH).mean()
    
    def get_params(self, deep=True):
        return {}
    
    def set_params(self, **params):
        for k, v in params:
            setattr(self, k, v)
        return self
    
# %%
A = np.random.rand(10, 4)
Y = np.random.randint(2, size=10, dtype=np.bool)
bc = BaggingClassifier(Foo())
bc.fit(A, Y)
YH = bc.predict(A)
print('BaggingClassifier Voting Result: ')
print(YH)
print('Ensemble Member Predictions: ')
for Ei in bc.estimators_:
    print(Ei.predict(A))
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
In the above code snippet, _BaggingClassifier_ should return an array of _True_ since it is the majority prediction of all ensemble members.

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->
_BaggingClassifier_ returns an array of False. This issue only occurs when the base estimator does not define the function _predict_proba_.

The issue appears to be due to lines 137 and 140 in ensemble/bagging.py.

https://github.com/scikit-learn/scikit-learn/blob/e14ac6d36d9dd069cc7fc2e51e4973514a003591/sklearn/ensemble/bagging.py#L137

https://github.com/scikit-learn/scikit-learn/blob/e14ac6d36d9dd069cc7fc2e51e4973514a003591/sklearn/ensemble/bagging.py#L140

The predictions of the ensemble members are directly used as indices into the original array. I'm guessing the prediction labels need to be converted into class labels using _estimator.classes__.

#### Versions
System:
    python: 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]
executable: C:\Users\XXXXXX\Anaconda3\pythonw.exe
   machine: Windows 2012 ServerR2

BLAS:
    macros: 
  lib_dirs: 
cblas_libs: cblas

Python deps:
       pip: 18.1
setuptools: 40.6.3
   sklearn: 0.20.1
     numpy: 1.15.4
     scipy: 1.1.0
    Cython: 0.29.2
    pandas: 0.23.4

<!-- Thanks for contributing! -->
",Logical Bug,Logical Bug
"Aborted (core dumped) in `RaggedBincount` ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value of size is close to the maximum value for the dtype, an integer overflow error will occur in following multiplication calculations.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

splits = tf.constant([0, 3, 5, 9], dtype=tf.int64)
values = tf.constant(1, shape=[3,3], dtype=tf.int64)
size = tf.constant(6522107765268123892, dtype=tf.int64)
weights = tf.constant(1, shape=[3,3], dtype=tf.float32)
counts = tf.raw_ops.RaggedBincount(splits=splits, values=values, size=size, weights=weights)
```


### Relevant log output

```shell
Status: INVALID_ARGUMENT: Encountered overflow when multiplying 3 with 6522107765268123892, result: -1
Aborted (core dumped)
```
",Runtime Error,Runtime Error
"[DevTools Bug]: Components tab freezes after inspecting ### Website or app

https://dev.permaplant.net

### Repro steps

1. Login
2. Go to Maps and create or open one
3. Look for the TimelinePicker component in the components tab
4. Click on it to inspect it

After that my components tab freezes and sometimes my RAM fills up endlessly. 

(If you need authentication, just contact me)

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Syntax Error,Syntax Error
"TensorFlow 2.13 distributed training fail ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04.3

### Mobile device

Linux Ubuntu 20.04.3

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.7, cuDNN 8.6

### GPU model and memory

3x NVIDIA GeForce RTX 3090

### Current behavior?

When trying to run multiple distributed trainings one after another, one of them fails with an `Collective ops is aborted by: ...` error. 

The reproducer attached to this issue produces the following error:
```
Collective ops is aborted by: Device /job:localhost/replica:0/task:0/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)
The error could be from a previous operation. Restart your program to reset.
	 [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_5585]
```
When run with TF 2.12 there is no such error.

The original code where I have encountered this problem results in
```
E                                           Collective ops is aborted by: Shape mismatch in the collective instance 100. Op at device /job:localhost/replica:0/task:0/device:GPU:1 expected shape [517169] but another member in the group expected shape [516734]. This is likely due to different input shapes at different members of the collective op.
E                                           The error could be from a previous operation. Restart your program to reset.
E                                           	 [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_49105]
```
but I wasn't able to reproduce this with a small code snippet.

### Standalone code to reproduce the issue

```shell
import pytest
import tensorflow as tf
import tensorflow_datasets as tfds


@pytest.mark.parametrize(""devices"", [1, 3, 2])
def test_distributed_fit(devices):
    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
    mnist_train, mnist_test = datasets['train'], datasets['test']

    if devices == 1:
        strategy = tf.distribute.OneDeviceStrategy(""/gpu:0"")
    else:
        strategy = tf.distribute.MirroredStrategy([f""/gpu:{i}"" for i in range(devices)])

    batch_size = 64 * strategy.num_replicas_in_sync
    train_dataset = mnist_test.cache().shuffle(10000).batch(batch_size)

    with strategy.scope():
        model = tf.keras.Sequential([
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(10)
        ])

        model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                      optimizer=tf.keras.optimizers.Adam(),
                      metrics=['accuracy'])

    model.fit(train_dataset, epochs=1)


if __name__ == '__main__':
    test_distributed_fit(1)
    test_distributed_fit(3)
    test_distributed_fit(2)
```


### Relevant log output

```shell
/home/nsavel/venvs/nncf_tf_213/bin/python /home/nsavel/workspace/nncf_tf_213/reproducer.py 
2023-07-18 16:47:21.693862: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-18 16:47:21.722428: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:7630] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-07-18 16:47:21.722456: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-07-18 16:47:21.722481: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-07-18 16:47:21.728124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 16:47:22.211027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
2023-07-18 16:47:24.321508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1833] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22292 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:17:00.0, compute capability: 8.6
2023-07-18 16:47:24.322042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1833] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22292 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6
2023-07-18 16:47:24.322425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1833] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22292 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:b3:00.0, compute capability: 8.6
2023-07-18 16:47:24.602273: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2023-07-18 16:47:25.946425: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcf358b4470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-07-18 16:47:25.946450: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2023-07-18 16:47:25.946455: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2023-07-18 16:47:25.946458: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2023-07-18 16:47:25.950178: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-07-18 16:47:26.074588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Loaded cuDNN version 8600
2023-07-18 16:47:26.171621: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
157/157 [==============================] - 2s 5ms/step - loss: 25.9054 - accuracy: 0.6873
2023-07-18 16:47:27.474184: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2023-07-18 16:47:30.690312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Loaded cuDNN version 8600
2023-07-18 16:47:30.822607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Loaded cuDNN version 8600
53/53 [==============================] - 3s 7ms/step - loss: 43.9234 - accuracy: 0.5655
2023-07-18 16:47:31.372876: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2023-07-18 16:47:32.398894: E tensorflow/core/common_runtime/base_collective_executor.cc:249] BaseCollectiveExecutor::StartAbort INTERNAL: Device /job:localhost/replica:0/task:0/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)
2023-07-18 16:47:32.398950: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7416489994643074752
2023-07-18 16:47:32.399024: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1224112818691547746
2023-07-18 16:47:32.399044: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10338356286700713842
2023-07-18 16:47:32.399063: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6809993284794892577
2023-07-18 16:47:32.399081: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12460047264292639245
2023-07-18 16:47:32.399097: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8051515006773529005
Traceback (most recent call last):
  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)
  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)
  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node CollectiveReduceV2 defined at (most recent call last):
  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1376, in train_function
    return step_function(self, iterator)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1376, in train_function
    return step_function(self, iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1359, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1376, in train_function
    return step_function(self, iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1359, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/optimizers/utils.py"", line 175, in _all_reduce_sum_fn
    return distribution.extended.batch_reduce_to(

Collective ops is aborted by: Device /job:localhost/replica:0/task:0/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)
The error could be from a previous operation. Restart your program to reset.
	 [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_5585]

Process finished with exit code 1
```
",Runtime Error,Runtime Error
"Bug: Input minLength not working after type changed I have an input with type password with minLength 8, and i have a toggle to change the type to text (password toggle).
when i type with length < 8 and press submit, it prevent to submit. but, when i toggle the password so it change the type to text and submit the form, it can submitted. btw i using useRef()

React version:
react: ^17.0.1
react-dom: ^17.0.1

## Steps To Reproduce

1. type character < minLength
2. submit form
3. toggle form type
4. submit form

Link to code example: [DEMO](https://codesandbox.io/s/young-shape-4tciw)

## The current behavior
The form get submitted even when it under minLength

## The expected behavior
The form not get submitted when it under minLength
",Security Vulnerability,Security Vulnerability
"[DevTools Bug] Minified React error #310; visit https://react.dev/errors/310 for the full message or use the non-minified dev environment for full errors and additional helpful warnings. ### Website or app

http://localhost:1234/restaurants/323532

### Repro steps

i'm not able to open components features and use it properly

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

6.0.1-c7c68ef842

### Error message (automated)

Minified React error #310; visit https://react.dev/errors/310 for the full message or use the non-minified dev environment for full errors and additional helpful warnings.

### Error call stack (automated)

```text
at updateWorkInProgressHook (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:53845)
    at Object.updateCallback [as useCallback] (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:63009)
    at t.useCallback (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:222673)
    at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1329500)
    at renderWithHooks (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:52283)
    at updateFunctionComponent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:81757)
    at beginWork (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:95976)
    at performUnitOfWork (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:154382)
    at workLoopSync (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:154250)
    at renderRootSync (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:153981)
```


### Error component stack (automated)

```text
at InspectedElementContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1328724)
    at da (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1315454)
    at div (<anonymous>)
    at InspectedElementErrorBoundaryWrapper (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1321764)
    at NativeStyleContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1354133)
    at div (<anonymous>)
    at div (<anonymous>)
    at OwnersListContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1268367)
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1297652)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1393839
    at da (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1315454)
    at div (<anonymous>)
    at div (<anonymous>)
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1318165)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1318362
    at div (<anonymous>)
    at div (<anonymous>)
    at div (<anonymous>)
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1318165)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1395852)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1387571)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1209877)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1238028)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1375269)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1544258)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Minified React error #310; visit https://react.dev/errors/310 for the full message or use the non-minified dev environment for full errors and additional helpful warnings. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
",Syntax Error,Syntax Error
"Missing syntax highlighting for PHP `if-else` alternate syntax 
Type: <b>Bug</b>

While using the alternate syntax for the `if` statement in PHP, the `else` keyword is not being highlighted the same as `if` keyword.

![Image](https://github.com/user-attachments/assets/ff2898a7-a22f-41a8-864b-6ac00af9c523)



VS Code version: Code 1.95.3 (f1a4fb101478ce6ec82fe9627c43efbf9e98c813, 2024-11-13T14:50:04.152Z)
OS version: Windows_NT x64 10.0.22631
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) Ultra 5 125H (18 x 2995)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|15.61GB (5.37GB free)|
|Process Argv|--crash-reporter-id 42ce4d94-f018-448f-be6f-b9071de35767|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (22)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-intelephense-client|bme|1.12.6
simple-react-snippets|bur|1.2.8
vscode-eslint|dba|3.0.10
intelli-php-vscode|DEV|0.12.15062
phptools-vscode|DEV|1.53.16379
profiler-php-vscode|DEV|1.53.16379
es7-react-js-snippets|dsz|4.4.3
prettier-vscode|esb|11.0.0
code-runner|for|0.12.2
copilot|Git|1.246.0
copilot-chat|Git|0.22.4
path-autocomplete|ion|1.25.0
remote-wsl|ms-|0.88.5
cmake-tools|ms-|1.19.52
cpptools|ms-|1.22.11
cpptools-extension-pack|ms-|1.3.0
ts-file-path-support|ms-|1.0.0
vscode-react-native|msj|1.13.0
es7-react-js-snippets|rod|1.9.3
vscode-icons|vsc|12.9.0
vscode-wakatime|Wak|24.9.1
five-server|yan|0.3.1

(2 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
724cj586:31013169
dvdeprecation:31068756
dwnewjupyter:31046869
newcmakeconfigv2:31071590
nativerepl1:31139838
pythonrstrctxt:31112756
nativeloc2:31185842
cf971741:31144450
iacca1:31171482
notype1:31157159
5fd0e150:31155592
dwcopilot:31170013
stablechunks:31184530

```

</details>

<!-- generated by issue reporter -->",UI/UX Bug,UI/UX Bug
"[DevTools Bug]: Excessive memory usage, even when not in use (Firefox) ### Website or app

The issue happens on the browser, not on a specific site

### Repro steps

1. Use Firefox
2. open a couple of tabs not even using react (for example for me, I have my mail client, youtube etc open).
3. check about:performance
4. confirm that the memory usage of the extension is around 200mb, which is a lot considering the extension isn't even in use.

What I would do is I would add a config option about the domains the extension is supposed to run on, and only start up everything the extension does when the user is on one of those domains, because I really don't want an extension I'm not even using at the moment to be a memory hog.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Performance Issue,Performance Issue
"VSCode opens automatically when I plug in a mass storage device or click on ""open with File Manager"" (dolphin) <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No
yes
<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- 1.93
- OS Version: 
- Kubuntu 23.10

Steps to Reproduce:

1. plug in msc
2. click on ""open with file manager""


I think vscode helped itself to auto-handle some mime-type that I did not ask it to, which usually gets handled by dolphin. Please don't do this microsoft. undo it. really lame mcro d*** move. vscode is awesome, but disguisting to consider itself anywhere close to a sufficient replacement for dolphin.",Dependency Issue,Dependency Issue
"Hover - codicons shift in hover Steps to Reproduce:

1. Launch VS Code Insiders and open a folder/workspace that contains a git repository
2. Enable git blame editor decoration and start bar item
3. Open a file and click on a line so that blame information is revealed
4. Hover over the editor decoration and see the hover load and then shift the codicons

https://github.com/user-attachments/assets/26eca974-0bb0-47d9-979f-f700571e8248",Dependency Issue,Dependency Issue
"BUG: StandardScaler partial_fit overflows The recent implementation of `partial_fit` for `StandardScaler` can overflow. A use case there is to transform indefinitely long stream of data, but that is problematic with the current implementation. The reason is that to compute the running mean, [we keep track](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py#L788) of the sample sum.

Here the code to reproduce the behavior. To simulate long stream of data would take long time; instead, I use samples with very large norm but the effect is the same. The same batch is presented to the transformer many times. The mean should be same.

```python
from sklearn.preprocessing import StandardScaler
import numpy as np

rng = np.random.RandomState(0)

def gen_1d_uniform_batch(min_, max_, n):
    return rng.uniform(min_, max_, size=(n, 1))

max_f = np.finfo(np.float64).max / 1e5
min_f = max_f / 1e2
stream_dim = 100
batch_dim = 500000
print(""mean overflow: batch vs online on %d repetitions"" % stream_dim)

X = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)

scaler = StandardScaler(with_std=False).fit(X)
print(scaler.mean_)
[  1.79769313e+301]

iscaler = StandardScaler(with_std=False)
batch = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)
for _ in range(stream_dim):
    iscaler = iscaler.partial_fit(batch)
RuntimeWarning: overflow encountered in add
  updated_mean = (last_sum + new_sum) / updated_sample_count

print(iscaler.mean_)
[ inf]
```
",Security Vulnerability,Security Vulnerability
"Devtools v4 does not work with Firefox's private window <!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

* bug
* This issue has been reported in https://github.com/facebook/react-devtools/issues/1383

**What is the current behavior?**


Steps to Reproduce is here:

1. Environments are:
2. Open the page which uses react with a private window.
3. Open Firefox's devtools.

Actual Result is:

* react devtools' _component_ pane show `Unable to find React on the page.`
* From about:debugging, we can see the below messsage:

```
SecurityError: Permission denied to access property ""container"" on cross-origin object main.js:51:305877
    Kl moz-extension://56db142d-3d36-b04e-91ca-a7504c7708a5/build/main.js:51
    apply self-hosted:4417
    applySafeWithoutClone resource://gre/modules/ExtensionCommon.jsm:588
    asyncWithoutClone resource://gre/modules/ExtensionCommon.jsm:2400
```



**What is the expected behavior?**

react devtools work

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

* Firefox 68
* react devtools v4.0.5
* react v16.9",Security Vulnerability,Security Vulnerability
"During an edits generation, a scrollbar appears below the tab being edited This only happened once so far, but I have tab wrapping enabled and a good amount of tabs open

![Image](https://github.com/user-attachments/assets/54264444-5723-482c-836f-5eb025faff43)
<img width=""1262"" alt=""Image"" src=""https://github.com/user-attachments/assets/c519edef-ed37-4f18-b415-693cc2d666bb"" />
",UI/UX Bug,UI/UX Bug
"[DevTools Bug] Cannot remove node ""366"" because no matching node was found in the Store. ### Website or app

http://localhost:3000/welcome

### Repro steps

### I created token for user authentication and pass into context so that user can reuse that token to call an API 
when i logout or login on my localhost i see this error 

**STEP 1 Create a token** 
export const executeBasicAuthenticationService
    = (token) => apiClient.get(`/basicauth`,{
        headers: {
            Authorization: token
        }
    })

**STEP-2 Pass into context**

async function login(username, password) {



  const baToken = 'Basic ' + window.btoa( username + "":"" + password )
  
  try{

  
  const response = await executeBasicAuthenticationService(baToken)
  
  if(response.status==200){
        setAuthenticated(true)
        setUsername(username)
        settoken(baToken)
         return true
       
    } else {
       logout()
        return false
      
     }
}  catch(error){
  logout()
  return false
}

 }

function logout(){
setAuthenticated(false)
settoken(null)
setUsername(null)
}

    return (
  <AuthContext.Provider value={ {isAuthenticated,setAuthenticated,login,logout,username,token} }>

    {children}
  </AuthContext.Provider>


    )
}

    

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

5.3.1-ccb20cb88b

### Error message (automated)

Cannot remove node ""366"" because no matching node was found in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1174132
    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1141877)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1143565
    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1551564)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
",Syntax Error,Syntax Error
"Tensorflow Profiler does not work on WSL2: Failed to load libcupti (is it installed and accessible?)  ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom code

Yes

### OS platform and distribution

Windows 10 WSL Ubuntu

### Mobile device

Ubuntu 22.04

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

_No response_

### Current behavior?

After following exactly the steps mentioned in https://www.tensorflow.org/install/pip for installing Tensorflow on WSL2, and installing the latest version of the profiler plugin, the Tensorboard profiler does not seem to work.

This is with a fresh WSL2 install, miniconda install, etc.

![image](https://github.com/tensorflow/tensorflow/assets/11645696/adbaf3c1-4f03-43fe-80d7-39c484ac91a7)

```
Failed to load libcupti (is it installed and accessible?) 

No step marker observed and hence the step time is unknown. This may happen if (1) training steps are not instrumented (e.g., if you are not using Keras) or (2) the profiling duration is shorter than the step time. For (1), you need to add step instrumentation; for (2), you may try to profile longer. 
```

The problem does not seem to be that lubcupti fails to load (despite what is indicated by Tensorboard); libcupti seems to be found just fine, but there may be some problems - See the attached log output for possible clues as to what's happening. 

### Standalone code to reproduce the issue

```shell
# The below code is copied directly from https://github.com/keras-team/keras-io/blob/master/examples/vision/mnist_convnet.py - with the single addition of adding Tensorboard profiling.

import numpy as np
from tensorflow import keras
from tensorflow.keras import layers

""""""
## Prepare the data
""""""

# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# Load the data and split it between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Scale images to the [0, 1] range
x_train = x_train.astype(""float32"") / 255
x_test = x_test.astype(""float32"") / 255
# Make sure images have shape (28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print(""x_train shape:"", x_train.shape)
print(x_train.shape[0], ""train samples"")
print(x_test.shape[0], ""test samples"")


# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

""""""
## Build the model
""""""

model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=""softmax""),
    ]
)

model.summary()

""""""
## Train the model
""""""

batch_size = 128
epochs = 15

model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[
    keras.callbacks.TensorBoard(profile_batch=[20, 30])
])

""""""
## Evaluate the trained model
""""""

score = model.evaluate(x_test, y_test, verbose=0)
print(""Test loss:"", score[0])
print(""Test accuracy:"", score[1])
```


### Relevant log output

```shell
17/422 [>.............................] - ETA: 2s - loss: 2.0755 - accuracy: 0.38602023-07-07 15:45:09.034256: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2023-07-07 15:45:09.034284: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2023-07-07 15:45:09.034300: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-07-07 15:45:09.034304: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error.
2023-07-07 15:45:09.034307: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.
2023-07-07 15:45:09.034309: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1730] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error 
 25/422 [>.............................] - ETA: 2s - loss: 1.8839 - accuracy: 0.46882023-07-07 15:45:09.105818: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2023-07-07 15:45:09.105940: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:142] cuptiFinalize: ignored due to a previous error.
2023-07-07 15:45:09.105943: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.
2023-07-07 15:45:09.105945: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1822] function cupti_interface_->Finalize()failed with error 
2023-07-07 15:45:09.107652: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-07-07 15:45:09.107660: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-07-07 15:45:09.107663: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_collector.cc:541]  GpuTracer has collected 0 callback api events and 0 activity events. 
2023-07-07 15:45:09.107809: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
```
",Dependency Issue,Dependency Issue
"[DevTools Bug] Cannot add child ""1161"" to parent ""942"" because parent node was not found in the Store. ### Website or app

chrome on local host

### Repro steps

it happen with every component that i mount 
<img width=""1470"" alt=""Screenshot 2023-03-23 at 1 04 00 PM"" src=""https://user-images.githubusercontent.com/110327079/227134701-d665feca-3326-401e-b957-007c41318be6.png"">


### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.2-1a88fbb67

### Error message (automated)

Cannot add child ""1161"" to parent ""942"" because parent node was not found in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:27939:43
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:25892:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26061:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56323:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add child  to parent  because parent node was not found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
",Logical Bug,Logical Bug
"tf.linalg.eigvals outputs UnboundLocalError when receiving a float16 tensor ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Given a float16 tensor, tf.linalg.eigvals outputs `UnboundLocalError: local variable 'out_dtype' referenced before assignment`. If tf.linalg.eigvals does not accept float16 tensor, it would be better if it can be explicit in the documentation and the error message can point this issue out.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tensor = tf.constant([[1,2],[3,4]], dtype=tf.float16)
tf_out = tf.linalg.eigvals(tf.constant(tensor))
print(""TensorFlow's result: "",tf_out)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
UnboundLocalError                         Traceback (most recent call last)
<ipython-input-69-5a0470cb5082> in <cell line: 3>()
      1 import tensorflow as tf
      2 tensor = tf.constant([[1,2],[3,4]], dtype=tf.float16)
----> 3 tf_out = tf.linalg.eigvals(tf.constant(tensor))
      4 print(""TensorFlow's result: "",tf_out)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/linalg_ops.py in eigvals(tensor, name)
    431   elif tensor.dtype == dtypes.float64 or tensor.dtype == dtypes.complex128:
    432     out_dtype = dtypes.complex128
--> 433   e, _ = gen_linalg_ops.eig(tensor, Tout=out_dtype, compute_v=False, name=name)
    434   return e
    435 

UnboundLocalError: local variable 'out_dtype' referenced before assignment
```
",Logical Bug,Logical Bug
"Diff viewer: misaligned lines with inlays + word wrap <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No
Yes
<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.96.4
- OS Version: Windows 11

Steps to Reproduce:
1. Make a code change in a reasonably long file, in a language that has inlay hints, so there's a diff
2. Open the diff viewer, ensure word wrap is enabled (<kbd>Alt</kbd><kbd>Z</kbd>), and scroll around throughout the file
3. Watch as the lines get misaligned between the added and removed side
4. Use <kbd>Ctrl</kbd><kbd>Alt</kbd> (with `editor.inlayHints.enabled` set to `onUnlessPressed`) to temporarily hide the inlays and notice how they also get misaligned

Video of the bug in action:

[Video (70 seconds)](https://files.keavon.com/-/HarshAcclaimedEuropeanfiresalamander/capture_40_.mp4), showing both the scrolling-caused misalignment and triggering it with toggling inlays.

Description:

This is an issue that has frustrated my many times a day for years, so I'm finally filling an issue for it.

The diff editor is meant to keep your left (removals) and right (additions) side's lines aligned. But scrolling throughout the code file, or turning on/off inlay hints, causes a misalignment to occur and persist. The result is that the red removals and green additions for a line can be considerably misaligned.

This happens under the conditions that word wrap is enabled (so a line of code may wrap across multiple displayed lines on the left, right, or both sides) and inlays are enabled (so the width of a line of code may become elongated).

My theory is that scrolling within the document causes the LSP to generate more inlays within the newly viewed areas, changing the line widths which changes the wrapping which changes the number of wrapped lines occupied by one line of code; and it does so differently on the right from the left, since the LSP generates no inlays for the left side.

The workaround for this is to either type something to make it recalculate, or briefly toggle word wrap by tapping <kbd>Alt</kbd><kbd>Z</kbd> twice. But a proper fix for this bug would be a real quality of life improvement.",UI/UX Bug,UI/UX Bug
"Git - Git blame editor decoration appears before the folding ... one The ... should be before the git one

![Image](https://github.com/user-attachments/assets/8a7704f1-4a36-40db-94b4-4d80d448f9bc)
",UI/UX Bug,UI/UX Bug
"Clarify Walkthrough step complete criteria >Should the step be marked as completed? 
> 
> No the step should NOT be marked. ?? 
> Just tried it my self and I see that the step is not marked as complete when it reappears.  

 _Originally posted by @bhavyaus in [#226184](https://github.com/microsoft/vscode/issues/226184#issuecomment-2438417032)_",UI/UX Bug,UI/UX Bug
"The result of tf.quantization.fake_quant_with_min_max_args is inconsistent between CPU and GPU ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.2

### GPU model and memory

_No response_

### Current behavior?

For the same input, `tf.quantization.fake_quant_with_min_max_args ` produces inconsistent results on CPU and GPU.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = tf.constant([0.0, 1.0, 2.0, 3.0, 4.0])

with tf.device('cpu:0'):
    quantized_input_data = tf.quantization.fake_quant_with_min_max_args(input_data, min=-6.0, max=6.0)
    print(quantized_input_data)

with tf.device('gpu:0'):
    quantized_input_data = tf.quantization.fake_quant_with_min_max_args(input_data, min=-6.0, max=6.0)
    print(quantized_input_data)
```


### Relevant log output

```shell
tf.Tensor([0.        0.9882353 2.0235295 3.0117648 4.       ], shape=(5,), dtype=float32)
tf.Tensor([0.        0.9882353 1.9764706 3.0117648 4.       ], shape=(5,), dtype=float32)
```
",Performance Issue,Performance Issue
"//tensorflow/python/ops/ragged:ragged_cross_op_test is flaky <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.8.13

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Test sometimes fails with segfault

### Standalone code to reproduce the issue

```shell
bazel test --build_tests_only --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --local_test_jobs=64 --test_tag_filters=-nopip,-no_pip,-oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 -k -- //bazel_pip/tensorflow/... -//bazel_pip/tensorflow/compiler/tf2tensorrt/... -//bazel_pip/tensorflow/compiler/xrt/... -//bazel_pip/tensorflow/core/tpu/... -//bazel_pip/tensorflow/go/... -//bazel_pip/tensorflow/java/... -//bazel_pip/tensorflow/python/integration_testing/... -//bazel_pip/tensorflow/tools/toolchains/... -//bazel_pip/tensorflow/lite/... -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:conv_ops_test
```


### Relevant log output

```shell
[ RUN      ] RaggedCrossOpTest.testRaggedCrossInvalidValue
INFO:tensorflow:Running testRaggedCrossInvalidValue in GRAPH mode.
I0522 15:40:37.724678 281472914997264 test_util.py:1494] Running testRaggedCrossInvalidValue in GRAPH mode.
Fatal Python error: Segmentation fault

Thread 0x0000ffff851cc010 (most recent call first):
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1477 in _call_tf_sessionrun
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1384 in _run_fn
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1401 in _do_call
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1394 in _do_run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1214 in _run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 971 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 2061 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 2693 in evaluate
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.py"", line 478 in testRaggedCrossInvalidValue
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 1498 in decorated
  File ""/usr/lib/python3.10/unittest/case.py"", line 549 in _callTestMethod
  File ""/usr/lib/python3.10/unittest/case.py"", line 591 in run
  File ""/usr/lib/python3.10/unittest/case.py"", line 650 in __call__
  File ""/usr/lib/python3.10/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.10/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.10/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.10/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.10/unittest/runner.py"", line 184 in run
  File ""/usr/lib/python3.10/unittest/main.py"", line 271 in runTests
  File ""/usr/lib/python3.10/unittest/main.py"", line 101 in __init__
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2527 in _run_and_get_tests_result
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2561 in run_tests
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2155 in _run_in_app
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2060 in main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/app.py"", line 254 in _run_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/app.py"", line 308 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.py"", line 497 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, tensorflow.python.framework.fast_tensor_util, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label (total: 72)
```
</details>",Runtime Error,Runtime Error
"LinearDiscriminantAnalysis does not handle degenerate problems well As investigated when trying to solve unstable common tests in #18667, it seems that `LinearDiscriminantAnalysis` is bad a handling classification problems that are too trivial such as illustrated in the following (using MKL):

```python
>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>>> LinearDiscriminantAnalysis(solver=""svd"").fit([[0], [1], [1]], [0, 1, 1])
Traceback (most recent call last):
  File ""<ipython-input-6-034ea174ac0e>"", line 1, in <module>
    LinearDiscriminantAnalysis(solver=""svd"").fit([[0], [1], [1]], [0, 1, 1])
  File ""/home/ogrisel/code/scikit-learn/sklearn/discriminant_analysis.py"", line 553, in fit
    self._solve_svd(X, y)
  File ""/home/ogrisel/code/scikit-learn/sklearn/discriminant_analysis.py"", line 478, in _solve_svd
    _, S, Vt = linalg.svd(X, full_matrices=0)
  File ""/home/ogrisel/miniforge3/envs/dev/lib/python3.8/site-packages/scipy/linalg/decomp_svd.py"", line 121, in svd
    lwork = _compute_lwork(gesXd_lwork, a1.shape[0], a1.shape[1],
  File ""/home/ogrisel/miniforge3/envs/dev/lib/python3.8/site-packages/scipy/linalg/lapack.py"", line 950, in _compute_lwork
    ret = routine(*args, **kwargs)
ValueError: On entry to DGESDD parameter number 10 had an illegal value

>>> LinearDiscriminantAnalysis(solver=""eigen"").fit([[0], [1], [1]], [0, 1, 1])
/home/ogrisel/code/scikit-learn/sklearn/covariance/_empirical_covariance.py:88: UserWarning: Only one sample available. You may want to reshape your data array
  warnings.warn(""Only one sample available. ""
Traceback (most recent call last):
  File ""<ipython-input-7-57ea859bdde2>"", line 1, in <module>
    LinearDiscriminantAnalysis(solver=""eigen"").fit([[0], [1], [1]], [0, 1, 1])
  File ""/home/ogrisel/code/scikit-learn/sklearn/discriminant_analysis.py"", line 558, in fit
    self._solve_eigen(X, y,
  File ""/home/ogrisel/code/scikit-learn/sklearn/discriminant_analysis.py"", line 419, in _solve_eigen
    evals, evecs = linalg.eigh(Sb, Sw)
  File ""/home/ogrisel/miniforge3/envs/dev/lib/python3.8/site-packages/scipy/linalg/decomp.py"", line 578, in eigh
    raise LinAlgError('The leading minor of order {} of B is not '
LinAlgError: The leading minor of order 1 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.

>>> lda = LinearDiscriminantAnalysis(solver=""lsqr"").fit([[0], [1], [1]], [0, 1, 1])
/home/ogrisel/code/scikit-learn/sklearn/covariance/_empirical_covariance.py:88: UserWarning: Only one sample available. You may want to reshape your data array
  warnings.warn(""Only one sample available. ""
LinearDiscriminantAnalysis(solver='lsqr')
```

The least squares solver does not fail (but still raises a warning) and the resulting model is unable to classifiy the training set correctly:

```python
>>> lda.predict([[0], [1]])
array([1, 1])
```

The above was done with scipy 1.5.2 with OpenBLAS from conda-forge under Linux.

I am not sure what we could do about it. Maybe we could at least try to raise the same exception for all the solvers when the problem is degenerate. The original exception of the eigen cause could still be raised as the cause of the user friendly exception (`raise e1 from e2` in Python 3). For the SVD case this happens when the estimated rank is 0.",Logical Bug,Logical Bug
"tf.data.Dataset prefetch not fetching data asynchronously <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian/Linux 11

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

After implementing a data pipeline using tf.data.Dataset to pull image data from Google Cloud Storage, TensorBoard profiler shows that the GPU compute and CPU prefetch are running synchronously. I used data.Dataset.AUTOTUNE to determine the appropriate prefetch batch size. Monitoring GPU usage while the model is running confirms this with the GPU at 0% utilization to actually computing something for about a 2:1 ratio, which is reflected in the profiler. CPU usage when monitored does not appear to max out.

I expected the prefetch to occur concurrently with GPU processing as described in the data.Dataset documentation and tutorials.

![ch](https://github.com/tensorflow/tensorflow/assets/79778984/e96ad312-12b0-4bbb-b06e-f4e4976714b3)

![cp](https://github.com/tensorflow/tensorflow/assets/79778984/f52959d1-23fd-45ed-ba57-5a532afd0972)

![gp](https://github.com/tensorflow/tensorflow/assets/79778984/2b2e15b6-2cf7-40d8-89b1-98889f151863)


### Standalone code to reproduce the issue

```shell
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""
os.environ['TF_GPU_ALLOCATOR'] = ""cuda_malloc_async""
config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

def get_label(file_path):
    parts = tf.strings.split(file_path, os.path.sep)
    one_hot = parts[-2] == class_names
    return tf.argmax(one_hot)

def decode_img(img):
    img = tf.io.decode_image(img, channels=3, expand_animations = False)
    img = tf.image.resize(img, [244, 244])
    img = tf.cast(img, tf.float32)
    return img

def process_path(file_path):
    label = get_label(file_path)
    img = tf.io.read_file(file_path)
    img = decode_img(img)
    return img, label

def configure_for_performance(ds):
    ds = ds.batch(128)
    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)
    return ds

files = tf.data.Dataset.list_files((data_dir + '/*/*.png'), shuffle=False)
files = files.shuffle(image_count, reshuffle_each_iteration=False)

val_size = int(image_count * 0.2)

train_files = files.skip(val_size)
val_files = files.take(val_size)

train_ds = train_files.interleave(lambda x: tf.data.Dataset.from_tensor_slices([x]), cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)

val_ds = val_files.interleave(lambda x: tf.data.Dataset.from_tensor_slices([x]), cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE)
val_ds = val_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)

train_ds = configure_for_performance(train_ds)
val_ds = configure_for_performance(val_ds)
```


### Relevant log output

_No response_</details>",Performance Issue,Performance Issue
"textarea does not show warning when switching from uncontrolled to controlled like inputs do <!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

Bug

**What is the current behavior?**

While things like `<input>` correctly get a warning when switching from uncrontrolled to controlled, I'm noticing `<textarea>` does not

**If the current behavior is a bug, please provide the steps to reproduce and if possible a minimal demo of the problem. Your bug will get fixed much faster if we can run your code and it doesn't have dependencies other than React. Paste the link to your JSFiddle (https://jsfiddle.net/Luktwrdm/) or CodeSandbox (https://codesandbox.io/s/new) example below:**

Here's a codesandbox. Type in the input field, we see error (correct), change to textarea and start over, type in field and we don't see the error (incorrect I think) https://codesandbox.io/s/recursing-dawn-jls8i

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

16.8
",Logical Bug,Logical Bug
"ValueError: No gradients provided for any variable ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.12

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have run a PyTorch code that computes the gradient of the gradient w.r.t some computation. It works just fine. Now, I want to translate PyTorch code into TensorFlow but got some errors.

## Standalone code to reproduce the issue

Here is the reproducible code. [Gist](https://colab.research.google.com/drive/1GPhctZNrXynrCQ0qNbLyMDmuixQtC0fw?usp=sharing).

The above collab is small and quickly reproduces the run of PyTorch and TensorFlow. PyTorch runs as expected but TensorLow doesn't. Below is the main spot to look at:


**Main Part**

In PyTorch, 

```python
rand_model = Rnadom()
model = Model()
ran_optim = torch.optim.SGD(
    ran_model.parameters()
)

model_params = model.parameters()
loss_mod  = model.forward(x)
loss_rand = model.forward(y)

model_grad = torch.autograd.grad(loss_mod, model_params)
rand_grad  = torch.autograd.grad(
    loss_rand, 
    model_params, 
    create_graph=True
)
    
loss = some_method(model_grad, rand_grad)   
rand_model.zero_grad()
loss.backward()
ran_optim.step()
```
In `pytorch`, the above `create_graph=True` is crucial. 

In TensorFlow, I tried 

```python
ran_model = Random()
ran_optim = tf.keras.optimizers.SGD()

model = Model()
model.build(input_shape=(1, 784))
optim = tf.keras.optimizers.SGD(0.01)
model_params = model.trainable_variables

with tf.GradientTape(persistent=True) as tape:
    tape.watch(ran_model.trainable_variables)
    loss_mod = tf.reduce_mean(tf.math.log(model(x)[:, i]))
    loss_rand = tf.reduce_mean(tf.math.log(model(y)[:, i]))
grads_mod = tape.gradient(loss_mod, model_params)
grads_rand = tape.gradient(loss_rand, model_params)

loss = some_method(model_grad, rand_grad)  
ran_model_grads = tape.gradient(loss, ran_model.trainable_variables)
ran_optim.apply_gradients(
  zip(ran_model_grads, ran_model.trainable_variables)
)
```

The `tf` code gives the following error. 

```yaml
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-01562609cda8> in <cell line: 33>()
     44         loss += tf.reduce_sum(tf.stack([a, b], axis=0))
     45     ran_model_grads = tape.gradient(loss, ran_model.trainable_variables)
---> 46     ran_optim.apply_gradients(zip(ran_model_grads, ran_model.trainable_variables))
     47 
     48 

3 frames
/usr/local/lib/python3.10/dist-packages/keras/optimizers/utils.py in filter_empty_gradients(grads_and_vars)
     75     if not filtered:
     76         variable = ([v.name for _, v in grads_and_vars],)
---> 77         raise ValueError(
     78             f""No gradients provided for any variable: {variable}. ""
     79             f""Provided `grads_and_vars` is {grads_and_vars}.""

ValueError: No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(10, 1, 784) dtype=float32, numpy=
```

- This is probably because the `ran_model_grads, ran_model.trainable_variables` are not connected. As mentioned in this [doc](https://www.tensorflow.org/guide/autodiff), 

> When a **target** is not connected to a **source**, the gradient will return `None`

- In PyTorch, `create_graph=True` is used to compute the gradient of the gradient in the later part. To compute [grad-of-grad](https://www.tensorflow.org/guide/advanced_autodiff#example_input_gradient_regularization), but didn't work (shown below). The reason probably is the same as before, source and target are not connected.

```python
for i in range(5):
    
    with tf.GradientTape() as tape1:
        loss_mod = tf.reduce_mean(tf.math.log(model(x)[:, i]))
    grads_mod = tape1.gradient(loss_mod, model_params)
    
    
    with tf.GradientTape() as tape3:
        with tf.GradientTape() as tape2:
            loss_rand = tf.reduce_mean(tf.math.log(model(y)[:, i]))
        grads_rand = tape2.gradient(loss_rand, model_params)

    loss = 0
    for a, b in zip(grads_mod, grads_rand):
        loss += tf.reduce_sum(tf.stack([a, b], axis=0))
    [ISSUE] > ran_model_grads = tape3.gradient(loss, ran_model.trainable_variables)
    ran_optim.apply_gradients(zip(ran_model_grads, ran_model.trainable_variables))
```

But in this case, how to resolve this in TensorFlow?",Runtime Error,Runtime Error
"Cannot read properties of undefined (reading 'isVisible') ```javascript
TypeError: Cannot read properties of undefined (reading 'isVisible')
at xEi.convertModelPositionToViewPosition in src/vs/editor/common/viewModel/viewModelLines.ts:846:66
at IEi.convertModelPositionToViewPosition in src/vs/editor/common/viewModel/viewModelLines.ts:1086:22
at _a.Mb in out-vscode/vs/editor/browser/widget/codeEditor/vs/editor/browser/widget/codeEditor/codeEditorWidget.ts:575:65
at _a.getTopForPosition in out-vscode/vs/editor/browser/widget/codeEditor/vs/editor/browser/widget/codeEditor/codeEditorWidget.ts:567:27
at om.cursorAtBoundary in src/vs/workbench/contrib/notebook/browser/viewModel/baseCellViewModel.ts:626:44
at b in src/vs/workbench/contrib/notebook/browser/view/notebookCellList.ts:185:20
at recomputeContext in src/vs/workbench/contrib/notebook/browser/view/notebookCellList.ts:226:7
at x.B in src/vs/base/common/event.ts:1242:13
at x.C in src/vs/base/common/event.ts:1253:9
at x.fire in src/vs/base/common/event.ts:1277:9
at NQ.value in src/vs/workbench/contrib/notebook/browser/viewModel/baseCellViewModel.ts:307:95
at x.B in src/vs/base/common/event.ts:1242:13
at x.C in src/vs/base/common/event.ts:1253:9
at x.fire in src/vs/base/common/event.ts:1277:9
at NQ.value in out-vscode/vs/editor/browser/widget/codeEditor/vs/editor/browser/widget/codeEditor/codeEditorWidget.ts:1736:39
at x.B in src/vs/base/common/event.ts:1242:13
at x.fire in src/vs/base/common/event.ts:1273:9
at Zxi.s in src/vs/editor/common/viewModelEventDispatcher.ts:64:18
at Zxi.endEmitViewEvents in src/vs/editor/common/viewModelEventDispatcher.ts:109:8
at <anonymous> in src/vs/editor/common/viewModel/viewModelImpl.ts:1116:27
at cb in out-vscode/vs/editor/browser/widget/codeEditor/vs/editor/browser/widget/codeEditor/codeEditorWidget.ts:1660:14
at REi.U in src/vs/editor/common/viewModel/viewModelImpl.ts:1111:36
at REi.setSelections in src/vs/editor/common/viewModel/viewModelImpl.ts:1034:8
at _a.setSelections in out-vscode/vs/editor/browser/widget/codeEditor/vs/editor/browser/widget/codeEditor/codeEditorWidget.ts:902:29
at om.setSelections in src/vs/workbench/contrib/notebook/browser/viewModel/baseCellViewModel.ts:531:23
at r in src/vs/workbench/contrib/notebook/browser/view/notebookCellEditorPool.ts:96:11
at _update in src/vs/workbench/contrib/notebook/browser/view/notebookCellEditorPool.ts:105:5
at x.B in src/vs/base/common/event.ts:1242:13
at x.C in src/vs/base/common/event.ts:1253:9
at x.fire in src/vs/base/common/event.ts:1277:9
at NQ.value in out-vscode/vs/editor/browser/widget/codeEditor/vs/editor/browser/widget/codeEditor/codeEditorWidget.ts:1751:36
at x.B in src/vs/base/common/event.ts:1242:13
at x.fire in src/vs/base/common/event.ts:1273:9
at Zxi.s in src/vs/editor/common/viewModelEventDispatcher.ts:64:18
at Zxi.endEmitViewEvents in src/vs/editor/common/viewModelEventDispatcher.ts:109:8
at <anonymous> in src/vs/editor/common/viewModel/viewModelImpl.ts:412:27
at listener in src/vs/editor/common/model/textModel.ts:237:38
at x.B in src/vs/base/common/event.ts:1242:13
at x.C in src/vs/base/common/event.ts:1253:9
at x.fire in src/vs/base/common/event.ts:1277:9
at Oxi.endDeferredEmit in src/vs/editor/common/model/textModel.ts:2515:23
at Om.xb in src/vs/editor/common/model/textModel.ts:1410:23
at Om._applyUndo in src/vs/editor/common/model/textModel.ts:1383:8
at UT.undo in out-vscode/vs/editor/common/model/vs/editor/common/model/editStack.ts:219:14
at <anonymous> in out-vscode/vs/platform/undoRedo/common/vs/platform/undoRedo/common/undoRedoService.ts:1028:67
at invoke in out-vscode/vs/platform/undoRedo/common/vs/platform/undoRedo/common/undoRedoService.ts:745:13
at <anonymous> in out-vscode/vs/platform/undoRedo/common/vs/platform/undoRedo/common/undoRedoService.ts:1028:16
at callback in out-vscode/vs/platform/undoRedo/common/vs/platform/undoRedo/common/undoRedoService.ts:788:11
at E8e.x in out-vscode/vs/platform/undoRedo/common/vs/platform/undoRedo/common/undoRedoService.ts:1026:15
at E8e.A in out-vscode/vs/platform/undoRedo/common/vs/platform/undoRedo/common/undoRedoService.ts:1109:17
at E8e.undo in out-vscode/vs/platform/undoRedo/common/vs/platform/undoRedo/common/undoRedoService.ts:1076:15
at Om.undo in src/vs/editor/common/model/textModel.ts:1556:32
at cke.runEditorCommand in out-vscode/vs/editor/browser/vs/editor/browser/coreCommands.ts:2102:29
at cke._runEditorCommand in out-vscode/vs/editor/browser/vs/editor/browser/coreCommands.ts:341:23
at Object.implementation in out-vscode/vs/editor/browser/vs/editor/browser/coreCommands.ts:312:17
at bT.runCommand in out-vscode/vs/editor/browser/vs/editor/browser/editorExtensions.ts:229:24
at handler in out-vscode/vs/editor/browser/vs/editor/browser/editorExtensions.ts:155:38
at fn in src/vs/platform/instantiation/common/instantiationService.ts:109:11
at o6e.n in src/vs/workbench/services/commands/common/commandService.ts:95:46
at o6e.executeCommand in src/vs/workbench/services/commands/common/commandService.ts:60:17
at OG.M in out-vscode/vs/platform/keybinding/common/vs/platform/keybinding/common/abstractKeybindingService.ts:370:29
at OG.J in out-vscode/vs/platform/keybinding/common/vs/platform/keybinding/common/abstractKeybindingService.ts:225:15
at <anonymous> in out-vscode/vs/workbench/services/keybinding/browser/vs/workbench/services/keybinding/browser/keybindingService.ts:281:38
```
[Go to Errors Site](https://errors.code.visualstudio.com/card?ch=d78a74bcdfad14d5d3b1b782f87255d802b57511&bH=4f227861-8638-518a-9b43-d3984d0914a0)",Runtime Error,Logical Bug
"Use of Keras `jit_compile` in a distribution strategy causes a `std::system_error` <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1.1.33

### GPU model and memory

_No response_

### Current Behaviour?


The following error is thrown during training after a number of steps / epochs 
```shell
terminate called after throwing an instance of 'std::system_error'
what():  Resource temporarily unavailable
```
I am able to reproduce this error in colab with my sample code



### Standalone code to reproduce the issue

```python
import keras
import tensorflow as tf

def build_model_() -> keras.Model:
    input = tf.keras.layers.Input(shape=(5,), name='input_a')
    x = tf.keras.layers.Dense(512, activation = 'relu')(input)
    x = tf.keras.layers.Dense(512, activation = 'relu')(x)
    output = tf.keras.layers.Dense(1, name='output')(x)
    model = tf.keras.models.Model(inputs=input, outputs=output)
    return model


strategy = tf.distribute.MirroredStrategy()
print(f""Can see {strategy.num_replicas_in_sync} gpus"")
with strategy.scope():
  model = build_model_()
  model.compile(loss = 'mse', jit_compile=True)

BATCH_SIZE_PER_REPLICA = 1024
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA

dataset = tf.data.Dataset.from_tensors(
    (tf.ones(5), 1)
).repeat(10_000_000).batch(GLOBAL_BATCH_SIZE).with_options(options)

history = model.fit(
    x = dataset,
    epochs=7,
    verbose = 1,
)
```


### Relevant log output

_No response_</details>",Runtime Error,Runtime Error
"TFLite ConvTranspose3D implemented typo ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

in tflite convtranspose3d optimized kernel, pad depth seems typo error
![image](https://github.com/tensorflow/tensorflow/assets/30307463/39bdf859-f236-48f9-b02a-c8231d11cb88)

And also compared with same convtransposed3d with torch interface, tflite have mismatch

### Standalone code to reproduce the issue

```shell
in tflite convtranspose3d optimized kernel, pad depth seems typo error
![image](https://github.com/tensorflow/tensorflow/assets/30307463/39bdf859-f236-48f9-b02a-c8231d11cb88)

here, seems 
const int spatial_dim_1_padding_after =
      params.padding_values.depth + params.padding_values.depth_offset;
```


### Relevant log output

_No response_",Logical Bug,Performance Issue
"DevTools: Re-enable postMessage transferable for faster ArrayBuffer transfers I got this on FB.com sandbox:

<img width=""815"" alt=""screen shot 2019-03-01 at 1 15 24 pm"" src=""https://user-images.githubusercontent.com/810438/53640457-26dcbb00-3c24-11e9-828f-a987ffeec4da.png"">


---
Originally reported by @gaearon via https://github.com/bvaughn/react-devtools-experimental/issues/25",Runtime Error,Runtime Error
"GPU rendering: workbench.colorCustomizations for brackets only applies after window reload Testing #234577, I only see the bracket color change from `workbench.colorCustomizations` applied after I reload the window. Non gpu lines show up correctly without the reload ",UI/UX Bug,UI/UX Bug
"[DevTools Bug] Cannot add node ""1"" because a node with that id is already in the Store. ### Website or app

local repo

### Repro steps

just loaded the local  react app

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

5.3.1-ccb20cb88b

### Error message (automated)

Cannot add node ""1"" because a node with that id is already in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1172435
    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1141877)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1143565
    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1551564)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add node  because a node with that id is already in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
",Syntax Error,Syntax Error
" Test_on_batch() gives the same loss output on different batches in a single run ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes


### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

windows 10

### Python version

3.10.12

### Current behavior?

I noticed the problem when I got a straight horizontal line on plotting the test results on the trained network. I used the sequential models.

I use train_on_batch(), which gives converging losses. When I switch to test_on_batch(), the losses remain the same for different batches. When I restart the test with different test files, it will give a different loss value, which remains the same for all the batches. In other words, the loss from test_on_batch() remains the some for all batches in a single run.

It's a sequential model.

Here is the code of the section:

    print('mfccs3 value = ', tf.keras.backend.eval(mfccs3[1,:]) )               
    #logs = vadModel.train_on_batch(mfccs3, vadLabel)
    logs = vadModel.test_on_batch(mfccs3, vadLabel) 
    print('string logs = ', str(logs))
The result is:
index = 1

```
mfccs3 value = [[-8.2793800e+01 -5.9538417e+00 9.8302096e-01 -3.5255635e-01
3.0392697e-01 -6.4597696e-01 2.2358397e-02 2.5344249e-02
-6.8171650e-01 -3.7053981e-01 -3.4044239e-01 -8.1056818e-02]]
```

string logs = 0.2398043

index = 2

```
mfccs3 value = [[-69.159195 -2.2269542 4.2501264 -1.3486748 0.62957734
-3.2606528 -3.253118 -3.5308673 -1.1313365 -1.1839466
-2.330786 -1.6313086 ]]
```

string logs = 0.2398043

index = 3

```
mfccs3 value = [[-64.894104 -1.892648 0.11392474 -0.81098145 -1.4640433
-1.1901256 -1.7744782 -0.85753983 -0.9694403 -0.8149232
-1.0680746 -1.0442001 ]]
```

string logs = 0.2398043

You can see that the inputs for test_on_batch() have changed. However, the loss remains the same. I use the same code for train_on_batch(), which gives converging losses.



### Standalone code to reproduce the issue

```shell
logs = vadModel.train_on_batch(mfccs3, vadLabel)
"""""" vs.  """"""
    logs = vadModel.test_on_batch(mfccs3, vadLabel) 

it's just these two lines for a sequential model.
```


### Relevant log output

```shell
I even tried latest Tensorflow version. It has the same problem.

tensorflow version: 2.15.0-dev20230817
listOfFiles 1681
2023-08-17 15:42:12.560549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
model length =  7
Model: ""sequential""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 multiple                  2640      
                                                                 
 dense (Dense)               multiple                  210       
                                                                 
 dense_1 (Dense)             multiple                  11        
                                                                 
=================================================================
Total params: 2861 (11.18 KB)
Trainable params: 2861 (11.18 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```
",Logical Bug,Logical Bug
"Aborted (core dumped) in `tf.config.threading.set_intra_op_parallelism_threads` ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Crash triggered when input negative numbers into tf.config.threading.set_intra_op_parallelism_threads.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/17JV6ppGU1XtQg25PKa8itDhihAspgHIz?usp=sharing
```


### Relevant log output

```shell
2024-08-10 21:29:55.538868: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-10 21:29:55.869155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-10 21:29:55.967845: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-10 21:29:56.002644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-10 21:29:56.222202: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-10 21:30:05.961788: F external/local_tsl/tsl/platform/threadpool.cc:112] Check failed: num_threads >= 1 (1 vs. -1)
Aborted (core dumped)
```
",Runtime Error,Runtime Error
"Bug: Webpack process and setImmediate polyfill interferes with time slicing Here's the copy paste of the polyfill: https://gist.github.com/gaearon/5d2f7bedf76d7bbec2b5c1ee04861ce3

This reproes with old CRA (e.g. `react-scripts@0.7.0`). I saw this on this branch: https://github.com/Swizec/react-fractals/pull/16

The result is concurrent renders get grouped into a huge blocking task:

<img width=""815"" alt=""Screenshot 2021-06-15 at 18 01 27"" src=""https://user-images.githubusercontent.com/810438/122094843-8581cf00-ce04-11eb-9a40-fd3758301255.png"">

Not sure why yet.",Performance Issue,Performance Issue
"data leak in GBDT due to warm start (This is about the non-histogram-based version of GBDTs)

X is split into train and validation data with `train_test_split(random_state=self.random_state)`.

As @johannfaouzi noted, in a warm starting context, this will produce a leak if If `self.random_state` is a `RandomState` instance: some samples that were used for training in a previous `fit` might be used for validation now.

~~I think the right fix would be to raise a `ValueError` if the provided random state isn't a number and early-stopping is activated~~",Security Vulnerability,Security Vulnerability
"Missing _ZdlPv symbol in _argkmin_classmode for manylinux wheels produced by meson The current work-around is to use `-fno-sized-deallocation` see https://github.com/scikit-learn/scikit-learn/pull/28506#discussion_r1512897297 for more details.

This can be reproduced locally with cibuildwheel.
```
python -m cibuildwheel --only cp312-manylinux_x86_64
```
will produced a manylinux wheel is in the wheelhouse folder which you can install through something like this:
```
pip install wheelhouse/scikit_learn-1.5.dev0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
```

Traceback from build log:
```
â¯ python -c 'import sklearn.metrics'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/__init__.py"", line 7, in <module>
    from . import cluster
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/cluster/__init__.py"", line 25, in <module>
    from ._unsupervised import (
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/cluster/_unsupervised.py"", line 23, in <module>
    from ..pairwise import _VALID_METRICS, pairwise_distances, pairwise_distances_chunked
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/pairwise.py"", line 43, in <module>
    from ._pairwise_distances_reduction import ArgKmin
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/__init__.py"", line 94, in <module>
    from ._dispatcher import (
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py"", line 17, in <module>
    from ._argkmin_classmode import (
ImportError: /home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZdlPv
```
",Dependency Issue,Dependency Issue
"dangerouslySetInnerHTML is left empty on client render on top of bad server markup when rendering HTML This seems to be an edge case of https://github.com/facebook/react/issues/11789 fixed in https://github.com/facebook/react/pull/13353/files.

I ran into this when trying to hydrate content rendered with https://github.com/prismicio/prismic-dom `asHtml` method.

**Do you want to request a *feature* or report a *bug*?**

Bug? I think. 

**What is the current behavior?**

Current behavior:

1. Server-side stuff comes in from server and contains the things we need
2. Hydration mismatch happens
3. dangerouslySetInnerHTML is called with correct value but an empty string gets rendered instead

I tried to replicate the issue on https://codesandbox.io/s/2xojk10jln but failed.

The following testcase for `packages/react-dom/src/__tests__/ReactDOMServerIntegrationElements-test.js` produces the same result (I tried it first with the same PrismicDOM.RichText.asHtml(obj) call I have in the app) but I am not sure if it's correct:

  ```js

# test case
    itRenders(
      'a div with dangerouslySetInnerHTML set to html inserted',
      async render => {
        const obj = '<li>bar</li>';
        const e = await render(
          <div dangerouslySetInnerHTML={{__html: obj }} />,
        );
        expect(e.childNodes.length).toBe(1);
        expect(e.firstChild.tagName).toBe('LI');
        expect(e.firstChild.childNodes.length).toBe(1);
      },
    );
```

```bash
      â renders a div with dangerouslySetInnerHTML set to html return value of function called with server string render (190ms)
      â renders a div with dangerouslySetInnerHTML set to html return value of function called with server stream render (52ms)
      â renders a div with dangerouslySetInnerHTML set to html return value of function called with clean client render (37ms)
      â renders a div with dangerouslySetInnerHTML set to html return value of function called with client render on top of good server markup (74ms)
      â renders a div with dangerouslySetInnerHTML set to html return value of function called with client render on top of bad server markup (34ms)

  â ReactDOMServerIntegration âº ... âº renders a div with dangerouslySetInnerHTML set to html return value of function called with client render on top of bad server markup

    expect(received).toBe(expected) // Object.is equality

    Expected: ""bar""
    Received: """"
```


**What is the expected behavior?**

The client render would have rendered `<li>bar</li>`

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

commit c05b4b8  (latest master) and >16.8.

Sorry for a bit vague bug report.
",Logical Bug,Logical Bug
"debug.onDidChangeActiveDebugSession not always fired when active session implicitly changes after old active session terminated <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.94.1 (Universal)
- OS Version: Sequoia 15.0.1

Version: 1.94.1 (Universal)
Commit: e10f2369d0d9614a452462f2e01cdc4aa9486296
Date: 2024-10-05T05:44:32.189Z
Electron: 30.5.1
ElectronBuildId: 10262041
Chromium: 124.0.6367.243
Node.js: 20.16.0
V8: 12.4.254.20-electron.0
OS: Darwin arm64 24.0.0

Steps to Reproduce:

1. Subscribe to `debug.onDidTerminateDebugSession` and `debug.onDidChangeActiveDebugSession`
2. Start two debug sessions.
3. Click on the second one in the call stack view to make it active.
4. Press terminate on the second session via the popup button in the call stack view

Correctly receive the `terminate` event but not always a `active session changed` event.

When the second session is killed, the UI correctly reverts to show the state of the first session - so it is now de facto the active one. E.g. variables and buttons update. The combo box in the toolbar updates to reflect the only remaining session (but doesn't disappear, despite there being only one session to show).

It causes us a problem as we have our own custom debug type and a UI panel that we update to reflect the active debug session, showing a diagram relevant to the active session (if one of ours). When the second session is terminated and the first one becomes active by default, we should update the UI as well, but we don't get the event. And because the vscode UI has de facto changed to have the first one active (but without firing an event), there isn't anything the user can do that will trigger that event to fire.

I'm running our extension from vscode in the extension host.
",Logical Bug,Logical Bug
"MaxListenersExceededWarning: Possible EventEmitter memory leak detected. <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.95.3
- OS Version: Ubuntu 22.04.5 LTS

Steps to Reproduce:

1. Close VSCode with 8 projects opened (most of them DevContainer projects)
2. Open VSCode again from terminal ( running `code --disable-extensions --trace-warnings` to disable extensions and to show warning details) without specifying any folder or file (so last working session is used opening the 8 projects)
3. Following log is printed in the terminal:
```bash
Warning: 'trace-warnings' is not in the list of known options, but still passed to Electron/Chromium.
[main 2024-11-19T10:24:29.689Z] update#setState idle
(node:471892) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 child-process-gone listeners added to [App]. MaxListeners is 10. Use emitter.setMaxListeners() to increase limit
    at genericNodeError (node:internal/errors:984:15)
    at wrappedFn (node:internal/errors:538:14)
    at _addListener (node:events:593:17)
    at App.addListener (node:events:611:10)
    at Object.X [as onWillAddFirstListener] (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:7366)
    at q (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:31:1282)
    at Na.H (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:13550)
    at Na.F (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:11857)
    at Na.start (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:15161)
    at vh.start (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:29191)
    at Object.call (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:36:4563)
    at df.s (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:19676)
    at df.q (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:19199)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:18601)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.C (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:816)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:1033)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:4908)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:964)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:5092)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:964)
    at T (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:7343)
    at IpcMainImpl.i (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:36:21224)
    at IpcMainImpl.emit (node:events:531:35)
    at WebContents.<anonymous> (node:electron/js2c/browser_init:2:85782)
    at WebContents.emit (node:events:519:28)
[main 2024-11-19T10:24:59.835Z] update#setState checking for updates
[main 2024-11-19T10:25:00.551Z] update#setState idle
```
",Security Vulnerability,Security Vulnerability
"BUG internal indexing tools trigger error with pandas < 2.0.0 [#28375](https://github.com/scikit-learn/scikit-learn/pull/28375#issuecomment-2088926826) triggers errors for pandas < 2.0.0, despite just using scikit-learn internal functionalities.

As documented in https://scikit-learn.org/dev/install.html, we have pandas >= 1.1.3.",Syntax Error,Syntax Error
Terminal suggest: fig generator completions often complain about the executable not existing Eg. `npm global remove |` will fail with npm ENOENT,Runtime Error,Dependency Issue
"JSON rendering: Losing syntax coloring after certain size ### Applies To

- [ ] Notebooks (.ipynb files)
- [ ] Interactive Window and\/or Cell Scripts (.py files with \#%% markers)

### What happened?

<img width=""3120"" alt=""Image"" src=""https://github.com/user-attachments/assets/b11f85a7-ad18-405c-9e5c-1850fab9da64"" />


### VS Code Version

1.96.0

### Jupyter Extension Version

2024.11.0

### Jupyter logs

```shell
Visual Studio Code (1.96.0, undefined, desktop)
Jupyter Extension Version: 2024.11.0.
Python Extension Version: 2024.22.0.
Pylance Extension Version: 2024.12.1.
Platform: darwin (arm64).
Temp Storage folder ~/Library/Application Support/Code/User/globalStorage/ms-toolsai.jupyter/version-2024.11.0
Workspace folder ~/work/vscode-jupyter-json, Home = /Users/vesa.vilhonen
12:19:00.403 [info] Starting Kernel (Python Path: ~/.venv/bin/python, Venv, 3.13.0) for '~/work/vscode-jupyter-json/problem.ipynb' (disableUI=true)
12:19:00.475 [info] Process Execution: ~/.venv/bin/python -m pip list
12:19:00.476 [info] Process Execution: ~/.venv/bin/python -c ""import ipykernel; print(ipykernel.__version__); print(""5dc3a68c-e34e-4080-9c3e-2a532b2ccb4d""); print(ipykernel.__file__)""
12:19:00.477 [info] Process Execution: ~/.venv/bin/python -m ipykernel_launcher --f=/Users/~/Library/Jupyter/runtime/kernel-v376d4969e47eafaac7a1a841ee24f71589557f6bc.json
    > cwd: //Users/~/work/vscode-jupyter-json
12:19:00.932 [info] Kernel successfully started
12:19:00.934 [info] Process Execution: ~/.venv/bin/python /Users/~/.vscode/extensions/ms-toolsai.jupyter-2024.11.0-darwin-arm64/pythonFiles/printJupyterDataDir.py
```

### Coding Language and Runtime Version

Python 3.13.0 but happens with every version

### Language Extension Version (if applicable)

_No response_

### Anaconda Version (if applicable)

_No response_

### Running Jupyter locally or remotely?

Local",UI/UX Bug,UI/UX Bug
"Report issue fails with HTTP 401 # Behaviour

After running the ""Python: Report Issue..."" command and filling everything out, the ""Create on GitHub"" button appears to do nothing.

The extension is making a request using `Authorization: Bearer gho_xxx` and gets a HTTP 401 ""Bad credentials"" response from the GitHub API.

## Steps to reproduce:

1. Follow https://github.com/microsoft/vscode-python/wiki/Reporting-a-bug#the-extension-loads or https://github.com/microsoft/vscode-python/wiki/Reporting-a-bug#the-extension-wont-even-start
2. Press ""Create on GitHub""
3. Toggle Developer Tools and check the console logs and network requests

# Diagnostic data

No relevant output for `Python` in the `Output` panel",Security Vulnerability,Security Vulnerability
"Source control missing control buttons 
Type: <b>Bug</b>

Since the last update the list section ""Source Control Repositories"" is missing some buttons, next to the respecting branch names. Also now I'm asked which repo to commit to everytime I try to commit, instead of commiting to the correct repo, that's selected above.

VS Code version: Code 1.97.0 (33fc5a94a3f99ebe7087e8fe79fbe1d37a251016, 2025-02-04T22:41:26.688Z)
OS version: Windows_NT x64 10.0.22631
Modes:


<!-- generated by issue reporter -->",UI/UX Bug,UI/UX Bug
"`FunctionTransformer` need `feature_names_out` even if `func` returns DataFrame ### Describe the bug

Trying to call `transform` for `FunctionTransformer` for which `feature_names_out` is configured raises error that advises to use `set_output(transform='pandas')`. But this doesn't change anything.

### Steps/Code to Reproduce

```py
import numpy as np
import pandas as pd
from sklearn.preprocessing import FunctionTransformer

my_transformer = FunctionTransformer(
    lambda X : pd.concat(
        [
            X[col].rename(f""{col} {str(power)}"")**power
            for col in X
            for power in range(2,4)
        ],
        axis=1
    ),
    feature_names_out = (
        lambda transformer, input_features: [
            f""{feature} {power_str}""
            for feature in input_features
            for power_str in [""square"", ""cubic""]
        ]
    )
)
# I specified transform=pandas
my_transformer.set_output(transform='pandas')
sample_size = 10
X = pd.DataFrame({
    ""feature 1"" : [1,2,3,4,5],
    ""feature 2"" : [3,4,5,6,7]
})
my_transformer.fit(X)
my_transformer.transform(X)
```

### Expected Results

`pandas.DataFrame` like following

|    |   feature 1 square |   feature 1 cubic |   feature 2 square |   feature 2 cubic |
|---:|-------------------:|------------------:|-------------------:|------------------:|
|  0 |                  1 |                 1 |                  9 |                27 |
|  1 |                  4 |                 8 |                 16 |                64 |
|  2 |                  9 |                27 |                 25 |               125 |
|  3 |                 16 |                84 |                 36 |               216 |
|  4 |                 25 |               125 |                 49 |               343 |

### Actual Results

```
ValueError: The output generated by `func` have different column names than the ones provided by `get_feature_names_out`. Got output with columns names: ['feature 1 2', 'feature 1 3', 'feature 2 2', 'feature 2 3'] and `get_feature_names_out` returned: ['feature 1 square', 'feature 1 cubic', 'feature 2 square', 'feature 2 cubic']. The column names can be overridden by setting `set_output(transform='pandas')` or `set_output(transform='polars')` such that the column names are set to the names provided by `get_feature_names_out`.
```

### Versions

```shell
System:
    python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]
executable: /usr/bin/python3
   machine: Linux-6.5.0-14-generic-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.4.1.post1
          pip: 24.0
   setuptools: 68.2.2
        numpy: 1.24.2
        scipy: 1.11.1
       Cython: None
       pandas: 2.2.1
   matplotlib: 3.7.1
       joblib: 1.3.1
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/fedor/.local/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
        version: 0.3.21
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12

       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/fedor/.local/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
    num_threads: 12

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/fedor/.local/lib/python3.10/site-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12
```
```
",Syntax Error,Syntax Error
"[DevTools Bug]: event.metaKey + f to focus SearchInput doesn't work on Windows ### Website or app

https://beta.reactjs.org/

### Repro steps

1. Open React Dev Tools -> Components
2. Try hitting (Windows Key) + f

On Windows 10/11, the Feedback Hub opens up. It will not focus on the search input.

I understand this probably works fine on Mac, but on Windows it'd be great to use a key that won't be intercepted by Windows. Like `Shift + f`. or `Ctrl + Alt + f`. Or, perhaps as soon as I start typing (unless I'm typing in another focused input). Or, if I press `/`. Something!

I thought at first DevTools didn't have a keyboard shortcut but then I looked at the source code and saw it uses [`metaKey`](https://developer.mozilla.org/en-US/docs/Web/API/KeyboardEvent/metaKey) which _totally_ doesn't work on Firefox in Windows.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Dependency Issue,Dependency Issue
"""inline suggestions"" spinner on the language server status item I've seen this every day in the vscode repo for the past few days at least

<img width=""305"" alt=""Image"" src=""https://github.com/user-attachments/assets/fcfb8b95-9dd3-4228-a4ae-36867fa2cfd8"" />",Syntax Error,Syntax Error
"Linux: writing elevated fails with high `ulimit` values <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.96.2 (code_1.96.2-1734607745_amd64.deb)
- OS Version: Ubuntu 24.10

Steps to Reproduce:

1. Open a file that is not writable by the user
2. Edit the file
3. Try to save it
4. Error pops up stating that not enough permissions
5. Retry as Sudo option appears
6. Enter password
7. Saving process running forever

Same problem has been discussed here: https://github.com/microsoft/vscode/issues/234311. 
Is there any progress or plan? Or any workaround ?",Security Vulnerability,Security Vulnerability
"Tests failing when cuda installed but no GPU is present after doing `conda install pytorch cupy`, my tests fail with:

```
FAILED sklearn/metrics/tests/test_common.py::test_array_api_compliance[
accuracy_score-check_array_api_binary_classification_metric-cupy-None-None] 
- cupy_backends.cuda.api.runtime.CUDARuntimeError: cudaErrorNoDevice: 
- no CUDA-capable device is detected
```

I don't think tests should ever fail for this, should they?

cc @ogrisel @betatim ",Runtime Error,Dependency Issue
"Unexpected warning when hydrating with portal and SSR **Do you want to request a *feature* or report a *bug*?**

*bug*

**What is the current behavior?**

Given the following (simplified) snippet:

```jsx
class HoverMenu extends React.Component {
  render() {
    if (typeof document === 'undefined') return null
    const root = document.getElementById('root')
    return ReactDOM.createPortal(<div>Hello World</div>, root)
  }
}

class Para extends React.Component {
  render() {
    return (
      <span>
        Some Text
        <HoverMenu />
      </span>
    )
  }
} 
```

where `div#root` is a valid `div` that exists, the following error is shown when hydrating after SSR:

`Warning: Expected server HTML to contain a matching <div> in <span>`

The warning goes away if I update the definition of `HoverMenu` to:

```jsx
class HoverMenu extends React.Component {
  componentDidMount() {
    this.setState({ isActive: true })
  }
  render() {
    const { isActive} = this.state
    if (!isActive) return null
    const root = document.getElementById('root')
    return ReactDOM.createPortal(<div>Hello World</div>, root)
  }
}
```

I'd prefer not to do that because of the double rendering caused by `setState` in `componentDidMount`.

I don't quite understand what that error is telling me. No `<div />` is rendered server-side in either case. The error is particularly confusing, as the `HoverMenu` DOM `div` is not even rendered inside a DOM `span`. (I wonder if this is happening because `HoverMenu` is nested inside a React `span`.)

**What is the expected behavior?**

No error is thrown. Or, at least that the error message is clearer.

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

Chrome 65
React 16.2
(SSR through Next 5.1)
",Runtime Error,Runtime Error
"MultiOutputClassifier does not rely on estimator to provide pairwise tag ### Describe the bug

I use the `MultiOutputClassifier` function to make `SVC` multilabel. 

Then, if I use the linear or rbf kernel the cross_validation function works perfectly fine.

However, when I use `SVC` with precomputed kernel is having an `ValueError: Precomputed matrix must be a square matrix`. 

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import pairwise_distances
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import cross_val_score, cross_validate

svm = SVC(kernel='precomputed', C=100, random_state=42)
multilabel_classifier = MultiOutputClassifier(svm, n_jobs=-1)

X = np.random.rand(1000, 1000)
y = np.random.randint(0, 2, size=(1000, 6))

kernel_eucl = pairwise_distances(X, metric='euclidean')

cross_validate(
    multilabel_classifier, kernel_eucl, y, cv=10, scoring='f1_weighted', n_jobs=-1
)
```

### Expected Results

An weighted f1-score.

### Actual Results

```pytb
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:

 File ""C:\Users\bscuser\anaconda3\Lib\site-packages\sklearn\svm\_base.py"", line 217, in fit
    raise ValueError(
ValueError: Precomputed matrix must be a square matrix. Input is a 900x1000 matrix.
```

### Versions

```shell
System:
    python: 3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\bscuser\anaconda3\python.exe
   machine: Windows-10-10.0.19045-SP0

Python dependencies:
      sklearn: 1.2.2
          pip: 23.3.1
   setuptools: 68.2.2
        numpy: 1.26.4
        scipy: 1.11.4
       Cython: None
       pandas: 2.1.4
   matplotlib: 3.8.0
       joblib: 1.2.0
threadpoolctl: 2.2.0

Built with OpenMP: True

threadpoolctl info:
       filepath: C:\Users\bscuser\anaconda3\Library\bin\mkl_rt.2.dll
         prefix: mkl_rt
       user_api: blas
   internal_api: mkl
        version: 2023.1-Product
    num_threads: 4
threading_layer: intel

       filepath: C:\Users\bscuser\anaconda3\vcomp140.dll
         prefix: vcomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 4

       filepath: C:\Users\bscuser\anaconda3\Library\bin\libiomp5md.dll
         prefix: libiomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 4
```
",Syntax Error,Syntax Error
"nushell doesn't work with debugpy (python debugger) <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Well. I need Python extension to make it run.

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->

# The issue description (system details at the bottom):

Seems like even though it says that nushell is supported now in VSCode in August release ([**here**](https://code.visualstudio.com/updates/v1_93#_terminal) is the announce), it still can't be used as one of Internal Terminals when you try to debug python script. It gets call with a sequence not compatible with nushell syntax like this one.

a) Pixi's environemnt python bad command example:
```
cmd /C ""c:\dev\data_processing\.pixi\envs\default\python.exe c:\Users\360ci\.vscode\extensions\ms-python.debugpy-2024.12.0-win32-x64\bundled\libs\debugpy\adapter/../..\debugpy\launcher 54917 -- c:\dev\data_processing\test.py ""
```
b) Conda's environment python bad command example:
```
c: && cd c:\dev\sfms && cmd /C ""c:\programs\miniconda\envs\data_proc\python.exe c:\Users\360ci\.vscode\extensions\ms-python.debugpy-2024.12.0-win32-x64\bundled\libs\debugpy\adapter/../..\debugpy\launcher 50270 -- c:\dev\sfms\test.py ""
```
Screenshot:
![Image](https://github.com/user-attachments/assets/14e504d5-dcb7-4ecc-ad7d-6f6af1c904fb)

It DOES work when you just try to run the script because comparing with debug sequence it doesn't have nushell incompatible command in this case. Because when you try to just run it correctly forms the line. Here is example of the command which works

Conda's good and compatible command when you just run without debugger:
```
C:/programs/miniconda/envs/data_proc/python.exe c:/dev/sfms/test.py
```

@anthonykim1, I saw you latest work on having nushell and julia integrated into VSCode, but I think there is a small chance that I misunderstand how it is expected to work?

The isssue is also discussed here in nushell repo:
https://github.com/nushell/nushell/issues/14022
Here is also one of my comments on this problem with some advices from one of the authors:
https://github.com/nushell/nushell/issues/2775#issuecomment-2412005812

@IanManske point outed [here](https://github.com/nushell/nushell/issues/14022#issuecomment-2403325882
) that the problem in these lines where nushell is not supported:
https://github.com/microsoft/vscode/blob/321e1e5b8a0af43a0ee7549713f606936a1ac9ac/src/vs/workbench/contrib/debug/node/terminals.ts#L81

# Details

- VS Code Version: 1.94.2
- OS Version: Win10: 10.0.19045 Build 19045
- Plugins: Python plugin, but no nushell plugin (is it needed?)

Steps to Reproduce:

1. Install nushell in your system.
2. Go to Settings, to to Terminal > Integrated > Default Profile: Windows, set up ""path"" to installed nushell
3. Have a small python program like ```print(""Hello nu in vscode"")```
4. Click on the ""Run"" arrow, selecting ""Python Debugger: Debug Python File""
5. Get error message caused by ill-formed command ```Error:   × Invalid literal``` complaining (see the above the full command it calls) about ""\"" or ""&&"" symbols in the string.",Dependency Issue,Logical Bug
"`Segmentation fault` in `tf.raw_ops.RaggedRange` and `tf.ragged.range` when `starts` or `limits` has large (postive/negative) values. ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`Segmentation fault` in `tf.raw_ops.RaggedRange` and `tf.ragged.range` when `starts` or `limits` has large (postive/negative) values.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1c5J_tSNwLdPmi0TXMaJNNhUb-Ha__dw8?usp=sharing
```


### Relevant log output

```shell
segmentation fault (core dumped)
```
",Runtime Error,Runtime Error
"Stateful LSTM bug with batch size ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid. 
Basically the error is the same as this https://github.com/tensorflow/tensorflow/issues/64061

Below is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

# Set a fixed batch size
batch_size = 32

# Create some random training data
# We'll have sequences of length 5, with 1 feature per time step
sequence_length = 5
num_features = 1
num_samples = 100  # Total number of samples (must be divisible by batch_size)

# Ensure num_samples is a multiple of batch_size
num_samples = (num_samples // batch_size) * batch_size

X_train = np.random.rand(num_samples, sequence_length, num_features)
y_train = np.random.rand(num_samples, 1)  # Example target values

# Reshape y_train to match expected output shape if needed
y_train = y_train.reshape(-1,1)

# Create the stateful LSTM model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.LSTM(units=64,  # Number of LSTM units
                               batch_input_shape=(batch_size, sequence_length, num_features),
                               stateful=True,
                               return_sequences=False)) #often false for a final prediction

model.add(tf.keras.layers.Dense(units=1)) # Output layer with 1 unit

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
epochs = 10

for epoch in range(epochs):
    # Shuffle data indices for each epoch (important for stateful LSTMs)
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    X_train = X_train[indices]
    y_train = y_train[indices]

    model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False) # Shuffle must be false

    # Reset states after each epoch (essential for stateful LSTMs)
    model.reset_states()
```

### Relevant log output

```shell

```",Syntax Error,Syntax Error
"Report issue fails with HTTP 401 # Behaviour

After running the ""Python: Report Issue..."" command and filling everything out, the ""Create on GitHub"" button appears to do nothing.

The extension is making a request using `Authorization: Bearer gho_xxx` and gets a HTTP 401 ""Bad credentials"" response from the GitHub API.

## Steps to reproduce:

1. Follow https://github.com/microsoft/vscode-python/wiki/Reporting-a-bug#the-extension-loads or https://github.com/microsoft/vscode-python/wiki/Reporting-a-bug#the-extension-wont-even-start
2. Press ""Create on GitHub""
3. Toggle Developer Tools and check the console logs and network requests

# Diagnostic data

No relevant output for `Python` in the `Output` panel",Security Vulnerability,Security Vulnerability
"Bug: Memory leak in react while focusing input elements <!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

I 've noticed that having an `input` element focused and unmounting its owner component doesn't actually free up this component from the js heap  , I 've created this simple blnkr to demonstarate this which add a dummy `X` to a ref inside a component that has an input element

React version: 17.0.2

## Steps To Reproduce

1. Go to this [plnkr](https://plnkr.co/edit/IiqhT2JEvZcAJBMu?open=index.js&deferRun=1&preview)  
2. Click Tab Two
3. Click the `Grow button` a few times 
4. Take a heap snapshot 
5. Click the Grow button a few more times 
6. **IMPORTANT** Focus the input element by clicking into it
7. Click Tab One ( This unmount the component that own the ref to the list ) 
8. Take another heap snapshot ( after running the GC)  
9. Compare between the two snapshots 
10. The added string `X` will be found within the `concat check the screenshot 
<img width=""1507"" alt=""Screenshot 2023-01-28 at 1 13 12 AM"" src=""https://user-images.githubusercontent.com/28496859/215225175-f64cf1ee-f6b9-4dff-93d0-4fdebc3747e9.png"">


<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

Link to code example: can be found above

<!--
  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a
  repository on GitHub, or provide a minimal code example that reproduces the
  problem. You may provide a screenshot of the application if you think it is
  relevant to your bug report. Here are some tips for providing a minimal
  example: https://stackoverflow.com/help/mcve.
-->

## The current behavior

The current component that own the input element is still being there in the memory and doesn't free up its own resources afer being unmounted.

## The expected behavior
The current component shloud free up its resources whenever being unmounted 
",Performance Issue,Performance Issue
"Quick search does not retain search term when moving to view Notice the term isn't carried over to the view:

![Image](https://github.com/user-attachments/assets/f9185c18-257f-44b1-b29b-5a85dca47b33)
",UI/UX Bug,UI/UX Bug
"//tensorflow/python/distribute:moving_averages_test is flaky <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute:moving_averages_test fails sometimes.

x86 log
https://source.cloud.google.com/results/invocations/1e048065-76db-4dab-b1a5-093dd542b27d/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5383507734/jobs/9770356325#step:5:8417

Looks like a network port conflict issue

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
INFO: From Testing //tensorflow/python/distribute:moving_averages_test_cpu (shard 3 of 5):
==================== Test output for //tensorflow/python/distribute:moving_averages_test_cpu (shard 3 of 5):
2023-06-28 10:24:27.228811: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 10:24:27.325938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph
INFO:tensorflow:time(__main__.AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph): 0.03s
I0628 10:24:32.088060 139996622530368 test_util.py:2464] time(__main__.AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph): 0.03s
[  SKIPPED ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph
[ RUN      ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MultiWorkerMirrored4x1CPU_mode_graph
W0628 10:24:32.126455 139996622530368 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.
W0628 10:24:32.126971 139996622530368 collective_all_reduce_strategy.py:394] Collective ops is not configured at program startup. Some performance features may not be enabled.
INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)
I0628 10:24:32.164073 139996622530368 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/device:CPU:0',)
INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
I0628 10:24:32.171577 139996622530368 collective_all_reduce_strategy.py:446] Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
INFO:tensorflow:Using local port 43531
I0628 10:24:32.173116 139996622530368 test_util.py:3796] Using local port 43531
INFO:tensorflow:Using local port 34115
I0628 10:24:32.173517 139996622530368 test_util.py:3796] Using local port 34115
INFO:tensorflow:Using local port 34873
I0628 10:24:32.173699 139996622530368 test_util.py:3796] Using local port 34873
INFO:tensorflow:Using local port 40455
I0628 10:24:32.173828 139996622530368 test_util.py:3796] Using local port 40455
2023-06-28 10:24:32.849027: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 10:24:32.904538: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-28 10:24:32.917529: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 10:24:32.995322: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[chief-0]:     I0628 10:24:34.534415 140662937323328 multi_process_runner.py:840] Subprocess with PID 1244 (chief, 0) is now being started.
[chief-0]:     I0628 10:24:34.534823 140662937323328 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""chief"": [""localhost:43531""], ""worker"": [""localhost:34115"", ""localhost:34873"", ""localhost:40455""]}, ""task"": {""type"": ""chief"", ""index"": 0}, ""rpc_layer"": ""grpc""}'
I0628 10:24:34.561606 139996622530368 multi_process_runner.py:989] Waiting for the result from chief-0
[worker-1]:    I0628 10:24:34.596723 140662937323328 multi_process_runner.py:840] Subprocess with PID 1479 (worker, 1) is now being started.
[worker-1]:    I0628 10:24:34.597120 140662937323328 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""chief"": [""localhost:43531""], ""worker"": [""localhost:34115"", ""localhost:34873"", ""localhost:40455""]}, ""task"": {""type"": ""worker"", ""index"": 1}, ""rpc_layer"": ""grpc""}'
[worker-1]:    2023-06-28 10:24:34.655212: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:34873
[worker-1]:    INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
[worker-1]:    I0628 10:24:34.661444 140662937323328 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
[chief-0]:     E0628 10:24:34.621842982    1244 server_chttp2.cc:40]        {""created"":""@1687947874.621805165"",""description"":""No address added out of total 1 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":395,""referenced_errors"":[{""created"":""@1687947874.621803913"",""description"":""Failed to add any wildcard listeners"",""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":342,""referenced_errors"":[{""created"":""@1687947874.621786452"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::]:43531""},{""created"":""@1687947874.621803346"",""description"":""Unable to configure socket"",""fd"":9,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1687947874.621800946"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}
[chief-0]:     2023-06-28 10:24:34.621944: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server
[worker-0]:    I0628 10:24:34.608072 140662937323328 multi_process_runner.py:840] Subprocess with PID 1472 (worker, 0) is now being started.
[chief-0]:     2023-06-28 10:24:34.622255: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:703] Could not start gRPC server
[chief-0]:     Process _Process-3:
[chief-0]:     Traceback (most recent call last):
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/strategy_combinations.py"", line 207, in skip_if_cannot_start_grpc_server
[chief-0]:         return _create_multi_worker_mirrored()
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/strategy_combinations.py"", line 189, in _create_multi_worker_mirrored
[chief-0]:         strategy = CollectiveAllReduceStrategy(cluster_resolver=resolver)
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 186, in __init__
[chief-0]:         CollectiveAllReduceExtended(
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 339, in __init__
[chief-0]:         self._initialize_strategy(self._cluster_resolver, devices=devices)
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 358, in _initialize_strategy
[chief-0]:         self._initialize_multi_worker(cluster_resolver)
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 530, in _initialize_multi_worker
[chief-0]:         context.context().ensure_initialized()
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 610, in ensure_initialized
[chief-0]:         pywrap_tfe.TFE_EnableCollectiveOps(context_handle, server_def_str)
[chief-0]:     tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server
```
</details>",Runtime Error,Runtime Error
"[Compiler Bug]: eslint plugin rule only applied to first instance inside a file ### What kind of issue is this?

- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)
- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)
- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)
- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)

### Link to repro

https://playground.react.dev/#N4Igzg9grgTgxgUxALhHCA7MAXABAMwglwF5cAKASlID4LqS7gBfAHQ3YQA8AHCGPABME+AIZQANnnxQMcbAEtMuALIBPAII8eVXMHa5c6LNKIBhURIkAjUXADWpXACUEd7ADooYBBau2HckIIKkoDI0wcXFsYPxs7RzJXdy8fOID7IKJQ9nCYBGxYDFwAHkEFADcaAAkEK2IAdX4JQRKAenKqgG52NgwQZiA

### Repro steps

For the rule 'Expected the first argument to be an inline function expression', it is only applied to the first violation even if there is more than one violation. (Example in playground link.) Moreover, on my setup if I eslint-disable the first violation and then eslint-enable after, then subsequent violations still aren't shown. I couldn't get eslint-disable to work in the playground though.

### How often does this bug happen?

Every time

### What version of React are you using?

18.3.1

### What version of React Compiler are you using?

n/a eslint-plugin issue",Logical Bug,Logical Bug
"deleted files don't go to trash 
Type: <b>Bug</b>

I delete files in the explorer using `del` key of keyboard, files are deleted and cannot be found in the trash bin

VS Code version: Code 1.95.2 (e8653663e8840adaf45af01eab5c627a5af81807, 2024-11-07T11:07:22.054Z)
OS version: Linux x64 6.8.0-48-generic snap
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-1365U (12 x 3700)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 1|
|Memory (System)|31.00GB (22.61GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 8447ec3e-1827-4960-bb59-12316efae9e7|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|wayland|
</details><details><summary>Extensions (14)</summary>

Extension|Author (truncated)|Version
---|---|---
gitlens|eam|15.6.3
ftp-simple|hum|0.7.6
git-graph|mhu|1.30.0
autopep8|ms-|2024.0.0
debugpy|ms-|2024.12.0
python|ms-|2024.18.1
vscode-pylance|ms-|2024.11.1
jupyter|ms-|2024.10.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.21
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.388.0
LiveServer|rit|5.7.9


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
pythongtdpath:30769146
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
bdiig495:31013172
dvdeprecation:31068756
dwnewjupyter:31046869
impr_priority:31102340
nativerepl2:31139839
refactort:31108082
pythonrstrctxt:31112756
cf971741:31144450
iacca1:31171482
notype1cf:31157160
5fd0e150:31155592
dwcopilot:31170013
j44ff735:31177056

```

</details>

<!-- generated by issue reporter -->",Security Vulnerability,Security Vulnerability
"XLA inference fails complaning about branch shape mismatches <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A100 40 GB

### Current Behaviour?

```shell
I have the following function. 

First, I initialize a CLIP-based text encoder:


from keras_cv.models.stable_diffusion.text_encoder import TextEncoder

MAX_PROMPT_LENGTH = 77
text_encoder = TextEncoder(MAX_PROMPT_LENGTH)


Then, I am using the `text_encoder` like so in a function that I use to serialize the `text_encoder` as a `SavedModel`:

```python
from keras_cv.models.stable_diffusion.constants import _UNCONDITIONAL_TOKENS
import tensorflow as tf 

signature_dict = {
    ""tokens"": tf.TensorSpec(shape=[None, 77], dtype=tf.int32, name=""tokens""),
}

def text_encoder_exporter(model: tf.keras.Model):
    BATCH_SIZE = 3
    MAX_PROMPT_LENGTH = 77
    POS_IDS = tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)
    UNCONDITIONAL_TOKENS = tf.convert_to_tensor([_UNCONDITIONAL_TOKENS], dtype=tf.int32)

    @tf.function(input_signature=[signature_dict])
    def serving_fn(inputs):
        # context
        encoded_text = model([inputs[""tokens""], POS_IDS], training=False)
        encoded_text = tf.squeeze(encoded_text)

        if tf.rank(encoded_text) == 2:
            encoded_text = tf.repeat(
                tf.expand_dims(encoded_text, axis=0), BATCH_SIZE, axis=0
            )

        # unconditional context
        unconditional_context = model([UNCONDITIONAL_TOKENS, POS_IDS], training=False)

        unconditional_context = tf.repeat(unconditional_context, BATCH_SIZE, axis=0)
        return {""context"": encoded_text, ""unconditional_context"": unconditional_context}

    return serving_fn
```

Serialization:

```python
tf.saved_model.save(
    text_encoder,
    ""./text_encoder/1/"",
    signatures={""serving_default"": text_encoder_exporter(text_encoder)},
)
```

Now, while attempting to XLA-compile:

```python
from tensorflow.python.saved_model import tag_constants

batch_size = 3
saved_model_loaded = tf.saved_model.load(
    ""./text_encoder/1/"", tags=[tag_constants.SERVING]
)
text_encoder_predict_fn = saved_model_loaded.signatures[""serving_default""]
# Raises error
xla_text_encoder_predict_fn = tf.function(text_encoder_predict_fn, jit_compile=True)
xla_text_encoder_predict_fn(
    tokens=tf.ones((batch_size, MAX_PROMPT_LENGTH), tf.int32)
).keys()
```
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/gist/sayakpaul/d7dafc252752a6c1ce10e85d8162b8ea/scratchpad.ipynb
```


### Relevant log output

```shell
/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50   try:
     51     ctx.ensure_initialized()
---> 52     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                         inputs, attrs, num_outputs)
     54   except core._NotOkStatusException as e:

InvalidArgumentError: left_branch_shape.rank() != right_branch_shape.rank() (4 vs 3)
	 [[{{function_node __inference_serving_fn_55983}}{{node cond}}]] [Op:__inference_function_60000]
```
</details>",Runtime Error,Runtime Error
"Code are constantly freezes 
Type: <b>Bug</b>

When I'm trying to type in the code area itself and any popups (Auto complete, Copilot, code line information) will stop updating for a few seconds and is constantly happening. Scrolling will partially show the top and bottom of the area until all of it unfreezes again. This started happening with 1.95.2 I believe.

VS Code version: Code 1.95.3 (f1a4fb101478ce6ec82fe9627c43efbf9e98c813, 2024-11-13T14:50:04.152Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 5 2600 Six-Core Processor             (12 x 3394)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.93GB (11.80GB free)|
|Process Argv|D:\\Joshs PC\\Games\\Servers\\Minecraft\\1.20.6 Paper - NeonCraftCity\\plugins\\Denizen\\scripts\\serverOther.dsc --crash-reporter-id 0ea9fbe9-16bc-407b-a35e-44a995b881ff|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (22)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-css-formatter|aes|1.0.2
vscode-m3u|af4|1.0.0
spellright|ban|3.0.140
denizenscript|den|1.4.7
copilot|Git|1.248.0
copilot-chat|Git|0.23.2024102903
vscode-autohotkey-plus-plus|mar|6.4.1
rainbow-csv|mec|3.13.0
vscode-html-format|moh|0.1.6
csharp|ms-|2.55.29
vscode-dotnet-runtime|ms-|2.2.3
debugpy|ms-|2024.12.0
python|ms-|2024.20.0
vscode-pylance|ms-|2024.12.1
remote-wsl|ms-|0.88.5
powershell|ms-|2024.4.0
vsliveshare|ms-|1.0.5941
indent-rainbow|ode|8.3.1
LiveServer|rit|5.7.9
HOCON|sab|0.0.1
vscode-gradle|vsc|3.16.4
volar|Vue|2.1.10


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
vsaa593cf:30376535
py29gd2263:31024239
vscaac:30438847
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
pythonnoceb:30805159
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
dvdeprecation:31068756
dwnewjupytercf:31046870
2f103344:31071589
nativerepl1:31139838
pythonrstrctxt:31112756
nativeloc2:31192216
cf971741:31144450
iacca1:31171482
notype1cf:31157160
5fd0e150:31155592
dwcopilot:31170013
stablechunks:31184530

```

</details>

<!-- generated by issue reporter -->",Performance Issue,Performance Issue
"Change event fires extra times before IME composition ends ### Extra details

* Similar discussion with extra details and reproducing analysis: https://github.com/facebook/react/issues/8683
* Previous attempt to fix it: https://github.com/facebook/react/pull/8438 (includes some unit tests, but sufficient to be confident in the fix)

------

### Original Issue


When I was trying this [example](https://jsfiddle.net/reactjs/n47gckhr/light/) from https://facebook.github.io/react/blog/2013/11/05/thinking-in-react.html, any Chinese characters inputted by Chinese pinyin input method would fire too many renders like:

![screen shot 2015-05-21 at 14 04 36](https://cloud.githubusercontent.com/assets/1091472/7742565/2c8625b0-ffc3-11e4-8ac2-d7eb22a3aef3.png)

Actually I would expect those not to fire before I confirm the Chinese character.

Then I tried another kind of input method - wubi input method, I got this:

![screen shot 2015-05-21 at 14 17 15](https://cloud.githubusercontent.com/assets/1091472/7742657/42c27bac-ffc4-11e4-8a3a-7d4550e88a59.png)

It's weird too. So I did a test [in jQuery](http://jsbin.com/yepogahobo/1/edit?html,js,console,output):

![screen shot 2015-05-21 at 14 05 12](https://cloud.githubusercontent.com/assets/1091472/7742591/71da6842-ffc3-11e4-9d7a-a8438721029c.png)

Only after I press the space bar to confirm the character, the `keyup` event would fire.

I know it might be different between the implementation of jQuery `keyup` and react `onChange` , but I would expect the way how jQuery `keyup` handles Chinese characters instead of react's `onChange`.
",Performance Issue,Performance Issue
"Memory leak I'm getting this in dev console. Based on git-blame at contentHoverStatusBar.ts:48, assigning to you  @aiday-mar 

Not sure how to repro, but should be traceable by code inspection.

```
Error: Trying to add a disposable to a DisposableStore that has already been disposed of. The added object will be leaked!
    at rhi.add (lifecycle.ts:425:18)
    at qse.B (lifecycle.ts:497:22)
    at qse.addAction (contentHoverStatusBar.ts:48:8)
    at markerHoverParticipant.ts:239:23
```

![Image](https://github.com/user-attachments/assets/e7d5a8e0-eda1-4c1e-8ce3-2f41d2c30587)

Version: 1.96.0-insider
Commit: 275faf6f08b7aa50843f3c18406b4d5969784e52
Date: 2024-11-28T18:04:55.375Z
Electron: 32.2.6
ElectronBuildId: 10629634
Chromium: 128.0.6613.186
Node.js: 20.18.1
V8: 12.8.374.38-electron.0
OS: Darwin arm64 24.1.0
",Security Vulnerability,Security Vulnerability
"User settings UI does not update when configuration changes Does this issue occur when all extensions are disabled?: Yes

Version: 1.96.2 (Universal)
Commit: fabdb6a30b49f79a7aba0f2ad9df9b399473380f
Date: 2024-12-19T10:22:47.216Z
Electron: 32.2.6
ElectronBuildId: 10629634
Chromium: 128.0.6613.186
Node.js: 20.18.1
V8: 12.8.374.38-electron.0
OS: Darwin arm64 24.2.0

Steps to Reproduce:

1. Implement configuration change handler for a configuration value that will update a configuration value dynamically (so that when the user has the user settings window open a configuration value will update)
2. Update a configuration value in the user settings UI that will trigger an update to a setting while the user settings window is opened and focused.

Note that the UI will become stale.  If the UI loses focus it will update but it will not update as long as the UI is focused.
",Dependency Issue,Dependency Issue
"VS Code Speech not detecting any voice <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No
YES, No advice available on the internet has resolved the issue so far.

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 
Version: 1.97.0-insider
Commit: 6a5c8cf95aa814b5cb92de617096144b1f45af2d
Date: 2024-12-12T05:04:17.220Z
Electron: 32.2.6
ElectronBuildId: 10629634
Chromium: 128.0.6613.186
Node.js: 20.18.1
V8: 12.8.374.38-electron.0
OS: Linux x64 6.8.0-50-generic
Exact Linux distro: Ubuntu 24.04.1 LTS

### Summary of Tests and Findings

#### Steps to Reproduce:

1. Verified microphone functionality:
   - Microphone works as expected in other applications.
   - Tested using `pactl` and `parecord` to confirm audio input is functioning properly.
2. Reinstalled **VS Code** and **VS Code Insiders** from scratch.
3. Installed the required VS Code extension for speech-to-text functionality.
4. When pressing **Alt+Ctrl+V**, the microphone icon appears, indicating that the session starts successfully. However, speech is not transcribed to text.
5. Examined the speech log:
   ```
   2024-12-12 11:37:33.892 [trace] [vscode-speech-0] starting speech-to-text session 
   (language: en-US, model: Microsoft Speech Recognizer en-US FP Model V9, 
   path: /home/d/.vscode-insiders/extensions/ms-vscode.vscode-speech-0.12.1-linux-x64/assets/stt)
   ```
6. Examined  renedring.log after starting vscode speech by pressing ctrl+alt+v 

```
[error] Extension host (LocalProcess pid: 32490) terminated unexpectedly. The following extensions were running: vscode.github-authentication, vscode.emmet, vscode.git-base, vscode.git, vscode.github, vscode.debug-auto-launch, vscode.merge-conflict, ms-vscode.vscode-speech
[info] Automatically restarting the extension host.

```
### Related Issue

The issue appears to align with a previously **Closed** GitHub issue #207290  where users reported similar behavior:  
- The speech-to-text extension starts but does not transcribe audio input despite showing no visible errors or crashes.  

---",Runtime Error,Runtime Error
"Automatically adjust minimap scale according to display scale <!-- ???? Do Not Delete This! feature_request_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

I have a 27"" 4K monitor as my main display and a 24"" 1080p monitor as a secondary display. To make everything have consistent sizes between the two displays, I have set my main display to 175% scale. The majority of VS Code scales accordingly, but the minimap does not, which results in it being very small and difficult to use. I can change `editor.minimap.scale` to 2 to account for this on my main display, but then if I move a window over to my secondary display, the minimap is far too large.

I would like the minimap to automatically account for display scaling (or have a way to set a different scale per display) such that it is a consistent size on my two displays. This may require supporting non-integer scales (#84168), or it could round to the nearest integer (which may not be ideal for 150% scale).",UI/UX Bug,UI/UX Bug
"The latest Insider build breaks Language Input Method applications (like Unikey) <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: VSCode 1.96.4; VSCode Insider 1.97.0
- OS Version: Windows 11 24H2

Steps to Reproduce:

My Input Method application works fine with VSCode Stable 1.96.4, but not working properly with VSCode Insider 1.97.0.
Please view the videos bellow to see what happened:

With VSCode Stable 1.96.4:

https://github.com/user-attachments/assets/1b8e6617-6dcf-444b-8e11-f154564eccdc

With VSCode Insider 1.97.0:

https://github.com/user-attachments/assets/bc778e02-c122-4f00-a958-399e82903ee7

Maybe related: https://github.com/Microsoft/vscode/issues/239301

Thanks!",Dependency Issue,Dependency Issue
"inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

getting inconsistent results of ```tf.image.adjust_hue``` between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

images = tf.constant([
    [[ 1.9720840,  2.1302242, -0.1902120],
     [ 0.6557856, -1.3016001,  1.1452782]],
    
    [[-2.2193234,  0.3198028,  0.9568117],
     [-0.3937407, -0.0503466, -0.3693791]]
], dtype=tf.float32)

delta = tf.constant(-0.7441734, dtype=tf.float32)

with tf.device('CPU:0'):
    adjusted_cpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on CPU:\n"", adjusted_cpu)

with tf.device('GPU:0'):
    adjusted_gpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on GPU:\n"", adjusted_gpu)


is_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e-5, rtol=1e-6)

max_abs_diff = tf.reduce_max(tf.abs(adjusted_cpu - adjusted_gpu)).numpy()

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:"", is_consistent.numpy())
```

### Relevant log output

```shell
Adjusted Hue on CPU:
 tf.Tensor(
[[[-0.190212    2.1302242   1.2092681 ]
  [ 1.1452782  -0.48211157 -1.3016001 ]]

 [[ 0.11679006 -2.2193234   0.9568117 ]
  [-0.3937407  -0.25841027 -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Adjusted Hue on GPU:
 tf.Tensor(
[[[-0.19021209  2.1302242   1.209268  ]
  [ 1.1452781  -0.48211193 -1.3016001 ]]

 [[ 0.11678863 -2.2193234   0.95681167]
  [-0.0503466  -0.0503466  -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Max absolute difference: 0.3433941
Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False
```",Logical Bug,Logical Bug
"[DevTools Bug]: Source map error: URL: react_devtools_backend_compact.js.map ### Website or app

https://github.com/avenida714/alec-synth/blob/dccf984d89ae44558c70ff93dfa03b1227d5df5b/src/App.jsx#L63-L64C17

Link found on the vite getting started page: https://stackblitz.com/edit/vitejs-vite-zff6zx?file=src%2FApp.jsx&terminal=dev

### Repro steps

â ï¸ This is not my code (not public) â ï¸. While I searched for this error I stumble upon this repo. I had this issue when adding the plugin for first time.

1. Open the website
2. Open console
3. See the error

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Dependency Issue,Dependency Issue
"//tensorflow/python/distribute:vars_test_2gpu is flaky <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute:vars_test_2gpu timeouts sometimes.

x86 log
https://source.cloud.google.com/results/invocations/769764d8-8dc9-46fa-a284-78062efe3bd9/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5363572382/jobs/9731245377#step:5:9313

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=rbe --config=pycpp --config=build_event_export
```


### Relevant log output

```shell
[ RUN      ] SyncOnReadScatterReplicaTest.testScatterMax_test_aggregation_VariableAggregationMEAN_distribution_MultiWorkerMirrored2x1CPU_mode_graph_usevarpolicy_True
W0628 01:02:37.513197 140343714740032 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.
W0628 01:02:37.513532 140343714740032 collective_all_reduce_strategy.py:394] Collective ops is not configured at program startup. Some performance features may not be enabled.
INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)
I0628 01:02:37.516878 140343714740032 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/device:CPU:0',)
INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
I0628 01:02:37.517337 140343714740032 collective_all_reduce_strategy.py:446] Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
[chief-0]:     W0628 01:02:37.520201 140325400614720 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
I0628 01:02:37.521484 140343714740032 multi_process_runner.py:989] Waiting for the result from chief-0
[worker-0]:    W0628 01:02:37.523296 140325400614720 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
[chief-0]:     W0628 01:02:37.521138 140325400614720 context.py:881] Enabling collective ops after program startup may cause error when accessing previously created tensors.
[worker-0]:    W0628 01:02:37.524588 140325400614720 context.py:881] Enabling collective ops after program startup may cause error when accessing previously created tensors.
[chief-0]:     INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:chief/replica:0/task:0/device:CPU:0', '/job:chief/replica:0/task:0/device:CPU:1']
[chief-0]:     I0628 01:02:37.525059 140325400614720 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['/job:chief/replica:0/task:0/device:CPU:0', '/job:chief/replica:0/task:0/device:CPU:1']
[worker-0]:    INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:CPU:1']
[worker-0]:    I0628 01:02:37.527691 140325400614720 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:CPU:1']
[chief-0]:     INFO:tensorflow:Using MirroredStrategy with devices ('/job:chief/task:0/device:CPU:0',)
[chief-0]:     I0628 01:02:37.529053 140325400614720 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/job:chief/task:0/device:CPU:0',)
[chief-0]:     INFO:tensorflow:Check health not enabled.
[chief-0]:     I0628 01:02:37.529462 140325400614720 collective_all_reduce_strategy.py:574] Check health not enabled.
[chief-0]:     INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('/job:chief/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[chief-0]:     I0628 01:02:37.529720 140325400614720 collective_all_reduce_strategy.py:576] MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('/job:chief/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[worker-0]:    INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0/device:CPU:0',)
[worker-0]:    I0628 01:02:37.531570 140325400614720 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/job:worker/task:0/device:CPU:0',)
[worker-0]:    INFO:tensorflow:Check health not enabled.
[worker-0]:    I0628 01:02:37.532161 140325400614720 collective_all_reduce_strategy.py:574] Check health not enabled.
[worker-0]:    INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[worker-0]:    I0628 01:02:37.532691 140325400614720 collective_all_reduce_strategy.py:576] MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[worker-0]:    2023-06-28 01:02:37.540481: I tensorflow/core/distributed_runtime/master.cc:240] Scanning workers for devices: 1 total workers
[chief-0]:     2023-06-28 01:02:37.541763: I tensorflow/core/distributed_runtime/master.cc:240] Scanning workers for devices: 1 total workers
-- Test timed out at 2023-06-28 01:07:29 UTC --
Thread 0x00007fa429ef8700 (most recent call first):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fa42a779700 (most recent call first):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 527 in _process_watchdog
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fa44d0fc700 (most recent call first):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Current thread 0x00007fa451437740 (most recent call first):
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 379 in _recv
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 414 in _recv_bytes
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 250 in recv
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 991 in run
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/combinations.py"", line 580 in decorator
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343 in execute_test_method
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360 in decorated
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/parameterized.py"", line 314 in bound_param_test
  File ""/usr/lib/python3.9/unittest/case.py"", line 550 in _callTestMethod
  File ""/usr/lib/python3.9/unittest/case.py"", line 592 in run
  File ""/usr/lib/python3.9/unittest/case.py"", line 651 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/runner.py"", line 184 in run
  File ""/usr/lib/python3.9/unittest/main.py"", line 271 in runTests
  File ""/usr/lib/python3.9/unittest/main.py"", line 101 in __init__
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/app.py"", line 258 in _run_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/app.py"", line 312 in run
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/eager/test.py"", line 25 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 167 in test_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 1455 in test_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/test_util.py"", line 138 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/vars_test.py"", line 1336 in <module>
```
</details>",Runtime Error,Runtime Error
"Single character change triggers 3 'textDocument/inlayHint' request in Python <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.97.0-insiders
- OS Version: windows

```
	Line    42: 2025-02-03 12:11:20.878 [info] [Trace - 12:11:20 PM] Sending request 'textDocument/inlayHint - (50)'.
	Line  4534: 2025-02-03 12:11:21.548 [info] [Trace - 12:11:21 PM] Sending request 'textDocument/inlayHint - (56)'.
	Line  4552: 2025-02-03 12:11:21.648 [info] [Trace - 12:11:21 PM] Sending request 'textDocument/inlayHint - (57)'.
```
full LSP trace
[log.txt](https://github.com/user-attachments/files/18648248/log.txt)",UI/UX Bug,UI/UX Bug
"Terminal Cursor is at the wrong place with Python3.13 
The terminal cursor seems to consistently be at wrong (further in the line) place, when user tries to use python3.13 inside the terminal:
![Image](https://github.com/user-attachments/assets/59081356-6531-4eac-b4a7-f73794ccf428)

I think it is VS Code thing, since the cursor seems ""normal"" for external terminal and Pycharm. 
 ![Image](https://github.com/user-attachments/assets/ad757361-94c9-4113-b771-fed95f4b3333)

The only Python prompt specific code that I recall is:
https://github.com/microsoft/vscode/blob/0561ca03c895239131a526d8731da0e53027167c/src/vs/platform/terminal/common/capabilities/commandDetectionCapability.ts#L961 but wondering if this is the right place to look at.

",UI/UX Bug,UI/UX Bug
"SequentialFeatureSelector is not working with ColumnTransformer ### Describe the bug

Please see the code.

### Steps/Code to Reproduce

```python
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

import pandas as pd
import numpy as np


# dummy data
N = 100
dummy_x = pd.DataFrame(
    np.random.randn(N,3),
    columns = list('abc'),
)

dummy_y = pd.DataFrame(
    np.random.choice([0,1], size= (N,1)),
    columns = ['label'],
)


num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy=""median"")),
        ('std_scaler', StandardScaler()),
    ])

ct_parts = [
                ('num', num_pipeline, [0,1,2]),
]

data_preparation_pipe = ColumnTransformer(ct_parts, remainder='passthrough')


from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.ensemble import GradientBoostingClassifier

model = Pipeline(
    [
        # ('data_prep', num_pipeline),
        ('data_prep', data_preparation_pipe),
        ('ML', GradientBoostingClassifier()),
    ]
)

sfs = SequentialFeatureSelector(
    model,
)

sfs.fit(dummy_x, dummy_y)
```




### Expected Results

No error

### Actual Results

```python-traceback
Traceback (most recent call last):
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3460, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""/tmp/ipykernel_3433582/3663298101.py"", line 1, in <module>
    sfs.fit(dummy_x, dummy_y)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/feature_selection/_sequential.py"", line 268, in fit
    new_feature_idx, new_score = self._get_best_new_feature_score(
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/feature_selection/_sequential.py"", line 299, in _get_best_new_feature_score
    scores[feature_idx] = cross_val_score(
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/model_selection/_validation.py"", line 515, in cross_val_score
    cv_results = cross_validate(
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/model_selection/_validation.py"", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/model_selection/_validation.py"", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 5 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
5 fits failed with the following error:
Traceback (most recent call last):
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/utils/__init__.py"", line 416, in _get_column_indices
    idx = _safe_indexing(np.arange(n_columns), key)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/utils/__init__.py"", line 356, in _safe_indexing
    return _array_indexing(X, indices, indices_dtype, axis=axis)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/utils/__init__.py"", line 185, in _array_indexing
    return array[key] if axis == 0 else array[:, key]
IndexError: index 1 is out of bounds for axis 0 with size 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/model_selection/_validation.py"", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/pipeline.py"", line 401, in fit
    Xt = self._fit(X, y, **fit_params_steps)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/pipeline.py"", line 359, in _fit
    X, fitted_transformer = fit_transform_one_cached(
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/joblib/memory.py"", line 349, in __call__
    return self.func(*args, **kwargs)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/pipeline.py"", line 893, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/utils/_set_output.py"", line 142, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py"", line 724, in fit_transform
    self._validate_column_callables(X)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py"", line 426, in _validate_column_callables
    transformer_to_input_indices[name] = _get_column_indices(X, columns)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/utils/__init__.py"", line 418, in _get_column_indices
    raise ValueError(
ValueError: all features must be in [0, 0] or [-1, 0]
```

### Versions

```shell
System:
    python: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0]
executable: /software/anaconda3/envs/TOSC_ML/bin/python
   machine: Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.28

Python dependencies:
      sklearn: 1.2.1
          pip: 23.0
   setuptools: 67.3.2
        numpy: 1.23.5
        scipy: 1.10.0
       Cython: None
       pandas: 1.5.3
   matplotlib: 3.7.0
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /software/anaconda3/envs/TOSC_ML/lib/libopenblasp-r0.3.21.so
        version: 0.3.21
threading_layer: pthreads
   architecture: Zen
    num_threads: 128

       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /software/anaconda3/envs/TOSC_ML/lib/libgomp.so.1.0.0
        version: None
    num_threads: 128
```
",Syntax Error,Syntax Error
"Weights are being normalized using number of samples as opposed to sum in GaussianMixture ### Describe the bug

Weights are being normalized at Line https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/mixture/_gaussian_mixture.py#L718 using `n_samples`. It should be done using `weights.sum()` as
done in `_m_step()` here: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/mixture/_gaussian_mixture.py#L756.

### Steps/Code to Reproduce

Weights are being normalized at Line https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/mixture/_gaussian_mixture.py#L718 using `n_samples`. It should be done using `weights.sum()` as
done in `_m_step()` here: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/mixture/_gaussian_mixture.py#L756.

### Expected Results

Correct weights.

### Actual Results

Incorrect weights.

### Versions

```shell
System:
    python: 3.9.13 (main, May 24 2022, 21:28:31)  [Clang 13.1.6 (clang-1316.0.21.2)]
executable: /Users/kshitijgoel/Documents/main/code.nosync/self_organizing_gmm/.venv/bin/python
   machine: macOS-12.4-x86_64-i386-64bit

Python dependencies:
      sklearn: 1.1.1
          pip: 22.2.1
   setuptools: 62.3.2
        numpy: 1.23.1
        scipy: 1.8.1
       Cython: 0.29.30
       pandas: 1.4.3
   matplotlib: 3.5.2
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: False

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/kshitijgoel/Documents/main/code.nosync/self_organizing_gmm/.venv/lib/python3.9/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.20
threading_layer: pthreads
   architecture: Haswell
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/kshitijgoel/Documents/main/code.nosync/self_organizing_gmm/.venv/lib/python3.9/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.17
threading_layer: pthreads
   architecture: Haswell
    num_threads: 8
```
",Logical Bug,Syntax Error
"Editor GPU: Scrolling through long comment causes noticeable lag From https://github.com/microsoft/vscode/issues/221145#issuecomment-2612075061

https://github.com/paritytech/polkadot-sdk/blob/e9393a9afc3b33cc2d01b7820a8f186434196758/substrate/frame/nis/src/lib.rs#L1-L75",Performance Issue,Performance Issue
"Is the check of strict convergence in KMeans too expensive for the benefits ? ### Describe the bug

In `KMeans` scikit-learn defines [`strict_convergence`](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/cluster/_kmeans.py#L701) as the event of producing the same label assignments at two successive iterations.

When this happens, it means convergence for both labels and centroids (set aside possible oscillations due to numerical instability, were the iterations to continue).

But checking for strict convergence seems to be somewhat expensive (one loop over the last two label assignments of each sample per iteration), and if the user properly set `tol` it doesn't seem necessary at all ?

Checking for strict convergence seems to really help when `tol==0`. With `tol==0` I've seen cases of endless oscillations around 0 because of numerical instability, but never reaching 0, and finally terminating at `max_iter` iterations.

For the general case, isn't it detrimental to performance though ? one can expect the performance cost to be significant for small dimensions of data, for which an additional pass on a column is marginally more expensive.

So I would maybe suggest the following improvements:

- [ ] enable automatically the strict convergence checks only if `tol==0` (or if `tol` is ""very small"")
- [ ] maybe expose to the user the choice of enabling strict convergence at each iteration ?


### Versions

```shell
1.3
```
",Performance Issue,Performance Issue
"Visualisation of Tests breaks # Behaviour

Whenever some of `pytest` parametrised test fail, they are not cleaned up before showing updated run and new text is printed on top of old

![Image](https://github.com/user-attachments/assets/5312ab4d-d5c0-49be-8524-25a26bd57d57)


## Steps to reproduce:

1. Setup paremetrised pytest
2. Run tests with failure
3. Fix failure
4. Run test again



# Diagnostic data

Output for Python Test Log

<details>

<summary>Output for <code>Python</code> in the <code>Output</code> panel (<code>View</code>?<code>Output</code>, change the drop-down the upper-right of the <code>Output</code> panel to <code>Python</code>)
</summary>

<p>

```
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]
backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]
backend/tests/test_pipeline.py::test_pipeline[inexpected]

8 tests collected in 0.39s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.Running pytest with args: ['-p', 'vscode_pytest', '--quiet', '--tb=short', '--rootdir=/Users/ilyakochik/Developer/refining-company/sql-refining', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[inexpected]']
.....F..                                                                 [100%]
=================================== FAILURES ===================================
___________________ test_pipeline[src.code.ingest_content.1] ___________________
backend/tests/test_pipeline.py:194: in test_pipeline
    assert correct[name] == captured[name], f""Snapshots '{name}' are different""
E   AssertionError: Snapshots 'src.code.ingest_content.1' are different
E   assert Strings are not equal:
E     --- 
E     +++ 
E     @@ -6,394 +5,0 @@
E     -    { ""Query(0-accounts.sql:26:1)"": {
E     -        ""expressions"": [
E     -          { ""Expression(0-accounts.sql:28:5) = Expression(Binary_expression(Column(?.?.revenue), Column(?.?.contract_duration_days)))"": {
E     -              ""columns"": [...
E     
E     ...Full output truncated (390 lines hidden), use '-vv' to show
=========================== short test summary info ============================
FAILED backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]
1 failed, 7 passed in 0.35s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.
==================================== ERRORS ====================================
_______________ ERROR collecting backend/tests/test_pipeline.py ________________
backend/tests/test_pipeline.py:190: in <module>
    @pytest.mark.parametrize(""name,captured,correct"", get_test_params())
backend/tests/test_pipeline.py:181: in get_test_params
    captured = capture_snapshots(config)
backend/tests/test_pipeline.py:150: in capture_snapshots
    server.main(**init[""server:main""])
backend/src/server.py:99: in main
    session.load_codebase(codebase_path)
backend/src/workspace.py:22: in load_codebase
    self.queries_codebase = code.from_dir(self.path_codebase)
backend/src/code.py:111: in from_dir
    tree = ingest_content(tree, str(file.relative_to(dir)), file.read_text())
backend/tests/test_pipeline.py:124: in wrapper
    result = target(*args, **kwargs)
backend/src/code.py:127: in ingest_content
    queries=tree.queries() + updated_queries,
E   TypeError: 'list' object is not callable
------------------------------- Captured stderr --------------------------------
Loading codebase from /Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/inputs/codebase
=========================== short test summary info ============================
ERROR backend/tests/test_pipeline.py - TypeError: 'list' object is not callable
!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!
no tests collected, 1 error in 0.43s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]
backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]
backend/tests/test_pipeline.py::test_pipeline[inexpected]

8 tests collected in 0.32s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.Running pytest with args: ['-p', 'vscode_pytest', '--quiet', '--tb=short', '--rootdir=/Users/ilyakochik/Developer/refining-company/sql-refining', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]']
.                                                                        [100%]
1 passed in 0.31s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.Running pytest with args: ['-p', 'vscode_pytest', '--quiet', '--tb=short', '--rootdir=/Users/ilyakochik/Developer/refining-company/sql-refining', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[inexpected]']
........                                                                 [100%]
8 passed in 0.31s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]
backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]
backend/tests/test_pipeline.py::test_pipeline[inexpected]

8 tests collected in 0.33s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.Running pytest with args: ['-p', 'vscode_pytest', '--quiet', '--tb=short', '--rootdir=/Users/ilyakochik/Developer/refining-company/sql-refining', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[inexpected]']
........                                                                 [100%]
8 passed in 0.34s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]
backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]
backend/tests/test_pipeline.py::test_pipeline[inexpected]

8 tests collected in 0.37s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.Running pytest with args: ['-p', 'vscode_pytest', '--quiet', '--tb=short', '--rootdir=/Users/ilyakochik/Developer/refining-company/sql-refining', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[inexpected]']
F...FF.F                                                                 [100%]
=================================== FAILURES ===================================
___________________ test_pipeline[src.code.ingest_content.2] ___________________
backend/tests/test_pipeline.py:193: in test_pipeline
    assert name in captured, f""Snapshot '{name}' was not captured""
E   AssertionError: Snapshot 'src.code.ingest_content.2' was not captured
E   assert 'src.code.ingest_content.2' in {'src.code.ingest.0': '{\n  ""files"": { ""0-accounts.sql"": ""<Tree>"" },\n  ""queries"": [\n    { ""Query(0-accounts.sql:26:1...s.industry"" ] } ],\n          ""alias"": ""industry_tech"" } } ],\n    ""reliability"": 1,\n    ""similarity"": 0.95 } ]', ...}
___________________ test_pipeline[src.code.ingest_content.0] ___________________
backend/tests/test_pipeline.py:193: in test_pipeline
    assert name in captured, f""Snapshot '{name}' was not captured""
E   AssertionError: Snapshot 'src.code.ingest_content.0' was not captured
E   assert 'src.code.ingest_content.0' in {'src.code.ingest.0': '{\n  ""files"": { ""0-accounts.sql"": ""<Tree>"" },\n  ""queries"": [\n    { ""Query(0-accounts.sql:26:1...s.industry"" ] } ],\n          ""alias"": ""industry_tech"" } } ],\n    ""reliability"": 1,\n    ""similarity"": 0.95 } ]', ...}
___________________ test_pipeline[src.code.ingest_content.1] ___________________
backend/tests/test_pipeline.py:193: in test_pipeline
    assert name in captured, f""Snapshot '{name}' was not captured""
E   AssertionError: Snapshot 'src.code.ingest_content.1' was not captured
E   assert 'src.code.ingest_content.1' in {'src.code.ingest.0': '{\n  ""files"": { ""0-accounts.sql"": ""<Tree>"" },\n  ""queries"": [\n    { ""Query(0-accounts.sql:26:1...s.industry"" ] } ],\n          ""alias"": ""industry_tech"" } } ],\n    ""reliability"": 1,\n    ""similarity"": 0.95 } ]', ...}
__________________________ test_pipeline[inexpected] ___________________________
backend/tests/test_pipeline.py:197: in test_pipeline
    assert not extra_snapshots, f""Unexpected snapshots captured: {extra_snapshots}""
E   AssertionError: Unexpected snapshots captured: {'src.code.ingest.1', 'src.code.ingest.0', 'src.code.ingest.2'}
E   assert not {'src.code.ingest.0', 'src.code.ingest.1', 'src.code.ingest.2'}
=========================== short test summary info ============================
FAILED backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]
FAILED backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]
FAILED backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]
FAILED backend/tests/test_pipeline.py::test_pipeline[inexpected] - AssertionE...
4 failed, 4 passed in 0.34s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]
backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]
backend/tests/test_pipeline.py::test_pipeline[inexpected]

8 tests collected in 0.31s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.Running pytest with args: ['-p', 'vscode_pytest', '--quiet', '--tb=short', '--rootdir=/Users/ilyakochik/Developer/refining-company/sql-refining', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[inexpected]']

no tests ran in 0.31s
ERROR: not found: /Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.2]
(no match in any of [<Module test_pipeline.py>])

ERROR: not found: /Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.0]
(no match in any of [<Module test_pipeline.py>])

ERROR: not found: /Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest_content.1]
(no match in any of [<Module test_pipeline.py>])

Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.0]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.1]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]
backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.2]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]
backend/tests/test_pipeline.py::test_pipeline[inexpected]

8 tests collected in 0.31s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.Running pytest with args: ['-p', 'vscode_pytest', '--quiet', '--tb=short', '--rootdir=/Users/ilyakochik/Developer/refining-company/sql-refining', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[inexpected]']
........                                                                 [100%]
8 passed in 0.31s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.0]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.1]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]
backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.2]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]
backend/tests/test_pipeline.py::test_pipeline[inexpected]

8 tests collected in 0.32s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.0]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.1]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]
backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]
backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.2]
backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]
backend/tests/test_pipeline.py::test_pipeline[inexpected]

8 tests collected in 0.31s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.Running pytest with args: ['-p', 'vscode_pytest', '--quiet', '--tb=short', '--rootdir=/Users/ilyakochik/Developer/refining-company/sql-refining', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.logic.compare.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.1]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.code.ingest.2]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[src.sql.parse.0]', '/Users/ilyakochik/Developer/refining-company/sql-refining/backend/tests/test_pipeline.py::test_pipeline[inexpected]']
........                                                                 [100%]
8 passed in 0.32s
Starting now, all test run output will be sent to the Test Result panel, while test discovery output will be sent to the ""Python"" output channel instead of the ""Python Test Log"" channel. The ""Python Test Log"" channel will be deprecated within the next month. See https://github.com/microsoft/vscode-python/wiki/New-Method-for-Output-Handling-in-Python-Testing for details.
```

</p>
</details>
",UI/UX Bug,UI/UX Bug
"Packing of terminal-suggest fails on 'fs/promises' while building from source <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

No, this is a build issue.

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: main branch
- OS Version: Windows and Ubunuto
- Node Version: 22

Steps to Reproduce:

1. clone repository
2. run `npm run watch-web` or `npm run watch`

```
 npm run watch-web

> code-oss-dev@1.97.0 watch-web
> node ./node_modules/gulp/bin/gulp.js watch-web

(node:8081) [DEP0040] DeprecationWarning: The `punycode` module is deprecated. Please use a userland alternative instead.
(Use `node --trace-deprecation ...` to show where the warning was created)
[13:39:23] Using gulpfile /mnt/c/Users/xxxx/Documents/Repos/vscode/gulpfile.js
[13:39:23] Starting 'watch-web'...
[13:41:39] Finished packaging web extension configuration-editing with 0 errors.
[13:41:39] Finished packaging web extension css-language-features/client with 0 errors.
[13:41:39] Finished packaging web extension css-language-features/server with 0 errors.
[13:41:39] Finished packaging web extension emmet with 0 errors.
[13:41:39] Finished packaging web extension extension-editing with 0 errors.
[13:41:39] Finished packaging web extension git-base with 0 errors.
[13:41:39] Finished packaging web extension github-authentication with 0 errors.
[13:41:39] Finished packaging web extension html-language-features/client with 0 errors.
[13:41:39] Finished packaging web extension html-language-features/server with 0 errors.
[13:41:39] Finished packaging web extension ipynb with 0 errors.
[13:41:39] Finished packaging web extension ipynb with 0 errors.
[13:41:39] Finished packaging web extension json-language-features/client with 0 errors.
[13:41:39] Finished packaging web extension json-language-features/server with 0 errors.
[13:41:39] Finished packaging web extension markdown-language-features with 0 errors.
[13:41:39] Finished packaging web extension markdown-math with 0 errors.
[13:41:39] Finished packaging web extension media-preview with 0 errors.
[13:41:39] Finished packaging web extension merge-conflict with 0 errors.
[13:41:39] Finished packaging web extension microsoft-authentication with 0 errors.
[13:41:39] Finished packaging web extension npm with 0 errors.
[13:41:39] Finished packaging web extension references-view with 0 errors.
[13:41:39] Finished packaging web extension search-result with 0 errors.
[13:41:39] Finished packaging web extension simple-browser with 0 errors.
[13:41:39] Finished packaging web extension terminal-suggest with 1 errors.
[13:41:39] {
  moduleIdentifier: '/mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/ts-loader/index.js??ruleSet[1].rules[0].use[0]!/mnt/c/Users/xxxx/Documents/Repos/vscode/extensions/mangle-loader.js??ruleSet[1].rules[0].use[1]!/mnt/c/Users/xxxxt/Documents/Repos/vscode/extensions/terminal-suggest/src/terminalSuggestMain.ts',
  moduleName: './src/terminalSuggestMain.ts',
  loc: '50:24-46',
  message: ""Module not found: Error: Can't resolve 'fs/promises' in '/mnt/c/Users/xxxx/Documents/Repos/vscode/extensions/terminal-suggest/src'"",
  moduleId: 0,
  moduleTrace: [],
  details: ""resolve 'fs/promises' in '/mnt/c/Users/xxxx/Documents/Repos/vscode/extensions/terminal-suggest/src'\n"" +
    '  Parsed request is a module\n' +
    '  using description file: /mnt/c/Users/xxxx/Documents/Repos/vscode/extensions/terminal-suggest/package.json (relative path: ./src)\n' +
    ""    Field 'browser' doesn't contain a valid alias configuration\n"" +
    '    resolve as module\n' +
    ""      /mnt/c/Users/xxxx/Documents/Repos/vscode/extensions/terminal-suggest/src/node_modules doesn't exist or is not a directory\n"" +
    ""      /mnt/c/Users/xxxx/Documents/Repos/vscode/extensions/terminal-suggest/node_modules doesn't exist or is not a directory\n"" +
    '      looking for modules in /mnt/c/Users/xxxx/Documents/Repos/vscode/extensions/node_modules\n' +
    ""        /mnt/c/Users/xxxx/Documents/Repos/vscode/extensions/node_modules/fs doesn't exist\n"" +
    '      looking for modules in /mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules\n' +
    ""        /mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/fs doesn't exist\n"" +
    ""      /mnt/c/Users/xxxx/Documents/Repos/node_modules doesn't exist or is not a directory\n"" +
    ""      /mnt/c/Users/xxxx/Documents/node_modules doesn't exist or is not a directory\n"" +
    '      looking for modules in /mnt/c/Users/xxxx/node_modules\n' +
    ""        /mnt/c/Users/xxxx/node_modules/fs doesn't exist\n"" +
    ""      /mnt/c/Users/node_modules doesn't exist or is not a directory\n"" +
    ""      /mnt/c/node_modules doesn't exist or is not a directory\n"" +
    ""      /mnt/node_modules doesn't exist or is not a directory\n"" +
    ""      /node_modules doesn't exist or is not a directory"",
  stack: ""ModuleNotFoundError: Module not found: Error: Can't resolve 'fs/promises' in '/mnt/c/Users/xxxx/Documents/Repos/vscode/extensions/terminal-suggest/src'\n"" +
    '    at /mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/webpack/lib/Compilation.js:2109:28\n' +
    '    at /mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/webpack/lib/NormalModuleFactory.js:908:13\n' +
    '    at eval (eval at create (/mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/tapable/lib/HookCodeFactory.js:33:10), <anonymous>:10:1)\n' +
    '    at /mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/webpack/lib/NormalModuleFactory.js:333:22\n' +
    '    at eval (eval at create (/mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/tapable/lib/HookCodeFactory.js:33:10), <anonymous>:9:1)\n' +
    '    at /mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/webpack/lib/NormalModuleFactory.js:512:22\n' +
    '    at /mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/webpack/lib/NormalModuleFactory.js:150:10\n' +
    '    at /mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/webpack/lib/NormalModuleFactory.js:775:25\n' +
    '    at /mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/webpack/lib/NormalModuleFactory.js:992:8\n' +
    '    at /mnt/c/Users/xxxx/Documents/Repos/vscode/node_modules/webpack/lib/NormalModuleFactory.js:1121:5'
}
[13:41:39] Finished packaging web extension typescript-language-features with 0 errors.
[13:41:39] Finished packaging web extension typescript-language-features with 0 errors.
[13:41:39] Finished packaging web extension vscode-test-resolver with 0 errors.
```",Dependency Issue,Dependency Issue
"Terminal suggest: Widget scrolls with terminal buffer Mouse wheel scrolling in terminal:

![Image](https://github.com/user-attachments/assets/5dd5604f-9ac9-4c0a-a003-006a0cd788c3)

In editor:

![Image](https://github.com/user-attachments/assets/22856109-234c-409d-afc2-47071e6c4745)

We should do what the editor does ideally, but if that's too tough we can just hide it on scroll which would improve the experience imo.",Dependency Issue,Dependency Issue
"The average should not be computed in L2Pool2d The [ONNX L2Pool2d](https://github.com/onnx/onnx/blob/main/docs/Operators.md#lppool), [DirectML L2 Pooling Desc](https://docs.microsoft.com/en-us/windows/win32/api/directml/ns-directml-dml_lp_pooling_operator_desc) and [CoreML's l2_pool2d](https://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.iOS15.pool.l2_pool) calculate the l2 pooling by the expression `Y = (X1^2 + X2^2 + ... + Xn^2) ^ (1/2)`,  but [TFLite L2_PooL2d kernel implementation](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/internal/optimized/optimized_ops.h;l=3341?q=src%2Ftensorflow%2Flite%2Fkernels%2Finternal%2Foptimized%2Foptimized_ops.h) has the average with the count of sum elements  `Y=((X1^2 + X2^2 + ... + Xn^2)/n) ^ (1/2)`,  is it an issue of the kernel implementation?

BTW, [the kernel of l2_norm](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/internal/optimized/optimized_ops.h;l=1424?q=L2Normalization&ss=chromium%2Fchromium%2Fsrc) also has no the average.

",Logical Bug,Performance Issue
"Tensorflow distributes training throws exception on mac m2 ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

Mac OS 14.2.1

### Mobile device

_No response_

### Python version

3.11.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to run distributed training using `tf.distribute.experimental.MultiWorkerMirroredStrategy()` on two Mac M2 machines. However, training does not start on the GPU, and the code throws the attached exception.
The distributed training works fine if I use CPU only.
I have installed the latest `tensorflow-metal 1.1.0`.

Is `MultiWorkerMirroredStrategy` supported on Mac M2?

### Standalone code to reproduce the issue

```shell
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import Sequential
import tensorflow as tf
from tensorflow.keras.layers import Dense
import datetime
import os
import keras
import json
import glob


print(""Tensforflow version: "", tf.__version__)
print(""Availabe devices: "", devices)
if len(devices) == 0:
    print(""No devices for mac found"")
    exit(1)

    

directory = os.environ['TF_FOLDER']

checkpoint_dir = os.path.join(directory, ""ckpt"")
if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)

path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'
df = read_csv(path, header=None)

X, y = df.values[:, :-1], df.values[:, -1]

X = X.astype('float32')

y = LabelEncoder().fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

n_features = X_train.shape[1]


def get_compiled_model():
       model = tf.keras.Sequential([
            tf.keras.layers.Flatten(input_shape=(n_features,)),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(10)
        ])
       model.summary()
       model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
       return model

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))

with strategy.scope():
    model = get_compiled_model()

log_dir = os.path.join(directory, ""logs/fit/"") + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)


class CustomModelCheckpoint(keras.callbacks.ModelCheckpoint):
    def __init__(self, filepath, max_to_keep=100, **kwargs):
        super().__init__(filepath, **kwargs)
        self.filepath = filepath
        self.max_to_keep = max_to_keep

    def on_epoch_end(self, epoch, logs=None):
        super().on_epoch_end(epoch, logs)
        files = sorted(glob.glob(self.filepath.format(epoch='*')))
        if len(files) > self.max_to_keep:
            for f in files[:-self.max_to_keep]:
                os.remove(f)


callbacks = [
        CustomModelCheckpoint(
            filepath=checkpoint_dir + ""/ckpt-{epoch}"", save_freq=""epoch"", max_to_keep=10, save_weights_only=True
        ),
       keras.callbacks.TensorBoard('tensorboard_logs')
    ]

latest = tf.train.latest_checkpoint(checkpoint_dir)
if latest:
    print(""Loading model checkpoint {} ...\n"".format(latest))
    model.load_weights(latest)

model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1,callbacks=callbacks)

loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Accuracy: {acc:.3f}')
```


### Relevant log output

```shell
2024-01-28 14:46:40.395499: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.
Epoch 1/100
2024-01-28 14:46:40.778281: W tensorflow/core/framework/op_kernel.cc:1803] INTERNAL: Failed to build OpKernel for Add : No registered 'Add' OpKernel for 'GPU' devices compatible with node {{node Add}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_INT64, _device=""/job:worker/replica:0/task:0/device:GPU:0""
	.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_BFLOAT16]
  device='DEFAULT'; T in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_STRING]

Traceback (most recent call last):
  File ""/Users/inet11/git/tensorflow/model.py"", line 129, in <module>
    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1,callbacks=callbacks)
  File ""/Users/inet11/git/tensorflow/email/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/Users/inet11/git/tensorflow/email/tf/lib/python3.11/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node Add defined at (most recent call last):
<stack traces unavailable>
2 root error(s) found.
  (0) INTERNAL:  Failed to build OpKernel for Add : No registered 'Add' OpKernel for 'GPU' devices compatible with node {{node Add}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_INT64, _device=""/job:worker/replica:0/task:0/device:GPU:0""
	.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_BFLOAT16]
  device='DEFAULT'; T in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_STRING]

	 [[CollectiveReduceV2]]
  (1) CANCELLED:  Function was cancelled before it was started
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_1260]
```
",Dependency Issue,Dependency Issue
"deleted files don't go to trash 
Type: <b>Bug</b>

I delete files in the explorer using `del` key of keyboard, files are deleted and cannot be found in the trash bin

VS Code version: Code 1.95.2 (e8653663e8840adaf45af01eab5c627a5af81807, 2024-11-07T11:07:22.054Z)
OS version: Linux x64 6.8.0-48-generic snap
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-1365U (12 x 3700)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 1|
|Memory (System)|31.00GB (22.61GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 8447ec3e-1827-4960-bb59-12316efae9e7|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|wayland|
</details><details><summary>Extensions (14)</summary>

Extension|Author (truncated)|Version
---|---|---
gitlens|eam|15.6.3
ftp-simple|hum|0.7.6
git-graph|mhu|1.30.0
autopep8|ms-|2024.0.0
debugpy|ms-|2024.12.0
python|ms-|2024.18.1
vscode-pylance|ms-|2024.11.1
jupyter|ms-|2024.10.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.21
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.388.0
LiveServer|rit|5.7.9


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
pythongtdpath:30769146
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
bdiig495:31013172
dvdeprecation:31068756
dwnewjupyter:31046869
impr_priority:31102340
nativerepl2:31139839
refactort:31108082
pythonrstrctxt:31112756
cf971741:31144450
iacca1:31171482
notype1cf:31157160
5fd0e150:31155592
dwcopilot:31170013
j44ff735:31177056

```

</details>

<!-- generated by issue reporter -->",Security Vulnerability,Security Vulnerability
"VS Code Speech not detecting any voice <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No
YES, No advice available on the internet has resolved the issue so far.

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 
- OS Version: 
Version: 1.97.0-insider
Commit: 6a5c8cf95aa814b5cb92de617096144b1f45af2d
Date: 2024-12-12T05:04:17.220Z
Electron: 32.2.6
ElectronBuildId: 10629634
Chromium: 128.0.6613.186
Node.js: 20.18.1
V8: 12.8.374.38-electron.0
OS: Linux x64 6.8.0-50-generic
Exact Linux distro: Ubuntu 24.04.1 LTS

### Summary of Tests and Findings

#### Steps to Reproduce:

1. Verified microphone functionality:
   - Microphone works as expected in other applications.
   - Tested using `pactl` and `parecord` to confirm audio input is functioning properly.
2. Reinstalled **VS Code** and **VS Code Insiders** from scratch.
3. Installed the required VS Code extension for speech-to-text functionality.
4. When pressing **Alt+Ctrl+V**, the microphone icon appears, indicating that the session starts successfully. However, speech is not transcribed to text.
5. Examined the speech log:
   ```
   2024-12-12 11:37:33.892 [trace] [vscode-speech-0] starting speech-to-text session 
   (language: en-US, model: Microsoft Speech Recognizer en-US FP Model V9, 
   path: /home/d/.vscode-insiders/extensions/ms-vscode.vscode-speech-0.12.1-linux-x64/assets/stt)
   ```
6. Examined  renedring.log after starting vscode speech by pressing ctrl+alt+v 

```
[error] Extension host (LocalProcess pid: 32490) terminated unexpectedly. The following extensions were running: vscode.github-authentication, vscode.emmet, vscode.git-base, vscode.git, vscode.github, vscode.debug-auto-launch, vscode.merge-conflict, ms-vscode.vscode-speech
[info] Automatically restarting the extension host.

```
### Related Issue

The issue appears to align with a previously **Closed** GitHub issue #207290  where users reported similar behavior:  
- The speech-to-text extension starts but does not transcribe audio input despite showing no visible errors or crashes.  

---",Runtime Error,Runtime Error
"KMeans(init='k-means++') performance issue with OpenBLAS I open this issue to investigate a performance problem that might be related to #17230.

I adapted the reproducer of #17230 to display more info and make it work on a medium-size ranom dataset.

```python
from sklearn import cluster
from time import time
from pprint import pprint
from threadpoolctl import threadpool_info
import numpy as np


pprint(threadpool_info())
rng = np.random.RandomState(0)
data = rng.randn(5000, 50)
t0_global = time()
for k in range(1, 15):
    t0 = time()
    # print(f""Running k-means with k={k}: "", end="""", flush=True)
    cluster.KMeans(
        n_clusters=k,
        random_state=42,
        n_init=10,
        max_iter=2000,
        algorithm='full',
        init='k-means++').fit(data)
    # print(f""{time() - t0:.3f} s"")

print(f""Total duration: {time() - t0_global:.3f} s"")
```

I tried to run this on Linux with scikit-learn master (therefore including the #16499 fix)  with 2 different builds of scipy (with openblas from pypi and MKL from anaconda) and various values for `OMP_NUM_THREADS` (unset, `OMP_NUM_THREADS=1`, `OMP_NUM_THREADS=2`, `OMP_NUM_THREADS=4`) on a laptop with 2 physical cpu cores (4 logical cpus).

In both cases, I use the same scikit-learn binaries (built with GCC in editable mode). I just change the env.

The summary is:

- with MKL there is not problem: large or unset values of `OMP_NUM_THREADS` are faster than `OMP_NUM_THREADS=1`;
- with OpenBLAS without explicit setting of `OMP_NUM_THREADS` or setting a large value for it is significanlty slower forced sequential run with `OMP_NUM_THREADS=1`.

I will include my runs in the first comment. 

/cc @jeremiedbb ",Performance Issue,Performance Issue
"Jit-compiling `tf.while_loop` inside `tf.vectorized_map` raises `InvalidArgumentError` ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

Yes

### OS platform and distribution

Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

MRE
-------
The following mock-up of `cumsum` attempts to JIT compile a `tf.vectorized_map`ped function containing a `tf.scan`:
```python
import tensorflow as tf

def cumsum(xs):
    return tf.scan(
        lambda a, x: a + x, elems=xs
    )

@tf.function(jit_compile=True)
def vec_cumsum(xs):
    return tf.vectorized_map(cumsum, elems=xs)

xs_batched = tf.reshape(tf.range(30), (3, 10))
vec_cumsum(xs_batched)
```

__Expected behaviour__: `vec_cumsum(xs_batched)` returns a batch of cumulative sums.

__Actual behaviour__: Even though all data structures are known statically at JIT compile time, an InvalidArgumentError is raised with ""No registered 'TensorListReserve'"".  The fault is clearly related to `tf.scan`'s use of `tf.while_loop`, as a (longer) example using naked `tf.while_loop(..., max_iterations=n)` will confirm.

In JAX, it is possible to jit-compile a `vmap`ped function containing a `lax.while_loop` indicating that this is possible in HLO.  It seems the `tf.function(jit_compile=True)` machinery may be mis-transpiling to HLO somehow.

May be related to #73367 also involving `tf.vectorized_map` and `tf.while_loop` (albeit with reversed scope)?

### Standalone code to reproduce the issue

```shell
Colab MRE: https://colab.research.google.com/drive/1bmq1t3PdtebCSlNd0t-iEFrXX7Q0qqZp?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-26ea491cd046> in <cell line: 13>()
     11 
     12 # Fails with ""No registered 'TensorListReserve'""""
---> 13 vec_cumsum(xs_batched)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_vec_cumsum_462[_XlaMustCompile=true,config_proto=13561319589895757934,executor_type=11160318154034397263] on XLA_CPU_JIT: TensorListReserve (No registered 'TensorListReserve' OpKernel for XLA_CPU_JIT devices compatible with node {{function_node __inference_while_fn_428}}{{node while_init/TensorArrayV2_4}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: element_dtype=DT_VARIANT, shape_type=DT_INT32){{function_node __inference_while_fn_428}}{{node while_init/TensorArrayV2_4}}
```
",Logical Bug,Logical Bug
"Windows free-threaded CPython 3.13 ValueError: concurrent send_bytes() calls are not supported Noticed in [build log](https://github.com/scikit-learn/scikit-learn/actions/runs/13233133978/job/36933421850#step:5:2813). An automated issue was opened in https://github.com/scikit-learn/scikit-learn/issues/30801 and closed the next day.

This needs some investigation to figure out whether this can be reproduced locally and whether this is actually Windows-specific.

This may be a joblib issue as well.

```
================================== FAILURES ===================================
  _____________________________ test_absolute_error _____________________________
  
      def test_absolute_error():
          # For coverage only.
          X, y = make_regression(n_samples=500, random_state=0)
          gbdt = HistGradientBoostingRegressor(loss=""absolute_error"", random_state=0)
  >       gbdt.fit(X, y)
  
  ..\venv-test\Lib\site-packages\sklearn\ensemble\_hist_gradient_boosting\tests\test_gradient_boosting.py:225: 
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  ..\venv-test\Lib\site-packages\sklearn\base.py:1389: in wrapper
      return fit_method(estimator, *args, **kwargs)
  ..\venv-test\Lib\site-packages\sklearn\ensemble\_hist_gradient_boosting\gradient_boosting.py:663: in fit
      X_binned_train = self._bin_data(X_train, is_training_data=True)
  ..\venv-test\Lib\site-packages\sklearn\ensemble\_hist_gradient_boosting\gradient_boosting.py:1178: in _bin_data
      X_binned = self._bin_mapper.fit_transform(X)  # F-aligned array
  ..\venv-test\Lib\site-packages\sklearn\utils\_set_output.py:319: in wrapped
      data_to_wrap = f(self, X, *args, **kwargs)
  ..\venv-test\Lib\site-packages\sklearn\base.py:918: in fit_transform
      return self.fit(X, **fit_params).transform(X)
  ..\venv-test\Lib\site-packages\sklearn\ensemble\_hist_gradient_boosting\binning.py:234: in fit
      non_cat_thresholds = Parallel(n_jobs=self.n_threads, backend=""threading"")(
  ..\venv-test\Lib\site-packages\sklearn\utils\parallel.py:82: in __call__
      return super().__call__(iterable_with_config_and_warning_filters)
  ..\venv-test\Lib\site-packages\joblib\parallel.py:2007: in __call__
      return output if self.return_generator else list(output)
  ..\venv-test\Lib\site-packages\joblib\parallel.py:1711: in _get_outputs
      self._terminate_and_reset()
  ..\venv-test\Lib\site-packages\joblib\parallel.py:1386: in _terminate_and_reset
      self._backend.terminate()
  ..\venv-test\Lib\site-packages\joblib\_parallel_backends.py:262: in terminate
      self._pool.close()
  ..\..\..\..\pypa\cibuildwheel\Cache\nuget-cpython\python-freethreaded.3.13.0\tools\Lib\multiprocessing\pool.py:652: in close
      self._change_notifier.put(None)
  ..\..\..\..\pypa\cibuildwheel\Cache\nuget-cpython\python-freethreaded.3.13.0\tools\Lib\multiprocessing\queues.py:394: in put
      self._writer.send_bytes(obj)
  ..\..\..\..\pypa\cibuildwheel\Cache\nuget-cpython\python-freethreaded.3.13.0\tools\Lib\multiprocessing\connection.py:200: in send_bytes
      self._send_bytes(m[offset:offset + size])
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  
  self = <multiprocessing.connection.PipeConnection object at 0x00000243F43267C0>
  buf = <memory at 0x00000243F4C65AC0>
  
      def _send_bytes(self, buf):
          if self._send_ov is not None:
              # A connection should only be used by a single thread
  >           raise ValueError(""concurrent send_bytes() calls ""
                               ""are not supported"")
  E           ValueError: concurrent send_bytes() calls are not supported
  
  ..\..\..\..\pypa\cibuildwheel\Cache\nuget-cpython\python-freethreaded.3.13.0\tools\Lib\multiprocessing\connection.py:287: ValueError
```
",Syntax Error,Syntax Error
"Segmentation fault in `tf.config.experimental_connect_to_cluster` with `@tf.function` ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?

When configuring `tf.config.experimental_connect_to_cluster` and using the `@tf.function` decorator on a function, 
even though the function's content is `pass`, a `Segmentation fault (core dumped)` still occurs.

When the `@tf.function` decorator is commented out, the program executes with a return code of 0 and no errors occur.


### Standalone code to reproduce the issue

```shell
This simple code repeats the problem:

import tensorflow as tf

cluster = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                           ""worker1.example.com:2222"",
                                           ""worker2.example.com:2222""],
                                ""ps"": [""ps0.example.com:2222"",
                                       ""ps1.example.com:2222""]})

tf.config.experimental_connect_to_cluster(
    cluster,
    job_name='worker',
    task_index=0
)


@tf.function
def test():
    pass

test()
```


### Relevant log output

```shell
The error message I got:

2024-04-15 13:29:11.263846: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-15 13:29:11.264202: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-15 13:29:11.267063: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-15 13:29:11.305386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-15 13:29:11.992670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-15 13:29:12.465636: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
E0415 13:29:12.509477560 2432185 server_chttp2.cc:40]        {""created"":""@1713187752.509411414"",""description"":""No address added out of total 1 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":395,""referenced_errors"":[{""created"":""@1713187752.509406740"",""description"":""Failed to add any wildcard listeners"",""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":342,""referenced_errors"":[{""created"":""@1713187752.509393568"",""description"":""Unable to configure socket"",""fd"":15,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1713187752.509389969"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]},{""created"":""@1713187752.509406103"",""description"":""Unable to configure socket"",""fd"":15,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1713187752.509403589"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}
2024-04-15 13:29:12.509541: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server
2024-04-15 13:29:12.509869: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:450] Could not start gRPC server
Segmentation fault (core dumped)
```
```
",Runtime Error,Runtime Error
"test: waitThrottleDelayBetweenWorkUnits is flaky https://dev.azure.com/monacotools/Monaco/_build/results?buildId=296767&view=logs&j=78322c78-e078-5e20-184d-1b2bb6aed118&t=6d18ead1-26fd-5e07-64e3-cefc56c57b3e

Also see https://github.com/microsoft/vscode/pull/230335",Logical Bug,Logical Bug
"HistGradientBoostingClassifier slow in prediction mode While HistGradientBoostingClassifier is 100 faster than GradientBoostingClassifier when fitting the model, I found it to be very slow in case of predicting the class probabilities, in my case about 100 times slower :-(

For example: 
GradientBoostingClassifier: 3.2 min for training for 1 million examples. 32 ms for 1000 predictions.
HistGradientBoostingClassifier: 7s for training. 1s for 1000 predictions",Performance Issue,Performance Issue
"Perplexity not monotonically decreasing for batch Latent Dirichlet Allocation When using the batch method, the perplexity in LDA should be non-increasing in every iteration, right?
I have cases where it does increase. If this is indeed a bug, I'll investigate.
",UI/UX Bug,Logical Bug
"KernelDensity(bandwidth='silverman') doesn't throw proper error for 1d X Essentially the bandwidth estimation codepath is not covered in the common tests, but it should be :)",Syntax Error,Syntax Error
"pandas dtypes ""boolean"" not supported in classification target <!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Describe the bug
Pandas has extended NumPy's dtypes and these dtypes extensions are not all supported as targets in a sklearn classifier. In particular, if the target y is a Pandas ""boolean"" dtype, a classifier such as LogisticRegression fails whereas if the target is a numpy ""bool"" dtype, the classifier will not fail.

#### Steps/Code to Reproduce
```python
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.DataFrame({""boolean_col"": pd.Series([False, True, False, True], dtype=""boolean""),
                   ""bool_col"": pd.Series([False, True, False, True], dtype=""bool""),
                   ""num_col"": pd.Series([1, 2, 3, 4])})

clf = LogisticRegression()
```




#### Expected Results
```python
clf.fit(df[[""num_col""]], df.bool_col)
--------------------------------------------------------------------------------------------------------------
LogisticRegression()
```
#### Actual Results
```python
clf.fit(df[[""num_col""]], df.boolean_col)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-9a0c5abf260e> in <module>
----> 1 clf.fit(df[[""num_col""]], df.boolean_col)

~\AppData\Local\Continuum\anaconda3\envs\lognode\lib\site-packages\sklearn\linear_model\_logistic.py in fit(self, X, y, sample_weight)
   1343                                    order=""C"",
   1344                                    accept_large_sparse=solver != 'liblinear')
-> 1345         check_classification_targets(y)
   1346         self.classes_ = np.unique(y)
   1347 

~\AppData\Local\Continuum\anaconda3\envs\lognode\lib\site-packages\sklearn\utils\multiclass.py in check_classification_targets(y)
    170     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',
    171                       'multilabel-indicator', 'multilabel-sequences']:
--> 172         raise ValueError(""Unknown label type: %r"" % y_type)
    173 
    174 

ValueError: Unknown label type: 'unknown'
```
#### Versions
System:
    python: 3.8.2 (default, Apr 14 2020, 19:01:40) [MSC v.1916 64 bit (AMD64)]
executable: ~\AppData\Local\Continuum\anaconda3\envs\lognode\python.exe
   machine: Windows-10-10.0.18362-SP0

Python dependencies:
          pip: 20.0.2
   setuptools: 46.1.3.post20200330
      sklearn: 0.23.1
        numpy: 1.18.1
        scipy: 1.4.1
       Cython: None
       pandas: 1.0.3
   matplotlib: 3.2.1
       joblib: 0.14.1
threadpoolctl: 2.1.0

Built with OpenMP: True",Dependency Issue,Dependency Issue
"[DevTools Bug]: Cannot read properties of undefined (reading 'toLowerCase') ### Website or app

React app in development phase

### Repro steps

<img width=""1067"" alt=""image"" src=""https://github.com/facebook/react/assets/57346455/7b4b06c1-37f8-4edb-b67e-c816e983c441"">

Occurs when you click on a component on the Component tab.
Recently, I worked on replacing it with react-hot-loader -> react-refresh.
The above error has occurred since the react-refresh application.
No error occurs when react-refresh is removed from the webpack.
The react-refresh is set the same as the official document.


### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

4.28.5

### Error message (automated)

Cannot read properties of undefined (reading 'toLowerCase')

### Error call stack (automated)

```text
utils.js:626 Uncaught TypeError: Cannot read properties of undefined (reading 'toLowerCase')
    at formatDataForPreview (utils.js:626:1)
    at dehydrate (hydration.js:83:1)
    at cleanForBridge (utils.js:26:1)
    at Object.inspectElement (renderer.js:3356:1)
    at agent.js:175:1
    at Bridge.emit (events.js:37:1)
    at bridge.js:136:1
    at listener (backendManager.js:1:25284)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Dependency Issue,Dependency Issue
"`tf.raw_ops.ExtractImagePatches`: Assertion failure in shape inference step ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.ExtractImagePatches` can lead to assertion failure in shape inference step.
Error location is [here](https://github.com/tensorflow/tensorflow/blob/f5daeae21404e8d672c785cc0c4c469ddc1b1a8a/tensorflow/core/ops/array_ops.cc#L2686-L2687):
```C++
      TF_RETURN_IF_ERROR(c->Multiply(
          c->Dim(input_shape, 3), ksize_rows * ksize_cols, &output_depth_dim));
```
Because it does not check validity of `ksize_rows * ksize_cols`, the negative value of it is fed to [`Multiply`](https://github.com/tensorflow/tensorflow/blob/83f1804f3427ae888e62b26b5bcba8afc9e24ef7/tensorflow/core/framework/shape_inference.cc#L1096C1-L1098C56):
```C++
Status InferenceContext::Multiply(DimensionHandle first,
                                  DimensionOrConstant second,
                                  DimensionHandle* out)
```
And it ends up with assertion failure at [a constructor of `DimensionOrConstant`](https://github.com/tensorflow/tensorflow/blob/daea84aad67d1fcc8d58c648e1da58ee576445f9/tensorflow/core/framework/shape_inference.h#L891-L896).

### Standalone code to reproduce the issue

```Python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

tf.raw_ops.ExtractImagePatches(
    images=tf.random.normal([1,1,1,1]),
    ksizes=[1,-1,2,1],
    strides=[1,1,1,1],
    rates=[1,1,1,1],
    padding=""VALID"",
    name=None
)
```


### Relevant log output

Release build: Outputs nothing

Debug build:
```shell
2024-02-27 20:05:58.701202: F ./tensorflow/core/framework/shape_inference.h:891] Check failed: val >= 0 || val == InferenceContext::kUnknownDim Dimension must be non-negative or equal to InferenceContext::kUnknownDim but got -2
Aborted (core dumped)
```
",Logical Bug,Logical Bug
"Editor over cuts off markdown content in backticks when it exceeds the length of the line ![Image](https://github.com/user-attachments/assets/34f7d7ef-af2d-4ab9-9489-9a483e5eebc9)

Expanded:

![Image](https://github.com/user-attachments/assets/f1961ec9-0a7f-4ad5-ba6b-02876a1f1a18)

Can we tweak `whitespace` for this?
",UI/UX Bug,UI/UX Bug
"feature_importance causes a BSOD on Windows 10 
#### Describe the bug
Running permutation_importance on a medium-sized data set results in a BSOD on Windows 10. The dataset is 470605 x 332, code is running in a Jupyter notebook, Python version 3.7.6, scikit version 0.22.1.
The BSOD is a KERNEL_SECURITY_CHECK_FAILURE, with ERROR_CODE: `(NTSTATUS) 0xc0000409 - The system detected an overrun of a stack-based buffer in this application. This overrun could potentially allow a malicious user to gain control of this application.`
The machine has a Ryzen 5 3600 with 16GB of RAM.

#### Steps/Code to Reproduce

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
rf = RandomForestClassifier(n_estimators = 250,
                           n_jobs = -1,
                           oob_score = True,
                           bootstrap = True,
                           random_state = 42)
rf.fit(X_train, y_train)
permImp = permutation_importance(rf,
                                 X_val,
                                 y_val,
                                 scoring='f1',
                                 n_repeats=5,
                                 n_jobs=-1,
                                 random_state=42)
```

#### Expected Results
No BSOD, permutation importance computed.

#### Actual Results
BSOD after ~1-2 minutes

#### Versions
> sklearn.show_versions()

System:
    python: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\lucag\anaconda3\python.exe
   machine: Windows-10-10.0.18362-SP0

Python dependencies:
       pip: 20.0.2
setuptools: 45.2.0.post20200210
   sklearn: 0.22.1
     numpy: 1.18.1
     scipy: 1.4.1
    Cython: 0.29.15
    pandas: 1.0.1
matplotlib: 3.1.3
    joblib: 0.14.1

Built with OpenMP: True
<!-- Thanks for contributing! -->
",Dependency Issue,Dependency Issue
"Git - Built in git plugin does not work with symlink <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->

Version: 1.94.2 (user setup)
Commit: 384ff7382de624fb94dbaf6da11977bba1ecd427
Date: 2024-10-09T16:08:44.566Z
Electron: 30.5.1
ElectronBuildId: 10262041
Chromium: 124.0.6367.243
Node.js: 20.16.0
V8: 12.4.254.20-electron.0
OS: Windows_NT x64 10.0.22631

Steps to Reproduce:

1.  init a directory `A` with git 
2.  Use `ln -s` to build a soft link from `A` to `B`
3.  Use vscode to open folder `B`

If I modify something, I cannot see the built in git plugin work. For example, I cannot see the color bar.
",Dependency Issue,Dependency Issue
"inconsistent .proto file package names break gRPC message/field parsing in Wireshark <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

As noted in #12445, there is inconsistency among the package names in the TensorFlow `.proto` files. Searching for `.proto` file package declarations within the codebase reveals a wide variety of package names, including `tensorflow.dummy`.
https://github.com/search?q=repo%3Atensorflow%2Ftensorflow+%22package+tensorflow%22&type=code&p=2

This has a problematic effect when trying to parse the Protobuf fields in TensorFlow gRPC messages within the [Wireshark](https://www.wireshark.org/) network capturing tool. In Wireshark, the built-in parsing functionality requires the package/service names within the `.proto` files to match the package/service names in the captured gRPC messages, so currently, [CoordinationService](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tsl/protobuf/coordination_service.proto) (`package tensorflow`) messages parse properly, while message types and field names in [WorkerService](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker_service.proto) (`package tensorflow.grpc`) messages cannot be parsed, and appear as _unknown_.

Current workaround: using a script to replace all instances of `""tensorflow.grpc""` with `""tensorflow""` in the `.proto` files.

### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tensorflow/blob/d0863698de84277282df6f2865795aaa1e22ace5/tensorflow/tsl/protobuf/coordination_service.proto#L3

https://github.com/tensorflow/tensorflow/blob/d0863698de84277282df6f2865795aaa1e22ace5/tensorflow/core/protobuf/worker_service.proto#L18
```


### Relevant log output

_No response_</details>",Dependency Issue,Dependency Issue
"[DevTools Bug]: Excessive memory usage, even when not in use (Firefox) ### Website or app

The issue happens on the browser, not on a specific site

### Repro steps

1. Use Firefox
2. open a couple of tabs not even using react (for example for me, I have my mail client, youtube etc open).
3. check about:performance
4. confirm that the memory usage of the extension is around 200mb, which is a lot considering the extension isn't even in use.

What I would do is I would add a config option about the domains the extension is supposed to run on, and only start up everything the extension does when the user is on one of those domains, because I really don't want an extension I'm not even using at the moment to be a memory hog.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Performance Issue,Performance Issue
"Crash in T-SNE ### Describe the bug

I got a crash using the (external) [hdbscan package](https://github.com/scikit-learn-contrib/hdbscan) in some special situation. I [debugged it](https://github.com/scikit-learn-contrib/hdbscan/issues/623) and found out that it happens in the scikit-learn package, specifically its T-SNE implementation. The hdbscan maintainer ([Leland McInnes](https://github.com/lmcinnes)!) suggested to report it here.

Is this something that should be guarded for in scikit-learn?

### Steps/Code to Reproduce

The simplest way to reproduce it (using the hdbscan package) is:
```py
import numpy as np
import hdbscan
model = hdbscan.HDBSCAN(gen_min_span_tree=True)
data = np.zeros((91, 3))
clustering = model.fit(data)
clustering.minimum_spanning_tree_.plot()
```
Note that it also happens when only a relative small proportion of points are equal (but only sometimes?), this is just the easiest way to show it. By default some warnings are displayed:
> ...\sklearn\decomposition\_pca.py:685: RuntimeWarning: invalid value encountered in divide
>   self.explained_variance_ratio_ = self.explained_variance_ / total_var
> ...\sklearn\manifold\_t_sne.py:1002: RuntimeWarning: invalid value encountered in divide
>   X_embedded = X_embedded / np.std(X_embedded[:, 0]) * 1e-4 

In the end it appears to be a problem in `sklearn.manifold._t_sne._barnes_hut_tsne.gradient()`, not (always?) being able to handle `nan` values. For example, this reproduces the crash:
```py
import numpy as np
from sklearn.manifold._t_sne import _barnes_hut_tsne
neighbors = np.array([1, 2, 0, 2, 0, 1], dtype='int64')
val_P = np.full_like(neighbors, 2 / 45, dtype='float32')
pos_output = np.full((3, 2), np.nan, dtype='float32')
forces = np.zeros_like(pos_output)
indptr = np.arange(7, step=2, dtype='int64')
_barnes_hut_tsne.gradient(val_P, pos_output, neighbors, indptr, forces, 0.5, 2, 11)
```
One layer deeper, the crash occurs inside `sklearn.neighbors._quad_tree._QuadTree.build_tree()`, as follows:
```py
import numpy as np
from sklearn.neighbors._quad_tree import _QuadTree
qt = _QuadTree(2, 11)
X = np.full((3, 2), np.nan, dtype='float32')
qt.build_tree(X)
```
The output of this (due to `verbose=11`) up to the crash is:
> [QuadTree] bounding box axis 0 : [nan, nan]
> [QuadTree] bounding box axis 1 : [nan, nan]
> [QuadTree] Inserting depth 0
> [QuadTree] inserted point 0 in cell 0
> [QuadTree] Inserting depth 0
> [QuadTree] inserted point 0 in new child 1
> [QuadTree] Inserting depth 0
> [QuadTree] Inserting depth 1
> ...
> [QuadTree] Inserting depth 6271
> [QuadTree] inserted point 0 in new child 6272
> [QuadTree] Inserting depth 6271
> [QuadTree] Inserting depth 6272

I didn't dig into the QuadTree code.

### Expected Results

I would expect anything but Python crashing.

### Actual Results

Python crashed: its console window just went away silently (on Windows).

### Versions

```shell
System:
Â  Â  python: 3.12.1 (tags/v3.12.1:2305ca5, Dec Â 7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)]
executable: C:\Users\Public\Software\Python\python.exe
Â  Â machine: Windows-11-10.0.22631-SP0

Python dependencies:
Â  Â  Â  sklearn: 1.4.0
Â  Â  Â  Â  Â  pip: 23.3.2
Â  Â setuptools: 69.0.3
Â  Â  Â  Â  numpy: 1.26.3
Â  Â  Â  Â  scipy: 1.12.0
Â  Â  Â  Â Cython: 3.0.8
Â  Â  Â  Â pandas: 2.2.0
Â  Â matplotlib: 3.8.2
Â  Â  Â  Â joblib: 1.3.2
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
Â  Â  Â  Â user_api: blas
Â  Â internal_api: openblas
Â  Â  num_threads: 16
Â  Â  Â  Â  Â prefix: libopenblas
Â  Â  Â  Â filepath: C:\Users\Public\Software\Python\Lib\site-packages\numpy.libs\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll
Â  Â  Â  Â  version: 0.3.23.dev
threading_layer: pthreads
Â  Â architecture: Cooperlake

Â  Â  Â  Â user_api: openmp
Â  Â internal_api: openmp
Â  Â  num_threads: 16
Â  Â  Â  Â  Â prefix: vcomp
Â  Â  Â  Â filepath: C:\Users\Public\Software\Python\Lib\site-packages\sklearn\.libs\vcomp140.dll
Â  Â  Â  Â  version: None

Â  Â  Â  Â user_api: blas
Â  Â internal_api: openblas
Â  Â  num_threads: 16
Â  Â  Â  Â  Â prefix: libopenblas
Â  Â  Â  Â filepath: C:\Users\Public\Software\Python\Lib\site-packages\scipy.libs\libopenblas_v0.3.20-571-g3dec11c6-gcc_10_3_0-c2315440d6b6cef5037bad648efc8c59.dll
Â  Â  Â  Â  version: 0.3.21.dev
threading_layer: pthreads
Â  Â architecture: Cooperlake
```
",Syntax Error,Syntax Error
"Code error when feature name has multiple `_` ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

5.15.149-99.162.amzn2.x86_64

### Mobile device

_No response_

### Python version

Python 3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Expect there shouldn't be errors just by changing feature name. 

### Standalone code to reproduce the issue
This is a code sample that will work normally

```
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Concatenate
from tensorflow.keras import Model

import pandas as pd
import numpy as np

df = pd.DataFrame()
numeric_feature_name = 'a' * 27
categorical_feature_name = 'b' * 11
df[numeric_feature_name] = range(1000)
df[categorical_feature_name] = 'a'
df['label'] = 1

numeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')
categorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=""string"")
encoding_layer = get_category_encoding_layer(vocab=['a'])
encoded_categorical_feature = encoding_layer(categorical_feature_layer)

all_inputs = [numeric_feature_layer, categorical_feature_layer]
encoded_features = [numeric_feature_layer, encoded_categorical_feature]
concat_features = Concatenate()(encoded_features)
output = Dense(units=1, activation='sigmoid')(concat_features)
model = Model(inputs=all_inputs, outputs=output)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

dataframe_x = df[[numeric_feature_name, categorical_feature_name]]
dataframe_y = df['label']
df2 = ((dict(dataframe_x), dataframe_y))
ds = tf.data.Dataset.from_tensor_slices(df2)
ds = ds.batch(32)
ds_train = ds

model.fit(
    ds_train,
    epochs=10,
    batch_size=300,
    verbose=1
)
```

However, if I change the feature name, the same code will throw error

```
df = pd.DataFrame()
## Just change the feature name here
numeric_feature_name = 'a_b_c_d_e_f_g' 
categorical_feature_name = 'a_b_c_d_e_f'
df[numeric_feature_name] = range(1000)
df[categorical_feature_name] = 'a'
df['label'] = 1

numeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')
categorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=""string"")
encoding_layer = get_category_encoding_layer(vocab=['a'])
encoded_categorical_feature = encoding_layer(categorical_feature_layer)

all_inputs = [numeric_feature_layer, categorical_feature_layer]
encoded_features = [numeric_feature_layer, encoded_categorical_feature]
concat_features = Concatenate()(encoded_features)
output = Dense(units=1, activation='sigmoid')(concat_features)
model = Model(inputs=all_inputs, outputs=output)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

dataframe_x = df[[numeric_feature_name, categorical_feature_name]]
dataframe_y = df['label']
df2 = ((dict(dataframe_x), dataframe_y))
ds = tf.data.Dataset.from_tensor_slices(df2)
ds = ds.batch(32)
ds_train = ds

model.fit(
    ds_train,
    epochs=10,
    batch_size=300,
    verbose=1
)
```

We have tested that this error is on 2.17.0 and if we are using 2.15 tensorflow, both codes will run smoothly.
```


### Relevant log output

```shell
Epoch 1/10
2024-09-18 05:28:05.240962: W tensorflow/core/framework/op_kernel.cc:1817] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported
---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
Cell In[14], line 8
      5 ds = ds.batch(32)
      6 ds_train = ds
----> 8 model.fit(
      9     ds_train,
     10     epochs=10,
     11     batch_size=300,
     12     verbose=1
     13 )

File ~/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51 try:
     52   ctx.ensure_initialized()
---> 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                       inputs, attrs, num_outputs)
     55 except core._NotOkStatusException as e:
     56   if name is not None:

UnimplementedError: Graph execution error:

Detected at node functional_5_1/Cast defined at (most recent call last):
  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel_launcher.py"", line 18, in <module>

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/traitlets/config/application.py"", line 1075, in launch_instance

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 739, in start

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 205, in start

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 534, in process_one

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 362, in execute_request

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 778, in execute_request

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 449, in do_execute

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 549, in run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3075, in run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3130, in _run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 128, in _pseudo_sync_runner

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3334, in run_cell_async

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3517, in run_ast_nodes

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3577, in run_code

  File ""/tmp/ipykernel_37779/4021243845.py"", line 8, in <module>

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 320, in fit

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 121, in one_step_on_iterator

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 108, in one_step_on_data

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 51, in train_step

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/layers/layer.py"", line 901, in __call__

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/ops/operation.py"", line 46, in __call__

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 167, in call

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 258, in _standardize_inputs

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 218, in _convert_inputs_to_tensors

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/ops/core.py"", line 822, in convert_to_tensor

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py"", line 132, in convert_to_tensor

Cast string to float is not supported
	 [[{{node functional_5_1/Cast}}]] [Op:__inference_one_step_on_iterator_6125]
```
",Syntax Error,Syntax Error
"The test case label_image .py of tensorflow2.4.1 source code fails to be execued. ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.4.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.12

### Bazel version

3.7

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The test case label_image.py fails to be executed,and the message ""module 'tensorfle' has no attribute 'GrapDef'"" is displayed.
![image](https://github.com/user-attachments/assets/e4b7b56d-2589-41fe-8395-1743c941dd49)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
graph_def = tf.GraphDef()
```


### Relevant log output

_No response_",Logical Bug,Runtime Error
"data leak in GBDT due to warm start (This is about the non-histogram-based version of GBDTs)

X is split into train and validation data with `train_test_split(random_state=self.random_state)`.

As @johannfaouzi noted, in a warm starting context, this will produce a leak if If `self.random_state` is a `RandomState` instance: some samples that were used for training in a previous `fit` might be used for validation now.

~~I think the right fix would be to raise a `ValueError` if the provided random state isn't a number and early-stopping is activated~~",Security Vulnerability,Security Vulnerability
"""inline suggestions"" spinner on the language server status item I've seen this every day in the vscode repo for the past few days at least

<img width=""305"" alt=""Image"" src=""https://github.com/user-attachments/assets/fcfb8b95-9dd3-4228-a4ae-36867fa2cfd8"" />",Syntax Error,Syntax Error
"Bug: React test triggers mouseout event for disabled button <!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

React version: 16.14.0

## Steps To Reproduce

1. Create a test file in a react project, and paste the following code.
2. Run the test
3. The test for onMouseLeave event fails.

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

```javascript
import { act } from ""react-dom/test-utils"";
import React, { useState } from ""react"";
import ReactDOM from ""react-dom"";

const Counter = () => {
  const increaseCount = () => {
    setCount((prev) => prev + 1);
  };
  const [count, setCount] = useState(0);
  return (
    <div>
      <p>You clicked {count} times</p>
      <button
        disabled
        onClick={increaseCount}
        onMouseEnter={increaseCount}
        onMouseLeave={increaseCount}
      >
        Click me
      </button>
    </div>
  );
};

describe(""button with react test-utils"", () => {
  let container;

  beforeEach(() => {
    container = document.createElement(""div"");
    document.body.appendChild(container);
  });

  afterEach(() => {
    document.body.removeChild(container);
    container = null;
  });

  it(""should not trigger onClick when button is disabled"", () => {
    act(() => {
      ReactDOM.render(<Counter />, container);
    });
    const button = container.querySelector(""button"");
    const label = container.querySelector(""p"");

    act(() => {
      button.dispatchEvent(new MouseEvent(""click"", { bubbles: true }));
    });

    expect(label.textContent).toBe(""You clicked 0 times"");
  });
  it(""should not trigger onMouseEnter when button is disabled"", () => {
    act(() => {
      ReactDOM.render(<Counter />, container);
    });
    const button = container.querySelector(""button"");
    const label = container.querySelector(""p"");

    act(() => {
      button.dispatchEvent(new MouseEvent(""mouseover"", { bubbles: true }));
    });

    expect(label.textContent).toBe(""You clicked 0 times"");
  });
  it(""should not trigger onMouseLeave when button is disabled"", () => {
    act(() => {
      ReactDOM.render(<Counter />, container);
    });
    const button = container.querySelector(""button"");
    const label = container.querySelector(""p"");

    act(() => {
      button.dispatchEvent(new MouseEvent(""mouseout"", { bubbles: true }));
    });

    expect(label.textContent).toBe(""You clicked 0 times"");
  });
});

```

<!--
  Please provide a CodeSandbox (https://codesandbox.io/s/new), a link to a
  repository on GitHub, or provide a minimal code example that reproduces the
  problem. You may provide a screenshot of the application if you think it is
  relevant to your bug report. Here are some tips for providing a minimal
  example: https://stackoverflow.com/help/mcve.
-->


## The current behavior
<img width=""664"" alt=""Screenshot 2021-03-09 at 21 45 13"" src=""https://user-images.githubusercontent.com/2574511/110506309-c9d44380-8120-11eb-9164-ce2ae7316889.png"">


## The expected behavior
All tests should pass",Logical Bug,Logical Bug
"tf.concat (and tf.transpose) inside a for loop with tf.range in the context of a GradientTape while using XLA dosn't work ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.15.0.post1, 2.15.0

### Custom code

Yes

### OS platform and distribution

WSL Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

RTX 3080 Ti

### Current behavior?

### Current behavior
Even tho the shapes of the elements which are concatenated are well defined to compile time, when running the code provided, the execution fails with the error log provided. This is also the case, if you enforce the shapes with using tf.reshape(). 
If you remove either 
- the GradientTape context
- or the for loop with tf.range
- or the XLA compilation 

the code will work as expected.

Unrolling the for loop with range() is not desired since the number of iterations will be >50000 in the final project. Also converting the for loop in a tf.while loop with maximum_iterations specified will result in the same error. Specifying the batchsize to a constant value (also in the input_signature) won't resolve the issue either. Also changing the tf.device between GPU / CPU won't resolve the issue.
The same error arises if you try to use tf.transpose()


### Expected behavior
The arrays with well defined shapes at compile time will be concatenated / transposed when using a for loop with tf.range in the context of a GradientTape while using XLA (jit_compile=True)

### Standalone code to reproduce the issue

```shell
class Model():
    def __init__(self, batchsize):
        self.batchsize = batchsize
        self.g = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=""g"")
        self.m = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=""m"")
        self.d = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=""d"")
        self.k = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=""k"")
        self.y0 = tf.ones([self.batchsize, 2, 1])


    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 2, 1), dtype=tf.float32)],
                 jit_compile=False)
    def compute(self, yi):
        A1 = tf.concat([tf.zeros_like(self.m), tf.ones_like(self.m)], 2)
        A2 = tf.concat([-self.k/self.m, -self.d/self.m], 2)
        A = tf.concat([A1, A2], 1)
        #A = tf.transpose(A, perm=[0, 2, 1])
        B = tf.concat([tf.zeros_like(self.g), self.g], 1)

        dy = tf.linalg.matmul(A, yi) + B
        return dy
    

class Estimator():
    def __init__(self, model):
        self.model = model
        self.dt = tf.constant(0.001)

    @tf.function(jit_compile=True, reduce_retracing=False)
    def estimate(self):
        with tf.GradientTape() as tape:
            yi = self.model.y0
            for i in tf.range(10):
                dyi = self.model.compute(yi)
                yi = yi + dyi*self.dt
        grads = tape.gradient(yi, [self.model.g, self.model.m, self.model.d, self.model.k])
        return grads



device = ""CPU:0""
#device = ""GPU:0""

with tf.device(device):    
    batchsize = 5
    model = Model(batchsize)
    estimator = Estimator(model)
    grads = estimator.estimate()
```


### Relevant log output

```shell
OP_REQUIRES failed at concat_op.cc:168 : INVALID_ARGUMENT: Input 0 to node `gradient_tape/while/gradients/while/StatefulPartitionedCall_grad/PartitionedCall/gradients/concat_3_grad/ConcatOffset` with op ConcatOffset must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.
```
",Dependency Issue,Dependency Issue
"Is the check of strict convergence in KMeans too expensive for the benefits ? ### Describe the bug

In `KMeans` scikit-learn defines [`strict_convergence`](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/cluster/_kmeans.py#L701) as the event of producing the same label assignments at two successive iterations.

When this happens, it means convergence for both labels and centroids (set aside possible oscillations due to numerical instability, were the iterations to continue).

But checking for strict convergence seems to be somewhat expensive (one loop over the last two label assignments of each sample per iteration), and if the user properly set `tol` it doesn't seem necessary at all ?

Checking for strict convergence seems to really help when `tol==0`. With `tol==0` I've seen cases of endless oscillations around 0 because of numerical instability, but never reaching 0, and finally terminating at `max_iter` iterations.

For the general case, isn't it detrimental to performance though ? one can expect the performance cost to be significant for small dimensions of data, for which an additional pass on a column is marginally more expensive.

So I would maybe suggest the following improvements:

- [ ] enable automatically the strict convergence checks only if `tol==0` (or if `tol` is ""very small"")
- [ ] maybe expose to the user the choice of enabling strict convergence at each iteration ?


### Versions

```shell
1.3
```
",Performance Issue,Performance Issue
"`distance_threshold` not respected in `AgglomerativeClustering` with sparse `connectivity` ### Describe the bug

When passing a sparse `connectivity` to `AgglomerativeClustering` constrained by a given `distance_threshold`, the current implementation may return a clustering not respecting this constraint.

This is at least true for the 'complete' and 'average' linkage criteria. I found the origin of the problem in [`linkage_tree`](https://github.com/scikit-learn/scikit-learn/blob/80598905e517759b4696c74ecc35c6e2eb508cff/sklearn/cluster/_agglomerative.py#L384), because I saw the only moment the inter-point distances were calculated was here:

https://github.com/scikit-learn/scikit-learn/blob/80598905e517759b4696c74ecc35c6e2eb508cff/sklearn/cluster/_agglomerative.py#L565-L568

So the only distance info we have is for connected points. Then, for `linkage='complete'`, in the situation where you have clustered together two neighbours A and B, and in the next step you consider aggregating C, which is B's neighbour but not A's, if `d(A, C) > distance_threshold` but at the same time `d(B, C)` is the minimum remaining distance (at the top of the heap), then C will be aggregated with these two.

Now I'm not sure at all on how to fix this properly. One option is to compute all pairwise distances in the connection matrix `A` when `A` and the heap `inertia` are initialised:

 https://github.com/scikit-learn/scikit-learn/blob/80598905e517759b4696c74ecc35c6e2eb508cff/sklearn/cluster/_agglomerative.py#L596-L604

or to somehow compute the ones needed on the fly when two nodes with different neighbourhoods are merged. Both solutions seem quite expensive though.

To give you some context, I encountered this while trying to do a spatial aggregation of administrative boundaries (England's LSOAs), in such a way that an indicator (let's say average income) does not vary too much within the aggregated regions. In this kind of geographic context, this is a rather common task, the [Ward variant](https://pysal.org/spopt/generated/spopt.region.WardSpatial.html#spopt.region.WardSpatial) is even implemented in the `pysal` package.

### Steps/Code to Reproduce

```python
import numpy as np
import pandas as pd
from sklearn.feature_extraction.image import grid_to_graph
from sklearn.cluster import AgglomerativeClustering

rng = np.random.RandomState(0)
n_samples = 100
conn = np.ones([n_samples, n_samples], dtype=bool)
# Add some random sparsity, 70% at most
masked = (rng.random((n_samples, 70)) * n_samples).astype(int)
for i, nbh in enumerate(masked):
    conn[i, nbh] = False
    conn[nbh, i] = False
X = rng.randn(n_samples, 1)
clustering = AgglomerativeClustering(
    n_clusters=None,
    distance_threshold=1,
    connectivity=conn,
    linkage='complete',
    affinity='l1',
)
clustering.fit(X)
df = pd.DataFrame({'labels': clustering.labels_, 'x': X.flatten()})
clustered_df = df.groupby(""labels"")[""x""].agg([""min"", ""max""])

# This should be < distance_threshold
print((clustered_df['max'] - clustered_df['min']).max())
# 1.20248995732655
```

### Expected Results

A float lower than 1, meaning the linkage criterion was respected.

### Actual Results

1.20248995732655

### Versions

```shell
System:
    python: 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21)  [GCC 10.3.0]
executable: /home/thomaslouf/.conda/envs/ses-ling/bin/python
   machine: Linux-4.15.0-144-generic-x86_64-with-glibc2.27

Python dependencies:
      sklearn: 1.1.1
          pip: 22.1.2
   setuptools: 62.3.2
        numpy: 1.22.4
        scipy: 1.8.1
       Cython: None
       pandas: 1.4.2
   matplotlib: 3.5.2
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/thomaslouf/.conda/envs/ses-ling/lib/libopenblasp-r0.3.20.so
        version: 0.3.20
threading_layer: pthreads
   architecture: Bulldozer
    num_threads: 32

       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/thomaslouf/.conda/envs/ses-ling/lib/libgomp.so.1.0.0
        version: None
    num_threads: 1
```
",Logical Bug,Logical Bug
"tensorflow/c/eager/c_api_test fails to find GPU implementation of MatMulOp **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.7.1
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1/7.6.4.38
- GPU model and memory: GTX1080TI

**Describe the current behavior**

The 4 tests `CAPI.TensorHandleSilentCopy*` from `tensorflow/c/eager/c_api_test` fail each with 

```
tensorflow/c/eager/c_api_test.cc:442: Failure
Expected equality of these values:
  TF_GetCode(status.get())
    Which is: 3
  TF_OK
    Which is: 0
Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:GPU:
0, /job:localhost/replica:0/task:0/device:CPU:0].
```

Reason seems to be that `MatMulOp` is not found for GPU which is odd as seemingly everything else works.

**Standalone code to reproduce the issue**

`/tmp/ebbuild/TensorFlow/2.4.0/fosscuda-2019b-Python-3.7.4/tmp004t3B-bazel-tf/7277201245461b79db55d0e3e6d95f77/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/c_api_test_gpu --gtest_filter=*TensorHandleSilentCopy*`

**Other info / logs** 

For some reason this test works on our POWER9 nodes which have otherwise the exact same environment (same software versions etc), but use V100s",Logical Bug,Runtime Error
"`tf.data.Dataset.from_tensor_slices` allocates GPU RAM ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Passing a numpy array to `tf.data.Dataset.from_tensor_slices()` attempts to allocate the dataset as a tensor on the GPU device, and raises an exception if there is not enough GPU RAM available. 

This only started with TF 2.17.0. In all previous TF versions, all `tf.data.Dataset` operations were always pinned to the CPU.

To reproduce, create a numpy array larger than can fit on the GPU, and attempt to create a `tf.data.Dataset` from it.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

gpu_ram_gb = 12 # adjust for size of GPU 

gb = gpu_ram_gb+1; dtype = ""float64""
size = (gb * 1024**3) // tf.dtypes.as_dtype(dtype).size

x = np.zeros((size,), dtype = dtype)

tf.data.Dataset.from_tensor_slices(x)
```


### Relevant log output

```shell
2024-07-12 08:20:42.771788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-12 08:20:42.784893: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-12 08:20:42.788889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-12 08:20:42.798174: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-12 08:20:43.492876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1720786850.494498   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.527964   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.531351   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.535502   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.538786   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.541878   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.710210   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.711607   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.712908   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-07-12 08:20:50.714170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 34 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5
2024-07-12 08:20:50.715606: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 13958643712 exceeds 10% of free system memory.
2024-07-12 08:21:05.997032: W external/local_tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 13.00GiB (rounded to 13958643712)requested by op _EagerConst
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2024-07-12 08:21:05.997054: I external/local_tsl/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc
2024-07-12 08:21:05.997064: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (256): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997072: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997079: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1024): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997087: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997093: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997100: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997107: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997114: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997121: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997128: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997135: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997142: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997148: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997155: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997169: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997176: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997183: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997190: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997197: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1062] Bin for 13.00GiB was 256.00MiB, Chunk State: 
2024-07-12 08:21:05.997219: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: 
2024-07-12 08:21:05.997225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 0B
2024-07-12 08:21:05.997232: I external/local_tsl/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 0 memory_limit_: 35651584 available bytes: 35651584 curr_region_allocation_bytes_: 35651584
2024-07-12 08:21:05.997241: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: 
Limit:                        35651584
InUse:                               0
MaxInUse:                            0
NumAllocs:                           0
MaxAllocSize:                        0
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2024-07-12 08:21:05.997248: W external/local_tsl/tsl/framework/bfc_allocator.cc:494] <allocator contains no memory>

Traceback (most recent call last):
  File ""/home/tomasz/github/rstudio/keras3/test.py"", line 16, in <module>
    tf.data.Dataset.from_tensor_slices(x)
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 826, in from_tensor_slices
    return from_tensor_slices_op._from_tensor_slices(tensors, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py"", line 25, in _from_tensor_slices
    return _TensorSliceDataset(tensors, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py"", line 33, in __init__
    element = structure.normalize_element(element)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/util/structure.py"", line 134, in normalize_element
    ops.convert_to_tensor(t, name=""component_%d"" % i, dtype=dtype))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/ops.py"", line 713, in convert_to_tensor
    return tensor_conversion_registry.convert(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 234, in convert
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_tensor_conversion.py"", line 29, in _constant_tensor_conversion_function
    return constant_op.constant(v, dtype=dtype, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper
    return op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 276, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 289, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 301, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
```
",Runtime Error,Logical Bug
"Search Only in Open Editors does not work if file name has square bracket <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.94.2
- OS Version: Windows 11

Steps to Reproduce:

1. Name a file with closed or open square brackets []
2. Open in editor and search only in open editors
![Image](https://github.com/user-attachments/assets/d5ce22f3-95c8-425f-bc51-9d0ebb89818e)
",Logical Bug,Logical Bug
"Settings Defaults gets very confused when updating extensions. 
Type: <b>Bug</b>

If the settings schema has changed for an extension during an update, it is not getting correctly detected by VS Code Settings. 

## Steps to Reproduce

1. Install the Code Spell Checker extension version `v3.0.1` (the old version).
   ![Image](https://github.com/user-attachments/assets/49753d0b-b964-4331-9099-05ca7aa94d5d)
1. Restart VS Code -- this is necessary if already had the extension installed.
2. Go to the extension's settings.
   ![Image](https://github.com/user-attachments/assets/0c8a2153-a7ec-40a5-a984-5f6a59e7189a)
1. Notice the list of settings (59) -- Keep settings editor open.
   ![Image](https://github.com/user-attachments/assets/3aba9949-7fae-4cd9-9576-162266305a55)
1. Update the extension to v4.0.21.
   ![Image](https://github.com/user-attachments/assets/d3ef4827-ad08-443d-8dd0-ad6ec9766fd7)
1. Restart extensions as instructed.
2. Notice the list of settings (59) -- which is incorrect.
   ![Image](https://github.com/user-attachments/assets/2011a3d3-6192-4c3f-9709-68f15d5211c0)
1. Closing the settings editor and reopening doesn't make a difference. It is still showing stale settings.
2. Even Restarting the Extension Host does not work.
3. The only option is to Restart VS Code.
4. Restart VS Code.
5. Go to the Code Spell Checker settings. Notice that there are now 77.
   ![Image](https://github.com/user-attachments/assets/44c8140f-a78e-4968-8da0-c86bbcc79a29)


VS Code version: Code 1.95.3 (Universal) (f1a4fb101478ce6ec82fe9627c43efbf9e98c813, 2024-11-13T14:50:04.152Z)
OS version: Darwin arm64 24.1.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|3, 4, 3|
|Memory (System)|36.00GB (0.47GB free)|
|Process Argv|--crash-reporter-id 21ae0ece-62a2-4681-a5cf-c687ea23e753|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (95)</summary>

Extension|Author (truncated)|Version
---|---|---
terraform|4op|0.2.5
ada|Ada|26.0.202411173
commit-message-editor|ada|0.25.0
tsl-problem-matcher|amo|0.6.2
alignment|ann|0.3.0
cpupro|ant|0.1.1
vscode-zipfs|arc|3.0.0
astro-vscode|ast|2.15.4
markdown-mermaid|bie|1.27.0
mermaid-markdown-syntax-highlighting|bpr|1.7.0
npm-intellisense|chr|1.4.5
codesandbox-projects|Cod|0.2.142
jison-syntax-highlight|cru|0.1.1
scala|dal|0.0.5
vscode-jq-playground|dav|4.3.5
vscode-eslint|dba|3.0.13
vscode-wasm|dts|1.4.1
gitlens|eam|16.0.3
EditorConfig|Edi|0.16.4
prettier-vscode|esb|11.0.0
linter-gfortran|for|3.2.0
copilot|Git|1.245.0
copilot-chat|Git|0.22.4
remotehub|Git|0.64.0
vscode-github-actions|git|0.27.0
vscode-pull-request-github|Git|0.100.3
gitpod-desktop|git|0.0.180
yaml-plus-json|hil|1.12.2
mediawiki|jak|2.1.0
latex-workshop|Jam|10.5.6
svg|joc|1.5.4
jq-syntax-highlighting|jq-|0.0.2
vscode-tree-sitter-query|jri|0.0.6
language-haskell|jus|3.6.0
vscode-cfml|Kam|0.5.4
bison|lun|0.1.0
Kotlin|mat|1.7.1
Lisp|mat|0.1.12
rainbow-csv|mec|3.13.0
dotenv|mik|1.0.1
vscode-apache|mrm|1.2.0
vscode-puglint|mrm|2.3.0
vscode-azureresourcegroups|ms-|0.9.9
vscode-docker|ms-|1.29.3
vscode-language-pack-de|MS-|1.95.2024103009
vscode-dotnet-runtime|ms-|2.2.3
al|ms-|14.1.1180850
playwright|ms-|1.1.12
black-formatter|ms-|2024.4.0
debugpy|ms-|2024.12.0
isort|ms-|2023.10.1
python|ms-|2024.20.0
vscode-pylance|ms-|2024.11.3
jupyter|ms-|2024.10.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.21
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.391.0
remote-ssh|ms-|0.115.1
remote-ssh-edit|ms-|0.87.0
remote-wsl|ms-|0.88.5
azure-account|ms-|0.12.0
azure-repos|ms-|0.40.0
cpptools|ms-|1.22.11
cpptools-extension-pack|ms-|1.3.0
live-server|ms-|0.4.15
powershell|ms-|2024.4.0
remote-explorer|ms-|0.4.3
remote-repositories|ms-|0.42.0
test-adapter-converter|ms-|0.2.1
vscode-js-profile-flame|ms-|1.0.9
vsliveshare|ms-|1.0.5941
vetur|oct|0.37.3
common-lisp|qin|1.2.11
vscode-yaml|red|1.15.0
rust-analyzer|rus|0.3.2188
es6-mocha-snippets|spo|0.2.2
avro|str|0.5.0
code-spell-checker|str|4.0.21
hunspell|str|1.0.4
iconfont-preview|stx|0.0.5
svelte-vscode|sve|109.3.2
even-better-toml|tam|0.19.2
msbuild-project-tools|tin|0.6.6
es6-string-html|Tob|2.16.0
vscode-mermaid-editor|tom|0.19.1
vscode-mdx|uni|1.8.11
intellicode-api-usage-examples|Vis|0.2.9
vscodeintellicode|Vis|1.3.2
vscode-gradle|vsc|3.16.4
vscode-java-debug|vsc|0.58.1
debug|web|0.27.0
php-debug|xde|1.35.0
php-intellisense|zob|1.3.3

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
vscaac:30438847
c4g48928:30535728
azure-dev_surveyone:30548225
vscrp:30673768
2i9eh265:30646982
962ge761:30959799
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
945dj816:31013170
dvdeprecation:31068756
dwnewjupyter:31046869
nativerepl1:31139838
pythonrstrctxt:31112756
nativeloc2:31185842
cf971741:31144450
iacca1:31171482
notype1:31157159
5fd0e150:31155592
dwcopilot:31170013
stablechunks:31184530

```

</details>

<!-- generated by issue reporter -->",UI/UX Bug,UI/UX Bug
"Emmet class expansions retain period in .erb files 
Type: <b>Bug</b>

If you expand an Emmet class shortcut (e.g.: `.something`) vscode will expand the entry to `.<div class=""something""></div>`. The period at the start is incorrect.

![Image](https://github.com/user-attachments/assets/219c4e75-cfbc-4457-a09c-feded592cdee)

![Image](https://github.com/user-attachments/assets/3c55263e-e024-4fc2-8565-0673246e353a)


VS Code version: Code 1.95.2 (e8653663e8840adaf45af01eab5c627a5af81807, 2024-11-07T11:07:22.054Z)
OS version: Linux x64 6.11.7-300.fc41.x86_64
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-9700T CPU @ 2.00GHz (8 x 3600)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|2, 1, 1|
|Memory (System)|31.18GB (21.71GB free)|
|Process Argv|. --crash-reporter-id 0ca08db6-6cd2-4398-b705-c59f2340e5d3|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|gnome|
|XDG_CURRENT_DESKTOP|GNOME|
|XDG_SESSION_DESKTOP|gnome|
|XDG_SESSION_TYPE|wayland|
</details><details><summary>Extensions (58)</summary>

Extension|Author (truncated)|Version
---|---|---
language-x86-64-assembly|13x|3.1.4
icons-carbon|ant|0.2.6
vscode-eslint|dba|3.0.10
EditorConfig|Edi|0.16.4
prettier-vscode|esb|11.0.0
vscode-firefox-debug|fir|2.9.11
go|gol|0.42.1
lua-plus|jep|1.1.1
vscode-rdbg|Koi|0.2.2
cortex-debug|mar|1.12.1
debug-tracker-vscode|mcu|0.0.15
memory-view|mcu|0.0.25
peripheral-viewer|mcu|1.4.6
rtos-views|mcu|0.0.7
dotenv|mik|1.0.1
vscode-docker|ms-|1.29.3
debugpy|ms-|2024.12.0
python|ms-|2024.20.0
vscode-pylance|ms-|2024.11.2
remote-containers|ms-|0.388.0
remote-ssh|ms-|0.115.1
remote-ssh-edit|ms-|0.87.0
remote-wsl|ms-|0.88.5
vscode-remote-extensionpack|ms-|0.26.0
cpptools|ms-|1.22.11
cpptools-extension-pack|ms-|1.3.0
makefile-tools|ms-|0.11.13
powershell|ms-|2024.4.0
remote-explorer|ms-|0.4.3
remote-server|ms-|1.5.2
vscode-serial-monitor|ms-|0.13.1
playdate|Ort|0.9.3
pico-w-go|pau|4.0.6
material-icon-theme|PKi|5.14.1
java|red|1.36.0
vscode-xml|red|0.27.1
rust-analyzer|rus|0.3.2180
ruby-lsp|Sho|0.8.13
ayu-green|Sir|1.0.1
lua|sum|3.13.1
svelte-vscode|sve|109.2.3
even-better-toml|tam|0.19.2
ayu|tea|1.0.5
vscode-standard-ruby|tes|0.0.16
vscode-tinygo|tin|0.5.0
cmake|twx|0.0.17
intellicode-api-usage-examples|Vis|0.2.9
vscodeintellicode|Vis|1.3.2
vscode-gradle|vsc|3.16.4
vscode-java-debug|vsc|0.58.1
vscode-java-dependency|vsc|0.24.1
vscode-java-pack|vsc|0.29.0
vscode-java-test|vsc|0.43.0
vscode-maven|vsc|0.44.0
vscode-icons|vsc|12.9.0
volar|Vue|2.1.10
material-theme|zhu|3.17.6
linkerscript|Zix|1.0.4

(5 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
vsaa593cf:30376535
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
2e7ec940:31000449
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
945dj816:31013170
dvdeprecation:31068756
dwnewjupyter:31046869
newcmakeconfigv2:31071590
impr_priority:31102340
nativerepl1:31139838
pythonrstrctxt:31112756
cf971741:31144450
iacca1:31171482
notype1cf:31157160
5fd0e150:31155592
dwcopilot:31170013
j44ff735:31181874

```

</details>

<!-- generated by issue reporter -->",Syntax Error,Syntax Error
"Cross endianness and bitness pickle issues with KNeighborsClassifier / KDTree Reported in https://github.com/scikit-learn/scikit-learn/issues/21237 (cross endianness). There is a similar issue for the cross bitness, to reproduce:

Generate a pickle on a 64bit machine:
```py
from sklearn.datasets import make_classification
X, y = make_classification(random_state=0)

from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(algorithm='kd_tree')
clf.fit(X, y)
import pickle
pickle.dump(clf, open('/tmp/kneighbors.pkl', 'wb'))
```

Open it on a 32bit machine:
```
docker run -it -v /tmp:/io lesteve/i386-scikit-learn python3 -c 'import pickle; pickle.load(open(""/io/kneighbors.pkl"", ""rb""))'
```
Output:
```
WARNING: The requested image's platform (linux/386) does not match the detected host platform (linux/amd64) and no specific platform was requested
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""sklearn/neighbors/_binary_tree.pxi"", line 1062, in sklearn.neighbors._kd_tree.BinaryTree.__setstate__
    self._update_memviews()
  File ""sklearn/neighbors/_binary_tree.pxi"", line 1004, in sklearn.neighbors._kd_tree.BinaryTree._update_memviews
    self.idx_array = self.idx_array_arr
ValueError: Buffer dtype mismatch, expected 'ITYPE_t' but got 'long long'
```

Quite likely solving the issue is rather similar to https://github.com/scikit-learn/scikit-learn/pull/21552 and https://github.com/scikit-learn/scikit-learn/pull/21539.",Dependency Issue,Dependency Issue
"feature_importance causes a BSOD on Windows 10 
#### Describe the bug
Running permutation_importance on a medium-sized data set results in a BSOD on Windows 10. The dataset is 470605 x 332, code is running in a Jupyter notebook, Python version 3.7.6, scikit version 0.22.1.
The BSOD is a KERNEL_SECURITY_CHECK_FAILURE, with ERROR_CODE: `(NTSTATUS) 0xc0000409 - The system detected an overrun of a stack-based buffer in this application. This overrun could potentially allow a malicious user to gain control of this application.`
The machine has a Ryzen 5 3600 with 16GB of RAM.

#### Steps/Code to Reproduce

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
rf = RandomForestClassifier(n_estimators = 250,
                           n_jobs = -1,
                           oob_score = True,
                           bootstrap = True,
                           random_state = 42)
rf.fit(X_train, y_train)
permImp = permutation_importance(rf,
                                 X_val,
                                 y_val,
                                 scoring='f1',
                                 n_repeats=5,
                                 n_jobs=-1,
                                 random_state=42)
```

#### Expected Results
No BSOD, permutation importance computed.

#### Actual Results
BSOD after ~1-2 minutes

#### Versions
> sklearn.show_versions()

System:
    python: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\lucag\anaconda3\python.exe
   machine: Windows-10-10.0.18362-SP0

Python dependencies:
       pip: 20.0.2
setuptools: 45.2.0.post20200210
   sklearn: 0.22.1
     numpy: 1.18.1
     scipy: 1.4.1
    Cython: 0.29.15
    pandas: 1.0.1
matplotlib: 3.1.3
    joblib: 0.14.1

Built with OpenMP: True
<!-- Thanks for contributing! -->
",Dependency Issue,Dependency Issue
"[DevTools Bug]: event.metaKey + f to focus SearchInput doesn't work on Windows ### Website or app

https://beta.reactjs.org/

### Repro steps

1. Open React Dev Tools -> Components
2. Try hitting (Windows Key) + f

On Windows 10/11, the Feedback Hub opens up. It will not focus on the search input.

I understand this probably works fine on Mac, but on Windows it'd be great to use a key that won't be intercepted by Windows. Like `Shift + f`. or `Ctrl + Alt + f`. Or, perhaps as soon as I start typing (unless I'm typing in another focused input). Or, if I press `/`. Something!

I thought at first DevTools didn't have a keyboard shortcut but then I looked at the source code and saw it uses [`metaKey`](https://developer.mozilla.org/en-US/docs/Web/API/KeyboardEvent/metaKey) which _totally_ doesn't work on Firefox in Windows.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Dependency Issue,Dependency Issue
"Single linkage option in Agglomerative causes MemoryError for very large numbers #### Describe the bug
Using the single linkage option in Agglomerative clustering results in a MemoryError when the input data contains very large values, likely due to numeric overflows. 

#### Steps/Code to Reproduce
Example: 
Running the code below with half of the sample data does not produce a MemoryError (although the same numpy RuntimeWarning is being thrown). However, if the data contains as many very large numbers as in my sample below my machine with 16Gb RAM runs out of memory and I get a MemoryError. To me this seems a bit disproportionate as the sample data is not very large.

```python
from sklearn.cluster import AgglomerativeClustering

X = [[1.30830774e+307, 6.02217328e+307],
     [1.54166067e+308, 1.75812744e+308],
     [5.57938866e+307, 4.13840113e+307],
     [1.36302835e+308, 1.07968131e+308],
     [1.58772669e+308, 1.19380571e+307],
     [2.20362426e+307, 1.58814671e+308],
     [1.06216028e+308, 1.14258583e+308],
     [7.18031911e+307, 1.69661213e+308],
     [7.91182553e+307, 5.12892426e+307],
     [5.58470885e+307, 9.13566765e+306],
     [1.22366243e+308, 8.29427922e+307],
     [4.39205961e+306, 1.26048413e+308],
     [4.61599953e+306, 7.24075646e+307],
     [1.66596896e+308, 1.65498552e+308],
     [4.73958815e+307, 7.66412710e+307],
     [1.57013390e+308, 1.03527051e+308],
     [1.28464631e+308, 1.68216358e+308],
     [1.07121506e+308, 3.11489418e+307],
     [2.97524276e+307, 1.59238260e+306],
     [1.24529964e+308, 5.18478922e+306],
     [7.55088957e+307, 7.08726240e+307],
     [8.38161061e+307, 1.50363727e+307],
     [1.11502738e+308, 3.77564517e+307],
     [1.33977566e+308, 4.21606136e+307],
     [3.18410347e+306, 1.41182675e+308],
     [1.37949806e+307, 1.33091975e+308],
     [8.84125355e+307, 8.83334687e+307],
     [4.16683700e+307, 4.38042780e+307],
     [6.33989592e+307, 1.13127438e+308],
     [3.30270525e+307, 5.82271903e+307],
     [1.73464450e+308, 1.57922601e+308],
     [8.50840567e+307, 8.34750980e+307],
     [1.06389805e+308, 7.01361194e+307],
     [1.20971776e+308, 1.42173002e+308],
     [1.91271271e+307, 7.63302091e+307],
     [1.46375293e+308, 3.27352625e+307],
     [5.64255485e+307, 1.62562194e+308],
     [5.96180877e+307, 4.31806469e+307],
     [1.18773200e+308, 4.91146568e+307],
     [1.19298612e+307, 1.54110558e+308],
     [3.51302340e+307, 5.60375139e+307],
     [5.14281492e+307, 9.76302343e+307],
     [1.77408023e+308, 1.65760854e+308],
     [1.46544741e+308, 1.47445640e+308],
     [1.76109403e+308, 1.30923042e+308],
     [1.64984146e+308, 2.58303609e+307],
     [1.61558758e+307, 1.03985868e+308],
     [1.37977676e+308, 4.90921157e+307],
     [1.01105745e+308, 1.57678709e+307],
     [1.24672794e+308, 5.96657664e+307]]

clusterer = AgglomerativeClustering(linkage='single')
clusterer.fit(X)
```

#### Expected Results
No error is thrown or ValueError that specifies the range of allowed values. GaussianMixture clustering is handling the same data like this:
```
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
```

#### Actual Results

```python-traceback
C:\Program Files\Python37\lib\site-packages\numpy\core\fromnumeric.py:90: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

Traceback (most recent call last):
  File ""C:/Users/thaar/PycharmProjects/sklearn-dev/agglomerative_test.py"", line 58, in <module>
    clusterer.fit(X)
  File ""C:\Program Files\Python37\lib\site-packages\sklearn\cluster\_agglomerative.py"", line 898, in fit
    self.n_leaves_)
  File ""C:\Program Files\Python37\lib\site-packages\sklearn\cluster\_agglomerative.py"", line 674, in _hc_cut
    label[_hierarchical._hc_get_descendent(-node, children, n_leaves)] = i
  File ""sklearn\cluster\_hierarchical_fast.pyx"", line 96, in sklearn.cluster._hierarchical_fast._hc_get_descendent
MemoryError
```

#### Versions
System:
    python: 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\thaar\PycharmProjects\sklearn-dev\venv\Scripts\python.exe
   machine: Windows-10-10.0.18362-SP0

Python dependencies:
          pip: 20.1.1
   setuptools: 49.2.0
      sklearn: 0.23.1
        numpy: 1.18.4
        scipy: 1.4.1
       Cython: None
       pandas: 1.0.3
   matplotlib: 3.2.1
       joblib: 0.14.1
threadpoolctl: 2.0.0

Built with OpenMP: True

<!-- Thanks for contributing! -->
",Performance Issue,Performance Issue
"`Error in PredictCost() for the op: op: ""CropAndResize""` when using the `tf.image.crop_and_resize` op <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

RedHat Linux Enterprise 8.4

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA: 11.6

### GPU model and memory

V100, 32GB

### Current Behaviour?


I wanted to code the equivalent of [RandomResizedCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html) from torchvision.
I used the model from [this official keras tutorial](https://keras.io/examples/vision/nnclr/#random-resized-crops) and integrated it in my data pipeline.
When using it, I got the following warning:

```
2022-07-26 14:10:05.665573: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: 40 } dim { size: 40 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 16 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 16 } dim { size: 16 } dim { size: 3 } } }
2022-07-26 14:10:05.815003: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
```

At first, I didn't care because my code was running fine, but I had a CPU memory error after about 9 epochs of ImageNet training. This suggests that there is somehow a memory leak during training.

The same behaviour (although on GPU from that I understand) was also observed in this [SO question](https://stackoverflow.com/q/72642906/4332585).



### Standalone code to reproduce the issue


Unfortunately, the warning does not appear on Colab, but here is a link with the appropriate minimal example anyway: https://colab.research.google.com/drive/1QHa4kxPLfCjkfDvmwv9wj5yq19za5SpA?usp=sharing

However, locally (on my laptop without GPU) and on my server, the warning is thrown.

The full code is the following:

```python

import tensorflow as tf

class RandomResizedCrop(tf.keras.layers.Layer):
    # taken from
    # https://keras.io/examples/vision/nnclr/#random-resized-crops
    def __init__(self, scale, ratio, crop_shape):
        super(RandomResizedCrop, self).__init__()
        self.scale = scale
        self.log_ratio = (tf.math.log(ratio[0]), tf.math.log(ratio[1]))
        self.crop_shape = crop_shape

    def call(self, images):
        batch_size = tf.shape(images)[0]

        random_scales = tf.random.uniform(
            (batch_size,),
            self.scale[0],
            self.scale[1]
        )
        random_ratios = tf.exp(tf.random.uniform(
            (batch_size,),
            self.log_ratio[0],
            self.log_ratio[1]
        ))

        new_heights = tf.clip_by_value(
            tf.sqrt(random_scales / random_ratios),
            0,
            1,
        )
        new_widths = tf.clip_by_value(
            tf.sqrt(random_scales * random_ratios),
            0,
            1,
        )
        height_offsets = tf.random.uniform(
            (batch_size,),
            0,
            1 - new_heights,
        )
        width_offsets = tf.random.uniform(
            (batch_size,),
            0,
            1 - new_widths,
        )

        bounding_boxes = tf.stack(
            [
                height_offsets,
                width_offsets,
                height_offsets + new_heights,
                width_offsets + new_widths,
            ],
            axis=1,
        )
        images = tf.image.crop_and_resize(
            images,
            bounding_boxes,
            tf.range(batch_size),
            self.crop_shape,
        )
        return images

import tensorflow_datasets as tfds


ds = tfds.load('cifar10', split='train', as_supervised=True)
image_width = 16
crop = RandomResizedCrop(
    scale=(0.08, 1.0),
    ratio=(0.75, 1.33),
    crop_shape=(image_width, image_width),
)
data_aug_list = [
    tf.keras.layers.ZeroPadding2D(padding=4),
    crop,
]
data_aug_layer = tf.keras.models.Sequential(data_aug_list)
ds = ds.map(
  lambda x, y: (data_aug_layer(x[None], training=True)[0], y),
  num_parallel_calls=tf.data.experimental.AUTOTUNE,
)
ds = ds.shuffle(
  buffer_size=1000,  # For now a hardcoded value
  reshuffle_each_iteration=True,
).batch(
  32,
  num_parallel_calls=tf.data.experimental.AUTOTUNE,
)
ds = ds.prefetch(
  buffer_size=tf.data.experimental.AUTOTUNE,
)

res = next(iter(ds))  # warning is thrown here
```


### Relevant log output

```shell
2022-07-25 15:28:12.176012: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 16:47:53.335022: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 18:07:19.352253: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 19:26:43.163855: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 20:46:07.180534: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 22:05:27.767552: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 23:24:49.840029: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 00:44:13.601504: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 02:03:37.488186: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 03:22:57.948498: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 04:42:09.083767: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=1562147.0. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: r10i0n2: task 0: Out Of Memory
srun: launch/slurm: _step_signal: Terminating StepId=1562147.0
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=1562147.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
```
</details>",Performance Issue,Performance Issue
"tf.raw_ops.DatasetToTFRecord: Aborted (core dumped) ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.DatasetToTFRecord` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = [[1, 2], [3, 4], [5, 6]]

input_data_strings = [[str(d) for d in inner_list] for inner_list in input_data]

dataset = tf.data.Dataset.from_tensor_slices(input_data_strings)

filename = ""output.tfrecord""

tf.raw_ops.DatasetToTFRecord(input_dataset=dataset._variant_tensor, filename=filename, compression_type="""")
```


### Relevant log output

```shell
2024-03-14 05:41:32.476705: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
Aborted (core dumped)
```
",Runtime Error,Runtime Error
"RAM memory leak with tf.function when training multiple models in a loop ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

When I train multiple models in a loop, if I decorate the `train()` function with `@tf.function`, then the memory usage keeps on increasing after each loop iteration, even when I delete the model at the end of each loop and clear the TensorFlow graph/session.

The memory leak does not occur when `@tf.function` is removed. However, model training performance is significantly slower.

Colab notebook to reproduce issue:
https://colab.research.google.com/drive/1sJsGmcFeZVx6ImNbqnBsgF_LzIyPxXPW?usp=sharing

### Standalone code to reproduce the issue

```python
import gc
import os

import psutil
import tensorflow as tf


class MyModel:
    def __init__(self):
        self.dnn = tf.keras.Sequential([
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(1),
        ])
        self.optimizer = tf.optimizers.Adam()
    
    @tf.function   # if we remove this @tf.function decorator, then there is no memory leak
    def train(self, X):
        with tf.GradientTape() as tape:
            loss = tf.reduce_sum(self.dnn(X))
        grads = tape.gradient(loss, self.dnn.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.dnn.trainable_variables))

process = psutil.Process(os.getpid())
rss = int(process.memory_info().rss / 1024 / 1024)  # in MB
print(f'rss: {rss} MB')

X = tf.ones((50, 80))
for i in range(50):
    model = MyModel()
    for _ in range(20):
        model.train(X)

    del model
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()
    gc.collect()

    new_rss = int(process.memory_info().rss / 1024 / 1024)
    if new_rss > rss:
        rss_increase = new_rss - rss
        rss = new_rss
        print(f'Iter {i:05d}, rss increase: {rss_increase} MB, rss: {rss} MB')
```


### Relevant log output

```shell
rss: 593 MB
Iter 00000, rss increase: 31 MB, rss: 624 MB
Iter 00001, rss increase: 6 MB, rss: 630 MB
Iter 00002, rss increase: 5 MB, rss: 635 MB
Iter 00003, rss increase: 5 MB, rss: 640 MB
Iter 00004, rss increase: 5 MB, rss: 645 MB
Iter 00005, rss increase: 5 MB, rss: 650 MB
Iter 00006, rss increase: 5 MB, rss: 655 MB
Iter 00007, rss increase: 5 MB, rss: 660 MB
Iter 00008, rss increase: 5 MB, rss: 665 MB
Iter 00009, rss increase: 5 MB, rss: 670 MB
Iter 00010, rss increase: 4 MB, rss: 674 MB
---  <omitting some rows for brevity>  ---
Iter 00040, rss increase: 5 MB, rss: 822 MB
Iter 00041, rss increase: 5 MB, rss: 827 MB
Iter 00042, rss increase: 5 MB, rss: 832 MB
Iter 00043, rss increase: 5 MB, rss: 837 MB
Iter 00044, rss increase: 5 MB, rss: 842 MB
Iter 00045, rss increase: 5 MB, rss: 847 MB
Iter 00046, rss increase: 5 MB, rss: 852 MB
Iter 00047, rss increase: 5 MB, rss: 857 MB
Iter 00048, rss increase: 5 MB, rss: 862 MB
Iter 00049, rss increase: 5 MB, rss: 867 MB
```
",Performance Issue,Performance Issue
"[DevTools Bug]: React Devtools do not show source position for host component ### Website or app

none

### Repro steps

<img width=""1715"" alt=""image"" src=""https://github.com/user-attachments/assets/86400e7a-d653-4ee5-a9c7-f7287931a224"">
As you see, it should show source filed

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",UI/UX Bug,Syntax Error
"Inconsitency between C-contiguous and F-contiguous arrays ### No consistency between C-contiguous and F-contiguous arrays for LinearRegression()

At least for LinearRegression() : In some edge case (when X is almost singular), there is huge difference between C-contiguous and F-contiguous arrays predictions.

- This is due to the fact that array product gives different results between contiguous and F-contiguous arrays ([cf this Stack Overflow questions that I posted](https://stackoverflow.com/questions/76388886/python-rounding-errors-between-c-contiguous-and-f-contiguous-arrays-for-matrix))
- These ""edge cases"" can actually be quite common in time-series predictions, where a lot of auto-regressive features can easily be correlated
- I would strongly advise parsing all arrays to C-contiguous before doing the predictions/fitting.
- Please also note that **fitting** with F-contiguous or C-contiguous can also give different results.
- The worst is not that this is happening, it is that no warning are being raised whatsoever.
- Also, F-contiguous arrays are extremely common in pandas DataFrames, which is what a lot of developers are using in this context...

### Steps/Code to Reproduce

```python
import numpy as np; print(np.__version__) # 1.23.5
import scipy; print(scipy.__version__) # 1.10.0
import sklearn as sk; print(sk.__version__) # 1.2.1

from sklearn.linear_model import LinearRegression
import pandas as pd

# Parameters 
seed, N_obs, N_feat, mu_x, sigma_x, mu_y, sigma_y = 0, 100, 1000, 100, 0.1, 100, 1

# 1) Creating a weird edge-case X, y :
np.random.seed(seed)
s = pd.Series(np.random.normal(mu_x, sigma_x, N_obs))
X = np.stack([s.ewm(com=com).mean() for com in np.arange(N_feat)]).T
y = np.random.normal(mu_y, sigma_y, N_obs)

# 2) Showing that there is different results for C-cont vs F-cont arrays :
model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)
y_pred_c = model.predict(np.ascontiguousarray(X))

# Either just plot it and see :
import matplotlib.pyplot as plt
plt.scatter(y_pred, y_pred_c)

# Or look at the data :
np.var(y_pred)
np.var(y_pred - y_pred_c)
np.corrcoef(y_pred, y_pred_c)[0,1] # == 0.40295584536349216
# --> y_pred EXTREMELY different than y_pred_c
```

### Expected Results

We expect y_pred to be fully equal to y_pred_c.
Or at least `np.corrcoef(y_pred, y_pred_c)[0,1] > .99`


### Actual Results

`np.corrcoef(y_pred, y_pred_c)[0,1] # == 0.40295584536349216`
- ypred and y_pred_c are totally different.

### Versions

```shell
System:
    python: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0]
executable: /opt/anaconda3/bin/python
   machine: Linux-5.10.0-23-cloud-amd64-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.2.1
          pip: 22.3.1
   setuptools: 65.6.3
        numpy: 1.23.5
        scipy: 1.10.0
       Cython: None
       pandas: 1.5.3
   matplotlib: 3.7.0
       joblib: 1.1.1
threadpoolctl: 2.2.0

Built with OpenMP: True

threadpoolctl info:
       filepath: /opt/anaconda3/lib/libmkl_rt.so.1
         prefix: libmkl_rt
       user_api: blas
   internal_api: mkl
        version: 2021.4-Product
    num_threads: 64
threading_layer: intel

       filepath: /opt/anaconda3/lib/libiomp5.so
         prefix: libiomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 128

       filepath: /opt/anaconda3/lib/libgomp.so.1.0.0
         prefix: libgomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 128
```
",Syntax Error,Syntax Error
"XLA compiled `tf.reshape` throws ValueError: Shape must be rank 1 but is rank 0 ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.13, 2.15.0-dev20230913

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following model calls `tf.reshape` on an input tensor, and throws ValueError when compiled with XLA. Without XLA compilation, it runs smoothly. 

This issue is observed on TF 2.13 and also nightly. See the [gist](https://colab.research.google.com/gist/dengyinlin/d9368054f16a011f9d1fed3cede96b95/xla-compiled-tf-reshape-throws-valueerror-shape-must-be-rank-1-but-is-rank-0.ipynb) for more detail.


### Standalone code to reproduce the issue

```python
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True)
  def call(self, x1):
    x2 = tf.reshape(x1, (2, 2))
    return tf.reshape(x2, (4))

m = Model()

input_shape = (4)
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

y = m(x1)
print(y)
```


### Relevant log output

```shell
ValueError: Shape must be rank 1 but is rank 0 for '{{node Reshape_1}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](Reshape, Reshape_1/shape)' with input shapes: [2,2], [].
```
",Runtime Error,Runtime Error
"How can I exit the XLAControlFlowContext when inside a jit_compile tf.function? Exit() function take no effect. ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Python version

3.10

### CUDA/cuDNN version

CUDA12.3/cuDNN9.0

### GPU model and memory

GTX4090 24GiB

### Current behavior?

I have a custom op that is a normal OpKernel unable to be compile by XLA cluster. But unfortunately, when the user use this custom op, they have to wrap it into a tf.funtion. So somehow when the use Keras jit_compile or something else, errors happened.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

@tf.function(jit_compile=True)
def test(a):
    b = a + a
    ctx = tf.__internal__.get_enclosing_xla_context()
    ctx.Exit()
    tf.print(b)
    ctx.Enter()
    return b * b

test(tf.constant(1))
```


### Relevant log output

```shell
2024-03-14 02:28:52.720666: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:296 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_test_9[_XlaMustCompile=true,config_proto=9241198235816212909,executor_type=11160318154034397263] on XLA_GPU_JIT: StringFormat (No registered 'StringFormat' OpKernel for XLA_GPU_JIT devices compatible with node {{node StringFormat}}){{node StringFormat}}
The op is created at: 
File ""xla_context_test.py"", line 12, in <module>
  test(tf.constant(1))
File ""xla_context_test.py"", line 8, in test
  tf.print(b)
Traceback (most recent call last):
  File ""xla_context_test.py"", line 12, in <module>
    test(tf.constant(1))
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_test_9[_XlaMustCompile=true,config_proto=9241198235816212909,executor_type=11160318154034397263] on XLA_GPU_JIT: StringFormat (No registered 'StringFormat' OpKernel for XLA_GPU_JIT devices compatible with node {{node StringFormat}}){{node StringFormat}}
The op is created at: 
File ""xla_context_test.py"", line 12, in <module>
  test(tf.constant(1))
File ""xla_context_test.py"", line 8, in test
  tf.print(b) [Op:__inference_test_9]
```
",Dependency Issue,Dependency Issue
"[DevTools Bug]: Components without own dimensions not highlighted at all ### Website or app

https://codesandbox.io/s/trusting-night-35uuz?file=/src/App.js

### Repro steps

Create a fixed element with an absolute child like this: 
```js
  return (
    <div style={{ position: ""fixed"", left: point.x, top: point.y }}>
      <div
        style={{
          position: ""absolute"",
          color: ""blue""
        }}
      >
        Popover
      </div>
    </div>
  );
```
On top of that implement `rAF` loop updating the `point`. In such a scenario the root `div` gets constantly rerendered but no highlight is drawn on the canvas. Or rather - it is drawn ([here](https://github.com/facebook/react/blob/54f6ae9b1c0489784f6a95bbe26ffec31816d74a/packages/react-devtools-shared/src/backend/views/TraceUpdates/canvas.js#L49)), but its dimensions are 0x0 so nothing gets highlighted from the user perspective.

If you believe this is an issue I would gladly work on a fix. I'm unsure what would be the proper fix here. My ideas are those:
1. either introduce more heuristics for measuring nodes in [this function](https://github.com/facebook/react/blob/cae635054e17a6f107a39d328649137b83f25972/packages/react-devtools-shared/src/backend/views/TraceUpdates/index.js#L135-L143)
2. or always draw at least 2px x 2px highlight

cc @bvaughn 

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Performance Issue,Performance Issue
"Updates not being installed automatically 
Type: <b>Bug</b>

I am having manually install all the updates.

VS Code version: Code 1.95.1 (65edc4939843c90c34d61f4ce11704f09d3e5cb6, 2024-10-31T05:14:54.222Z)
OS version: Linux x64 6.8.0-48-generic
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|AMD Ryzen 7 4700U with Radeon Graphics (8 x 3891)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: disabled_off<br>webnn: disabled_off|
|Load (avg)|4, 4, 4|
|Memory (System)|30.75GB (10.95GB free)|
|Process Argv|--crash-reporter-id c456e7cc-403b-4b0c-be5f-c2666a60695d|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|x11|
</details><details><summary>Extensions (9)</summary>

Extension|Author (truncated)|Version
---|---|---
vscode-intelephense-client|bme|1.12.6
browserstack-vscode|bro|1.2.4
githistory|don|0.6.20
gitlens|eam|16.0.1
vscode-mysql|for|0.5.0
mysql-syntax|jak|1.3.1
php-cs-fixer|jun|0.3.21
vscode-docker|ms-|1.29.3
remote-containers|ms-|0.388.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
724cj586:31013169
dvdeprecation:31068756
dwnewjupyter:31046869
impr_priority:31102340
nativerepl2:31139839
pythonrstrctxt:31112756
cf971741:31144450
iacca1:31171482
notype1cf:31157160
5fd0e150:31155592
dwcopilot:31170013
stablechunks:31181875

```

</details>

<!-- generated by issue reporter -->",Dependency Issue,Dependency Issue
"BaggingClassifier uses Class Label as Index to Array when Voting <!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->
BaggingClassifier uses Class Label as Index to Array when Voting

#### Steps/Code to Reproduce

Provide a base estimator to _BaggingClassifier_ that does not define the function _predict_proba_. This results in _BaggingClassifier_ resorting to voting. It appears the code for performing voting uses class labels as array indices instead of looking up the index of the class label in the _classes__ member.

Example:
```python
import numpy as np
from sklearn.ensemble import BaggingClassifier

class Foo:
    
    def __init__(self):
        pass
    
    def fit(self, X, Y, W=None):
        return self
    
    def predict(self, X):
        return np.full(X.shape[0], True, np.bool)
    
    def score(self, X, Y):
        YH = self.predict(X)
        return (Y == YH).mean()
    
    def get_params(self, deep=True):
        return {}
    
    def set_params(self, **params):
        for k, v in params:
            setattr(self, k, v)
        return self
    
# %%
A = np.random.rand(10, 4)
Y = np.random.randint(2, size=10, dtype=np.bool)
bc = BaggingClassifier(Foo())
bc.fit(A, Y)
YH = bc.predict(A)
print('BaggingClassifier Voting Result: ')
print(YH)
print('Ensemble Member Predictions: ')
for Ei in bc.estimators_:
    print(Ei.predict(A))
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
In the above code snippet, _BaggingClassifier_ should return an array of _True_ since it is the majority prediction of all ensemble members.

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->
_BaggingClassifier_ returns an array of False. This issue only occurs when the base estimator does not define the function _predict_proba_.

The issue appears to be due to lines 137 and 140 in ensemble/bagging.py.

https://github.com/scikit-learn/scikit-learn/blob/e14ac6d36d9dd069cc7fc2e51e4973514a003591/sklearn/ensemble/bagging.py#L137

https://github.com/scikit-learn/scikit-learn/blob/e14ac6d36d9dd069cc7fc2e51e4973514a003591/sklearn/ensemble/bagging.py#L140

The predictions of the ensemble members are directly used as indices into the original array. I'm guessing the prediction labels need to be converted into class labels using _estimator.classes__.

#### Versions
System:
    python: 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]
executable: C:\Users\XXXXXX\Anaconda3\pythonw.exe
   machine: Windows 2012 ServerR2

BLAS:
    macros: 
  lib_dirs: 
cblas_libs: cblas

Python deps:
       pip: 18.1
setuptools: 40.6.3
   sklearn: 0.20.1
     numpy: 1.15.4
     scipy: 1.1.0
    Cython: 0.29.2
    pandas: 0.23.4

<!-- Thanks for contributing! -->
",Logical Bug,Logical Bug
"tf.raw_ops.ResourceApplyProximalAdagrad: Aborted (core dumped) ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.ResourceApplyProximalAdagrad` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3])
lr = 0.01
l1 = 0.1
l2 = 0.01
grad = tf.constant([0.1, 0.2, 0.3], dtype=tf.float64)

var_resource = tf.raw_ops.VarHandleOp(dtype=var.dtype, shape=var.shape)
accum_resource = tf.raw_ops.VarHandleOp(dtype=accum.dtype, shape=accum.shape)

assign_var = tf.raw_ops.AssignVariableOp(resource=var_resource, value=var)
assign_accum = tf.raw_ops.AssignVariableOp(resource=accum_resource, value=accum)

result = tf.raw_ops.ResourceApplyProximalAdagrad(var=var_resource, accum=accum_resource, lr=lr, l1=l1, l2=l2, grad=grad)
```


### Relevant log output

```shell
2024-03-14 05:57:20.787563: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (1 vs. 2) double expected, got float
Aborted (core dumped)
```
",Runtime Error,Runtime Error
"When searching rtl letters in Arabic using `Ctrl+F` the word gets reversed and disconnected <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version:  1.95.2
- OS Version: Linux x64 6.11.6-arch1-1

Steps to Reproduce:

1.  Open A file with Arabic text
2. click `Ctrl+F`
3. search for a part of the word
4. observe the word being reversed and incorrectly aligned

Wrong behavior below:
![Image](https://github.com/user-attachments/assets/38d614a4-29bf-4140-800b-06d091532277)


How it looks when highlighted using side search bar (correct)
![Image](https://github.com/user-attachments/assets/37f98147-f2be-4c71-aed9-4a18a129dec1)


",UI/UX Bug,UI/UX Bug
"Tensorflow 2.15 Docker image cannot find the GPU drivers, but nvidia-smi can. ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

TF 2.15.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.2

### GPU model and memory

NVIDIA TITAN V

### Current behavior?

Running TensorFlow 2.15.0 from within Docker image does not find GPU drivers.

nvidia-smi reports the GPUs as available.

This works as intended with a TF 2.14 image in the same machine.


### Standalone code to reproduce the issue

```shell
docker run --gpus all -it tensorflow/tensorflow:2.15.0-gpu-jupyter bash

<within the container>

# nvidia-smi

# python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```


### Relevant log output

```shell
# nvidia-smi

Thu Nov 16 16:47:22 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA TITAN V                 Off | 00000000:65:00.0 Off |                  N/A |
| 29%   46C    P8              27W / 250W |   1693MiB / 12288MiB |      2%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

# python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""

2023-11-16 16:46:54.131081: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2023-11-16 16:46:54.255566: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-11-16 16:46:54.255623: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-11-16 16:46:54.276648: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-11-16 16:46:54.327586: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2023-11-16 16:46:54.328299: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-16 16:46:56.486851: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]
```
",Dependency Issue,Dependency Issue
"MaxListenersExceededWarning: Possible EventEmitter memory leak detected. <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.95.3
- OS Version: Ubuntu 22.04.5 LTS

Steps to Reproduce:

1. Close VSCode with 8 projects opened (most of them DevContainer projects)
2. Open VSCode again from terminal ( running `code --disable-extensions --trace-warnings` to disable extensions and to show warning details) without specifying any folder or file (so last working session is used opening the 8 projects)
3. Following log is printed in the terminal:
```bash
Warning: 'trace-warnings' is not in the list of known options, but still passed to Electron/Chromium.
[main 2024-11-19T10:24:29.689Z] update#setState idle
(node:471892) MaxListenersExceededWarning: Possible EventEmitter memory leak detected. 11 child-process-gone listeners added to [App]. MaxListeners is 10. Use emitter.setMaxListeners() to increase limit
    at genericNodeError (node:internal/errors:984:15)
    at wrappedFn (node:internal/errors:538:14)
    at _addListener (node:events:593:17)
    at App.addListener (node:events:611:10)
    at Object.X [as onWillAddFirstListener] (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:7366)
    at q (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:31:1282)
    at Na.H (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:13550)
    at Na.F (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:11857)
    at Na.start (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:15161)
    at vh.start (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:46:29191)
    at Object.call (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:36:4563)
    at df.s (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:19676)
    at df.q (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:19199)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:34:18601)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.C (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:816)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:1033)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:4908)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:964)
    at qo.value (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:5092)
    at P.B (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:746)
    at P.fire (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:33:964)
    at T (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:28:7343)
    at IpcMainImpl.i (file:///opt/VSCode-linux-x64-1.95.3/resources/app/out/main.js:36:21224)
    at IpcMainImpl.emit (node:events:531:35)
    at WebContents.<anonymous> (node:electron/js2c/browser_init:2:85782)
    at WebContents.emit (node:events:519:28)
[main 2024-11-19T10:24:59.835Z] update#setState checking for updates
[main 2024-11-19T10:25:00.551Z] update#setState idle
```
",Security Vulnerability,Security Vulnerability
"VSCode freezing during search <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->

Version: 1.95.3 (user setup)
Commit: f1a4fb101478ce6ec82fe9627c43efbf9e98c813
Date: 2024-11-13T14:50:04.152Z
Electron: 32.2.1
ElectronBuildId: 10427718
Chromium: 128.0.6613.186
Node.js: 20.18.0
V8: 12.8.374.38-electron.0
OS: Windows_NT x64 10.0.19045

Steps to Reproduce:

Scenario 1:
![Image](https://github.com/user-attachments/assets/066e64b5-873d-4808-b298-905b2d62cc12)

Scenario 2:
![Image](https://github.com/user-attachments/assets/3fece497-ecc7-4054-b86a-fd2111b85a1c)
The path in files.includes can be changed to an absolute path. 
![Image](https://github.com/user-attachments/assets/191396fd-3324-4a3e-9862-8b2bb94e0f94)

1. Open the vscode source code project.
2. Open the settings.json file.
3. Search`\((.*\n.*)*\);`and select the regular button.
4. The vscode freezes and a dialog is displayed.
![Image](https://github.com/user-attachments/assets/2468b048-1068-418d-a00d-f58b1a275381)

",Performance Issue,Performance Issue
"[Compiler Bug]: Performance - `useEffect` without dependencies should be left alone ### What kind of issue is this?

- [X] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)
- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)
- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)
- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)

### Link to repro

https://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEUYCAwgBYCGmA5gmAKIBuCMAngCp4C2CACgCURYAB1iROITA4iAbRbUANgBoiZHADUVAXSIBeEmQDKOajkHDDAPiIBZC5QB0MWgBMIvYUIDcEiSJjBCY0NARcAWsDO3FJIM0dZQFHHBc3TE9vIT9AogBfXMw8mAQcWGIlZX9MfIDMDGx8QmCAdTcABySoBABBMAAlBDRrOKDpTFkiKsNgqloGZjZOHn5hGvGZOVK0WdIEIZGqorz90PDI6Ni8oJ3nOFhSzDkjKo2Ck8lS8phiHZq6pgQPkgA

### Repro steps

I was playing around with a small repo I had and noticed something similar to #30782. (Essentially, I was tracking a variable so that the callback identity could be stable to avoid rapid-fire rerenders of the whole page)

Wrapping it in a useEffect works and probably more correct, but it generates redundant memoization:

```js
function useWrapValueAsRef() {
  const $ = _c(2);
  const val = useChangesEveryTime();
  const ref = useRef(val);
  let t0;
  if ($[0] !== val) {
    t0 = () => {
      ref.current = val;
    };
    $[0] = val;
    $[1] = t0;
  } else {
    t0 = $[1];
  }
  useEffect(t0);
  return ref;
}
```

where I would expect it to just leave everything alone.

Ignoring the contrived source of the variable (in the real app I'm testing with, it probably changes a couple times on page load), it does the check every time, which is redundant because the useEffect will always run the latest closure anyways. I'm not sure if this is because of some sort of optimization in useEffect but I don't see why that would be the case.

I don't imagine this to be a huge issue, but it's technically less performant as the code size is bigger and it uses 2 array slots.

### How often does this bug happen?

Every time

### What version of React are you using?

React Playground (19-RC)",Performance Issue,Performance Issue
"Segmentation fault when running tensorflow.python.framework.kernels.get_registered_kernels_for_op ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Probably due to feeding None argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.framework import kernels
try:
  arg_0 = None
  out = kernels.get_registered_kernels_for_op(arg_0,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-12 13:41:10.388491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Segmentation fault

```
```
",Performance Issue,Performance Issue
"[BUG] race condition in local rendezvous ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### TensorFlow version

2.10, 2.12, doesn't really matter

### Custom code

Yes

### OS platform and distribution

ubuntu 20.04

### Python version

3.8, 3.9

### Bazel version

5.3.0

### GCC/compiler version

9.4

### Current behavior?

The story is kinda bit long and it took us months to debug this issue. I'll try to keep it short.

### Background and what the problem is
We run the [DLRM from NV DLE](https://github.com/NVIDIA/DeepLearningExamples/tree/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM) in our CI daily test. 

Very rare, like once every few weeks, the daily test report such error.

`
W tensorflow/core/framework/op_kernel.cc:1874] OP_REQUIRES failed at strided_slice_op.cc:112 : INVALID_ARGUMENT: Expected begin, end, and strides to be 1D equal size tensors, but got shapes [65536], [1], and [1] instead.
`

The[ strided slice op validator](https://github.com/tensorflow/tensorflow/blob/3a029b19c9c156cd68cab671b5ce95bde839f15e/tensorflow/core/util/strided_slice_op.cc#L214) isn't happy about the input parameters.

The problem is like a ghost. It appears once every 1 or 2 weeks which makes it very hard to debug. But one thing is for sure is that it's not a random dram bit flip caused by cosmic rays because the error systoms is stable: one of (begin, end, stride) tensor's shape is incorrectly set to [65535] instead of [1]

### Evidence

I'll skip the lengthy debug process and jump to the last step.

![image](https://github.com/tensorflow/tensorflow/assets/8800468/d73ddf6d-7cc9-4a10-90f8-1e4d3885c91b)
![image](https://github.com/tensorflow/tensorflow/assets/8800468/0b99c252-59b0-490d-8321-924adb70be72)


We observed a malfunctioned SEND/RECV pair.

The item tensor in SENDOP has a shape of [1] but during the local rendezvous process the recevied tensor got a shape of [65536]. This tensor later flows to strided slice op and triggers the grumpy validator.

![image](https://github.com/tensorflow/tensorflow/assets/8800468/7626ef2f-0de4-4eb0-8c25-3cfd9618b359)

We grep the key hash (red highlighted 1632...9087) in the debug log.  There are two threads: 66787 and 63997. Most of the time the rendezvous runs in a SEND-RECV pattern within the same thread. 

But in the green rectangle you can see two threads interleaved and a SEND-SEND-RECV-RECV pattern is observed. Soon after this the bomb exploded.

### Analysis

It seems the root cause is a bug in the rendezvous mechanism. Two unrelated operations generated the same communication Rendezvous key. 

Assuming there are 2 threads, when the CPU load is not heavy, in most cases the scheduling order is

- THREAD A: SEND(KEY, VALUE_A)

- THREAD A: RECV(KEY)

- THREAD B: SEND(KEY, VALUE_B)

- THREAD B: RECV(KEY)

In this way, everyone will be fine even if the keys are the same. It is equivalent to time-division multiplexing of the same KEY.

When the CPU load becomes heavy (which is the case in our daily test scenario), thread scheduling becomes unpredictable. It is possible that such a pattern may occur

- THREAD A: SEND(KEY, VALUE_A)

- THREAD B: SEND(KEY, VALUE_B)

- THREAD A: RECV(KEY)

- THREAD B: RECV(KEY)

This will cause THREAD A to incorrectly receive the data sent by THREAD B.


### Related python code

1. Thread A is running the normal [training loop.](https://github.com/NVIDIA/DeepLearningExamples/blob/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM/main.py#L281)
2. Thread B is created for the RawBinaryDataset class. The pre-processing creates an [asynchronous thread pool.](https://github.com/NVIDIA/DeepLearningExamples/blob/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM/split_binary_dataset.py#L138)
3. Thread A is running[ x = x[self.begin_idx:self.end_idx]](https://github.com/NVIDIA/DeepLearningExamples/blob/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM/model.py#L58). Translated into StridedSlice operation by eager runtime. begin_idx, end_idx and the implicit stride are all constant tensors on CPU, and StridedSlice is an device operator. So these three tensors need to be sent to the device side.
4. Thread B is running[ tensor = tf.expand_dims(tensor, axis=1). ](https://github.com/NVIDIA/DeepLearningExamples/blob/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM/split_binary_dataset.py#L204). Execute expand_dims. Similar to thread A, the expand_dims operator runs on the device side while the input is on CPU, and the tensor needs to be sent to the device. The shape of this tensor is [65536].
5. Due to the defect of Rendezous::CreateKey, the keys generated by these two operators in the eager runtime are exactly the same.
6. Multi-threading + HASH KEY collision + SEND-SEND-RECV-RECV, all three together, BOOOM.

### FIX

![image](https://github.com/tensorflow/tensorflow/assets/8800468/f50ed76a-9cef-4bf7-87b2-e4a298236dd9)

The key string is 
/job:localhost/replica:0/task:0/device:CPU:0;ea74dbce35f0ab7e;/job:localhost/replica:0/task:0/device:MLU:0;edge_2_input;0:0

The rendezvous key consists 5 parts.
- src_device: /job:localhost/replica:0/task:0/device:CPU:0;
- src_incarnation: ea74dbce35f0ab7e;
- dst_device: /job:localhost/replica:0/task:0/device:MLU:0;
- name: edge_2_input;
- frame_inter: 0:0

In this case,  4 out of 5 (src_device, src_incarnation, dst_device_frame_iter) are naturely indistinguishable.

Unless we add new field in the key, the only field we can play with is `name`.

The creation of `edge_2_input` involes another few tons of code.

![image](https://github.com/tensorflow/tensorflow/assets/8800468/b2d85cd7-1e47-492b-be50-1e3d63f13d55)

A quick fix for my current problem is simply add `dst_name` in the `tensor_name_attr` during graph partition. The names will become `edge_2_input_strided_slice` and `edge_2_input_expand_dims` thus my problem is solved.

```diff
diff --git a/tensorflow/core/graph/graph_partition.cc b/tensorflow/core/graph/graph_partition.cc
index a4f09383c63..57a4e919526 100644
--- a/tensorflow/core/graph/graph_partition.cc
+++ b/tensorflow/core/graph/graph_partition.cc
@@ -1147,7 +1147,8 @@ Status Partition(const PartitionOptions& opts, Graph* g,
         tensor_name_attr = opts.get_tensor_name_attr(edge);
       } else {
         tensor_name_attr =
-            strings::StrCat(""edge_"", edge->id(), ""_"", edge->src()->name());
+            strings::StrCat(""edge_"", edge->id(), ""_"", edge->src()->name(),
+            ""_"", edge->dst()->name());
       }
```

But a more approriate and generic fix might be to have a unique src node name. The source name will be like input_xxxxxx and input_yyyyyy. But this sounds like a fundamental change and I'm not sure if this would break too many things. And I'm not sure about the right place to make the change, like manipulating the input name a little bit [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/eager/execute.cc#L988).

I'd like to hear your opinion on this problem and I'd like to file a PR if you could point me the right place to apply the fix.

Cheers
Hengwen

### Standalone code to reproduce the issue

A minimum case derived from the [DLRM from NV DLE](https://github.com/NVIDIA/DeepLearningExamples/tree/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM) 

Click run all and you can reproduce the error in a few seconds.

https://colab.research.google.com/drive/1CUiyuG2Aob-Fj8xllrmbx2w3cNBXFYqU?usp=sharing




",Logical Bug,Logical Bug
"[Compiler Bug]: Values used as indexes are not memoized ### What kind of issue is this?

- [X] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)
- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)
- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)
- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)

### Link to repro

https://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEAwhALYAOhCmOAFMEQG4CGANlAkQL4CURYAB1iROITA4ieTABMEADyIBeIosq0weZggBiWXAUz02nBHxFEi7BFOg5KUKatNcwAbRnyFAXQDclkQA9EH2js7ScooBojC2sMQAPLLaAHzAYU48iUEpzKkxPDEgPEA

### Repro steps

In the code below, the call to `expensiveFunction(value)` does not appear to be memoized (it is preserved as is at the top level in the JS output).

This seems to be due to it being used as an index in `let output = values[index]`, because when setting `output` to be `index` itself instead, it gets memoized properly.

Code:
```js
function Component({ value }) {
  const index = expensiveFunction(value)
  let output = values[index];
  //output = index;
  return <div>{output}</div>;
};
```

**_Edit:_** 
Additionally, if `index` was manually memoized with `useMemo`, the compiler will remove the manual memoization, resulting in potentially much slower code than the original code.

### How often does this bug happen?

Every time

### What version of React are you using?

19",Performance Issue,Performance Issue
"Default argument pos_label=1 is not ignored in f1_score metric for multiclass classification ### Describe the bug

I get a `ValueError` for `pos_label=1` default argument value to `f1_score` metric with argument `average='micro'` for the iris flower classification problem:

```pytb
ValueError: pos_label=1 is not a valid label: It should be one of ['setosa' 'versicolor' 'virginica']
```

According to the documentation, the `pos_label` argument should be ignored for the multiclass problem:

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#f1-score

_The class to report if `average='binary'` and the data is binary, otherwise this parameter is ignored._

Setting `pos_label` explicitly to None solves the problem and produces the expected output, see below.

### Steps/Code to Reproduce

```python
# Import necessary libraries
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import make_scorer, f1_score

# Load the Iris dataset
data = load_iris()
X = data.data  # Features
y = data.target  # Labels

# Convert labels to string type
y = np.array([data.target_names[label] for label in data.target])

# Initialize the Linear Discriminant Analysis classifier
classifier = LinearDiscriminantAnalysis()

# Define a custom scorer using F1 score with average='micro'
f1_scorer = make_scorer(f1_score, average='micro', pos_label=1)

# Perform cross-validation with cross_val_score
try:
    scores = cross_val_score(classifier, X, y, cv=5, scoring=f1_scorer)
    print(f""Cross-validated F1 Scores (micro average): {scores}"")
    print(f""Mean F1 Score: {np.mean(scores)}"")
except ValueError as e:
    print(f""Error: {e}"")
```

### Expected Results

```
Cross-validated F1 Scores (micro average): [1.         1.         0.96666667 0.93333333 1.        ]
Mean F1 Score: 0.9800000000000001
```

### Actual Results

```pytb
Cross-validated F1 Scores (micro average): [nan nan nan nan nan]
Mean F1 Score: nan
[C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\model_selection\_validation.py:1000](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/model_selection/_validation.py#line=999): UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File ""[C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py"", line 139](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/metrics/_scorer.py#line=138), in __call__
    score = scorer._score(
            ^^^^^^^^^^^^^^
  File ""[C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py"", line 371](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/metrics/_scorer.py#line=370), in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File ""[C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py"", line 89](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/metrics/_scorer.py#line=88), in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File ""[C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\utils\_response.py"", line 204](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/utils/_response.py#line=203), in _get_response_values
    raise ValueError(
ValueError: pos_label=1 is not a valid label: It should be one of ['setosa' 'versicolor' 'virginica']
```

### Versions

```shell
System:
    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\rgt0227\AppData\Local\anaconda3\python.exe
   machine: Windows-10-10.0.19045-SP0

Python dependencies:
      sklearn: 1.5.0
          pip: 23.2.1
   setuptools: 68.0.0
        numpy: 1.26.2
        scipy: 1.11.4
       Cython: None
       pandas: 2.1.1
   matplotlib: 3.8.0
       joblib: 1.2.0
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: mkl
    num_threads: 8
         prefix: mkl_rt
       filepath: C:\Users\rgt0227\AppData\Local\anaconda3\Library\bin\mkl_rt.2.dll
        version: 2023.1-Product
threading_layer: intel

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: vcomp
       filepath: C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None
```
",Syntax Error,Syntax Error
"Memory leak I'm getting this in dev console. Based on git-blame at contentHoverStatusBar.ts:48, assigning to you  @aiday-mar 

Not sure how to repro, but should be traceable by code inspection.

```
Error: Trying to add a disposable to a DisposableStore that has already been disposed of. The added object will be leaked!
    at rhi.add (lifecycle.ts:425:18)
    at qse.B (lifecycle.ts:497:22)
    at qse.addAction (contentHoverStatusBar.ts:48:8)
    at markerHoverParticipant.ts:239:23
```

![Image](https://github.com/user-attachments/assets/e7d5a8e0-eda1-4c1e-8ce3-2f41d2c30587)

Version: 1.96.0-insider
Commit: 275faf6f08b7aa50843f3c18406b4d5969784e52
Date: 2024-11-28T18:04:55.375Z
Electron: 32.2.6
ElectronBuildId: 10629634
Chromium: 128.0.6613.186
Node.js: 20.18.1
V8: 12.8.374.38-electron.0
OS: Darwin arm64 24.1.0
",Security Vulnerability,Security Vulnerability
"Bug: `hidden` attribute does not accept string values <!--
  Please provide a clear and concise description of what the bug is. Include
  screenshots if needed. Please test using the latest version of the relevant
  React packages to make sure your issue has not already been fixed.
-->

React version: 

```
 ""react"": ""18.3.0-next-522f47345-20220614"",
 ""react-dom"": ""18.3.0-next-522f47345-20220614"",
```  

## Steps To Reproduce

```jsx
<span hidden=""until-found"">Hello React<span>
```

<!--
  Your bug will get fixed much faster if we can run your code and it doesn't
  have dependencies other than React. Issues without reproduction steps or
  code examples may be immediately closed as not actionable.
-->

Link to code example:

https://codesandbox.io/s/headless-violet-ygn593

## The current behavior

React will turn hidden into a boolean attribute and removes ""until-found"":

![preview of the dom after react-dom rendered until-found](https://user-images.githubusercontent.com/4113649/174130287-c07a5d53-e31c-43b4-8925-84a1c3dbf18d.jpg)

## The expected behavior

React does not strip ""until-found"" to allow using [hidden-until-found in Chrome 102+](https://developer.chrome.com/articles/hidden-until-found/) for better SEO and accessibility.",Logical Bug,Logical Bug
"Terminal search gives up after only 1000 search highlights * run a script that prints `TRIES n`
* search for regex `TRIES \d`
* scroll around a little
* :bug: search matches aren't reliable

<img width=""1239"" alt=""Screenshot 2024-10-03 at 09 34 55"" src=""https://github.com/user-attachments/assets/9431e122-dbc9-4283-8c2c-3854080554a8"">
",Dependency Issue,Dependency Issue
"[DevTools Bug]: Profiling not supported error with up-to-date React. ### Website or app

https://vercel.com/login

### Repro steps

When I open React Developer Tools profiler page, I get the following error:

```
Profiling not supported.
Profiling support requires either a development or profiling build of React v16.5+.

Learn more at [reactjs.org/link/profiling](https://fb.me/react-devtools-profiling).
```

I first saw the problem on my own app, which is running React 18 and NextJS 15, but was able to reproduce the problem on every other NextJS react page I could find.

I am running Google Chrome 131.0.6778.204 on Fedora 40. My version of React Developer Tools is 6.0.1 (10/15/2024). 

If I test the same conditions, but on Firefox, profiling works, suggesting it is a problem with React Dev Tools.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Syntax Error,Syntax Error
"Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed. Aborted (core dumped) ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0.post1

### Custom code

Yes

### OS platform and distribution

Rocky Linux 8.9

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

8.9.2.26

### GPU model and memory

 NVIDIA A100-SXM4-80GB 

### Current behavior?

I am trying to run a simple denoising autoencoder. my training data and label data are 900 samples of healpy maps with nside 64 resolution, loaded as numpy array. After normalising the maps, I used tf.data.Dataset.from_tensor_slices, to create dataset. when I used random noise to create these maps and ran on jupyter notebook, although took ages to initiate training after doing model.fit(), but it did run and produced some result. knowing that the model works, I tried to run on GPU with real data. this is where the issue started.  it shows the following error: Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed. Aborted (core dumped), and the process stops. 

### Standalone code to reproduce the issue

```shell
Here is a colab link:

https://colab.research.google.com/drive/1odbf3gQT9h-DI6zdh5e1llmG9zhjY45l?usp=sharing

It runs on colab. but it doesn't run in the terminal.
```


### Relevant log output

```shell
2024-03-27 12:14:07.288975: F external/local_tsl/tsl/platform/default/env.cc:74] Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed.
Aborted (core dumped)
```
",Runtime Error,Runtime Error
"`tf.raw_ops.ConjugateTranspose`: negative value of `perm` can lead to out-of-bounds read ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In `tf.raw_ops.ConjugateTranspose`, negative value of `perm` can lead to out-of-bounds read.
[Here](https://github.com/tensorflow/tensorflow/blob/617b5e97d6a9a71e1972dfe6fead5bf460094658/tensorflow/core/kernels/transpose_functor_cpu.cc#L52),
```C++
        i_idx += ratio * in_strides[perm[i]];
```
as there is no guard which checks validity of `perm`, when its value is -1 `in_strides[perm[i]]` can be an out-of-bounds reading(I guess -1 would be interpreted as an `SIZE_T_MAX` or something).
Note that the below code ends up with absl assertion failure in debug build.

### Standalone code to reproduce the issue

```Python
import tensorflow as tf

tf.raw_ops.ConjugateTranspose(
    x=tf.random.normal([2]),
    perm=[-1])
```


### Relevant log output

Release Build:
```shell
    Outputs nothing
```

Debug Build:
```shell
python: external/com_google_absl/absl/container/inlined_vector.h:363: auto absl::InlinedVector<long, 8>::operator[](size_type)::(anonymous class)::operator()() const [T = long, N = 8, A = std::allocator<long>]: Assertion `false && ""i < size()""' failed.
Aborted (core dumped)
```
",Security Vulnerability,Security Vulnerability
"deleted files don't go to trash 
Type: <b>Bug</b>

I delete files in the explorer using `del` key of keyboard, files are deleted and cannot be found in the trash bin

VS Code version: Code 1.95.2 (e8653663e8840adaf45af01eab5c627a5af81807, 2024-11-07T11:07:22.054Z)
OS version: Linux x64 6.8.0-48-generic snap
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-1365U (12 x 3700)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 1|
|Memory (System)|31.00GB (22.61GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 8447ec3e-1827-4960-bb59-12316efae9e7|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|wayland|
</details><details><summary>Extensions (14)</summary>

Extension|Author (truncated)|Version
---|---|---
gitlens|eam|15.6.3
ftp-simple|hum|0.7.6
git-graph|mhu|1.30.0
autopep8|ms-|2024.0.0
debugpy|ms-|2024.12.0
python|ms-|2024.18.1
vscode-pylance|ms-|2024.11.1
jupyter|ms-|2024.10.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.21
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.388.0
LiveServer|rit|5.7.9


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
pythongtdpath:30769146
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
bdiig495:31013172
dvdeprecation:31068756
dwnewjupyter:31046869
impr_priority:31102340
nativerepl2:31139839
refactort:31108082
pythonrstrctxt:31112756
cf971741:31144450
iacca1:31171482
notype1cf:31157160
5fd0e150:31155592
dwcopilot:31170013
j44ff735:31177056

```

</details>

<!-- generated by issue reporter -->",Security Vulnerability,Security Vulnerability
"PERF predict_proba is slow when n_jobs > 1 for random forests The use of `n_jobs > 1` for small batch can slow down the prediction for forest models. This is probably due to the overhead incurred by using `joblib` (create thread, check system info, ...) which is dominate the runtime compared to the computations. This was reported originally in joblib/joblib#982.

A couple of ideas to solve this:
- Set `n_jobs=1` when the size of the batch is small.
- Introduce a `n_jobs_predict` parameters that would default to `1/n_jobs` for forests but that can be set separately.

",Performance Issue,Performance Issue
"Denial of Service in tf.raw_ops.Unstage ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?

Hi,
When using `tf.raw_ops.Unstage`, a `Denial of Service` was encountered under normal invocation, with all passed parameters meeting the requirements of the API documentation.
After a long wait, there was still no response.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tensor = tf.constant([1.0, 2.0, 3.0])
stage_op = tf.raw_ops.Stage(values=[tensor])
print(tensor.dtype)  # <dtype: 'float32'>

unstage_op = tf.raw_ops.Unstage(dtypes=[tensor.dtype])  # Hang

with tf.compat.v1.Session() as sess:
    print(""here"")
    sess.run(stage_op)
    result = sess.run(unstage_op)

print(result)  # Expected output: [1. 2. 3.]
```


### Relevant log output

```shell
2024-04-22 02:23:12.166249: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-22 02:23:12.166825: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-22 02:23:12.169923: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-22 02:23:12.215234: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-22 02:23:13.066132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-22 02:23:13.789282: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
<dtype: 'float32'>
```
",Security Vulnerability,Security Vulnerability
"[DevTools Bug] Cannot remove node ""92"" because no matching node was found in the Store. ### Website or app

http://localhost:3000/

### Repro steps

I was trying to inspect my component tree via react dev tools, but each time I make an interaction on the website which changes the state, it throws an error 

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

6.0.1-c7c68ef842

### Error message (automated)

Cannot remove node ""92"" because no matching node was found in the Store.

### Error call stack (automated)

```text
at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173889
    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)
    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390
    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
",Syntax Error,Syntax Error
"`Check failed` in `f.raw_ops.CropAndResizeGradBoxes` when `boxes` and `box_indices` are empty.  ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`Check failed` in `f.raw_ops.CropAndResizeGradBoxes` when `boxes` and `box_indices` are empty, which causes the program to crash. See the colab link below for details.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1jt4YeBZkcM_o_tqxDsIKePHoeFZ-CdCS?usp=sharing
```


### Relevant log output

```shell
Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions
```
",Runtime Error,Runtime Error
"Compilation fails on Ubuntu 20.04 when using TensorRT 8.  **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.4.1, 2.5, etc
- Python version: 3.8
- Installed using virtualenv? pip? conda?: no, built from source
- Bazel version (if compiling from source): 3.1 (for TF 2.4.1), 3.7.2 (for TF 2.5.0-rcx)
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- CUDA/cuDNN version: Cuda 11.1, cudnn8 (8.0.5.39-1+cuda11.1) or Cuda-11-2, libcudnn 8.1.1, 8.2, 
- GPU model and memory: GTX-1080ti
- TensorRT (crucial): 8.0.0-1+cuda11.0, or 8.0.0-1+cuda11.3

**Describe the problem**
When compiling with support for TensorRT 8 (via libnvinfer8), compilation fails (log is below). 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
When configuring the build, make sure you build with TensorRT support, and make sure TensorRT version 8 is selected. Build TF as usual. Compilation will fail. 

If you install  TensorRT version 7 manually (from debs available for Ubuntu 18.04), compilation will complete just fine.

**Any other info / logs**
Relevant error: 
`C++ compilation of rule '//tensorflow/compiler/tf2tensorrt:tensorrt_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command`

`In file included from bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:54,
                 from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:
bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInferRuntime.h:2264:51: note: from previous declaration 'nvinfer1::IPluginRegistry* getPluginRegistry() noexcept'
 2264 | extern ""C"" TENSORRTAPI nvinfer1::IPluginRegistry* getPluginRegistry() noexcept;`

Full log here: 
[gesdm-tf2.5.0rc3-error.txt](https://github.com/tensorflow/tensorflow/files/6469944/gesdm-tf2.5.0rc3-error.txt)

",Runtime Error,Runtime Error
"[DevTools Bug]:  [DevTools Bug] Cannot reorder children for node ""0"" because no matching node was found in the Store. ### Website or app

https://www.autotrack.nl/aanbod

### Repro steps

1. Start profiling in chrome
2. Click on any filter
3. Uncaught Error: Cannot reorder children for node ""0"" because no matching node was found in the Store.
Dismiss
The error was thrown at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1175534
    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1140783)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1142390
    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1552529)

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

6.0.1-c7c68ef842

### Error message (automated)

Cannot reorder children for node ""0"" because no matching node was found in the Store.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1175534
    at v.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1140783)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1142390
    at bridgeListener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1552529)
```

### Error component stack (automated)

```text

```

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot reorder children for node  because no matching node was found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```",Syntax Error,Syntax Error
"Linux: writing elevated fails with high `ulimit` values <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.96.2 (code_1.96.2-1734607745_amd64.deb)
- OS Version: Ubuntu 24.10

Steps to Reproduce:

1. Open a file that is not writable by the user
2. Edit the file
3. Try to save it
4. Error pops up stating that not enough permissions
5. Retry as Sudo option appears
6. Enter password
7. Saving process running forever

Same problem has been discussed here: https://github.com/microsoft/vscode/issues/234311. 
Is there any progress or plan? Or any workaround ?",Security Vulnerability,Security Vulnerability
"inconsistent .proto file package names break gRPC message/field parsing in Wireshark <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

As noted in #12445, there is inconsistency among the package names in the TensorFlow `.proto` files. Searching for `.proto` file package declarations within the codebase reveals a wide variety of package names, including `tensorflow.dummy`.
https://github.com/search?q=repo%3Atensorflow%2Ftensorflow+%22package+tensorflow%22&type=code&p=2

This has a problematic effect when trying to parse the Protobuf fields in TensorFlow gRPC messages within the [Wireshark](https://www.wireshark.org/) network capturing tool. In Wireshark, the built-in parsing functionality requires the package/service names within the `.proto` files to match the package/service names in the captured gRPC messages, so currently, [CoordinationService](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tsl/protobuf/coordination_service.proto) (`package tensorflow`) messages parse properly, while message types and field names in [WorkerService](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker_service.proto) (`package tensorflow.grpc`) messages cannot be parsed, and appear as _unknown_.

Current workaround: using a script to replace all instances of `""tensorflow.grpc""` with `""tensorflow""` in the `.proto` files.

### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tensorflow/blob/d0863698de84277282df6f2865795aaa1e22ace5/tensorflow/tsl/protobuf/coordination_service.proto#L3

https://github.com/tensorflow/tensorflow/blob/d0863698de84277282df6f2865795aaa1e22ace5/tensorflow/core/protobuf/worker_service.proto#L18
```


### Relevant log output

_No response_</details>",Dependency Issue,Dependency Issue
"Memory leak when using MultiWorkerMirroredStrategy for distributed training **System information**

- Have I written custom code: YES
- OS Platform and Distribution: CentOS 7.3
- TensorFlow installed from: pip
- TensorFlow version: 2.3.0
- Python version:3.7.7
- CPU ONLY

**Describe the current behavior**

When I use `MultiWorkerMirroredStrategy` for distributed training, as the number of training epochs increases, memory usage of tensorflow is also increasing, until beyond the memory limitation.

But the memory usage of stand-alone(not distributed) training is always stable.

Because I use cpu only for distributed training, I can't get any memory infomation from tensorboard using profiler.

**Standalone code to reproduce the issue**

Note that I don't know how to use `MultiWorkerMirroredStrategy` in `colab`, so I just give the reproduce steps here, and it's very easy.

1. Training Code (worker.py)

```python
import os
import json
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from absl import app, flags
import numpy as np

FLAGS = flags.FLAGS
flags.DEFINE_string(""logs"", ""logs"", ""logs dir"")
flags.DEFINE_integer(""index"", 0, ""worker index"")

class ThreeLayerMLP(keras.Model):
    def __init__(self, name=None):
        super().__init__(name=name)
        self.dense_1 = layers.Dense(32, activation='relu', name='dense_1')
        self.dense_2 = layers.Dense(16, activation='relu', name='dense_2')
        self.pred_layer = layers.Dense(
            1,
            activation='sigmoid',
            name='predictions',
        )

    def call(self, inputs, training=None):
        print(inputs.shape)
        x = self.dense_1(inputs)
        x = self.dense_2(x)
        return self.pred_layer(x)


def prepare_data():
    np.random.seed(0)
    x_train, y_train = (
        np.random.random((6000000, 31)),
        np.random.randint(2, size=(6000000, 1)),
    )

    x_val, y_val = (
        np.random.random((10000, 31)),
        np.random.randint(2, size=(10000, 1)),
    )

    return ((x_train, y_train), (x_val, y_val))


def main(argv):
    del argv  # Unused args
    tf_config = {
        ""cluster"": {
            ""worker"": [""ip1:12345"", ""ip2:12345""],
        },
        ""task"": {
            ""index"": FLAGS.index,
            ""type"": ""worker""
        }
    }
    os.environ[""TF_CONFIG""] = json.dumps(tf_config)
    print(json.loads(os.environ[""TF_CONFIG""]))
    # distributed strategy
    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    BATCH_SIZE_PER_REPLICA = 128
    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
    print('Number of devices: %d' % strategy.num_replicas_in_sync)

    with strategy.scope():
        model = ThreeLayerMLP(name='3_layer_mlp')
        model.compile(
            loss=tf.keras.losses.BinaryCrossentropy(),
            optimizer=keras.optimizers.RMSprop(),
            metrics=[""AUC""],
        )

    tensorboard_callback = tf.keras.callbacks.TensorBoard(
        log_dir=FLAGS.logs,
        histogram_freq=10,
        update_freq=100,
    )

    ((x_train, y_train), (x_val, y_val)) = prepare_data()

    model.fit(
        x_train,
        y_train,
        epochs=100,
        batch_size=BATCH_SIZE,
        validation_data=(x_val, y_val),
        callbacks=[tensorboard_callback],
    )


if __name__ == '__main__':
    app.run(main)
```

2. Distributed training: change the `ip1` and `ip2` to your machine's ip in the codes above, and execute the command below seperately:

```shell
python worker.py --index=0
python worker.py --index=1
```

3. The memory change curve of distributed training in my machine is shown as belowï¼
![image](https://user-images.githubusercontent.com/15494997/91021608-94ee1c80-e626-11ea-9adc-c775b3ff575a.png)

4. The memory usage of stand-alone training is only 3-4G.",Performance Issue,Performance Issue
"`FunctionTransformer` need `feature_names_out` even if `func` returns DataFrame ### Describe the bug

Trying to call `transform` for `FunctionTransformer` for which `feature_names_out` is configured raises error that advises to use `set_output(transform='pandas')`. But this doesn't change anything.

### Steps/Code to Reproduce

```py
import numpy as np
import pandas as pd
from sklearn.preprocessing import FunctionTransformer

my_transformer = FunctionTransformer(
    lambda X : pd.concat(
        [
            X[col].rename(f""{col} {str(power)}"")**power
            for col in X
            for power in range(2,4)
        ],
        axis=1
    ),
    feature_names_out = (
        lambda transformer, input_features: [
            f""{feature} {power_str}""
            for feature in input_features
            for power_str in [""square"", ""cubic""]
        ]
    )
)
# I specified transform=pandas
my_transformer.set_output(transform='pandas')
sample_size = 10
X = pd.DataFrame({
    ""feature 1"" : [1,2,3,4,5],
    ""feature 2"" : [3,4,5,6,7]
})
my_transformer.fit(X)
my_transformer.transform(X)
```

### Expected Results

`pandas.DataFrame` like following

|    |   feature 1 square |   feature 1 cubic |   feature 2 square |   feature 2 cubic |
|---:|-------------------:|------------------:|-------------------:|------------------:|
|  0 |                  1 |                 1 |                  9 |                27 |
|  1 |                  4 |                 8 |                 16 |                64 |
|  2 |                  9 |                27 |                 25 |               125 |
|  3 |                 16 |                84 |                 36 |               216 |
|  4 |                 25 |               125 |                 49 |               343 |

### Actual Results

```
ValueError: The output generated by `func` have different column names than the ones provided by `get_feature_names_out`. Got output with columns names: ['feature 1 2', 'feature 1 3', 'feature 2 2', 'feature 2 3'] and `get_feature_names_out` returned: ['feature 1 square', 'feature 1 cubic', 'feature 2 square', 'feature 2 cubic']. The column names can be overridden by setting `set_output(transform='pandas')` or `set_output(transform='polars')` such that the column names are set to the names provided by `get_feature_names_out`.
```

### Versions

```shell
System:
    python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]
executable: /usr/bin/python3
   machine: Linux-6.5.0-14-generic-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.4.1.post1
          pip: 24.0
   setuptools: 68.2.2
        numpy: 1.24.2
        scipy: 1.11.1
       Cython: None
       pandas: 2.2.1
   matplotlib: 3.7.1
       joblib: 1.3.1
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/fedor/.local/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
        version: 0.3.21
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12

       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/fedor/.local/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
    num_threads: 12

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/fedor/.local/lib/python3.10/site-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12
```
```
",Syntax Error,Syntax Error
"[DevTools Bug]: Components tab freezes after inspecting ### Website or app

https://dev.permaplant.net

### Repro steps

1. Login
2. Go to Maps and create or open one
3. Look for the TimelinePicker component in the components tab
4. Click on it to inspect it

After that my components tab freezes and sometimes my RAM fills up endlessly. 

(If you need authentication, just contact me)

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Syntax Error,Syntax Error
"`tf.raw_ops.ExtractImagePatches`: Assertion failure in shape inference step ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.ExtractImagePatches` can lead to assertion failure in shape inference step.
Error location is [here](https://github.com/tensorflow/tensorflow/blob/f5daeae21404e8d672c785cc0c4c469ddc1b1a8a/tensorflow/core/ops/array_ops.cc#L2686-L2687):
```C++
      TF_RETURN_IF_ERROR(c->Multiply(
          c->Dim(input_shape, 3), ksize_rows * ksize_cols, &output_depth_dim));
```
Because it does not check validity of `ksize_rows * ksize_cols`, the negative value of it is fed to [`Multiply`](https://github.com/tensorflow/tensorflow/blob/83f1804f3427ae888e62b26b5bcba8afc9e24ef7/tensorflow/core/framework/shape_inference.cc#L1096C1-L1098C56):
```C++
Status InferenceContext::Multiply(DimensionHandle first,
                                  DimensionOrConstant second,
                                  DimensionHandle* out)
```
And it ends up with assertion failure at [a constructor of `DimensionOrConstant`](https://github.com/tensorflow/tensorflow/blob/daea84aad67d1fcc8d58c648e1da58ee576445f9/tensorflow/core/framework/shape_inference.h#L891-L896).

### Standalone code to reproduce the issue

```Python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

tf.raw_ops.ExtractImagePatches(
    images=tf.random.normal([1,1,1,1]),
    ksizes=[1,-1,2,1],
    strides=[1,1,1,1],
    rates=[1,1,1,1],
    padding=""VALID"",
    name=None
)
```


### Relevant log output

Release build: Outputs nothing

Debug build:
```shell
2024-02-27 20:05:58.701202: F ./tensorflow/core/framework/shape_inference.h:891] Check failed: val >= 0 || val == InferenceContext::kUnknownDim Dimension must be non-negative or equal to InferenceContext::kUnknownDim but got -2
Aborted (core dumped)
```
",Logical Bug,Logical Bug
"C++ API `SparseFillEmptyRows` can lead to `std::length_error` ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

C++ API `SparseFillEmptyRows` can lead to `std::length_error`.
Error location is [here](https://github.com/tensorflow/tensorflow/blob/9da417e3f63215efe995b83e7b9f9b34115a424e/tensorflow/core/kernels/fill_empty_rows_functor.h#L114C36-L114C46):
```C++
    std::vector<Tindex> csr_offset(dense_rows, 0);
```
Because lack of checking negativity, negative value of `dense_rows` is fed to the constructor of `std::vector`. By integer underflow, the value is converted to extremely large number and it ends up with `std::length_error`.

### Standalone code to reproduce the issue

```C++
#include ""tensorflow/cc/framework/scope.h""
#include ""tensorflow/core/graph/graph.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/cc/ops/array_ops.h""
#include ""tensorflow/cc/ops/standard_ops.h""

using namespace tensorflow;

int main() {
  SessionOptions options;
  std::unique_ptr<tensorflow::Session>
    session(tensorflow::NewSession(options));
  Scope scope = Scope::NewRootScope();

  Input indices = {{1l}};
  Input values = {1};
  Input dense_shape = {-1l};
  Input default_value = 1;
  auto target = ops::SparseFillEmptyRows(scope.WithOpName(""target""), indices, values, dense_shape, default_value);

  Status status;
  GraphDef graph_def;
  status = scope.ToGraphDef(&graph_def);
  if (!status.ok()) {
    LOG(WARNING) << ""Could not build graph: "" << status.message();
  }

  status = session->Create(graph_def);
  if (!status.ok()) {
    LOG(WARNING) << ""Could not create session: "" << status.message();
  }

  std::vector<Tensor> outputs;
  status = session->Run({}, {""target""}, {""target""}, &outputs);
  if (!status.ok()) {
    LOG(WARNING) << ""Could not run session: "" << status.message();
  }

  return 0;
}
```


### Relevant log output

```shell
terminate called after throwing an instance of 'std::length_error'
  what():  cannot create std::vector larger than max_size()
Aborted (core dumped)
```
",Logical Bug,Runtime Error
"[DevTools Bug]: Source map error: URL: react_devtools_backend_compact.js.map ### Website or app

https://github.com/avenida714/alec-synth/blob/dccf984d89ae44558c70ff93dfa03b1227d5df5b/src/App.jsx#L63-L64C17

Link found on the vite getting started page: https://stackblitz.com/edit/vitejs-vite-zff6zx?file=src%2FApp.jsx&terminal=dev

### Repro steps

â ï¸ This is not my code (not public) â ï¸. While I searched for this error I stumble upon this repo. I had this issue when adding the plugin for first time.

1. Open the website
2. Open console
3. See the error

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Dependency Issue,Dependency Issue
"[DevTools Bug] Cannot add child ""301"" to parent ""155"" because parent node was not found in the Store. ### Website or app

www.github.com

### Repro steps

The error was thrown at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173126
    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)
    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390
    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

6.0.1-c7c68ef842

### Error message (automated)

Cannot add child ""301"" to parent ""155"" because parent node was not found in the Store.

### Error call stack (automated)

```text
at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173126
    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)
    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390
    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)
```

### Error component stack (automated)

```text

```

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add child  to parent  because parent node was not found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```",Syntax Error,Syntax Error
"`tf.raw_ops.ArgMax`: Heap buffer overflow ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.ArgMax` can lead to heap buffer overflow.
[Error location](https://github.com/tensorflow/tensorflow/blob/774b0c3e97b5ef60bfc9c54961347dd0bc3660a8/tensorflow/core/kernels/argmax_op.cc#L59):
```C++
    const int32_t dim = internal::SubtleMustCopy(dimension.scalar<int32>()());
```
It copies scalar value of `dimension` without checking exact type. Therefore when `dimension` is an `int16` it reads over the bound.
Note that `int16` is an allowed type for `dimension` according to [opdef](https://github.com/tensorflow/tensorflow/blob/41b93a8b310086f69aab6b6369d2af9d5178881d/tensorflow/core/ops/math_ops.cc#L1153-L1160):
```C++
REGISTER_OP(""ArgMax"")
    .Input(""input: T"")
    .Input(""dimension: Tidx"")
    .Output(""output: output_type"")
    .Attr(""T: {realnumbertype, quantizedtype, bool}"")
    .Attr(""Tidx: {int16, int32, int64} = DT_INT32"")
    .Attr(""output_type: {int16, uint16, int32, int64} = DT_INT64"")
    .SetShapeFn(ArgOpShape);
```

### Standalone code to reproduce the issue

```python
import tensorflow as tf

tf.raw_ops.ArgMax(
    input=tf.random.normal([1,1,1,1]),
    dimension=tf.constant(1,shape=[],dtype=tf.int16),
    output_type=tf.dtypes.int64,
    name=None
)
```


### Relevant log output

The below log needs ASAN build.
```shell
=================================================================
==4008222==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x609000000400 at pc 0x7fa6a0dca809 bp 0x7ffe63b29d90 sp 0x7ffe63b29d88
READ of size 4 at 0x609000000400 thread T0
    #0 0x7fa6a0dca808 in int const tensorflow::internal::SubtleMustCopy<int>(int const&) /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10
    #1 0x7fa6a0dca808 in tensorflow::ArgOp<Eigen::ThreadPoolDevice, float, long, tensorflow::functor::ArgMax<Eigen::ThreadPoolDevice, float, long>>::Compute(tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/kernels/argmax_op.cc:59:25
    #2 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #3 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #4 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #5 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #6 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #7 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #8 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #9 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #10 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #11 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #12 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #13 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #14 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #15 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #16 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #17 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #18 0x7fa6b989418d in TFE_Py_FastPathExecute_C(_object*) /proc/self/cwd/tensorflow/python/eager/pywrap_tfe_src.cc:3979:3
    #19 0x7fa667d2683e in pybind11_init__pywrap_tfe(pybind11::module_&)::$_60::operator()(pybind11::args) const /proc/self/cwd/tensorflow/python/tfe_wrapper.cc:1276:35
    #20 0x7fa667d2683e in pybind11::object pybind11::detail::argument_loader<pybind11::args>::call_impl<pybind11::object, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, 0ul, pybind11::detail::void_type>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, std::integer_sequence<unsigned long, 0ul>, pybind11::detail::void_type&&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1443:16
    #21 0x7fa667d2683e in std::enable_if<!std::is_void<pybind11::object>::value, pybind11::object>::type pybind11::detail::argument_loader<pybind11::args>::call<pybind11::object, pybind11::detail::void_type, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1411:42
    #22 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:248:69
    #23 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:223:21
    #24 0x7fa667d67a59 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:939:30
    #25 0x528186 in cfunction_call /usr/local/src/conda/python-3.11.7/Objects/methodobject.c:542:18
    #26 0x503a0b in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18
    #27 0x510f32 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:4769:23
    #28 0x538732 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #29 0x538732 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #30 0x538732 in _PyFunction_Vectorcall /usr/local/src/conda/python-3.11.7/Objects/call.c:393:16
    #31 0x5426bb in _PyVectorcall_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:257:24
    #32 0x5426bb in _PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:328:16
    #33 0x5426bb in PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:355:12
    #34 0x514ff0 in do_call_core /usr/local/src/conda/python-3.11.7/Python/ceval.c:7352:12
    #35 0x514ff0 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:5376:22
    #36 0x5cb559 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #37 0x5cb559 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #38 0x5cac2e in PyEval_EvalCode /usr/local/src/conda/python-3.11.7/Python/ceval.c:1148:21
    #39 0x5ebcf6 in run_eval_code_obj /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1710:9
    #40 0x5e788f in run_mod /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1731:19
    #41 0x5fc831 in pyrun_file /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1626:15
    #42 0x5fbbfe in _PyRun_SimpleFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:440:13
    #43 0x5fb922 in _PyRun_AnyFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:79:15
    #44 0x5f65cd in pymain_run_file_obj /usr/local/src/conda/python-3.11.7/Modules/main.c:360:15
    #45 0x5f65cd in pymain_run_file /usr/local/src/conda/python-3.11.7/Modules/main.c:379:15
    #46 0x5f65cd in pymain_run_python /usr/local/src/conda/python-3.11.7/Modules/main.c:601:21
    #47 0x5f65cd in Py_RunMain /usr/local/src/conda/python-3.11.7/Modules/main.c:680:5
    #48 0x5bb3d8 in Py_BytesMain /usr/local/src/conda/python-3.11.7/Modules/main.c:734:12
    #49 0x7fa791408d8f in __libc_start_call_main csu/../sysdeps/nptl/libc_start_call_main.h:58:16
    #50 0x7fa791408e3f in __libc_start_main csu/../csu/libc-start.c:392:3
    #51 0x5bb222 in _start (/home/loft/anaconda3/envs/tf-latest-asan/bin/python3.11+0x5bb222)

0x609000000402 is located 0 bytes after 2-byte region [0x609000000400,0x609000000402)
allocated by thread T0 here:
    #0 0x7fa7917ed617 in __interceptor_posix_memalign /home/runner/work/llvm-project/llvm-project/final/llvm-project/compiler-rt/lib/asan/asan_malloc_linux.cpp:145:3
    #1 0x7fa6be4f6902 in tsl::port::AlignedMalloc(unsigned long, int) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4a18902)
    #2 0x7fa6bc2d3bcc in tsl::(anonymous namespace)::CPUAllocator::AllocateRaw(unsigned long, unsigned long) cpu_allocator_impl.cc
    #3 0x7fa6bc4883bc in short* tensorflow::TypedAllocator::Allocate<short>(tsl::Allocator*, unsigned long, tsl::AllocationAttributes const&) /proc/self/cwd/./tensorflow/core/framework/typed_allocator.h:47:24
    #4 0x7fa6bc4883bc in tensorflow::(anonymous namespace)::Buffer<short>::Buffer(tsl::Allocator*, long, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:574:21
    #5 0x7fa6bc4883bc in tensorflow::Tensor::Tensor(tsl::Allocator*, tensorflow::DataType, tensorflow::TensorShape const&, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:986:5
    #6 0x7fa6bbef871d in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:764:10
    #7 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes) /proc/self/cwd/./tensorflow/core/framework/op_kernel.h:1270:12
    #8 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**, tsl::AllocatorAttributes) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:822:14
    #9 0x7fa6bbef4368 in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:728:10
    #10 0x7fa6a9bf0207 in tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e9fe207)
    #11 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #12 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #13 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #14 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #15 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #16 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #17 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #18 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #19 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #20 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #21 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #22 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #23 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #24 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #25 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #26 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #27 0x7fa6b985dcbf in tensorflow::EagerCast(TFE_Context*, TFE_TensorHandle*, TF_DataType, TF_DataType, TSL_Status*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:259:3
    #28 0x7fa6b985e67b in tensorflow::ConvertToEagerTensorUncached(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:317:11
    #29 0x7fa6b985f8a1 in tensorflow::ConvertToEagerTensor(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:405:14
    #30 0x7fa6b9860088 in EagerTensor_init /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:529:18
    #31 0x5039d2 in type_call /usr/local/src/conda/python-3.11.7/Objects/typeobject.c:1103:19
    #32 0x5039d2 in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18

SUMMARY: AddressSanitizer: heap-buffer-overflow /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10 in int const tensorflow::internal::SubtleMustCopy<int>(int const&)
Shadow bytes around the buggy address:
  0x609000000180: fa fa fa fa fa fa fa fa fd fa fa fa fa fa fa fa
  0x609000000200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000280: fd fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000300: 04 fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000380: fd fd fd fd fa fa fa fa fa fa fa fa fa fa fa fa
=>0x609000000400:[02]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000480: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000500: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000580: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000600: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000680: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
==4008222==ABORTING
```
",Security Vulnerability,Security Vulnerability
"Denial of Service in tf.raw_ops.Unstage ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?

Hi,
When using `tf.raw_ops.Unstage`, a `Denial of Service` was encountered under normal invocation, with all passed parameters meeting the requirements of the API documentation.
After a long wait, there was still no response.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tensor = tf.constant([1.0, 2.0, 3.0])
stage_op = tf.raw_ops.Stage(values=[tensor])
print(tensor.dtype)  # <dtype: 'float32'>

unstage_op = tf.raw_ops.Unstage(dtypes=[tensor.dtype])  # Hang

with tf.compat.v1.Session() as sess:
    print(""here"")
    sess.run(stage_op)
    result = sess.run(unstage_op)

print(result)  # Expected output: [1. 2. 3.]
```


### Relevant log output

```shell
2024-04-22 02:23:12.166249: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-22 02:23:12.166825: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-22 02:23:12.169923: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-22 02:23:12.215234: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-22 02:23:13.066132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-22 02:23:13.789282: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
<dtype: 'float32'>
```
",Security Vulnerability,Security Vulnerability
"Terminal Cursor is at the wrong place with Python3.13 
The terminal cursor seems to consistently be at wrong (further in the line) place, when user tries to use python3.13 inside the terminal:
![Image](https://github.com/user-attachments/assets/59081356-6531-4eac-b4a7-f73794ccf428)

I think it is VS Code thing, since the cursor seems ""normal"" for external terminal and Pycharm. 
 ![Image](https://github.com/user-attachments/assets/ad757361-94c9-4113-b771-fed95f4b3333)

The only Python prompt specific code that I recall is:
https://github.com/microsoft/vscode/blob/0561ca03c895239131a526d8731da0e53027167c/src/vs/platform/terminal/common/capabilities/commandDetectionCapability.ts#L961 but wondering if this is the right place to look at.

",UI/UX Bug,UI/UX Bug
"[Compiler Bug]: ""Mutating a value returned from 'useContext()', which should not be mutated"" When the value is a Ref ### What kind of issue is this?

- [X] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)
- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)
- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)
- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)

### Link to repro

https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAjggDswCBZATwGETcMCBeQmBAQ3tuPvVwApgAHWKCebALaYANggBKCAGbJ8wOLFbdlxKFKkBfYXoCUw4Rmx58AE0VsdBBVGJxcASxL4ACmw25akkgRuPiMVYXxCEjJ8DAlpOUV8ZigwBIU+AHIAOQgMkxFiCNZcWEK+cIj8AB4APgrK6qouHlwAOk8YCAA3VxsYfC62KSgERmBgWMkZeQU9PTrChsqq6gALVykrf2xiIIIAegWl6v2mugZ2zp6+o+XDiqMAbkNTYkdnNw81ja2IAN3gqEhIsiKQCMAYug4tNEnokvgUghmgw+GduAx8hVEQBRBQKBAuPghJI1MKLCKTeIzVpqGC+eEZACaCDAGQqegANPgANqUmEKAC6+QiFWKpWqVlcXXuxAMxBAeiAA

### Repro steps

1. Instantiate a Context
2. In Parent, Instantiate a ref
3. Wrap the child in the Context's Provider, and pass the ref as a value
4. Get the ref in the child, and mutate it in a useEffect

### How often does this bug happen?

Every time

### What version of React are you using?

18.3.1

### What version of React Compiler are you using?

19.0.0-beta-63b359f-20241101",Logical Bug,Logical Bug
"Inconsistent crossbrowser onBeforeInput paste event behavior ### React version: 17.0.2
In Chrome, pasting triggers `onPaste` and `onBeforeInput`, with the `nativeEvent`s `ClipboardEvent` and `TextEvent` respectively.

In Firefox, pasting only triggers `onPaste` with `nativeEvent` `paste`.

If I'm not mistaken, this seems to be the code responsible for not triggering `onBeforeInput` on native `paste` events:
https://github.com/facebook/react/blob/27c9c95e23ddedb9163373950e364dd62038f6c0/packages/react-dom/src/events/plugins/BeforeInputEventPlugin.js#L328-L331

### React version: 17.0.2 and 18.0.0

17.0.2
https://codesandbox.io/s/condescending-cerf-e1qt9?file=/src/App.js
18.0.0
https://codesandbox.io/s/sparkling-sun-fylti?file=/src/App.js",UI/UX Bug,UI/UX Bug
"Devcontainers ignores terminal profile settings from settings.json when opening local integrated terminal <!-- Please search existing issues to avoid creating duplicates, and review our troubleshooting tips: https://code.visualstudio.com/docs/remote/troubleshooting -->
<!-- Please attach logs to help us diagnose your issue. Learn more here: https://code.visualstudio.com/docs/remote/troubleshooting#_reporting-issues and here: https://code.visualstudio.com/docs/remote/vscode-server#_where-can-i-provide-feedback-or-report-an-issue -->
<!-- Also please test using the latest insiders build to make sure your issue has not already been fixed: https://code.visualstudio.com/insiders/ -->

- VSCode Version: 1.94.2
- Local OS Version: Flatpak Freedesktop SDK runtime 24.08
- Remote OS Version: not relevant, we're talking about a local terminal outside of the container
- Remote Extension/Connection Type: Containers

Steps to Reproduce:

1. Add some configuration to the project's `.vscode/settings.json` that touches the terminal. In my case the following to launch a shell on my host outside of the Flatpak sandbox or container:
```json
{
    ""terminal.integrated.defaultProfile.linux"": ""bash"",
    ""terminal.integrated.profiles.linux"": {
        ""bash"": {
            ""path"": ""/app/bin/host-spawn"",
            ""icon"": ""terminal-bash"",
            ""args"": [""fish""],
            ""overrideName"": true
        }
    }
}
```
2. Press Ctrl + Shift + P and type `Terminal: Create New Integrated Terminal (Local)` to open a local terminal outside of the container

In my case this _should_ open the Fish shell on my host, and if I run the project outside of devcontainers this works as expected. However when the project is opened within a container and I open a local integrated terminal this is completely ignored and I get a `/bin/sh` shell (which is the default within Flatpak). It also suddenly complains about ""bash"" not being a valid option for `terminal.integrated.defaultProfile.linux`.

Yes I am running through Flatpak to make things complicated but I don't see how this has anything to do with that environment, this would just as well not work outside of it.

<!-- Check to see if the problem is general, with a specific extension, or only happens when remote -->
Does this issue occur when you try this locally?: No
Does this issue occur when you try this locally and all extensions are disabled?: No
",Logical Bug,Logical Bug
"Denial of Service in tf.raw_ops.Unstage ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?

Hi,
When using `tf.raw_ops.Unstage`, a `Denial of Service` was encountered under normal invocation, with all passed parameters meeting the requirements of the API documentation.
After a long wait, there was still no response.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tensor = tf.constant([1.0, 2.0, 3.0])
stage_op = tf.raw_ops.Stage(values=[tensor])
print(tensor.dtype)  # <dtype: 'float32'>

unstage_op = tf.raw_ops.Unstage(dtypes=[tensor.dtype])  # Hang

with tf.compat.v1.Session() as sess:
    print(""here"")
    sess.run(stage_op)
    result = sess.run(unstage_op)

print(result)  # Expected output: [1. 2. 3.]
```


### Relevant log output

```shell
2024-04-22 02:23:12.166249: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-22 02:23:12.166825: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-22 02:23:12.169923: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-22 02:23:12.215234: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-22 02:23:13.066132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-22 02:23:13.789282: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
<dtype: 'float32'>
```
",Security Vulnerability,Security Vulnerability
"BUG: StandardScaler partial_fit overflows The recent implementation of `partial_fit` for `StandardScaler` can overflow. A use case there is to transform indefinitely long stream of data, but that is problematic with the current implementation. The reason is that to compute the running mean, [we keep track](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py#L788) of the sample sum.

Here the code to reproduce the behavior. To simulate long stream of data would take long time; instead, I use samples with very large norm but the effect is the same. The same batch is presented to the transformer many times. The mean should be same.

```python
from sklearn.preprocessing import StandardScaler
import numpy as np

rng = np.random.RandomState(0)

def gen_1d_uniform_batch(min_, max_, n):
    return rng.uniform(min_, max_, size=(n, 1))

max_f = np.finfo(np.float64).max / 1e5
min_f = max_f / 1e2
stream_dim = 100
batch_dim = 500000
print(""mean overflow: batch vs online on %d repetitions"" % stream_dim)

X = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)

scaler = StandardScaler(with_std=False).fit(X)
print(scaler.mean_)
[  1.79769313e+301]

iscaler = StandardScaler(with_std=False)
batch = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)
for _ in range(stream_dim):
    iscaler = iscaler.partial_fit(batch)
RuntimeWarning: overflow encountered in add
  updated_mean = (last_sum + new_sum) / updated_sample_count

print(iscaler.mean_)
[ inf]
```
",Security Vulnerability,Security Vulnerability
"`tf.raw_ops.ArgMax`: Heap buffer overflow ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.ArgMax` can lead to heap buffer overflow.
[Error location](https://github.com/tensorflow/tensorflow/blob/774b0c3e97b5ef60bfc9c54961347dd0bc3660a8/tensorflow/core/kernels/argmax_op.cc#L59):
```C++
    const int32_t dim = internal::SubtleMustCopy(dimension.scalar<int32>()());
```
It copies scalar value of `dimension` without checking exact type. Therefore when `dimension` is an `int16` it reads over the bound.
Note that `int16` is an allowed type for `dimension` according to [opdef](https://github.com/tensorflow/tensorflow/blob/41b93a8b310086f69aab6b6369d2af9d5178881d/tensorflow/core/ops/math_ops.cc#L1153-L1160):
```C++
REGISTER_OP(""ArgMax"")
    .Input(""input: T"")
    .Input(""dimension: Tidx"")
    .Output(""output: output_type"")
    .Attr(""T: {realnumbertype, quantizedtype, bool}"")
    .Attr(""Tidx: {int16, int32, int64} = DT_INT32"")
    .Attr(""output_type: {int16, uint16, int32, int64} = DT_INT64"")
    .SetShapeFn(ArgOpShape);
```

### Standalone code to reproduce the issue

```python
import tensorflow as tf

tf.raw_ops.ArgMax(
    input=tf.random.normal([1,1,1,1]),
    dimension=tf.constant(1,shape=[],dtype=tf.int16),
    output_type=tf.dtypes.int64,
    name=None
)
```


### Relevant log output

The below log needs ASAN build.
```shell
=================================================================
==4008222==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x609000000400 at pc 0x7fa6a0dca809 bp 0x7ffe63b29d90 sp 0x7ffe63b29d88
READ of size 4 at 0x609000000400 thread T0
    #0 0x7fa6a0dca808 in int const tensorflow::internal::SubtleMustCopy<int>(int const&) /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10
    #1 0x7fa6a0dca808 in tensorflow::ArgOp<Eigen::ThreadPoolDevice, float, long, tensorflow::functor::ArgMax<Eigen::ThreadPoolDevice, float, long>>::Compute(tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/kernels/argmax_op.cc:59:25
    #2 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #3 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #4 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #5 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #6 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #7 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #8 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #9 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #10 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #11 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #12 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #13 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #14 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #15 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #16 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #17 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #18 0x7fa6b989418d in TFE_Py_FastPathExecute_C(_object*) /proc/self/cwd/tensorflow/python/eager/pywrap_tfe_src.cc:3979:3
    #19 0x7fa667d2683e in pybind11_init__pywrap_tfe(pybind11::module_&)::$_60::operator()(pybind11::args) const /proc/self/cwd/tensorflow/python/tfe_wrapper.cc:1276:35
    #20 0x7fa667d2683e in pybind11::object pybind11::detail::argument_loader<pybind11::args>::call_impl<pybind11::object, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, 0ul, pybind11::detail::void_type>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, std::integer_sequence<unsigned long, 0ul>, pybind11::detail::void_type&&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1443:16
    #21 0x7fa667d2683e in std::enable_if<!std::is_void<pybind11::object>::value, pybind11::object>::type pybind11::detail::argument_loader<pybind11::args>::call<pybind11::object, pybind11::detail::void_type, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1411:42
    #22 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:248:69
    #23 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:223:21
    #24 0x7fa667d67a59 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:939:30
    #25 0x528186 in cfunction_call /usr/local/src/conda/python-3.11.7/Objects/methodobject.c:542:18
    #26 0x503a0b in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18
    #27 0x510f32 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:4769:23
    #28 0x538732 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #29 0x538732 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #30 0x538732 in _PyFunction_Vectorcall /usr/local/src/conda/python-3.11.7/Objects/call.c:393:16
    #31 0x5426bb in _PyVectorcall_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:257:24
    #32 0x5426bb in _PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:328:16
    #33 0x5426bb in PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:355:12
    #34 0x514ff0 in do_call_core /usr/local/src/conda/python-3.11.7/Python/ceval.c:7352:12
    #35 0x514ff0 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:5376:22
    #36 0x5cb559 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #37 0x5cb559 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #38 0x5cac2e in PyEval_EvalCode /usr/local/src/conda/python-3.11.7/Python/ceval.c:1148:21
    #39 0x5ebcf6 in run_eval_code_obj /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1710:9
    #40 0x5e788f in run_mod /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1731:19
    #41 0x5fc831 in pyrun_file /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1626:15
    #42 0x5fbbfe in _PyRun_SimpleFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:440:13
    #43 0x5fb922 in _PyRun_AnyFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:79:15
    #44 0x5f65cd in pymain_run_file_obj /usr/local/src/conda/python-3.11.7/Modules/main.c:360:15
    #45 0x5f65cd in pymain_run_file /usr/local/src/conda/python-3.11.7/Modules/main.c:379:15
    #46 0x5f65cd in pymain_run_python /usr/local/src/conda/python-3.11.7/Modules/main.c:601:21
    #47 0x5f65cd in Py_RunMain /usr/local/src/conda/python-3.11.7/Modules/main.c:680:5
    #48 0x5bb3d8 in Py_BytesMain /usr/local/src/conda/python-3.11.7/Modules/main.c:734:12
    #49 0x7fa791408d8f in __libc_start_call_main csu/../sysdeps/nptl/libc_start_call_main.h:58:16
    #50 0x7fa791408e3f in __libc_start_main csu/../csu/libc-start.c:392:3
    #51 0x5bb222 in _start (/home/loft/anaconda3/envs/tf-latest-asan/bin/python3.11+0x5bb222)

0x609000000402 is located 0 bytes after 2-byte region [0x609000000400,0x609000000402)
allocated by thread T0 here:
    #0 0x7fa7917ed617 in __interceptor_posix_memalign /home/runner/work/llvm-project/llvm-project/final/llvm-project/compiler-rt/lib/asan/asan_malloc_linux.cpp:145:3
    #1 0x7fa6be4f6902 in tsl::port::AlignedMalloc(unsigned long, int) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4a18902)
    #2 0x7fa6bc2d3bcc in tsl::(anonymous namespace)::CPUAllocator::AllocateRaw(unsigned long, unsigned long) cpu_allocator_impl.cc
    #3 0x7fa6bc4883bc in short* tensorflow::TypedAllocator::Allocate<short>(tsl::Allocator*, unsigned long, tsl::AllocationAttributes const&) /proc/self/cwd/./tensorflow/core/framework/typed_allocator.h:47:24
    #4 0x7fa6bc4883bc in tensorflow::(anonymous namespace)::Buffer<short>::Buffer(tsl::Allocator*, long, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:574:21
    #5 0x7fa6bc4883bc in tensorflow::Tensor::Tensor(tsl::Allocator*, tensorflow::DataType, tensorflow::TensorShape const&, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:986:5
    #6 0x7fa6bbef871d in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:764:10
    #7 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes) /proc/self/cwd/./tensorflow/core/framework/op_kernel.h:1270:12
    #8 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**, tsl::AllocatorAttributes) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:822:14
    #9 0x7fa6bbef4368 in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:728:10
    #10 0x7fa6a9bf0207 in tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e9fe207)
    #11 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #12 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #13 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #14 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #15 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #16 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #17 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #18 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #19 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #20 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #21 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #22 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #23 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #24 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #25 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #26 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #27 0x7fa6b985dcbf in tensorflow::EagerCast(TFE_Context*, TFE_TensorHandle*, TF_DataType, TF_DataType, TSL_Status*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:259:3
    #28 0x7fa6b985e67b in tensorflow::ConvertToEagerTensorUncached(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:317:11
    #29 0x7fa6b985f8a1 in tensorflow::ConvertToEagerTensor(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:405:14
    #30 0x7fa6b9860088 in EagerTensor_init /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:529:18
    #31 0x5039d2 in type_call /usr/local/src/conda/python-3.11.7/Objects/typeobject.c:1103:19
    #32 0x5039d2 in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18

SUMMARY: AddressSanitizer: heap-buffer-overflow /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10 in int const tensorflow::internal::SubtleMustCopy<int>(int const&)
Shadow bytes around the buggy address:
  0x609000000180: fa fa fa fa fa fa fa fa fd fa fa fa fa fa fa fa
  0x609000000200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000280: fd fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000300: 04 fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000380: fd fd fd fd fa fa fa fa fa fa fa fa fa fa fa fa
=>0x609000000400:[02]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000480: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000500: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000580: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000600: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000680: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
==4008222==ABORTING
```
",Security Vulnerability,Security Vulnerability
"[Compiler Bug]: Memoizing a debounced function complains about ref access ### What kind of issue is this?

- [ ] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)
- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)
- [X] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)
- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)

### Link to repro

https://playground.react.dev/#N4Igzg9grgTgxgUxALhAgHgBwjALgAgBMEAzAQygBsCSoA7OXASwjvwFkBPAQU0wAoAlPmAAdNvjiswBQhAC2AUUoAlUvgC8+NWUYA6KGARqSQ8fknSClJjIDyUXAhgnN2hLtwGjJsxPOWdDL4APpgcDAQlJQAKhAAQhC4uApuhggAwmTRAEa6ANb8QpoAfCIBFnJKqqR6cLAwCHRe4ZHRcfxiEhYWKZjI+Db2js4mdQ1NLRFRlAASCEwA5gAWuAA0FT05CMtkAG4sMAOi4PIQScsnG934AL6CANwBt2v4ANoAuo8BUkEErTM4olkqktOl2AgzvxNsUNGViDloAwEPwwtN2gkkil5K8AIwABnxgmuPXeaLasUxIPkHxJ+G+dHEAUauFgbAAPIQmHsSvNohB8AB1HCUQjsgD0XJ5Tzot3EIFuQA

### Repro steps

Attempting to memoize a debounced function that access a ref gives me this react compiler lint violation:
> Ref values (the `current` property) may not be accessed during render. 

However, the underlying function isn't called during render, it's simply referenced, and only called when the memoized, debounced function is called. In my case, it's called in an event handler, which is safe.

### How often does this bug happen?

Every time

### What version of React are you using?

18.3.1",Logical Bug,Logical Bug
"bucketize -function wrong results on GPU ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0-dev20240624

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Bucketize function returns wrong results when executed on GPU.   

Tested with  2.9.3, 2.15.0 and 2.18.0-dev20240624 and observed same incorrect behavior.

See the example below.   The CPU and GPU results are different so both can't be correct. GPU result seems to be wrong.

Can reproduce in [colab](https://colab.research.google.com/drive/1hSHgt5f31eUG-OsAy0FJ0zlcBhMgPV-c?authuser=0#scrollTo=nCDoKVOFw3ML)





### Standalone code to reproduce the issue

```shell
from tensorflow.python.ops.gen_math_ops import bucketize
import tensorflow as tf

print(tf.__version__)
gpus = tf.config.list_physical_devices('GPU')
assert gpus

x = [0,1,2,3]
boundaries = [0.1, 1.1]
with tf.device(""/CPU:0""):
    print(bucketize(x, boundaries=boundaries))
with tf.device(""/GPU:0""):
    print(bucketize(x, boundaries=boundaries))
```


### Relevant log output

```shell
2.18.0-dev20240624
tf.Tensor([0 1 2 2], shape=(4,), dtype=int32)
tf.Tensor([1 2 2 2], shape=(4,), dtype=int32)
```
",Logical Bug,Logical Bug
"BUG: StandardScaler partial_fit overflows The recent implementation of `partial_fit` for `StandardScaler` can overflow. A use case there is to transform indefinitely long stream of data, but that is problematic with the current implementation. The reason is that to compute the running mean, [we keep track](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py#L788) of the sample sum.

Here the code to reproduce the behavior. To simulate long stream of data would take long time; instead, I use samples with very large norm but the effect is the same. The same batch is presented to the transformer many times. The mean should be same.

```python
from sklearn.preprocessing import StandardScaler
import numpy as np

rng = np.random.RandomState(0)

def gen_1d_uniform_batch(min_, max_, n):
    return rng.uniform(min_, max_, size=(n, 1))

max_f = np.finfo(np.float64).max / 1e5
min_f = max_f / 1e2
stream_dim = 100
batch_dim = 500000
print(""mean overflow: batch vs online on %d repetitions"" % stream_dim)

X = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)

scaler = StandardScaler(with_std=False).fit(X)
print(scaler.mean_)
[  1.79769313e+301]

iscaler = StandardScaler(with_std=False)
batch = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)
for _ in range(stream_dim):
    iscaler = iscaler.partial_fit(batch)
RuntimeWarning: overflow encountered in add
  updated_mean = (last_sum + new_sum) / updated_sample_count

print(iscaler.mean_)
[ inf]
```
",Security Vulnerability,Security Vulnerability
"Linux: writing elevated fails with high `ulimit` values <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.96.2 (code_1.96.2-1734607745_amd64.deb)
- OS Version: Ubuntu 24.10

Steps to Reproduce:

1. Open a file that is not writable by the user
2. Edit the file
3. Try to save it
4. Error pops up stating that not enough permissions
5. Retry as Sudo option appears
6. Enter password
7. Saving process running forever

Same problem has been discussed here: https://github.com/microsoft/vscode/issues/234311. 
Is there any progress or plan? Or any workaround ?",Security Vulnerability,Security Vulnerability
"Devtools v4 does not work with Firefox's private window <!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

* bug
* This issue has been reported in https://github.com/facebook/react-devtools/issues/1383

**What is the current behavior?**


Steps to Reproduce is here:

1. Environments are:
2. Open the page which uses react with a private window.
3. Open Firefox's devtools.

Actual Result is:

* react devtools' _component_ pane show `Unable to find React on the page.`
* From about:debugging, we can see the below messsage:

```
SecurityError: Permission denied to access property ""container"" on cross-origin object main.js:51:305877
    Kl moz-extension://56db142d-3d36-b04e-91ca-a7504c7708a5/build/main.js:51
    apply self-hosted:4417
    applySafeWithoutClone resource://gre/modules/ExtensionCommon.jsm:588
    asyncWithoutClone resource://gre/modules/ExtensionCommon.jsm:2400
```



**What is the expected behavior?**

react devtools work

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

* Firefox 68
* react devtools v4.0.5
* react v16.9",Security Vulnerability,Security Vulnerability
"Signature verification failed with 'PackageIntegrityCheckFailed' error. 
Type: <b>Bug</b>

Ever since upgrading to the most recent release of vscode I cannot install or upgrade any vscode extensions.

Cannot install ... extension because Visual Studio Code cannot verify the extension signature

Signature verification failed with 'PackageIntegrityCheckFailed' error.

VS Code version: Code 1.94.0 (Universal) (d78a74bcdfad14d5d3b1b782f87255d802b57511, 2024-10-02T13:08:12.626Z)
OS version: Darwin arm64 23.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Pro (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|2, 2, 2|
|Memory (System)|32.00GB (1.83GB free)|
|Process Argv|--crash-reporter-id dfc65e10-fbb4-4b6c-919e-dce0edfa49eb|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (23)</summary>

Extension|Author (truncated)|Version
---|---|---
unique-lines|bib|1.0.0
ruff|cha|2024.50.0
gitlens|eam|15.5.1
terraform|has|2.32.3
vscode-duplicate|mrm|1.2.1
vscode-docker|ms-|1.29.3
debugpy|ms-|2024.10.0
isort|ms-|2023.10.1
python|ms-|2024.14.1
vscode-pylance|ms-|2024.9.2
datawrangler|ms-|1.10.0
jupyter|ms-|2024.8.1
jupyter-renderers|ms-|1.0.19
vscode-jupyter-cell-tags|ms-|0.1.9
remote-ssh|ms-|0.114.3
remote-ssh-edit|ms-|0.87.0
remote-explorer|ms-|0.4.3
vscode-thunder-client|ran|2.27.0
vscode-xml|red|0.27.1
vscode-yaml|red|1.15.0
stardog-rdf-grammars|sta|0.2.1
even-better-toml|tam|0.19.2
gistfs|vsl|0.6.0


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368:30146709
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
pythongtdpath:30769146
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
2e7ec940:31000449
pythontbext0:30879054
accentitlementsc:30995553
dsvsc016:30899300
dsvsc017:30899301
dsvsc018:30899302
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
bdiig495:31013172
a69g1124:31058053
dvdeprecation:31068756
dwnewjupytercf:31046870
2f103344:31071589
impr_priority:31102340
nativerepl1:31139838
refactort:31108082
pythonrstrctxt:31112756
flighttreat:31134774
wkspc-onlycs-t:31132770
nativeloc2:31134642
wkspc-ranged-t:31151552
cf971741:31144450
defaultse:31146405
iacca2:31150323
notype1:31151523
cc771715:31146322

```

</details>

<!-- generated by issue reporter -->",Dependency Issue,Dependency Issue
"f_regression takes square root of negative values when constant columns are present #### Description
`sklearn.feature_selection.f_regression` raises `RuntimeWarning: invalid value encountered in sqrt` when an array with any constant column is passed in. 

If this is the expected behavior, this issue should be closed. However, it seem suspicious that this only happens when `center` is `True`, and even so it may be worthwhile to check for constant columns and raise a more helpful warning.

If the call to `f_regression` below is replaced with
`f_regression(X, y, center=False)` then all values are returned with no NaN's or warnings.

I'm still not sure what conditions are needed for the error to occur. A constant column usually creates a division by zero warning, which appears to be the correct behavior. This specific array always results in square root of a negative, which is unexpected.

#### Steps/Code to Reproduce
```python
import numpy as np
from sklearn.feature_selection import f_regression

X = np.array([[  0.92  ,   0.2674,  54.934 ,  26.    ],
              [  0.92  ,   0.2674,  54.934 ,  28.    ],
              [  0.92  ,   0.2674,  54.934 ,  23.    ],
              [  0.92  ,   0.2674,  54.934 ,  21.    ],
              [  0.92  ,   0.2674,  54.934 ,  29.    ],
              [  0.92  ,   0.2674,  54.934 ,  25.    ],
              [  0.92  ,   0.2674,  54.934 ,  23.    ],
              [  0.92  ,   0.2674,  54.934 ,  28.    ],
              [  0.92  ,   0.2674,  54.934 ,  27.    ],
              [  0.92  ,   0.2674,  54.934 ,  27.    ]])
y = np.array([ 0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92])

f_regression(X, y)
```

#### Expected Results
No warning raised and no NaN's in output
#### Actual Results

```
/usr/local/miniconda3/envs/testml/lib/python3.6/site-packages/scikit_learn-0.19.1-py3.6-macosx-10.7-x86_64.egg/sklearn/feature_selection/univariate_selection.py:292: RuntimeWarning: invalid value encountered in sqrt
  n_samples * X_means ** 2)
/usr/local/miniconda3/envs/testml/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater
  return (self.a < x) & (x < self.b)
/usr/local/miniconda3/envs/testml/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less
  return (self.a < x) & (x < self.b)
/usr/local/miniconda3/envs/testml/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal
  cond2 = cond0 & (x <= self.a)
```
```python
(array([        nan, -8.        , -8.        , -8.07593079]),
 array([ nan,   1.,   1.,   1.]))
```
#### Another example without the error
```python
...
f_regression(X, y, center=False)
```
returns 
```
(array([ -2.02661983e+16,  -2.02661983e+16,  -2.02661983e+16,
          9.57231884e+02]),
 array([  1.00000000e+00,   1.00000000e+00,   1.00000000e+00,
          1.88654093e-10]))
```

#### Versions
Darwin-15.6.0-x86_64-i386-64bit
Python 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) 
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
NumPy 1.12.1
SciPy 1.1.0
Scikit-Learn 0.19.1",Runtime Error,Runtime Error
"LocallyLinearEmbedding : n_neighbors <= n_samples ### Describe the bug

Minor bug in `LocallyLinearEmbedding`'s parameter validation:

https://github.com/scikit-learn/scikit-learn/blob/70fdc843a4b8182d97a3508c1a426acc5e87e980/sklearn/manifold/_locally_linear.py#L226-L230

The `if` condition contradicts the error message in the case that `n_neighbors == N`. So you get a message like

```python-traceback
ValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3""
```

which doesn't make sense.

### Steps/Code to Reproduce

```python
import numpy as np
import sklearn.manifold

X = np.random.randn(3, 5)

embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])

embedder.fit_transform(X)
```

### Expected Results

n/a

### Actual Results

```python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1119], line 8
      4 X = np.random.randn(3, 5)
      6 embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])
----> 8 embedder.fit_transform(X)

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/utils/_set_output.py:313, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)
    311 @wraps(f)
    312 def wrapped(self, X, *args, **kwargs):
--> 313     data_to_wrap = f(self, X, *args, **kwargs)
    314     if isinstance(data_to_wrap, tuple):
    315         # only wrap the first output for cross decomposition
    316         return_tuple = (
    317             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    318             *data_to_wrap[1:],
    319         )

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:848, in LocallyLinearEmbedding.fit_transform(self, X, y)
    831 @_fit_context(prefer_skip_nested_validation=True)
    832 def fit_transform(self, X, y=None):
    833     \""\""\""Compute the embedding vectors for data X and transform X.
    834 
    835     Parameters
   (...)
    846         Returns the instance itself.
    847     \""\""\""
--> 848     self._fit_transform(X)
    849     return self.embedding_

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:795, in LocallyLinearEmbedding._fit_transform(self, X)
    793 X = self._validate_data(X, dtype=float)
    794 self.nbrs_.fit(X)
--> 795 self.embedding_, self.reconstruction_error_ = _locally_linear_embedding(
    796     X=self.nbrs_,
    797     n_neighbors=self.n_neighbors,
    798     n_components=self.n_components,
    799     eigen_solver=self.eigen_solver,
    800     tol=self.tol,
    801     max_iter=self.max_iter,
    802     method=self.method,
    803     hessian_tol=self.hessian_tol,
    804     modified_tol=self.modified_tol,
    805     random_state=random_state,
    806     reg=self.reg,
    807     n_jobs=self.n_jobs,
    808 )
    809 self._n_features_out = self.embedding_.shape[1]

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:227, in _locally_linear_embedding(X, n_neighbors, n_components, reg, eigen_solver, tol, max_iter, method, hessian_tol, modified_tol, random_state, n_jobs)
    223     raise ValueError(
    224         \""output dimension must be less than or equal to input dimension\""
    225     )
    226 if n_neighbors >= N:
--> 227     raise ValueError(
    228         \""Expected n_neighbors <= n_samples,  but n_samples = %d, n_neighbors = %d\""
    229         % (N, n_neighbors)
    230     )
    232 M_sparse = eigen_solver != \""dense\""
    234 if method == \""standard\"":

ValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3""
```

### Versions

```shell
System:
    python: 3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:47) [Clang 13.0.0 (clang-1300.0.29.30)]
executable: /usr/local/bin/python3
   machine: macOS-14.5-arm64-arm-64bit

Python dependencies:
      sklearn: 1.5.0
          pip: 24.0
   setuptools: 70.0.0
        numpy: 1.26.4
        scipy: 1.13.0
       Cython: 3.0.10
       pandas: 2.2.2
   matplotlib: 3.8.4
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 11
         prefix: libopenblas
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: blas
   internal_api: openblas
    num_threads: 11
         prefix: libopenblas
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.26.dev
threading_layer: pthreads
   architecture: neoversen1

       user_api: openmp
   internal_api: openmp
    num_threads: 11
         prefix: libomp
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```
```
",Syntax Error,Syntax Error
"Formula for dual gap of elastic net in coordinate descent solver ### Describe the bug
The computation of the dual gap for the elastic net in the coordinate descent solver (`enet_coordinate_descent`) might be wrong.

The elastic net minimizes
```
Primal(w) = (1/2) * ||y - X w||_2^2 + alpha * ||w||_1 + beta/2 * ||w||_2^2
```

### Lasso
For the pure Lasso, i.e. `beta=0`, the dual becomes [1]
```
Dual(nu) = -1/2 * ||nu||_2^2 - nu'y   if ||X'nu||_â = max(abs(X'nu)) <= alpha   else -â
```
with `nu = Xw - y` (=minus the residual `R`) as possible dual point. This yields the Lasso dual gap
```
Primal(w) - Dual(nu)
```
In case of `||X'nu||_â > alpha`, one uses a down-scaled variable `nu` in the dual.

### Elastic Net
For the elastic net, the dual, see section 5.2.3 in [2], becomes
```
Dual(nu) = -1/2 * ||nu||_2^2 - nu'y - 1/(2*beta) * sum_j (|X_j'nu| - alpha)_+^2
```
with `X_j` the j-th column of `X` and `x_+ = max(0, x)` the positive part.

### References
[1] Kim, Seung-Jean et al. âAn Interior-Point Method for Large-Scale $\ell_1$-Regularized Least Squares.â IEEE Journal of Selected Topics in Signal Processing 1 (2007): 606-617. [pdf link](http://stanford.edu/~boyd/papers/pdf/l1_ls.pdf)
[2] Mendler-DÃ¼nner, Celestine et al. âPrimal-Dual Rates and Certificates.â ICML (2016). [arxiv link](https://arxiv.org/pdf/1602.05205.pdf)

### Expected Results

```python
if beta > =0:
    R = y - X @ w  # note: R = -nu
    R_norm2 = R @ R
    gap = R_norm2 - R @ y + alpha * l1_norm + beta/2 * l2_norm
    gap += 1/(2*beta) * np.sum(np.max(0, np.abs(X.T @ R) - alpha)**2)
```
At least a test for it, similar to the pure Lasso case in https://github.com/scikit-learn/scikit-learn/blob/2c2f31d68c21b7647557b2f776e86c05954d80bf/sklearn/linear_model/tests/test_coordinate_descent.py#L292

### Actual Results

I start to be rattled by the `-beta * w` term in line 205.
https://github.com/scikit-learn/scikit-learn/blob/2c2f31d68c21b7647557b2f776e86c05954d80bf/sklearn/linear_model/_cd_fast.pyx#L205-L236


",Logical Bug,Logical Bug
" TypeError: this __dict__ descriptor does not support '_DictWrapper' objects during trivial model save ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No (bug doesn't exist in tf-nightly 2.17.0.dev20240312)

### Source

source

### TensorFlow version

v2.16.1

### Custom code

Yes

### OS platform and distribution

OSX

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When calling ` tf.saved_model.save(model, saved_model_path)`

we see:

```
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py:190: in list_children
    for name, child in super(_AugmentedGraphView, self).list_children(
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py:75: in list_children
    for name, ref in super(ObjectGraphView,
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/trackable_view.py:85: in children
    ref = converter.convert_to_trackable(ref, parent=obj)
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/trackable/converter.py:31: in convert_to_trackable
    if (tensor_util.is_tf_type(obj) and
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/framework/tensor_util.py:1156: in is_tf_type
    return isinstance(x, tf_type_classes)
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/typing.py:1871: in __instancecheck__
    val = getattr_static(instance, attr)
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py:1839: in getattr_static
    instance_result = _check_instance(obj, attr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = DictWrapper({'input_shape': [(None, 16), (None, 32)]}), attr = 'is_tensor_like'

    def _check_instance(obj, attr):
        instance_dict = {}
        try:
>           instance_dict = object.__getattribute__(obj, ""__dict__"")
E           TypeError: this __dict__ descriptor does not support '_DictWrapper' objects

../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py:1793: TypeError
```

I suspect this is related to #59869 which was supposedly fixed. However, in 2.16.1 TF removes the pin on wrapt and the issue indeed persists. I've even tried downgrading wrapt to 1.14.1 and the issue remains.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras import layers
import tempfile

def get_two_tower_models():
    online_features = [layers.Input(shape=(32,)), layers.Input(shape=(16,))]
    offline_features = [layers.Input(shape=(16,)), layers.Input(shape=(32,))]
    all_features = []
    all_features.extend(online_features)
    all_features.extend(offline_features)

    def get_offline_tower(offline_features):
        offline_inputs = layers.concatenate(offline_features, name=""offline_concatenated"")
        offline_hidden = layers.Dense(32, activation=""tanh"", name=""offline_hidden_1"")(offline_inputs)
        offline_hidden = layers.Dense(16, activation=""tanh"", name=""offline_hidden_2"")(offline_hidden)
        offline_final_embed = layers.Dense(8, name=""offline_hidden_3"")(offline_hidden)

        return offline_final_embed

    def get_online_tower(online_features):
        online_inputs = layers.concatenate(online_features, name=""online_concatenated"")
        online_hidden = layers.Dense(32, activation=""tanh"", name=""online_hidden_1"")(online_inputs)
        online_hidden = layers.Dense(16, activation=""tanh"", name=""online_hidden_2"")(online_hidden)
        online_final_embed = layers.Dense(8, name=""online_hidden_3"")(online_hidden)

        return online_final_embed

    offline_tower_embed = get_offline_tower(offline_features)
    online_tower_embed = get_online_tower(online_features)

    # We normalize vectors with L2 norm to make sure we get the cosine similarity
    offline_online_dot = layers.Dot(axes=1, normalize=True)([offline_tower_embed, online_tower_embed])

    offline_tower_model = tf.keras.Model(inputs=offline_features, outputs=offline_tower_embed)
    online_tower_model = tf.keras.Model(inputs=online_features, outputs=online_tower_embed)

    full_model = tf.keras.Model(inputs=all_features, outputs=offline_online_dot)

    return (full_model, offline_tower_model, online_tower_model)

full_model, offline_tower_model, online_tower_model = get_two_tower_models()

with tempfile.TemporaryDirectory() as tmpdirname:
    tf.saved_model.save(online_tower_model, tmpdirname)
```
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/Users/alfredo_luque/repos/git.musta.ch/airbnb/bighead-service/packages/ml-frameworks/tensorflow/tests/minimal_repro.py"", line 44, in <module>
    tf.saved_model.save(online_tower_model, tmpdirname)
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 1392, in save
    save_and_return_nodes(obj, export_dir, signatures, options)
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 1427, in save_and_return_nodes
    _build_meta_graph(obj, signatures, options, meta_graph_def))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 1642, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 1564, in _build_meta_graph_impl
    saveable_view = _SaveableView(augmented_graph_view, options)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 285, in __init__
    checkpoint_util.objects_ids_and_slot_variables_and_paths(
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/util.py"", line 160, in objects_ids_and_slot_variables_and_paths
    trackable_objects, node_paths = graph_view.breadth_first_traversal()
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py"", line 124, in breadth_first_traversal
    return self._breadth_first_traversal()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 156, in _breadth_first_traversal
    super(_AugmentedGraphView, self)._breadth_first_traversal())
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py"", line 128, in _breadth_first_traversal
    return super(ObjectGraphView, self)._descendants_with_paths()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/trackable_view.py"", line 111, in _descendants_with_paths
    for name, dependency in self.children(current_trackable).items():
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py"", line 97, in children
    for name, ref in self.list_children(obj, **kwargs):
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 190, in list_children
    for name, child in super(_AugmentedGraphView, self).list_children(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py"", line 75, in list_children
    for name, ref in super(ObjectGraphView,
                     ^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/trackable_view.py"", line 85, in children
    ref = converter.convert_to_trackable(ref, parent=obj)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/trackable/converter.py"", line 31, in convert_to_trackable
    if (tensor_util.is_tf_type(obj) and
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/framework/tensor_util.py"", line 1156, in is_tf_type
    return isinstance(x, tf_type_classes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/typing.py"", line 1871, in __instancecheck__
    val = getattr_static(instance, attr)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py"", line 1839, in getattr_static
    instance_result = _check_instance(obj, attr)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py"", line 1793, in _check_instance
    instance_dict = object.__getattribute__(obj, ""__dict__"")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: this __dict__ descriptor does not support '_DictWrapper' objects
```
",Runtime Error,Runtime Error
"NDCG in case of abscence of relevant items ### Describe the bug

In `sklearn.metrics._ndcg_sample_scores`, there is a counterintuitive handling of the case where all true relevances are equal to zero for some samples. In this case, DCG = 0, IDCG = 0, and the whole NDCG is not defined. In `sklearn` implementation it is defined as 0 and included in the averaged NDCG calculation.  The least leads to strange effects, like `ndcg_score(y,y) != 1`; moreover, it affects the metric value in non-trivial cases too.

In the original 2002 paper where NDCG is proposed, it is not stated how to handle such situations, but it is clearly mentioned that 
```
The (D)CG vectors for each IR technique can be normalized by dividing them
by the corresponding ideal (D)CG vectors, component by component. In this way,
for any vector position, the normalized value 1 represents ideal performance,
and values in the range [0, 1) the share of ideal performance cumulated by each
technique.
```
meaning that NDCG(y,y) must always be 1. 


I suggest excluding observations without relevant items and/or throwing a warning.

### Steps/Code to Reproduce

```
>>> from sklearn.metrics import ndcg_score
>>> y = np.array([[1.0, 0.0, 1.0], [0.0, 0.0, 0.0]])
>>> ndcg_score(y, y)
```


### Expected Results

1.

### Actual Results

0.5

### Versions

```shell
This code was not changed in 1.5, so I guess for newer versions the issue also is actual.



System:
    python: 3.11.8 (main, Feb 12 2024, 14:50:05) [GCC 13.2.1 20230801]
executable: /usr/bin/python3
   machine: Linux-6.6.19-1-MANJARO-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.3.1
          pip: 24.0
   setuptools: 69.0.3
        numpy: 1.26.4
        scipy: 1.10.1
       Cython: 3.0.9
       pandas: 1.5.3
   matplotlib: 3.7.1
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/arabella/.local/lib/python3.11/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/arabella/.local/lib/python3.11/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: Haswell
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/arabella/.local/lib/python3.11/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so
        version: 0.3.18
threading_layer: pthreads
   architecture: Haswell
    num_threads: 8
```
",Performance Issue,Performance Issue
"Recursive variable font is rendered with wrong space width <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.96.2
- OS Version: Windows 11 24H2

Steps to Reproduce:

1. Install the VF version of Recursive font (Recursive_VF_1.085.ttf) from www.recursive.design
2. Change the editor font to ""Recursive Mono Linear""

    `settings.json`:
    ```json
    {
        ""editor.fontFamily"": ""'Recursive Mono Linear'""
    }
    ```

3. Observe that spaces are too narrow:
    ![Image](https://github.com/user-attachments/assets/d8fdcf39-384c-434d-870e-5e1c748cbeda)
    compared to e.g. Consolas:
    ![Image](https://github.com/user-attachments/assets/59bf142f-d0ff-466c-b9c3-1832700169d6)
    The bug is not in the font itself: it renders just fine in Zed for Windows.
    ![Image](https://github.com/user-attachments/assets/06b5485d-816e-4fb2-8e79-ac5a1c331d65)


",UI/UX Bug,UI/UX Bug
"LocallyLinearEmbedding : n_neighbors <= n_samples ### Describe the bug

Minor bug in `LocallyLinearEmbedding`'s parameter validation:

https://github.com/scikit-learn/scikit-learn/blob/70fdc843a4b8182d97a3508c1a426acc5e87e980/sklearn/manifold/_locally_linear.py#L226-L230

The `if` condition contradicts the error message in the case that `n_neighbors == N`. So you get a message like

```python-traceback
ValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3""
```

which doesn't make sense.

### Steps/Code to Reproduce

```python
import numpy as np
import sklearn.manifold

X = np.random.randn(3, 5)

embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])

embedder.fit_transform(X)
```

### Expected Results

n/a

### Actual Results

```python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1119], line 8
      4 X = np.random.randn(3, 5)
      6 embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])
----> 8 embedder.fit_transform(X)

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/utils/_set_output.py:313, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)
    311 @wraps(f)
    312 def wrapped(self, X, *args, **kwargs):
--> 313     data_to_wrap = f(self, X, *args, **kwargs)
    314     if isinstance(data_to_wrap, tuple):
    315         # only wrap the first output for cross decomposition
    316         return_tuple = (
    317             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    318             *data_to_wrap[1:],
    319         )

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:848, in LocallyLinearEmbedding.fit_transform(self, X, y)
    831 @_fit_context(prefer_skip_nested_validation=True)
    832 def fit_transform(self, X, y=None):
    833     \""\""\""Compute the embedding vectors for data X and transform X.
    834 
    835     Parameters
   (...)
    846         Returns the instance itself.
    847     \""\""\""
--> 848     self._fit_transform(X)
    849     return self.embedding_

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:795, in LocallyLinearEmbedding._fit_transform(self, X)
    793 X = self._validate_data(X, dtype=float)
    794 self.nbrs_.fit(X)
--> 795 self.embedding_, self.reconstruction_error_ = _locally_linear_embedding(
    796     X=self.nbrs_,
    797     n_neighbors=self.n_neighbors,
    798     n_components=self.n_components,
    799     eigen_solver=self.eigen_solver,
    800     tol=self.tol,
    801     max_iter=self.max_iter,
    802     method=self.method,
    803     hessian_tol=self.hessian_tol,
    804     modified_tol=self.modified_tol,
    805     random_state=random_state,
    806     reg=self.reg,
    807     n_jobs=self.n_jobs,
    808 )
    809 self._n_features_out = self.embedding_.shape[1]

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:227, in _locally_linear_embedding(X, n_neighbors, n_components, reg, eigen_solver, tol, max_iter, method, hessian_tol, modified_tol, random_state, n_jobs)
    223     raise ValueError(
    224         \""output dimension must be less than or equal to input dimension\""
    225     )
    226 if n_neighbors >= N:
--> 227     raise ValueError(
    228         \""Expected n_neighbors <= n_samples,  but n_samples = %d, n_neighbors = %d\""
    229         % (N, n_neighbors)
    230     )
    232 M_sparse = eigen_solver != \""dense\""
    234 if method == \""standard\"":

ValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3""
```

### Versions

```shell
System:
    python: 3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:47) [Clang 13.0.0 (clang-1300.0.29.30)]
executable: /usr/local/bin/python3
   machine: macOS-14.5-arm64-arm-64bit

Python dependencies:
      sklearn: 1.5.0
          pip: 24.0
   setuptools: 70.0.0
        numpy: 1.26.4
        scipy: 1.13.0
       Cython: 3.0.10
       pandas: 2.2.2
   matplotlib: 3.8.4
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 11
         prefix: libopenblas
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: blas
   internal_api: openblas
    num_threads: 11
         prefix: libopenblas
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.26.dev
threading_layer: pthreads
   architecture: neoversen1

       user_api: openmp
   internal_api: openmp
    num_threads: 11
         prefix: libomp
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```
```
",Syntax Error,Syntax Error
"PERF predict_proba is slow when n_jobs > 1 for random forests The use of `n_jobs > 1` for small batch can slow down the prediction for forest models. This is probably due to the overhead incurred by using `joblib` (create thread, check system info, ...) which is dominate the runtime compared to the computations. This was reported originally in joblib/joblib#982.

A couple of ideas to solve this:
- Set `n_jobs=1` when the size of the batch is small.
- Introduce a `n_jobs_predict` parameters that would default to `1/n_jobs` for forests but that can be set separately.

",Performance Issue,Performance Issue
"Visual Studio Code doesn't show translated walkthrough strings until restarting app or reloading window 
Type: <b>Bug</b>

1. Select ""Configure Display Language"" from comand palette and choose e.g. ""spanish (es)""
2. Restart application
Observe that application menus and UI is in spanish as selected
3. Go to extension marketplace and install a localized extension, e.g. ms-vscode.cpptools [defined in this repo](https://github.com/microsoft/vscode-cpptools/tree/main/Extension)
4. Open extension walkthrough in welcome page. 
Walkthrough steps will be in english and not spanish, although markdown content in side-panel will in fact be in spanish.
5. select ""Developer: Reload Window"" from comand palette
Walkthrough steps will now be in spanish, as expected

I'm using version 1.96.4, but this same issue repros in latest insider build.

[Video of repro](https://github.com/user-attachments/assets/6b8c9929-15ef-407d-aa42-958a73109e5a)


VS Code version: Code 1.96.4 (Universal) (cd4ee3b1c348a13bafd8f9ad8060705f6d4b9cba, 2025-01-16T00:16:19.038Z)
OS version: Darwin arm64 24.3.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M3 Pro (12 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|107, 34, 15|
|Memory (System)|18.00GB (0.04GB free)|
|Process Argv|--crash-reporter-id b0326380-260b-46e7-98c9-19544d8a9260|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (16)</summary>

Extension|Author (truncated)|Version
---|---|---
kotlin|fwc|0.2.36
remotehub|Git|0.64.0
vscode-docker|ms-|1.29.4
vscode-language-pack-es|MS-|1.96.2024121109
csdevkit|ms-|1.15.34
csharp|ms-|2.61.28
vscode-dotnet-runtime|ms-|2.2.6
debugpy|ms-|2024.14.0
python|ms-|2024.22.2
vscode-pylance|ms-|2024.12.1
remote-containers|ms-|0.394.0
azure-repos|ms-|0.40.0
cpptools|ms-|1.23.5
remote-repositories|ms-|0.42.0
vscode-typescript-next|ms-|5.8.20250129
openai-chatgpt-adhoc|ope|0.0.1731981761


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
a9j8j154:30646983
962ge761:30959799
pythonnoceb:30805159
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
dsvsc021:30996838
dvdeprecation:31068756
dwnewjupytercf:31046870
nativerepl1:31139838
pythonrstrctxt:31112756
nativeloc2:31192216
cf971741:31144450
iacca1:31171482
5fd0e150:31155592
dwcopilot:31170013
stablechunks:31184530
6074i472:31201624
dwoutputs:31217127
hdaa2157:31222309
copilot_t_ci:31222730

```

</details>

<!-- generated by issue reporter -->

",Runtime Error,Dependency Issue
"`tf.raw_ops.ArgMax`: Heap buffer overflow ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.ArgMax` can lead to heap buffer overflow.
[Error location](https://github.com/tensorflow/tensorflow/blob/774b0c3e97b5ef60bfc9c54961347dd0bc3660a8/tensorflow/core/kernels/argmax_op.cc#L59):
```C++
    const int32_t dim = internal::SubtleMustCopy(dimension.scalar<int32>()());
```
It copies scalar value of `dimension` without checking exact type. Therefore when `dimension` is an `int16` it reads over the bound.
Note that `int16` is an allowed type for `dimension` according to [opdef](https://github.com/tensorflow/tensorflow/blob/41b93a8b310086f69aab6b6369d2af9d5178881d/tensorflow/core/ops/math_ops.cc#L1153-L1160):
```C++
REGISTER_OP(""ArgMax"")
    .Input(""input: T"")
    .Input(""dimension: Tidx"")
    .Output(""output: output_type"")
    .Attr(""T: {realnumbertype, quantizedtype, bool}"")
    .Attr(""Tidx: {int16, int32, int64} = DT_INT32"")
    .Attr(""output_type: {int16, uint16, int32, int64} = DT_INT64"")
    .SetShapeFn(ArgOpShape);
```

### Standalone code to reproduce the issue

```python
import tensorflow as tf

tf.raw_ops.ArgMax(
    input=tf.random.normal([1,1,1,1]),
    dimension=tf.constant(1,shape=[],dtype=tf.int16),
    output_type=tf.dtypes.int64,
    name=None
)
```


### Relevant log output

The below log needs ASAN build.
```shell
=================================================================
==4008222==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x609000000400 at pc 0x7fa6a0dca809 bp 0x7ffe63b29d90 sp 0x7ffe63b29d88
READ of size 4 at 0x609000000400 thread T0
    #0 0x7fa6a0dca808 in int const tensorflow::internal::SubtleMustCopy<int>(int const&) /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10
    #1 0x7fa6a0dca808 in tensorflow::ArgOp<Eigen::ThreadPoolDevice, float, long, tensorflow::functor::ArgMax<Eigen::ThreadPoolDevice, float, long>>::Compute(tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/kernels/argmax_op.cc:59:25
    #2 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #3 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #4 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #5 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #6 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #7 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #8 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #9 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #10 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #11 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #12 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #13 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #14 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #15 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #16 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #17 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #18 0x7fa6b989418d in TFE_Py_FastPathExecute_C(_object*) /proc/self/cwd/tensorflow/python/eager/pywrap_tfe_src.cc:3979:3
    #19 0x7fa667d2683e in pybind11_init__pywrap_tfe(pybind11::module_&)::$_60::operator()(pybind11::args) const /proc/self/cwd/tensorflow/python/tfe_wrapper.cc:1276:35
    #20 0x7fa667d2683e in pybind11::object pybind11::detail::argument_loader<pybind11::args>::call_impl<pybind11::object, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, 0ul, pybind11::detail::void_type>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, std::integer_sequence<unsigned long, 0ul>, pybind11::detail::void_type&&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1443:16
    #21 0x7fa667d2683e in std::enable_if<!std::is_void<pybind11::object>::value, pybind11::object>::type pybind11::detail::argument_loader<pybind11::args>::call<pybind11::object, pybind11::detail::void_type, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1411:42
    #22 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:248:69
    #23 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:223:21
    #24 0x7fa667d67a59 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:939:30
    #25 0x528186 in cfunction_call /usr/local/src/conda/python-3.11.7/Objects/methodobject.c:542:18
    #26 0x503a0b in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18
    #27 0x510f32 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:4769:23
    #28 0x538732 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #29 0x538732 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #30 0x538732 in _PyFunction_Vectorcall /usr/local/src/conda/python-3.11.7/Objects/call.c:393:16
    #31 0x5426bb in _PyVectorcall_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:257:24
    #32 0x5426bb in _PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:328:16
    #33 0x5426bb in PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:355:12
    #34 0x514ff0 in do_call_core /usr/local/src/conda/python-3.11.7/Python/ceval.c:7352:12
    #35 0x514ff0 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:5376:22
    #36 0x5cb559 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #37 0x5cb559 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #38 0x5cac2e in PyEval_EvalCode /usr/local/src/conda/python-3.11.7/Python/ceval.c:1148:21
    #39 0x5ebcf6 in run_eval_code_obj /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1710:9
    #40 0x5e788f in run_mod /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1731:19
    #41 0x5fc831 in pyrun_file /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1626:15
    #42 0x5fbbfe in _PyRun_SimpleFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:440:13
    #43 0x5fb922 in _PyRun_AnyFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:79:15
    #44 0x5f65cd in pymain_run_file_obj /usr/local/src/conda/python-3.11.7/Modules/main.c:360:15
    #45 0x5f65cd in pymain_run_file /usr/local/src/conda/python-3.11.7/Modules/main.c:379:15
    #46 0x5f65cd in pymain_run_python /usr/local/src/conda/python-3.11.7/Modules/main.c:601:21
    #47 0x5f65cd in Py_RunMain /usr/local/src/conda/python-3.11.7/Modules/main.c:680:5
    #48 0x5bb3d8 in Py_BytesMain /usr/local/src/conda/python-3.11.7/Modules/main.c:734:12
    #49 0x7fa791408d8f in __libc_start_call_main csu/../sysdeps/nptl/libc_start_call_main.h:58:16
    #50 0x7fa791408e3f in __libc_start_main csu/../csu/libc-start.c:392:3
    #51 0x5bb222 in _start (/home/loft/anaconda3/envs/tf-latest-asan/bin/python3.11+0x5bb222)

0x609000000402 is located 0 bytes after 2-byte region [0x609000000400,0x609000000402)
allocated by thread T0 here:
    #0 0x7fa7917ed617 in __interceptor_posix_memalign /home/runner/work/llvm-project/llvm-project/final/llvm-project/compiler-rt/lib/asan/asan_malloc_linux.cpp:145:3
    #1 0x7fa6be4f6902 in tsl::port::AlignedMalloc(unsigned long, int) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4a18902)
    #2 0x7fa6bc2d3bcc in tsl::(anonymous namespace)::CPUAllocator::AllocateRaw(unsigned long, unsigned long) cpu_allocator_impl.cc
    #3 0x7fa6bc4883bc in short* tensorflow::TypedAllocator::Allocate<short>(tsl::Allocator*, unsigned long, tsl::AllocationAttributes const&) /proc/self/cwd/./tensorflow/core/framework/typed_allocator.h:47:24
    #4 0x7fa6bc4883bc in tensorflow::(anonymous namespace)::Buffer<short>::Buffer(tsl::Allocator*, long, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:574:21
    #5 0x7fa6bc4883bc in tensorflow::Tensor::Tensor(tsl::Allocator*, tensorflow::DataType, tensorflow::TensorShape const&, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:986:5
    #6 0x7fa6bbef871d in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:764:10
    #7 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes) /proc/self/cwd/./tensorflow/core/framework/op_kernel.h:1270:12
    #8 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**, tsl::AllocatorAttributes) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:822:14
    #9 0x7fa6bbef4368 in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:728:10
    #10 0x7fa6a9bf0207 in tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e9fe207)
    #11 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #12 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #13 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #14 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #15 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #16 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #17 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #18 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #19 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #20 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #21 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #22 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #23 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #24 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #25 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #26 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #27 0x7fa6b985dcbf in tensorflow::EagerCast(TFE_Context*, TFE_TensorHandle*, TF_DataType, TF_DataType, TSL_Status*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:259:3
    #28 0x7fa6b985e67b in tensorflow::ConvertToEagerTensorUncached(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:317:11
    #29 0x7fa6b985f8a1 in tensorflow::ConvertToEagerTensor(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:405:14
    #30 0x7fa6b9860088 in EagerTensor_init /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:529:18
    #31 0x5039d2 in type_call /usr/local/src/conda/python-3.11.7/Objects/typeobject.c:1103:19
    #32 0x5039d2 in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18

SUMMARY: AddressSanitizer: heap-buffer-overflow /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10 in int const tensorflow::internal::SubtleMustCopy<int>(int const&)
Shadow bytes around the buggy address:
  0x609000000180: fa fa fa fa fa fa fa fa fd fa fa fa fa fa fa fa
  0x609000000200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000280: fd fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000300: 04 fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000380: fd fd fd fd fa fa fa fa fa fa fa fa fa fa fa fa
=>0x609000000400:[02]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000480: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000500: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000580: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000600: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000680: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
==4008222==ABORTING
```
",Security Vulnerability,Security Vulnerability
"`notebook.addFindMatchToSelection` keeps looping Testing #235047

When pressing <kbd>Cmd D</kbd> in a regular editor, find results keep getting added to the multiple selection collection. It will be a noop, once all find results are covered by selections.

?? In notebooks, <kbd>Cmd D</kbd> will never stop looping around find results.

https://github.com/user-attachments/assets/70dccdb2-1d20-4c52-95bf-d7733e64d97e
",Performance Issue,Performance Issue
"[DevTools Bug] getCommitTree(): Invalid commit ""1"" for root ""445"". There are only ""1"" commits. ### Website or app

https://abhayyportfolio.netlify.app/

### Repro steps

I am just checking my website performance by the use of profiler 

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

5.2.0-1717ab0171

### Error message (automated)

getCommitTree(): Invalid commit ""1"" for root ""445"". There are only ""1"" commits.

### Error call stack (automated)

```text
at getCommitTree (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1150777)
    at fe.getCommitTree (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1154452)
    at CommitFlamegraphAutoSizer (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1396842)
    at renderWithHooks (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:76682)
    at updateFunctionComponent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:103097)
    at beginWork (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:116771)
    at performUnitOfWork (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:175653)
    at workLoopSync (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:175521)
    at renderRootSync (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:175252)
    at recoverFromConcurrentError (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:171121)
```


### Error component stack (automated)

```text
at CommitFlamegraphAutoSizer (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1396643)
    at div
    at div
    at div
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1291561)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1528595
    at ua (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1309094)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1311805)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1312002
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1311805)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1387831)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1379486)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1203530)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1231781)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1316268)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:1:1536331)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=getCommitTree(): Invalid commit  for root . There are only  commits. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
",Performance Issue,Performance Issue
"`Check failed` in `tf.raw_ops.LoadAndRemapMatrix` when the rank of `ckpt_path` is not equal to the rank of `old_tensor_name`. ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the rank of `ckpt_path` is not equal to the rank of `old_tensor_name`, `tf.raw_ops.LoadAndRemapMatrix` triggers `Check fail` error.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1Eba1FTUMG_as_2FtrSUcAt44Z-LWPX4Z?usp=sharing
```


### Relevant log output

```shell
Check failed: NDIMS == dims() (1 vs. 0)Asking for tensor of 1 dimensions from a tensor of 0 dimensions
```
",Runtime Error,Runtime Error
"unique operation on strings returns bogus indices **System information**
- Have I written custom code: No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.9
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.8.
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1.243
- GPU model and memory: None

**Describe the current behavior**

Executing the test that runs `unique` on strings/chars returns invalid values for the index tensor. Some values are plain out of bounds (very big or negative) and others seem to point to wrong values. This very much looks like some kind of memory corruption.
So far this has only been reproducible on a cascade lake CPU, no GPUs in the system but TF was built with CUDA support.

**Describe the expected behavior**

Valid values returned

**Standalone code to reproduce the issue**

```
    import numpy as np
    from tensorflow.python.ops import array_ops

    def testString():
        indx = np.random.randint(65, high=122, size=7000)
        x = [chr(i) for i in indx]
        y, idx = array_ops.unique(x)
        tf_y, tf_idx = (y.numpy(), idx.numpy())
        print(','.join(str(i) for i in tf_idx))

        assert len(x) == len(tf_idx)
        assert len(tf_y) == len(np.unique(x))

        for i in range(len(x)):
            assert x[i] == tf_y[tf_idx[i]].decode('ascii')

    testString()
```
This test was extracted from the TF test suite as-is.
Errors include the last assert failing as well as errors like `IndexError: index 7566446 is out of bounds for axis 0 with size 57`

**Other info / logs**

For a part of the log from the TF test suite see https://gist.github.com/boegel/26f768c82080e593add3924fc7bc76cf
The `print` of `tf_idx` shows things like:
`-15829468,126746879,293189,50530320,291592,3,-16616924,58720256,66085632,66085120,1,0,7352690,946952,1538050,51149349,10,126746628,11,10,7,33583628,-16551388,126747138,126746628,50491941,101480976,126746879,16742418,14,20,752147,19,134209791,126749222,17,126747141,3840,16,2,33585676,18,387021328,70664,219117604,486963728,27,66343024,50530320,24,50756133,27,126762239,4096,7,-16223708,126749187,33977217,20,9,4,293189,20,4,40,24,23,8,13,-16025575,31,26,2375,40,22,126749187,8,19,36,6,26,126746627,22,36,133292287,133292287,44,21,34,36,13,31,26,13,24,43,10,-16354780,31,41,419459596,38,39,8,1,43,2,44,1,4,11,2,45,7,22,41,29,6,18,39,126746630,5,8,3,1,26,14,40,36,32,8,988163,25,7,42,18,3,41,50,32,2,8,22,22,17,39,18,5,21,30,14,22,34,43,43,48,6,24,50,20,32,24,29,35,8,40,9,16,18,46,50,18,7,13,31,37,47,12,2,27,39,6,37,38,44,45,29,16,0,2,31,1,29,8,26,9,30,32,66084976,29,29,29,24,15,39,35,49,37,30,0,27,39,386169362,9,41,4,7,126749211,5,24,46,4,0,49 [...]` (remaining values look ok)


To stress that: This seems to be highly system dependent. We compile with `-march=native` and see it only on the cascade lake system. The string test is the only one that fails, all other unique tests succeed.
This sounds to me like some case of undefined behavior where an optimization corrupts the absl hashmap used.",Runtime Error,Runtime Error
"[XLA] TF XLA outputs abnormal value when compiling `Embedding` ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

For `Embedding` operator, when I set the `input_dim=1` (which means it indexed from 0 to 0), the output always returns **0 without XLA**.

After compilation, the outputs are usually some random tensors.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.random.set_seed(42)
x = tf.constant([1])


# uncompiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()

output1 = m(x)


# compiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    @tf.function(jit_compile=True)
    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()
output2 = m(x)

print(output1)
print(output2)
```


### Relevant log output

```shell
tf.Tensor([[0.]], shape=(1, 1), dtype=float32)
tf.Tensor([[-0.00567592]], shape=(1, 1), dtype=float32)
```
",Logical Bug,Runtime Error
"[DevTools Bug]: No way to debug suspense events ### Website or app

n/a

### Repro steps

The react dev-tools have not been updated with support for debugging suspense issues.

For examples:
- In the profiler, you can see that a suspense even happened and caused a re-render, but you cannot see which component actually caused the suspense (ie. called `use` or similar) to trigger.
- There doesn't appear to be any kind of logging that can be turned on of suspense events (which would tell you which component suspended)


### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Syntax Error,Syntax Error
"Terminal suggest: Widget scrolls with terminal buffer Mouse wheel scrolling in terminal:

![Image](https://github.com/user-attachments/assets/5dd5604f-9ac9-4c0a-a003-006a0cd788c3)

In editor:

![Image](https://github.com/user-attachments/assets/22856109-234c-409d-afc2-47071e6c4745)

We should do what the editor does ideally, but if that's too tough we can just hide it on scroll which would improve the experience imo.",Dependency Issue,Dependency Issue
"How to fix Microsoft Visual studio code, language pack issue from English to French at first launch. <!-- ???? Do Not Delete This! feature_request_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

We are facing issue regarding the Microsoft Visual studio code --language pack, so we need to change the display language of the software to French(FR), so as per information provided on visual studio code page, we downloaded the language pack and also edited the argv.json file but even Tho keeping this files and folders priorly after installation, still at first launch we are getting English language only. Please refer attached image 1

![Image](https://github.com/user-attachments/assets/5f850a4f-9c2e-4081-9ba6-34ff5b6f1a7e)

. As we are getting French language at 2nd launch(Refer image 2),

![Image](https://github.com/user-attachments/assets/29faeab3-4274-44e1-aed8-f04a5905a8f4)
we also tried to add start process of code.exe and killed it so it gets launched at once into the script. but still even after keeping user data files, language package and also start process and kill through script still English language gets displayed.

Summary: On first launch we are getting English language even after keeping all the needed data in user context, but after closing the software and relaunch it manually then only it displays French lang. Not of first launch. Please help us understand if any additional parameters files to be added to make it work on first launch.
",Dependency Issue,Dependency Issue
"Memory leak in training ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1 and 2.17

### Custom code

Yes

### OS platform and distribution

Debian 11

### Mobile device

_No response_

### Python version

Python 3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda/12.0.0_gcc-10.4.0 and cudnn/8.9.7.29-12_gcc-10.4.0

### GPU model and memory

different GPU, among which Tesla V100-SXM2-32GB

### Current behavior?

I have a memory leak (GPU memory and RAM constantly increase) during my training. 

**This does not happen with Tensorflow 2.11.1**. 

[Here is the project](https://github.com/deep-finder/tirfm-deepfinder).
Unfortunately, this is not my code and I do not have time to create a minimal standalone code.

### Standalone code to reproduce the issue

```shell
def printMemoryUsage(self):
        gpus = tf.config.list_physical_devices('GPU')             
        for gpu in gpus:
            gpuNameRoot = gpu.name.split(':')[0] + ':'
            memory_info = tf.config.experimental.get_memory_info(gpu.name.replace(gpuNameRoot, ''))
            self.display(f'Memory info of GPU {gpu.name}: current: {memory_info[""current""]/1e9:.2f}, peak: {memory_info[""peak""]/1e9:.2f}')
            try:
                import psutil
                virtual_memory = psutil.virtual_memory()
                print(f'Memory info of CPU: total:{virtual_memory[0]/1e9:.2f}Gb, available: {virtual_memory[1]/1e9:.2f}Gb, percent: {virtual_memory[2]}%')
            except:
                pass

[...]
        # Training loop:
        for e in range(self.epochs):
            # TRAINING:
            start = time.time()
            list_loss_train = []
            list_acc_train = []
            for it in range(self.steps_per_epoch):
                if self.flag_direct_read:
                    batch_data, batch_target = self.generate_batch_direct_read(path_data, path_target, self.batch_size, objlist_train)
                else:
                    batch_data, batch_target, idx_list = self.generate_batch_from_array(data_list, target_list, self.batch_size, objlist_train)

                if self.sample_weights is not None:
                    sample_weight = self.sample_weights[idx_list]
                else:
                    sample_weight = None

                loss_train = self.net.train_on_batch(batch_data, batch_target,
                                                     class_weight=self.class_weight,
                                                     sample_weight=sample_weight)

                self.display('epoch %d/%d - it %d/%d - loss: %0.3f - acc: %0.3f' % (e + 1, self.epochs, it + 1, self.steps_per_epoch, loss_train[0], loss_train[1]))

                self.printMemoryUsage()

                list_loss_train.append(loss_train[0])
                list_acc_train.append(loss_train[1])
                del batch_data
                del batch_target
                if idx_list is not None:
                    del idx_list
                gc.collect()
```


### Relevant log output

```shell
With Tensorflow 2.11.1:


=============================================================
epoch 3/50 - it 1/100 - loss: 2.012 - acc: 0.976
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 2/100 - loss: 2.008 - acc: 0.985
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 3/100 - loss: 2.004 - acc: 0.992
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 4/100 - loss: 2.006 - acc: 0.987
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 5/100 - loss: 2.006 - acc: 0.989
```


With Tensorflow 2.16.1 and 2.17:

```
epoch 1/50 - it 6/100 - loss: 2.473 - acc: 0.305
Memory info of GPU /physical_device:GPU:0: current: 0.18, peak: 2.55
Memory info of CPU: total:201.37Gb, available: 190.78Gb, percent: 5.3%
epoch 1/50 - it 7/100 - loss: 2.470 - acc: 0.394
Memory info of GPU /physical_device:GPU:0: current: 0.21, peak: 2.58
Memory info of CPU: total:201.37Gb, available: 190.58Gb, percent: 5.4%
epoch 1/50 - it 8/100 - loss: 2.466 - acc: 0.463
Memory info of GPU /physical_device:GPU:0: current: 0.24, peak: 2.61
Memory info of CPU: total:201.37Gb, available: 190.38Gb, percent: 5.5%
epoch 1/50 - it 9/100 - loss: 2.461 - acc: 0.519
Memory info of GPU /physical_device:GPU:0: current: 0.27, peak: 2.64
Memory info of CPU: total:201.37Gb, available: 190.11Gb, percent: 5.6%
epoch 1/50 - it 10/100 - loss: 2.455 - acc: 0.564
Memory info of GPU /physical_device:GPU:0: current: 0.29, peak: 2.67
Memory info of CPU: total:201.37Gb, available: 189.94Gb, percent: 5.7%
epoch 1/50 - it 11/100 - loss: 2.448 - acc: 0.603
Memory info of GPU /physical_device:GPU:0: current: 0.32, peak: 2.70
Memory info of CPU: total:201.37Gb, available: 189.67Gb, percent: 5.8%
epoch 1/50 - it 12/100 - loss: 2.437 - acc: 0.634
Memory info of GPU /physical_device:GPU:0: current: 0.35, peak: 2.72
Memory info of CPU: total:201.37Gb, available: 189.48Gb, percent: 5.9%
epoch 1/50 - it 13/100 - loss: 2.425 - acc: 0.662
Memory info of GPU /physical_device:GPU:0: current: 0.38, peak: 2.75
Memory info of CPU: total:201.37Gb, available: 189.21Gb, percent: 6.0%
epoch 1/50 - it 14/100 - loss: 2.408 - acc: 0.685
Memory info of GPU /physical_device:GPU:0: current: 0.41, peak: 2.78
Memory info of CPU: total:201.37Gb, available: 189.03Gb, percent: 6.1%
epoch 1/50 - it 15/100 - loss: 2.389 - acc: 0.705
Memory info of GPU /physical_device:GPU:0: current: 0.44, peak: 2.81
Memory info of CPU: total:201.37Gb, available: 188.79Gb, percent: 6.2%
epoch 1/50 - it 16/100 - loss: 2.369 - acc: 0.723
Memory info of GPU /physical_device:GPU:0: current: 0.46, peak: 2.84
Memory info of CPU: total:201.37Gb, available: 188.54Gb, percent: 6.4%
epoch 1/50 - it 17/100 - loss: 2.350 - acc: 0.739
Memory info of GPU /physical_device:GPU:0: current: 0.49, peak: 2.87
Memory info of CPU: total:201.37Gb, available: 188.37Gb, percent: 6.5%
epoch 1/50 - it 18/100 - loss: 2.332 - acc: 0.753
Memory info of GPU /physical_device:GPU:0: current: 0.52, peak: 2.89
Memory info of CPU: total:201.37Gb, available: 188.10Gb, percent: 6.6%
epoch 1/50 - it 19/100 - loss: 2.315 - acc: 0.765
Memory info of GPU /physical_device:GPU:0: current: 0.55, peak: 2.92
Memory info of CPU: total:201.37Gb, available: 187.87Gb, percent: 6.7%
epoch 1/50 - it 20/100 - loss: 2.300 - acc: 0.776
Memory info of GPU /physical_device:GPU:0: current: 0.58, peak: 2.95
Memory info of CPU: total:201.37Gb, available: 187.64Gb, percent: 6.8%
epoch 1/50 - it 21/100 - loss: 2.286 - acc: 0.786
Memory info of GPU /physical_device:GPU:0: current: 0.61, peak: 2.98
Memory info of CPU: total:201.37Gb, available: 187.49Gb, percent: 6.9%
epoch 1/50 - it 22/100 - loss: 2.274 - acc: 0.795
Memory info of GPU /physical_device:GPU:0: current: 0.63, peak: 3.01
Memory info of CPU: total:201.37Gb, available: 187.25Gb, percent: 7.0%
epoch 1/50 - it 23/100 - loss: 2.262 - acc: 0.804
Memory info of GPU /physical_device:GPU:0: current: 0.66, peak: 3.04
Memory info of CPU: total:201.37Gb, available: 186.97Gb, percent: 7.1%
epoch 1/50 - it 24/100 - loss: 2.251 - acc: 0.811
Memory info of GPU /physical_device:GPU:0: current: 0.69, peak: 3.06
Memory info of CPU: total:201.37Gb, available: 186.81Gb, percent: 7.2%
epoch 1/50 - it 25/100 - loss: 2.241 - acc: 0.818
Memory info of GPU /physical_device:GPU:0: current: 0.72, peak: 3.09
Memory info of CPU: total:201.37Gb, available: 186.59Gb, percent: 7.3%
epoch 1/50 - it 26/100 - loss: 2.232 - acc: 0.825
Memory info of GPU /physical_device:GPU:0: current: 0.75, peak: 3.12
Memory info of CPU: total:201.37Gb, available: 186.36Gb, percent: 7.5%
epoch 1/50 - it 27/100 - loss: 2.224 - acc: 0.831
Memory info of GPU /physical_device:GPU:0: current: 0.78, peak: 3.15
Memory info of CPU: total:201.37Gb, available: 186.09Gb, percent: 7.6%
epoch 1/50 - it 28/100 - loss: 2.216 - acc: 0.836
Memory info of GPU /physical_device:GPU:0: current: 0.80, peak: 3.18
Memory info of CPU: total:201.37Gb, available: 185.90Gb, percent: 7.7%
epoch 1/50 - it 29/100 - loss: 2.209 - acc: 0.841
Memory info of GPU /physical_device:GPU:0: current: 0.83, peak: 3.21
Memory info of CPU: total:201.37Gb, available: 185.65Gb, percent: 7.8%
epoch 1/50 - it 30/100 - loss: 2.202 - acc: 0.846
Memory info of GPU /physical_device:GPU:0: current: 0.86, peak: 3.23
Memory info of CPU: total:201.37Gb, available: 185.46Gb, percent: 7.9%
epoch 1/50 - it 31/100 - loss: 2.196 - acc: 0.851
Memory info of GPU /physical_device:GPU:0: current: 0.89, peak: 3.26
Memory info of CPU: total:201.37Gb, available: 185.24Gb, percent: 8.0%
epoch 1/50 - it 32/100 - loss: 2.190 - acc: 0.855
Memory info of GPU /physical_device:GPU:0: current: 0.92, peak: 3.29
Memory info of CPU: total:201.37Gb, available: 184.99Gb, percent: 8.1%
epoch 1/50 - it 33/100 - loss: 2.185 - acc: 0.859
Memory info of GPU /physical_device:GPU:0: current: 0.95, peak: 3.32
Memory info of CPU: total:201.37Gb, available: 184.75Gb, percent: 8.3%
epoch 1/50 - it 34/100 - loss: 2.180 - acc: 0.863
Memory info of GPU /physical_device:GPU:0: current: 0.97, peak: 3.35
Memory info of CPU: total:201.37Gb, available: 184.51Gb, percent: 8.4%
epoch 1/50 - it 35/100 - loss: 2.174 - acc: 0.866
Memory info of GPU /physical_device:GPU:0: current: 1.00, peak: 3.38
Memory info of CPU: total:201.37Gb, available: 184.36Gb, percent: 8.4%
epoch 1/50 - it 36/100 - loss: 2.170 - acc: 0.870
Memory info of GPU /physical_device:GPU:0: current: 1.03, peak: 3.40
Memory info of CPU: total:201.37Gb, available: 184.06Gb, percent: 8.6%
epoch 1/50 - it 37/100 - loss: 2.165 - acc: 0.873
Memory info of GPU /physical_device:GPU:0: current: 1.06, peak: 3.43
Memory info of CPU: total:201.37Gb, available: 183.89Gb, percent: 8.7%
epoch 1/50 - it 38/100 - loss: 2.161 - acc: 0.876
Memory info of GPU /physical_device:GPU:0: current: 1.09, peak: 3.46
Memory info of CPU: total:201.37Gb, available: 183.66Gb, percent: 8.8%
epoch 1/50 - it 39/100 - loss: 2.157 - acc: 0.879
Memory info of GPU /physical_device:GPU:0: current: 1.11, peak: 3.49
Memory info of CPU: total:201.37Gb, available: 183.42Gb, percent: 8.9%
epoch 1/50 - it 40/100 - loss: 2.153 - acc: 0.881
Memory info of GPU /physical_device:GPU:0: current: 1.14, peak: 3.52
Memory info of CPU: total:201.37Gb, available: 183.26Gb, percent: 9.0%
epoch 1/50 - it 41/100 - loss: 2.150 - acc: 0.884
Memory info of GPU /physical_device:GPU:0: current: 1.17, peak: 3.54
Memory info of CPU: total:201.37Gb, available: 183.02Gb, percent: 9.1%
```
```
",Performance Issue,Performance Issue
"LookupError when computing nested gradient with UpSampling2D **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7.4
- CUDA/cuDNN version: 10.1
- GPU model and memory: NVIDIA GeForce GTX 1070 8GB VRAM

**Describe the current behavior**
The provided code fails with the following error (see Colab link for full stacktrace):
`LookupError: gradient registry has no entry for: ResizeNearestNeighborGrad`

**Describe the expected behavior**
The GradientTape.gradient method should be able to compute the gradient.

**Standalone code to reproduce the issue**
[Google Colab standalone code](https://colab.research.google.com/drive/1eGMvXJxVvY7zb8OISkS1H3D90N0J380d?usp=sharing)

**Other info / logs**
I encountered the error while developing a GAN. I used Conv2DTranspose for upsampling at first but encountered artifacts. After changing Conv2DTranspose to a combination of UpSampling2D and Conv2D (as is common in GAN's) the inner gradient stopped working.",Runtime Error,Runtime Error
"enable the edit > show writing tools menu for editor <!-- ???? Do Not Delete This! feature_request_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

On macOS with Apple Intelligence enabled, most third-party applications display the ""Writing Tools"" menu option in the edit menu, providing writing suggestions and proofreading options. For those writing documentations in vscode, enabling this menu could prove beneficial.

e.g. here's the menu in google chrome,

![Image](https://github.com/user-attachments/assets/ffb50ba0-5e59-42a4-8309-567100980e0f)
",Dependency Issue,Dependency Issue
"Linux: writing elevated fails with high `ulimit` values <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.96.2 (code_1.96.2-1734607745_amd64.deb)
- OS Version: Ubuntu 24.10

Steps to Reproduce:

1. Open a file that is not writable by the user
2. Edit the file
3. Try to save it
4. Error pops up stating that not enough permissions
5. Retry as Sudo option appears
6. Enter password
7. Saving process running forever

Same problem has been discussed here: https://github.com/microsoft/vscode/issues/234311. 
Is there any progress or plan? Or any workaround ?",Security Vulnerability,Security Vulnerability
"Denial of Service in tf.raw_ops.Unstage ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?

Hi,
When using `tf.raw_ops.Unstage`, a `Denial of Service` was encountered under normal invocation, with all passed parameters meeting the requirements of the API documentation.
After a long wait, there was still no response.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tensor = tf.constant([1.0, 2.0, 3.0])
stage_op = tf.raw_ops.Stage(values=[tensor])
print(tensor.dtype)  # <dtype: 'float32'>

unstage_op = tf.raw_ops.Unstage(dtypes=[tensor.dtype])  # Hang

with tf.compat.v1.Session() as sess:
    print(""here"")
    sess.run(stage_op)
    result = sess.run(unstage_op)

print(result)  # Expected output: [1. 2. 3.]
```


### Relevant log output

```shell
2024-04-22 02:23:12.166249: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-22 02:23:12.166825: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-22 02:23:12.169923: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-22 02:23:12.215234: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-22 02:23:13.066132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-22 02:23:13.789282: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
<dtype: 'float32'>
```
",Security Vulnerability,Security Vulnerability
"[DevTools Bug]: Not seeing source / file path ### Website or app

[Any](https://reactjs.org/)

### Repro steps

Open react dev tools in browser
Select a react component from the tree
Notice that no Source (file path) is printed in the bottom right panel (see description screenshot in https://github.com/facebook/react/pull/17567)



### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",UI/UX Bug,UI/UX Bug
"Git - VSCode Git extension doesn't ask for remote user password, when trying to clone via ssh <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.93.0
- OS Version: Operating System: Manjaro Linux 
KDE Plasma Version: 6.1.5
KDE Frameworks Version: 6.5.0
Qt Version: 6.7.2
Kernel Version: 6.6.52-1-MANJARO (64-bit)
Graphics Platform: Wayland

I noticed that when you try to clone a repository with the Git extension via SSH and don't have public key authentication configured, VSCode doesn't ask for the remote user password.
Instead a not very meaningful error is shown:
![image](https://github.com/user-attachments/assets/859d0026-211a-4665-b015-7ce0802f4fce)
In the logs you can find more information: 
```
> git clone ssh://zzz2324188@example.com:4711/srv/pk/git/pk2425/VOB02/zzz2324188.git /home/laurenz/checkouts_pp/test/zzz2324188 --progress
/usr/lib/code/extensions/git/dist/askpass-main.js:1
(()=>{""use strict"";var e={7549:(e,s,r)=>{Object.defineProperty(s,""__esModule"",{value:!0}),s.IPCClient=void 0;const t=r(8611);s.IPCClient=class{constructor(e){this.handlerName=e;const s=process.env.VSCODE_GIT_IPC_HANDLE;if(!s)throw new Error(""Missing VSCODE_GIT_IPC_HANDLE"");this.ipcHandlePath=s}call(e){const s={socketPath:this.ipcHandlePath,path:`/${this.handlerName}`,method:""POST""};return new Promise(((r,n)=>{const o=t.request(s,(e=>{if(200!==e.statusCode)return n(new Error(`Bad status code: ${e.statusCode}`));const s=[];e.on(""data"",(e=>s.push(e))),e.on(""end"",(()=>r(JSON.parse(Buffer.concat(s).toString(""utf8"")))))}));o.on(""error"",(e=>n(e))),o.write(JSON.stringify(e)),o.end()}))}}},9896:e=>{e.exports=require(""fs"")},8611:e=>{e.exports=require(""http"")}},s={};function r(t){var n=s[t];if(void 0!==n)return n.exports;var o=s[t]={exports:{}};return e[t](o,o.exports,r),o.exports}var t={};(()=>{var e=t;Object.defineProperty(e,""__esModule"",{value:!0});const s=r(9896),n=r(7549);function o(e){console.error(""Missing or invalid credentials.""),console.error(e),process.exit(1)}!function(e){if(!process.env.VSCODE_GIT_ASKPASS_PIPE)return o(""Missing pipe"");if(!process.env.VSCODE_GIT_ASKPASS_TYPE)return o(""Missing type"");if(""https""!==process.env.VSCODE_GIT_ASKPASS_TYPE&&""ssh""!==process.env.VSCODE_GIT_ASKPASS_TYPE)return o(`Invalid type: ${process.env.VSCODE_GIT_ASKPASS_TYPE}`);if(""fetch""===process.env.VSCODE_GIT_COMMAND&&process.env.VSCODE_GIT_FETCH_SILENT)return o(""Skip silent fetch commands"");const r=process.env.VSCODE_GIT_ASKPASS_PIPE,t=process.env.VSCODE_GIT_ASKPASS_TYPE,i=""https""===t?e[2]:e[3];let c,a,p;""https""===t&&(c=e[4].replace(/^[""']+|[""':]+$/g,"""")),""ssh""===t&&(/passphrase/i.test(i)?a=e[6]?.replace(/^[""']+|[""':]+$/g,""""):(c=e[6].replace(/^[""']+|[""':]+$/g,""""),p=e[15])),new n.IPCClient(""askpass"").call({askpassType:t,request:i,host:c,file:a,fingerprint:p}).then((e=>{s.writeFileSync(r,e+""\n""),setTimeout((()=>process.exit(0)),0)})).catch((e=>o(e)))}(process.argv)})();var n=exports;for(var o in t)n[o]=t[o];t.__esModule&&Object.defineProperty(n,""__esModule"",{value:!0})})();
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            

TypeError: Cannot read properties of undefined (reading 'replace')
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1748
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1967
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1983
    at Object.<anonymous> (/usr/lib/code/extensions/git/dist/askpass-main.js:1:2089)
    at Module._compile (node:internal/modules/cjs/loader:1373:14)
    at Module._extensions..js (node:internal/modules/cjs/loader:1432:10)
    at Module.load (node:internal/modules/cjs/loader:1215:32)
    at Module._load (node:internal/modules/cjs/loader:1031:12)
    at c._load (node:electron/js2c/node_init:2:13801)
    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:189:12)

Node.js v20.16.0
Permission denied, please try again.
/usr/lib/code/extensions/git/dist/askpass-main.js:1
(()=>{""use strict"";var e={7549:(e,s,r)=>{Object.defineProperty(s,""__esModule"",{value:!0}),s.IPCClient=void 0;const t=r(8611);s.IPCClient=class{constructor(e){this.handlerName=e;const s=process.env.VSCODE_GIT_IPC_HANDLE;if(!s)throw new Error(""Missing VSCODE_GIT_IPC_HANDLE"");this.ipcHandlePath=s}call(e){const s={socketPath:this.ipcHandlePath,path:`/${this.handlerName}`,method:""POST""};return new Promise(((r,n)=>{const o=t.request(s,(e=>{if(200!==e.statusCode)return n(new Error(`Bad status code: ${e.statusCode}`));const s=[];e.on(""data"",(e=>s.push(e))),e.on(""end"",(()=>r(JSON.parse(Buffer.concat(s).toString(""utf8"")))))}));o.on(""error"",(e=>n(e))),o.write(JSON.stringify(e)),o.end()}))}}},9896:e=>{e.exports=require(""fs"")},8611:e=>{e.exports=require(""http"")}},s={};function r(t){var n=s[t];if(void 0!==n)return n.exports;var o=s[t]={exports:{}};return e[t](o,o.exports,r),o.exports}var t={};(()=>{var e=t;Object.defineProperty(e,""__esModule"",{value:!0});const s=r(9896),n=r(7549);function o(e){console.error(""Missing or invalid credentials.""),console.error(e),process.exit(1)}!function(e){if(!process.env.VSCODE_GIT_ASKPASS_PIPE)return o(""Missing pipe"");if(!process.env.VSCODE_GIT_ASKPASS_TYPE)return o(""Missing type"");if(""https""!==process.env.VSCODE_GIT_ASKPASS_TYPE&&""ssh""!==process.env.VSCODE_GIT_ASKPASS_TYPE)return o(`Invalid type: ${process.env.VSCODE_GIT_ASKPASS_TYPE}`);if(""fetch""===process.env.VSCODE_GIT_COMMAND&&process.env.VSCODE_GIT_FETCH_SILENT)return o(""Skip silent fetch commands"");const r=process.env.VSCODE_GIT_ASKPASS_PIPE,t=process.env.VSCODE_GIT_ASKPASS_TYPE,i=""https""===t?e[2]:e[3];let c,a,p;""https""===t&&(c=e[4].replace(/^[""']+|[""':]+$/g,"""")),""ssh""===t&&(/passphrase/i.test(i)?a=e[6]?.replace(/^[""']+|[""':]+$/g,""""):(c=e[6].replace(/^[""']+|[""':]+$/g,""""),p=e[15])),new n.IPCClient(""askpass"").call({askpassType:t,request:i,host:c,file:a,fingerprint:p}).then((e=>{s.writeFileSync(r,e+""\n""),setTimeout((()=>process.exit(0)),0)})).catch((e=>o(e)))}(process.argv)})();var n=exports;for(var o in t)n[o]=t[o];t.__esModule&&Object.defineProperty(n,""__esModule"",{value:!0})})();
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            

TypeError: Cannot read properties of undefined (reading 'replace')
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1748
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1967
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1983
    at Object.<anonymous> (/usr/lib/code/extensions/git/dist/askpass-main.js:1:2089)
    at Module._compile (node:internal/modules/cjs/loader:1373:14)
    at Module._extensions..js (node:internal/modules/cjs/loader:1432:10)
    at Module.load (node:internal/modules/cjs/loader:1215:32)
    at Module._load (node:internal/modules/cjs/loader:1031:12)
    at c._load (node:electron/js2c/node_init:2:13801)
    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:189:12)

Node.js v20.16.0
Permission denied, please try again.
/usr/lib/code/extensions/git/dist/askpass-main.js:1
(()=>{""use strict"";var e={7549:(e,s,r)=>{Object.defineProperty(s,""__esModule"",{value:!0}),s.IPCClient=void 0;const t=r(8611);s.IPCClient=class{constructor(e){this.handlerName=e;const s=process.env.VSCODE_GIT_IPC_HANDLE;if(!s)throw new Error(""Missing VSCODE_GIT_IPC_HANDLE"");this.ipcHandlePath=s}call(e){const s={socketPath:this.ipcHandlePath,path:`/${this.handlerName}`,method:""POST""};return new Promise(((r,n)=>{const o=t.request(s,(e=>{if(200!==e.statusCode)return n(new Error(`Bad status code: ${e.statusCode}`));const s=[];e.on(""data"",(e=>s.push(e))),e.on(""end"",(()=>r(JSON.parse(Buffer.concat(s).toString(""utf8"")))))}));o.on(""error"",(e=>n(e))),o.write(JSON.stringify(e)),o.end()}))}}},9896:e=>{e.exports=require(""fs"")},8611:e=>{e.exports=require(""http"")}},s={};function r(t){var n=s[t];if(void 0!==n)return n.exports;var o=s[t]={exports:{}};return e[t](o,o.exports,r),o.exports}var t={};(()=>{var e=t;Object.defineProperty(e,""__esModule"",{value:!0});const s=r(9896),n=r(7549);function o(e){console.error(""Missing or invalid credentials.""),console.error(e),process.exit(1)}!function(e){if(!process.env.VSCODE_GIT_ASKPASS_PIPE)return o(""Missing pipe"");if(!process.env.VSCODE_GIT_ASKPASS_TYPE)return o(""Missing type"");if(""https""!==process.env.VSCODE_GIT_ASKPASS_TYPE&&""ssh""!==process.env.VSCODE_GIT_ASKPASS_TYPE)return o(`Invalid type: ${process.env.VSCODE_GIT_ASKPASS_TYPE}`);if(""fetch""===process.env.VSCODE_GIT_COMMAND&&process.env.VSCODE_GIT_FETCH_SILENT)return o(""Skip silent fetch commands"");const r=process.env.VSCODE_GIT_ASKPASS_PIPE,t=process.env.VSCODE_GIT_ASKPASS_TYPE,i=""https""===t?e[2]:e[3];let c,a,p;""https""===t&&(c=e[4].replace(/^[""']+|[""':]+$/g,"""")),""ssh""===t&&(/passphrase/i.test(i)?a=e[6]?.replace(/^[""']+|[""':]+$/g,""""):(c=e[6].replace(/^[""']+|[""':]+$/g,""""),p=e[15])),new n.IPCClient(""askpass"").call({askpassType:t,request:i,host:c,file:a,fingerprint:p}).then((e=>{s.writeFileSync(r,e+""\n""),setTimeout((()=>process.exit(0)),0)})).catch((e=>o(e)))}(process.argv)})();var n=exports;for(var o in t)n[o]=t[o];t.__esModule&&Object.defineProperty(n,""__esModule"",{value:!0})})();
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            

TypeError: Cannot read properties of undefined (reading 'replace')
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1748
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1967
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1983
    at Object.<anonymous> (/usr/lib/code/extensions/git/dist/askpass-main.js:1:2089)
    at Module._compile (node:internal/modules/cjs/loader:1373:14)
    at Module._extensions..js (node:internal/modules/cjs/loader:1432:10)
    at Module.load (node:internal/modules/cjs/loader:1215:32)
    at Module._load (node:internal/modules/cjs/loader:1031:12)
    at c._load (node:electron/js2c/node_init:2:13801)
    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:189:12)

Node.js v20.16.0
zzz2324188@example.com: Permission denied (publickey,password).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
```

I am aware, that the optimal solution would be to copy the public key to the remote and let SSH handle the authentication, or use HTTPS and the Git credential manager (which I have installed, but apparently it doesn't open up for SSH connections).
But there can be situations where none of this workarounds are viable.

Steps to Reproduce:

1. F1 
2. Git Clone
3. Enter something like this `ssh://user@example.com:4711/path/to/your/repo.git


Edit: The same error occurs for all other Git actions that involve the remote, like pushing, pulling, etc.",Security Vulnerability,Security Vulnerability
"Inconsistency in SyncBatchNormalization/BatchNormalization(synchronized=True) Results during Distributed Training on CPUs <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

_No response_

### Current Behaviour?

I am reporting an issue encountered during the distributed training of a model with `SyncBatchNormalization` layers on CPUs. The issue presents as a relatively large inconsistency between the results produced when training the same model on a single device versus multiple devices.

We've prepared a reproduction script demonstrating this issue. We set the weights of the `SyncBatchNormalization`/`BatchNormalization` layer to reproduce this bug. After conducting a one-step training, the predictions from the identical model trained on a single CPU and two CPUs exhibited relatively large differences (measured by L-infinity distance, Linf = 0.00074467063). As a comparison, we build another model with exactly the same architecture but removing the `SyncBatchNormalization` layer, and the prediction difference from this model trained on a single CPU and two CPUs is Linf = 1.0131771e-09.

Another comparison experiment is on GPUs. We trained the same model containing `SyncBatchNormalization` layer on a single GPU and two GPUs. The Linf of modelâs prediction result is 2.5331974e-07. Itâs much smaller than 1CPU vs 2CPU (Linf = 0.00074467063). This is weird because I would expect the CPU executions to be more deterministic than GPUs, and I would expect larger inconsistencies in the GPU runs if the inconsistencies are caused by variance.

To ensure a controlled environment, we used the same training data for one-step training on both setups, and then evaluated the model. To guarantee that the distributed trainings are expected to produce the same results given different number of devices, we use `MirroredStrategy`, keep the global batch size the same, and use `tf.keras.losses.Reduction.AUTO` reduction. To make the difference more apparent given a limited amount of training data, we deliberately chose a relatively high learning rate (lr=10).

It's noteworthy that `SyncBatchNormalization` has been deprecated as of TensorFlow version 2.12. Therefore, we also conducted the same experiment using TensorFlow's nightly build (2.13.0-dev20230428), replacing `SyncBatchNormalization()` with `BatchNormalization(synchronized=True)`. This experiment still manifested the same inconsistency. The prediction from the model containing batchnorm layer trained on a single CPU and two CPUs exhibits a substantial difference of Linf = 0.00010111928, while for the model removing batchnorm layer itâs Linf = 6.212488e-36.


### Standalone code to reproduce the issue

```shell
For TF 2.11.0 CPU experiments, the model contains `SyncBatchNormalization` layer:
https://colab.research.google.com/drive/11EseXxq_uHweY7omt7JCr1mc-42Kf1H5?usp=sharing

For TF 2.11.0 GPU experiments, the model contains `SyncBatchNormalization` layer (fix seed to generate same inputs):
https://colab.research.google.com/drive/1m_gCWzb_OKVTYAMMs8YbdKUYM3aINNPV?usp=sharing 

For TF nightly 2.13.0-dev20230428, the model contains `BatchNormalization(synchronized=True)` layer:
https://colab.research.google.com/drive/1N-aXPcfckVb8fPDSmghMlzBOFEGq6RzM?usp=sharing
```


### Relevant log output

For TF 2.11.0:

```shell
contain syncbatchnorm:  True
1CPU vs 2CPU: 0.00074467063

contain syncbatchnorm:  False
1CPU vs 2CPU: 1.0131771e-09

contain syncbatchnorm:  True
1GPU vs 2GPU: 2.5331974e-07
```

For TF nightly 2.13.0-dev20230428:

```shell
contain syncbatchnorm:  True
1CPU vs 2CPU: 0.00010111928

contain syncbatchnorm:  False
1CPU vs 2CPU: 6.212488e-36
```

</details>",UI/UX Bug,UI/UX Bug
"Low contrast on test error marker I really like the test error marker but feel the contrast between background and text is too little:

![Image](https://github.com/user-attachments/assets/68527f6f-f98b-458f-9e5e-301e8d93c4e5)

@meganrogge can probably advise",UI/UX Bug,UI/UX Bug
"[TFLite C++] Signature calculating CategoricalCrossentropy loss produces wrong result <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I've created a simple model in Python (TF version 2.10) and converted it for tflite. The model has two signatures, one for inference and other for training. When I run those signatures in Python, everything works correctly, I get good inference result and good training loss. When I load the converted tflite model with the C++ TFLite API (built from source, from branch r2.13) and run those signatures: inference works as intended, training works as intended (the accuracy on the test set is steadily rising), but the reported loss is totally random. At first I thought that loss might be accumulated since it is rising to five digits, but that is not the case since it rises and falls in a random fashion. It seems like there is some bug in the ops used for CategoricalCrossentropy C++ TFLite implementation.

I've tried building tensorflow from r2.12 and r2.13 and I get the same behavior. I've tried r2.10 also but then I couldn't even run the signatures with C++ TFLite API, I was getting bunch of segmentation faults. I couldn't find anywhere the documentation on what ops for backward prop are available in C++ TFLite API, maybe some of those which are used in CategoricalCrossentropy loss calculation are not yet available, or there is a bug in their implementation.

### Standalone code to reproduce the issue

Here is a Python code I am using to create model with signatures:
```
IMG_SIZE = 28

class Model(tf.Module):
    def __init__(self):
        self.model = tf.keras.Sequential([
            tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE), name='flatten'),
            tf.keras.layers.Dense(
                units=10,
                kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                bias_initializer=tf.keras.initializers.Ones(),
                name='dense'
            ),
        ])

        opt = tf.keras.optimizers.SGD(learning_rate=0.1)
        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
        self.model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])

    # The `train` function takes a batch of input images and labels.
    @tf.function(input_signature=[
        tf.TensorSpec([32, IMG_SIZE, IMG_SIZE], tf.float32),
        tf.TensorSpec([32, 10], tf.float32),
    ])
    def train(self, x, y):
        with tf.GradientTape() as tape:
            prediction = self.model(x)
            loss = self.model.loss(y, prediction)
        gradients = tape.gradient(loss, self.model.trainable_variables)
        self.model.optimizer.apply_gradients(
            zip(gradients, self.model.trainable_variables))
        result = {""loss"": loss}
        return result

    @tf.function(input_signature=[
        tf.TensorSpec([1, IMG_SIZE, IMG_SIZE], tf.float32),
    ])
    def infer(self, x):
        logits = self.model(x)
        probabilities = tf.nn.softmax(logits, axis=-1)
        return {
            ""output"": probabilities,
            ""logits"": logits
        }
```

And here is the C++ code I am using to run the tflite model:
```
std::unique_ptr<tflite::FlatBufferModel> model =
    tflite::FlatBufferModel::BuildFromFile(tflite_model_path);
if (model == nullptr)
{
    std::cout << ""Failed to load model"" << std::endl;
    return;
}

tflite::ops::builtin::BuiltinOpResolver resolver;
tflite::InterpreterBuilder builder(*model, resolver);
std::unique_ptr<tflite::Interpreter> interpreter;
builder(&interpreter);
if (interpreter == nullptr)
{
    std::cout << ""Failed to create interpreter"" << std::endl;
    return;
}

if (interpreter->AllocateTensors() != kTfLiteOk)
{
    std::cout << ""Failed to alocate interpreter tensors"" << std::endl;
    return;
}

tflite::SignatureRunner* train_runner = interpreter->GetSignatureRunner(""train"");

TfLiteTensor* input_data_tensor = train_runner->input_tensor(train_runner->input_names()[0]);
float* input_data = input_data_tensor->data.f;
TfLiteTensor* input_labels_tensor = train_runner->input_tensor(train_runner->input_names()[1]);
float* input_labels = input_labels_tensor->data.f;

// Here I fill in the input data and labels, code redacted for brevity.

if (train_runner->Invoke() != kTfLiteOk)
{
    std::cout << ""Error invoking train interpreter signature"" << std::endl;
    return;
}

const TfLiteTensor* output_tensor = train_runner->output_tensor(train_runner->output_names()[0]);
float* output = output_tensor->data.f;
std::cout << ""Training finished with loss: "" << output[0] << std::endl;
```

Please let me know if you need more details, or full source code.


### Relevant log output

Here are the losses from batch to batch, as you can see they are too high and pretty much random. I repeat: the model is training correctly which I can see because the accuracy on the test set is steadily rising, so these loss values do not make sense.
```
Training of batch 1 finished with loss: 172.813
Training of batch 2 finished with loss: 30406.2
Training of batch 3 finished with loss: 35372.7
Training of batch 4 finished with loss: 30955.9
Training of batch 5 finished with loss: 30645.5
Training of batch 6 finished with loss: 39069.4
Training of batch 7 finished with loss: 25181.5
Training of batch 8 finished with loss: 28106.7
Training of batch 9 finished with loss: 12969.1
Training of batch 10 finished with loss: 3079.69
Training of batch 11 finished with loss: 3693.12
Training of batch 12 finished with loss: 3314.77
Training of batch 13 finished with loss: 4591.12
Training of batch 14 finished with loss: 5880.76
Training of batch 15 finished with loss: 5654.75
Training of batch 16 finished with loss: 10133.1
Training of batch 17 finished with loss: 9301.94
Training of batch 18 finished with loss: 11654.5
Training of batch 19 finished with loss: 11827.8
Training of batch 20 finished with loss: 22028.1
Training of batch 21 finished with loss: 8553.58
```

</details>",Runtime Error,Logical Bug
"Git - Git blame editor decoration appears before the folding ... one The ... should be before the git one

![Image](https://github.com/user-attachments/assets/8a7704f1-4a36-40db-94b4-4d80d448f9bc)
",UI/UX Bug,UI/UX Bug
"Memory leak in Conv2D/Activation on GPU **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary, the standard docker distribution
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0
- GPU model and memory: GeForce RTX 2070, 8GB

**Describe the current behavior**
I upgraded to TF 2.4.0 from TF 2.1.2, and training a very simple convolutional network, which worked fine in 2.1.2, started running out of memory during training. I distilled a simple reproducible example that demonstrates the issue. Each training epoch consumes about 50MB of additional memory and, given enough epochs, it grows to infinity (or 32 GB in my case). It only occurs on GPU, the same thing runs fine on CPU.

**Describe the expected behavior**
Memory not growing, or growing only very little

**Standalone code to reproduce the issue**
```
import gc
import os
import psutil
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Conv2D, Flatten, BatchNormalization, Activation

physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)


input_tensor = tf.keras.layers.Input(shape=(512,64,1))

x = Conv2D(filters=32, kernel_size=(5,5), strides=(2,2), padding='same')(input_tensor)
# Commented out on purpose - see Note 1 below
# x = BatchNormalization()(x)
x = Activation('relu')(x)

x = Conv2D(filters=64, kernel_size=(4,4), strides=(2,2), padding='same')(x)
# Commented out on purpose - see Note 1 below
# x = BatchNormalization()(x)
x = Activation('relu')(x)

x = Conv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same')(x)
# Commented out on purpose - see Note 1 below
# x = BatchNormalization()(x)
x = Activation('relu')(x)

x = Conv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same')(x)
# Commented out on purpose - see Note 1 below
# x = BatchNormalization()(x)
x = Activation('relu')(x)

x = Flatten()(x)

x = Dense(5, activation='sigmoid')(x)

model = tf.keras.Model(inputs=input_tensor, outputs=x)


train_x = np.random.random((2048, 512, 64, 1))
train_y = np.random.random((2048, 5))

model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam())

process = psutil.Process(os.getpid())

for i in range(50):
    model.fit(train_x, train_y, epochs=1, batch_size=32, verbose=0)
    gc.collect()
    print(i, process.memory_info().rss // 1000000)
```

**Note 1**
Now, if you uncomment the BatchNormalization() layers creation, the memory problem disappears. So, it is somehow caused by the Activation layer following immediately the Conv2D

**Note 2**
The memory problem also occurs if I train multiple epochs in a single fit() call, such as 
```
model.fit(train_x, train_y, epochs=50, batch_size=32)
```
I used the for loop only to be able to call garbage collection and print the memory.

**Note 3**
A Conv2D layer with activation embedded in it, such as
```
Conv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same', activation='relu')
```
also causes the memory issue



",Performance Issue,Runtime Error
"[Compiler Bug]: Performance - `useEffect` without dependencies should be left alone ### What kind of issue is this?

- [X] React Compiler core (the JS output is incorrect, or your app works incorrectly after optimization)
- [ ] babel-plugin-react-compiler (build issue installing or using the Babel plugin)
- [ ] eslint-plugin-react-compiler (build issue installing or using the eslint plugin)
- [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script)

### Link to repro

https://playground.react.dev/#N4Igzg9grgTgxgUxALhAMygOzgFwJYSYAEUYCAwgBYCGmA5gmAKIBuCMAngCp4C2CACgCURYAB1iROITA4iAbRbUANgBoiZHADUVAXSIBeEmQDKOajkHDDAPiIBZC5QB0MWgBMIvYUIDcEiSJjBCY0NARcAWsDO3FJIM0dZQFHHBc3TE9vIT9AogBfXMw8mAQcWGIlZX9MfIDMDGx8QmCAdTcABySoBABBMAAlBDRrOKDpTFkiKsNgqloGZjZOHn5hGvGZOVK0WdIEIZGqorz90PDI6Ni8oJ3nOFhSzDkjKo2Ck8lS8phiHZq6pgQPkgA

### Repro steps

I was playing around with a small repo I had and noticed something similar to #30782. (Essentially, I was tracking a variable so that the callback identity could be stable to avoid rapid-fire rerenders of the whole page)

Wrapping it in a useEffect works and probably more correct, but it generates redundant memoization:

```js
function useWrapValueAsRef() {
  const $ = _c(2);
  const val = useChangesEveryTime();
  const ref = useRef(val);
  let t0;
  if ($[0] !== val) {
    t0 = () => {
      ref.current = val;
    };
    $[0] = val;
    $[1] = t0;
  } else {
    t0 = $[1];
  }
  useEffect(t0);
  return ref;
}
```

where I would expect it to just leave everything alone.

Ignoring the contrived source of the variable (in the real app I'm testing with, it probably changes a couple times on page load), it does the check every time, which is redundant because the useEffect will always run the latest closure anyways. I'm not sure if this is because of some sort of optimization in useEffect but I don't see why that would be the case.

I don't imagine this to be a huge issue, but it's technically less performant as the code size is bigger and it uses 2 array slots.

### How often does this bug happen?

Every time

### What version of React are you using?

React Playground (19-RC)",Performance Issue,Performance Issue
"Automatic bandwidth calculation valid only for normalized data ### Describe the bug

`sklearn.neighbors.KernelDensity` supports automatic (optimal) bandwidth calculation via `bandwidth = 'silverman'` and `bandwidth = 'scott'`. The algorithm computes the appropriate observation-weighted bandwidth factors (proportional to nobs^0.2) but does not adjust for the standard deviation or interquartile range of the dataset. Roughly, the algorithm should scale the dataset's standard error by the algorithmic bandwidth factors.

See, e.g., [Wikipedia](https://en.wikipedia.org/wiki/Kernel_density_estimation#A_rule-of-thumb_bandwidth_estimator). The implementation in `scipy.stats._kde` is correct.

### Steps/Code to Reproduce
```python
import matplotlib.pyplot as plot
import numpy as np
from sklearn.neighbors import KernelDensity
from scipy.stats import gaussian_kde

data = np.random.normal( scale = 0.01, size = 100 )

#
# 1. sklearn (auto)
#
kd_sklearn_auto = KernelDensity( kernel = 'gaussian', bandwidth = 'silverman' )
kd_sklearn_auto.fit( np.reshape( data, ( -1, 1 ) ) )

#
# 2. sklearn (manual)
#
kd_sklearn_manual = KernelDensity( kernel = 'gaussian', bandwidth = 0.9 * np.std( data ) / len( data ) ** ( 1 / 5 ) )
kd_sklearn_manual.fit( np.reshape( data, ( -1, 1 ) ) )

#
# 3. scipy
#
kd_scipy = gaussian_kde( data, bw_method = 'silverman' )

#
# 4. show the difference
#
xs = np.arange( start = -0.05, stop = 0.05, step = 1e-4 )
plot.plot( xs, np.exp( kd_sklearn_auto.score_samples( np.reshape( xs, ( -1, 1 ) ) ) ), label = 'KDE SKLearn (auto)' )
plot.plot( xs, np.exp( kd_sklearn_manual.score_samples( np.reshape( xs, ( -1, 1 ) ) ) ), label = 'KDE SKLearn (manual)' )
plot.plot( xs, kd_scipy.pdf( xs ), label = 'KDE SciPy' )
plot.hist( data, label = 'Data' )
plot.legend()
plot.show()
```

### Expected Results

Automatic SKLearn bandwidth curve should approximately match SciPy bandwidth curve, roughly the shape of the underlying data histogram.

### Actual Results

Automatic SKLearn bandwidth curve generates a flat PDF.

### Versions

```shell
System:
    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]
executable: /local_disk0/.ephemeral_nfs/envs/pythonEnv-67c47d19-3f15-49a2-ab8f-bcf25a2bc29f/bin/python
   machine: Linux-5.15.0-1038-azure-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.2.2
          pip: 21.2.4
   setuptools: 58.0.4
        numpy: 1.20.3
        scipy: 1.7.1
       Cython: 0.29.24
       pandas: 1.3.4
   matplotlib: 3.4.3
       joblib: 1.2.0
threadpoolctl: 2.2.0

Built with OpenMP: True

threadpoolctl info:
       filepath: /databricks/python3/lib/python3.9/site-packages/numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so
         prefix: libopenblas
       user_api: blas
   internal_api: openblas
        version: 0.3.13.dev
    num_threads: 6
threading_layer: pthreads
   architecture: Haswell

       filepath: /local_disk0/.ephemeral_nfs/envs/pythonEnv-67c47d19-3f15-49a2-ab8f-bcf25a2bc29f/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
         prefix: libgomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 6

       filepath: /databricks/python3/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-085ca80a.3.9.so
         prefix: libopenblas
       user_api: blas
   internal_api: openblas
        version: 0.3.9
    num_threads: 6
threading_layer: pthreads
   architecture: Haswell
```
",Syntax Error,Syntax Error
"""Save File"" dialog is not visible in KDE after upgrade to 1.94.0 <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.94.0
```
Version: 1.94.0
Commit: d78a74bcdfad14d5d3b1b782f87255d802b57511
Date: 2024-10-02T13:08:12.626Z
Electron: 30.5.1
ElectronBuildId: 10262041
Chromium: 124.0.6367.243
Node.js: 20.16.0
V8: 12.4.254.20-electron.0
OS: Linux x64 6.8.0-44-generic snap
```
- OS Version: [K]Ubuntu 24.04.1 LTS

Steps to Reproduce:

1.  Open a new (`Ctrl+N`)
2. Type in something
3. Try to save it (`Ctrl+S`)
4. No dialog is shown, but VSCode main window starts behaving like it is - e.g. if you click `Help->About` in meny, nothing happens.
5. `ps aux|grep kdialog` shows that there is a process is running with  command:
```
kdialog --attach=94371844 --title=Save As --getsavefilename /home/myuser/Documents/??????????????/asdasdas All Files (*.txt)|Plain Text (*.apib)|API Blueprint (*.bat *.cmd)|Batch (*.bib)|BibTeX (*.c *.i)|C (*.cake *.cs *.csx)|C# (*.c++ *.c++m *.cc *.ccm *.cpp *.cppm *.cxx *.cxxm *.hh *.hpp)|C++ (*.clj *.cljc *.cljs *.cljx *.clojure *.edn)|Clojure (*.cmake)|CMake (*.code-snippets)|Code Snippets (*.coffee *.cson *.iced)|CoffeeScript (*.css)|CSS (*.csv)|CSV (*.cu *.cuh)|CUDA C++ (*.dart)|Dart (*.diff *.patch *.rej)|Diff (*.disasm)|Disassembly (*.containerfile *.dockerfile)|Dockerfile (*.editorconfig)|EditorConfig (*.ejs)|EJS (*.fs *.fsi *.fsscript *.fsx)|F# (*.go)|Go (*.gotmpl *.tmpl)|Go Template File (*.gradle)|Gradle (*.gradle.kts)|Gradle Kotlin DSL (*.gql *.graphql *.graphqls)|GraphQL (*.gradle *.groovy *.gvy *.jenkinsfile *.nf)|Groovy (*.handlebars *.hbs *.hjs)|Handlebars (*.cginc *.compute *.fx *.fxh *.hlsl *.hlsli *.psh *.vsh)|HLSL (*.asp *.aspx *.htm *.html *.jshtm *.jsp *.mdoc *.shtml *.xht *.xhtml)|HTML (*.eslintignore *.git-blame-ignore-revs *.gitignore *.gitignore_global *.npmignore)|Ignore (*.ini)|Ini (*.class *.jav *.java)|Java (*.properties)|Java Properties (*.cjs *.es6 *.js *.mjs *.pac)|JavaScript (*.jsx)|JavaScript JSX (*.j2 *.jinja2)|Jinja (*.bowerrc *.css.map *.har *.js.map *.jscsrc *.jslintrc *.json *.jsonld *.ts.map *.webmanifest)|JSON (*.jsonl)|JSON Lines (*.babelrc *.code-workspace *.eslintrc *.eslintrc.json *.hintrc *.jsfmtrc *.jshintrc *.jsonc *.language-configuration.json *.swcrc)|JSON with Comments (*.jl)|Julia (*.jmd)|Julia Markdown (*.ctx *.ltx *.tex)|LaTeX (*.less)|Less (*.*.log.? *.log)|Log (*.lua)|Lua (*.mak *.mk)|Makefile (*.markdn *.markdown *.md *.mdown *.mdtext *.mdtxt *.mdwn *.mkd *.workbook)|Markdown (*.nix)|Nix (*.m)|Objective-C (*.mm)|Objective-C++ (*.PL *.pl *.pm *.pod *.psgi *.t)|Perl (*.ctp *.php *.php4 *.php5 *.phtml)|PHP (*.css *.pcss *.postcss)|PostCSS (*.ps1 *.psd1 *.psm1 *.psrc *.pssc)|PowerShell (*.prisma)|Prisma (*.cfg *.conf *.directory *.editorconfig *.gitattributes *.gitconfig *.gitmodules *.npmrc *.properties *.repo)|Properties (*.jade *.pug)|Pug (*.cpy *.gyp *.gypi *.ipy *.py *.pyi *.pyt *.pyw *.rpy)|Python (*.r *.rhistory *.rprofile *.rt)|R (*.nqp *.p6 *.pl6 *.pm6 *.raku *.rakudoc *.rakumod *.rakutest)|Raku (*.cshtml *.razor)|Razor (*.rast)|ra_syntax_tree (*.rst)|reStructuredText (*.erb *.gemspec *.podspec *.rake *.rb *.rbi *.rbx *.rjs *.ru)|Ruby (*.rs)|Rust (*.scss)|SCSS (*.code-search)|Search Result (*.shader)|ShaderLab (*.bash *.bash_aliases *.bash_login *.bash_logout *.bash_profile *.bashrc *.ebuild *.eclass *.profile *.sh)|Shell Script (*.dsql *.q *.sql)|SQL (*.svg)|SVG (*.swift)|Swift (*.tf)|Terraform (*.tfdeploy.hcl)|Terraform Deployment (*.tfmock.hcl)|Terraform Mock (*.tfstack.hcl)|Terraform Stack (*.tftest.hcl)|Terraform Test (*.tfvars)|terraform-vars (*.bbx *.cbx *.cls *.sty)|TeX (*.toml)|TOML (*.tab *.tsv)|TSV (*.cts *.mts *.ts)|TypeScript (*.tsx)|TypeScript JSX (*.bas *.brs *.vb *.vba *.vbs)|Visual Basic (*.vue)|vue (*.wasm *.wat)|WebAssembly Text Format (*.ascx *.atom *.axaml *.axml *.bpmn *.cpt *.csl *.csproj *.xml *.xsd)|XML (*.xsl *.xslt)|XSL (*.cff *.eyaml *.eyml *.yaml *.yaml-tmlanguage *.yaml-tmpreferences *.yaml-tmtheme *.yml)|All Files (*)
```
Killing it by PID unfreezes main vscode window.

Disabling all extensions (with `--diable-extensions` option on command line) didn't help.

Reverting VSCode snap to `38c31bc7` (which is 1.93.1) fixes things.",Logical Bug,Logical Bug
"tritonserver preload trt plugin got warning message and many core files : Failed to compile generated PTX with ptxas. Falling back to compilation by driver. ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.2

### Custom code

No

### OS platform and distribution

linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tritonserver preload trt plugin got warning message and many core  dump files
`
2024-08-16 10:09:14.975649: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:16.033970: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:16.701031: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:17.498157: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:18.328719: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
`

![image](https://github.com/user-attachments/assets/c9cf824b-f27e-456d-aa43-af501aae0694)

I have an ensmble model, the first part is the tf model, the second part is the trt model. I have a trt plugin, and I load it as LD_PRELOAD. It won't be a problem if I load the two models separately. But when I load them at the same time this warning comes up and produces a lot of coredump files. Why is that? I don't understand how the trt plugin will affect tf

### Standalone code to reproduce the issue

```shell
LD_PRELOAD=/app/lib/ops/libtrtplugin.so --model-repository=/opt/model-repo-copy --model-control-mode=explicit --load-model=first_model --load-model=second_model  --load-model=ensmble_model  --log-verbose=0 --http-port=xxx--grpc-port=xxx --metrics-port=xxx --backend-config=tensorflow,version=2 --backend-config=tensorrt,version-compatible=true --disable-auto-complete-config 
```


### Relevant log output

_No response_",Dependency Issue,Dependency Issue
"TreeView nesting performance TreeView has some serious performance problems at high nesting levels
(unrelated to https://github.com/microsoft/vscode/issues/232263)

Steps to Reproduce:

1. create JSON file with snippet
```json
[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]
```
2. open OutLine
3. scroll down to the bottom
4. freeze ??

![Image](https://github.com/user-attachments/assets/eff89294-d304-4072-a5ed-8ad71789502d)

performance is also exaggerated more by StickyScroll `""workbench.tree.enableStickyScroll"": true`
this also affects the dropdown Breadcrumbs view

dup https://github.com/microsoft/vscode/issues/205840

cc @alexr00 

Does this issue occur when all extensions are disabled?: Yes

- VS Code Version: 1.96.0
- OS Version: Windows 11
",Performance Issue,Performance Issue
"Bug: onPointerDown not called when rendered in 'display: contents' root `onPointerDown` listeners are not called in iOS Safari when a React application is rendered within an element that has the `display: contents` CSS rule. This issue seems to be specific to iOS, as it works fine on macOS and Windows.

This bug was discovered through a report in the Astro framework. A user noted that a React component became unclickable after a View Transition, which Astro emulates using JavaScript. Upon investigation, I traced the issue to elements with the `display: contents` rule.

The issue may be related to the `trapClickOnNonInteractiveElement` hack. When a fake `pointerdown` listener is added, the bug disappears and everything works correctly. However, I haven't found any documentation on this behavior, so this is just a hypothesis.

React version: 18.3.1

## Steps To Reproduce

Render React component with <button onPointerDown={() => console.log('called')}>Click me</button> in an element with `display: contents` applied. Try clicking the button in iOS Safari.

See [this page](https://v3ron.github.io/react-pointer-down-ios-bug/) for reproduction.
See [this repository](https://github.com/V3RON/react-pointer-down-ios-bug) for source code.

## The current behavior

`onPointerDown` listener is not called

## The expected behavior

`onPointerDown` listener should be called
",UI/UX Bug,UI/UX Bug
"The result of tf.quantization.fake_quant_with_min_max_args is inconsistent between CPU and GPU ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.2

### GPU model and memory

_No response_

### Current behavior?

For the same input, `tf.quantization.fake_quant_with_min_max_args ` produces inconsistent results on CPU and GPU.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = tf.constant([0.0, 1.0, 2.0, 3.0, 4.0])

with tf.device('cpu:0'):
    quantized_input_data = tf.quantization.fake_quant_with_min_max_args(input_data, min=-6.0, max=6.0)
    print(quantized_input_data)

with tf.device('gpu:0'):
    quantized_input_data = tf.quantization.fake_quant_with_min_max_args(input_data, min=-6.0, max=6.0)
    print(quantized_input_data)
```


### Relevant log output

```shell
tf.Tensor([0.        0.9882353 2.0235295 3.0117648 4.       ], shape=(5,), dtype=float32)
tf.Tensor([0.        0.9882353 1.9764706 3.0117648 4.       ], shape=(5,), dtype=float32)
```
",Performance Issue,Performance Issue
"Command Center should shrink when not enough space When Vscode is in English the menu bar contains the items ""file"", ""edit"", ""selection"", ""view"", ""go"", ""run"", ""terminal"" and ""help"".
![Image](https://github.com/user-attachments/assets/ca2be6d6-b0a0-4bec-83f8-a3d495a9152e)
When I change the display language to Spanish it shows like this:
![Image](https://github.com/user-attachments/assets/46893226-fb7a-4bcc-8ccf-86cc0e0b7c45)

Best regards",UI/UX Bug,UI/UX Bug
"Numpy Array Error when Training LogisticRegressionCV ### Describe the bug

When I attempt to train LogisticRegressionCV, I get the error: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.

The inputs to LogisticRegressionCV are:
- X = output of StandardScaler.fit_transform, with a shape of (24, 12) and dtype of float64
- y = array with shape (24, ) and dtype of int64

I checked that there are not null or infinite values in either array.

### Steps/Code to Reproduce

```
data_for_color_training = training_data.loc[training_data[""IdentCode""] == color]
x_train = data_for_color_training.loc[:, (data_for_color_training.columns.str.startswith(""dx"") | data_for_color_training.columns.str.startswith(""dy"") | data_for_color_training.columns.str.startswith(""dz""))]
y_train = data_for_color_training[""Grade""]
			
#--------------Edit specific pre-processing and model here-----------------------------
scaler = StandardScaler()
X_train = scaler.fit_transform(x_train)
modelTrained = LogisticRegressionCV(max_iter=10000).fit(X_train, y_train)
```

### Expected Results

The variable modelTrained will output a LogisticRegressionCV model.

### Actual Results
```traceback
Traceback (most recent call last):
  File ""C:\Users\...\PythonScripts\UserDefinedModel.py"", line 22, in train_predict_by_colorcode
    modelTrained = LogisticRegressionCV(max_iter=10000).fit(X_train, y_train)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_logistic.py"", line 1912, in fit
    coefs_paths = np.reshape(
                  ^^^^^^^^^^^
  File ""<__array_function__ internals>"", line 200, in reshape
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 298, in reshape
    return _wrapfunc(a, 'reshape', newshape, order=order)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 54, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 43, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
                     ^^^^^^^^^^^^
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 298, in reshape
    return _wrapfunc(a, 'reshape', newshape, order=order)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 54, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 43, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
                     ^^^^^^^^^^^^
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.
```
### Versions

```shell
System:
    python: 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]
executable: C:\Users\...\AppData\Local\Programs\Python\Python311\python.exe
   machine: Windows-10-10.0.19044-SP0

Python dependencies:
      sklearn: 1.2.2
          pip: 22.3.1
   setuptools: 65.5.0
        numpy: 1.24.1
        scipy: 1.10.0
       Cython: None
       pandas: 1.5.3
   matplotlib: 3.6.3
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\.libs\libopenblas64__v0.3.21-gcc_10_3_0.dll
        version: 0.3.21
threading_layer: pthreads
   architecture: Haswell
    num_threads: 16

       user_api: openmp
internal_api: openmp
         prefix: vcomp
       filepath: C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None
    num_threads: 16

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\scipy.libs\libopenblas-802f9ed1179cb9c9b03d67ff79f48187.dll
        version: 0.3.18
threading_layer: pthreads
   architecture: Haswell
    num_threads: 16
```
",Syntax Error,Syntax Error
"`tf.data.Dataset.from_tensor_slices` allocates GPU RAM ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Passing a numpy array to `tf.data.Dataset.from_tensor_slices()` attempts to allocate the dataset as a tensor on the GPU device, and raises an exception if there is not enough GPU RAM available. 

This only started with TF 2.17.0. In all previous TF versions, all `tf.data.Dataset` operations were always pinned to the CPU.

To reproduce, create a numpy array larger than can fit on the GPU, and attempt to create a `tf.data.Dataset` from it.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

gpu_ram_gb = 12 # adjust for size of GPU 

gb = gpu_ram_gb+1; dtype = ""float64""
size = (gb * 1024**3) // tf.dtypes.as_dtype(dtype).size

x = np.zeros((size,), dtype = dtype)

tf.data.Dataset.from_tensor_slices(x)
```


### Relevant log output

```shell
2024-07-12 08:20:42.771788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-12 08:20:42.784893: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-12 08:20:42.788889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-12 08:20:42.798174: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-12 08:20:43.492876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1720786850.494498   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.527964   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.531351   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.535502   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.538786   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.541878   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.710210   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.711607   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.712908   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-07-12 08:20:50.714170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 34 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5
2024-07-12 08:20:50.715606: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 13958643712 exceeds 10% of free system memory.
2024-07-12 08:21:05.997032: W external/local_tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 13.00GiB (rounded to 13958643712)requested by op _EagerConst
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2024-07-12 08:21:05.997054: I external/local_tsl/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc
2024-07-12 08:21:05.997064: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (256): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997072: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997079: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1024): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997087: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997093: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997100: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997107: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997114: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997121: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997128: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997135: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997142: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997148: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997155: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997169: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997176: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997183: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997190: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997197: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1062] Bin for 13.00GiB was 256.00MiB, Chunk State: 
2024-07-12 08:21:05.997219: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: 
2024-07-12 08:21:05.997225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 0B
2024-07-12 08:21:05.997232: I external/local_tsl/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 0 memory_limit_: 35651584 available bytes: 35651584 curr_region_allocation_bytes_: 35651584
2024-07-12 08:21:05.997241: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: 
Limit:                        35651584
InUse:                               0
MaxInUse:                            0
NumAllocs:                           0
MaxAllocSize:                        0
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2024-07-12 08:21:05.997248: W external/local_tsl/tsl/framework/bfc_allocator.cc:494] <allocator contains no memory>

Traceback (most recent call last):
  File ""/home/tomasz/github/rstudio/keras3/test.py"", line 16, in <module>
    tf.data.Dataset.from_tensor_slices(x)
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 826, in from_tensor_slices
    return from_tensor_slices_op._from_tensor_slices(tensors, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py"", line 25, in _from_tensor_slices
    return _TensorSliceDataset(tensors, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py"", line 33, in __init__
    element = structure.normalize_element(element)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/util/structure.py"", line 134, in normalize_element
    ops.convert_to_tensor(t, name=""component_%d"" % i, dtype=dtype))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/ops.py"", line 713, in convert_to_tensor
    return tensor_conversion_registry.convert(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 234, in convert
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_tensor_conversion.py"", line 29, in _constant_tensor_conversion_function
    return constant_op.constant(v, dtype=dtype, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper
    return op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 276, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 289, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 301, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
```
",Runtime Error,Logical Bug
"Error in using multi-label classification in partial_fit() in OvR - StackOverflow Question: https://stackoverflow.com/questions/42280439/multi-label-out-of-core-learning-for-text-data-valueerror-on-partial-fit

#### Description
When using OneVsRestClassifier() with partial_fit() method, errors are thrown. When using fit(), no errors are thrown and everything works.

#### Steps/Code to Reproduce
```
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.multiclass import OneVsRestClassifier
import numpy as np

categories = ['a','b','c']
X = [""This is a test"", ""This is another attempt"", ""And this is a test too!""]
Y = [['a', 'b'],['b', 'c'],['a', 'b']] 

mlb = MultiLabelBinarizer(classes=categories)
vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,         non_negative=True)
clf = OneVsRestClassifier(MultinomialNB(alpha=0.01))

X_train = vectorizer.fit_transform(X)
Y_train = mlb.fit_transform(Y)

- Case1
clf.partial_fit(X_train, Y_train, categories)
- Case2
clf.partial_fit(X_train, Y_train, mlb.transform(Y))
```
#### Description of code

- Case1   Using classes=categories without transforming 
```partial_fit(X_train, Y_train, classes=categories)```

        ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

- Case2   Using classes=mlb.transform(categories) i.e. after transforming from same multilabelbinarizer
```partial_fit(X_train, Y_train, classes=mlb.transform(categories))```

         ValueError: The object was not fitted with multilabel input.

#### Expected Results
No error is thrown as when using fit().

#### Actual Results
- Case1
> Traceback
 (most recent call last):
  File ""/path_to_module/Check.py"", line 18, in <module>
    clf.partial_fit(X_train, Y_train, categories)
  File ""/library/python2.7/dist-packages/sklearn/multiclass.py"", line 260, in partial_fit
    if np.setdiff1d(y, self.classes_):
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

- Case2
> Traceback
 (most recent call last):
  File ""/path_to_module/Check.py"", line 18, in <module>
    clf.partial_fit(X_train, Y_train, mlb.transform(Y))
  File ""/library/python2.7/dist-packages/sklearn/multiclass.py"", line 265, in partial_fit
    Y = self.label_binarizer_.transform(y)
  File ""/library/python2.7/dist-packages/sklearn/preprocessing/label.py"", line 329, in transform
    raise ValueError(""The object was not fitted with multilabel""
ValueError: The object was not fitted with multilabel input.

#### Observation
- In Case1, the error is because https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/multiclass.py#L260 is returning an array of booleans whereas it expects a single boolean value hence the error. But couldnt find what to do about it. How to pass Y or classes into it.

- In Case2, the error occurs because partial_fit() calls the _check_partial_fit_first_call() function which sets the clf.classes_ in a different way using unique_labels() as seen here https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/multiclass.py#L308.
These unique labels are passed to clf.label_binarizer_ in this line https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/multiclass.py#L258, which leads to it assuming type of targets as 'multiclass' whereas actual type of target is multilabel, hence this error.
The fit() method handles classes_ in a different way (doesnt use unique_labels) and hence everything works correctly 

#### Versions
Linux-3.16.0-77-generic-x86_64-with-Ubuntu-14.04-trusty
('Python', '2.7.6 (default, Oct 26 2016, 20:30:19) \n[GCC 4.8.4]')
('NumPy', '1.12.0')
('SciPy', '0.18.1')
('Scikit-Learn', '0.18.1')

",Runtime Error,Runtime Error
"Compilation fails on Ubuntu 20.04 when using TensorRT 8.  **System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.4.1, 2.5, etc
- Python version: 3.8
- Installed using virtualenv? pip? conda?: no, built from source
- Bazel version (if compiling from source): 3.1 (for TF 2.4.1), 3.7.2 (for TF 2.5.0-rcx)
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- CUDA/cuDNN version: Cuda 11.1, cudnn8 (8.0.5.39-1+cuda11.1) or Cuda-11-2, libcudnn 8.1.1, 8.2, 
- GPU model and memory: GTX-1080ti
- TensorRT (crucial): 8.0.0-1+cuda11.0, or 8.0.0-1+cuda11.3

**Describe the problem**
When compiling with support for TensorRT 8 (via libnvinfer8), compilation fails (log is below). 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
When configuring the build, make sure you build with TensorRT support, and make sure TensorRT version 8 is selected. Build TF as usual. Compilation will fail. 

If you install  TensorRT version 7 manually (from debs available for Ubuntu 18.04), compilation will complete just fine.

**Any other info / logs**
Relevant error: 
`C++ compilation of rule '//tensorflow/compiler/tf2tensorrt:tensorrt_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command`

`In file included from bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:54,
                 from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:
bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInferRuntime.h:2264:51: note: from previous declaration 'nvinfer1::IPluginRegistry* getPluginRegistry() noexcept'
 2264 | extern ""C"" TENSORRTAPI nvinfer1::IPluginRegistry* getPluginRegistry() noexcept;`

Full log here: 
[gesdm-tf2.5.0rc3-error.txt](https://github.com/tensorflow/tensorflow/files/6469944/gesdm-tf2.5.0rc3-error.txt)

",Runtime Error,Runtime Error
"`shellIntegration.ps1` adds additional errors to `$Error` variable when errors occur ## Env info

- VS Code Version: 1.95.3
- OS Version:  Windows 11 24H2 x64
- PowerShell version: v7.4.6 x64
- Extension: PowerShell v2024.0

## Reproduce

In a PowerShell Extension terminal, do:

```pwsh
# Clear error
$Error.Clear()

# Invoke request known to fail
$null = Invoke-RestMethod -Method 'Post' -Uri 'https://graph.microsoft.com' -Body @{'test' = [string]'test'} 

# Count errors, should be 1 but is 3
$Error.Count
```

I then found out by looking at `$Error[0]` that one of the errors were added by VSCode `shellIntegration.ps1`.

```pwsh
PS > $Error[0].InvocationInfo

MyCommand             : Select-Object
BoundParameters       : {}
UnboundArguments      : {}
ScriptLineNumber      : 218
OffsetInLine          : 64
HistoryId             : 55
ScriptName            : 
Line                  :             $global:Error[0] | Where-Object { $_ -ne $null } | Select-Object -ExpandProperty InvocationInfo

Statement             : Select-Object -ExpandProperty InvocationInfo
PositionMessage       : At line:218 char:64
                        +  bject { $_ -ne $null } | Select-Object -ExpandProperty InvocationInfo
                        +                            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
PSScriptRoot          : 
PSCommandPath         : 
InvocationName        : Select-Object
PipelineLength        : 0
PipelinePosition      : 0
ExpectingInput        : False
CommandOrigin         : Internal
DisplayScriptPosition :

PS > $Error[0].ScriptStackTrace

at Update-PoshErrorCode, <No file>: line 218
at <ScriptBlock>, <No file>: line 272
at Global:Prompt, C:\Program Files\Microsoft VS Code\resources\app\out\vs\workbench\contrib\terminal\common\scripts\shellIntegration.ps1: line 96

PS >
```

Could find `shellIntegration.ps1`, but I can't find that invocation line in it.

* <https://github.com/microsoft/vscode/blob/main/src/vs/workbench/contrib/terminal/common/scripts/shellIntegration.ps1>

Doing the same in ""vanilla"" `pwsh -NoProfile` only adds one error to the `$Error` variable:

```pwsh
PowerShell 7.4.6
PS > $null = Invoke-RestMethod -Method 'Post' -Uri 'https://graph.microsoft.com' -Body @{'test' = [string]'test'}
Invoke-RestMethod: Response status code does not indicate success: 405 (Method Not Allowed).
PS > $Error.Count
1
PS >
```",Runtime Error,Syntax Error
"Tooltip for Close Button in Dialog Box is Obscured by the Dialog Itself <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
## Title:
Tooltip for Close Button in Dialog Box is Obscured by the Dialog Itself

This issue affects the usability of the dialog box, as users cannot see the tooltip for the close button, which may lead to confusion or difficulty in closing the dialog.

## Steps to Reproduce:

![Image](https://github.com/user-attachments/assets/3ed6c046-095f-4cbd-af74-bd2e4d442393)

<img width=""146"" alt=""Image"" src=""https://github.com/user-attachments/assets/266c2b55-76d9-42c0-b9a3-a68bfa31b11e"" />

1. Perform an action in VS Code that triggers a dialog box to appear.
2. Hover the mouse over the close button (typically the 'X' in the top-right corner) of the dialog box.
3. Observe that the tooltip for the close button is displayed but is obscured by the dialog box itself.

## Expected Behavior:
The tooltip should be displayed above or outside the dialog box, ensuring it is fully visible to the user.

## Actual Behavior:
The tooltip is rendered behind the dialog box, making it partially or completely invisible.

## Additional Information:
- **Does this issue occur when all extensions are disabled?**: Yes

I also conducted tests using the latest codebase of the VS Code project, and the aforementioned bug can be reproduced as well.

- **VS Code Version**:
  - Version: 1.96.2 (system setup)
  - Commit: fabdb6a30b49f79a7aba0f2ad9df9b399473380f
  - Date: 2024-12-19T10:22:47.216Z
  - Electron: 32.2.6
  - ElectronBuildId: 10629634
  - Chromium: 128.0.6613.186
  - Node.js: 20.18.1
  - V8: 12.8.374.38-electron.0
  - OS: Windows_NT x64 10.0.26100

## OS Version:
Windows 11 24H2
",UI/UX Bug,UI/UX Bug
"process.report.getReport is very slow when called in Extension Host, blocks most functionality <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->

Version: 1.97.0-insider (user setup)
Commit: cc14c75c962284dfd2e2493dd487e3b81fb15d23
Date: 2025-01-23T10:37:55.126Z
Electron: 32.2.7
ElectronBuildId: 10660205
Chromium: 128.0.6613.186
Node.js: 20.18.1
V8: 12.8.374.38-electron.0
OS: Windows_NT x64 10.0.26100

Steps to Reproduce:

1. Clone https://github.com/jakebailey/vscode-getreport-issue
2. Run the extension
3. Run the ""Hello World"" command
4. Attempt to do anything with the editor

![Image](https://github.com/user-attachments/assets/8204f381-14e1-4088-bb33-74208c27ee9f)

I discovered this as part of https://github.com/dprint/dprint-vscode/pull/99; for months my editor has been unresponsive at startup, unable to hover or even copy and paste now that there are paste providers, and this particular call is the culprit.

If I run Node directly, or call VS Code ""as node"", it does not exhibit these problems. My impression is that this has to do with VS Code patching all of the networking calls, since unfortunately `getReport` calls to the network (even though effectively all users use it to check if libc is musl or not).

Related: https://github.com/nodejs/node/issues/55576
",Performance Issue,Performance Issue
"Diff viewer: misaligned lines with inlays + word wrap <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No
Yes
<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.96.4
- OS Version: Windows 11

Steps to Reproduce:
1. Make a code change in a reasonably long file, in a language that has inlay hints, so there's a diff
2. Open the diff viewer, ensure word wrap is enabled (<kbd>Alt</kbd><kbd>Z</kbd>), and scroll around throughout the file
3. Watch as the lines get misaligned between the added and removed side
4. Use <kbd>Ctrl</kbd><kbd>Alt</kbd> (with `editor.inlayHints.enabled` set to `onUnlessPressed`) to temporarily hide the inlays and notice how they also get misaligned

Video of the bug in action:

[Video (70 seconds)](https://files.keavon.com/-/HarshAcclaimedEuropeanfiresalamander/capture_40_.mp4), showing both the scrolling-caused misalignment and triggering it with toggling inlays.

Description:

This is an issue that has frustrated my many times a day for years, so I'm finally filling an issue for it.

The diff editor is meant to keep your left (removals) and right (additions) side's lines aligned. But scrolling throughout the code file, or turning on/off inlay hints, causes a misalignment to occur and persist. The result is that the red removals and green additions for a line can be considerably misaligned.

This happens under the conditions that word wrap is enabled (so a line of code may wrap across multiple displayed lines on the left, right, or both sides) and inlays are enabled (so the width of a line of code may become elongated).

My theory is that scrolling within the document causes the LSP to generate more inlays within the newly viewed areas, changing the line widths which changes the wrapping which changes the number of wrapped lines occupied by one line of code; and it does so differently on the right from the left, since the LSP generates no inlays for the left side.

The workaround for this is to either type something to make it recalculate, or briefly toggle word wrap by tapping <kbd>Alt</kbd><kbd>Z</kbd> twice. But a proper fix for this bug would be a real quality of life improvement.",UI/UX Bug,UI/UX Bug
"tf.io.gfile.glob does not list all files in a Google Cloud Storage bucket **System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): ?
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): /
- CUDA/cuDNN version: /
- GPU model and memory: /

**Describe the current behavior**
When listing file with `tf.io.gfile.glob` not all images are returned. It seems it is not resolving the folders recursively. 
When using the same path with gsutils  we get the correct image count.


**Describe the expected behavior**
When using the same gs:// path with **gsutils** we get the correct amount of images. 

**Code to reproduce the issue**
In order to reproduce the behavior I prepared a Google Bucket with the following structure. The bucket is public accessible, please feel free to use it to reproduce the behavior on your end: `gs://tensorflow-issue-reproduction`

![level0](https://user-images.githubusercontent.com/1991664/69040135-93375e80-09ed-11ea-927f-da445d2bae10.png)
![level1](https://user-images.githubusercontent.com/1991664/69040139-94688b80-09ed-11ea-9e19-28af47c3bcee.png)
![level2](https://user-images.githubusercontent.com/1991664/69040146-96cae580-09ed-11ea-8894-c69a82acbbaa.png)
![level3](https://user-images.githubusercontent.com/1991664/69040150-9894a900-09ed-11ea-94dc-49fb2d9b3b9b.png)

In summary we have 4 jpg images nested in different folder levels.

TensorFlow 2 code to reproduce
```
files = tf.io.gfile.glob('gs://tensorflow-issue-reproduction/**/*.jpg')
print('file count: ', len(files))
# found files 1
```

gsutil command which works properly
```
gsutil du gs://tensorflow-issue-reproduction/**/*.jpg | wc -l
# found files 4
```

**Other info / logs**
/

Best regards
Sascha
",Runtime Error,Runtime Error
"Devtools v4 does not work with Firefox's private window <!--
  Note: if the issue is about documentation or the website, please file it at:
  https://github.com/reactjs/reactjs.org/issues/new
-->

**Do you want to request a *feature* or report a *bug*?**

* bug
* This issue has been reported in https://github.com/facebook/react-devtools/issues/1383

**What is the current behavior?**


Steps to Reproduce is here:

1. Environments are:
2. Open the page which uses react with a private window.
3. Open Firefox's devtools.

Actual Result is:

* react devtools' _component_ pane show `Unable to find React on the page.`
* From about:debugging, we can see the below messsage:

```
SecurityError: Permission denied to access property ""container"" on cross-origin object main.js:51:305877
    Kl moz-extension://56db142d-3d36-b04e-91ca-a7504c7708a5/build/main.js:51
    apply self-hosted:4417
    applySafeWithoutClone resource://gre/modules/ExtensionCommon.jsm:588
    asyncWithoutClone resource://gre/modules/ExtensionCommon.jsm:2400
```



**What is the expected behavior?**

react devtools work

**Which versions of React, and which browser / OS are affected by this issue? Did this work in previous versions of React?**

* Firefox 68
* react devtools v4.0.5
* react v16.9",Security Vulnerability,Security Vulnerability
"Unable to hide TPUs ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12

### Custom code

No

### OS platform and distribution

Kaggle Notebooks

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Unable to hide TPUs from TensorFlow. The consequence of this is that if we want to use JAX along with TensorFlow, only one of them will be able to initialize the TPU system, and the other will fail. We won't be able to use `tfds`, `tf.image` or any TF operation per se if we can't hide TPUs from being used by TF. I want all these operations to run on CPU only, and leverage JAX for TPU. Here is the code to test it on a TPU machine:

```python
import tenorflow as tf

tf.config.set_visible_devices([], device_type=""TPU_SYSTEM"")
tf.config.set_visible_devices([], device_type=""TPU"")

print(tf.config.list_logical_devices())

# output:
# [LogicalDevice(name='/device:CPU:0', device_type='CPU'),
#  LogicalDevice(name='/device:TPU_SYSTEM:0', device_type='TPU_SYSTEM'),
#  LogicalDevice(name='/device:TPU:0', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:1', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:2', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:3', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:4', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:5', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:6', device_type='TPU'),
# LogicalDevice(name='/device:TPU:7', device_type='TPU')]
```

This also doesn't work:

```python
physical_devices = tf.config.list_physical_devices()
tf.config.set_visible_devices(physical_devices[0], 'CPU')
```

### Standalone code to reproduce the issue

```shell
import tenorflow as tf

tf.config.set_visible_devices([], device_type=""TPU_SYSTEM"")
tf.config.set_visible_devices([], device_type=""TPU"")

print(tf.config.list_logical_devices())

# This also doesn't work:
physical_devices = tf.config.list_physical_devices()
tf.config.set_visible_devices(physical_devices[0], 'CPU')



### Relevant log output

_No response_",Performance Issue,Performance Issue
"`tf.raw_ops.LoopCond` aborts in Debug build ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.LoopCond` aborts in Debug build(compiled with `--config=dbg`).
In release build, it does not crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# it aborts only when compiled with `--config=dbg`
tf.raw_ops.LoopCond(input=True)
```


### Relevant log output

```shell
2024-02-18 02:37:24.039192: F tensorflow/core/graph/graph_partition.cc:644] Check failed: !frame_name.empty() 
Aborted (core dumped)
```
",Dependency Issue,Dependency Issue
"Runtime error when renaming an existing folder into a nested folder structure Repro:

1. Create folder `foo`
2. Click on `foo` in explorer
3. F2 (space on mac)
4. Change to `foo/bar`
5. Enter, ?? runtime error

![Image](https://github.com/user-attachments/assets/c1582eef-a371-4abe-98d8-32991d74988c)

Version: 1.97.0-insider (user setup)
Commit: 33fc5a94a3f99ebe7087e8fe79fbe1d37a251016
Date: 2025-02-04T18:50:12.241Z
Electron: 32.2.7
ElectronBuildId: 10660205
Chromium: 128.0.6613.186
Node.js: 20.18.1
V8: 12.8.374.38-electron.0
OS: Windows_NT x64 10.0.26100",UI/UX Bug,Dependency Issue
"[DevTools Bug]: Components tab freezes after inspecting ### Website or app

https://dev.permaplant.net

### Repro steps

1. Login
2. Go to Maps and create or open one
3. Look for the TimelinePicker component in the components tab
4. Click on it to inspect it

After that my components tab freezes and sometimes my RAM fills up endlessly. 

(If you need authentication, just contact me)

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Syntax Error,Syntax Error
"`LogOutputChannel` LogLevel property race condition if Global Log Level and Output Panel Log Level are not the same <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes/No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.96-insiders
- OS Version: Windows/Linux (Tested locally and in Codespaces)

## Reproduction
1. Start this branch in Codespaces and use Launch task Run Extension [Isolated Profile]
https://github.com/JustinGrote/vscode-extension-issues/tree/issue/logOutputWindowTraceChange
1. Change log level to something other than info
1. Choose ""Restart Extension Host""

Relevant Code:
https://github.com/JustinGrote/vscode-extension-issues/blob/issue/logOutputWindowTraceChange/src/extension.ts

## Expected
A `LogOutputChannel` starts with the `logLevel` that the user preference has specified

## Actual
Always starts at `Info` and gets set sometime later, however logs seem to filter normally. 

EDIT: This only happens if the default log level and the output pane log level are not the same, and ""Set as Default"" has not been used to modify the `argv.json`. If default log level and output pane log level are the same, the ondidChange does not fire later and the startup log level is correct.

https://github.com/user-attachments/assets/f7e53748-d024-40cf-a8cb-b47eb7418a0d

## Relevance
I have a custom logger that relies on checking the logLevel and do a noop if it does not meet the required level.



## Potential Fixes
- Update the property before processing any further log calls
- Provide a promise that can be awaited for when the LogOutputChannel is ready to receive logs at the user preferenced log level.
- I considered waiting for an onDidChangeLogLevel but this does not trigger if the user preference specified `info`, so a spurious onDidChangeLogLevel for info as well would also suffice.",Logical Bug,Logical Bug
"NuSVC argument `class_weight` is not used ### Describe the bug

Like `SVC`, the class `NuSVC` takes argument `class_weight`. However, it looks like this argument is not used. After a quick look at the libsvm C code within sklearn as well as [libsvm's original documentation](https://www.csie.ntu.edu.tw/~cjlin/libsvm/), this seems to be expected: ""`wi` set the parameter C of class i to weight*C, for C-SVC"". I suggest that this argument should be removed from `NuSVC`'s constructor and from the documentation.

### Steps/Code to Reproduce

```python
from sklearn.svm import SVC, NuSVC

X = [[1., 2, 3], [0, 5, 2]]
y = [-1, 1]

NuSVC(verbose=True).fit(X, y).dual_coef_

optimization finished, #iter = 0
C = 2.587063
obj = 1.293532, rho = 0.000000
nSV = 2, nBSV = 0
Total nSV = 2
Out: [LibSVM]array([[-1.29353162,  1.29353162]])

SVC(C=2.587063, verbose=True).fit(X, y).dual_coef_

optimization finished, #iter = 1
obj = -1.293532, rho = 0.000000
nSV = 2, nBSV = 0
Total nSV = 2
Out: [LibSVM]array([[-1.29353162,  1.29353162]])

NuSVC(class_weight={-1:1.5, 1:.2}, verbose=True).fit(X, y).dual_coef_

optimization finished, #iter = 0
C = 2.587063
obj = 1.293532, rho = 0.000000
nSV = 2, nBSV = 0
Total nSV = 2
Out: [LibSVM]array([[-1.29353162,  1.29353162]])

SVC(C=2.587063, class_weight={-1:1.5, 1:.2}, verbose=True).fit(X, y).dual_coef_

optimization finished, #iter = 1
obj = -0.827860, rho = -0.600000
nSV = 2, nBSV = 1
Total nSV = 2
Out: [LibSVM]array([[-0.5174126,  0.5174126]])


NuSVC(class_weight={-1:0, 1:0}).fit(X, y).dual_coef_
Out: array([[-1.29353162,  1.29353162]])

SVC(class_weight={-1:0, 1:0}).fit(X, y).dual_coef_
Out: array([], shape=(1, 0), dtype=float64)
```


### Expected Results

As in the case of no `class_weight`, `NuSVM` should give the same `dual_coef_` as an `SVC` with the same `C`.
Also `class_weight={-1:0, 1:0}` should give the ""empty"" result.

### Actual Results

In all cases above `NuSVM` with class weight behaves exactly as when no weights are given.

### Versions

```shell
System:
    python: 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03)  [GCC 11.3.0]
executable: .../bin/python3.9
   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.5.2
          pip: 23.0.1
   setuptools: 67.6.0
        numpy: 2.0.2
        scipy: 1.13.1
       Cython: None
       pandas: None
   matplotlib: 3.9.2
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: blis
    num_threads: 1
         prefix: libblis
       filepath: .../lib/libblis.so.4.0.0
        version: 0.9.0
threading_layer: pthreads
   architecture: skx

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libgomp
       filepath: .../lib/libgomp.so.1.0.0
        version: None
```
",Logical Bug,Logical Bug
"[DevTools Bug]: Source map error: URL: react_devtools_backend_compact.js.map ### Website or app

https://github.com/avenida714/alec-synth/blob/dccf984d89ae44558c70ff93dfa03b1227d5df5b/src/App.jsx#L63-L64C17

Link found on the vite getting started page: https://stackblitz.com/edit/vitejs-vite-zff6zx?file=src%2FApp.jsx&terminal=dev

### Repro steps

â ï¸ This is not my code (not public) â ï¸. While I searched for this error I stumble upon this repo. I had this issue when adding the plugin for first time.

1. Open the website
2. Open console
3. See the error

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Dependency Issue,Dependency Issue
"Issue Reporter should warn against including private or sensitive info <!-- ???? Do Not Delete This! feature_request_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

Since issues created by the Issue Reporter can end up in public repos, the issue reporter should warn users not to include private or sensitive data in the title or details of the issue.

While there is a notice to ""[review the guidance we provide](https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions)"", it doesn't mention anything about private or sensitive information.

![Image](https://github.com/user-attachments/assets/1a03128e-0a5a-46ca-9f63-872ca0a5b1fd)


",Security Vulnerability,Security Vulnerability
"Issue reporter fails without telling the user why if content is too large Repro:

1. Fill issue reporter with 100k of context
2. Try create an issue on GitHub.

I'm also seeing this FYI

![Image](https://github.com/user-attachments/assets/8fdaf5e9-6482-46ec-b0f6-a8fe3d496f1a)

Trying to do an internal copilot issue report which adds a bunch of context.",Logical Bug,Logical Bug
"`randomized_svd` incorrect for complex valued matrices ### Describe the bug

The `randomized_svd` utility function accepts complex valued inputs without error, but the result is inconsistent with `scipy.linalg.svd`.

### Steps/Code to Reproduce

```python
import numpy as np
from scipy import linalg
from sklearn.utils.extmath import randomized_svd

rng = np.random.RandomState(42)
X = rng.randn(100, 20) + 1j * rng.randn(100, 20)

_, s, _ = linalg.svd(X)
_, s2, _ = randomized_svd(X, n_components=5)

print(""s:"", s[:5])
print(""s2:"", s2[:5])
```

### Expected Results

I expected the singular values to be numerically close.

### Actual Results

```
s: [19.81481515 18.69019042 17.62107998 17.23689681 16.3148512 ]
s2: [11.25690754  9.97157079  9.01542947  8.06160863  7.54068744]
```

### Versions

```shell
System:
    python: 3.11.4 (main, Jul  5 2023, 08:40:20) [Clang 14.0.6 ]
executable: /Users/clane/miniconda3/bin/python
   machine: macOS-13.7-arm64-arm-64bit

Python dependencies:
      sklearn: 1.7.dev0
          pip: 25.0
   setuptools: 65.5.0
        numpy: 2.2.2
        scipy: 1.15.1
       Cython: 3.0.11
       pandas: 2.2.3
   matplotlib: 3.10.0
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libscipy_openblas
       filepath: /Users/clane/Projects/misc/scikit-learn/.venv/lib/python3.11/site-packages/numpy/.dylibs/libscipy_openblas64_.dylib
        version: 0.3.28
threading_layer: pthreads
   architecture: neoversen1

       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libscipy_openblas
       filepath: /Users/clane/Projects/misc/scikit-learn/.venv/lib/python3.11/site-packages/scipy/.dylibs/libscipy_openblas.dylib
        version: 0.3.28
threading_layer: pthreads
   architecture: neoversen1

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: /opt/homebrew/Cellar/libomp/19.1.3/lib/libomp.dylib
        version: None
```",Syntax Error,Syntax Error
"terminal.integrated.shellIntegration.enabled causes terminal to become progressively unresponsive Type: <b>Performance Issue</b>

- Press command-j to open the terminal.
- Type a command that outputs 10000 lines of text, for instance:
```
python -c ""for i in range(10000): print(f'This is a long long long long long line of text {i}')""
```
- Press the up arrow to display the previous long command at the shell prompt in the terminal and then hold the backspace key to delete this new command entirely without executing it.

Observed: the cursor progresses very slowly to the left and becomes almost stuck with very high CPU usage when holding the backspace key.

Workaround: setting `terminal.integrated.shellIntegration.enabled` to `false`, and starting a new `zsh` shell (even without terminating the previous shell) makes the terminal responsive again.

I am using the zsh shell with some custom prompt and other zsh custom config.

I found about the culprit in this stackoverlow answer:
https://stackoverflow.com/a/78624068/163740

VS Code version: Code 1.94.2 (Universal) (384ff7382de624fb94dbaf6da11977bba1ecd427, 2024-10-09T16:08:44.566Z)
OS version: Darwin arm64 23.5.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 (8 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|7, 8, 9|
|Memory (System)|16.00GB (0.04GB free)|
|Process Argv|--crash-reporter-id a9b46850-8582-479a-9ccf-67c15c22fa0a|
|Screen Reader|no|
|VM|0%|
</details><details>
<summary>Process Info</summary>

```
CPU %	Mem MB	   PID	Process
    4	   115	 96110	code main
    0	    49	 96114	   gpu-process
    0	    33	 96115	   utility-network-service
    0	   131	 96116	window [1] (Settings  scikit-learn)
    0	    49	 96141	ptyHost
    0	     0	 96199	     /bin/zsh -i
    0	     0	 96428	       /bin/zsh -i
    0	     0	 96508	     /bin/zsh -il
    0	     0	 96697	       /bin/zsh -il
    0	     0	 97377	     /bin/zsh -i
    0	     0	 97749	       /bin/zsh -i
    0	    66	 96142	extensionHost [1]
    0	     0	 96151	     /Users/ogrisel/.vscode/extensions/ms-python.python-2024.16.1-darwin-arm64/python-env-tools/bin/pet server
    0	     0	 96160	     /Users/ogrisel/miniforge3/envs/dev/bin/python /Users/ogrisel/.vscode/extensions/ms-python.black-formatter-2024.4.0/bundled/tool/lsp_server.py --stdio
    0	     0	 96178	     /Users/ogrisel/.vscode/extensions/ms-vscode.cpptools-1.22.10-darwin-arm64/bin/cpptools
    0	    49	 96205	     electron-nodejs (bundle.js )
    0	    33	 96269	     electron-nodejs (server-node.js )
    0	    49	 96189	shared-process
    0	     0	 98803	     /bin/ps -ax -o pid=,ppid=,pcpu=,pmem=,command=
    0	    49	 96191	fileWatcher [1]
```

</details>
<details>
<summary>Workspace Info</summary>

```
|  Window (Settings  scikit-learn)
|    Folder (scikit-learn): 6642 files
|      File types: json(1199) py(987) rst(865) o(288) so(272) dep(272) c(215)
|                  png(170) pyx(113) new(111)
|      Conf files: github-actions(17) makefile(4) settings.json(1);
```

</details>
<details><summary>Extensions (34)</summary>

Extension|Author (truncated)|Version
---|---|---
ruff|cha|2024.52.0
copilot|Git|1.242.0
copilot-chat|Git|0.21.2
vscode-github-actions|git|0.27.0
vscode-cython|ktn|1.0.3
vscode-pdf|mat|0.0.6
black-formatter|ms-|2024.4.0
debugpy|ms-|2024.12.0
python|ms-|2024.16.1
vscode-pylance|ms-|2024.10.1
datawrangler|ms-|1.12.0
jupyter|ms-|2024.9.1
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.19
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.388.0
remote-ssh|ms-|0.115.0
remote-ssh-edit|ms-|0.87.0
remote-wsl|ms-|0.88.4
vscode-remote-extensionpack|ms-|0.26.0
cmake-tools|ms-|1.19.52
cpptools|ms-|1.22.10
cpptools-extension-pack|ms-|1.3.0
makefile-tools|ms-|0.11.13
remote-explorer|ms-|0.4.3
remote-server|ms-|1.5.2
vscode-speech|ms-|0.10.0
vsliveshare|ms-|1.0.5941
vscode-xml|red|0.27.1
rust-analyzer|rus|0.3.2154
rewrap|stk|1.16.3
even-better-toml|tam|0.19.2
cmake|twx|0.0.17

(1 theme extensions excluded)

</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805:30301674
binariesv615:30325510
vsaa593cf:30376535
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
962ge761:30959799
pythongtdpath:30769146
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
9c06g630:31013171
dvdeprecation:31068756
dwnewjupytercf:31046870
impr_priority:31102340
nativerepl1:31139838
refactort:31108082
pythonrstrctxt:31112756
wkspc-onlycs-t:31132770
wkspc-ranged-t:31151552
cf971741:31144450
defaultse:31146405
iacca2:31156134
notype1:31157159
5fd0e150:31155592
dwcopilot:31164048
iconenabled:31158251

```

</details>

<!-- generated by issue reporter -->",Performance Issue,Performance Issue
"tf.compat.v1.train.MonitoredTrainingSession failed to restore checkpoint_dir variables from s3 ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.7.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current behavior?

[TOC]

Our project uses tf.compat.v1.train.MonitoredTrainingSession to create a training session. Typically, we need to restore a pre-trained model from S3.

## 1. Error encountered in my project
Before switching to TensorFlow 1, we used TensorFlow 1.15.1 and passed the S3 path to `checkpoint_dir` like this:
```python
import tensorflow as tf
.....
checkpoint_dir = ""s3://xxx/xx/""
tf.compat.v1.train.MonitoredTrainingSession(...., checkpoint_dir=checkpoint_dir, ...)
```
`checkpoint_dir` contains everything needed to restore variables, including checkpoint, graph.pbtxt, etc. Everything works fine.

After switching to TensorFlow 2.7.0, we realized that the Modular File System has been introduced into TensorFlow. So, we installed TensorFlow-io version 0.23.0, which is compatible with TensorFlow 2.7.0. The code becomes:
```python
import tensorflow as tf
import tensorflow_io as tfio
.....
checkpoint_dir = ""s3://xxx/xx/""
tf.compat.v1.train.MonitoredTrainingSession(...., checkpoint_dir=checkpoint_dir, ...)
```
However, it no longer works, and an error is reported:
```
.....
2023-08-02 16:02:40.147093: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:207 : DATA_LOSS: truncated block read
Traceback (most recent call last):
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1380, in _do_call
    return fn(*args)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1364, in _run_fn
    target_list, run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1458, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.
  (0) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
         [[save/RestoreV2/_1]]
  (1) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
0 successful operations.
0 derived errors ignored.
.....
```

## 2. Reproduce the issue using simple code
To rule out the possibility that the issue is caused by the complexity of the model in my project, I reproduced it using a very simple code.
### 2.1 Step 1: Train the model
First, I used the following code to train a very simple model and save it in a local directory:
```python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
x = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""x"")
y = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""y"")

W = tf.Variable(tf.zeros([1, 1]), name=""W"")
b = tf.Variable(tf.zeros([1]), name=""b"")

y_pred = tf.matmul(x, W) + b
loss = tf.reduce_mean(tf.square(y - y_pred))

optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01)

global_step = tf.compat.v1.train.get_or_create_global_step()
train_op = optimizer.minimize(loss, global_step=global_step)

x_train = [[1], [2], [3], [4]]
y_train = [[0], [-1], [-2], [-3]]

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
hooks = [tf.compat.v1.train.StopAtStepHook(last_step=500)]

checkpoint_dir = './checkpoints'

with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,
                                                 config=config,
                                                 hooks=hooks) as sess:
    while not sess.should_stop():
        sess.run(train_op, feed_dict={x: x_train, y: y_train})
```
### 2.2 Step 2: Upload the model to S3
Then, I used S3 tools to upload all materials in `./checkpoints` to a remote S3 path:
```
s3cmd put ./checkpoints/ s3://xxxx/xxx/checkpoints/
```
### 2.3 Step 3: Restore the model from S3 (error)
Finally, I restored the model training using the following code, and an error was reported:
```python
import tensorflow as tf
import tensorflow_io as tfio

tf.compat.v1.disable_eager_execution()

x = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""x"")
y = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""y"")

W = tf.Variable(tf.zeros([1, 1]), name=""W"")
b = tf.Variable(tf.zeros([1]), name=""b"")

y_pred = tf.matmul(x, W) + b
loss = tf.reduce_mean(tf.square(y - y_pred))

optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01)

global_step = tf.compat.v1.train.get_or_create_global_step()

train_op = optimizer.minimize(loss, global_step=global_step)

x_train = [[1], [2], [3], [4]]
y_train = [[0], [-1], [-2], [-3]]

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True

checkpoint_dir = 's3://xxxx/xxx/checkpoints/'

hooks = [tf.compat.v1.train.StopAtStepHook(last_step=2000)]
with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
    while not sess.should_stop():
        sess.run(train_op, feed_dict={x: x_train, y: y_train})
```
The full log is shown below:
```

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:401: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
2023-08-02 16:43:08.483327: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-02 16:43:09.090602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38415 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:0e:00.0, compute capability: 8.0
2023-08-02 16:43:09.854875: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:207 : DATA_LOSS: truncated block read
Traceback (most recent call last):
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1380, in _do_call
    return fn(*args)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1364, in _run_fn
    target_list, run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1458, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.
  (0) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
         [[save/RestoreV2/_1]]
  (1) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_s3.py"", line 36, in <module>
    with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 616, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1062, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 761, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1267, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1272, in _create_session
    return self._sess_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 914, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 681, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py"", line 321, in prepare_session
    config=config)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py"", line 251, in _restore_checkpoint
    sess, saver, ckpt.model_checkpoint_path)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers
    saver.restore(sess, path)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 1405, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 971, in run
    run_metadata_ptr)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1194, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_run
    run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1399, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.
  (0) DATA_LOSS: truncated block read
         [[node save/RestoreV2
 (defined at train_s3.py:36)
]]
         [[save/RestoreV2/_1]]
  (1) DATA_LOSS: truncated block read
         [[node save/RestoreV2
 (defined at train_s3.py:36)
]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node save/RestoreV2:
In[0] save/Const:
In[1] save/RestoreV2/tensor_names:
In[2] save/RestoreV2/shape_and_slices:

Operation defined at: (most recent call last)
>>>   File ""train_s3.py"", line 36, in <module>
>>>     with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
>>> 

Input Source operations connected to node save/RestoreV2:
In[0] save/Const:
In[1] save/RestoreV2/tensor_names:
In[2] save/RestoreV2/shape_and_slices:

Operation defined at: (most recent call last)
>>>   File ""train_s3.py"", line 36, in <module>
>>>     with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
>>> 

Original stack trace for 'save/RestoreV2':
  File ""train_s3.py"", line 36, in <module>
    with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 616, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1062, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 761, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1267, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1272, in _create_session
    return self._sess_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 914, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 672, in create_session
    self._scaffold.finalize()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize
    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 625, in _get_saver_or_default
    saver = Saver(sharded=True, allow_empty=True)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 923, in __init__
    self.build()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 935, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 973, in _build
    build_restore=build_restore)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 528, in _build_internal
    restore_sequentially, reshape)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 407, in _AddShardedRestoreOps
    name=""restore_shard""))
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 354, in _AddRestoreOps
    restore_sequentially)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 601, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1504, in restore_v2
    name=name)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 746, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3705, in _create_op_internal
    op_def=op_def)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 2101, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)

```

## 3. Test tf.io and s3 connectivity
I also use the following code to test if tf.io can access s3
```
import tensorflow as tf
import tensorflow_io as tfio
s3_path = ""s3://xxxxx/xxx/checkpoints/checkpoint""
ret = tf.io.read_file(s3_path)
print(ret)
```
And it works fine:
```
2023-08-02 16:48:17.619754: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-02 16:48:18.226059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38415 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:0e:00.0, compute capability: 8.0
tf.Tensor(b'model_checkpoint_path: ""model.ckpt-1000""\nall_model_checkpoint_paths: ""model.ckpt-0""\nall_model_checkpoint_paths: ""model.ckpt-500""\nall_model_checkpoint_paths: ""model.ckpt-1000""\n', shape=(), dtype=string)
```



### Standalone code to reproduce the issue

```shell
see above
```


### Relevant log output

```shell
see above
```
",Performance Issue,Performance Issue
"ctrl+hover preview disappears instantly 
Steps to Reproduce:

1. hold ctrl then hover over a definition

Expected:
Hover pops up

Actual:
Hover doesn't pop up or only pops up for a single frame

https://github.com/user-attachments/assets/73df7ca3-e253-411c-ae0b-88a6de25e2b4

```js
function abc() { }

abc();
```

also notice that giving focus to another window fixes it (when the main window is out of focus)

Does this issue occur when all extensions are disabled?: Yes

- VS Code Version: 1.97.0 and [insiders](https://insiders.vscode.dev/)
- OS Version: Windows 11
",UI/UX Bug,UI/UX Bug
"//tensorflow/python/ops/ragged:ragged_cross_op_test is flaky <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.8.13

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Test sometimes fails with segfault

### Standalone code to reproduce the issue

```shell
bazel test --build_tests_only --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --local_test_jobs=64 --test_tag_filters=-nopip,-no_pip,-oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 -k -- //bazel_pip/tensorflow/... -//bazel_pip/tensorflow/compiler/tf2tensorrt/... -//bazel_pip/tensorflow/compiler/xrt/... -//bazel_pip/tensorflow/core/tpu/... -//bazel_pip/tensorflow/go/... -//bazel_pip/tensorflow/java/... -//bazel_pip/tensorflow/python/integration_testing/... -//bazel_pip/tensorflow/tools/toolchains/... -//bazel_pip/tensorflow/lite/... -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:conv_ops_test
```


### Relevant log output

```shell
[ RUN      ] RaggedCrossOpTest.testRaggedCrossInvalidValue
INFO:tensorflow:Running testRaggedCrossInvalidValue in GRAPH mode.
I0522 15:40:37.724678 281472914997264 test_util.py:1494] Running testRaggedCrossInvalidValue in GRAPH mode.
Fatal Python error: Segmentation fault

Thread 0x0000ffff851cc010 (most recent call first):
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1477 in _call_tf_sessionrun
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1384 in _run_fn
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1401 in _do_call
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1394 in _do_run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1214 in _run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 971 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 2061 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 2693 in evaluate
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.py"", line 478 in testRaggedCrossInvalidValue
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 1498 in decorated
  File ""/usr/lib/python3.10/unittest/case.py"", line 549 in _callTestMethod
  File ""/usr/lib/python3.10/unittest/case.py"", line 591 in run
  File ""/usr/lib/python3.10/unittest/case.py"", line 650 in __call__
  File ""/usr/lib/python3.10/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.10/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.10/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.10/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.10/unittest/runner.py"", line 184 in run
  File ""/usr/lib/python3.10/unittest/main.py"", line 271 in runTests
  File ""/usr/lib/python3.10/unittest/main.py"", line 101 in __init__
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2527 in _run_and_get_tests_result
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2561 in run_tests
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2155 in _run_in_app
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2060 in main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/app.py"", line 254 in _run_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/app.py"", line 308 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.py"", line 497 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, tensorflow.python.framework.fast_tensor_util, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label (total: 72)
```
</details>",Runtime Error,Runtime Error
"Dictionary learning is slower with n_jobs > 1 Setting n_jobs > 1 in MiniBatchDictionaryLearning (and in function dictionary_learning_online) leads to worse performance.

Multi processing is handled in sklearn.decompositions, function dict_learning, l 249 

``` python
    code_views = Parallel(n_jobs=n_jobs)(
        delayed(_sparse_encode)(
            X[this_slice], dictionary, gram, cov[:, this_slice], algorithm,
            regularization=regularization, copy_cov=copy_cov,
            init=init[this_slice] if init is not None else None,
            max_iter=max_iter)
        for this_slice in slices)
```

Minimal example :
https://gist.github.com/arthurmensch/091d16c135f4a3ba5580

Output n_jobs = 1

```
Distorting image...
Extracting reference patches...
done in 0.05s.
Learning the dictionary...
done in 5.12s.
Extracting noisy patches... 
done in 0.02s.
Lasso LARS...
done in 10.24s.
```

Output n_jobs == 2

```
Distorting image...
Extracting reference patches...
done in 0.05s.
Learning the dictionary...
done in 78.98s.
Extracting noisy patches... 
done in 0.02s.
Lasso LARS...
done in 6.15s.
```

Output n_jobs == 4

```
Distorting image...
Extracting reference patches...
done in 0.05s.
Learning the dictionary...
done in 83.24s.
Extracting noisy patches... 
done in 0.02s.
Lasso LARS...
done in 3.82s.
```

We can see that transform function of MiniBatchDictionaryLearning (relying on sparse_encode function) benefits from multi-processing as expected.

Dictionary learning relies on successive calls of sparse_encode function : slowness may come from this.
",Performance Issue,Performance Issue
"[DevTools Bug]: Settings / Components / Hide components where...  - need to be set on each reload. ### Website or app

https://react.dev/

### Repro steps

_When filtering components under ""Hide components where... "" you need to modify the filter each time you've reloaded the page and/or hide/show the Chrome DevTools. Otherwise the filter doesn't bite and the hidden components still show._

**For example:**
Name matches Anonymous

Works fine the first time you enter it - all Anonymous components are hidden.
Reload the page and the filter no longer applies. You need to open:  Settings / Components / Hide components where...  and modify the entry and it works again:

**For example:**
Name matches Anonymous*

Or toggle OFF - close the dialog - open the dialog and toggle ON - Now Anonymous is hidden again.


<img width=""820"" alt=""Screenshot 2024-11-04 at 13 10 37"" src=""https://github.com/user-attachments/assets/7d7bf89f-d801-4605-9100-140036e67589"">


Using 6.0.1-c7c68ef842 of the extension
Chrome Version 130.0.6723.92 (Official Build) (arm64)

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Syntax Error,Syntax Error
"RAM memory leak with tf.function when training multiple models in a loop ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

When I train multiple models in a loop, if I decorate the `train()` function with `@tf.function`, then the memory usage keeps on increasing after each loop iteration, even when I delete the model at the end of each loop and clear the TensorFlow graph/session.

The memory leak does not occur when `@tf.function` is removed. However, model training performance is significantly slower.

Colab notebook to reproduce issue:
https://colab.research.google.com/drive/1sJsGmcFeZVx6ImNbqnBsgF_LzIyPxXPW?usp=sharing

### Standalone code to reproduce the issue

```python
import gc
import os

import psutil
import tensorflow as tf


class MyModel:
    def __init__(self):
        self.dnn = tf.keras.Sequential([
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(1),
        ])
        self.optimizer = tf.optimizers.Adam()
    
    @tf.function   # if we remove this @tf.function decorator, then there is no memory leak
    def train(self, X):
        with tf.GradientTape() as tape:
            loss = tf.reduce_sum(self.dnn(X))
        grads = tape.gradient(loss, self.dnn.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.dnn.trainable_variables))

process = psutil.Process(os.getpid())
rss = int(process.memory_info().rss / 1024 / 1024)  # in MB
print(f'rss: {rss} MB')

X = tf.ones((50, 80))
for i in range(50):
    model = MyModel()
    for _ in range(20):
        model.train(X)

    del model
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()
    gc.collect()

    new_rss = int(process.memory_info().rss / 1024 / 1024)
    if new_rss > rss:
        rss_increase = new_rss - rss
        rss = new_rss
        print(f'Iter {i:05d}, rss increase: {rss_increase} MB, rss: {rss} MB')
```


### Relevant log output

```shell
rss: 593 MB
Iter 00000, rss increase: 31 MB, rss: 624 MB
Iter 00001, rss increase: 6 MB, rss: 630 MB
Iter 00002, rss increase: 5 MB, rss: 635 MB
Iter 00003, rss increase: 5 MB, rss: 640 MB
Iter 00004, rss increase: 5 MB, rss: 645 MB
Iter 00005, rss increase: 5 MB, rss: 650 MB
Iter 00006, rss increase: 5 MB, rss: 655 MB
Iter 00007, rss increase: 5 MB, rss: 660 MB
Iter 00008, rss increase: 5 MB, rss: 665 MB
Iter 00009, rss increase: 5 MB, rss: 670 MB
Iter 00010, rss increase: 4 MB, rss: 674 MB
---  <omitting some rows for brevity>  ---
Iter 00040, rss increase: 5 MB, rss: 822 MB
Iter 00041, rss increase: 5 MB, rss: 827 MB
Iter 00042, rss increase: 5 MB, rss: 832 MB
Iter 00043, rss increase: 5 MB, rss: 837 MB
Iter 00044, rss increase: 5 MB, rss: 842 MB
Iter 00045, rss increase: 5 MB, rss: 847 MB
Iter 00046, rss increase: 5 MB, rss: 852 MB
Iter 00047, rss increase: 5 MB, rss: 857 MB
Iter 00048, rss increase: 5 MB, rss: 862 MB
Iter 00049, rss increase: 5 MB, rss: 867 MB
```
",Performance Issue,Performance Issue
"Test coverage shows incorrect coverage margin decorations when filtered While verifying #235147, related to #234946

I'm confused here since why would an area suddenly appear covered when I filter to a specific test, vs. the overall coverage results which includes that test?

https://github.com/user-attachments/assets/add27ad5-bc51-4bbc-900f-5b5f26f719d6",Logical Bug,Logical Bug
"[DevTools Bug]: Flame graph entries overlapping ### Website or app

https://web-enable-react-profil-1na9tm.herokuapp.com/

### Repro steps

1. Go to https://web-enable-react-profil-1na9tm.herokuapp.com/
2. Open profiler
3. Click ""reload and start profiling""
4. After the reload completes, stop profiling
5. Step through the commits until you see this:
   <img width=""1840"" alt=""image"" src=""https://github.com/facebook/react/assets/921609/7162b403-d940-4028-a20f-27d73d48e55c"">

The flame graph entries are overlapping other entries.

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

4.27.8 (5/19/2023)

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",UI/UX Bug,UI/UX Bug
"copilot-debug terminal integration breaks python venv activation - Copilot Chat Extension Version: 0.23.2
- VS Code Version: 1.96.2
- OS Version: Windows 11
- Logs:

Steps to Reproduce:

1. Start with a clean VSCode profile
2. Install the [python extension](https://marketplace.visualstudio.com/items?itemName=ms-python.python)
3. Create a virtual environment and set it as the python interpreter
4. Open a new terminal and run `(get-command python).Source` (or equivalent for your shell): it should point to the python interpreter from the virtual environment, because the Python extension activates the environment for you and prepends it to `env:PATH`
5. Now, install Copilot Chat and restart VSCode
6. Open a new terminal and re-run `(get-command python).Source`: this time, it should point to the global python interpreter.

The issue seems to be that both Python extension and Copilot Chat seem to be trying to modify the PATH at the same time, probably overriding each others changes (even if in my case, Copilot Chat seems to always override Python).

Even worse, Copilot Chat seems to completely ignore the `""github.copilot.chat.copilotDebugCommand.enabled"": false` setting, with the extension *still* trying to add features to the terminal.
**EDIT:** Adding a separate issue for this, microsoft/vscode-copilot-release#3499 ",Dependency Issue,Dependency Issue
"[DevTools Bug] The ""path"" argument must be of type string. Received undefined ### Website or app

private repo

### Repro steps

- Just opening the devtools in a React Native App and inspecting components I got this error: The ""path"" argument must be of type string. Received undefined.
<img width=""1046"" alt=""Screenshot 2024-09-06 at 4 54 59â¯PM"" src=""https://github.com/user-attachments/assets/50d942d8-85f4-4bb1-8a1f-039fe302b370"">

### Development Environment
Device: MacBook Air M1 (2020)
Processor: Apple Silicon (M1)
Operating System: macOS Sonoma v.14.6.1
Architecture: ARM64
Package Manager: Yarn 

### Dependencies
React: 18.2.0
React Native: 0.74.2
React Navigation:
@react-navigation/native: 6.1.17
@react-navigation/stack: 6.3.29
@react-navigation/drawer: 6.6.15
@react-navigation/bottom-tabs: 6.5.20
@react-navigation/native-stack: 6.9.26


### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-core

### DevTools version (automated)

4.28.5-ef8a840bd

### Error message (automated)

The ""path"" argument must be of type string. Received undefined

### Error call stack (automated)

```text
at __node_internal_captureLargerStackTrace (node:internal/errors:484:5)
    at new NodeError (node:internal/errors:393:5)
    at validateString (node:internal/validators:163:11)
    at isAbsolute (node:path:1157:5)
    at f_ (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1371384)
    at k_ (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1376557)
    at xu (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1212586)
    at an (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:39841)
    at Ns (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:117187)
    at Il (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:101534)
    at Rl (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:101463)
    at Nl (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:101281)
    at Sl (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:98382)
    at pl (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:97812)
    at Immediate.D [as _onImmediate] (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:185245)
    at process.processImmediate (node:internal/timers:471:21)
```


### Error component stack (automated)

```text
at xu (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1211637)
    at Fl (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1182513)
    at Suspense
    at ms (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1166739)
    at div
    at Hs (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1172940)
    at cu (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1202150)
    at div
    at div
    at si (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1127258)
    at ko (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1151045)
    at /Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1231603
    at ms (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1166739)
    at /Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1169395
    at div
    at div
    at div
    at Ss (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1169229)
    at ic (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1233344)
    at Wu (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1225803)
    at ut (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1073442)
    at jt (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1100719)
    at Os (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1173613)
    at i_ (/Users/cayolegal/Documents/BDP/labarra-app-py/node_modules/react-devtools-core/dist/standalone.js:2:1367821)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=The ""path"" argument must be of type string. Received undefined in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
",Syntax Error,Syntax Error
"Large inconsistencies in tf.signal.stft's and tf.signal.inverse_stft's results with @tf.function decorator for certain inputs <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0-dev20230509

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?

`tf.signal.stft` and `tf.signal.inverse_stft` has large inconsistencies in their results with or without @tf.function for some inputs. This issue seems to be unrelated to precision errors, as previously discussed under issues (#57960 and #57961), given that the inconsistencies can reach very high values, such as 7.530909102308483e+252+6.2143661415679e-310j. I open this issue because the behavior still exists in the latest nightly version of tensorfow.

Further investigation finds that it is because the results are different during each run and thus the inconsistencies are different, where sometimes the discrepancies are extremely large, while at other times they are relatively small. It appears that the inconsistencies are non-deterministic, which indicates a potential issue with the underlying implementation.

I rerun the reproduction code several times and record the large inconsistencies below in the log file.

The reproduction colab links are here:
For tf.signal.stft, https://colab.research.google.com/drive/1WleKXby71iZXOL12r8nIN8B_jd2wJQks?usp=sharing.
For tf.signal.inverse_stft, https://colab.research.google.com/drive/1MhNfkZgltqQqHw8kKG2zQi8ivwvKfRoj?usp=sharing.


### Standalone code to reproduce the issue

```shell
# for tf.signal.stft
import tensorflow as tf
import numpy as np

print(tf.__version__)

input = {'fft_length': 46, 'frame_step': 19, 'frame_length': 0, 'signals': np.array([[[[-8.75314539e+307, -4.03838038e+307,  8.23775798e+307, -1.32627219e+307,  1.19815521e+307,  4.57117750e+307],
                                                                              [-4.74761327e+307, -4.71580522e+307, -5.88832102e+307, -6.48759076e+307, -4.36028464e+307, -4.77775171e+307],
                                                                              [ 1.20113701e+307, -7.60106094e+307,  7.22716917e+307, 2.17687950e+307, -5.25271143e+306,  5.41182394e+307]]]])}

output1 = tf.signal.stft(**input)

@tf.function
def fun_wrapper(x):
    return tf.signal.stft(**x)

output2 = fun_wrapper(input)

print(np.allclose(output1, output2))
print(np.max(np.subtract(output1, output2)))

# for tf.signal.inverse_stft
import tensorflow as tf
import numpy as np

print(tf.__version__)

input = {'frame_step': 29343, 'frame_length': 61, 'stfts': np.array([[]], dtype=np.complex64)}

output1 = tf.signal.inverse_stft(**input)

@tf.function
def fun_wrapper(x):
    return tf.signal.inverse_stft(**x)

output2 = fun_wrapper(input)

print(np.allclose(output1, output2))
print(np.max(np.subtract(output1, output2)))
```


### Relevant log output

```shell
### for tf.signal.stft
False
(1.2623837153272947e+180+2.19373012209e-312j)

False
(6.443468248812391e+278-3.2e-322j)

False
(2.347922071768121e+228+1.74e-321j)

### for tf.signal.inverse_stft
False
1.4412957e+32

False
7.529253e+23

False
7800730000.0
```
</details>",Runtime Error,Logical Bug
"Memory leak in training ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1 and 2.17

### Custom code

Yes

### OS platform and distribution

Debian 11

### Mobile device

_No response_

### Python version

Python 3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda/12.0.0_gcc-10.4.0 and cudnn/8.9.7.29-12_gcc-10.4.0

### GPU model and memory

different GPU, among which Tesla V100-SXM2-32GB

### Current behavior?

I have a memory leak (GPU memory and RAM constantly increase) during my training. 

**This does not happen with Tensorflow 2.11.1**. 

[Here is the project](https://github.com/deep-finder/tirfm-deepfinder).
Unfortunately, this is not my code and I do not have time to create a minimal standalone code.

### Standalone code to reproduce the issue

```shell
def printMemoryUsage(self):
        gpus = tf.config.list_physical_devices('GPU')             
        for gpu in gpus:
            gpuNameRoot = gpu.name.split(':')[0] + ':'
            memory_info = tf.config.experimental.get_memory_info(gpu.name.replace(gpuNameRoot, ''))
            self.display(f'Memory info of GPU {gpu.name}: current: {memory_info[""current""]/1e9:.2f}, peak: {memory_info[""peak""]/1e9:.2f}')
            try:
                import psutil
                virtual_memory = psutil.virtual_memory()
                print(f'Memory info of CPU: total:{virtual_memory[0]/1e9:.2f}Gb, available: {virtual_memory[1]/1e9:.2f}Gb, percent: {virtual_memory[2]}%')
            except:
                pass

[...]
        # Training loop:
        for e in range(self.epochs):
            # TRAINING:
            start = time.time()
            list_loss_train = []
            list_acc_train = []
            for it in range(self.steps_per_epoch):
                if self.flag_direct_read:
                    batch_data, batch_target = self.generate_batch_direct_read(path_data, path_target, self.batch_size, objlist_train)
                else:
                    batch_data, batch_target, idx_list = self.generate_batch_from_array(data_list, target_list, self.batch_size, objlist_train)

                if self.sample_weights is not None:
                    sample_weight = self.sample_weights[idx_list]
                else:
                    sample_weight = None

                loss_train = self.net.train_on_batch(batch_data, batch_target,
                                                     class_weight=self.class_weight,
                                                     sample_weight=sample_weight)

                self.display('epoch %d/%d - it %d/%d - loss: %0.3f - acc: %0.3f' % (e + 1, self.epochs, it + 1, self.steps_per_epoch, loss_train[0], loss_train[1]))

                self.printMemoryUsage()

                list_loss_train.append(loss_train[0])
                list_acc_train.append(loss_train[1])
                del batch_data
                del batch_target
                if idx_list is not None:
                    del idx_list
                gc.collect()
```


### Relevant log output

```shell
With Tensorflow 2.11.1:


=============================================================
epoch 3/50 - it 1/100 - loss: 2.012 - acc: 0.976
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 2/100 - loss: 2.008 - acc: 0.985
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 3/100 - loss: 2.004 - acc: 0.992
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 4/100 - loss: 2.006 - acc: 0.987
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 5/100 - loss: 2.006 - acc: 0.989
```


With Tensorflow 2.16.1 and 2.17:

```
epoch 1/50 - it 6/100 - loss: 2.473 - acc: 0.305
Memory info of GPU /physical_device:GPU:0: current: 0.18, peak: 2.55
Memory info of CPU: total:201.37Gb, available: 190.78Gb, percent: 5.3%
epoch 1/50 - it 7/100 - loss: 2.470 - acc: 0.394
Memory info of GPU /physical_device:GPU:0: current: 0.21, peak: 2.58
Memory info of CPU: total:201.37Gb, available: 190.58Gb, percent: 5.4%
epoch 1/50 - it 8/100 - loss: 2.466 - acc: 0.463
Memory info of GPU /physical_device:GPU:0: current: 0.24, peak: 2.61
Memory info of CPU: total:201.37Gb, available: 190.38Gb, percent: 5.5%
epoch 1/50 - it 9/100 - loss: 2.461 - acc: 0.519
Memory info of GPU /physical_device:GPU:0: current: 0.27, peak: 2.64
Memory info of CPU: total:201.37Gb, available: 190.11Gb, percent: 5.6%
epoch 1/50 - it 10/100 - loss: 2.455 - acc: 0.564
Memory info of GPU /physical_device:GPU:0: current: 0.29, peak: 2.67
Memory info of CPU: total:201.37Gb, available: 189.94Gb, percent: 5.7%
epoch 1/50 - it 11/100 - loss: 2.448 - acc: 0.603
Memory info of GPU /physical_device:GPU:0: current: 0.32, peak: 2.70
Memory info of CPU: total:201.37Gb, available: 189.67Gb, percent: 5.8%
epoch 1/50 - it 12/100 - loss: 2.437 - acc: 0.634
Memory info of GPU /physical_device:GPU:0: current: 0.35, peak: 2.72
Memory info of CPU: total:201.37Gb, available: 189.48Gb, percent: 5.9%
epoch 1/50 - it 13/100 - loss: 2.425 - acc: 0.662
Memory info of GPU /physical_device:GPU:0: current: 0.38, peak: 2.75
Memory info of CPU: total:201.37Gb, available: 189.21Gb, percent: 6.0%
epoch 1/50 - it 14/100 - loss: 2.408 - acc: 0.685
Memory info of GPU /physical_device:GPU:0: current: 0.41, peak: 2.78
Memory info of CPU: total:201.37Gb, available: 189.03Gb, percent: 6.1%
epoch 1/50 - it 15/100 - loss: 2.389 - acc: 0.705
Memory info of GPU /physical_device:GPU:0: current: 0.44, peak: 2.81
Memory info of CPU: total:201.37Gb, available: 188.79Gb, percent: 6.2%
epoch 1/50 - it 16/100 - loss: 2.369 - acc: 0.723
Memory info of GPU /physical_device:GPU:0: current: 0.46, peak: 2.84
Memory info of CPU: total:201.37Gb, available: 188.54Gb, percent: 6.4%
epoch 1/50 - it 17/100 - loss: 2.350 - acc: 0.739
Memory info of GPU /physical_device:GPU:0: current: 0.49, peak: 2.87
Memory info of CPU: total:201.37Gb, available: 188.37Gb, percent: 6.5%
epoch 1/50 - it 18/100 - loss: 2.332 - acc: 0.753
Memory info of GPU /physical_device:GPU:0: current: 0.52, peak: 2.89
Memory info of CPU: total:201.37Gb, available: 188.10Gb, percent: 6.6%
epoch 1/50 - it 19/100 - loss: 2.315 - acc: 0.765
Memory info of GPU /physical_device:GPU:0: current: 0.55, peak: 2.92
Memory info of CPU: total:201.37Gb, available: 187.87Gb, percent: 6.7%
epoch 1/50 - it 20/100 - loss: 2.300 - acc: 0.776
Memory info of GPU /physical_device:GPU:0: current: 0.58, peak: 2.95
Memory info of CPU: total:201.37Gb, available: 187.64Gb, percent: 6.8%
epoch 1/50 - it 21/100 - loss: 2.286 - acc: 0.786
Memory info of GPU /physical_device:GPU:0: current: 0.61, peak: 2.98
Memory info of CPU: total:201.37Gb, available: 187.49Gb, percent: 6.9%
epoch 1/50 - it 22/100 - loss: 2.274 - acc: 0.795
Memory info of GPU /physical_device:GPU:0: current: 0.63, peak: 3.01
Memory info of CPU: total:201.37Gb, available: 187.25Gb, percent: 7.0%
epoch 1/50 - it 23/100 - loss: 2.262 - acc: 0.804
Memory info of GPU /physical_device:GPU:0: current: 0.66, peak: 3.04
Memory info of CPU: total:201.37Gb, available: 186.97Gb, percent: 7.1%
epoch 1/50 - it 24/100 - loss: 2.251 - acc: 0.811
Memory info of GPU /physical_device:GPU:0: current: 0.69, peak: 3.06
Memory info of CPU: total:201.37Gb, available: 186.81Gb, percent: 7.2%
epoch 1/50 - it 25/100 - loss: 2.241 - acc: 0.818
Memory info of GPU /physical_device:GPU:0: current: 0.72, peak: 3.09
Memory info of CPU: total:201.37Gb, available: 186.59Gb, percent: 7.3%
epoch 1/50 - it 26/100 - loss: 2.232 - acc: 0.825
Memory info of GPU /physical_device:GPU:0: current: 0.75, peak: 3.12
Memory info of CPU: total:201.37Gb, available: 186.36Gb, percent: 7.5%
epoch 1/50 - it 27/100 - loss: 2.224 - acc: 0.831
Memory info of GPU /physical_device:GPU:0: current: 0.78, peak: 3.15
Memory info of CPU: total:201.37Gb, available: 186.09Gb, percent: 7.6%
epoch 1/50 - it 28/100 - loss: 2.216 - acc: 0.836
Memory info of GPU /physical_device:GPU:0: current: 0.80, peak: 3.18
Memory info of CPU: total:201.37Gb, available: 185.90Gb, percent: 7.7%
epoch 1/50 - it 29/100 - loss: 2.209 - acc: 0.841
Memory info of GPU /physical_device:GPU:0: current: 0.83, peak: 3.21
Memory info of CPU: total:201.37Gb, available: 185.65Gb, percent: 7.8%
epoch 1/50 - it 30/100 - loss: 2.202 - acc: 0.846
Memory info of GPU /physical_device:GPU:0: current: 0.86, peak: 3.23
Memory info of CPU: total:201.37Gb, available: 185.46Gb, percent: 7.9%
epoch 1/50 - it 31/100 - loss: 2.196 - acc: 0.851
Memory info of GPU /physical_device:GPU:0: current: 0.89, peak: 3.26
Memory info of CPU: total:201.37Gb, available: 185.24Gb, percent: 8.0%
epoch 1/50 - it 32/100 - loss: 2.190 - acc: 0.855
Memory info of GPU /physical_device:GPU:0: current: 0.92, peak: 3.29
Memory info of CPU: total:201.37Gb, available: 184.99Gb, percent: 8.1%
epoch 1/50 - it 33/100 - loss: 2.185 - acc: 0.859
Memory info of GPU /physical_device:GPU:0: current: 0.95, peak: 3.32
Memory info of CPU: total:201.37Gb, available: 184.75Gb, percent: 8.3%
epoch 1/50 - it 34/100 - loss: 2.180 - acc: 0.863
Memory info of GPU /physical_device:GPU:0: current: 0.97, peak: 3.35
Memory info of CPU: total:201.37Gb, available: 184.51Gb, percent: 8.4%
epoch 1/50 - it 35/100 - loss: 2.174 - acc: 0.866
Memory info of GPU /physical_device:GPU:0: current: 1.00, peak: 3.38
Memory info of CPU: total:201.37Gb, available: 184.36Gb, percent: 8.4%
epoch 1/50 - it 36/100 - loss: 2.170 - acc: 0.870
Memory info of GPU /physical_device:GPU:0: current: 1.03, peak: 3.40
Memory info of CPU: total:201.37Gb, available: 184.06Gb, percent: 8.6%
epoch 1/50 - it 37/100 - loss: 2.165 - acc: 0.873
Memory info of GPU /physical_device:GPU:0: current: 1.06, peak: 3.43
Memory info of CPU: total:201.37Gb, available: 183.89Gb, percent: 8.7%
epoch 1/50 - it 38/100 - loss: 2.161 - acc: 0.876
Memory info of GPU /physical_device:GPU:0: current: 1.09, peak: 3.46
Memory info of CPU: total:201.37Gb, available: 183.66Gb, percent: 8.8%
epoch 1/50 - it 39/100 - loss: 2.157 - acc: 0.879
Memory info of GPU /physical_device:GPU:0: current: 1.11, peak: 3.49
Memory info of CPU: total:201.37Gb, available: 183.42Gb, percent: 8.9%
epoch 1/50 - it 40/100 - loss: 2.153 - acc: 0.881
Memory info of GPU /physical_device:GPU:0: current: 1.14, peak: 3.52
Memory info of CPU: total:201.37Gb, available: 183.26Gb, percent: 9.0%
epoch 1/50 - it 41/100 - loss: 2.150 - acc: 0.884
Memory info of GPU /physical_device:GPU:0: current: 1.17, peak: 3.54
Memory info of CPU: total:201.37Gb, available: 183.02Gb, percent: 9.1%
```
```
",Performance Issue,Performance Issue
"[DevTools Bug] Commit tree does not contain fiber ""4543"". This is a bug in React DevTools. ### Website or app

https://kai.dev.dxos.org/



### Repro steps

1. Open devtools components panel and trigger a re-render in the app.
2. In my case sometimes the component tree doesnt appear at first, but pressing the devtools extension action button in the browser navbar makes it load.

### More info

App built with react `18.2.0` and vite

To repro with Vite running in dev mode:

```
# checkout https://github.com/dxos/dxos/tree/main/packages/experimental/kai

pnpm install
pnpm -w nx serve kai
```

Open devtools component view and trigger a re-render in the app.



### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.27.1-47f63dc54

### Error message (automated)

Commit tree does not contain fiber ""4543"". This is a bug in React DevTools.

### Error call stack (automated)

```text
at updateTree (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26856:21)
    at getCommitTree (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26716:20)
    at SidebarCommitInfo_SidebarCommitInfo (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:55240:42)
    at Ri (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:16122:7)
    at tk (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:17190:7)
    at kn (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:20548:86)
    at gn (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:20091:11)
    at fn (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:20011:23)
    at Um (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:19991:5)
    at Wm (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:19795:7)
```


### Error component stack (automated)

```text
at SidebarCommitInfo_SidebarCommitInfo (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:55220:34)
    at div
    at div
    at SettingsModalContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:37690:3)
    at Profiler_Profiler (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:55516:34)
    at ErrorBoundary_ErrorBoundary (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39222:5)
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39394:3)
    at PortaledContent (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39424:5)
    at div
    at div
    at div
    at ThemeProvider (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39394:3)
    at TimelineContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44671:3)
    at ProfilerContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:44100:3)
    at TreeContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:31925:3)
    at SettingsContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:32569:3)
    at ModalDialogContextController (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:39819:3)
    at DevTools_DevTools (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:56024:3)
```


### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Commit tree does not contain fiber . This is a bug in React DevTools. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
",Logical Bug,Logical Bug
"Screen cheese in the debug console ![Image](https://github.com/user-attachments/assets/eaf409e2-39cc-4a7a-aa68-46e276f66f62)

I've also seen it appear to be empty, but with a scrollbar and a lot of distance to scroll.",UI/UX Bug,UI/UX Bug
"The VS Code Server with Remote SSH extension or Code Server experiences a memory leak <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: No

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: In VSCode with Remote SSH extension , Or  Code Server
- OS Version: Connecting Windows to the Linux Server

Phenomenon
The VS Code Server with Remote SSH extension or Code Server experiences a memory leak issue

https://github.com/microsoft/vscode/blob/921f7ca0fc92817f6ab30ecc93850870cd0bd414/src/vs/base/parts/ipc/node/ipc.net.ts#L646 ?
The length of the array '_recordedInflateBytes' only grows over time and is not released unless the entry process is closed.

The snapshot information is as follows:

![Image](https://github.com/user-attachments/assets/962744df-95ee-4d94-b676-52629054a92f)
![Image](https://github.com/user-attachments/assets/6abd9847-70e8-4368-954a-5484b5da12fc)
![Image](https://github.com/user-attachments/assets/baf4403e-3437-4867-832f-30ba151e1546)

```
 PersistentProtocol  <- src\vs\base\parts\ipc\common\ipc.net.ts
  _socket: WebSocketNodeSocket -> 
    _flowManager:WebSocketFlowManager ->
      _zlibInflateStream: ZlibInflateStream ->
        _recordedInflateBytes: VSBuffer[] -> The length of the array only grows with time  (65411 items in heapsnap shot)
```
<!-- Failed to upload ""snapshot.png"" -->


Steps to Reproduce:

1. In VSCode version 1.94.2 with Remote SSH extension , 
2. disable the 'Use Exec server' option to switch the VSCode server to a WebSocket connection type to reproduce the issue. 
3. Wait for one week and check the entry process.

1. Code Server.
2.  Wait for one week and check the entry process.

Thought?

The recording of inflated bytes results in a memory leak. The Recording inflated bytes is not necessary if the socket will not be sent to ExtensionHost process. The solution is to disable sending the socket to the ExtensionHost process and as a result, the need to write inflated bytes is eliminated. A named pipe can be used to communicate with the ExtensionHost instead of sending a socket.
https://github.com/microsoft/vscode/blob/389bf6c37fb5c3e177abe762656ea8728c2cc3a4/src/vs/server/node/extensionHostConnection.ts#L128
the _canSendSocket property is always true for non-Windows platforms. The named pipe is only used on Windows. Are there any problems or concerns?",Security Vulnerability,Security Vulnerability
"Automatic bandwidth calculation valid only for normalized data ### Describe the bug

`sklearn.neighbors.KernelDensity` supports automatic (optimal) bandwidth calculation via `bandwidth = 'silverman'` and `bandwidth = 'scott'`. The algorithm computes the appropriate observation-weighted bandwidth factors (proportional to nobs^0.2) but does not adjust for the standard deviation or interquartile range of the dataset. Roughly, the algorithm should scale the dataset's standard error by the algorithmic bandwidth factors.

See, e.g., [Wikipedia](https://en.wikipedia.org/wiki/Kernel_density_estimation#A_rule-of-thumb_bandwidth_estimator). The implementation in `scipy.stats._kde` is correct.

### Steps/Code to Reproduce
```python
import matplotlib.pyplot as plot
import numpy as np
from sklearn.neighbors import KernelDensity
from scipy.stats import gaussian_kde

data = np.random.normal( scale = 0.01, size = 100 )

#
# 1. sklearn (auto)
#
kd_sklearn_auto = KernelDensity( kernel = 'gaussian', bandwidth = 'silverman' )
kd_sklearn_auto.fit( np.reshape( data, ( -1, 1 ) ) )

#
# 2. sklearn (manual)
#
kd_sklearn_manual = KernelDensity( kernel = 'gaussian', bandwidth = 0.9 * np.std( data ) / len( data ) ** ( 1 / 5 ) )
kd_sklearn_manual.fit( np.reshape( data, ( -1, 1 ) ) )

#
# 3. scipy
#
kd_scipy = gaussian_kde( data, bw_method = 'silverman' )

#
# 4. show the difference
#
xs = np.arange( start = -0.05, stop = 0.05, step = 1e-4 )
plot.plot( xs, np.exp( kd_sklearn_auto.score_samples( np.reshape( xs, ( -1, 1 ) ) ) ), label = 'KDE SKLearn (auto)' )
plot.plot( xs, np.exp( kd_sklearn_manual.score_samples( np.reshape( xs, ( -1, 1 ) ) ) ), label = 'KDE SKLearn (manual)' )
plot.plot( xs, kd_scipy.pdf( xs ), label = 'KDE SciPy' )
plot.hist( data, label = 'Data' )
plot.legend()
plot.show()
```

### Expected Results

Automatic SKLearn bandwidth curve should approximately match SciPy bandwidth curve, roughly the shape of the underlying data histogram.

### Actual Results

Automatic SKLearn bandwidth curve generates a flat PDF.

### Versions

```shell
System:
    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]
executable: /local_disk0/.ephemeral_nfs/envs/pythonEnv-67c47d19-3f15-49a2-ab8f-bcf25a2bc29f/bin/python
   machine: Linux-5.15.0-1038-azure-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.2.2
          pip: 21.2.4
   setuptools: 58.0.4
        numpy: 1.20.3
        scipy: 1.7.1
       Cython: 0.29.24
       pandas: 1.3.4
   matplotlib: 3.4.3
       joblib: 1.2.0
threadpoolctl: 2.2.0

Built with OpenMP: True

threadpoolctl info:
       filepath: /databricks/python3/lib/python3.9/site-packages/numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so
         prefix: libopenblas
       user_api: blas
   internal_api: openblas
        version: 0.3.13.dev
    num_threads: 6
threading_layer: pthreads
   architecture: Haswell

       filepath: /local_disk0/.ephemeral_nfs/envs/pythonEnv-67c47d19-3f15-49a2-ab8f-bcf25a2bc29f/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
         prefix: libgomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 6

       filepath: /databricks/python3/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-085ca80a.3.9.so
         prefix: libopenblas
       user_api: blas
   internal_api: openblas
        version: 0.3.9
    num_threads: 6
threading_layer: pthreads
   architecture: Haswell
```
",Syntax Error,Syntax Error
"Bug: Input minLength not working after type changed I have an input with type password with minLength 8, and i have a toggle to change the type to text (password toggle).
when i type with length < 8 and press submit, it prevent to submit. but, when i toggle the password so it change the type to text and submit the form, it can submitted. btw i using useRef()

React version:
react: ^17.0.1
react-dom: ^17.0.1

## Steps To Reproduce

1. type character < minLength
2. submit form
3. toggle form type
4. submit form

Link to code example: [DEMO](https://codesandbox.io/s/young-shape-4tciw)

## The current behavior
The form get submitted even when it under minLength

## The expected behavior
The form not get submitted when it under minLength
",Security Vulnerability,Security Vulnerability
"[DevTools Bug] Cannot remove node ""92"" because no matching node was found in the Store. ### Website or app

http://localhost:3000/

### Repro steps

I was trying to inspect my component tree via react dev tools, but each time I make an interaction on the website which changes the state, it throws an error 

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

6.0.1-c7c68ef842

### Error message (automated)

Cannot remove node ""92"" because no matching node was found in the Store.

### Error call stack (automated)

```text
at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173889
    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)
    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390
    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot remove node  because no matching node was found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
",Syntax Error,Syntax Error
"[DevTools Bug] Cannot add child ""301"" to parent ""155"" because parent node was not found in the Store. ### Website or app

www.github.com

### Repro steps

The error was thrown at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173126
    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)
    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390
    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

6.0.1-c7c68ef842

### Error message (automated)

Cannot add child ""301"" to parent ""155"" because parent node was not found in the Store.

### Error call stack (automated)

```text
at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1173126
    at v.emit (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1140783)
    at chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1142390
    at bridgeListener (chrome-extension://gpphkfbcpidddadnkolkpfckpihlkkil/build/main.js:1:1552662)
```

### Error component stack (automated)

```text

```

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Cannot add child  to parent  because parent node was not found in the Store. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```",Syntax Error,Syntax Error
"[DevTools Bug]: Labels are cut off on Firefox on Ubuntu 22.04 ### Website or app

https://github.com/iprotoni/react-skill-assessments

### Repro steps

 the key labels on the right hand pane are cut off whenever using React DevTools on Firefox latest version on Ubuntu 22.04

### How often does this bug happen?

Every time

### DevTools package (automated)

_No response_

### DevTools version (automated)

_No response_

### Error message (automated)

_No response_

### Error call stack (automated)

_No response_

### Error component stack (automated)

_No response_

### GitHub query string (automated)

_No response_",Dependency Issue,Dependency Issue
"Git - False line-blame info Not sure how to reproduce but I navigates some source and eventually returned to `inlineChatActions`. It's last line shows git blame from `vscode.d.ts`

* `src/vscode-dts/vscode.d.ts#L4885`, and 
* `src/vs/workbench/contrib/inlineChat/browser/inlineChatActions.ts`

<img width=""856"" alt=""Screenshot 2025-02-17 at 08 57 12"" src=""https://github.com/user-attachments/assets/a8d9a493-877c-4077-876c-eae79b1f6959"" />
",Dependency Issue,Dependency Issue
"SGDOneClassSVM model does not converge with default stopping criteria(stops prematurely) ### Describe the bug

SGDOneClassSVM does not converge with default early stopping criteria, because the used loss is not actual loss, but only error, which can be easily 0.0 and then increase as the model converges to adequate solution. That is, the used for stopping and reported with verbose ""loss"" value doesn't accout for the full model formula/regularization. Also, pay attention to bias term to gauge convergence.
https://github.com/scikit-learn/scikit-learn/blob/c7839c48363d1531af9a00abfcb9d911ecfcb2b2/sklearn/linear_model/_sgd_fast.pyx.tp#L482
The optimization almost always stops after 6 epochs, the initial epoch, plus the 5 for stopping tolerance (can't change the number of epochs for the stopping tolerance btw).
The problem does not manifest with toy data(small dimensiaonal), becasue 6 epochs is likely enough for convergence to satisfactory solution.
In the reproduction code, mind the console output and comments. Possible workaround at the end of reproduction code, is to use tol=None with manual epoch limit(max_iter), but that slows the optimization down by a lot, since forbids the use of learning_rate=""adaptive"".

### Steps/Code to Reproduce
```python
import numpy as np
from matplotlib import pyplot as plt
import pandas as pd
#from sklearn.linear_model import Ridge
from sklearn.datasets import make_regression
from timeit import timeit
from sklearn.linear_model import SGDOneClassSVM
from sklearn.svm import OneClassSVM
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler

np.random.seed(123)

#no matter the feature count, optimization stops in 6 epochs
print(""fitting different feature counts:"")
featCnts = [10, 1000, 25000]
for featCnt in featCnts:
    print(""\n10k samples, {} features"".format(featCnt))
    x, y = make_regression(10000, featCnt, n_informative=featCnt // 10)
    x = MinMaxScaler().fit_transform(x) + 1.0 #make positive
    model = SGDOneClassSVM(nu=0.01, verbose=10)#, tol=1e-10)
    model.fit(x) #see console output

#model reports 0 ""loss"" even after just 1 sample
print(""\n\npartial fit test:"")
x, y = make_regression(10000, 2500//4, n_informative=250)
xPos = StandardScaler().fit_transform(x) + 100 #highly positive data
#Note: linear one class svm has to run with e.g. positive data for correct function(not centered about 0), otherwise data is not separable from origin
print(""xPos data mean"", xPos.mean())
model = SGDOneClassSVM(nu=0.01, verbose=10)#, tol=1e-10)
print(""fit 1 sample"")
model.partial_fit(xPos[:1], y) #gives avg loss 0.0
print(""fit 100 samples"")
model.partial_fit(xPos[:100], y)

#demonstrate that model output(decision function) is far from expected
# by comparing with a slow reference model
print(""\n\nreference comparison:"")
testNu = 0.5
modelSgd = make_pipeline( MinMaxScaler(), SGDOneClassSVM(nu=testNu, verbose=10) )
modelSgd.fit(x)
modelSgdDecFn = modelSgd.decision_function(x)
modelSgdClass = modelSgd.predict(x)

refMinMaxModel = make_pipeline( MinMaxScaler(), OneClassSVM(nu=testNu, verbose=10, tol=1e-10, kernel=""linear"", shrinking=False) ).fit(x)
refMinMax = refMinMaxModel.decision_function(x)
refMinMaxClass = refMinMaxModel.predict(x)

#slow sgd, manually tuned learning rate model
invScalingPower = np.emath.logn( len(x) * 9000, 100 ) #target lr reduction coefficient after N samples (len(x) * epoch count to get sample count)
#invScalingPower =0.2
print(""invScalingPower"", invScalingPower )
sgdMinMaxModel = make_pipeline( MinMaxScaler(), SGDOneClassSVM(nu=testNu, verbose=10, max_iter=10000//10, tol=None,
    learning_rate=""constant"", eta0=0.001, power_t=invScalingPower, average=True) ).fit(x)#, sgdoneclasssvm__coef_init=np.random.normal(-100.0, 1.0, x.shape[1])) # sgdoneclasssvm__offset_init=1000)
sgdMinMax = sgdMinMaxModel.decision_function(x)
sgdMinMaxClass = sgdMinMaxModel.predict(x)


df = pd.DataFrame({""sklearn default modelSgd"": modelSgdDecFn,  ""reference ocsvm refMinMax"": refMinMax, ""manual stopping sgdMinMax"": sgdMinMax, })
print(""decision function:\n"", df)
print(""correlation between decision_function:\n"", df.corr())

df = pd.DataFrame({""sklearn default modelSgd"": modelSgdClass,  ""reference ocsvm refMinMax"": refMinMaxClass, ""manual stopping sgdMinMax"": sgdMinMaxClass, })
print(""correlation between predict:\n"", df.corr())
```

### Expected Results

Model convergence criteria works adequately (especially important for the most efficient learning_rate=""adaptive""), and model reaches convergence.

### Actual Results

Optimization stops prematurely, usually after 6 epochs:
```
fitting different feature counts:

10k samples, 10 features
-- Epoch 1
Norm: 1.41, NNZs: 10, Bias: -5.067461, T: 10000, Avg. loss: 0.000088
Total training time: 0.00 seconds.
-- Epoch 2
Norm: 1.66, NNZs: 10, Bias: -6.148603, T: 20000, Avg. loss: 0.000187
Total training time: 0.00 seconds.
-- Epoch 3
Norm: 1.80, NNZs: 10, Bias: -6.766193, T: 30000, Avg. loss: 0.000267
Total training time: 0.00 seconds.
-- Epoch 4
Norm: 1.87, NNZs: 10, Bias: -7.203003, T: 40000, Avg. loss: 0.000191
Total training time: 0.00 seconds.
-- Epoch 5
Norm: 1.97, NNZs: 10, Bias: -7.528345, T: 50000, Avg. loss: 0.000193
Total training time: 0.00 seconds.
-- Epoch 6
Norm: 2.01, NNZs: 10, Bias: -7.798230, T: 60000, Avg. loss: 0.000285
Total training time: 0.00 seconds.
Convergence after 6 epochs took 0.01 seconds

10k samples, 1000 features
-- Epoch 1
Norm: 0.95, NNZs: 1000, Bias: -5.741972, T: 10000, Avg. loss: 0.000000
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 0.47, NNZs: 1000, Bias: -7.123019, T: 20000, Avg. loss: 0.000000
Total training time: 0.02 seconds.
-- Epoch 3
Norm: 0.32, NNZs: 1000, Bias: -7.932197, T: 30000, Avg. loss: 0.000000
Total training time: 0.03 seconds.
-- Epoch 4
Norm: 0.24, NNZs: 1000, Bias: -8.506685, T: 40000, Avg. loss: 0.000000
Total training time: 0.05 seconds.
-- Epoch 5
Norm: 0.38, NNZs: 1000, Bias: -8.948081, T: 50000, Avg. loss: 0.000001
Total training time: 0.06 seconds.
-- Epoch 6
Norm: 0.32, NNZs: 1000, Bias: -9.312374, T: 60000, Avg. loss: 0.000000
Total training time: 0.07 seconds.
Convergence after 6 epochs took 0.07 seconds

10k samples, 25000 features
-- Epoch 1
Norm: 4.73, NNZs: 25000, Bias: -5.741972, T: 10000, Avg. loss: 0.000000
Total training time: 0.24 seconds.
-- Epoch 2
Norm: 2.37, NNZs: 25000, Bias: -7.123019, T: 20000, Avg. loss: 0.000000
Total training time: 0.48 seconds.
-- Epoch 3
Norm: 1.58, NNZs: 25000, Bias: -7.932197, T: 30000, Avg. loss: 0.000000
Total training time: 0.71 seconds.
-- Epoch 4
Norm: 1.19, NNZs: 25000, Bias: -8.506685, T: 40000, Avg. loss: 0.000000
Total training time: 0.95 seconds.
-- Epoch 5
Norm: 0.95, NNZs: 25000, Bias: -8.952446, T: 50000, Avg. loss: 0.000000
Total training time: 1.19 seconds.
-- Epoch 6
Norm: 0.79, NNZs: 25000, Bias: -9.316738, T: 60000, Avg. loss: 0.000000
Total training time: 1.43 seconds.
Convergence after 6 epochs took 1.43 seconds


partial fit test:
xPos data mean 100.00000000000011
fit 1 sample
-- Epoch 1
Norm: 9397.76, NNZs: 625, Bias: 4.722997, T: 1, Avg. loss: 0.000000
Total training time: 0.00 seconds.
fit 100 samples
-- Epoch 1
Norm: 3262.77, NNZs: 625, Bias: 2.619430, T: 100, Avg. loss: 0.000000
Total training time: 0.00 seconds.


reference comparison:
-- Epoch 1
Norm: 1.21, NNZs: 625, Bias: -13.904809, T: 10000, Avg. loss: 0.001472
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 1.32, NNZs: 625, Bias: -15.210391, T: 20000, Avg. loss: 0.001823
Total training time: 0.02 seconds.
-- Epoch 3
Norm: 1.38, NNZs: 625, Bias: -15.971154, T: 30000, Avg. loss: 0.002099
Total training time: 0.03 seconds.
-- Epoch 4
Norm: 1.43, NNZs: 625, Bias: -16.509796, T: 40000, Avg. loss: 0.002299
Total training time: 0.04 seconds.
-- Epoch 5
Norm: 1.46, NNZs: 625, Bias: -16.926977, T: 50000, Avg. loss: 0.002480
Total training time: 0.04 seconds.
-- Epoch 6
Norm: 1.49, NNZs: 625, Bias: -17.267392, T: 60000, Avg. loss: 0.002436
Total training time: 0.05 seconds.
Convergence after 6 epochs took 0.05 seconds
..*
optimization finished, #iter = 2497
obj = 1924865835.187691, rho = 776455.657200
nSV = 5001, nBSV = 4999
[LibSVM]invScalingPower 0.25143814733164016
-- Epoch 1
Norm: 0.40, NNZs: 625, Bias: -3.925000, T: 10000, Avg. loss: 0.000117
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 0.79, NNZs: 625, Bias: -8.770000, T: 20000, Avg. loss: 0.000492
Total training time: 0.02 seconds.
-- Epoch 3

...

-- Epoch 998
Norm: 24.84, NNZs: 625, Bias: -309.593000, T: 9980000, Avg. loss: 1.322528
Total training time: 14.50 seconds.
-- Epoch 999
Norm: 24.80, NNZs: 625, Bias: -309.597000, T: 9990000, Avg. loss: 1.315051
Total training time: 14.52 seconds.
-- Epoch 1000
Norm: 24.80, NNZs: 625, Bias: -309.597000, T: 10000000, Avg. loss: 1.310013
Total training time: 14.53 seconds.
decision function:
       sklearn default modelSgd  reference ocsvm refMinMax  manual stopping sgdMinMax
0                     0.152997               -8169.563012                  -2.922611
1                     0.208675               -5888.609526                  -2.068050
2                     0.460575                4671.620928                   1.890327
3                     0.324515                -993.308774                  -0.232038
4                     0.376079                1109.737641                   0.558291
...                        ...                        ...                        ...
9995                  0.203837               -5984.511457                  -2.103481
9996                  0.263156               -3678.767938                  -1.238047
9997                  0.408527                2547.241215                   1.097869
9998                  0.408359                2477.709831                   1.068493
9999                  0.534379                7887.142835                   3.098472

[10000 rows x 3 columns]
correlation between decision_function:
                            sklearn default modelSgd  reference ocsvm refMinMax  manual stopping sgdMinMax
sklearn default modelSgd                   1.000000                   0.999918                   0.999919
reference ocsvm refMinMax                  0.999918                   1.000000                   1.000000
manual stopping sgdMinMax                  0.999919                   1.000000                   1.000000
correlation between predict:
                            sklearn default modelSgd  reference ocsvm refMinMax  manual stopping sgdMinMax
sklearn default modelSgd                   1.000000                   0.195700                   0.203691
reference ocsvm refMinMax                  0.195700                   1.000000                   0.960769
manual stopping sgdMinMax                  0.203691                   0.960769                   1.000000
```

### Versions

```shell
System:
    python: 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)]
executable: E:\Program Files\Python\Python37\python.exe
   machine: Windows-10-10.0.17763-SP0

Python dependencies:
          pip: 24.0
   setuptools: 68.0.0
      sklearn: 1.0.2
        numpy: 1.21.6
        scipy: 1.7.3
       Cython: 3.0.11
       pandas: 1.3.5
   matplotlib: 3.5.3
       joblib: 1.0.1
threadpoolctl: 2.1.0

Built with OpenMP: True
```
",Syntax Error,Syntax Error
"argsort incorrectly handles very small floating-point numbers and -0.0 compared to other libraries (PyTorch and JAX) ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using TensorFlow's argsort function on an array containing small floating-point numbers and both 0.0 and -0.0, the sort order is incorrect compared to other deep learning libraries such as PyTorch and JAX. TensorFlow incorrectly places 1.401298464324817e-45 (a very small positive number) before 0.0 and -0.0.

Expected behavior is that both 0.0 and -0.0 should be treated as equivalent and placed before any positive number, including very small ones like 1.401298464324817e-45. However, TensorFlow does not follow this behavior, whereas PyTorch correctly handles this.
```
import numpy as np
import torch
import tensorflow as tf
import jax.numpy as jnp

def test_argsort():
    # Input data, hardcoded as float32
    input_data = np.array([
        -0.0, 1.401298464324817e-45, 1.100000023841858, -0.0,
        5.960464477539063e-08, -2.0000100135803223, 1000000.0,
        722801.375, 0.0, -1.100000023841858
    ], dtype=np.float32)

    # PyTorch argsort
    pytorch_result = torch.argsort(torch.tensor(input_data, dtype=torch.float32)).numpy()
    print(f""PyTorch argsort result: {pytorch_result}"")

    # TensorFlow argsort
    tensorflow_result = tf.argsort(input_data).numpy().astype(np.int32)
    print(f""TensorFlow argsort result: {tensorflow_result}"")

    # JAX argsort
    jax_result = jnp.argsort(input_data).astype(np.int32)
    print(f""JAX argsort result: {jax_result}"")

if __name__ == ""__main__"":
    test_argsort()

```

### Standalone code to reproduce the issue

```shell
PyTorch argsort result: [5 9 0 3 8 1 4 2 7 6]
TensorFlow argsort result: [5 9 0 1 3 8 4 2 7 6]
JAX argsort result: [5 9 0 1 3 8 4 2 7 6]
```
Expected Behavior:
TensorFlow's argsort should place 0.0 and -0.0 before any positive number, including very small values like 1.401298464324817e-45. PyTorch demonstrates the correct behavior by treating 0.0 and -0.0 as equal and placing them in the correct order relative to other values.

Standalone Code to Reproduce the Issue:
The above Python code demonstrates the issue. It uses the same input data for PyTorch, TensorFlow, and JAX to show the difference in behavior. TensorFlow and JAX produce incorrect results by misplacing the small positive value before 0.0, while PyTorch produces the correct order.

Relevant Log Output:
No error logs are generated, but the incorrect behavior is clearly shown in the sorting results.
```


### Relevant log output

_No response_",Logical Bug,Logical Bug
"Exit code 137 in `tf.raw_ops.ResizeNearestNeighborGrad` ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value in the input `size` is too large, program exceeds the memory limit.

### Standalone code to reproduce the issue

```shell
# Signal --4;2024-05-14 00:40:39.895950: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-05-14 00:40:39.896316: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-14 00:40:39.900744: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-14 00:40:39.956390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-05-14 00:40:40.842339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

# ResizeNearestNeighborOpGrad

import tensorflow as tf

align_corners = True
half_pixel_centers = False
grads = tf.constant(1.5e+300, shape=[1,8,16,3], dtype=tf.float64)
size = tf.constant([65534,65534], shape=[2], dtype=tf.int32)
tf.raw_ops.ResizeNearestNeighborGrad(grads=grads, size=size, align_corners=align_corners, half_pixel_centers=half_pixel_centers)
```


### Relevant log output

```shell
(tensorflow-2.16.1-orig) root@b29bda27c601:/mnt# python tests/ResizeNearestNeighborGrad.py 
2024-05-15 08:43:01.439844: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-15 08:43:01.440211: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-05-15 08:43:01.444264: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-05-15 08:43:01.498275: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-15 08:43:02.744313: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Killed
```
```
",Performance Issue,Performance Issue
"TreeView nesting performance TreeView has some serious performance problems at high nesting levels
(unrelated to https://github.com/microsoft/vscode/issues/232263)

Steps to Reproduce:

1. create JSON file with snippet
```json
[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]]
```
2. open OutLine
3. scroll down to the bottom
4. freeze ??

![Image](https://github.com/user-attachments/assets/eff89294-d304-4072-a5ed-8ad71789502d)

performance is also exaggerated more by StickyScroll `""workbench.tree.enableStickyScroll"": true`
this also affects the dropdown Breadcrumbs view

dup https://github.com/microsoft/vscode/issues/205840

cc @alexr00 

Does this issue occur when all extensions are disabled?: Yes

- VS Code Version: 1.96.0
- OS Version: Windows 11
",Performance Issue,Performance Issue
"TSNE implementation finds variation where there is none #### Describe the bug
Manifold.TSNE gives back a lower-dimensional representation of the input data, which contains variation, even if the input data only contains zeros (it seems to do this in all cases where all input samples are the same).

#### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.manifold import TSNE

f = TSNE(n_components=2)

zero_array = np.zeros((993, 20))
tsne_array = f.fit_transform(zero_array)

```

#### Expected Results
A warning, or all data mapped to the same point.

#### Actual Results

A lower-dimensional array (here (993, 2) shape) with different vectors!
```python
[[-0.21821421 -0.16358966]
 [-0.21803701 -0.16340736]
 [-0.21847947 -0.16356741]
 ...
 [-0.07215934  0.15475279]
 [-0.4158346   0.2995374 ]
 [-0.21808246 -0.16424587]]
```

#### Versions

Python 3.7.5
NumPy 1.17.4
SciPy 1.3.2
Scikit-Learn 0.21.3

<!-- Thanks for contributing! -->
",UI/UX Bug,Logical Bug
"Wrong Mutual Information Calculation ### Describe the bug

#### Issue
I encountered a bug unexpectedly while reviewing some metrics in a project.
When calculating mutual information using the `mutual_info_classif`, I noticed values higher than entropy, which is [impossible](https://en.wikipedia.org/wiki/Mutual_information#/media/File:Figchannel2017ab.svg). There is no such issue with `mutual_info_regression` (although, there, the self-mi is far from entropy, which may be another interesting case).

##### Implication
Any algorithm sorting features based on `mutual_info_classif` or any metric based on this function may be affected.

Thanks a lot for putting time on this.


P.S. In the minimal example, the feature is fixed (all one). However, I encountered the same issue in other scenarios as well. The example is just more simplified. The problem persists on both Linux and Mac. I attached personal computer session info.

### Steps/Code to Reproduce

```python
import numpy as np
import pandas as pd

from sklearn.feature_selection import mutual_info_classif

big_n = 1_000_000
bug_df = pd.DataFrame({
    'feature': np.ones(big_n),
    'target': (np.arange(big_n) < 100).astype(int),
})
bug_df

mi = mutual_info_classif(bug_df[['feature']], bug_df['target'])
entropy = mutual_info_classif(bug_df[['target']], bug_df['target'])

print(f""mi: {mi[0] :.6f}"")
print(f""self-mi (entropy): {entropy[0] :.6f}"")

from scipy import stats

scipy_entropy = stats.entropy([bug_df['target'].mean(), 1 - bug_df['target'].mean()])

print(f""scipy entropy: {scipy_entropy :.6f}"")
```

### Expected Results

```
mi: 0.000000
self-mi (entropy): 0.001023
scipy entropy: 0.001021
```

### Actual Results

```
mi: 0.215495
self-mi (entropy): 0.001023
scipy entropy: 0.001021
```

### Versions

```shell
System:
    python: 3.10.13 (main, Sep 11 2023, 08:16:02) [Clang 14.0.6 ]
executable: /Users/*/miniconda3/envs/*/bin/python
   machine: macOS-15.1.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.6.1
          pip: 23.3.1
   setuptools: 68.2.2
        numpy: 1.26.1
        scipy: 1.15.1
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.8.2
       joblib: 1.3.2
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /Users/*/miniconda3/envs/*/lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: /Users/*/miniconda3/envs/*/lib/python3.10/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```",Syntax Error,Syntax Error
"tf.data batching slows down on Windows <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.2 cuDNN 8.1.0

### GPU model and memory

Nvidia A40 48GB, 128GB System RAM

### Current Behaviour?


While training a keras model using a [custom tensorflow dataset](https://www.tensorflow.org/datasets/add_dataset), the training steps slow down once the remaining sample count in the first epoch is smaller than the shuffle buffer size. The slowdown carries over to the following epochs and takes longer and longer.

I would expect the opposite behavior since all remaining samples should already be in the buffer and do not need to be loaded anymore. I have tested it on multiple Windows systems and this slowdown occurred on all of them. However, it does not happen on Linux-based systems.

I then used the tensorboard profiling plugin to investigate what is causing the slowdown. As you can see here, it seems to be an input-related problem:
![profiler_1](https://user-images.githubusercontent.com/1509163/166112234-b795f11e-934f-4631-86b1-c8d79e1e1448.png)

From the input operations you can see that `Iterator::Root::Prefetch::BatchV2` takes the most time (this is also the case when removing the prefetching):
![profiler_2](https://user-images.githubusercontent.com/1509163/166112243-798a33a9-b329-4103-9d24-b0b70306411d.png)

The slowdown can also be seen in the trace viewer:
![profiler_3](https://user-images.githubusercontent.com/1509163/166112328-9e637031-1166-4ac6-afef-47a8a534bf9b.png)

Here is a more detailed comparison of `BatchV2` durations from different steps:
![profiler_4](https://user-images.githubusercontent.com/1509163/166112335-119bb08e-7cd2-4a4b-a149-0de1580332a4.png)
![profiler_5](https://user-images.githubusercontent.com/1509163/166112342-dd0ea8a4-9d79-4ec3-adc9-87ac133e118e.png)


Here is the [profiling_data.zip](https://github.com/tensorflow/tensorflow/files/8597248/profiling_data.zip).



### Standalone code to reproduce the issue

In order to reproduce and profile this issue the following code can be used. When running the `main.py` for the first time it will generate the dummy dataset which takes around 20 minutes to write 24GB of dummy data.

**dummy_dataset.py**
```python
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds


class DummyDataset(tfds.core.GeneratorBasedBuilder):
    VERSION = tfds.core.Version('1.0.0')
    RELEASE_NOTES = {
        '1.0.0': 'Initial release.',
    }

    def _info(self) -> tfds.core.DatasetInfo:
        return tfds.core.DatasetInfo(
            builder=self,
            features=tfds.features.FeaturesDict({
                'audio': tfds.features.Audio(shape=(16000,), dtype=tf.float32),
                'label': tfds.features.Tensor(shape=(2,), dtype=tf.float32),
            }),
            supervised_keys=('audio', 'label')
        )

    def _split_generators(self, dl_manager: tfds.download.DownloadManager):
        return {
            'train': self._generate_examples(),
        }

    def _generate_examples(self):
        for i in range(400000): # If sample count is increased, issue starts later (tested with 1M+)
            yield i, {
                'audio': np.random.sample(16000).astype(dtype=np.float32),
                'label': tf.one_hot(np.random.choice(2), depth=2).numpy()
            }
```



**main.py**
```python
from dummy_dataset import DummyDataset
import tensorflow_datasets as tfds
import tensorflow as tf
import os
import datetime
from tensorflow import keras

# Configuration
shuffle_buffer = 300000 # Uses ~30GB RAM, can be lowered to 150000 if only 16GB RAM available
batch_size = 512
epochs = 3

optimizer = keras.optimizers.Adam(learning_rate=0.001)
losses = ['categorical_crossentropy']
metrics = [keras.metrics.CategoricalAccuracy()]

folder_name = datetime.datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")

# Prepare dataset (takes ~20min for the first time to generate ~24GB of dummy data)
ds_train, ds_info = tfds.load(
    name='DummyDataset',
    split='train',
    as_supervised=True,
    with_info=True,
    shuffle_files=True,
)
ds_train = ds_train.shuffle(shuffle_buffer)
ds_train = ds_train.batch(batch_size)
ds_train = ds_train.prefetch(tf.data.AUTOTUNE)

# Configure profiling
sample_count = ds_info.splits['train'].num_examples
slowdown_start = int((sample_count - shuffle_buffer) / batch_size)
profile_stop = slowdown_start + 150

callbacks = [keras.callbacks.TensorBoard(
    log_dir=os.path.join(os.getcwd(), 'tensorboard', folder_name),
    profile_batch=[slowdown_start, profile_stop]
)]

# Build and train model
input_layer = keras.layers.Input(shape=(16000,), dtype=tf.float32, name='audio_input')
output_layer = keras.layers.Dense(2, activation='softmax', name='prediction')(input_layer)
model = keras.Model(inputs=input_layer, outputs=output_layer, name='dummy_model')

model.compile(optimizer=optimizer, loss=losses, metrics=metrics)
model.summary()

print(f'Expected slowdown to start at batch {slowdown_start}, profiling batches {slowdown_start}-{profile_stop}')

model.fit(x=ds_train, epochs=epochs, verbose=1, callbacks=callbacks)
```

**requirements.txt**
```
tensorflow==2.8.0
tensorflow-datasets==4.5.2
tensorboard-plugin-profile
numpy
```


### Relevant log output

_No response_</details>",Performance Issue,Performance Issue
"[DevTools Bug] Children cannot be added or removed during a reorder operation. ### Website or app

https://github.com/HamaydaGabsi/Color-Project

### Repro steps

1. Select a palette
2. Select to color to view shades
3. Go back to the colors with the navigation arrow

### How often does this bug happen?

Every time

### DevTools package (automated)

react-devtools-extensions

### DevTools version (automated)

4.25.0-336ac8ceb

### Error message (automated)

Children cannot be added or removed during a reorder operation.

### Error call stack (automated)

```text
at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:26848:41
    at bridge_Bridge.emit (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24626:22)
    at chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:24795:14
    at listener (chrome-extension://fmkadmapgofadopljbjfkapdkoienihi/build/main.js:54959:39)
```


### Error component stack (automated)

_No response_

### GitHub query string (automated)

```text
https://api.github.com/search/issues?q=Children cannot be added or removed during a reorder operation. in:title is:issue is:open is:public label:""Component: Developer Tools"" repo:facebook/react
```
",UI/UX Bug,UI/UX Bug
"//tensorflow/python/distribute/failure_handling:gce_failure_handler_test is flaky <details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute/failure_handling:gce_failure_handler_test sometimes fails or timeouts.

x86 log
https://source.cloud.google.com/results/invocations/66d8ddaa-3dbe-4464-837c-053157b11659/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5354839739/jobs/9712379821#step:5:12470

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
INFO:tensorflow:time(__main__.GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker): 5.11s
I0628 05:40:30.221971 140074262497088 test_util.py:2464] time(__main__.GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker): 5.11s
[       OK ] GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker
======================================================================
FAIL: test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker (__main__.GceFailureHandlingTest)
GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker
test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker(api_wrapping_train=False, grace_period=7, input_arg='checkpoint', strategy_option='MWMS_multi_worker')
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test
    return test_method(self, **testcase_params)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360, in decorated
    execute_test_method()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343, in execute_test_method
    test_method(**kwargs_to_pass)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/combinations.py"", line 559, in decorator
    test_method(self, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.py"", line 417, in test_multiple_workers_preempted_consecutively
    mpr.join(timeout=250)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 649, in join
    self._reraise_if_subprocess_error(process_statuses)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 565, in _reraise_if_subprocess_error
    six.reraise(*process_status.exc_info)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/six_archive/six.py"", line 719, in reraise
    raise value
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 1060, in _run_contained
    return_value = fn(*args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.py"", line 211, in worker_fn
    self.assertNotEmpty(checkpoint_index)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/absl_py/absl/testing/absltest.py"", line 972, in assertNotEmpty
    self.fail('{!r} has length of 0.'.format(container), msg)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/absl_py/absl/testing/absltest.py"", line 1814, in fail
    return super(TestCase, self).fail(self._formatMessage(prefix, msg))
  File ""/usr/lib/python3.9/unittest/case.py"", line 676, in fail
    raise self.failureException(msg)
AssertionError: [] has length of 0.
```
</details>",Runtime Error,Runtime Error
"Inconsistent crossbrowser onBeforeInput paste event behavior ### React version: 17.0.2
In Chrome, pasting triggers `onPaste` and `onBeforeInput`, with the `nativeEvent`s `ClipboardEvent` and `TextEvent` respectively.

In Firefox, pasting only triggers `onPaste` with `nativeEvent` `paste`.

If I'm not mistaken, this seems to be the code responsible for not triggering `onBeforeInput` on native `paste` events:
https://github.com/facebook/react/blob/27c9c95e23ddedb9163373950e364dd62038f6c0/packages/react-dom/src/events/plugins/BeforeInputEventPlugin.js#L328-L331

### React version: 17.0.2 and 18.0.0

17.0.2
https://codesandbox.io/s/condescending-cerf-e1qt9?file=/src/App.js
18.0.0
https://codesandbox.io/s/sparkling-sun-fylti?file=/src/App.js",UI/UX Bug,UI/UX Bug
"terminal shell integration doesn't work for screen reader users sometimes From @jooyoungseo

waiting for the logs

![Image](https://github.com/user-attachments/assets/8c1bbcd1-c06f-4dd3-9cc3-49d91d730ab2)
",UI/UX Bug,UI/UX Bug
"Find & Replace in Files => Very Slow on Mac 
Type: <b>Bug</b>

After recent upgrade it became super slow. I executed ""Find and Replace"" action in files, about 200 files, about 400 replacements, it created many ""rg"" processes on MacOs which slowed Mac for about 5-10 minutes, coplete slowdown!!! It reports that files were processed, but I see ""rg"" processes very long time, not sure, I feel it hangs, I cannot run any Terminal commands for example. Happened few times already.

VS Code version: Code 1.95.3 (Universal) (f1a4fb101478ce6ec82fe9627c43efbf9e98c813, 2024-11-13T14:50:04.152Z)
OS version: Darwin arm64 23.6.0
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Apple M1 Max (10 x 2400)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|854, 236, 512|
|Memory (System)|64.00GB (1.14GB free)|
|Process Argv|--crash-reporter-id 176211f7-0493-4455-86c6-1e05a06f6d84|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (45)</summary>

Extension|Author (truncated)|Version
---|---|---
project-manager|ale|12.8.0
calva|bet|2.0.481
calva-spritz|bet|1.0.5
vscode-tailwindcss|bra|0.12.14
dart-code|Dar|3.100.0
flutter|Dar|3.100.0
code-runner|for|0.12.2
get-snippets|get|4.3.0
go|gol|0.42.1
arb-editor|Goo|0.2.1
firebase-snippets|has|0.0.1
vscode-drawio|hed|1.6.6
Sbt|itr|0.1.7
flutter-tree|mar|1.0.0
git-graph|mhu|1.30.0
xml-format|mik|1.1.3
inline-fold|moa|0.2.6
vscode-docker|ms-|1.29.3
debugpy|ms-|2024.12.0
python|ms-|2024.20.0
vscode-pylance|ms-|2024.11.2
jupyter|ms-|2024.10.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.21
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.388.0
vscode-yaml-sort|Pas|6.6.0
vscode-thunder-client|ran|2.30.0
java|red|1.36.0
LiveServer|rit|5.7.9
flutter-riverpod-snippets|rob|1.2.2
scala|sca|0.5.8
markdown-preview-enhanced|shd|0.8.15
iconfont-preview|stx|0.0.5
even-better-toml|tam|0.19.2
intellicode-api-usage-examples|Vis|0.2.9
vscodeintellicode|Vis|1.3.2
vscode-gradle|vsc|3.16.4
vscode-java-debug|vsc|0.58.1
vscode-java-dependency|vsc|0.24.1
vscode-java-pack|vsc|0.29.0
vscode-java-test|vsc|0.43.0
vscode-maven|vsc|0.44.0
markdown-all-in-one|yzh|3.6.2


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
da93g388:31013173
dvdeprecation:31068756
dwnewjupytercf:31046870
2f103344:31071589
nativerepl1:31139838
pythonrstrctxt:31112756
cf971741:31144450
iacca1:31171482
notype1:31157159
5fd0e150:31155592
dwcopilot:31170013
stablechunks:31184530

```

</details>

<!-- generated by issue reporter -->",Performance Issue,Performance Issue
"deleted files don't go to trash 
Type: <b>Bug</b>

I delete files in the explorer using `del` key of keyboard, files are deleted and cannot be found in the trash bin

VS Code version: Code 1.95.2 (e8653663e8840adaf45af01eab5c627a5af81807, 2024-11-07T11:07:22.054Z)
OS version: Linux x64 6.8.0-48-generic snap
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|13th Gen Intel(R) Core(TM) i7-1365U (12 x 3700)|
|GPU Status|2d_canvas: unavailable_software<br>canvas_oop_rasterization: disabled_off<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: disabled_software<br>multiple_raster_threads: enabled_on<br>opengl: disabled_off<br>rasterization: disabled_software<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: disabled_software<br>video_encode: disabled_software<br>vulkan: disabled_off<br>webgl: unavailable_software<br>webgl2: unavailable_software<br>webgpu: disabled_off<br>webnn: unavailable_software|
|Load (avg)|2, 2, 1|
|Memory (System)|31.00GB (22.61GB free)|
|Process Argv|--no-sandbox --force-user-env --crash-reporter-id 8447ec3e-1827-4960-bb59-12316efae9e7|
|Screen Reader|no|
|VM|0%|
|DESKTOP_SESSION|ubuntu|
|XDG_CURRENT_DESKTOP|Unity|
|XDG_SESSION_DESKTOP|ubuntu|
|XDG_SESSION_TYPE|wayland|
</details><details><summary>Extensions (14)</summary>

Extension|Author (truncated)|Version
---|---|---
gitlens|eam|15.6.3
ftp-simple|hum|0.7.6
git-graph|mhu|1.30.0
autopep8|ms-|2024.0.0
debugpy|ms-|2024.12.0
python|ms-|2024.18.1
vscode-pylance|ms-|2024.11.1
jupyter|ms-|2024.10.0
jupyter-keymap|ms-|1.1.2
jupyter-renderers|ms-|1.0.21
vscode-jupyter-cell-tags|ms-|0.1.9
vscode-jupyter-slideshow|ms-|0.1.6
remote-containers|ms-|0.388.0
LiveServer|rit|5.7.9


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vscod805cf:30301675
binariesv615:30325510
vsaa593:30376534
py29gd2263:31024239
c4g48928:30535728
azure-dev_surveyone:30548225
2i9eh265:30646982
962ge761:30959799
pythongtdpath:30769146
pythonnoceb:30805159
asynctok:30898717
pythonmypyd1:30879173
h48ei257:31000450
pythontbext0:30879054
cppperfnew:31000557
dsvsc020:30976470
pythonait:31006305
dsvsc021:30996838
bdiig495:31013172
dvdeprecation:31068756
dwnewjupyter:31046869
impr_priority:31102340
nativerepl2:31139839
refactort:31108082
pythonrstrctxt:31112756
cf971741:31144450
iacca1:31171482
notype1cf:31157160
5fd0e150:31155592
dwcopilot:31170013
j44ff735:31177056

```

</details>

<!-- generated by issue reporter -->",Security Vulnerability,Security Vulnerability
" Test_on_batch() gives the same loss output on different batches in a single run ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes


### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

windows 10

### Python version

3.10.12

### Current behavior?

I noticed the problem when I got a straight horizontal line on plotting the test results on the trained network. I used the sequential models.

I use train_on_batch(), which gives converging losses. When I switch to test_on_batch(), the losses remain the same for different batches. When I restart the test with different test files, it will give a different loss value, which remains the same for all the batches. In other words, the loss from test_on_batch() remains the some for all batches in a single run.

It's a sequential model.

Here is the code of the section:

    print('mfccs3 value = ', tf.keras.backend.eval(mfccs3[1,:]) )               
    #logs = vadModel.train_on_batch(mfccs3, vadLabel)
    logs = vadModel.test_on_batch(mfccs3, vadLabel) 
    print('string logs = ', str(logs))
The result is:
index = 1

```
mfccs3 value = [[-8.2793800e+01 -5.9538417e+00 9.8302096e-01 -3.5255635e-01
3.0392697e-01 -6.4597696e-01 2.2358397e-02 2.5344249e-02
-6.8171650e-01 -3.7053981e-01 -3.4044239e-01 -8.1056818e-02]]
```

string logs = 0.2398043

index = 2

```
mfccs3 value = [[-69.159195 -2.2269542 4.2501264 -1.3486748 0.62957734
-3.2606528 -3.253118 -3.5308673 -1.1313365 -1.1839466
-2.330786 -1.6313086 ]]
```

string logs = 0.2398043

index = 3

```
mfccs3 value = [[-64.894104 -1.892648 0.11392474 -0.81098145 -1.4640433
-1.1901256 -1.7744782 -0.85753983 -0.9694403 -0.8149232
-1.0680746 -1.0442001 ]]
```

string logs = 0.2398043

You can see that the inputs for test_on_batch() have changed. However, the loss remains the same. I use the same code for train_on_batch(), which gives converging losses.



### Standalone code to reproduce the issue

```shell
logs = vadModel.train_on_batch(mfccs3, vadLabel)
"""""" vs.  """"""
    logs = vadModel.test_on_batch(mfccs3, vadLabel) 

it's just these two lines for a sequential model.
```


### Relevant log output

```shell
I even tried latest Tensorflow version. It has the same problem.

tensorflow version: 2.15.0-dev20230817
listOfFiles 1681
2023-08-17 15:42:12.560549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
model length =  7
Model: ""sequential""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 multiple                  2640      
                                                                 
 dense (Dense)               multiple                  210       
                                                                 
 dense_1 (Dense)             multiple                  11        
                                                                 
=================================================================
Total params: 2861 (11.18 KB)
Trainable params: 2861 (11.18 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```
",Logical Bug,Logical Bug
"`notebook.addFindMatchToSelection` keeps looping Testing #235047

When pressing <kbd>Cmd D</kbd> in a regular editor, find results keep getting added to the multiple selection collection. It will be a noop, once all find results are covered by selections.

?? In notebooks, <kbd>Cmd D</kbd> will never stop looping around find results.

https://github.com/user-attachments/assets/70dccdb2-1d20-4c52-95bf-d7733e64d97e
",Performance Issue,Performance Issue
"Git - VSCode Git extension doesn't ask for remote user password, when trying to clone via ssh <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.93.0
- OS Version: Operating System: Manjaro Linux 
KDE Plasma Version: 6.1.5
KDE Frameworks Version: 6.5.0
Qt Version: 6.7.2
Kernel Version: 6.6.52-1-MANJARO (64-bit)
Graphics Platform: Wayland

I noticed that when you try to clone a repository with the Git extension via SSH and don't have public key authentication configured, VSCode doesn't ask for the remote user password.
Instead a not very meaningful error is shown:
![image](https://github.com/user-attachments/assets/859d0026-211a-4665-b015-7ce0802f4fce)
In the logs you can find more information: 
```
> git clone ssh://zzz2324188@example.com:4711/srv/pk/git/pk2425/VOB02/zzz2324188.git /home/laurenz/checkouts_pp/test/zzz2324188 --progress
/usr/lib/code/extensions/git/dist/askpass-main.js:1
(()=>{""use strict"";var e={7549:(e,s,r)=>{Object.defineProperty(s,""__esModule"",{value:!0}),s.IPCClient=void 0;const t=r(8611);s.IPCClient=class{constructor(e){this.handlerName=e;const s=process.env.VSCODE_GIT_IPC_HANDLE;if(!s)throw new Error(""Missing VSCODE_GIT_IPC_HANDLE"");this.ipcHandlePath=s}call(e){const s={socketPath:this.ipcHandlePath,path:`/${this.handlerName}`,method:""POST""};return new Promise(((r,n)=>{const o=t.request(s,(e=>{if(200!==e.statusCode)return n(new Error(`Bad status code: ${e.statusCode}`));const s=[];e.on(""data"",(e=>s.push(e))),e.on(""end"",(()=>r(JSON.parse(Buffer.concat(s).toString(""utf8"")))))}));o.on(""error"",(e=>n(e))),o.write(JSON.stringify(e)),o.end()}))}}},9896:e=>{e.exports=require(""fs"")},8611:e=>{e.exports=require(""http"")}},s={};function r(t){var n=s[t];if(void 0!==n)return n.exports;var o=s[t]={exports:{}};return e[t](o,o.exports,r),o.exports}var t={};(()=>{var e=t;Object.defineProperty(e,""__esModule"",{value:!0});const s=r(9896),n=r(7549);function o(e){console.error(""Missing or invalid credentials.""),console.error(e),process.exit(1)}!function(e){if(!process.env.VSCODE_GIT_ASKPASS_PIPE)return o(""Missing pipe"");if(!process.env.VSCODE_GIT_ASKPASS_TYPE)return o(""Missing type"");if(""https""!==process.env.VSCODE_GIT_ASKPASS_TYPE&&""ssh""!==process.env.VSCODE_GIT_ASKPASS_TYPE)return o(`Invalid type: ${process.env.VSCODE_GIT_ASKPASS_TYPE}`);if(""fetch""===process.env.VSCODE_GIT_COMMAND&&process.env.VSCODE_GIT_FETCH_SILENT)return o(""Skip silent fetch commands"");const r=process.env.VSCODE_GIT_ASKPASS_PIPE,t=process.env.VSCODE_GIT_ASKPASS_TYPE,i=""https""===t?e[2]:e[3];let c,a,p;""https""===t&&(c=e[4].replace(/^[""']+|[""':]+$/g,"""")),""ssh""===t&&(/passphrase/i.test(i)?a=e[6]?.replace(/^[""']+|[""':]+$/g,""""):(c=e[6].replace(/^[""']+|[""':]+$/g,""""),p=e[15])),new n.IPCClient(""askpass"").call({askpassType:t,request:i,host:c,file:a,fingerprint:p}).then((e=>{s.writeFileSync(r,e+""\n""),setTimeout((()=>process.exit(0)),0)})).catch((e=>o(e)))}(process.argv)})();var n=exports;for(var o in t)n[o]=t[o];t.__esModule&&Object.defineProperty(n,""__esModule"",{value:!0})})();
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            

TypeError: Cannot read properties of undefined (reading 'replace')
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1748
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1967
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1983
    at Object.<anonymous> (/usr/lib/code/extensions/git/dist/askpass-main.js:1:2089)
    at Module._compile (node:internal/modules/cjs/loader:1373:14)
    at Module._extensions..js (node:internal/modules/cjs/loader:1432:10)
    at Module.load (node:internal/modules/cjs/loader:1215:32)
    at Module._load (node:internal/modules/cjs/loader:1031:12)
    at c._load (node:electron/js2c/node_init:2:13801)
    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:189:12)

Node.js v20.16.0
Permission denied, please try again.
/usr/lib/code/extensions/git/dist/askpass-main.js:1
(()=>{""use strict"";var e={7549:(e,s,r)=>{Object.defineProperty(s,""__esModule"",{value:!0}),s.IPCClient=void 0;const t=r(8611);s.IPCClient=class{constructor(e){this.handlerName=e;const s=process.env.VSCODE_GIT_IPC_HANDLE;if(!s)throw new Error(""Missing VSCODE_GIT_IPC_HANDLE"");this.ipcHandlePath=s}call(e){const s={socketPath:this.ipcHandlePath,path:`/${this.handlerName}`,method:""POST""};return new Promise(((r,n)=>{const o=t.request(s,(e=>{if(200!==e.statusCode)return n(new Error(`Bad status code: ${e.statusCode}`));const s=[];e.on(""data"",(e=>s.push(e))),e.on(""end"",(()=>r(JSON.parse(Buffer.concat(s).toString(""utf8"")))))}));o.on(""error"",(e=>n(e))),o.write(JSON.stringify(e)),o.end()}))}}},9896:e=>{e.exports=require(""fs"")},8611:e=>{e.exports=require(""http"")}},s={};function r(t){var n=s[t];if(void 0!==n)return n.exports;var o=s[t]={exports:{}};return e[t](o,o.exports,r),o.exports}var t={};(()=>{var e=t;Object.defineProperty(e,""__esModule"",{value:!0});const s=r(9896),n=r(7549);function o(e){console.error(""Missing or invalid credentials.""),console.error(e),process.exit(1)}!function(e){if(!process.env.VSCODE_GIT_ASKPASS_PIPE)return o(""Missing pipe"");if(!process.env.VSCODE_GIT_ASKPASS_TYPE)return o(""Missing type"");if(""https""!==process.env.VSCODE_GIT_ASKPASS_TYPE&&""ssh""!==process.env.VSCODE_GIT_ASKPASS_TYPE)return o(`Invalid type: ${process.env.VSCODE_GIT_ASKPASS_TYPE}`);if(""fetch""===process.env.VSCODE_GIT_COMMAND&&process.env.VSCODE_GIT_FETCH_SILENT)return o(""Skip silent fetch commands"");const r=process.env.VSCODE_GIT_ASKPASS_PIPE,t=process.env.VSCODE_GIT_ASKPASS_TYPE,i=""https""===t?e[2]:e[3];let c,a,p;""https""===t&&(c=e[4].replace(/^[""']+|[""':]+$/g,"""")),""ssh""===t&&(/passphrase/i.test(i)?a=e[6]?.replace(/^[""']+|[""':]+$/g,""""):(c=e[6].replace(/^[""']+|[""':]+$/g,""""),p=e[15])),new n.IPCClient(""askpass"").call({askpassType:t,request:i,host:c,file:a,fingerprint:p}).then((e=>{s.writeFileSync(r,e+""\n""),setTimeout((()=>process.exit(0)),0)})).catch((e=>o(e)))}(process.argv)})();var n=exports;for(var o in t)n[o]=t[o];t.__esModule&&Object.defineProperty(n,""__esModule"",{value:!0})})();
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            

TypeError: Cannot read properties of undefined (reading 'replace')
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1748
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1967
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1983
    at Object.<anonymous> (/usr/lib/code/extensions/git/dist/askpass-main.js:1:2089)
    at Module._compile (node:internal/modules/cjs/loader:1373:14)
    at Module._extensions..js (node:internal/modules/cjs/loader:1432:10)
    at Module.load (node:internal/modules/cjs/loader:1215:32)
    at Module._load (node:internal/modules/cjs/loader:1031:12)
    at c._load (node:electron/js2c/node_init:2:13801)
    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:189:12)

Node.js v20.16.0
Permission denied, please try again.
/usr/lib/code/extensions/git/dist/askpass-main.js:1
(()=>{""use strict"";var e={7549:(e,s,r)=>{Object.defineProperty(s,""__esModule"",{value:!0}),s.IPCClient=void 0;const t=r(8611);s.IPCClient=class{constructor(e){this.handlerName=e;const s=process.env.VSCODE_GIT_IPC_HANDLE;if(!s)throw new Error(""Missing VSCODE_GIT_IPC_HANDLE"");this.ipcHandlePath=s}call(e){const s={socketPath:this.ipcHandlePath,path:`/${this.handlerName}`,method:""POST""};return new Promise(((r,n)=>{const o=t.request(s,(e=>{if(200!==e.statusCode)return n(new Error(`Bad status code: ${e.statusCode}`));const s=[];e.on(""data"",(e=>s.push(e))),e.on(""end"",(()=>r(JSON.parse(Buffer.concat(s).toString(""utf8"")))))}));o.on(""error"",(e=>n(e))),o.write(JSON.stringify(e)),o.end()}))}}},9896:e=>{e.exports=require(""fs"")},8611:e=>{e.exports=require(""http"")}},s={};function r(t){var n=s[t];if(void 0!==n)return n.exports;var o=s[t]={exports:{}};return e[t](o,o.exports,r),o.exports}var t={};(()=>{var e=t;Object.defineProperty(e,""__esModule"",{value:!0});const s=r(9896),n=r(7549);function o(e){console.error(""Missing or invalid credentials.""),console.error(e),process.exit(1)}!function(e){if(!process.env.VSCODE_GIT_ASKPASS_PIPE)return o(""Missing pipe"");if(!process.env.VSCODE_GIT_ASKPASS_TYPE)return o(""Missing type"");if(""https""!==process.env.VSCODE_GIT_ASKPASS_TYPE&&""ssh""!==process.env.VSCODE_GIT_ASKPASS_TYPE)return o(`Invalid type: ${process.env.VSCODE_GIT_ASKPASS_TYPE}`);if(""fetch""===process.env.VSCODE_GIT_COMMAND&&process.env.VSCODE_GIT_FETCH_SILENT)return o(""Skip silent fetch commands"");const r=process.env.VSCODE_GIT_ASKPASS_PIPE,t=process.env.VSCODE_GIT_ASKPASS_TYPE,i=""https""===t?e[2]:e[3];let c,a,p;""https""===t&&(c=e[4].replace(/^[""']+|[""':]+$/g,"""")),""ssh""===t&&(/passphrase/i.test(i)?a=e[6]?.replace(/^[""']+|[""':]+$/g,""""):(c=e[6].replace(/^[""']+|[""':]+$/g,""""),p=e[15])),new n.IPCClient(""askpass"").call({askpassType:t,request:i,host:c,file:a,fingerprint:p}).then((e=>{s.writeFileSync(r,e+""\n""),setTimeout((()=>process.exit(0)),0)})).catch((e=>o(e)))}(process.argv)})();var n=exports;for(var o in t)n[o]=t[o];t.__esModule&&Object.defineProperty(n,""__esModule"",{value:!0})})();
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            

TypeError: Cannot read properties of undefined (reading 'replace')
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1748
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1967
    at /usr/lib/code/extensions/git/dist/askpass-main.js:1:1983
    at Object.<anonymous> (/usr/lib/code/extensions/git/dist/askpass-main.js:1:2089)
    at Module._compile (node:internal/modules/cjs/loader:1373:14)
    at Module._extensions..js (node:internal/modules/cjs/loader:1432:10)
    at Module.load (node:internal/modules/cjs/loader:1215:32)
    at Module._load (node:internal/modules/cjs/loader:1031:12)
    at c._load (node:electron/js2c/node_init:2:13801)
    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:189:12)

Node.js v20.16.0
zzz2324188@example.com: Permission denied (publickey,password).
fatal: Could not read from remote repository.

Please make sure you have the correct access rights
and the repository exists.
```

I am aware, that the optimal solution would be to copy the public key to the remote and let SSH handle the authentication, or use HTTPS and the Git credential manager (which I have installed, but apparently it doesn't open up for SSH connections).
But there can be situations where none of this workarounds are viable.

Steps to Reproduce:

1. F1 
2. Git Clone
3. Enter something like this `ssh://user@example.com:4711/path/to/your/repo.git


Edit: The same error occurs for all other Git actions that involve the remote, like pushing, pulling, etc.",Security Vulnerability,Security Vulnerability
"Inaccurate Attribute Listing with dir(obj) for Classes Using available_if Conditional Method Decorator ### Describe the bug

When utilizing the `available_if` decorator from SciKit Learn to conditionally expose methods based on specific object state or conditions, we observe that the `dir(obj)` function may return inaccurate results. Specifically, `dir(obj)` continues to list methods that should be conditionally hidden based on the `available_if` decorator's logic. This discrepancy arises because the `__dir__` method on the affected classes does not dynamically account for this conditional availability. As a result, users and consuming code may be misled about the actual methods available for use on instances of the class at runtime, potentially leading to unexpected `AttributeErrors` when accessing supposedly available methods.

### Steps/Code to Reproduce

I will test with the SVC, but it can apply to other classes.

```python
from sklearn.svm import SVC
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)

model = SVC(probability=False)
model.fit(X[:100], y[:100])

# Check if 'predict_proba' is listed by dir()
print(""'predict_proba' in dir(model):"", ""predict_proba"" in dir(model))

# Attempt to call 'predict_proba'
try:
    prob_predictions = model.predict_proba(X[:2])
    print(""Predict_proba called successfully."")
except AttributeError as e:
    print(""Attempting to call 'predict_proba' raised an AttributeError:"", e)

```

### Expected Results

It should print out the following:
```
'predict_proba' in dir(model): False
Attempting to call 'predict_proba' raised an AttributeError: predict_proba is not available when probability=False
```

Methods decorated with `available_if` and whose conditions raise an `AttributeError` should not appear in the list returned by `dir(obj)`.

### Actual Results

It should print out the following:
```
'predict_proba' in dir(model): True
Attempting to call 'predict_proba' raised an AttributeError: predict_proba is not available when probability=False
```

Methods decorated with `available_if` and whose conditions raise an `AttributeError` appears in the list returned by `dir(obj)`.

Traceback when you show the error itself (comes from the check in `available_if`.
```
Traceback (most recent call last):
  File ""/Users/kmcgrady/Projects/test-scripts/xg_example.py"", line 48, in <module>
    prob_predictions = model.predict_proba(X[:2])
  File ""/Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/lib/python3.10/site-packages/sklearn/utils/_available_if.py"", line 31, in __get__
    if not self.check(obj):
  File ""/Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/lib/python3.10/site-packages/sklearn/svm/_base.py"", line 827, in _check_proba
    raise AttributeError(
AttributeError: predict_proba is not available when probability=False
```

### Versions

```shell
System:
    python: 3.10.10 (main, May 13 2023, 16:09:51) [Clang 14.0.3 (clang-1403.0.22.14.1)]
executable: /Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/bin/python
   machine: macOS-14.3.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.3.2
          pip: 23.3.1
   setuptools: 68.2.2
        numpy: 1.26.4
        scipy: 1.11.4
       Cython: None
       pandas: 2.2.1
   matplotlib: 3.8.2
       joblib: 1.3.2
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
    num_threads: 10
         prefix: libomp
       filepath: /Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/lib/python3.10/site-packages/sklearn/.dylibs/libomp.dylib
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 10
         prefix: libopenblas
       filepath: /Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: blas
   internal_api: openblas
    num_threads: 10
         prefix: libopenblas
       filepath: /Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/lib/python3.10/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: armv8
```
",Syntax Error,Syntax Error
"[Accessibility, Mouse]: When using the mouse to navigate a menu, the content behind the menu is reported to the screen reader. 
Type: <b>Bug</b>

This issue has existed for a long time.

## Steps to Reproduce

1. Run NVDA and turn on mouse tracking.
2. Install the Mouse enhancement Add-on in the Add-on Store to fix mouse tracking in Electron
3. On non-editor pages, use the mouse to navigate the help menu

## Actual Behavior

will report the contents of the back of the menu.

## Expected Behavior

Does not report the contents of the back of the menu.

## screenshot

https://github.com/user-attachments/assets/7f23b8a1-c77a-466c-bb76-c7aae7c938a9

VS Code version: Code - Insiders 1.97.0-insider (89f808979a5151bd91324e65d4f7ab1b62896983, 2024-12-20T05:04:19.167Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|12th Gen Intel(R) Core(TM) i7-12700 (12 x 2112)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|63.71GB (35.80GB free)|
|Process Argv|--crash-reporter-id d541528f-f74e-45c1-9b59-00b32f6da504 --crash-reporter-id d541528f-f74e-45c1-9b59-00b32f6da504|
|Screen Reader|yes|
|VM|0%|
</details>
<!-- generated by issue reporter -->",UI/UX Bug,UI/UX Bug
"average_precision_score produces unexpected output when scoring a single sample ### Describe the bug

When using `average_precision_score` and scoring a single sample, the metric ignores `y_score` and will always produce a score of 1.0 if `y_true = [1]` and otherwise will return a score of 0. I would have expected that it would instead raise an exception.

Potentially related to #30147, however I'm focusing on the minimal example with just a single sample.

### Steps/Code to Reproduce

```python
from sklearn.metrics import average_precision_score

y_score = [0]
y_true = [1]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 1.0

y_score = [1]
y_true = [1]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 1.0

y_score = [0.5]
y_true = [1]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 1.0

y_score = [0]
y_true = [0]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 0.0

y_score = [1]
y_true = [0]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 0.0

y_score = [0.5]
y_true = [0]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 0.0
```

Additionally, you can see that the average_precision_score returns a score opposite of what precision and recall return:

```python
from sklearn.metrics import average_precision_score, precision_score, recall_score

y_score = [0]
y_true = [1]

score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 1.0
score = precision_score(y_true=y_true, y_pred=y_score)
print(score)  # 0.0
score = recall_score(y_true=y_true, y_pred=y_score)
print(score)  # 0.0
```

### Expected Results

I would have expected the metric to raise an exception, similar to what happens when ROC_AUC is called with a single sample:

```python
score = roc_auc_score(y_true=y_true, y_score=y_score)
print(score)

```

```
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.
```

### Actual Results

Refer to code snippets above.

### Versions

```shell
System:
    python: 3.11.10 | packaged by conda-forge | (main, Sep 30 2024, 18:08:57) [GCC 13.3.0]
executable: /opt/conda/envs/ag-311/bin/python
   machine: Linux-5.15.0-1056-aws-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.5.1
          pip: 24.2
   setuptools: 60.2.0
        numpy: 1.26.4
        scipy: 1.12.0
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.9.2
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 128
         prefix: libopenblas
       filepath: /opt/conda/envs/ag-311/lib/libopenblasp-r0.3.28.so
        version: 0.3.28
threading_layer: pthreads
   architecture: SapphireRapids

       user_api: blas
   internal_api: openblas
    num_threads: 64
         prefix: libopenblas
       filepath: /opt/conda/envs/ag-311/lib/python3.11/site-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: Cooperlake

       user_api: openmp
   internal_api: openmp
    num_threads: 192
         prefix: libgomp
       filepath: /opt/conda/envs/ag-311/lib/python3.11/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
```
```
",Syntax Error,Syntax Error
"How can I exit the XLAControlFlowContext when inside a jit_compile tf.function? Exit() function take no effect. ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Python version

3.10

### CUDA/cuDNN version

CUDA12.3/cuDNN9.0

### GPU model and memory

GTX4090 24GiB

### Current behavior?

I have a custom op that is a normal OpKernel unable to be compile by XLA cluster. But unfortunately, when the user use this custom op, they have to wrap it into a tf.funtion. So somehow when the use Keras jit_compile or something else, errors happened.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

@tf.function(jit_compile=True)
def test(a):
    b = a + a
    ctx = tf.__internal__.get_enclosing_xla_context()
    ctx.Exit()
    tf.print(b)
    ctx.Enter()
    return b * b

test(tf.constant(1))
```


### Relevant log output

```shell
2024-03-14 02:28:52.720666: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:296 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_test_9[_XlaMustCompile=true,config_proto=9241198235816212909,executor_type=11160318154034397263] on XLA_GPU_JIT: StringFormat (No registered 'StringFormat' OpKernel for XLA_GPU_JIT devices compatible with node {{node StringFormat}}){{node StringFormat}}
The op is created at: 
File ""xla_context_test.py"", line 12, in <module>
  test(tf.constant(1))
File ""xla_context_test.py"", line 8, in test
  tf.print(b)
Traceback (most recent call last):
  File ""xla_context_test.py"", line 12, in <module>
    test(tf.constant(1))
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_test_9[_XlaMustCompile=true,config_proto=9241198235816212909,executor_type=11160318154034397263] on XLA_GPU_JIT: StringFormat (No registered 'StringFormat' OpKernel for XLA_GPU_JIT devices compatible with node {{node StringFormat}}){{node StringFormat}}
The op is created at: 
File ""xla_context_test.py"", line 12, in <module>
  test(tf.constant(1))
File ""xla_context_test.py"", line 8, in test
  tf.print(b) [Op:__inference_test_9]
```
",Dependency Issue,Dependency Issue
"An `aborted issue` could be raised in TensorFlow when I used API `math_ops.cast` and `array_ops.split` ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when I used API `math_ops.cast` and `array_ops.split` . 

### Standalone code to reproduce the issue

```shell
import numpy as np

from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import math_ops
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
sess = tf.compat.v1.Session()
with sess.as_default():
    a = math_ops.cast([2], dtypes.int32)
    b = math_ops.cast([1], dtypes.int32)
    value = np.random.rand(11, 11)
    array_ops.split(value, [a, b])
```


### Relevant log output

```shell
2024-09-12 15:49:03.711972: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions
Aborted (core dumped)
```
",Runtime Error,Runtime Error
"RAM memory leak with tf.function when training multiple models in a loop ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

When I train multiple models in a loop, if I decorate the `train()` function with `@tf.function`, then the memory usage keeps on increasing after each loop iteration, even when I delete the model at the end of each loop and clear the TensorFlow graph/session.

The memory leak does not occur when `@tf.function` is removed. However, model training performance is significantly slower.

Colab notebook to reproduce issue:
https://colab.research.google.com/drive/1sJsGmcFeZVx6ImNbqnBsgF_LzIyPxXPW?usp=sharing

### Standalone code to reproduce the issue

```python
import gc
import os

import psutil
import tensorflow as tf


class MyModel:
    def __init__(self):
        self.dnn = tf.keras.Sequential([
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(1),
        ])
        self.optimizer = tf.optimizers.Adam()
    
    @tf.function   # if we remove this @tf.function decorator, then there is no memory leak
    def train(self, X):
        with tf.GradientTape() as tape:
            loss = tf.reduce_sum(self.dnn(X))
        grads = tape.gradient(loss, self.dnn.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.dnn.trainable_variables))

process = psutil.Process(os.getpid())
rss = int(process.memory_info().rss / 1024 / 1024)  # in MB
print(f'rss: {rss} MB')

X = tf.ones((50, 80))
for i in range(50):
    model = MyModel()
    for _ in range(20):
        model.train(X)

    del model
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()
    gc.collect()

    new_rss = int(process.memory_info().rss / 1024 / 1024)
    if new_rss > rss:
        rss_increase = new_rss - rss
        rss = new_rss
        print(f'Iter {i:05d}, rss increase: {rss_increase} MB, rss: {rss} MB')
```


### Relevant log output

```shell
rss: 593 MB
Iter 00000, rss increase: 31 MB, rss: 624 MB
Iter 00001, rss increase: 6 MB, rss: 630 MB
Iter 00002, rss increase: 5 MB, rss: 635 MB
Iter 00003, rss increase: 5 MB, rss: 640 MB
Iter 00004, rss increase: 5 MB, rss: 645 MB
Iter 00005, rss increase: 5 MB, rss: 650 MB
Iter 00006, rss increase: 5 MB, rss: 655 MB
Iter 00007, rss increase: 5 MB, rss: 660 MB
Iter 00008, rss increase: 5 MB, rss: 665 MB
Iter 00009, rss increase: 5 MB, rss: 670 MB
Iter 00010, rss increase: 4 MB, rss: 674 MB
---  <omitting some rows for brevity>  ---
Iter 00040, rss increase: 5 MB, rss: 822 MB
Iter 00041, rss increase: 5 MB, rss: 827 MB
Iter 00042, rss increase: 5 MB, rss: 832 MB
Iter 00043, rss increase: 5 MB, rss: 837 MB
Iter 00044, rss increase: 5 MB, rss: 842 MB
Iter 00045, rss increase: 5 MB, rss: 847 MB
Iter 00046, rss increase: 5 MB, rss: 852 MB
Iter 00047, rss increase: 5 MB, rss: 857 MB
Iter 00048, rss increase: 5 MB, rss: 862 MB
Iter 00049, rss increase: 5 MB, rss: 867 MB
```
",Performance Issue,Performance Issue
"Task API for `ShellExecutionOptions.env` type does not support `undefined` values 
Type: <b>Bug</b>

The `TerminalOptions.env` API supports environment variable values that include both `string | undefined`.

The ask is for this to match the TerminalOptions API.

Usage:
We plan on using this in the Python extension where users can contribute environment variables via `.env` files. We want to make sure the experience is same for the env variable merge in both cases.

VS Code version: Code - Insiders 1.96.0-insider (69acde7458f428f0e6869de8915c9dd995cdda1a, 2024-11-21T05:04:38.064Z)
OS version: Windows_NT x64 10.0.26100
Modes:

<details>
<summary>System Info</summary>

|Item|Value|
|---|---|
|CPUs|Intel(R) Core(TM) i7-1065G7 CPU @ 1.30GHz (8 x 1498)|
|GPU Status|2d_canvas: enabled<br>canvas_oop_rasterization: enabled_on<br>direct_rendering_display_compositor: disabled_off_ok<br>gpu_compositing: enabled<br>multiple_raster_threads: enabled_on<br>opengl: enabled_on<br>rasterization: enabled<br>raw_draw: disabled_off_ok<br>skia_graphite: disabled_off<br>video_decode: enabled<br>video_encode: enabled<br>vulkan: disabled_off<br>webgl: enabled<br>webgl2: enabled<br>webgpu: enabled<br>webnn: disabled_off|
|Load (avg)|undefined|
|Memory (System)|31.60GB (12.74GB free)|
|Process Argv|--log trace --log ms-python.python=info --crash-reporter-id 4fb1ebc1-cf4c-4880-a88a-47738ec3768d|
|Screen Reader|no|
|VM|0%|
</details><details><summary>Extensions (18)</summary>

Extension|Author (truncated)|Version
---|---|---
tsl-problem-matcher|amo|0.6.2
ruff|cha|2024.54.0
esbuild-problem-matchers|con|0.0.3
vscode-eslint|dba|3.0.10
gitlens|eam|16.0.2
EditorConfig|Edi|0.16.4
prettier-vscode|esb|11.0.0
copilot|Git|1.245.1221
copilot-chat|Git|0.23.2024112103
vscode-github-actions|git|0.27.0
vscode-pull-request-github|Git|0.101.2024112104
debugpy|ms-|2024.13.2024111901
python|ms-|2024.21.0-dev
vscode-pylance|ms-|2024.11.101
remote-containers|ms-|0.388.0
remote-wsl|ms-|0.88.5
extension-test-runner|ms-|0.0.12
code-spell-checker|str|4.0.21


</details><details>
<summary>A/B Experiments</summary>

```
vsliv368cf:30146710
vspor879:30202332
vspor708:30202333
vspor363:30204092
vsc_aacf:30263846
pythonvspyt551:31179976
vscod805cf:30301675
vsaa593cf:30376535
py29gd2263:31024238
c4g48928:30535728
vscrpc:30624061
a9j8j154:30646983
962ge761:30841072
pythonnoceb:30776497
asynctok:30898717
dsvsc014:30777825
dsvsc015:30821418
pythonmypyd1:30859725
h48ei257:31000450
pythontbext0:30879054
cppperfnew:30980852
pythonait:30973460
0ee40948:31013168
dvdeprecation:31040973
dwnewjupytercf:31046870
newcmakeconfigv2:31071590
nativerepl1:31134653
pythonrstrctxt:31093868
nativeloc1:31118317
cf971741:31144450
notreesitter:31116712
e80f6927:31120813
i21gd607:31141543
iacca1:31150324
notype1:31143044
dwcopilot:31158714
h409b430:31177054
5b1c1929:31184661

```

</details>

<!-- generated by issue reporter -->",Dependency Issue,Dependency Issue
"Copy-paste screws up indentation for no reason AND alters contents of pasted string <!-- ???? Do Not Delete This! bug_report_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- ?? Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- ?? Search existing issues to avoid creating duplicates. -->
<!-- ?? Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- ?? Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- ?? Launch with `code --disable-extensions` to check. -->
Does this issue occur when all extensions are disabled?: Yes

<!-- ?? If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- ?? Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.95.3
- OS Version: Manjaro Linux

Steps to Reproduce:
=

1. I start with this code:

![Image](https://github.com/user-attachments/assets/0cf00f12-cb76-4fad-bc72-d309ab5ab5ad)

2. Then I start adding this:

![Image](https://github.com/user-attachments/assets/9aa28367-c837-4df8-8b7e-75f6cb56fc2b)

3. I copy this portion of code from above:

![Image](https://github.com/user-attachments/assets/20659f98-d77c-4261-bed3-a57b5ae39296)

4. And  I paste it here:

![Image](https://github.com/user-attachments/assets/6b2ae68c-812f-4741-a12e-b22972009b7e)

Expected behavior:
=
I should get this:

![Image](https://github.com/user-attachments/assets/d4af9857-fdf5-4d5c-b8da-6f5952b585c9)



Observed behavior:
=
I get this instead:
![Image](https://github.com/user-attachments/assets/4808fdb4-c847-4e24-903f-7b265f473d74)

This is completely bonkers! Not only is this **not sensible indentation**, but crucially, the pasted code **alters the contents of the string**!!!!!!

Sometimes, when pasting a multi-line string into a region of code with different indentation than the one the original string was copied from, one doesn't get the desired indentation and that's expected because the contents of the string are sacred. The IDE doesn't know whether the whitespaces inside the string matter, so, at least by default, it preserves the contents of the string at the expense of indentation. And that's expected.

But here, the resulting indentation makes no sense AND the contents of the pasted string are altered with  respect to the copied one!! This makes no sense whatsoever! The indentation of the code where the new code is pasted is the same as where the code is copied from, so there's no need to alter the indentation in the first place, but the IDE is removing spaces for no reason whatsoever AND it's changing the contents of  a pasted string, even unnecessarily. ",UI/UX Bug,UI/UX Bug
"Zoomed types truncates too quickly The content gets truncated too quickly imo, considering this is an explicit user action to get more information about the type:

![Image](https://github.com/user-attachments/assets/f6d8fcd1-86bd-4c62-b34f-be89c74437c6)
",UI/UX Bug,UI/UX Bug
"`tf.raw_ops.ArgMax`: Heap buffer overflow ### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.ArgMax` can lead to heap buffer overflow.
[Error location](https://github.com/tensorflow/tensorflow/blob/774b0c3e97b5ef60bfc9c54961347dd0bc3660a8/tensorflow/core/kernels/argmax_op.cc#L59):
```C++
    const int32_t dim = internal::SubtleMustCopy(dimension.scalar<int32>()());
```
It copies scalar value of `dimension` without checking exact type. Therefore when `dimension` is an `int16` it reads over the bound.
Note that `int16` is an allowed type for `dimension` according to [opdef](https://github.com/tensorflow/tensorflow/blob/41b93a8b310086f69aab6b6369d2af9d5178881d/tensorflow/core/ops/math_ops.cc#L1153-L1160):
```C++
REGISTER_OP(""ArgMax"")
    .Input(""input: T"")
    .Input(""dimension: Tidx"")
    .Output(""output: output_type"")
    .Attr(""T: {realnumbertype, quantizedtype, bool}"")
    .Attr(""Tidx: {int16, int32, int64} = DT_INT32"")
    .Attr(""output_type: {int16, uint16, int32, int64} = DT_INT64"")
    .SetShapeFn(ArgOpShape);
```

### Standalone code to reproduce the issue

```python
import tensorflow as tf

tf.raw_ops.ArgMax(
    input=tf.random.normal([1,1,1,1]),
    dimension=tf.constant(1,shape=[],dtype=tf.int16),
    output_type=tf.dtypes.int64,
    name=None
)
```


### Relevant log output

The below log needs ASAN build.
```shell
=================================================================
==4008222==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x609000000400 at pc 0x7fa6a0dca809 bp 0x7ffe63b29d90 sp 0x7ffe63b29d88
READ of size 4 at 0x609000000400 thread T0
    #0 0x7fa6a0dca808 in int const tensorflow::internal::SubtleMustCopy<int>(int const&) /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10
    #1 0x7fa6a0dca808 in tensorflow::ArgOp<Eigen::ThreadPoolDevice, float, long, tensorflow::functor::ArgMax<Eigen::ThreadPoolDevice, float, long>>::Compute(tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/kernels/argmax_op.cc:59:25
    #2 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #3 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #4 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #5 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #6 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #7 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #8 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #9 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #10 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #11 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #12 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #13 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #14 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #15 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #16 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #17 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #18 0x7fa6b989418d in TFE_Py_FastPathExecute_C(_object*) /proc/self/cwd/tensorflow/python/eager/pywrap_tfe_src.cc:3979:3
    #19 0x7fa667d2683e in pybind11_init__pywrap_tfe(pybind11::module_&)::$_60::operator()(pybind11::args) const /proc/self/cwd/tensorflow/python/tfe_wrapper.cc:1276:35
    #20 0x7fa667d2683e in pybind11::object pybind11::detail::argument_loader<pybind11::args>::call_impl<pybind11::object, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, 0ul, pybind11::detail::void_type>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, std::integer_sequence<unsigned long, 0ul>, pybind11::detail::void_type&&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1443:16
    #21 0x7fa667d2683e in std::enable_if<!std::is_void<pybind11::object>::value, pybind11::object>::type pybind11::detail::argument_loader<pybind11::args>::call<pybind11::object, pybind11::detail::void_type, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1411:42
    #22 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:248:69
    #23 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:223:21
    #24 0x7fa667d67a59 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:939:30
    #25 0x528186 in cfunction_call /usr/local/src/conda/python-3.11.7/Objects/methodobject.c:542:18
    #26 0x503a0b in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18
    #27 0x510f32 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:4769:23
    #28 0x538732 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #29 0x538732 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #30 0x538732 in _PyFunction_Vectorcall /usr/local/src/conda/python-3.11.7/Objects/call.c:393:16
    #31 0x5426bb in _PyVectorcall_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:257:24
    #32 0x5426bb in _PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:328:16
    #33 0x5426bb in PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:355:12
    #34 0x514ff0 in do_call_core /usr/local/src/conda/python-3.11.7/Python/ceval.c:7352:12
    #35 0x514ff0 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:5376:22
    #36 0x5cb559 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #37 0x5cb559 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #38 0x5cac2e in PyEval_EvalCode /usr/local/src/conda/python-3.11.7/Python/ceval.c:1148:21
    #39 0x5ebcf6 in run_eval_code_obj /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1710:9
    #40 0x5e788f in run_mod /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1731:19
    #41 0x5fc831 in pyrun_file /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1626:15
    #42 0x5fbbfe in _PyRun_SimpleFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:440:13
    #43 0x5fb922 in _PyRun_AnyFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:79:15
    #44 0x5f65cd in pymain_run_file_obj /usr/local/src/conda/python-3.11.7/Modules/main.c:360:15
    #45 0x5f65cd in pymain_run_file /usr/local/src/conda/python-3.11.7/Modules/main.c:379:15
    #46 0x5f65cd in pymain_run_python /usr/local/src/conda/python-3.11.7/Modules/main.c:601:21
    #47 0x5f65cd in Py_RunMain /usr/local/src/conda/python-3.11.7/Modules/main.c:680:5
    #48 0x5bb3d8 in Py_BytesMain /usr/local/src/conda/python-3.11.7/Modules/main.c:734:12
    #49 0x7fa791408d8f in __libc_start_call_main csu/../sysdeps/nptl/libc_start_call_main.h:58:16
    #50 0x7fa791408e3f in __libc_start_main csu/../csu/libc-start.c:392:3
    #51 0x5bb222 in _start (/home/loft/anaconda3/envs/tf-latest-asan/bin/python3.11+0x5bb222)

0x609000000402 is located 0 bytes after 2-byte region [0x609000000400,0x609000000402)
allocated by thread T0 here:
    #0 0x7fa7917ed617 in __interceptor_posix_memalign /home/runner/work/llvm-project/llvm-project/final/llvm-project/compiler-rt/lib/asan/asan_malloc_linux.cpp:145:3
    #1 0x7fa6be4f6902 in tsl::port::AlignedMalloc(unsigned long, int) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4a18902)
    #2 0x7fa6bc2d3bcc in tsl::(anonymous namespace)::CPUAllocator::AllocateRaw(unsigned long, unsigned long) cpu_allocator_impl.cc
    #3 0x7fa6bc4883bc in short* tensorflow::TypedAllocator::Allocate<short>(tsl::Allocator*, unsigned long, tsl::AllocationAttributes const&) /proc/self/cwd/./tensorflow/core/framework/typed_allocator.h:47:24
    #4 0x7fa6bc4883bc in tensorflow::(anonymous namespace)::Buffer<short>::Buffer(tsl::Allocator*, long, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:574:21
    #5 0x7fa6bc4883bc in tensorflow::Tensor::Tensor(tsl::Allocator*, tensorflow::DataType, tensorflow::TensorShape const&, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:986:5
    #6 0x7fa6bbef871d in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:764:10
    #7 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes) /proc/self/cwd/./tensorflow/core/framework/op_kernel.h:1270:12
    #8 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**, tsl::AllocatorAttributes) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:822:14
    #9 0x7fa6bbef4368 in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:728:10
    #10 0x7fa6a9bf0207 in tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e9fe207)
    #11 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #12 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #13 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #14 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #15 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #16 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #17 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #18 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #19 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #20 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #21 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #22 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #23 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #24 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #25 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #26 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #27 0x7fa6b985dcbf in tensorflow::EagerCast(TFE_Context*, TFE_TensorHandle*, TF_DataType, TF_DataType, TSL_Status*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:259:3
    #28 0x7fa6b985e67b in tensorflow::ConvertToEagerTensorUncached(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:317:11
    #29 0x7fa6b985f8a1 in tensorflow::ConvertToEagerTensor(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:405:14
    #30 0x7fa6b9860088 in EagerTensor_init /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:529:18
    #31 0x5039d2 in type_call /usr/local/src/conda/python-3.11.7/Objects/typeobject.c:1103:19
    #32 0x5039d2 in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18

SUMMARY: AddressSanitizer: heap-buffer-overflow /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10 in int const tensorflow::internal::SubtleMustCopy<int>(int const&)
Shadow bytes around the buggy address:
  0x609000000180: fa fa fa fa fa fa fa fa fd fa fa fa fa fa fa fa
  0x609000000200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000280: fd fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000300: 04 fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000380: fd fd fd fd fa fa fa fa fa fa fa fa fa fa fa fa
=>0x609000000400:[02]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000480: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000500: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000580: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000600: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000680: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
==4008222==ABORTING
```
",Security Vulnerability,Security Vulnerability
"config `http.proxy` in workspace settings <!-- ???? Do Not Delete This! feature_request_template ???? -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- Please search existing issues to avoid creating duplicates. -->

<!-- Describe the feature you'd like. -->

The `http.proxy` setting can only be configured in user settings. Im wondering if its possible to set it in workspace settings instead.

When using a dev container, the `http.proxy` setting is passed to the container. However, the container cannot use the proxy on the local machine unless additional configuration is done. 

If `http.proxy` could be configured directly in `.vscode/settings.json` or `.devcontainer/devcontainer.json`, it would make things much easier for users. Currently, we have to either modify the user settings each time or set up a more complex container network configuration.
",Dependency Issue,Dependency Issue
