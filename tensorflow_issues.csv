repository,title,description,labels,created_at,comments,reactions,url
tensorflow/tensorflow,`gradient_checker.compute_gradient` can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `Floating point exception` issue in TensorFlow when I used API `gradient_checker.compute_gradient`. I have confirmed that below code would crash on tf-nightly 2.19.0-dev20241025 (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1Ow6DQI7g-s7LpKUM1fy8OXbNcfw1ZH7r?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import math

from absl.testing import parameterized
import numpy as np
import tensorflow as tf
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import errors_impl
from tensorflow.python.framework import tensor_shape
from tensorflow.python.framework import test_util
from tensorflow.python.ops import gen_nn_ops
from tensorflow.python.ops import gradient_checker
from tensorflow.python.ops import gradients_impl
from tensorflow.python.ops import nn_ops
import tensorflow.python.ops.nn_grad  # pylint: disable=unused-import
from tensorflow.python.platform import test
from tensorflow.python.platform import tf_logging
from tensorflow.python.util.compat import collections_abc
from tensorflow.python.eager import context
def DtypesToTest(use_gpu):
  # double datatype is currently not supported for convolution ops
  # on the ROCm platform
  optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]
  if use_gpu:
    if not test_util.GpuSupportsHalfMatMulAndConv():
      return optional_float64 + [dtypes.float32]
    else:
      # It is important that float32 comes before float16 here,
      # as we will be using its gradients as reference for fp16 gradients.
      return optional_float64 + [dtypes.float32, dtypes.float16]
  else:
    return optional_float64 + [dtypes.float32, dtypes.float16, dtypes.bfloat16]
def _ConstructAndTestGradientForConfig(
    batch, input_shape, filter_shape, in_depth, out_depth, stride,
    padding, test_input, data_format, use_gpu):
  input_planes, input_rows, input_cols = input_shape
  filter_planes, filter_rows, filter_cols = filter_shape
  input_shape = [batch, input_planes, input_rows, input_cols, in_depth]
  filter_shape = [
      filter_planes, filter_rows, filter_cols, in_depth, out_depth
  ]
  if isinstance(stride, collections_abc.Iterable):
    strides = [1] + list(stride) + [1]
  else:
    strides = [1, stride, stride, stride, 1]
  if padding == ""VALID"":
    output_planes = int(
        math.ceil((input_planes - filter_planes + 1.0) / strides[1]))
    output_rows = int(
        math.ceil((input_rows - filter_rows + 1.0) / strides[2]))
    output_cols = int(
        math.ceil((input_cols - filter_cols + 1.0) / strides[3]))
  else:
    output_planes = int(math.ceil(float(input_planes) / strides[1]))
    output_rows = int(math.ceil(float(input_rows) / strides[2]))
    output_cols = int(math.ceil(float(input_cols) / strides[3]))
  output_shape = [batch, output_planes, output_rows, output_cols, out_depth]
  input_size = 1
  for x in input_shape:
    input_size *= x
  filter_size = 1
  for x in filter_shape:
    filter_size *= x
  input_data = [x * 1.0 / input_size for x in range(0, input_size)]
  filter_data = [x * 1.0 / filter_size for x in range(0, filter_size)]
  for data_type in DtypesToTest(use_gpu=use_gpu):
    # TODO(mjanusz): Modify gradient_checker to also provide max relative
    # error and synchronize the tolerance levels between the tests for forward
    # and backward computations.
    if data_type == dtypes.float64:
      tolerance = 1e-8
    elif data_type == dtypes.float32:
      tolerance = 5e-3
    elif data_type == dtypes.float16:
      tolerance = 5e-3 if test.is_built_with_rocm() else 1e-3
    elif data_type == dtypes.bfloat16:
      tolerance = 1e-2
    sess = tf.compat.v1.Session()
    with sess.as_default():
      orig_input_tensor = constant_op.constant(
          input_data, shape=input_shape, dtype=data_type, name=""input"")
      filter_tensor = constant_op.constant(
          filter_data, shape=filter_shape, dtype=data_type, name=""filter"")
      if data_format == ""NCDHW"":
        input_tensor = test_util.NHWCToNCHW(orig_input_tensor)
        new_strides = test_util.NHWCToNCHW(strides)
      else:
        input_tensor = orig_input_tensor
        new_strides = strides
      conv = nn_ops.conv3d(
          input_tensor,
          filter_tensor,
          new_strides,
          padding,
          data_format=data_format,
          name=""conv"")
      jacob_t, jacob_n = gradient_checker.compute_gradient(
          orig_input_tensor, input_shape, conv, output_shape)

with context.graph_mode():
  _ConstructAndTestGradientForConfig(data_format=""NDHWC"",use_gpu=False,batch=2, input_shape=(3, 7, 6), filter_shape=(3, 3, 3), in_depth=2, out_depth=0, stride=3, padding='VALID', test_input=True)
```

### Relevant log output

```shell
Fatal Python error: Floating point exception
```","['type:bug', 'type:support']",2025-02-11T16:38:56Z,0,0,https://github.com/tensorflow/tensorflow/issues/87063
tensorflow/tensorflow,TPU nan issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux (google colab default)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the 2.18.0 Tensorflow version, occur only when using TPU:
- At the middle of any epoch, loss turns out to be nan for the rest of the epoch 

### Standalone code to reproduce the issue

```shell
Shortest way: connect and connect to the colab tutorial on TPU, i.e. https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/tpu.ipynb#scrollTo=Tce3stUlHN0L
If the issue hasn't been fixed, the training loop will produce nan loss
```

### Relevant log output

```shell
Epoch 1/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 22s 47ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 2/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 3/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 4/5
231/300 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: nan - sparse_categorical_accuracy: nan
```","['type:bug', 'comp:tpus', 'TF 2.18']",2025-02-10T15:03:57Z,2,0,https://github.com/tensorflow/tensorflow/issues/86953
tensorflow/tensorflow,"`tf.summary_ops.write` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.write` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/13pz-hZNK_BaOnbYo_9CT2YMdoncPXEA3?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import summary_ops_v2 as summary_ops
from tensorflow.python.ops import variables
writer = summary_ops.create_file_writer_v2(""/tmp"")
mystep = variables.Variable(1, dtype=dtypes.int64)
with writer.as_default(step=[3, 0, 0, 2]):
    summary_ops.write('tag', 1.0)
```

### Relevant log output

```shell
2025-02-09 04:47:35.482562: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor
Aborted (core dumped)
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-09T04:49:39Z,1,0,https://github.com/tensorflow/tensorflow/issues/86918
tensorflow/tensorflow,"`tf.summary_ops.run_metadata_graphs` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.run_metadata_graphs` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1qCZb2sPj2l79IecGA1tz9CywtNlevAjf?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.ops import summary_ops_v2 as summary_ops
from tensorflow.core.protobuf import config_pb2
writer = summary_ops.create_file_writer_v2(""/tmp"")
meta = config_pb2.RunMetadata()
with writer.as_default([3, 0, 0, 2]):
    summary_ops.run_metadata_graphs(name='my_name', data=meta)
```

### Relevant log output

```shell
2025-02-09 04:39:36.666278: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor
Aborted (core dumped)
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-09T04:42:49Z,1,0,https://github.com/tensorflow/tensorflow/issues/86917
tensorflow/tensorflow,"`io_ops.restore_v2` aborts with ""Check failed: size >= 0 (0 vs. -3) ""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.io_ops.restore_v2` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20250207` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/17XwNF4WI3HqVJkLwk2VQjcTDAV42-6Bl?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.ops import io_ops

dtype = dtypes.uint4
with ops.Graph().as_default():
    op = io_ops.restore_v2('model', ['var1', 'var2'], ['', '-3 4 0,1:-'], [dtype, dtype])
```

### Relevant log output

```shell
2025-02-09 04:25:19.857968: F tensorflow/core/framework/tensor_shape.cc:413] Check failed: size >= 0 (0 vs. -3) 
Aborted (core dumped)
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-09T04:34:07Z,1,0,https://github.com/tensorflow/tensorflow/issues/86916
tensorflow/tensorflow,Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Macos 15.3 (worker 0) Macos 12.7.6(worker 1)

### Mobile device

_No response_

### Python version

3.8.20

### Bazel version

...

### GCC/compiler version

16.0.0 (apple M3) 14.0.0 (intel iris)

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M3 and Intel Iris Graphics 6100

### Current behavior?

When I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  #Build the model under the strategy

No error message is displayed, but the process no longer progresses after

•	 I followed the recommendations of the official documentation, but the problem persists.

### Standalone code to reproduce the issue

```shell
import json
import os
import numpy as np
import tensorflow as tf

TF_CONFIG = {
    ""cluster"": {
        ""worker"": [""192.168.0.68:12345"", ""192.168.0.68:12346""]
    },
    ""task"": {""type"": ""worker"", ""index"": 0}  # Modifier index pour chaque worker
}

os.environ[""TF_CONFIG""] = json.dumps(TF_CONFIG)

# Manually Load the MNIST dataset
data = np.load(""mnist.npz"")
x_train, y_train = data[""x_train""], data[""y_train""]
x_test, y_test = data[""x_test""], data[""y_test""]

# Normalize images
x_train, x_test = x_train / 255.0, x_test / 255.0

# Add a dimension to match TensorFlow's expectations
x_train = x_train[..., np.newaxis]
x_test = x_test[..., np.newaxis]

# Define the distribution strategy
strategy = tf.distribute.MultiWorkerMirroredStrategy()

# Build the model under the strategy
with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))

test_loss, test_acc = model.evaluate(x_test, y_test)
print(f""Test accuracy: {test_acc:.4f}"")
```

### Relevant log output

```shell
2025-02-08 12:29:20.630247: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3
2025-02-08 12:29:20.630286: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB
2025-02-08 12:29:20.630293: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB
2025-02-08 12:29:20.630324: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-02-08 12:29:20.630338: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-02-08 12:29:20.631249: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-02-08 12:29:20.631259: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-02-08 12:29:20.632347: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:449] Started server with target: grpc://192.168.0.68:12345
2025-02-08 12:29:20.637550: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 14287335759644278642
2025-02-08 12:29:20.637654: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:298] Coordination agent has successfully connected.
2025-02-08 12:29:37.728182: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 15227140312468372989
```","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.13']",2025-02-08T11:40:33Z,1,0,https://github.com/tensorflow/tensorflow/issues/86897
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.Rsqrt``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ``tf.raw_ops.Rsqrt``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x_0 = tf.constant([[[1.2031, 0.2168], [0.1177, 0.6758]], 
                    [[1.8203, 0.9688], [0.8828, 0.0767]]], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.Rsqrt(x=x_0)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.Rsqrt(x=x_0)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor(
[[[0.910156 2.14062]
  [2.92188 1.21875]]

 [[0.742188 1.01562]
  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)

Output on GPU: tf.Tensor(
[[[0.914062 2.15625]
  [2.90625 1.21875]]

 [[0.738281 1.01562]
  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-05T06:12:29Z,1,0,https://github.com/tensorflow/tensorflow/issues/86607
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.Tan``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

```tf.raw_ops.Tan``` produces inconsistent results between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

real = tf.constant([1.5634333], dtype=tf.float32)
imag = tf.constant([0.020735], dtype=tf.float32)

complex_tensor = tf.complex(real, imag)

with tf.device('/CPU:0'):
    result_cpu = tf.raw_ops.Tan(x=complex_tensor)
    print(result_cpu)

with tf.device('/GPU:0'):
    result_gpu = tf.raw_ops.Tan(x=complex_tensor)
    print(result_gpu)

##Comparing whole complex numbers
max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_cons = tf.experimental.numpy.allclose(result_cpu, result_gpu, rtol=1e-6,  atol=1e-5)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:"", is_cons.numpy())

##Comparing by parts
real_part_cpu = tf.math.real(result_cpu)
real_part_gpu = tf.math.real(result_gpu)
real_part_diff = tf.reduce_max(tf.abs(real_part_cpu - real_part_gpu)).numpy()
real_part_cons = tf.experimental.numpy.allclose(real_part_cpu, real_part_gpu, rtol=1e-6,  atol=1e-5)

imag_part_cpu = tf.math.imag(result_cpu)
imag_part_gpu = tf.math.imag(result_gpu)
imag_part_diff = tf.reduce_max(tf.abs(imag_part_cpu - imag_part_gpu)).numpy()
imag_part_cons = tf.experimental.numpy.allclose(imag_part_cpu, imag_part_gpu, rtol=1e-6,  atol=1e-5)

print(""Real parts absolute difference:"", real_part_diff)
print(""Real parts Consistency check with atol=1e-5 and rtol=1e-6:"", real_part_cons.numpy())

print(""Imag parts absolute difference:"", imag_part_diff)
print(""Imag parts Consistency check with atol=1e-5 and rtol=1e-6:"", imag_part_cons.numpy())
```

### Relevant log output

```shell
tf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64)
tf.Tensor([15.205605+42.834225j], shape=(1,), dtype=complex64)

Max absolute difference: 8.5064334e-05
Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False

Real parts absolute difference: 2.861023e-05
Real parts Consistency check with atol=1e-5 and rtol=1e-6: False

Imag parts absolute difference: 8.010864e-05
Imag parts Consistency check with atol=1e-5 and rtol=1e-6: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-04T03:18:15Z,2,0,https://github.com/tensorflow/tensorflow/issues/86506
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.LogSoftmax``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ``tf.raw_ops.LogSoftmax``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

logits = tf.constant([[0.0664, -2.3906]], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.LogSoftmax(logits=logits)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.LogSoftmax(logits=logits)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor([[-0.0825195 -2.53125]], shape=(1, 2), dtype=bfloat16)

Output on GPU: tf.Tensor([[-0.0825195 -2.54688]], shape=(1, 2), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-03T03:30:58Z,1,0,https://github.com/tensorflow/tensorflow/issues/86434
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.L2Loss``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ```tw.raw_ops.L2Loss``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

t = tf.constant([
    [[0.9922, -1.4922], 
     [0.0376,  0.1504], 
     [0.6172,  1.2266]],

    [[-0.1387,  1.3047], 
     [0.3535, -0.0471], 
     [0.0437,  0.2637]]
], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.L2Loss(t=t)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.L2Loss(t=t)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor(3.51562, shape=(), dtype=bfloat16)

Output on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-02T07:05:17Z,2,0,https://github.com/tensorflow/tensorflow/issues/86406
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.BiasAddGrad``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

```tf.raw_ops.BiasAddGrad``` produces inconsistent results between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

out_backprop = tf.constant([
    [
        [
            [[ 0.2207,  2.1094], [-0.3730, -1.0625], [ 1.7031,  0.7148]], 
            [[ 1.5078, -0.6719], [-0.6367,  0.5039], [-2.3281,  0.5078]]
        ],
        [
            [[-0.3574,  0.0461], [ 2.3750, -2.9688], [-0.5703, -2.0156]],
            [[ 0.8125,  1.7656], [-0.9570,  0.6250], [-0.6914, -0.4746]]
        ],
        [
            [[-0.3750, -0.7383], [ 0.3691,  0.4570], [ 1.1641,  0.2715]],
            [[-1.2969, -0.9844], [-0.4863,  1.0938], [-1.4297,  0.8086]]
        ]
    ],
    [
        [
            [[ 0.3730,  0.8477], [-0.3887,  1.2266], [ 0.0859, -0.5742]],
            [[-0.7383, -0.2432], [-0.7578, -0.8281], [-0.1660, -0.9336]]
        ],
        [
            [[ 1.4297,  0.6797], [-1.6172,  0.4941], [-0.3047, -0.3711]],
            [[-0.6250, -0.7617], [ 0.9453,  0.1064], [ 1.4062, -2.9531]]
        ],
        [
            [[-1.4297, -0.1387], [ 0.0625,  1.0469], [-0.1953,  1.6406]],
            [[-0.3047,  0.5117], [ 1.8125,  1.1797], [-0.8789, -0.4688]]
        ]
    ]
], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")
  print(""BiasAddGrad Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")
  print(""BiasAddGrad Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
BiasAddGrad Output on CPU: tf.Tensor([0.09375 -3.96875 1.70312], shape=(3,), dtype=bfloat16)

BiasAddGrad Output on GPU: tf.Tensor([0.078125 -4 1.70312], shape=(3,), dtype=bfloat16)

Max absolute difference: 0.03125
Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2025-02-01T10:43:06Z,5,0,https://github.com/tensorflow/tensorflow/issues/86378
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.BatchMatMulV2``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

getting inconsistent results of ```tf.raw_ops.BatchMatMulV2``` between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x_0 = tf.constant([
    [[[[ -1.3594, -0.3027], [-1.4141,  0.2969]],
      [[ -0.9141,  1.7812], [ 1.2266,  0.8594]]],

     [[[  0.8359, -0.9414], [-1.7969, -0.7461]],
      [[  0.3164,  0.3691], [ 0.7656,  0.2354]]]],

    [[[[ -0.5898,  1.3516], [ 0.4902, -0.1045]],
      [[ -0.1099,  1.5078], [ 0.2852, -0.0957]]],

     [[[-0.9883,  1.3203], [-0.2715, -1.7578]],
      [[ -0.1602, -0.4336], [-0.6875, -0.4492]]]]
], dtype=tf.bfloat16)

y = tf.constant([
    [[[  0.6836, -0.6562], [-0.5508, -0.8438]], 
     [[  1.6094, -0.9883], [-0.1318,  1.1094]]],

    [[[  0.4062, -1.1094], [-0.7188, -1.7578]], 
     [[ -1.0391, -0.6602], [ 0.8359, -0.6562]]]
], dtype=tf.bfloat16) 

with tf.device('CPU:0'):
    result_cpu = tf.raw_ops.BatchMatMulV2(
        x=x_0, 
        y=y,
    )
    print(result_cpu)

with tf.device('GPU:0'):
    result_gpu = tf.raw_ops.BatchMatMulV2(
        x=x_0, 
        y=y,
    )
    print(result_gpu)


max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
tf.Tensor(
[[[[[-0.761719 1.14062]
    [-1.125 0.675781]]

   [[-1.70312 2.875]
    [1.85938 -0.257812]]]


  [[[1.01562 0.726562]
    [-0.193359 3.29688]]

   [[-0.0201416 -0.449219]
    [-0.597656 -0.65625]]]]



 [[[[-1.14062 -0.75]
    [0.392578 -0.233398]]

   [[-0.375 1.78125]
    [0.470703 -0.386719]]]


  [[[-1.34375 -1.21875]
    [1.14844 3.39062]]

   [[-0.195312 0.388672]
    [0.337891 0.746094]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)

tf.Tensor(
[[[[[-0.761719 1.14844]
    [-1.13281 0.675781]]

   [[-1.70312 2.875]
    [1.85938 -0.259766]]]


  [[[1.01562 0.726562]
    [-0.193359 3.3125]]

   [[-0.0201416 -0.451172]
    [-0.597656 -0.660156]]]]



 [[[[-1.14844 -0.753906]
    [0.392578 -0.233398]]

   [[-0.375 1.78125]
    [0.470703 -0.388672]]]


  [[[-1.35156 -1.22656]
    [1.15625 3.39062]]

   [[-0.196289 0.390625]
    [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)


Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-01T07:13:36Z,2,0,https://github.com/tensorflow/tensorflow/issues/86350
tensorflow/tensorflow,Stateful LSTM bug with batch size,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid. 
Basically the error is the same as this https://github.com/tensorflow/tensorflow/issues/64061

Below is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

# Set a fixed batch size
batch_size = 32

# Create some random training data
# We'll have sequences of length 5, with 1 feature per time step
sequence_length = 5
num_features = 1
num_samples = 100  # Total number of samples (must be divisible by batch_size)

# Ensure num_samples is a multiple of batch_size
num_samples = (num_samples // batch_size) * batch_size

X_train = np.random.rand(num_samples, sequence_length, num_features)
y_train = np.random.rand(num_samples, 1)  # Example target values

# Reshape y_train to match expected output shape if needed
y_train = y_train.reshape(-1,1)

# Create the stateful LSTM model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.LSTM(units=64,  # Number of LSTM units
                               batch_input_shape=(batch_size, sequence_length, num_features),
                               stateful=True,
                               return_sequences=False)) #often false for a final prediction

model.add(tf.keras.layers.Dense(units=1)) # Output layer with 1 unit

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
epochs = 10

for epoch in range(epochs):
    # Shuffle data indices for each epoch (important for stateful LSTMs)
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    X_train = X_train[indices]
    y_train = y_train[indices]

    model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False) # Shuffle must be false

    # Reset states after each epoch (essential for stateful LSTMs)
    model.reset_states()
```

### Relevant log output

```shell

```","['stat:awaiting response', 'type:bug', 'comp:keras', 'TF 2.18']",2025-01-31T18:07:07Z,5,0,https://github.com/tensorflow/tensorflow/issues/86310
tensorflow/tensorflow,inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

getting inconsistent results of ```tf.image.adjust_hue``` between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

images = tf.constant([
    [[ 1.9720840,  2.1302242, -0.1902120],
     [ 0.6557856, -1.3016001,  1.1452782]],
    
    [[-2.2193234,  0.3198028,  0.9568117],
     [-0.3937407, -0.0503466, -0.3693791]]
], dtype=tf.float32)

delta = tf.constant(-0.7441734, dtype=tf.float32)

with tf.device('CPU:0'):
    adjusted_cpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on CPU:\n"", adjusted_cpu)

with tf.device('GPU:0'):
    adjusted_gpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on GPU:\n"", adjusted_gpu)


is_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e-5, rtol=1e-6)

max_abs_diff = tf.reduce_max(tf.abs(adjusted_cpu - adjusted_gpu)).numpy()

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:"", is_consistent.numpy())
```

### Relevant log output

```shell
Adjusted Hue on CPU:
 tf.Tensor(
[[[-0.190212    2.1302242   1.2092681 ]
  [ 1.1452782  -0.48211157 -1.3016001 ]]

 [[ 0.11679006 -2.2193234   0.9568117 ]
  [-0.3937407  -0.25841027 -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Adjusted Hue on GPU:
 tf.Tensor(
[[[-0.19021209  2.1302242   1.209268  ]
  [ 1.1452781  -0.48211193 -1.3016001 ]]

 [[ 0.11678863 -2.2193234   0.95681167]
  [-0.0503466  -0.0503466  -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Max absolute difference: 0.3433941
Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-01-31T07:40:34Z,4,1,https://github.com/tensorflow/tensorflow/issues/86256
tensorflow/tensorflow,Buffer allocation error in Tensorflow Lite with OpenCL backend on certain platforms,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

macOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I noticed a memory allocation error in clCreateBuffer. The issue seems to be caused by this:

1) TFlite tries to alloca 0xa2000000 bytes of memory (value stored as size_t)

2) The call ends up in this function (tensorflow/lite/experimental/litert/runtime/opencl/buffer.cc):

```
absl::Status CreateClBuffer(cl_context context, int size_in_bytes,
                            bool read_only, void* data, cl_mem* result) 
```

where the size is now int (i.e. 32 bit signed integer... so 0xa2000000 is interpreted as a negative value).

3) This function then calls clCreateBuffer, which takes the size argument as size_t again, and thus receives 0xffffffffa2000000, i.e. the signed 32 bit integer first sign extended to 64bit and then interpreted as unsigned, and thus resulting in a huge size.

The issue doesn't seem to appear with the same model on Android, probably because: The max buffer allocation size on macOS (M1) seems to be 9GB (according to clinfo), but on that android device it's only 1GB (so on android tflite never tries to allocate such a huge chunk of memory).

### Standalone code to reproduce the issue

```shell
Unfortunately I'm not allowed to share the code/model, but looking at the function signatures one can see the issue.
```

### Relevant log output

```shell
ERROR: Failed to allocate device memory (clCreateBuffer): Invalid buffer size
```","['type:bug', 'comp:lite', 'TF 2.13']",2025-01-29T12:11:28Z,0,0,https://github.com/tensorflow/tensorflow/issues/86048
tensorflow/tensorflow,TensorFlow warning shows whenever importing it,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802

### Custom code

No

### OS platform and distribution

Linux Ubuntu 24.10 x86_64

### Mobile device

_No response_

### Python version

3.12.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA:12.6

### GPU model and memory

_No response_

### Current behavior?

`cuFFT`, `cuDNN` and `cuBLAS` warnings showing after importing `TensorFlow`

- OS: Ubuntu 24.10 x86_64
- Host: G5 5590
- Kernel: 6.11.0-13-generic
- CPU: Intel i7-9750H (12) @ 4.500GHz
- GPU: NVIDIA GeForce GTX 1650 Mobile / Max-Q
- GPU: Intel CoffeeLake-H GT2 [UHD Graphics 630]
> whenever running the following code it gives that warning also it outputs the predicted output but after the warning:
```python
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```
> output:
```
2025-01-23 21:08:06.468437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737659286.484845  763412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737659286.489647  763412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-23 21:08:06.505984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```
> also i know that for that warning (`cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`) i must rebuild the tensor-flow from binaries enabling the AVX2 and FMA instructions but what about the others?

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```

### Relevant log output

```shell

```","['type:bug', '2.18.rc']",2025-01-23T21:56:39Z,3,0,https://github.com/tensorflow/tensorflow/issues/85604
tensorflow/tensorflow,"Tutorial ""Multi-worker training with Keras"" fails to complete","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v1.12.1-120353-gc5bd67bc56f 2.19.0-dev20250107

### Custom code

No

### OS platform and distribution

Debian 6.1.123-1 (2025-01-02) x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

Python 3.12.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the tutorial everything goes well until you start the second worker. Then the below failure occures.

2025-01-20 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-20 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-20 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-20 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-01-20 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""-1""
2025-01-20 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA
2025-01-20 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-01-20 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael
2025-01-20 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael
2025-01-20 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0
2025-01-20 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0
2025-01-20 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0
2025-01-20 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345
2025-01-20 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635
2025-01-20 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect.
2025-01-20 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers:
/job:worker/replica:0/task:1
I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.
2025-01-20 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870
2025-01-20 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect.
/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
  warnings.warn(
2025-01-20 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
Traceback (most recent call last):
  File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in <module>
    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Attempt to convert a value (PerReplica:{
  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=
array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>
}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.

### Standalone code to reproduce the issue

```shell
python main.py &> job_1.log
```

### Relevant log output

```shell
2025-01-20 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-20 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-20 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-20 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-01-20 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""-1""
2025-01-20 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA
2025-01-20 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-01-20 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael
2025-01-20 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael
2025-01-20 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0
2025-01-20 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0
2025-01-20 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0
2025-01-20 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345
2025-01-20 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635
2025-01-20 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect.
2025-01-20 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers:
/job:worker/replica:0/task:1
I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.
2025-01-20 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870
2025-01-20 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect.
/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
  warnings.warn(
2025-01-20 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
Traceback (most recent call last):
  File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in <module>
    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Attempt to convert a value (PerReplica:{
  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=
array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>
}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.
```","['type:bug', 'TF 2.18']",2025-01-20T14:03:18Z,2,0,https://github.com/tensorflow/tensorflow/issues/85351
tensorflow/tensorflow,Aborted  in `tf.raw_ops.RaggedGather`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.raw_ops.RaggedGather` triggers crash.

### Standalone code to reproduce the issue

```shell
params_nested_splits = tf.constant(0, shape=[3], dtype=tf.int64)
params_dense_values = tf.constant(1, shape=[0], dtype=tf.float32)
indices = tf.constant(0, shape=[], dtype=tf.int64)
OUTPUT_RAGGED_RANK = 1
PARAMS_RAGGED_RANK = 1

tf.raw_ops.RaggedGather(
    params_nested_splits=[params_nested_splits],
    params_dense_values=params_dense_values,
    indices=indices,
    OUTPUT_RAGGED_RANK=1,
    name=None
)
```

### Relevant log output

```shell
2025-01-18 09:30:00.549762: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64
Aborted (core dumped)
```","['type:bug', 'comp:ops', '2.17']",2025-01-18T09:32:16Z,1,0,https://github.com/tensorflow/tensorflow/issues/85242
tensorflow/tensorflow,Segmentation fault (core dumped) in `RaggedTensorToTensor`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

shape = tf.constant(-1, shape=[], dtype=tf.int64)
values = tf.constant(0, shape=[0], dtype=tf.int32)
default_value = tf.constant(0, shape=[], dtype=tf.int32)
row_partition_tensors = tf.constant([0, 1, 6], shape=[3], dtype=tf.int64)
row_partition_types = [""ROW_SPLITS""]

tf.raw_ops.RaggedTensorToTensor(
    shape=shape,
    values=values,
    default_value=default_value,
    row_partition_tensors=[row_partition_tensors],
    row_partition_types=row_partition_types)
```

### Relevant log output

```shell
Segmentation fault (core dumped)
```","['type:bug', 'comp:ops', '2.17']",2025-01-18T09:27:19Z,1,0,https://github.com/tensorflow/tensorflow/issues/85240
tensorflow/tensorflow,Seg Fault when iterate dataset created from data service,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Segfault when trying to iterate dataset get from data service.

### Standalone code to reproduce the issue

```shell
# start the data service file start_dataservice.py

import tensorflow as tf

dispatcher = tf.data.experimental.service.DispatchServer(
    tf.data.experimental.service.DispatcherConfig(port=50050), start=True
)
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True
)
print(""Starting Worker"")
worker.join()

# test file test_dataset_service.py
import tensorflow as tf
import numpy as np


flags = tf.compat.v1.app.flags

flags.DEFINE_bool(""local"", False, ""Run data service in process"")
flags.DEFINE_bool(""distribute"", False, ""Run data service in distributed_epoch mode"")
FLAGS = flags.FLAGS


def local_service():
    print(""Starting Local Service"")
    dispatcher = tf.data.experimental.service.DispatchServer(
        tf.data.experimental.service.DispatcherConfig(port=50050), start=True
    )
    dispatcher_address = dispatcher.target.split(""://"")[1]
    worker = tf.data.experimental.service.WorkerServer(
        tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True
    )
    print(""Dispatcher target is "", dispatcher.target)
    return dispatcher, worker, dispatcher.target


def apply_transformations(ds_train):
    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    ds_train = ds_train.cache()
    ds_train = ds_train.shuffle(60000)
    ds_train = ds_train.batch(128)
    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)
    return ds_train


(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
x_train = x_train / np.float32(255)
y_train = y_train.astype(np.int64)
ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))


def normalize_img(image, label):
    """"""Normalizes images: `uint8` -> `float32`.""""""
    return tf.cast(image, tf.float32) / 255.0, label


ds_train = apply_transformations(ds_train)
# Create dataset however you were before using the tf.data service.
dataset = ds_train
if FLAGS.local:
    dispatcher, worker, service = local_service()
else:
    dispatcher_address = ""localhost""
    dispatcher_port = ""50050""
    service = ""grpc://{}:{}"".format(dispatcher_address, dispatcher_port)
if FLAGS.distribute:
    processing_mode = ""distributed_epoch""
else:
    processing_mode = ""parallel_epochs""

# This will register the dataset with the tf.data service cluster so that
# tf.data workers can run the dataset to produce elements. The dataset returned
# from applying `distribute` will fetch elements produced by tf.data workers.
dataset = dataset.apply(
    tf.data.experimental.service.distribute(processing_mode=processing_mode, service=service)
)

for (x1, y1), (x2, y2) in zip(dataset, ds_train):
    np.allclose(x1, x2)
    np.allclose(y1, y2)

print(""verified mnist dataset locally vs over service"")

# script to run 
python -m pip install --upgrade pip
python -m pip install tensorflow==2.18.0
python -m pip install 'protobuf<4'
screen -d -m python start_dataservice.py
python3 test_dataset_service.py --local=False
```

### Relevant log output

```shell
2025-01-14 21:56:19.778399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736891779.795141    9168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1736891779.800177    9168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-14 21:56:19.815971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
I0000 00:00:1736891783.518634    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 889 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1c.0, compute capability: 8.0
I0000 00:00:1736891783.520395    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 37945 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1d.0, compute capability: 8.0
I0000 00:00:1736891783.522012    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 37945 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1c.0, compute capability: 8.0
I0000 00:00:1736891783.523626    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 37945 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1d.0, compute capability: 8.0
I0000 00:00:1736891783.525222    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 37945 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1c.0, compute capability: 8.0
I0000 00:00:1736891783.526807    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 37945 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1d.0, compute capability: 8.0
I0000 00:00:1736891783.528377    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 37945 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1c.0, compute capability: 8.0
I0000 00:00:1736891783.529933    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 37945 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1d.0, compute capability: 8.0
/test/bin/testDataservice: line 5:  9168 Segmentation fault      (core dumped) python ${BIN_DIR}/test_dataset_service.py --local=False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-01-14T21:57:20Z,0,0,https://github.com/tensorflow/tensorflow/issues/84897
tensorflow/tensorflow,GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

nightly

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Python version

Python 3.12

### CUDA/cuDNN version

CUDA 12.4

### GPU model and memory

A100 80GB

### Current behavior?

Start a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace. 
No any memory profile events or OP profiler, but only trace view.

### Standalone code to reproduce the issue

**tf_allreduce.py**
```python
import tensorflow as tf
from tensorflow.python.ops.collective_ops import all_reduce, all_reduce_v2
from tensorflow.python.eager import context
from tensorflow.core.protobuf import config_pb2
from tensorflow.python.distribute import cluster_resolver as cluster_resolver_lib

cluster_resolver = cluster_resolver_lib.TFConfigClusterResolver()
cluster = cluster_resolver.cluster_spec()
task_type = cluster_resolver.task_type
task_id = cluster_resolver.task_id

experimental_config = config_pb2.ConfigProto.Experimental(
    share_cluster_devices_in_session=False,
    share_session_state_in_clusterspec_propagation=False
)
config = config_pb2.ConfigProto(experimental=experimental_config)
config.experimental.collective_group_leader = '/job:worker/replica:0/task:0'
server = tf.distribute.Server(cluster,
                              job_name=task_type,
                              task_index=task_id,
                              protocol=""grpc"", # ""grpc+verbs""
                              config=config)
run_options = config_pb2.RunOptions()

with tf.compat.v1.Session(target=server.target, config=config) as sess:
    tensor = tf.Variable(tf.ones([2, 2]), dtype=tf.float32)
    init = tf.compat.v1.global_variables_initializer()
    sess.run(init)
    sess.run(tf.print([""tensor:"",tensor]))

    reduced_tensor = all_reduce(tensor, group_size=2, group_key=4321, instance_key=1234, merge_op='Add', final_op='Id', communication_hint='auto')
    run_options.experimental.collective_graph_key = 6
    while True:
        sess.run(tf.print([""reduced_tensor:"",reduced_tensor]), options=run_options)
```

Run script to start server.
```bash
CUDA_VISIBLE_DEVICES=0 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":0}}' python tf_allreduce.py&
CUDA_VISIBLE_DEVICES=1 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":1}}' python tf_allreduce.py&
```

 use capture_profile in Tensorboard or tf.profiler.experimental.client.trace.
```python
tf.profiler.experimental.client.trace(
  'grpc://localhost:2223,grpc://localhost:2224',
   '/tmp/my_tb_dir',
   2000,
)
```

Try to convert xplane.pb to memory_profile, nothing show.
```python
from tensorflow.python.profiler.internal import _pywrap_profiler as profiler_wrapper
json = profiler_wrapper.xspace_to_tools_data([""xxx.xplane""], ""memory_profile"")
```

**Relevant log output**
```
{""memoryProfilePerAllocator"":{},""numHosts"":1,""memoryIds"":[]}
```

Relative issue: #48146 ","['type:bug', 'comp:gpu', 'TF 2.18']",2025-01-09T09:26:20Z,0,0,https://github.com/tensorflow/tensorflow/issues/84460
tensorflow/tensorflow,Unable to connect to TPU through Cloud VM (metadata issue?),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

tpu-ubuntu2204-base

### Mobile device

_No response_

### Python version

3.11.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am on a VM instance trying to connect to a tpu v4-32 using a test script. I installed tensorflow-tpu on both the VM (in venv) and TPU (globally) as per the instructions from the google website.

It seems like there is an issue with getting TPU metadata.

It is able to connect to the metadata server when I request manually from the VM:

```
$ curl http://169.254.169.254/computeMetadata/v1/ -H ""Metadata-Flavor: Google""
instance/
oslogin/
project/
```

Any help would be appreciated!

### Standalone code to reproduce the issue

```shell
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name)
tf.config.experimental_connect_to_cluster(resolver)
try:
    tf.tpu.experimental.initialize_tpu_system(resolver)
    print(""TPU initialized:"", resolver.master())
except Exception as e:
    print(""Failed to initialize TPU:"", e)
```


### Relevant log output

```shell
$ python hello.py
2025-01-08 23:49:33.189260: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-08 23:49:33.221197: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:95] Opening library: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
2025-01-08 23:49:33.221290: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:121] Libtpu path is: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/site-packages/libtpu/libtpu.so
Failed to get TPU metadata (tpu-env) from instance metadata for variable CHIPS_PER_HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable ALT: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable WRAP: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (accelerator-type) from instance metadata for variable TPU_ACCELERATOR_TYPE: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

Failed to find host bounds for accelerator type: WARNING: could not determine TPU accelerator type, please set env var `TPU_ACCELERATOR_TYPE` manually, otherwise libtpu.so may not properly initialize.
Failed to get TPU metadata (agent-worker-number) from instance metadata for variable TPU_WORKER_ID: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

Failed to get TPU metadata (worker-network-endpoints) from instance metadata for variable TPU_WORKER_HOSTNAMES: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

WARNING: Logging before InitGoogle() is written to STDERR
E0000 00:00:1736380405.363400    3192 common_lib.cc:511] INVALID_ARGUMENT: Error: unexpected worker hostname 'WARNING: could not determine TPU worker hostnames or IP addresses' from env var TPU_WORKER_HOSTNAMES. Expecting a valid hostname or IP address without port number. (Full TPU workers' addr string: WARNING: could not determine TPU worker hostnames or IP addresses, please set env var `TPU_WORKER_HOSTNAMES` manually, otherwise libtpu.so may not properly initialize.)
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/libtpu_init_utils.cc:173
2025-01-08 23:56:48.526584: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736380609.730442    3192 context_distributed_manager.cc:762] unknown service tensorflow.WorkerService
Additional GRPC error information from remote target /job:worker/replica:0/task:0 while calling /tensorflow.WorkerService/GetStatus:
:{""created"":""@1736380609.730372913"",""description"":""Error received from peer ipv4:10.130.0.3:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""unknown service tensorflow.WorkerService"",""grpc_status"":12}
E0108 23:56:49.730822322    3192 completion_queue.cc:244]    assertion failed: queue.num_items() == 0
https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1ccaf0e04f&map= 
*** SIGABRT received by PID 3192 (TID 3192) on cpu 4 from PID 3192; stack trace: ***
PC: @     0x7f1ccaf5cebc  (unknown)  (unknown)
    @     0x7f1caa302841       1888  (unknown)
    @     0x7f1ccaf0e050   18460496  (unknown)
    @     0x7f1ccaed1c60  (unknown)  (unknown)
https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1caa302840,7f1ccaf0e04f,7f1ccaed1c5f&map= 
E0108 23:56:49.732558    3192 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked.
E0108 23:56:49.732569    3192 coredump_hook.cc:355] RAW: Skipping coredump since rlimit was 0 at process start.
E0108 23:56:49.732575    3192 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
E0108 23:56:49.732580    3192 coredump_hook.cc:411] RAW: Sending fingerprint to remote end.
E0108 23:56:49.732595    3192 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
E0108 23:56:49.732601    3192 coredump_hook.cc:472] RAW: Dumping core locally.
E0108 23:56:49.745981    3192 process_state.cc:805] RAW: Raising signal 6 with default behavior
Aborted
```
","['type:bug', 'comp:tpus', 'TF 2.18']",2025-01-09T00:04:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/84413
tensorflow/tensorflow,dictionaries in fit method of model load data in wrong order,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17; tf 2.18

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

the code is running in google collab.
The code below is an example of a model with multiple inputs and multiple outputs.
NOT working code with using **dictionaries** in method **fit** of model.

the link to collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing
the link to gist: https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd


### Standalone code to reproduce the issue

```shell
# collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing
# gist:      https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd

# fast code
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

vocabulary_size = 10000
num_tags = 100
num_departments = 4

# define three model inputs
title = keras.Input(shape=(vocabulary_size,), name=""title"")
text_body = keras.Input(shape=(vocabulary_size,), name=""text_body"")
tags = keras.Input(shape=(num_tags,), name=""tags"")

features = layers.Concatenate()([title, text_body, tags])
# one intermediate layer
features = layers.Dense(64, activation=""relu"")(features)

# Define two model outputs
priority = layers.Dense(1, activation=""sigmoid"", name=""priority"")(features)
department = layers.Dense(num_departments, activation=""softmax"", name=""department"")(features)

# set the model
model = keras.Model(inputs=[title, text_body, tags],
                    outputs=[priority, department])
# prepare data
num_samples = 1280
# The data is filled in with zeros and ones
title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))

# priority: [0., 1.]
priority_data = np.random.random(size=(num_samples, 1))
# class of 4 labels
department_data = np.random.randint(0, 2, size=(num_samples, num_departments))

# compile model
model.compile(optimizer=""rmsprop"",
              loss={""priority"": ""mean_squared_error"",
                    ""department"": ""categorical_crossentropy""},
              metrics={""priority"": [""mean_absolute_error""],
                       ""department"": [""accuracy""]})

# It doesn't matter how the model is compiled
# model.compile(optimizer=""rmsprop"",
#               loss=[""mean_squared_error"", ""categorical_crossentropy""],
#               metrics=[[""mean_absolute_error""], [""accuracy""]])


# NOT WORKING
# TRAIN MODEL WITH transferring the DICTIONARY to the method
model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},
          {""priority"": priority_data, ""department"": department_data},
          epochs=1
)

# WORK
# TRAIN MODEL WITHOUT transferring the DICTIONARY to the method
model.fit([title_data, text_body_data, tags_data],
          [priority_data, department_data],
          epochs=1
)

# ALSO WORK
# TRAIN MODEL WITH transferring the DICTIONARY to the method
# REPLACE priority and department
model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},
          {""priority"": department_data, ""department"": priority_data},
          epochs=1
)
```


### Relevant log output

_No response_","['stat:awaiting response', 'type:bug', 'stale', 'comp:keras', 'TF 2.18']",2025-01-07T13:08:06Z,3,0,https://github.com/tensorflow/tensorflow/issues/84278
tensorflow/tensorflow,keras model.save does not respect `include_optimizer=False`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0-dev20250105

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Saving a model using keras with `include_optizer = False` results in a model being saved with optimizer

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1x5NJs9nFxmExhuy8_f_fOehHmIOmk-CZ?usp=sharing
```


### Relevant log output

_No response_","['type:bug', 'comp:keras', 'TF 2.18']",2025-01-07T10:33:38Z,4,0,https://github.com/tensorflow/tensorflow/issues/84268
tensorflow/tensorflow,Encountered unresolved custom op: XlaDynamicSlice,"Hi, i am doing a task in converting T5 model to TFLite for Android. Currently, i am using T5ModelForConditionalGeneration from Huggingface to convert. The conversion is done with some below logging but when load the `generator` from Interpretor, and run an inference example, i have faced this error. You can reproduce with the colab provided below. AFAIK, this XlaDynamicSlice is in TF ops but why this op cannot be resolved in this cases. 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): PyPI 24.0 on Python 3.11.5
- TensorFlow version (or github SHA if from source): 2.18.0


**Provide the text output from tflite_convert**
In colab version, tflite_convert doesn't log anything, below log is in my local version
```
INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets
INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets
W0000 00:00:1736157114.568747 1061359 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1736157114.568765 1061359 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
2025-01-06 16:51:54.568997: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpaxxybw9x
2025-01-06 16:51:54.645325: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2025-01-06 16:51:54.645352: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpaxxybw9x
2025-01-06 16:51:55.085153: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2025-01-06 16:51:56.061632: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpaxxybw9x
2025-01-06 16:51:56.517300: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 1948307 microseconds.
2025-01-06 16:52:30.233639: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexStridedSlice
Details:
	tf.StridedSlice(tensor<?x?x?xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<?x1x?x?xf32>) : {begin_mask = 13 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 13 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
2025-01-06 16:52:30.233666: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3836] The following operation(s) need TFLite custom op implementation(s):
Custom ops: XlaDynamicSlice
Details:
	tf.XlaDynamicSlice(tensor<1x12x?x?xf32>, tensor<4xi64>, tensor<4xi64>) -> (tensor<1x12x1x?xf32>) : {device = """"}
See instructions: https://www.tensorflow.org/lite/guide/ops_custom
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
My reproduce code in Colab: https://colab.research.google.com/drive/1Rmhc_vpJSa7M1Vt-4ugV5uORaEfRJMOw?usp=sharing
Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
","['stat:awaiting response', 'type:bug', 'comp:lite', 'TFLiteConverter', 'TF 2.18']",2025-01-06T10:46:54Z,2,0,https://github.com/tensorflow/tensorflow/issues/84203
tensorflow/tensorflow,MFCC-Example-Model converted from TF to TFlite fails with IsPowerOfTwo-RuntimeError inside rfft2d,"### 1. System information

- OS Platform and Distribution: Linux Mint 6.2.9
- TensorFlow installation: pip
- TensorFlow library: 2.18.0 (latest)

### 2. Code

Below is a minimum example which triggers the rfft2d IsPowerOfTwo RuntimeError.
The MFCC-Calculation was directly taken from the tutorial from [tensorflow.org](https://www.tensorflow.org/api_docs/python/tf/signal/mfccs_from_log_mel_spectrograms#for_example)

```
import tensorflow as tf

class MFCCLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(MFCCLayer, self).__init__(**kwargs)

    def call(self, pcm):
        # A 1024-point STFT with frames of 64 ms and 75% overlap.
        stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256, fft_length=1024)
        spectrograms = tf.abs(stfts)

        # Warp the linear scale spectrograms into the mel-scale.
        num_spectrogram_bins = stfts.shape[-1]
        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80
        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
            num_mel_bins,
            num_spectrogram_bins,
            sample_rate,
            lower_edge_hertz,
            upper_edge_hertz,
        )
        mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)
        mel_spectrograms.set_shape(
            spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:])
        )

        # Compute a stabilized log to get log-magnitude mel-scale spectrograms.
        log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)

        # Compute MFCCs from log_mel_spectrograms and take the first 13.
        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[
            ..., :13
        ]
        print(""mfccs.shape: "", mfccs.shape)
        return mfccs


def build_model(input_shape):
    input_layer = tf.keras.layers.Input(shape=input_shape)
    output_layer = MFCCLayer()(input_layer)
    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer)


if __name__ == ""__main__"":
    batch_size, num_samples, sample_rate = 32, 32000, 16000.0
    # A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].
    pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)
    print(""pcm.shape: "", pcm.shape)

    model = build_model(pcm.shape)
    model.summary()

    # Convert to TensorFlow Lite and Save
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()

    with open(""mfcc.tflite"", ""wb"") as f:
        f.write(tflite_model)

    # Load the model and run inference
    with open(""mfcc.tflite"", ""rb"") as f:
        tflite_model = f.read()

    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    pcm = tf.expand_dims(pcm, axis=0)  # Add batch dimension

    interpreter.set_tensor(input_details[0][""index""], pcm)
    interpreter.invoke()  # <-- RuntimeError: tensorflow/lite/kernels/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.
    mfccs = interpreter.get_tensor(output_details[0][""index""])
    print(""mfccs.shape: "", mfccs.shape)
```


### 3. Failure after conversion
As far as I know, the RuntimeError should't happen, as all supplied stft-function arguments are power of two's?

I am unsure if this is just a user error from myself or this is a bug.
I couldn't find any info online, hence i ask here.

Is a MFCC-calculation model possible with TFlite?

Thanks for all help

","['stat:awaiting response', 'type:bug', 'stale', 'comp:lite', 'TFLiteConverter', 'TF 2.18']",2025-01-05T20:45:45Z,3,0,https://github.com/tensorflow/tensorflow/issues/84171
tensorflow/tensorflow,Broken compatibility with tensorflow-metal in 2.18,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

MacOS 15.2

### Mobile device

_No response_

### Python version

3.11.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M2 Max GPU 38-cores

### Current behavior?

Apple silicone GPU with tensorflow-metal==1.1.0  and python 3.11 works fine with tensorboard==2.17.0

This is normal output:
```
/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py 
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

Process finished with exit code 0
```

But if I upgrade tensorflow to 2.18 I'll have error, attached in ""Relevant log output"" issue section

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

if __name__ == '__main__':
    gpus = tf.config.experimental.list_physical_devices('GPU')
    print(gpus)
```


### Relevant log output

```shell
/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py 
Traceback (most recent call last):
  File ""/Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py"", line 1, in <module>
    import tensorflow as tf
  File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/__init__.py"", line 437, in <module>
    _ll.load_library(_plugin_dir)
  File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/python/framework/load_library.py"", line 151, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii
  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow-plugins/libmetal_plugin.dylib
  Expected in:     <2814A58E-D752-317B-8040-131217E2F9AA> /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so

Process finished with exit code 1
```
","['stat:awaiting response', 'type:bug', 'stale', 'comp:gpu', 'TF 2.18']",2025-01-05T17:26:17Z,5,0,https://github.com/tensorflow/tensorflow/issues/84167
tensorflow/tensorflow,The test case label_image .py of tensorflow2.4.1 source code fails to be execued.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.4.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.12

### Bazel version

3.7

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The test case label_image.py fails to be executed,and the message ""module 'tensorfle' has no attribute 'GrapDef'"" is displayed.
![image](https://github.com/user-attachments/assets/e4b7b56d-2589-41fe-8395-1743c941dd49)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
graph_def = tf.GraphDef()
```


### Relevant log output

_No response_","['stat:awaiting response', 'type:bug', 'stale', 'TF 2.4']",2025-01-03T03:03:30Z,6,0,https://github.com/tensorflow/tensorflow/issues/84039
tensorflow/tensorflow,Failing to convert MobileNetV3Large to TFLite w/ Integer q,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Windows 10 WSL
- TensorFlow installation (pip package or built from source):  2.10 (on Win 10) and 2.16.2 (on WSL)

### 2. Code

```
import tensorflow as tf
import numpy as np
from tensorflow.keras.applications import MobileNetV3Large
from tensorflow.keras.applications.mobilenet_v3 import preprocess_input
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Generate one sample image for testing
test_image = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))
test_image = np.clip(test_image, 0, 255).astype(np.float32)
preprocessed_image = preprocess_input(test_image.copy())

# Load model
model = MobileNetV3Large(
    weights='imagenet',
    include_top=True,
    input_shape=(224, 224, 3)
)

# Get original prediction
original_pred = model.predict(preprocessed_image, verbose=0)

# Convert to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable dynamic range quantization
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]
converter._experimental_disable_per_channel = True
converter.experimental_new_converter = True

# Convert
tflite_model = converter.convert()

# Get TFLite prediction
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0]['index'], preprocessed_image)
interpreter.invoke()
tflite_pred = interpreter.get_tensor(output_details[0]['index'])

# Calculate correlation
correlation, _ = pearsonr(original_pred.flatten(), tflite_pred.flatten())

# Visualize
plt.figure(figsize=(10, 5))

# Scatter plot
plt.subplot(1, 2, 1)
plt.scatter(original_pred.flatten(), tflite_pred.flatten(), alpha=0.5)
plt.plot([original_pred.min(), original_pred.max()],
         [original_pred.min(), original_pred.max()],
         'r--', label=f'Perfect Correlation\nActual: {correlation:.4f}')
plt.title('Original vs Quantized Predictions')
plt.xlabel('Original Model')
plt.ylabel('Quantized Model')
plt.legend()

# Distribution plot
plt.subplot(1, 2, 2)
plt.hist(np.abs(original_pred.flatten() - tflite_pred.flatten()),
         bins=50, alpha=0.75, label='Prediction Differences')
plt.title('Distribution of Prediction Differences')
plt.xlabel('|Original - Quantized|')
plt.ylabel('Count')
plt.legend()

plt.tight_layout()
plt.show()

print(f""\nResults:"")
print(f""Prediction correlation: {correlation:.4f}"")
print(f""Original model size: {len(model.get_weights()) / 1024 / 1024:.2f} MB"")
print(f""Quantized model size: {len(tflite_model) / 1024 / 1024:.2f} MB"")
print(f""Size reduction: {(1 - len(tflite_model) / len(model.get_weights())) * 100:.1f}%"")
```

### 3. Failure after conversion
1. TF 2.10 in Win10 Log:
Model produces wrong results. See plot made from code:
![image](https://github.com/user-attachments/assets/8823cbfc-88d9-4e2d-9ef7-c8a2adc3ef0a)

2. TF2.16 in WSL:
Model fails to convert. Gets error: `LLVM ERROR: Failed to infer result type(s).` (see log)


### 5. (optional) Any other info / logs
I ran this on 2 systems:

1. TF 2.10 in Win10 Log:
```
import sys; print('Python %s on %s' % (sys.version, sys.platform))
D:\code\ai_dev\venv\Scripts\python.exe ""C:/Program Files/JetBrains/PyCharm 2023.2.4/plugins/python/helpers/pydev/pydevd.py"" --multiprocess --qt-support=auto --client 127.0.0.1 --port 54366 --file C:\Users\Administrator\AppData\Roaming\JetBrains\PyCharm2023.2\scratches\tfmodel_tflite.py 
Connected to pydev debugger (build 232.10203.26)
2024-12-26 15:17:07.215039: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-26 15:17:07.670016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7423 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6
2024-12-26 15:17:11.111912: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8906
2024-12-26 15:17:12.036555: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 64). These functions will not be directly callable after loading.
2024-12-26 15:17:44.746406: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2024-12-26 15:17:44.746529: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2024-12-26 15:17:44.747230: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:44.771028: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }
2024-12-26 15:17:44.771129: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:44.886049: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
2024-12-26 15:17:44.904668: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.
2024-12-26 15:17:45.275249: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:45.402632: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 655396 microseconds.
2024-12-26 15:17:45.811466: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```

3. TF 2.16.2 in WSL log:
```
/root/ai_dev/.venv/bin/python /root/.pycharm_helpers/pydev/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 55955 --file /mnt/c/Users/Administrator/AppData/Roaming/JetBrains/PyCharm2023.2/scratches/tfmodel_tflite.py 
Connected to pydev debugger (build 232.10203.26)
2024-12-26 15:18:56.434366: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-26 15:18:57.803991: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-26 15:18:58.473972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-26 15:18:58.972456: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-12-26 15:18:58.975123: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-26 15:18:59.910805: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-26 15:19:06.124533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-12-26 15:19:15.989425: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node
Your kernel may have been built without NUMA support.
2024-12-26 15:19:17.013008: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1735255170.947341  469943 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.
W0000 00:00:1735255170.947401  469943 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.
2024-12-26 15:19:30.948060: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp9lkkwnp9
2024-12-26 15:19:30.953309: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-12-26 15:19:30.953334: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp9lkkwnp9
2024-12-26 15:19:31.011901: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-12-26 15:19:31.020594: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.
2024-12-26 15:19:31.231606: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /tmp/tmp9lkkwnp9
2024-12-26 15:19:31.297302: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 349244 microseconds.
2024-12-26 15:19:31.779723: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(fused[""ReadVariableOp:"", callsite(""MobileNetV3Large_1/conv_1/convolution/ReadVariableOp@__inference_serving_default_5035""(""/root/.pycharm_helpers/pydev/pydevd.py"":2199:1) at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":2181:1 at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":1493:1 at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":1500:1 at callsite(""/root/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py"":18:1 at callsite(""/mnt/c/Users/Administrator/AppData/Roaming/JetBrains/PyCharm2023.2/scratches/tfmodel_tflite.py"":34:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1175:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1129:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1636:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1614:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py"":205:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1537:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/layer.py"":58:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/layer.py"":112:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py"":899:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/operation.py"":46:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":156:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/models/functional.py"":182:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/function.py"":171:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/models/functional.py"":632:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py"":899:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/operation.py"":46:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":156:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"":243:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"":233:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/nn.py"":1183:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py"":301:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py"":274:1 at ""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py"":85:1))))))))))))))))))))))))))))))))]): error: missing attribute 'value'
LLVM ERROR: Failed to infer result type(s).

Process finished with exit code 134
```


","['stat:awaiting tensorflower', 'type:bug', 'comp:lite', 'TFLiteConverter', 'TF 2.16']",2024-12-26T23:21:09Z,10,0,https://github.com/tensorflow/tensorflow/issues/83754
tensorflow/tensorflow,How to run TFLite benchmark with QNN delegate in Android,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

No

### OS platform and distribution

macOS 15.2

### Mobile device

One Plus 7 Pro, Android 11

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have built/installed/run TFLite benchmark following this [instruction](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#on-android) for Android, and used TensorFlow 2.15.0 according to [issue#66015](https://github.com/tensorflow/tensorflow/issues/66015). I test the benchmark via the following commands and the output result seems correct.
```shell
adb push /Users/handleychen/Github/tensorflow/tensorflow/bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model /data/local/tmp
adb shell chmod +x /data/local/tmp/benchmark_model
adb shell ""mkdir /data/local/tmp/models""
adb push /Users/handleychen/Github/tensorflow/models/*.tflite /data/local/tmp/models
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --num_threads=4 --enable_op_profiling=true
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --use_gpu=true --enable_op_profiling=true
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --use_nnapi=true --enable_op_profiling=true
``` 
[benchmark result.txt](https://github.com/user-attachments/files/18197819/benchmark.result.txt)

Now I want to run the benchmark with QNN delegate. I [setup the on device environment](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/TfLite-Delegate_setup.html#on-device-environment-setup) and [run a QNN delegate using an external delegate](https://docs.qualcomm.com/bundle/publicresource/topics/80-70015-54/sample-applications.html#run-a-qnn-delegate-using-an-external-delegate). The [model](https://storage.googleapis.com/download.tensorflow.org/models/tflite/task_library/image_classification/android_java/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite) being tested comes from tflite example [image_classification](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification).  I tested the benchmark using the following commands, but the result was a failure.
```shell
adb shell ""mkdir /data/local/tmp/qnn_delegate""
adb push /Users/handleychen/Github/quic/SDK/qairt/2.26.0.240828/lib/aarch64-android/* /data/local/tmp/qnn_delegate
adb shell
cd /data/local/tmp
export LD_LIBRARY_PATH=/data/local/tmp/qnn_delegate
export ADSP_LIBRARY_PATH=""/data/local/tmp/qnn_delegate""
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:gpu;'
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;'
# I also tried setting htp_precision:1, but the result was the same.
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;htp_precision:1'
``` 
```shell
# for gpu delegate
……
INFO: Though EXTERNAL delegate is explicitly applied, the model graph will not be executed by the delegate.
……

# for npu delegate
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]
INFO: External delegate path: [/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so]
INFO: External delegate options: [backend_type:htp;htp_precision:1]
INFO: Loaded model /data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: EXTERNAL delegate created.
ERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008
ERROR: Restored original execution plan after delegate application failure.
ERROR: Failed to apply EXTERNAL delegate.
ERROR: Benchmarking failed.
``` 
The full output is attached. [benchmarkQNN result.txt](https://github.com/user-attachments/files/18206356/benchmarkQNN.result.txt)

I have also tested it on Android phones equipped with Snapdragon 855 and Gen 3 chips, and the results were the same.

Could anyone tell me how to deal with this?

### Standalone code to reproduce the issue

```shell
as described above
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:lite']",2024-12-19T12:40:25Z,4,0,https://github.com/tensorflow/tensorflow/issues/83344
tensorflow/tensorflow,Aborted (core dumped) in `LearnedUnigramCandidateSampler`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs,`tf.raw_ops.LearnedUnigramCandidateSampler` triggers crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

true_classes = tf.constant([], dtype=tf.int64)
num_true = 3590707793247644003
num_sampled = 126
unique = False
range_max = 186785497093039093
seed = 8997
seed2 = 0

tf.raw_ops.LearnedUnigramCandidateSampler(
    true_classes=true_classes,
    num_true=num_true,
    num_sampled=num_sampled,
    unique=unique,
    range_max=range_max,
    seed=seed,
    seed2=seed2,
    name=None
)
```


### Relevant log output

```shell
2024-12-17 11:36:29.305345: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at candidate_sampler_ops.cc:37 : INVALID_ARGUMENT: Attr num_true has value 3590707793247644003 out of range for an int32
2024-12-17 11:36:29.305378: F external/local_tsl/tsl/lib/random/weighted_picker.cc:28] Check failed: N >= 0 (0 vs. -2090015755)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-12-17T12:05:50Z,1,0,https://github.com/tensorflow/tensorflow/issues/83164
tensorflow/tensorflow,[XLA] `tf.keras.layers.LSTM` behaves differently on GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When executing LSTM on **XLA**, it fails.
However, when executing it without XLA, it passes.
The above failure is on GPU.
If I use CPU as backend, with or without XLA both pass the check.

### Standalone code to reproduce the issue

```python
import os
import tensorflow
import tensorflow as tf
tf.random.set_seed(42)
class RecurrentModel(tf.keras.Model):

    def __init__(self):
        super(RecurrentModel, self).__init__()
        self.lstm = tf.keras.layers.LSTM(units=64, return_sequences=True)

    @tf.function(jit_compile=True)
    def call(self, x):
        return self.lstm(x)


model = RecurrentModel()


input_shape = (10, 20, 1)
x = tf.random.normal(shape=input_shape)

inputs = [x]

output = model(*inputs)
print(output)
```


### Relevant log output

```shell
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-0938fdccd1fa> in <cell line: 24>()
     22 inputs = [x]
     23 
---> 24 output = model(*inputs)
     25 print(output)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Exception encountered when calling RecurrentModel.call().

Detected unsupported operations when trying to compile graph __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: CudnnRNNV3 (No registered 'CudnnRNNV3' OpKernel for XLA_GPU_JIT devices compatible with node {{node lstm_3_1/CudnnRNNV3}}){{node lstm_3_1/CudnnRNNV3}}
The op is created at: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-4-0938fdccd1fa>"", line 24, in <cell line: 24>
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 826, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1376, in _maybe_build
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 212, in compute_output_spec
File ""<ipython-input-1-0938fdccd1fa>"", line 13, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 901, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py"", line 570, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py"", line 406, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py"", line 537, in inner_loop
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py"", line 841, in lstm
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py"", line 933, in _cudnn_lstm
	tf2xla conversion failed while converting __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_877]

Arguments received by RecurrentModel.call():
  • x=tf.Tensor(shape=(10, 20, 1), dtype=float32)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'comp:xla', 'TF 2.18']",2024-12-16T13:57:22Z,2,0,https://github.com/tensorflow/tensorflow/issues/83063
tensorflow/tensorflow,`tensorflow.experimental.numpy.kron` not working with multidimensional Arrays,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

This is a new issue in replacement for https://github.com/tensorflow/tensorflow/issues/59761 as suggested by @tilakrayal

I tested the function against numpy and it throws an error when the `ndim` of the input tensors is greater than 2.
I run the code on the latest TensorFlow version on PyPI and the nightly version, and I get the same failures.

Also, I am not getting as much debug information only this error

`UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name: `

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
try:
    a = tf.constant(np.arange(100).reshape(2, 5, 2, 5))
    b = tf.constant(np.arange(24).reshape(2, 3, 4))
    print(a.ndim) # 4
    print(b.ndim) # 3

    y = tf.experimental.numpy.kron(a, b)

    print(y.shape)
except:
    print(""Can't use tf.experimental.numpy.kron on multi-dimensional arrays"")

x = np.arange(100).reshape(2, 5, 2, 5)
y = np.arange(24).reshape(2, 3, 4)

print(x.ndim) # 4
print(y.ndim) # 3

z = np.kron(x, y)

print(z.shape) # (2, 10, 6, 20)
```


### Relevant log output

```shell
UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-12-16T07:22:01Z,3,0,https://github.com/tensorflow/tensorflow/issues/83037
tensorflow/tensorflow,error message is inconsistent with documentation in `tf.raw_ops.MaxPoolGradWithArgmax`,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

According to [Doc tf.raw_ops.MaxPoolGradWithArgmax](https://www.tensorflow.org/api_docs/python/tf/raw_ops/MaxPoolGradWithArgmax), the argument `argmax` can be `int32` or `int64`. However, after actual testing, the parameter `argmax` can only support tensor input of data type `int64`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_tensor = tf.constant(1, shape=[1, 2, 2, 1], dtype=tf.float32)
grad_tensor = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.float32)
argmax_indices = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.int32)
ksize = [1, 2, 2, 1]
strides = [1, 1, 1, 1]
padding = ""VALID""

output_grad = tf.raw_ops.MaxPoolGradWithArgmax(
    input=input_tensor,
    grad=grad_tensor,
    argmax=argmax_indices,
    ksize=ksize,
    strides=strides,
    padding=padding,
    include_batch_in_index=False
)
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node MaxPoolGradWithArgmax}} = MaxPoolGradWithArgmax[T=DT_FLOAT, Targmax=DT_INT32, include_batch_in_index=false, ksize=[1, 2, 2, 1], padding=""VALID"", strides=[1, 1, 1, 1]]
All kernels registered for op MaxPoolGradWithArgmax:
  device='CPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT32]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT64]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Targmax in [DT_INT64]
 [Op:MaxPoolGradWithArgmax] name:
```
","['type:docs-bug', 'stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-12-12T13:14:53Z,1,0,https://github.com/tensorflow/tensorflow/issues/82837
tensorflow/tensorflow,Update Python in Docker images to 3.11.x and ditch `3.11.0rc1`,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Docker

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The Python version of the docker images is outdated and should be updated

### Standalone code to reproduce the issue

```shell
docker run -it tensorflow/tensorflow python
```


### Relevant log output

```shell
Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>>
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'TF 2.18']",2024-12-09T13:35:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/82529
tensorflow/tensorflow,"The warning ""The structure of `inputs` doesn't match the expected structure"" when training a functional model","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.13.1-0-gf841394b1b7 2.13.1 (Nightly: v1.12.1-119104-gf8fd6f53fa3 2.19.0-dev20241204)

### Custom code

Yes

### OS platform and distribution

Windows 11 23H2 22631.4460

### Mobile device

Windows 11 23H2 22631.4460

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the model is functional, not Sequential, the warning has occured:

```
Epoch 1/5
<path-to-python>\lib\site-packages\keras\src\models\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*
  warnings.warn(
```

Yes, the warning message has interrupted on parenthesis. When I've run the same code in Nightly, the warning message is:

```
Epoch 1/5
<path-to-python>\lib\site-packages\keras\src\models\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.
Expected: ['keras_tensor']
Received: inputs=Tensor(shape=(None, 10))
  warnings.warn(msg)
```

After the warning, the training continues normally, but because of this warning, I can't be sure that the model works as I expect.

I've traced the source and found that in `Lib\site-packages\keras\src\tree\optree_impl.py` on line 95 comparasion of expected and actual structure failed. Now I place the traced variables here:

```
>>> a
<tf.Tensor 'data:0' shape=(None, 10) dtype=float64>
>>> b
[<KerasTensor shape=(None, 10), dtype=float32, sparse=False, name=keras_tensor>]
>>> a_structure
PyTreeSpec(*, NoneIsLeaf)
>>> b_structure
PyTreeSpec([*], NoneIsLeaf)
```

The data passed to the `fit` function fully corresponds to the [documentation](https://keras.io/api/models/model_training_apis/#fit-method). The warning appears independently of whether I use numpy array or PyDataset as dataset of `fit` function.



### Standalone code to reproduce the issue

```shell
from keras.models import Model
from keras.layers import Dense, Input, Flatten, Concatenate
from keras import utils
import numpy as np
import tensorflow as tf

class SamplesSet(utils.PyDataset):
    
    def __init__(self, batch_size, **kwargs):
        super().__init__(**kwargs)
        self.batch_size = batch_size
        
    def __len__(self):
        return 1
    
    def __getitem__(self, idx):
        x1 = np.random.uniform(size=10*self.batch_size).reshape((self.batch_size, 10))
        y = np.arange(self.batch_size)
        return x1, y
    
train = SamplesSet(100)
x1_train = np.random.uniform(size=10*100).reshape((100, 10))
y_train = np.arange(100)

input1 = Input(shape=(10,))
l1 = Dense(1)(input1)
d2 = Dense(1, activation='sigmoid')(l1)
model = Model(inputs=[input1], outputs=[d2])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

history = model.fit(x1_train, y_train, epochs=5, verbose=1)
# In the all cases below warning occures too
# history = Model.fit(train, epochs=5, verbose=1) 
# ret = model.predict(np.arange(10)[np.newaxis,:])
# ret = model.predict(tf.constant([[0,1,2,3,4,5,6,7,8,9]]))
```


### Relevant log output

_No response_","['type:bug', 'comp:keras', 'TF 2.13']",2024-12-06T03:47:02Z,8,0,https://github.com/tensorflow/tensorflow/issues/82372
tensorflow/tensorflow,[XLA] TF XLA outputs abnormal value when compiling `Embedding`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

For `Embedding` operator, when I set the `input_dim=1` (which means it indexed from 0 to 0), the output always returns **0 without XLA**.

After compilation, the outputs are usually some random tensors.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.random.set_seed(42)
x = tf.constant([1])


# uncompiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()

output1 = m(x)


# compiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    @tf.function(jit_compile=True)
    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()
output2 = m(x)

print(output1)
print(output2)
```


### Relevant log output

```shell
tf.Tensor([[0.]], shape=(1, 1), dtype=float32)
tf.Tensor([[-0.00567592]], shape=(1, 1), dtype=float32)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.18']",2024-12-05T13:58:40Z,4,0,https://github.com/tensorflow/tensorflow/issues/82317
tensorflow/tensorflow,Are checkpoints broken in >= 2.16?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16, 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The example given in https://www.tensorflow.org/guide/checkpoint does not seem to work as expected in 2.16 and 2.17, while working fine in 2.15. After restoring and restarting the training process, it starts training from the very beginning.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1n76Mu5BhdBJBSXc7cXYJMr0lMDER2JRa?usp=sharing
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', '2.17']",2024-12-04T15:11:53Z,2,0,https://github.com/tensorflow/tensorflow/issues/82209
tensorflow/tensorflow,TPU not support TensorFlow 2.18 and 2.17.1,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.18 and tf. 2.17.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`import tensorflow as tf` results `segmentation fault core dumped`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:tpus', 'TF 2.18']",2024-12-04T14:56:53Z,7,0,https://github.com/tensorflow/tensorflow/issues/82208
tensorflow/tensorflow,Clarify the `constant_op.constant(2)` statement,"It would be helpful to clarify the `constant_op.constant(2)` statement by explaining the corresponding import statement.

https://github.com/tensorflow/tensorflow/blob/5bc9d26649cca274750ad3625bd93422617eed4b/tensorflow/python/ops/summary_ops_v2.py#L1062-L1066","['type:bug', 'comp:ops']",2024-12-02T18:05:41Z,4,0,https://github.com/tensorflow/tensorflow/issues/81954
tensorflow/tensorflow,MixedPrecision + XLA: Seen floating point types of different precisions,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

Google Colab default

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using bilinear interpolation + XLA + mixed_float16 policy issue raises during compilation.
Without bilinear interpolation or without XLA or without mixed_float16 there is no issue.

In Google Colab i have this issue only on CPU with TF 2.17 and both CPU & GPU with TF 2.18.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1joSiScbM7Stc9bn1C4R_4sFkDzakrTsJ?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-4-9cc273be2d5a> in <cell line: 28>()
     26 model.compile(loss='mse', optimizer='adam', run_eagerly=False, jit_compile=True)
     27 
---> 28 model.fit(dataset)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InternalError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>

  File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start

  File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 377, in dispatch_queue

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 250, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 748, in __init__

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code

  File ""<ipython-input-4-9cc273be2d5a>"", line 28, in <cell line: 28>

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 368, in fit

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 216, in function

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 129, in multi_step_on_iterator

during context [Unknown]: Seen floating point types of different precisions in %multiply.43589 = f32[2,8,8,1280]{3,2,1,0} multiply(f32[2,8,8,1280]{3,2,1,0} %add.43539, f16[2,8,8,1280]{3,2,1,0} %multiply.43588), metadata={op_type=""Mul"" op_name=""mul_9"" source_file=""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"" source_line=1196}, but mixed precision is disallowed.
	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_124474]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.18']",2024-11-29T07:11:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/81273
tensorflow/tensorflow,Some operators give different results on CPU and GPU when dealing with complex numbers that include `inf`.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The outputs of TensorFlow mathematical APIs (`sin, cos, tan, sinh, cosh, exp, and reduce_mean`) are inconsistent between the CPU and GPU when applied to complex inputs containing `inf`. 

### Standalone code to reproduce the issue

```shell
python
import tensorflow as tf

test_inputs = [
    tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128),
]

test_apis = [
    tf.math.sin, tf.math.cos, tf.math.tan,
    tf.math.sinh, tf.math.cosh, tf.math.exp, tf.math.reduce_mean
]

for api in test_apis:
    print(f""Testing {api.__name__}"")
    for x in test_inputs:
        try:
            with tf.device('/CPU'):
              cpu_out = api(x)
              print(f""CPU Output: {cpu_out}"")
            with tf.device('/GPU:0'):
              gpu_out = api(x)
              print(f""GPU Output: {gpu_out}"")
        except Exception as e:
            print(f""Error in {api.__name__}: {e}"")
```


### Relevant log output

```shell
Testing sin
CPU Output: [nan +0.j  0.+infj nan+infj]
GPU Output: [nan+nanj nan+infj nan+nanj]
Testing cos
CPU Output: [nan +0.j inf -0.j inf+nanj]
GPU Output: [nan+nanj inf+nanj nan+nanj]
Testing tan
CPU Output: [nan+0.j  0.+1.j  0.+1.j]
GPU Output: [nan+nanj  0. +1.j  0. +1.j]
Testing sinh
CPU Output: [inf +0.j  0.+nanj inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing cosh
CPU Output: [inf +0.j nan +0.j inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing exp
CPU Output: [inf +0.j nan+nanj inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing reduce_mean
CPU Output: (inf+infj)
GPU Output: (nan+nanj)
```
","['stat:awaiting response', 'type:bug', 'stale', 'comp:ops', '2.17']",2024-11-27T13:13:05Z,4,0,https://github.com/tensorflow/tensorflow/issues/80947
tensorflow/tensorflow,"When using `tf.math.log1p` and NumPy's `np.log1p` with the same complex input, the outputs are inconsistent.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow's `tf.math.log1p` produces inconsistent results with NumPy's `np.log1p` for complex inputs containing `inf`, such as `[inf+0.j, 0+inf.j, inf+inf.j]`. TensorFlow outputs `[inf+0.j, nan+nanj, nan+nanj]`, while NumPy returns `[inf+0.j, inf+1.57079633j, inf+0.78539816j]`.

### Standalone code to reproduce the issue

```shell
python
import tensorflow as tf
import numpy as np

test_input = tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128)

# TensorFlow computation
with tf.device('/CPU:0'):
    cpu_out = tf.math.log1p(test_input)

# NumPy computation
numpy_out = np.log1p(test_input.numpy())

print(f""CPU Output: {cpu_out}"")
print(f""NumPy Output: {numpy_out}"")
```


### Relevant log output

```shell
CPU Output: [inf +0.j nan+nanj nan+nanj]
NumPy Output: [inf+0.j         inf+1.57079633j inf+0.78539816j]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-26T13:36:51Z,3,0,https://github.com/tensorflow/tensorflow/issues/80850
tensorflow/tensorflow,Heap-buffer-overflow in `SparseMatrixSparseCholesky`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When batch_index is scalar, `SparseMatrixSparseCholesky` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

indices = tf.constant([[0, 0], [1, 1], [2, 1], [2, 2], [3, 3]], dtype=tf.int64)
values = tf.constant([1.0, 2.0, 1.0, 3.0, 4.0], tf.float32)
dense_shape = tf.constant([4, 4], dtype=tf.int64)
input = tf.raw_ops.SparseTensorToCSRSparseMatrix(
    indices=indices, values=values, dense_shape=dense_shape, name=None
)
permutation = tf.constant([4,1,1,1], dtype=tf.int32)

tf.raw_ops.SparseMatrixSparseCholesky(
  input=input, permutation=permutation, type=tf.float32
)
```


### Relevant log output

```shell
=================================================================
==3331846==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60700049d1d0 at pc 0x7faa202d6d87 bp 0x7ffd09bf8fe0 sp 0x7ffd09bf8fd0
WRITE of size 4 at 0x60700049d1d0 thread T0
    #0 0x7faa202d6d86 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2d86)
    #1 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f373a)
    #2 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f84e9)
    #3 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4201d41)
    #4 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x26d8d71)
    #5 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2adab6)
    #6 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cdc14a)
    #7 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1862fe6)
    #8 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x173f306)
    #9 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x179cf74)
    #10 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x17aa8b1)
    #11 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c582eb)
    #12 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aedc2c)
    #13 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31af066a)
    #14 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c2a939)
    #15 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31addde4)
    #16 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31ae1dd4)
    #17 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aebd26)
    #18 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x20ad4c33)
    #19 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c1fe5e)
    #20 0x7fa9f268645b in TFE_Execute (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x1065245b)
    #21 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so+0x3c2274)
    #22 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0xb7ccb)
    #23 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #24 0x51ad66  (/usr/bin/python3.11+0x51ad66)
    #25 0x4e75db in _PyObject_MakeTpCall (/usr/bin/python3.11+0x4e75db)
    #26 0x4fb151 in _PyEval_EvalFrameDefault (/usr/bin/python3.11+0x4fb151)
    #27 0x531822 in _PyFunction_Vectorcall (/usr/bin/python3.11+0x531822)
    #28 0x541194 in PyObject_Call (/usr/bin/python3.11+0x541194)
    #29 0x4fefe0 in _PyEval_EvalFrameDefault (/usr/bin/python3.11+0x4fefe0)
    #30 0x62e1b3  (/usr/bin/python3.11+0x62e1b3)
    #31 0x4f3a66 in PyEval_EvalCode (/usr/bin/python3.11+0x4f3a66)
    #32 0x647c36  (/usr/bin/python3.11+0x647c36)
    #33 0x64534f  (/usr/bin/python3.11+0x64534f)
    #34 0x650d14  (/usr/bin/python3.11+0x650d14)
    #35 0x650a63 in _PyRun_SimpleFileObject (/usr/bin/python3.11+0x650a63)
    #36 0x650832 in _PyRun_AnyFileObject (/usr/bin/python3.11+0x650832)
    #37 0x64f786 in Py_RunMain (/usr/bin/python3.11+0x64f786)
    #38 0x61ee0c in Py_BytesMain (/usr/bin/python3.11+0x61ee0c)
    #39 0x7faaf80d1d8f  (/lib/x86_64-linux-gnu/libc.so.6+0x29d8f)
    #40 0x7faaf80d1e3f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x29e3f)
    #41 0x61ec94 in _start (/usr/bin/python3.11+0x61ec94)

0x60700049d1d0 is located 0 bytes to the right of 80-byte region [0x60700049d180,0x60700049d1d0)
allocated by thread T0 here:
    #0 0x7faaf84bf887 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cpp:145
    #1 0x7faa202d6803 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2803)
    #2 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f373a)
    #3 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f84e9)
    #4 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4201d41)
    #5 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x26d8d71)
    #6 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2adab6)
    #7 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cdc14a)
    #8 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1862fe6)
    #9 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x173f306)
    #10 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x179cf74)
    #11 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x17aa8b1)
    #12 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c582eb)
    #13 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aedc2c)
    #14 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31af066a)
    #15 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c2a939)
    #16 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31addde4)
    #17 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31ae1dd4)
    #18 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aebd26)
    #19 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x20ad4c33)
    #20 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c1fe5e)
    #21 0x7fa9f268645b in TFE_Execute (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x1065245b)
    #22 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so+0x3c2274)
    #23 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0xb7ccb)
    #24 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #25 0x51ad66  (/usr/bin/python3.11+0x51ad66)

SUMMARY: AddressSanitizer: heap-buffer-overflow (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2d86) in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const
Shadow bytes around the buggy address:
  0x0c0e8008b9e0: 00 00 00 00 00 00 fa fa fa fa fd fd fd fd fd fd
  0x0c0e8008b9f0: fd fd fd fd fa fa fa fa 00 00 00 00 00 00 00 00
  0x0c0e8008ba00: 00 fa fa fa fa fa 00 00 00 00 00 00 00 00 04 fa
  0x0c0e8008ba10: fa fa fa fa 00 00 00 00 00 00 00 00 04 fa fa fa
  0x0c0e8008ba20: fa fa 00 00 00 00 00 00 00 00 04 fa fa fa fa fa
=>0x0c0e8008ba30: 00 00 00 00 00 00 00 00 00 00[fa]fa fa fa fa fa
  0x0c0e8008ba40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba50: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba60: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba70: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
==3331846==ABORTING
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-26T12:42:50Z,1,0,https://github.com/tensorflow/tensorflow/issues/80847
tensorflow/tensorflow,Aborted (core dumped) in `RaggedBincount`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value of size is close to the maximum value for the dtype, an integer overflow error will occur in following multiplication calculations.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

splits = tf.constant([0, 3, 5, 9], dtype=tf.int64)
values = tf.constant(1, shape=[3,3], dtype=tf.int64)
size = tf.constant(6522107765268123892, dtype=tf.int64)
weights = tf.constant(1, shape=[3,3], dtype=tf.float32)
counts = tf.raw_ops.RaggedBincount(splits=splits, values=values, size=size, weights=weights)
```


### Relevant log output

```shell
Status: INVALID_ARGUMENT: Encountered overflow when multiplying 3 with 6522107765268123892, result: -1
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-26T03:18:31Z,3,0,https://github.com/tensorflow/tensorflow/issues/80812
tensorflow/tensorflow,This method creates a model with a 100% memory leak loop using model. fit(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

ubuntu 2.2 or mac m1

### Mobile device

ubuntu 2.2 or mac m1

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend

### Standalone code to reproduce the issue

```shell
import gc

import keras
import numpy as np
import psutil
from keras.optimizers import Adam
from keras.layers import Dense, Dropout, Input, LSTM
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import time
import json


num_samples = 6
num_features = 3
num_classes = 4
epochs = 50
batch_size = 2
identifier = ""test_model""
num_iterations = 500  

def build_model(X, num_classes):
    model = Sequential()
    model.add(Input(shape=(X.shape[1], X.shape[2])))
    model.add(LSTM(16, return_sequences=True))
    model.add(LSTM(16))
    model.add(Dropout(0.4))
    model.add(Dense(8, activation='tanh'))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    return model


data_X = np.random.rand(num_samples, num_features)
data_Y = np.random.randint(0, num_classes, size=(num_samples, 1))  


data_Y = np.eye(num_classes)[data_Y.flatten()]  
print(type(data_X))

scaler = MinMaxScaler()
data_X_scaled = scaler.fit_transform(data_X)


train_X, test_X, train_Y, test_Y = train_test_split(data_X_scaled, data_Y, train_size=0.6, random_state=42)


train_X = np.expand_dims(train_X, axis=1)
test_X = np.expand_dims(test_X, axis=1)

best_loss = np.inf
best_model_data = None
for iteration in range(num_iterations):
   
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()
    model = build_model(train_X, num_classes)
 
    model_name = f""model_{iteration}""
    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=0, restore_best_weights=True)
    print(f""Iteration {iteration + 1}/{num_iterations}"")
    process = psutil.Process()
    mem_info = process.memory_info()
    print(f""start Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size
    try:
        history = model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, shuffle=True,
                            validation_data=(test_X, test_Y), verbose=0)
        current_loss = history.history['loss'][-1]
        print(f""Training model: {model.name}"")
    
        del model
        tf.keras.backend.clear_session()
        gc.collect()
    except Exception as e:
        print(""err:"", e)
    finally:
        process = psutil.Process()
        mem_info = process.memory_info()
        print(f""end Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size

print(""end！"")


if best_model_data:
    model_json = best_model_data[""model_architecture""]
    model_weights = json.loads(best_model_data[""model_weights""], object_hook=lambda d: np.array(d))
    model = tf.keras.models.model_from_json(model_json)
    model.set_weights(model_weights)
    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    print(""ok"")
else:
    print(""not found"")
```


### Relevant log output

```shell
Iteration 1/500
start Current memory usage: 450.69 MB
Training model: sequential
end Current memory usage: 524.41 MB
Iteration 2/500
start Current memory usage: 524.52 MB
Training model: sequential
end Current memory usage: 564.97 MB
Iteration 3/500
start Current memory usage: 564.98 MB
Training model: sequential
end Current memory usage: 598.00 MB
Iteration 4/500
start Current memory usage: 598.03 MB
Training model: sequential
end Current memory usage: 624.69 MB
Iteration 5/500
start Current memory usage: 624.69 MB
Training model: sequential
end Current memory usage: 653.89 MB
Iteration 6/500
start Current memory usage: 653.91 MB
Training model: sequential
end Current memory usage: 679.45 MB
Iteration 7/500
start Current memory usage: 679.45 MB
Training model: sequential
end Current memory usage: 701.59 MB
Iteration 8/500
start Current memory usage: 701.59 MB
Training model: sequential
end Current memory usage: 726.83 MB
Iteration 9/500
start Current memory usage: 726.84 MB
Training model: sequential
end Current memory usage: 749.56 MB
Iteration 10/500
start Current memory usage: 749.56 MB
Training model: sequential
end Current memory usage: 782.56 MB
Iteration 11/500
start Current memory usage: 782.56 MB
Training model: sequential
end Current memory usage: 805.92 MB
Iteration 12/500
start Current memory usage: 805.92 MB
Training model: sequential
end Current memory usage: 833.17 MB
Iteration 13/500
start Current memory usage: 833.17 MB
Training model: sequential
end Current memory usage: 852.84 MB
Iteration 14/500
start Current memory usage: 852.84 MB
Training model: sequential
end Current memory usage: 875.05 MB
Iteration 15/500
start Current memory usage: 875.06 MB
Training model: sequential
end Current memory usage: 901.56 MB
Iteration 16/500
start Current memory usage: 901.56 MB
Training model: sequential
end Current memory usage: 930.62 MB
Iteration 17/500
start Current memory usage: 705.70 MB
Training model: sequential
end Current memory usage: 762.64 MB
Iteration 18/500
start Current memory usage: 762.70 MB
Training model: sequential
end Current memory usage: 798.06 MB
Iteration 19/500
start Current memory usage: 798.17 MB
Training model: sequential
end Current memory usage: 824.98 MB
Iteration 20/500
start Current memory usage: 824.98 MB
Training model: sequential
end Current memory usage: 850.34 MB
Iteration 21/500
start Current memory usage: 850.42 MB
Training model: sequential
end Current memory usage: 876.81 MB
Iteration 22/500
start Current memory usage: 876.81 MB
Training model: sequential
end Current memory usage: 904.02 MB
Iteration 23/500
start Current memory usage: 904.08 MB
Training model: sequential
end Current memory usage: 929.70 MB
Iteration 24/500
start Current memory usage: 929.73 MB
Training model: sequential
end Current memory usage: 952.33 MB
Iteration 25/500
start Current memory usage: 952.34 MB
Training model: sequential
end Current memory usage: 952.28 MB
Iteration 26/500
start Current memory usage: 952.47 MB
Training model: sequential
end Current memory usage: 980.39 MB
Iteration 27/500
start Current memory usage: 978.78 MB
Training model: sequential
end Current memory usage: 999.02 MB
Iteration 28/500
start Current memory usage: 999.05 MB
Training model: sequential
end Current memory usage: 1023.50 MB
Iteration 29/500
start Current memory usage: 1023.53 MB
Training model: sequential
end Current memory usage: 1047.80 MB
Iteration 30/500
start Current memory usage: 1047.83 MB
Training model: sequential
end Current memory usage: 1068.88 MB
Iteration 31/500
start Current memory usage: 1068.94 MB
Training model: sequential
end Current memory usage: 1095.78 MB
Iteration 32/500
start Current memory usage: 1095.78 MB
Training model: sequential
end Current memory usage: 1119.03 MB
Iteration 33/500
start Current memory usage: 1119.03 MB
Training model: sequential
end Current memory usage: 1039.41 MB
Iteration 34/500
start Current memory usage: 1022.78 MB
Training model: sequential
end Current memory usage: 1040.88 MB
Iteration 35/500
start Current memory usage: 1040.70 MB
Training model: sequential
end Current memory usage: 1054.58 MB
Iteration 36/500
start Current memory usage: 1054.58 MB
Training model: sequential
end Current memory usage: 1076.16 MB
Iteration 37/500
start Current memory usage: 1076.19 MB
Training model: sequential
end Current memory usage: 1097.02 MB
Iteration 38/500
start Current memory usage: 1097.03 MB
Training model: sequential
end Current memory usage: 1113.70 MB
Iteration 39/500
start Current memory usage: 1114.12 MB
Training model: sequential
end Current memory usage: 1140.30 MB
Iteration 40/500
start Current memory usage: 1140.33 MB
Training model: sequential
end Current memory usage: 1163.81 MB
Iteration 41/500
start Current memory usage: 1163.86 MB
Training model: sequential
end Current memory usage: 1195.83 MB
Iteration 42/500
start Current memory usage: 1195.83 MB
Training model: sequential
end Current memory usage: 1221.53 MB
Iteration 43/500
start Current memory usage: 1221.55 MB
Training model: sequential
end Current memory usage: 1231.09 MB
Iteration 44/500
start Current memory usage: 1231.14 MB
Training model: sequential
end Current memory usage: 1245.78 MB
Iteration 45/500
start Current memory usage: 1199.55 MB
Training model: sequential
end Current memory usage: 1221.59 MB
Iteration 46/500
start Current memory usage: 1221.59 MB
Training model: sequential
end Current memory usage: 1249.11 MB
Iteration 47/500
start Current memory usage: 1249.22 MB
Training model: sequential
end Current memory usage: 1275.50 MB
Iteration 48/500
start Current memory usage: 1259.83 MB
Training model: sequential
end Current memory usage: 1290.91 MB
Iteration 49/500
start Current memory usage: 1285.67 MB
Training model: sequential
end Current memory usage: 1296.75 MB
Iteration 50/500
start Current memory usage: 1296.75 MB
Training model: sequential
end Current memory usage: 1306.59 MB
Iteration 51/500
start Current memory usage: 1306.59 MB
Training model: sequential
end Current memory usage: 1287.53 MB
Iteration 52/500
start Current memory usage: 1287.53 MB
Training model: sequential
end Current memory usage: 1297.23 MB
Iteration 53/500
start Current memory usage: 1297.25 MB
Training model: sequential
end Current memory usage: 1285.45 MB
Iteration 54/500
start Current memory usage: 1285.45 MB
Training model: sequential
end Current memory usage: 1290.36 MB
Iteration 55/500
start Current memory usage: 1282.14 MB
Training model: sequential
end Current memory usage: 1302.14 MB
Iteration 56/500
start Current memory usage: 1302.14 MB
Training model: sequential
end Current memory usage: 1287.70 MB
Iteration 57/500
start Current memory usage: 1287.75 MB
Training model: sequential
end Current memory usage: 1282.77 MB
Iteration 58/500
start Current memory usage: 1271.38 MB
Training model: sequential
end Current memory usage: 1232.14 MB
Iteration 59/500
start Current memory usage: 1212.70 MB
Training model: sequential
end Current memory usage: 1201.16 MB
Iteration 60/500
start Current memory usage: 1200.53 MB
Training model: sequential
end Current memory usage: 1169.45 MB
Iteration 61/500
start Current memory usage: 1169.45 MB
Training model: sequential
end Current memory usage: 1209.73 MB
Iteration 62/500
start Current memory usage: 1207.19 MB
Training model: sequential
end Current memory usage: 1226.28 MB
Iteration 63/500
start Current memory usage: 1226.28 MB
Training model: sequential
end Current memory usage: 1231.45 MB
Iteration 64/500
start Current memory usage: 1210.11 MB
Training model: sequential
end Current memory usage: 1176.00 MB
Iteration 65/500
start Current memory usage: 1173.97 MB
Training model: sequential
end Current memory usage: 1201.42 MB
Iteration 66/500
start Current memory usage: 1201.42 MB
Training model: sequential
end Current memory usage: 1223.94 MB
Iteration 67/500
start Current memory usage: 1222.50 MB
Training model: sequential
end Current memory usage: 1229.80 MB
Iteration 68/500
start Current memory usage: 1227.14 MB
Training model: sequential
end Current memory usage: 1219.02 MB
Iteration 69/500
start Current memory usage: 1210.48 MB
Training model: sequential
end Current memory usage: 1247.17 MB
Iteration 70/500
start Current memory usage: 1245.94 MB
Training model: sequential
end Current memory usage: 1259.84 MB
Iteration 71/500
start Current memory usage: 1259.86 MB
Training model: sequential
end Current memory usage: 1286.39 MB
Iteration 72/500
start Current memory usage: 1286.53 MB
Training model: sequential
end Current memory usage: 1316.52 MB
Iteration 73/500
start Current memory usage: 1311.53 MB
Training model: sequential
end Current memory usage: 1338.72 MB
Iteration 74/500
start Current memory usage: 1338.75 MB
Training model: sequential
end Current memory usage: 1348.45 MB
Iteration 75/500
start Current memory usage: 1338.30 MB
Training model: sequential
end Current memory usage: 1354.97 MB
Iteration 76/500
start Current memory usage: 1353.83 MB
Training model: sequential
end Current memory usage: 1385.67 MB
Iteration 77/500
start Current memory usage: 1385.69 MB
Training model: sequential
end Current memory usage: 1408.83 MB
Iteration 78/500
start Current memory usage: 1408.88 MB
Training model: sequential
end Current memory usage: 1430.91 MB
Iteration 79/500
start Current memory usage: 1430.94 MB
Training model: sequential
end Current memory usage: 1443.62 MB
Iteration 80/500
start Current memory usage: 1428.00 MB
Training model: sequential
end Current memory usage: 1436.50 MB
Iteration 81/500
start Current memory usage: 1436.64 MB
Training model: sequential
end Current memory usage: 1454.66 MB
Iteration 82/500
start Current memory usage: 1440.91 MB
Training model: sequential
end Current memory usage: 1461.81 MB
Iteration 83/500
start Current memory usage: 1460.47 MB
Training model: sequential
end Current memory usage: 1481.19 MB
Iteration 84/500
start Current memory usage: 1481.19 MB
Training model: sequential
end Current memory usage: 1477.84 MB
Iteration 85/500
start Current memory usage: 1477.84 MB
Training model: sequential
end Current memory usage: 1493.55 MB
Iteration 86/500
start Current memory usage: 1493.58 MB
Training model: sequential
end Current memory usage: 1509.50 MB
Iteration 87/500
start Current memory usage: 1509.50 MB
Training model: sequential
end Current memory usage: 1543.94 MB
Iteration 88/500
start Current memory usage: 1542.83 MB
Training model: sequential
end Current memory usage: 1516.17 MB
Iteration 89/500
start Current memory usage: 1516.20 MB
Training model: sequential
end Current memory usage: 1470.17 MB
Iteration 90/500
start Current memory usage: 1470.22 MB
Training model: sequential
end Current memory usage: 1443.72 MB
Iteration 91/500
start Current memory usage: 1444.36 MB
Training model: sequential
end Current memory usage: 1486.23 MB
Iteration 92/500
start Current memory usage: 1476.41 MB
Training model: sequential
end Current memory usage: 1524.97 MB
Iteration 93/500
start Current memory usage: 1524.97 MB
Training model: sequential
end Current memory usage: 1534.94 MB
Iteration 94/500
start Current memory usage: 1551.98 MB
Training model: sequential
end Current memory usage: 1853.48 MB
Iteration 95/500
start Current memory usage: 1853.48 MB
Training model: sequential
end Current memory usage: 1790.12 MB
Iteration 96/500
start Current memory usage: 1792.27 MB
Training model: sequential
end Current memory usage: 1883.20 MB
Iteration 97/500
start Current memory usage: 1879.05 MB
Training model: sequential
end Current memory usage: 1759.69 MB
Iteration 98/500
start Current memory usage: 1669.66 MB
Training model: sequential
end Current memory usage: 1596.77 MB
Iteration 99/500
start Current memory usage: 1597.12 MB
Training model: sequential
end Current memory usage: 1568.83 MB
Iteration 100/500
start Current memory usage: 1532.98 MB
Training model: sequential
end Current memory usage: 1516.75 MB
Iteration 101/500
start Current memory usage: 1465.98 MB
Training model: sequential
end Current memory usage: 1486.66 MB
Iteration 102/500
start Current memory usage: 1483.34 MB
Training model: sequential
end Current memory usage: 1523.19 MB
Iteration 103/500
start Current memory usage: 1523.14 MB
Training model: sequential
end Current memory usage: 1532.77 MB
Iteration 104/500
start Current memory usage: 1531.14 MB
Training model: sequential
end Current memory usage: 1561.78 MB
Iteration 105/500
start Current memory usage: 1555.67 MB
Training model: sequential
end Current memory usage: 1586.70 MB
Iteration 106/500
start Current memory usage: 1586.75 MB
Training model: sequential
end Current memory usage: 1608.41 MB
Iteration 107/500
start Current memory usage: 1603.81 MB
Training model: sequential
end Current memory usage: 1629.00 MB
Iteration 108/500
start Current memory usage: 1629.05 MB
Training model: sequential
end Current memory usage: 1609.25 MB
Iteration 109/500
start Current memory usage: 1609.31 MB
Training model: sequential
end Current memory usage: 1630.09 MB
Iteration 110/500
start Current memory usage: 1629.20 MB
Training model: sequential
end Current memory usage: 1638.66 MB
Iteration 111/500
start Current memory usage: 1620.30 MB
Training model: sequential
end Current memory usage: 1642.81 MB
Iteration 112/500
start Current memory usage: 1642.94 MB
Training model: sequential
end Current memory usage: 1659.45 MB
Iteration 113/500
start Current memory usage: 1655.17 MB
Training model: sequential
end Current memory usage: 1687.80 MB
Iteration 114/500
start Current memory usage: 1673.33 MB
Training model: sequential
end Current memory usage: 1705.94 MB
Iteration 115/500
start Current memory usage: 1699.95 MB
Training model: sequential
end Current memory usage: 1708.22 MB
Iteration 116/500
start Current memory usage: 1707.88 MB
Training model: sequential
end Current memory usage: 1648.23 MB
Iteration 117/500
start Current memory usage: 1634.03 MB
Training model: sequential
end Current memory usage: 1670.97 MB
Iteration 118/500
start Current memory usage: 1671.97 MB
Training model: sequential
end Current memory usage: 1649.69 MB
Iteration 119/500
start Current memory usage: 1645.14 MB
Training model: sequential
end Current memory usage: 1698.64 MB
Iteration 120/500
start Current memory usage: 1699.69 MB
Training model: sequential
end Current memory usage: 1737.67 MB
Iteration 121/500
start Current memory usage: 1737.67 MB
Training model: sequential
end Current memory usage: 1738.05 MB
Iteration 122/500
start Current memory usage: 1721.47 MB
Training model: sequential
end Current memory usage: 1730.64 MB
Iteration 123/500
start Current memory usage: 1729.53 MB
Training model: sequential
end Current memory usage: 1766.12 MB
Iteration 124/500
start Current memory usage: 1761.22 MB
Training model: sequential
end Current memory usage: 1796.58 MB
Iteration 125/500
start Current memory usage: 1796.73 MB
Training model: sequential
end Current memory usage: 1709.02 MB
Iteration 126/500
start Current memory usage: 1721.67 MB
Training model: sequential
end Current memory usage: 1771.50 MB
Iteration 127/500
start Current memory usage: 1771.50 MB
Training model: sequential
end Current memory usage: 1777.38 MB
Iteration 128/500
start Current memory usage: 1757.58 MB
Training model: sequential
end Current memory usage: 1806.50 MB
Iteration 129/500
start Current memory usage: 1758.81 MB
Training model: sequential
end Current memory usage: 1812.45 MB
Iteration 130/500
start Current memory usage: 1812.86 MB
Training model: sequential
end Current memory usage: 1811.14 MB
Iteration 131/500
start Current memory usage: 1799.61 MB
Training model: sequential
end Current memory usage: 1835.33 MB
Iteration 132/500
start Current memory usage: 1716.38 MB
Training model: sequential
end Current memory usage: 1759.75 MB
Iteration 133/500
start Current memory usage: 1752.44 MB
Training model: sequential
end Current memory usage: 1818.41 MB
Iteration 134/500
start Current memory usage: 1811.42 MB
Training model: sequential
end Current memory usage: 1853.58 MB
Iteration 135/500
start Current memory usage: 1853.70 MB
Training model: sequential
end Current memory usage: 1858.50 MB
Iteration 136/500
start Current memory usage: 1858.56 MB
Training model: sequential
end Current memory usage: 1874.84 MB
Iteration 137/500
start Current memory usage: 1862.92 MB
Training model: sequential
end Current memory usage: 1768.23 MB
Iteration 138/500
start Current memory usage: 1762.73 MB
Training model: sequential
end Current memory usage: 1843.39 MB
Iteration 139/500
start Current memory usage: 1843.52 MB
Training model: sequential
end Current memory usage: 1885.88 MB
Iteration 140/500
start Current memory usage: 1885.95 MB
Training model: sequential
end Current memory usage: 1924.86 MB
Iteration 141/500
start Current memory usage: 1925.05 MB
Training model: sequential
end Current memory usage: 1946.80 MB
Iteration 142/500
start Current memory usage: 1946.69 MB
Training model: sequential
end Current memory usage: 1977.53 MB
Iteration 143/500
start Current memory usage: 1974.27 MB
Training model: sequential
end Current memory usage: 1995.17 MB
Iteration 144/500
start Current memory usage: 1992.41 MB
Training model: sequential
end Current memory usage: 1984.45 MB
Iteration 145/500
start Current memory usage: 1963.42 MB
Training model: sequential
end Current memory usage: 1947.31 MB
Iteration 146/500
start Current memory usage: 1944.47 MB
Training model: sequential
end Current memory usage: 1996.00 MB
Iteration 147/500
start Current memory usage: 1996.08 MB
Training model: sequential
end Current memory usage: 2008.41 MB
Iteration 148/500
start Current memory usage: 1999.69 MB
Training model: sequential
end Current memory usage: 1951.30 MB
Iteration 149/500
start Current memory usage: 1942.98 MB
Training model: sequential
end Current memory usage: 1992.28 MB
Iteration 150/500
start Current memory usage: 1982.86 MB
Training model: sequential
end Current memory usage: 2008.83 MB
Iteration 151/500
start Current memory usage: 2008.83 MB
Training model: sequential
end Current memory usage: 1946.42 MB
Iteration 152/500
start Current memory usage: 1946.92 MB
Training model: sequential
end Current memory usage: 1992.48 MB
Iteration 153/500
start Current memory usage: 1979.52 MB
Training model: sequential
end Current memory usage: 2035.66 MB
Iteration 154/500
start Current memory usage: 2023.91 MB
Training model: sequential
end Current memory usage: 2030.31 MB
Iteration 155/500
start Current memory usage: 1974.39 MB
Training model: sequential
end Current memory usage: 2029.30 MB
Iteration 156/500
start Current memory usage: 1997.42 MB
Training model: sequential
end Current memory usage: 2000.31 MB
Iteration 157/500
start Current memory usage: 1964.38 MB
Training model: sequential
end Current memory usage: 1979.45 MB
Iteration 158/500
start Current memory usage: 1973.12 MB
Training model: sequential
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF 2.18']",2024-11-25T12:12:55Z,7,1,https://github.com/tensorflow/tensorflow/issues/80753
tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.SparseConcat`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, `SparseConcat` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

indices1 = tf.constant(2, shape=[3,3], dtype=tf.int64)
values1 = tf.constant(""aaaabaaacaaadaaaeaaafaaagaaahaaaiaaajaaakaaalaaamaaanaaaoaaapaaaqaaaraaasaaataaauaaavaaawaaaxaaayaaazaabbaabcaabdaabeaabfaabgaabhaabiaabjaabkaablaabmaabnaaboaabpaabqaabraabsaabtaabuaabvaabwaabxaabyaabzaacbaaccaacdaaceaacfaacgaachaaciaacjaackaaclaacmaacnaacoaacpaacqaacraacsaactaacuaacvaacwaacxaacyaac"",
    shape=[3], dtype=tf.string)
shapes1 = tf.constant([5, 2, 2147483647], dtype=tf.int64)

indices2 = tf.constant(-2, shape=[4,3], dtype=tf.int64)
values2 = tf.constant("" "", shape=[4], dtype=tf.string)
shapes2 = tf.constant([5,1879048192,536870912], dtype=tf.int64)

concat_dim = 1
tf.raw_ops.SparseConcat(
    indices=[indices1, indices2], values=[values1, values2], shapes=[shapes1, shapes2], concat_dim=concat_dim, name=None
)
```


### Relevant log output

```shell
2024-11-24 06:36:10.994508: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements() status: INVALID_ARGUMENT: Shape [5,1879048194,2147483647] results in overflow when computing number of elements
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-24T06:40:09Z,3,0,https://github.com/tensorflow/tensorflow/issues/80669
tensorflow/tensorflow,Floating point exception (core dumped) in `tf.raw_ops.Reshape`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs,  `tf.raw_ops.Reshape` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tensor = tf.constant(-3.5e+35, shape=[5], dtype=tf.float32)
shape = tf.constant([0, 1879048192, 100000000, 1610612736, -1], dtype=tf.int32)

tf.raw_ops.Reshape(tensor=tensor, shape=shape)
```


### Relevant log output

```shell
Floating point exception (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-22T02:58:01Z,2,0,https://github.com/tensorflow/tensorflow/issues/80529
tensorflow/tensorflow,The documentation in `data_performance.ipynb` uses `py_function()` without an explanation,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.18

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the [""Better performance with the tf.data API"" guide](https://www.tensorflow.org/guide/data_performance), `tf.py_function()` is used several times in the mapping function. There are various performance boosts demonstrated by the guide with said mapping functions. However, there does not seem to be any practical reason why `tf.py_function()` is used inside the mapping functions. In fact, if you remove those, the behavior is the same; in other words, there doesn't seem to be a need for them at all.

Curiously, if you remove them from the examples and then perform the time measurements, the speedup goes away. For example, consider the following mapping function from [the guide](https://github.com/tensorflow/docs/blob/bbc0b9c70fc0bd4411793d1b0bcc56ef1dbc2405/site/en/guide/data_performance.ipynb#L447-L450):

```python
def mapped_function(s):
    # Do some hard pre-processing
    tf.py_function(lambda: time.sleep(0.03), [], ())
    return s
```

In the sequential case, it is shown that the following mapping code results in a time measurement of 0.49222556600034295:

```python
benchmark(
    ArtificialDataset()
    .map(mapped_function)
)
```

And, the optimized (parallel) version as follows results in a time measurement of 0.36392719900049997:

```python
benchmark(
    ArtificialDataset()
    .map(
        mapped_function,
        num_parallel_calls=tf.data.AUTOTUNE
    )
)
```

But, if I remove the `tf.py_function()` from the mapping function, I get comparable measurements from both examples, namely, 0.22448736599994845 and 0.2266392660001202:

```python
def mapped_function(s):
    # Do some hard pre-processing
    lambda: time.sleep(0.03), [], ()
    return s
```

In fact, that measurement is even better, which makes me believe that this example is contrived to show a performance benefit by using `num_parallel_calls` when in fact TF is already optimizing the code without it. Frivolously wrapping the function in `tf.py_function()` is most likely  causing TensorFlow *not* to optimize the function. Thus, is `num_parallel_calls` even needed to achieve better performance?

### Standalone code to reproduce the issue

```shell
Mentioned in the above description.
```


### Relevant log output

_No response_","['type:docs-bug', 'stat:awaiting tensorflower', 'type:bug']",2024-11-20T16:01:24Z,3,0,https://github.com/tensorflow/tensorflow/issues/80365
tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.MatrixSolve`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the matrix argument is empty and the gpu is available, tf.raw_ops.MatrixSolve triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.MatrixSolve(matrix=tf.random.uniform([], dtype=tf.dtypes.double, maxval=1000000000), rhs=tf.random.uniform([1, 2], dtype=tf.dtypes.double, maxval=1000000000), adjoint=True)
```


### Relevant log output

```shell
2024-11-20 14:51:08.714846: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 14:51:08.775383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 14:51:08.852267: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 14:51:08.876168: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 14:51:08.931206: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 14:51:16.385650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 14:51:16.387914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 14:51:16.542383: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-20T06:53:21Z,2,0,https://github.com/tensorflow/tensorflow/issues/80331
