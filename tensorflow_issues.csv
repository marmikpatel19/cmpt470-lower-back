repository,title,description,labels,created_at,comments,reactions,url
tensorflow/tensorflow,`gradient_checker.compute_gradient` can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `Floating point exception` issue in TensorFlow when I used API `gradient_checker.compute_gradient`. I have confirmed that below code would crash on tf-nightly 2.19.0-dev20241025 (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1Ow6DQI7g-s7LpKUM1fy8OXbNcfw1ZH7r?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import math

from absl.testing import parameterized
import numpy as np
import tensorflow as tf
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import errors_impl
from tensorflow.python.framework import tensor_shape
from tensorflow.python.framework import test_util
from tensorflow.python.ops import gen_nn_ops
from tensorflow.python.ops import gradient_checker
from tensorflow.python.ops import gradients_impl
from tensorflow.python.ops import nn_ops
import tensorflow.python.ops.nn_grad  # pylint: disable=unused-import
from tensorflow.python.platform import test
from tensorflow.python.platform import tf_logging
from tensorflow.python.util.compat import collections_abc
from tensorflow.python.eager import context
def DtypesToTest(use_gpu):
  # double datatype is currently not supported for convolution ops
  # on the ROCm platform
  optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]
  if use_gpu:
    if not test_util.GpuSupportsHalfMatMulAndConv():
      return optional_float64 + [dtypes.float32]
    else:
      # It is important that float32 comes before float16 here,
      # as we will be using its gradients as reference for fp16 gradients.
      return optional_float64 + [dtypes.float32, dtypes.float16]
  else:
    return optional_float64 + [dtypes.float32, dtypes.float16, dtypes.bfloat16]
def _ConstructAndTestGradientForConfig(
    batch, input_shape, filter_shape, in_depth, out_depth, stride,
    padding, test_input, data_format, use_gpu):
  input_planes, input_rows, input_cols = input_shape
  filter_planes, filter_rows, filter_cols = filter_shape
  input_shape = [batch, input_planes, input_rows, input_cols, in_depth]
  filter_shape = [
      filter_planes, filter_rows, filter_cols, in_depth, out_depth
  ]
  if isinstance(stride, collections_abc.Iterable):
    strides = [1] + list(stride) + [1]
  else:
    strides = [1, stride, stride, stride, 1]
  if padding == ""VALID"":
    output_planes = int(
        math.ceil((input_planes - filter_planes + 1.0) / strides[1]))
    output_rows = int(
        math.ceil((input_rows - filter_rows + 1.0) / strides[2]))
    output_cols = int(
        math.ceil((input_cols - filter_cols + 1.0) / strides[3]))
  else:
    output_planes = int(math.ceil(float(input_planes) / strides[1]))
    output_rows = int(math.ceil(float(input_rows) / strides[2]))
    output_cols = int(math.ceil(float(input_cols) / strides[3]))
  output_shape = [batch, output_planes, output_rows, output_cols, out_depth]
  input_size = 1
  for x in input_shape:
    input_size *= x
  filter_size = 1
  for x in filter_shape:
    filter_size *= x
  input_data = [x * 1.0 / input_size for x in range(0, input_size)]
  filter_data = [x * 1.0 / filter_size for x in range(0, filter_size)]
  for data_type in DtypesToTest(use_gpu=use_gpu):
    # TODO(mjanusz): Modify gradient_checker to also provide max relative
    # error and synchronize the tolerance levels between the tests for forward
    # and backward computations.
    if data_type == dtypes.float64:
      tolerance = 1e-8
    elif data_type == dtypes.float32:
      tolerance = 5e-3
    elif data_type == dtypes.float16:
      tolerance = 5e-3 if test.is_built_with_rocm() else 1e-3
    elif data_type == dtypes.bfloat16:
      tolerance = 1e-2
    sess = tf.compat.v1.Session()
    with sess.as_default():
      orig_input_tensor = constant_op.constant(
          input_data, shape=input_shape, dtype=data_type, name=""input"")
      filter_tensor = constant_op.constant(
          filter_data, shape=filter_shape, dtype=data_type, name=""filter"")
      if data_format == ""NCDHW"":
        input_tensor = test_util.NHWCToNCHW(orig_input_tensor)
        new_strides = test_util.NHWCToNCHW(strides)
      else:
        input_tensor = orig_input_tensor
        new_strides = strides
      conv = nn_ops.conv3d(
          input_tensor,
          filter_tensor,
          new_strides,
          padding,
          data_format=data_format,
          name=""conv"")
      jacob_t, jacob_n = gradient_checker.compute_gradient(
          orig_input_tensor, input_shape, conv, output_shape)

with context.graph_mode():
  _ConstructAndTestGradientForConfig(data_format=""NDHWC"",use_gpu=False,batch=2, input_shape=(3, 7, 6), filter_shape=(3, 3, 3), in_depth=2, out_depth=0, stride=3, padding='VALID', test_input=True)
```

### Relevant log output

```shell
Fatal Python error: Floating point exception
```","['type:bug', 'type:support']",2025-02-11T16:38:56Z,0,0,https://github.com/tensorflow/tensorflow/issues/87063
tensorflow/tensorflow,TPU nan issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux (google colab default)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the 2.18.0 Tensorflow version, occur only when using TPU:
- At the middle of any epoch, loss turns out to be nan for the rest of the epoch 

### Standalone code to reproduce the issue

```shell
Shortest way: connect and connect to the colab tutorial on TPU, i.e. https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/tpu.ipynb#scrollTo=Tce3stUlHN0L
If the issue hasn't been fixed, the training loop will produce nan loss
```

### Relevant log output

```shell
Epoch 1/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 22s 47ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 2/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 3/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 4/5
231/300 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: nan - sparse_categorical_accuracy: nan
```","['type:bug', 'comp:tpus', 'TF 2.18']",2025-02-10T15:03:57Z,2,0,https://github.com/tensorflow/tensorflow/issues/86953
tensorflow/tensorflow,"`tf.summary_ops.write` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.write` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/13pz-hZNK_BaOnbYo_9CT2YMdoncPXEA3?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import summary_ops_v2 as summary_ops
from tensorflow.python.ops import variables
writer = summary_ops.create_file_writer_v2(""/tmp"")
mystep = variables.Variable(1, dtype=dtypes.int64)
with writer.as_default(step=[3, 0, 0, 2]):
    summary_ops.write('tag', 1.0)
```

### Relevant log output

```shell
2025-02-09 04:47:35.482562: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor
Aborted (core dumped)
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-09T04:49:39Z,1,0,https://github.com/tensorflow/tensorflow/issues/86918
tensorflow/tensorflow,"`tf.summary_ops.run_metadata_graphs` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.run_metadata_graphs` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1qCZb2sPj2l79IecGA1tz9CywtNlevAjf?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.ops import summary_ops_v2 as summary_ops
from tensorflow.core.protobuf import config_pb2
writer = summary_ops.create_file_writer_v2(""/tmp"")
meta = config_pb2.RunMetadata()
with writer.as_default([3, 0, 0, 2]):
    summary_ops.run_metadata_graphs(name='my_name', data=meta)
```

### Relevant log output

```shell
2025-02-09 04:39:36.666278: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor
Aborted (core dumped)
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-09T04:42:49Z,1,0,https://github.com/tensorflow/tensorflow/issues/86917
tensorflow/tensorflow,"`io_ops.restore_v2` aborts with ""Check failed: size >= 0 (0 vs. -3) ""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.io_ops.restore_v2` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20250207` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/17XwNF4WI3HqVJkLwk2VQjcTDAV42-6Bl?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.ops import io_ops

dtype = dtypes.uint4
with ops.Graph().as_default():
    op = io_ops.restore_v2('model', ['var1', 'var2'], ['', '-3 4 0,1:-'], [dtype, dtype])
```

### Relevant log output

```shell
2025-02-09 04:25:19.857968: F tensorflow/core/framework/tensor_shape.cc:413] Check failed: size >= 0 (0 vs. -3) 
Aborted (core dumped)
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-09T04:34:07Z,1,0,https://github.com/tensorflow/tensorflow/issues/86916
tensorflow/tensorflow,Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Macos 15.3 (worker 0) Macos 12.7.6(worker 1)

### Mobile device

_No response_

### Python version

3.8.20

### Bazel version

...

### GCC/compiler version

16.0.0 (apple M3) 14.0.0 (intel iris)

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M3 and Intel Iris Graphics 6100

### Current behavior?

When I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  #Build the model under the strategy

No error message is displayed, but the process no longer progresses after

•	 I followed the recommendations of the official documentation, but the problem persists.

### Standalone code to reproduce the issue

```shell
import json
import os
import numpy as np
import tensorflow as tf

TF_CONFIG = {
    ""cluster"": {
        ""worker"": [""192.168.0.68:12345"", ""192.168.0.68:12346""]
    },
    ""task"": {""type"": ""worker"", ""index"": 0}  # Modifier index pour chaque worker
}

os.environ[""TF_CONFIG""] = json.dumps(TF_CONFIG)

# Manually Load the MNIST dataset
data = np.load(""mnist.npz"")
x_train, y_train = data[""x_train""], data[""y_train""]
x_test, y_test = data[""x_test""], data[""y_test""]

# Normalize images
x_train, x_test = x_train / 255.0, x_test / 255.0

# Add a dimension to match TensorFlow's expectations
x_train = x_train[..., np.newaxis]
x_test = x_test[..., np.newaxis]

# Define the distribution strategy
strategy = tf.distribute.MultiWorkerMirroredStrategy()

# Build the model under the strategy
with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))

test_loss, test_acc = model.evaluate(x_test, y_test)
print(f""Test accuracy: {test_acc:.4f}"")
```

### Relevant log output

```shell
2025-02-08 12:29:20.630247: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3
2025-02-08 12:29:20.630286: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB
2025-02-08 12:29:20.630293: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB
2025-02-08 12:29:20.630324: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-02-08 12:29:20.630338: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-02-08 12:29:20.631249: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-02-08 12:29:20.631259: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-02-08 12:29:20.632347: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:449] Started server with target: grpc://192.168.0.68:12345
2025-02-08 12:29:20.637550: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 14287335759644278642
2025-02-08 12:29:20.637654: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:298] Coordination agent has successfully connected.
2025-02-08 12:29:37.728182: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 15227140312468372989
```","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.13']",2025-02-08T11:40:33Z,1,0,https://github.com/tensorflow/tensorflow/issues/86897
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.Rsqrt``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ``tf.raw_ops.Rsqrt``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x_0 = tf.constant([[[1.2031, 0.2168], [0.1177, 0.6758]], 
                    [[1.8203, 0.9688], [0.8828, 0.0767]]], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.Rsqrt(x=x_0)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.Rsqrt(x=x_0)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor(
[[[0.910156 2.14062]
  [2.92188 1.21875]]

 [[0.742188 1.01562]
  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)

Output on GPU: tf.Tensor(
[[[0.914062 2.15625]
  [2.90625 1.21875]]

 [[0.738281 1.01562]
  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-05T06:12:29Z,1,0,https://github.com/tensorflow/tensorflow/issues/86607
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.Tan``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

```tf.raw_ops.Tan``` produces inconsistent results between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

real = tf.constant([1.5634333], dtype=tf.float32)
imag = tf.constant([0.020735], dtype=tf.float32)

complex_tensor = tf.complex(real, imag)

with tf.device('/CPU:0'):
    result_cpu = tf.raw_ops.Tan(x=complex_tensor)
    print(result_cpu)

with tf.device('/GPU:0'):
    result_gpu = tf.raw_ops.Tan(x=complex_tensor)
    print(result_gpu)

##Comparing whole complex numbers
max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_cons = tf.experimental.numpy.allclose(result_cpu, result_gpu, rtol=1e-6,  atol=1e-5)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:"", is_cons.numpy())

##Comparing by parts
real_part_cpu = tf.math.real(result_cpu)
real_part_gpu = tf.math.real(result_gpu)
real_part_diff = tf.reduce_max(tf.abs(real_part_cpu - real_part_gpu)).numpy()
real_part_cons = tf.experimental.numpy.allclose(real_part_cpu, real_part_gpu, rtol=1e-6,  atol=1e-5)

imag_part_cpu = tf.math.imag(result_cpu)
imag_part_gpu = tf.math.imag(result_gpu)
imag_part_diff = tf.reduce_max(tf.abs(imag_part_cpu - imag_part_gpu)).numpy()
imag_part_cons = tf.experimental.numpy.allclose(imag_part_cpu, imag_part_gpu, rtol=1e-6,  atol=1e-5)

print(""Real parts absolute difference:"", real_part_diff)
print(""Real parts Consistency check with atol=1e-5 and rtol=1e-6:"", real_part_cons.numpy())

print(""Imag parts absolute difference:"", imag_part_diff)
print(""Imag parts Consistency check with atol=1e-5 and rtol=1e-6:"", imag_part_cons.numpy())
```

### Relevant log output

```shell
tf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64)
tf.Tensor([15.205605+42.834225j], shape=(1,), dtype=complex64)

Max absolute difference: 8.5064334e-05
Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False

Real parts absolute difference: 2.861023e-05
Real parts Consistency check with atol=1e-5 and rtol=1e-6: False

Imag parts absolute difference: 8.010864e-05
Imag parts Consistency check with atol=1e-5 and rtol=1e-6: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-04T03:18:15Z,2,0,https://github.com/tensorflow/tensorflow/issues/86506
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.LogSoftmax``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ``tf.raw_ops.LogSoftmax``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

logits = tf.constant([[0.0664, -2.3906]], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.LogSoftmax(logits=logits)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.LogSoftmax(logits=logits)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor([[-0.0825195 -2.53125]], shape=(1, 2), dtype=bfloat16)

Output on GPU: tf.Tensor([[-0.0825195 -2.54688]], shape=(1, 2), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-03T03:30:58Z,1,0,https://github.com/tensorflow/tensorflow/issues/86434
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.L2Loss``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ```tw.raw_ops.L2Loss``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

t = tf.constant([
    [[0.9922, -1.4922], 
     [0.0376,  0.1504], 
     [0.6172,  1.2266]],

    [[-0.1387,  1.3047], 
     [0.3535, -0.0471], 
     [0.0437,  0.2637]]
], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.L2Loss(t=t)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.L2Loss(t=t)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor(3.51562, shape=(), dtype=bfloat16)

Output on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-02T07:05:17Z,2,0,https://github.com/tensorflow/tensorflow/issues/86406
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.BiasAddGrad``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

```tf.raw_ops.BiasAddGrad``` produces inconsistent results between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

out_backprop = tf.constant([
    [
        [
            [[ 0.2207,  2.1094], [-0.3730, -1.0625], [ 1.7031,  0.7148]], 
            [[ 1.5078, -0.6719], [-0.6367,  0.5039], [-2.3281,  0.5078]]
        ],
        [
            [[-0.3574,  0.0461], [ 2.3750, -2.9688], [-0.5703, -2.0156]],
            [[ 0.8125,  1.7656], [-0.9570,  0.6250], [-0.6914, -0.4746]]
        ],
        [
            [[-0.3750, -0.7383], [ 0.3691,  0.4570], [ 1.1641,  0.2715]],
            [[-1.2969, -0.9844], [-0.4863,  1.0938], [-1.4297,  0.8086]]
        ]
    ],
    [
        [
            [[ 0.3730,  0.8477], [-0.3887,  1.2266], [ 0.0859, -0.5742]],
            [[-0.7383, -0.2432], [-0.7578, -0.8281], [-0.1660, -0.9336]]
        ],
        [
            [[ 1.4297,  0.6797], [-1.6172,  0.4941], [-0.3047, -0.3711]],
            [[-0.6250, -0.7617], [ 0.9453,  0.1064], [ 1.4062, -2.9531]]
        ],
        [
            [[-1.4297, -0.1387], [ 0.0625,  1.0469], [-0.1953,  1.6406]],
            [[-0.3047,  0.5117], [ 1.8125,  1.1797], [-0.8789, -0.4688]]
        ]
    ]
], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")
  print(""BiasAddGrad Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")
  print(""BiasAddGrad Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
BiasAddGrad Output on CPU: tf.Tensor([0.09375 -3.96875 1.70312], shape=(3,), dtype=bfloat16)

BiasAddGrad Output on GPU: tf.Tensor([0.078125 -4 1.70312], shape=(3,), dtype=bfloat16)

Max absolute difference: 0.03125
Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2025-02-01T10:43:06Z,5,0,https://github.com/tensorflow/tensorflow/issues/86378
tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.BatchMatMulV2``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

getting inconsistent results of ```tf.raw_ops.BatchMatMulV2``` between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x_0 = tf.constant([
    [[[[ -1.3594, -0.3027], [-1.4141,  0.2969]],
      [[ -0.9141,  1.7812], [ 1.2266,  0.8594]]],

     [[[  0.8359, -0.9414], [-1.7969, -0.7461]],
      [[  0.3164,  0.3691], [ 0.7656,  0.2354]]]],

    [[[[ -0.5898,  1.3516], [ 0.4902, -0.1045]],
      [[ -0.1099,  1.5078], [ 0.2852, -0.0957]]],

     [[[-0.9883,  1.3203], [-0.2715, -1.7578]],
      [[ -0.1602, -0.4336], [-0.6875, -0.4492]]]]
], dtype=tf.bfloat16)

y = tf.constant([
    [[[  0.6836, -0.6562], [-0.5508, -0.8438]], 
     [[  1.6094, -0.9883], [-0.1318,  1.1094]]],

    [[[  0.4062, -1.1094], [-0.7188, -1.7578]], 
     [[ -1.0391, -0.6602], [ 0.8359, -0.6562]]]
], dtype=tf.bfloat16) 

with tf.device('CPU:0'):
    result_cpu = tf.raw_ops.BatchMatMulV2(
        x=x_0, 
        y=y,
    )
    print(result_cpu)

with tf.device('GPU:0'):
    result_gpu = tf.raw_ops.BatchMatMulV2(
        x=x_0, 
        y=y,
    )
    print(result_gpu)


max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
tf.Tensor(
[[[[[-0.761719 1.14062]
    [-1.125 0.675781]]

   [[-1.70312 2.875]
    [1.85938 -0.257812]]]


  [[[1.01562 0.726562]
    [-0.193359 3.29688]]

   [[-0.0201416 -0.449219]
    [-0.597656 -0.65625]]]]



 [[[[-1.14062 -0.75]
    [0.392578 -0.233398]]

   [[-0.375 1.78125]
    [0.470703 -0.386719]]]


  [[[-1.34375 -1.21875]
    [1.14844 3.39062]]

   [[-0.195312 0.388672]
    [0.337891 0.746094]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)

tf.Tensor(
[[[[[-0.761719 1.14844]
    [-1.13281 0.675781]]

   [[-1.70312 2.875]
    [1.85938 -0.259766]]]


  [[[1.01562 0.726562]
    [-0.193359 3.3125]]

   [[-0.0201416 -0.451172]
    [-0.597656 -0.660156]]]]



 [[[[-1.14844 -0.753906]
    [0.392578 -0.233398]]

   [[-0.375 1.78125]
    [0.470703 -0.388672]]]


  [[[-1.35156 -1.22656]
    [1.15625 3.39062]]

   [[-0.196289 0.390625]
    [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)


Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-01T07:13:36Z,2,0,https://github.com/tensorflow/tensorflow/issues/86350
tensorflow/tensorflow,Stateful LSTM bug with batch size,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid. 
Basically the error is the same as this https://github.com/tensorflow/tensorflow/issues/64061

Below is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

# Set a fixed batch size
batch_size = 32

# Create some random training data
# We'll have sequences of length 5, with 1 feature per time step
sequence_length = 5
num_features = 1
num_samples = 100  # Total number of samples (must be divisible by batch_size)

# Ensure num_samples is a multiple of batch_size
num_samples = (num_samples // batch_size) * batch_size

X_train = np.random.rand(num_samples, sequence_length, num_features)
y_train = np.random.rand(num_samples, 1)  # Example target values

# Reshape y_train to match expected output shape if needed
y_train = y_train.reshape(-1,1)

# Create the stateful LSTM model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.LSTM(units=64,  # Number of LSTM units
                               batch_input_shape=(batch_size, sequence_length, num_features),
                               stateful=True,
                               return_sequences=False)) #often false for a final prediction

model.add(tf.keras.layers.Dense(units=1)) # Output layer with 1 unit

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
epochs = 10

for epoch in range(epochs):
    # Shuffle data indices for each epoch (important for stateful LSTMs)
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    X_train = X_train[indices]
    y_train = y_train[indices]

    model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False) # Shuffle must be false

    # Reset states after each epoch (essential for stateful LSTMs)
    model.reset_states()
```

### Relevant log output

```shell

```","['stat:awaiting response', 'type:bug', 'comp:keras', 'TF 2.18']",2025-01-31T18:07:07Z,5,0,https://github.com/tensorflow/tensorflow/issues/86310
tensorflow/tensorflow,inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

getting inconsistent results of ```tf.image.adjust_hue``` between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

images = tf.constant([
    [[ 1.9720840,  2.1302242, -0.1902120],
     [ 0.6557856, -1.3016001,  1.1452782]],
    
    [[-2.2193234,  0.3198028,  0.9568117],
     [-0.3937407, -0.0503466, -0.3693791]]
], dtype=tf.float32)

delta = tf.constant(-0.7441734, dtype=tf.float32)

with tf.device('CPU:0'):
    adjusted_cpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on CPU:\n"", adjusted_cpu)

with tf.device('GPU:0'):
    adjusted_gpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on GPU:\n"", adjusted_gpu)


is_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e-5, rtol=1e-6)

max_abs_diff = tf.reduce_max(tf.abs(adjusted_cpu - adjusted_gpu)).numpy()

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:"", is_consistent.numpy())
```

### Relevant log output

```shell
Adjusted Hue on CPU:
 tf.Tensor(
[[[-0.190212    2.1302242   1.2092681 ]
  [ 1.1452782  -0.48211157 -1.3016001 ]]

 [[ 0.11679006 -2.2193234   0.9568117 ]
  [-0.3937407  -0.25841027 -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Adjusted Hue on GPU:
 tf.Tensor(
[[[-0.19021209  2.1302242   1.209268  ]
  [ 1.1452781  -0.48211193 -1.3016001 ]]

 [[ 0.11678863 -2.2193234   0.95681167]
  [-0.0503466  -0.0503466  -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Max absolute difference: 0.3433941
Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-01-31T07:40:34Z,4,1,https://github.com/tensorflow/tensorflow/issues/86256
tensorflow/tensorflow,Buffer allocation error in Tensorflow Lite with OpenCL backend on certain platforms,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

macOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I noticed a memory allocation error in clCreateBuffer. The issue seems to be caused by this:

1) TFlite tries to alloca 0xa2000000 bytes of memory (value stored as size_t)

2) The call ends up in this function (tensorflow/lite/experimental/litert/runtime/opencl/buffer.cc):

```
absl::Status CreateClBuffer(cl_context context, int size_in_bytes,
                            bool read_only, void* data, cl_mem* result) 
```

where the size is now int (i.e. 32 bit signed integer... so 0xa2000000 is interpreted as a negative value).

3) This function then calls clCreateBuffer, which takes the size argument as size_t again, and thus receives 0xffffffffa2000000, i.e. the signed 32 bit integer first sign extended to 64bit and then interpreted as unsigned, and thus resulting in a huge size.

The issue doesn't seem to appear with the same model on Android, probably because: The max buffer allocation size on macOS (M1) seems to be 9GB (according to clinfo), but on that android device it's only 1GB (so on android tflite never tries to allocate such a huge chunk of memory).

### Standalone code to reproduce the issue

```shell
Unfortunately I'm not allowed to share the code/model, but looking at the function signatures one can see the issue.
```

### Relevant log output

```shell
ERROR: Failed to allocate device memory (clCreateBuffer): Invalid buffer size
```","['type:bug', 'comp:lite', 'TF 2.13']",2025-01-29T12:11:28Z,0,0,https://github.com/tensorflow/tensorflow/issues/86048
tensorflow/tensorflow,TensorFlow warning shows whenever importing it,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802

### Custom code

No

### OS platform and distribution

Linux Ubuntu 24.10 x86_64

### Mobile device

_No response_

### Python version

3.12.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA:12.6

### GPU model and memory

_No response_

### Current behavior?

`cuFFT`, `cuDNN` and `cuBLAS` warnings showing after importing `TensorFlow`

- OS: Ubuntu 24.10 x86_64
- Host: G5 5590
- Kernel: 6.11.0-13-generic
- CPU: Intel i7-9750H (12) @ 4.500GHz
- GPU: NVIDIA GeForce GTX 1650 Mobile / Max-Q
- GPU: Intel CoffeeLake-H GT2 [UHD Graphics 630]
> whenever running the following code it gives that warning also it outputs the predicted output but after the warning:
```python
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```
> output:
```
2025-01-23 21:08:06.468437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737659286.484845  763412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737659286.489647  763412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-23 21:08:06.505984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```
> also i know that for that warning (`cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`) i must rebuild the tensor-flow from binaries enabling the AVX2 and FMA instructions but what about the others?

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```

### Relevant log output

```shell

```","['type:bug', '2.18.rc']",2025-01-23T21:56:39Z,3,0,https://github.com/tensorflow/tensorflow/issues/85604
tensorflow/tensorflow,"Tutorial ""Multi-worker training with Keras"" fails to complete","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v1.12.1-120353-gc5bd67bc56f 2.19.0-dev20250107

### Custom code

No

### OS platform and distribution

Debian 6.1.123-1 (2025-01-02) x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

Python 3.12.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the tutorial everything goes well until you start the second worker. Then the below failure occures.

2025-01-20 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-20 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-20 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-20 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-01-20 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""-1""
2025-01-20 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA
2025-01-20 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-01-20 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael
2025-01-20 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael
2025-01-20 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0
2025-01-20 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0
2025-01-20 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0
2025-01-20 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345
2025-01-20 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635
2025-01-20 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect.
2025-01-20 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers:
/job:worker/replica:0/task:1
I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.
2025-01-20 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870
2025-01-20 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect.
/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
  warnings.warn(
2025-01-20 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
Traceback (most recent call last):
  File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in <module>
    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Attempt to convert a value (PerReplica:{
  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=
array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>
}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.

### Standalone code to reproduce the issue

```shell
python main.py &> job_1.log
```

### Relevant log output

```shell
2025-01-20 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-20 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-20 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-20 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-01-20 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""-1""
2025-01-20 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA
2025-01-20 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-01-20 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael
2025-01-20 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael
2025-01-20 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0
2025-01-20 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0
2025-01-20 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0
2025-01-20 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345
2025-01-20 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635
2025-01-20 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect.
2025-01-20 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers:
/job:worker/replica:0/task:1
I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.
2025-01-20 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870
2025-01-20 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect.
/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
  warnings.warn(
2025-01-20 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
Traceback (most recent call last):
  File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in <module>
    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Attempt to convert a value (PerReplica:{
  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=
array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>
}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.
```","['type:bug', 'TF 2.18']",2025-01-20T14:03:18Z,2,0,https://github.com/tensorflow/tensorflow/issues/85351
tensorflow/tensorflow,Aborted  in `tf.raw_ops.RaggedGather`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.raw_ops.RaggedGather` triggers crash.

### Standalone code to reproduce the issue

```shell
params_nested_splits = tf.constant(0, shape=[3], dtype=tf.int64)
params_dense_values = tf.constant(1, shape=[0], dtype=tf.float32)
indices = tf.constant(0, shape=[], dtype=tf.int64)
OUTPUT_RAGGED_RANK = 1
PARAMS_RAGGED_RANK = 1

tf.raw_ops.RaggedGather(
    params_nested_splits=[params_nested_splits],
    params_dense_values=params_dense_values,
    indices=indices,
    OUTPUT_RAGGED_RANK=1,
    name=None
)
```

### Relevant log output

```shell
2025-01-18 09:30:00.549762: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64
Aborted (core dumped)
```","['type:bug', 'comp:ops', '2.17']",2025-01-18T09:32:16Z,1,0,https://github.com/tensorflow/tensorflow/issues/85242
tensorflow/tensorflow,Segmentation fault (core dumped) in `RaggedTensorToTensor`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

shape = tf.constant(-1, shape=[], dtype=tf.int64)
values = tf.constant(0, shape=[0], dtype=tf.int32)
default_value = tf.constant(0, shape=[], dtype=tf.int32)
row_partition_tensors = tf.constant([0, 1, 6], shape=[3], dtype=tf.int64)
row_partition_types = [""ROW_SPLITS""]

tf.raw_ops.RaggedTensorToTensor(
    shape=shape,
    values=values,
    default_value=default_value,
    row_partition_tensors=[row_partition_tensors],
    row_partition_types=row_partition_types)
```

### Relevant log output

```shell
Segmentation fault (core dumped)
```","['type:bug', 'comp:ops', '2.17']",2025-01-18T09:27:19Z,1,0,https://github.com/tensorflow/tensorflow/issues/85240
tensorflow/tensorflow,Seg Fault when iterate dataset created from data service,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Segfault when trying to iterate dataset get from data service.

### Standalone code to reproduce the issue

```shell
# start the data service file start_dataservice.py

import tensorflow as tf

dispatcher = tf.data.experimental.service.DispatchServer(
    tf.data.experimental.service.DispatcherConfig(port=50050), start=True
)
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True
)
print(""Starting Worker"")
worker.join()

# test file test_dataset_service.py
import tensorflow as tf
import numpy as np


flags = tf.compat.v1.app.flags

flags.DEFINE_bool(""local"", False, ""Run data service in process"")
flags.DEFINE_bool(""distribute"", False, ""Run data service in distributed_epoch mode"")
FLAGS = flags.FLAGS


def local_service():
    print(""Starting Local Service"")
    dispatcher = tf.data.experimental.service.DispatchServer(
        tf.data.experimental.service.DispatcherConfig(port=50050), start=True
    )
    dispatcher_address = dispatcher.target.split(""://"")[1]
    worker = tf.data.experimental.service.WorkerServer(
        tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True
    )
    print(""Dispatcher target is "", dispatcher.target)
    return dispatcher, worker, dispatcher.target


def apply_transformations(ds_train):
    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    ds_train = ds_train.cache()
    ds_train = ds_train.shuffle(60000)
    ds_train = ds_train.batch(128)
    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)
    return ds_train


(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
x_train = x_train / np.float32(255)
y_train = y_train.astype(np.int64)
ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))


def normalize_img(image, label):
    """"""Normalizes images: `uint8` -> `float32`.""""""
    return tf.cast(image, tf.float32) / 255.0, label


ds_train = apply_transformations(ds_train)
# Create dataset however you were before using the tf.data service.
dataset = ds_train
if FLAGS.local:
    dispatcher, worker, service = local_service()
else:
    dispatcher_address = ""localhost""
    dispatcher_port = ""50050""
    service = ""grpc://{}:{}"".format(dispatcher_address, dispatcher_port)
if FLAGS.distribute:
    processing_mode = ""distributed_epoch""
else:
    processing_mode = ""parallel_epochs""

# This will register the dataset with the tf.data service cluster so that
# tf.data workers can run the dataset to produce elements. The dataset returned
# from applying `distribute` will fetch elements produced by tf.data workers.
dataset = dataset.apply(
    tf.data.experimental.service.distribute(processing_mode=processing_mode, service=service)
)

for (x1, y1), (x2, y2) in zip(dataset, ds_train):
    np.allclose(x1, x2)
    np.allclose(y1, y2)

print(""verified mnist dataset locally vs over service"")

# script to run 
python -m pip install --upgrade pip
python -m pip install tensorflow==2.18.0
python -m pip install 'protobuf<4'
screen -d -m python start_dataservice.py
python3 test_dataset_service.py --local=False
```

### Relevant log output

```shell
2025-01-14 21:56:19.778399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736891779.795141    9168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1736891779.800177    9168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-14 21:56:19.815971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
I0000 00:00:1736891783.518634    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 889 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1c.0, compute capability: 8.0
I0000 00:00:1736891783.520395    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 37945 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1d.0, compute capability: 8.0
I0000 00:00:1736891783.522012    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 37945 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1c.0, compute capability: 8.0
I0000 00:00:1736891783.523626    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 37945 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1d.0, compute capability: 8.0
I0000 00:00:1736891783.525222    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 37945 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1c.0, compute capability: 8.0
I0000 00:00:1736891783.526807    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 37945 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1d.0, compute capability: 8.0
I0000 00:00:1736891783.528377    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 37945 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1c.0, compute capability: 8.0
I0000 00:00:1736891783.529933    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 37945 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1d.0, compute capability: 8.0
/test/bin/testDataservice: line 5:  9168 Segmentation fault      (core dumped) python ${BIN_DIR}/test_dataset_service.py --local=False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-01-14T21:57:20Z,0,0,https://github.com/tensorflow/tensorflow/issues/84897
tensorflow/tensorflow,GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

nightly

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Python version

Python 3.12

### CUDA/cuDNN version

CUDA 12.4

### GPU model and memory

A100 80GB

### Current behavior?

Start a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace. 
No any memory profile events or OP profiler, but only trace view.

### Standalone code to reproduce the issue

**tf_allreduce.py**
```python
import tensorflow as tf
from tensorflow.python.ops.collective_ops import all_reduce, all_reduce_v2
from tensorflow.python.eager import context
from tensorflow.core.protobuf import config_pb2
from tensorflow.python.distribute import cluster_resolver as cluster_resolver_lib

cluster_resolver = cluster_resolver_lib.TFConfigClusterResolver()
cluster = cluster_resolver.cluster_spec()
task_type = cluster_resolver.task_type
task_id = cluster_resolver.task_id

experimental_config = config_pb2.ConfigProto.Experimental(
    share_cluster_devices_in_session=False,
    share_session_state_in_clusterspec_propagation=False
)
config = config_pb2.ConfigProto(experimental=experimental_config)
config.experimental.collective_group_leader = '/job:worker/replica:0/task:0'
server = tf.distribute.Server(cluster,
                              job_name=task_type,
                              task_index=task_id,
                              protocol=""grpc"", # ""grpc+verbs""
                              config=config)
run_options = config_pb2.RunOptions()

with tf.compat.v1.Session(target=server.target, config=config) as sess:
    tensor = tf.Variable(tf.ones([2, 2]), dtype=tf.float32)
    init = tf.compat.v1.global_variables_initializer()
    sess.run(init)
    sess.run(tf.print([""tensor:"",tensor]))

    reduced_tensor = all_reduce(tensor, group_size=2, group_key=4321, instance_key=1234, merge_op='Add', final_op='Id', communication_hint='auto')
    run_options.experimental.collective_graph_key = 6
    while True:
        sess.run(tf.print([""reduced_tensor:"",reduced_tensor]), options=run_options)
```

Run script to start server.
```bash
CUDA_VISIBLE_DEVICES=0 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":0}}' python tf_allreduce.py&
CUDA_VISIBLE_DEVICES=1 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":1}}' python tf_allreduce.py&
```

 use capture_profile in Tensorboard or tf.profiler.experimental.client.trace.
```python
tf.profiler.experimental.client.trace(
  'grpc://localhost:2223,grpc://localhost:2224',
   '/tmp/my_tb_dir',
   2000,
)
```

Try to convert xplane.pb to memory_profile, nothing show.
```python
from tensorflow.python.profiler.internal import _pywrap_profiler as profiler_wrapper
json = profiler_wrapper.xspace_to_tools_data([""xxx.xplane""], ""memory_profile"")
```

**Relevant log output**
```
{""memoryProfilePerAllocator"":{},""numHosts"":1,""memoryIds"":[]}
```

Relative issue: #48146 ","['type:bug', 'comp:gpu', 'TF 2.18']",2025-01-09T09:26:20Z,0,0,https://github.com/tensorflow/tensorflow/issues/84460
tensorflow/tensorflow,Unable to connect to TPU through Cloud VM (metadata issue?),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

tpu-ubuntu2204-base

### Mobile device

_No response_

### Python version

3.11.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am on a VM instance trying to connect to a tpu v4-32 using a test script. I installed tensorflow-tpu on both the VM (in venv) and TPU (globally) as per the instructions from the google website.

It seems like there is an issue with getting TPU metadata.

It is able to connect to the metadata server when I request manually from the VM:

```
$ curl http://169.254.169.254/computeMetadata/v1/ -H ""Metadata-Flavor: Google""
instance/
oslogin/
project/
```

Any help would be appreciated!

### Standalone code to reproduce the issue

```shell
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name)
tf.config.experimental_connect_to_cluster(resolver)
try:
    tf.tpu.experimental.initialize_tpu_system(resolver)
    print(""TPU initialized:"", resolver.master())
except Exception as e:
    print(""Failed to initialize TPU:"", e)
```


### Relevant log output

```shell
$ python hello.py
2025-01-08 23:49:33.189260: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-08 23:49:33.221197: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:95] Opening library: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
2025-01-08 23:49:33.221290: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:121] Libtpu path is: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/site-packages/libtpu/libtpu.so
Failed to get TPU metadata (tpu-env) from instance metadata for variable CHIPS_PER_HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable ALT: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable WRAP: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (accelerator-type) from instance metadata for variable TPU_ACCELERATOR_TYPE: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

Failed to find host bounds for accelerator type: WARNING: could not determine TPU accelerator type, please set env var `TPU_ACCELERATOR_TYPE` manually, otherwise libtpu.so may not properly initialize.
Failed to get TPU metadata (agent-worker-number) from instance metadata for variable TPU_WORKER_ID: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

Failed to get TPU metadata (worker-network-endpoints) from instance metadata for variable TPU_WORKER_HOSTNAMES: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

WARNING: Logging before InitGoogle() is written to STDERR
E0000 00:00:1736380405.363400    3192 common_lib.cc:511] INVALID_ARGUMENT: Error: unexpected worker hostname 'WARNING: could not determine TPU worker hostnames or IP addresses' from env var TPU_WORKER_HOSTNAMES. Expecting a valid hostname or IP address without port number. (Full TPU workers' addr string: WARNING: could not determine TPU worker hostnames or IP addresses, please set env var `TPU_WORKER_HOSTNAMES` manually, otherwise libtpu.so may not properly initialize.)
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/libtpu_init_utils.cc:173
2025-01-08 23:56:48.526584: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736380609.730442    3192 context_distributed_manager.cc:762] unknown service tensorflow.WorkerService
Additional GRPC error information from remote target /job:worker/replica:0/task:0 while calling /tensorflow.WorkerService/GetStatus:
:{""created"":""@1736380609.730372913"",""description"":""Error received from peer ipv4:10.130.0.3:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""unknown service tensorflow.WorkerService"",""grpc_status"":12}
E0108 23:56:49.730822322    3192 completion_queue.cc:244]    assertion failed: queue.num_items() == 0
https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1ccaf0e04f&map= 
*** SIGABRT received by PID 3192 (TID 3192) on cpu 4 from PID 3192; stack trace: ***
PC: @     0x7f1ccaf5cebc  (unknown)  (unknown)
    @     0x7f1caa302841       1888  (unknown)
    @     0x7f1ccaf0e050   18460496  (unknown)
    @     0x7f1ccaed1c60  (unknown)  (unknown)
https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1caa302840,7f1ccaf0e04f,7f1ccaed1c5f&map= 
E0108 23:56:49.732558    3192 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked.
E0108 23:56:49.732569    3192 coredump_hook.cc:355] RAW: Skipping coredump since rlimit was 0 at process start.
E0108 23:56:49.732575    3192 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
E0108 23:56:49.732580    3192 coredump_hook.cc:411] RAW: Sending fingerprint to remote end.
E0108 23:56:49.732595    3192 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
E0108 23:56:49.732601    3192 coredump_hook.cc:472] RAW: Dumping core locally.
E0108 23:56:49.745981    3192 process_state.cc:805] RAW: Raising signal 6 with default behavior
Aborted
```
","['type:bug', 'comp:tpus', 'TF 2.18']",2025-01-09T00:04:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/84413
tensorflow/tensorflow,dictionaries in fit method of model load data in wrong order,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17; tf 2.18

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

the code is running in google collab.
The code below is an example of a model with multiple inputs and multiple outputs.
NOT working code with using **dictionaries** in method **fit** of model.

the link to collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing
the link to gist: https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd


### Standalone code to reproduce the issue

```shell
# collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing
# gist:      https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd

# fast code
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

vocabulary_size = 10000
num_tags = 100
num_departments = 4

# define three model inputs
title = keras.Input(shape=(vocabulary_size,), name=""title"")
text_body = keras.Input(shape=(vocabulary_size,), name=""text_body"")
tags = keras.Input(shape=(num_tags,), name=""tags"")

features = layers.Concatenate()([title, text_body, tags])
# one intermediate layer
features = layers.Dense(64, activation=""relu"")(features)

# Define two model outputs
priority = layers.Dense(1, activation=""sigmoid"", name=""priority"")(features)
department = layers.Dense(num_departments, activation=""softmax"", name=""department"")(features)

# set the model
model = keras.Model(inputs=[title, text_body, tags],
                    outputs=[priority, department])
# prepare data
num_samples = 1280
# The data is filled in with zeros and ones
title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))

# priority: [0., 1.]
priority_data = np.random.random(size=(num_samples, 1))
# class of 4 labels
department_data = np.random.randint(0, 2, size=(num_samples, num_departments))

# compile model
model.compile(optimizer=""rmsprop"",
              loss={""priority"": ""mean_squared_error"",
                    ""department"": ""categorical_crossentropy""},
              metrics={""priority"": [""mean_absolute_error""],
                       ""department"": [""accuracy""]})

# It doesn't matter how the model is compiled
# model.compile(optimizer=""rmsprop"",
#               loss=[""mean_squared_error"", ""categorical_crossentropy""],
#               metrics=[[""mean_absolute_error""], [""accuracy""]])


# NOT WORKING
# TRAIN MODEL WITH transferring the DICTIONARY to the method
model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},
          {""priority"": priority_data, ""department"": department_data},
          epochs=1
)

# WORK
# TRAIN MODEL WITHOUT transferring the DICTIONARY to the method
model.fit([title_data, text_body_data, tags_data],
          [priority_data, department_data],
          epochs=1
)

# ALSO WORK
# TRAIN MODEL WITH transferring the DICTIONARY to the method
# REPLACE priority and department
model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},
          {""priority"": department_data, ""department"": priority_data},
          epochs=1
)
```


### Relevant log output

_No response_","['stat:awaiting response', 'type:bug', 'stale', 'comp:keras', 'TF 2.18']",2025-01-07T13:08:06Z,3,0,https://github.com/tensorflow/tensorflow/issues/84278
tensorflow/tensorflow,keras model.save does not respect `include_optimizer=False`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0-dev20250105

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Saving a model using keras with `include_optizer = False` results in a model being saved with optimizer

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1x5NJs9nFxmExhuy8_f_fOehHmIOmk-CZ?usp=sharing
```


### Relevant log output

_No response_","['type:bug', 'comp:keras', 'TF 2.18']",2025-01-07T10:33:38Z,4,0,https://github.com/tensorflow/tensorflow/issues/84268
tensorflow/tensorflow,Encountered unresolved custom op: XlaDynamicSlice,"Hi, i am doing a task in converting T5 model to TFLite for Android. Currently, i am using T5ModelForConditionalGeneration from Huggingface to convert. The conversion is done with some below logging but when load the `generator` from Interpretor, and run an inference example, i have faced this error. You can reproduce with the colab provided below. AFAIK, this XlaDynamicSlice is in TF ops but why this op cannot be resolved in this cases. 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): PyPI 24.0 on Python 3.11.5
- TensorFlow version (or github SHA if from source): 2.18.0


**Provide the text output from tflite_convert**
In colab version, tflite_convert doesn't log anything, below log is in my local version
```
INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets
INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets
W0000 00:00:1736157114.568747 1061359 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1736157114.568765 1061359 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
2025-01-06 16:51:54.568997: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpaxxybw9x
2025-01-06 16:51:54.645325: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2025-01-06 16:51:54.645352: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpaxxybw9x
2025-01-06 16:51:55.085153: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2025-01-06 16:51:56.061632: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpaxxybw9x
2025-01-06 16:51:56.517300: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 1948307 microseconds.
2025-01-06 16:52:30.233639: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexStridedSlice
Details:
	tf.StridedSlice(tensor<?x?x?xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<?x1x?x?xf32>) : {begin_mask = 13 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 13 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
2025-01-06 16:52:30.233666: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3836] The following operation(s) need TFLite custom op implementation(s):
Custom ops: XlaDynamicSlice
Details:
	tf.XlaDynamicSlice(tensor<1x12x?x?xf32>, tensor<4xi64>, tensor<4xi64>) -> (tensor<1x12x1x?xf32>) : {device = """"}
See instructions: https://www.tensorflow.org/lite/guide/ops_custom
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
My reproduce code in Colab: https://colab.research.google.com/drive/1Rmhc_vpJSa7M1Vt-4ugV5uORaEfRJMOw?usp=sharing
Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
","['stat:awaiting response', 'type:bug', 'comp:lite', 'TFLiteConverter', 'TF 2.18']",2025-01-06T10:46:54Z,2,0,https://github.com/tensorflow/tensorflow/issues/84203
tensorflow/tensorflow,MFCC-Example-Model converted from TF to TFlite fails with IsPowerOfTwo-RuntimeError inside rfft2d,"### 1. System information

- OS Platform and Distribution: Linux Mint 6.2.9
- TensorFlow installation: pip
- TensorFlow library: 2.18.0 (latest)

### 2. Code

Below is a minimum example which triggers the rfft2d IsPowerOfTwo RuntimeError.
The MFCC-Calculation was directly taken from the tutorial from [tensorflow.org](https://www.tensorflow.org/api_docs/python/tf/signal/mfccs_from_log_mel_spectrograms#for_example)

```
import tensorflow as tf

class MFCCLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(MFCCLayer, self).__init__(**kwargs)

    def call(self, pcm):
        # A 1024-point STFT with frames of 64 ms and 75% overlap.
        stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256, fft_length=1024)
        spectrograms = tf.abs(stfts)

        # Warp the linear scale spectrograms into the mel-scale.
        num_spectrogram_bins = stfts.shape[-1]
        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80
        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
            num_mel_bins,
            num_spectrogram_bins,
            sample_rate,
            lower_edge_hertz,
            upper_edge_hertz,
        )
        mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)
        mel_spectrograms.set_shape(
            spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:])
        )

        # Compute a stabilized log to get log-magnitude mel-scale spectrograms.
        log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)

        # Compute MFCCs from log_mel_spectrograms and take the first 13.
        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[
            ..., :13
        ]
        print(""mfccs.shape: "", mfccs.shape)
        return mfccs


def build_model(input_shape):
    input_layer = tf.keras.layers.Input(shape=input_shape)
    output_layer = MFCCLayer()(input_layer)
    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer)


if __name__ == ""__main__"":
    batch_size, num_samples, sample_rate = 32, 32000, 16000.0
    # A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].
    pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)
    print(""pcm.shape: "", pcm.shape)

    model = build_model(pcm.shape)
    model.summary()

    # Convert to TensorFlow Lite and Save
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()

    with open(""mfcc.tflite"", ""wb"") as f:
        f.write(tflite_model)

    # Load the model and run inference
    with open(""mfcc.tflite"", ""rb"") as f:
        tflite_model = f.read()

    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    pcm = tf.expand_dims(pcm, axis=0)  # Add batch dimension

    interpreter.set_tensor(input_details[0][""index""], pcm)
    interpreter.invoke()  # <-- RuntimeError: tensorflow/lite/kernels/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.
    mfccs = interpreter.get_tensor(output_details[0][""index""])
    print(""mfccs.shape: "", mfccs.shape)
```


### 3. Failure after conversion
As far as I know, the RuntimeError should't happen, as all supplied stft-function arguments are power of two's?

I am unsure if this is just a user error from myself or this is a bug.
I couldn't find any info online, hence i ask here.

Is a MFCC-calculation model possible with TFlite?

Thanks for all help

","['stat:awaiting response', 'type:bug', 'stale', 'comp:lite', 'TFLiteConverter', 'TF 2.18']",2025-01-05T20:45:45Z,3,0,https://github.com/tensorflow/tensorflow/issues/84171
tensorflow/tensorflow,Broken compatibility with tensorflow-metal in 2.18,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

MacOS 15.2

### Mobile device

_No response_

### Python version

3.11.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M2 Max GPU 38-cores

### Current behavior?

Apple silicone GPU with tensorflow-metal==1.1.0  and python 3.11 works fine with tensorboard==2.17.0

This is normal output:
```
/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py 
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

Process finished with exit code 0
```

But if I upgrade tensorflow to 2.18 I'll have error, attached in ""Relevant log output"" issue section

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

if __name__ == '__main__':
    gpus = tf.config.experimental.list_physical_devices('GPU')
    print(gpus)
```


### Relevant log output

```shell
/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py 
Traceback (most recent call last):
  File ""/Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py"", line 1, in <module>
    import tensorflow as tf
  File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/__init__.py"", line 437, in <module>
    _ll.load_library(_plugin_dir)
  File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/python/framework/load_library.py"", line 151, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii
  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow-plugins/libmetal_plugin.dylib
  Expected in:     <2814A58E-D752-317B-8040-131217E2F9AA> /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so

Process finished with exit code 1
```
","['stat:awaiting response', 'type:bug', 'stale', 'comp:gpu', 'TF 2.18']",2025-01-05T17:26:17Z,5,0,https://github.com/tensorflow/tensorflow/issues/84167
tensorflow/tensorflow,The test case label_image .py of tensorflow2.4.1 source code fails to be execued.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.4.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.12

### Bazel version

3.7

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The test case label_image.py fails to be executed,and the message ""module 'tensorfle' has no attribute 'GrapDef'"" is displayed.
![image](https://github.com/user-attachments/assets/e4b7b56d-2589-41fe-8395-1743c941dd49)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
graph_def = tf.GraphDef()
```


### Relevant log output

_No response_","['stat:awaiting response', 'type:bug', 'stale', 'TF 2.4']",2025-01-03T03:03:30Z,6,0,https://github.com/tensorflow/tensorflow/issues/84039
tensorflow/tensorflow,Failing to convert MobileNetV3Large to TFLite w/ Integer q,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Windows 10 WSL
- TensorFlow installation (pip package or built from source):  2.10 (on Win 10) and 2.16.2 (on WSL)

### 2. Code

```
import tensorflow as tf
import numpy as np
from tensorflow.keras.applications import MobileNetV3Large
from tensorflow.keras.applications.mobilenet_v3 import preprocess_input
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Generate one sample image for testing
test_image = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))
test_image = np.clip(test_image, 0, 255).astype(np.float32)
preprocessed_image = preprocess_input(test_image.copy())

# Load model
model = MobileNetV3Large(
    weights='imagenet',
    include_top=True,
    input_shape=(224, 224, 3)
)

# Get original prediction
original_pred = model.predict(preprocessed_image, verbose=0)

# Convert to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable dynamic range quantization
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]
converter._experimental_disable_per_channel = True
converter.experimental_new_converter = True

# Convert
tflite_model = converter.convert()

# Get TFLite prediction
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0]['index'], preprocessed_image)
interpreter.invoke()
tflite_pred = interpreter.get_tensor(output_details[0]['index'])

# Calculate correlation
correlation, _ = pearsonr(original_pred.flatten(), tflite_pred.flatten())

# Visualize
plt.figure(figsize=(10, 5))

# Scatter plot
plt.subplot(1, 2, 1)
plt.scatter(original_pred.flatten(), tflite_pred.flatten(), alpha=0.5)
plt.plot([original_pred.min(), original_pred.max()],
         [original_pred.min(), original_pred.max()],
         'r--', label=f'Perfect Correlation\nActual: {correlation:.4f}')
plt.title('Original vs Quantized Predictions')
plt.xlabel('Original Model')
plt.ylabel('Quantized Model')
plt.legend()

# Distribution plot
plt.subplot(1, 2, 2)
plt.hist(np.abs(original_pred.flatten() - tflite_pred.flatten()),
         bins=50, alpha=0.75, label='Prediction Differences')
plt.title('Distribution of Prediction Differences')
plt.xlabel('|Original - Quantized|')
plt.ylabel('Count')
plt.legend()

plt.tight_layout()
plt.show()

print(f""\nResults:"")
print(f""Prediction correlation: {correlation:.4f}"")
print(f""Original model size: {len(model.get_weights()) / 1024 / 1024:.2f} MB"")
print(f""Quantized model size: {len(tflite_model) / 1024 / 1024:.2f} MB"")
print(f""Size reduction: {(1 - len(tflite_model) / len(model.get_weights())) * 100:.1f}%"")
```

### 3. Failure after conversion
1. TF 2.10 in Win10 Log:
Model produces wrong results. See plot made from code:
![image](https://github.com/user-attachments/assets/8823cbfc-88d9-4e2d-9ef7-c8a2adc3ef0a)

2. TF2.16 in WSL:
Model fails to convert. Gets error: `LLVM ERROR: Failed to infer result type(s).` (see log)


### 5. (optional) Any other info / logs
I ran this on 2 systems:

1. TF 2.10 in Win10 Log:
```
import sys; print('Python %s on %s' % (sys.version, sys.platform))
D:\code\ai_dev\venv\Scripts\python.exe ""C:/Program Files/JetBrains/PyCharm 2023.2.4/plugins/python/helpers/pydev/pydevd.py"" --multiprocess --qt-support=auto --client 127.0.0.1 --port 54366 --file C:\Users\Administrator\AppData\Roaming\JetBrains\PyCharm2023.2\scratches\tfmodel_tflite.py 
Connected to pydev debugger (build 232.10203.26)
2024-12-26 15:17:07.215039: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-26 15:17:07.670016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7423 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6
2024-12-26 15:17:11.111912: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8906
2024-12-26 15:17:12.036555: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 64). These functions will not be directly callable after loading.
2024-12-26 15:17:44.746406: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2024-12-26 15:17:44.746529: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2024-12-26 15:17:44.747230: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:44.771028: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }
2024-12-26 15:17:44.771129: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:44.886049: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
2024-12-26 15:17:44.904668: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.
2024-12-26 15:17:45.275249: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:45.402632: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 655396 microseconds.
2024-12-26 15:17:45.811466: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```

3. TF 2.16.2 in WSL log:
```
/root/ai_dev/.venv/bin/python /root/.pycharm_helpers/pydev/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 55955 --file /mnt/c/Users/Administrator/AppData/Roaming/JetBrains/PyCharm2023.2/scratches/tfmodel_tflite.py 
Connected to pydev debugger (build 232.10203.26)
2024-12-26 15:18:56.434366: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-26 15:18:57.803991: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-26 15:18:58.473972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-26 15:18:58.972456: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-12-26 15:18:58.975123: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-26 15:18:59.910805: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-26 15:19:06.124533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-12-26 15:19:15.989425: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node
Your kernel may have been built without NUMA support.
2024-12-26 15:19:17.013008: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1735255170.947341  469943 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.
W0000 00:00:1735255170.947401  469943 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.
2024-12-26 15:19:30.948060: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp9lkkwnp9
2024-12-26 15:19:30.953309: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-12-26 15:19:30.953334: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp9lkkwnp9
2024-12-26 15:19:31.011901: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-12-26 15:19:31.020594: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.
2024-12-26 15:19:31.231606: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /tmp/tmp9lkkwnp9
2024-12-26 15:19:31.297302: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 349244 microseconds.
2024-12-26 15:19:31.779723: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(fused[""ReadVariableOp:"", callsite(""MobileNetV3Large_1/conv_1/convolution/ReadVariableOp@__inference_serving_default_5035""(""/root/.pycharm_helpers/pydev/pydevd.py"":2199:1) at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":2181:1 at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":1493:1 at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":1500:1 at callsite(""/root/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py"":18:1 at callsite(""/mnt/c/Users/Administrator/AppData/Roaming/JetBrains/PyCharm2023.2/scratches/tfmodel_tflite.py"":34:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1175:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1129:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1636:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1614:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py"":205:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1537:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/layer.py"":58:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/layer.py"":112:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py"":899:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/operation.py"":46:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":156:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/models/functional.py"":182:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/function.py"":171:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/models/functional.py"":632:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py"":899:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/operation.py"":46:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":156:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"":243:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"":233:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/nn.py"":1183:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py"":301:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py"":274:1 at ""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py"":85:1))))))))))))))))))))))))))))))))]): error: missing attribute 'value'
LLVM ERROR: Failed to infer result type(s).

Process finished with exit code 134
```


","['stat:awaiting tensorflower', 'type:bug', 'comp:lite', 'TFLiteConverter', 'TF 2.16']",2024-12-26T23:21:09Z,10,0,https://github.com/tensorflow/tensorflow/issues/83754
tensorflow/tensorflow,How to run TFLite benchmark with QNN delegate in Android,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

No

### OS platform and distribution

macOS 15.2

### Mobile device

One Plus 7 Pro, Android 11

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have built/installed/run TFLite benchmark following this [instruction](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#on-android) for Android, and used TensorFlow 2.15.0 according to [issue#66015](https://github.com/tensorflow/tensorflow/issues/66015). I test the benchmark via the following commands and the output result seems correct.
```shell
adb push /Users/handleychen/Github/tensorflow/tensorflow/bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model /data/local/tmp
adb shell chmod +x /data/local/tmp/benchmark_model
adb shell ""mkdir /data/local/tmp/models""
adb push /Users/handleychen/Github/tensorflow/models/*.tflite /data/local/tmp/models
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --num_threads=4 --enable_op_profiling=true
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --use_gpu=true --enable_op_profiling=true
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --use_nnapi=true --enable_op_profiling=true
``` 
[benchmark result.txt](https://github.com/user-attachments/files/18197819/benchmark.result.txt)

Now I want to run the benchmark with QNN delegate. I [setup the on device environment](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/TfLite-Delegate_setup.html#on-device-environment-setup) and [run a QNN delegate using an external delegate](https://docs.qualcomm.com/bundle/publicresource/topics/80-70015-54/sample-applications.html#run-a-qnn-delegate-using-an-external-delegate). The [model](https://storage.googleapis.com/download.tensorflow.org/models/tflite/task_library/image_classification/android_java/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite) being tested comes from tflite example [image_classification](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification).  I tested the benchmark using the following commands, but the result was a failure.
```shell
adb shell ""mkdir /data/local/tmp/qnn_delegate""
adb push /Users/handleychen/Github/quic/SDK/qairt/2.26.0.240828/lib/aarch64-android/* /data/local/tmp/qnn_delegate
adb shell
cd /data/local/tmp
export LD_LIBRARY_PATH=/data/local/tmp/qnn_delegate
export ADSP_LIBRARY_PATH=""/data/local/tmp/qnn_delegate""
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:gpu;'
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;'
# I also tried setting htp_precision:1, but the result was the same.
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;htp_precision:1'
``` 
```shell
# for gpu delegate
……
INFO: Though EXTERNAL delegate is explicitly applied, the model graph will not be executed by the delegate.
……

# for npu delegate
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]
INFO: External delegate path: [/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so]
INFO: External delegate options: [backend_type:htp;htp_precision:1]
INFO: Loaded model /data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: EXTERNAL delegate created.
ERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008
ERROR: Restored original execution plan after delegate application failure.
ERROR: Failed to apply EXTERNAL delegate.
ERROR: Benchmarking failed.
``` 
The full output is attached. [benchmarkQNN result.txt](https://github.com/user-attachments/files/18206356/benchmarkQNN.result.txt)

I have also tested it on Android phones equipped with Snapdragon 855 and Gen 3 chips, and the results were the same.

Could anyone tell me how to deal with this?

### Standalone code to reproduce the issue

```shell
as described above
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:lite']",2024-12-19T12:40:25Z,4,0,https://github.com/tensorflow/tensorflow/issues/83344
tensorflow/tensorflow,Aborted (core dumped) in `LearnedUnigramCandidateSampler`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs,`tf.raw_ops.LearnedUnigramCandidateSampler` triggers crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

true_classes = tf.constant([], dtype=tf.int64)
num_true = 3590707793247644003
num_sampled = 126
unique = False
range_max = 186785497093039093
seed = 8997
seed2 = 0

tf.raw_ops.LearnedUnigramCandidateSampler(
    true_classes=true_classes,
    num_true=num_true,
    num_sampled=num_sampled,
    unique=unique,
    range_max=range_max,
    seed=seed,
    seed2=seed2,
    name=None
)
```


### Relevant log output

```shell
2024-12-17 11:36:29.305345: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at candidate_sampler_ops.cc:37 : INVALID_ARGUMENT: Attr num_true has value 3590707793247644003 out of range for an int32
2024-12-17 11:36:29.305378: F external/local_tsl/tsl/lib/random/weighted_picker.cc:28] Check failed: N >= 0 (0 vs. -2090015755)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-12-17T12:05:50Z,1,0,https://github.com/tensorflow/tensorflow/issues/83164
tensorflow/tensorflow,[XLA] `tf.keras.layers.LSTM` behaves differently on GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When executing LSTM on **XLA**, it fails.
However, when executing it without XLA, it passes.
The above failure is on GPU.
If I use CPU as backend, with or without XLA both pass the check.

### Standalone code to reproduce the issue

```python
import os
import tensorflow
import tensorflow as tf
tf.random.set_seed(42)
class RecurrentModel(tf.keras.Model):

    def __init__(self):
        super(RecurrentModel, self).__init__()
        self.lstm = tf.keras.layers.LSTM(units=64, return_sequences=True)

    @tf.function(jit_compile=True)
    def call(self, x):
        return self.lstm(x)


model = RecurrentModel()


input_shape = (10, 20, 1)
x = tf.random.normal(shape=input_shape)

inputs = [x]

output = model(*inputs)
print(output)
```


### Relevant log output

```shell
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-0938fdccd1fa> in <cell line: 24>()
     22 inputs = [x]
     23 
---> 24 output = model(*inputs)
     25 print(output)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Exception encountered when calling RecurrentModel.call().

Detected unsupported operations when trying to compile graph __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: CudnnRNNV3 (No registered 'CudnnRNNV3' OpKernel for XLA_GPU_JIT devices compatible with node {{node lstm_3_1/CudnnRNNV3}}){{node lstm_3_1/CudnnRNNV3}}
The op is created at: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-4-0938fdccd1fa>"", line 24, in <cell line: 24>
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 826, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1376, in _maybe_build
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 212, in compute_output_spec
File ""<ipython-input-1-0938fdccd1fa>"", line 13, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 901, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py"", line 570, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py"", line 406, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py"", line 537, in inner_loop
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py"", line 841, in lstm
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py"", line 933, in _cudnn_lstm
	tf2xla conversion failed while converting __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_877]

Arguments received by RecurrentModel.call():
  • x=tf.Tensor(shape=(10, 20, 1), dtype=float32)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'comp:xla', 'TF 2.18']",2024-12-16T13:57:22Z,2,0,https://github.com/tensorflow/tensorflow/issues/83063
tensorflow/tensorflow,`tensorflow.experimental.numpy.kron` not working with multidimensional Arrays,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

This is a new issue in replacement for https://github.com/tensorflow/tensorflow/issues/59761 as suggested by @tilakrayal

I tested the function against numpy and it throws an error when the `ndim` of the input tensors is greater than 2.
I run the code on the latest TensorFlow version on PyPI and the nightly version, and I get the same failures.

Also, I am not getting as much debug information only this error

`UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name: `

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
try:
    a = tf.constant(np.arange(100).reshape(2, 5, 2, 5))
    b = tf.constant(np.arange(24).reshape(2, 3, 4))
    print(a.ndim) # 4
    print(b.ndim) # 3

    y = tf.experimental.numpy.kron(a, b)

    print(y.shape)
except:
    print(""Can't use tf.experimental.numpy.kron on multi-dimensional arrays"")

x = np.arange(100).reshape(2, 5, 2, 5)
y = np.arange(24).reshape(2, 3, 4)

print(x.ndim) # 4
print(y.ndim) # 3

z = np.kron(x, y)

print(z.shape) # (2, 10, 6, 20)
```


### Relevant log output

```shell
UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-12-16T07:22:01Z,3,0,https://github.com/tensorflow/tensorflow/issues/83037
tensorflow/tensorflow,error message is inconsistent with documentation in `tf.raw_ops.MaxPoolGradWithArgmax`,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

According to [Doc tf.raw_ops.MaxPoolGradWithArgmax](https://www.tensorflow.org/api_docs/python/tf/raw_ops/MaxPoolGradWithArgmax), the argument `argmax` can be `int32` or `int64`. However, after actual testing, the parameter `argmax` can only support tensor input of data type `int64`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_tensor = tf.constant(1, shape=[1, 2, 2, 1], dtype=tf.float32)
grad_tensor = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.float32)
argmax_indices = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.int32)
ksize = [1, 2, 2, 1]
strides = [1, 1, 1, 1]
padding = ""VALID""

output_grad = tf.raw_ops.MaxPoolGradWithArgmax(
    input=input_tensor,
    grad=grad_tensor,
    argmax=argmax_indices,
    ksize=ksize,
    strides=strides,
    padding=padding,
    include_batch_in_index=False
)
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node MaxPoolGradWithArgmax}} = MaxPoolGradWithArgmax[T=DT_FLOAT, Targmax=DT_INT32, include_batch_in_index=false, ksize=[1, 2, 2, 1], padding=""VALID"", strides=[1, 1, 1, 1]]
All kernels registered for op MaxPoolGradWithArgmax:
  device='CPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT32]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT64]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Targmax in [DT_INT64]
 [Op:MaxPoolGradWithArgmax] name:
```
","['type:docs-bug', 'stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-12-12T13:14:53Z,1,0,https://github.com/tensorflow/tensorflow/issues/82837
tensorflow/tensorflow,Update Python in Docker images to 3.11.x and ditch `3.11.0rc1`,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Docker

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The Python version of the docker images is outdated and should be updated

### Standalone code to reproduce the issue

```shell
docker run -it tensorflow/tensorflow python
```


### Relevant log output

```shell
Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>>
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'TF 2.18']",2024-12-09T13:35:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/82529
tensorflow/tensorflow,"The warning ""The structure of `inputs` doesn't match the expected structure"" when training a functional model","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.13.1-0-gf841394b1b7 2.13.1 (Nightly: v1.12.1-119104-gf8fd6f53fa3 2.19.0-dev20241204)

### Custom code

Yes

### OS platform and distribution

Windows 11 23H2 22631.4460

### Mobile device

Windows 11 23H2 22631.4460

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the model is functional, not Sequential, the warning has occured:

```
Epoch 1/5
<path-to-python>\lib\site-packages\keras\src\models\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*
  warnings.warn(
```

Yes, the warning message has interrupted on parenthesis. When I've run the same code in Nightly, the warning message is:

```
Epoch 1/5
<path-to-python>\lib\site-packages\keras\src\models\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.
Expected: ['keras_tensor']
Received: inputs=Tensor(shape=(None, 10))
  warnings.warn(msg)
```

After the warning, the training continues normally, but because of this warning, I can't be sure that the model works as I expect.

I've traced the source and found that in `Lib\site-packages\keras\src\tree\optree_impl.py` on line 95 comparasion of expected and actual structure failed. Now I place the traced variables here:

```
>>> a
<tf.Tensor 'data:0' shape=(None, 10) dtype=float64>
>>> b
[<KerasTensor shape=(None, 10), dtype=float32, sparse=False, name=keras_tensor>]
>>> a_structure
PyTreeSpec(*, NoneIsLeaf)
>>> b_structure
PyTreeSpec([*], NoneIsLeaf)
```

The data passed to the `fit` function fully corresponds to the [documentation](https://keras.io/api/models/model_training_apis/#fit-method). The warning appears independently of whether I use numpy array or PyDataset as dataset of `fit` function.



### Standalone code to reproduce the issue

```shell
from keras.models import Model
from keras.layers import Dense, Input, Flatten, Concatenate
from keras import utils
import numpy as np
import tensorflow as tf

class SamplesSet(utils.PyDataset):
    
    def __init__(self, batch_size, **kwargs):
        super().__init__(**kwargs)
        self.batch_size = batch_size
        
    def __len__(self):
        return 1
    
    def __getitem__(self, idx):
        x1 = np.random.uniform(size=10*self.batch_size).reshape((self.batch_size, 10))
        y = np.arange(self.batch_size)
        return x1, y
    
train = SamplesSet(100)
x1_train = np.random.uniform(size=10*100).reshape((100, 10))
y_train = np.arange(100)

input1 = Input(shape=(10,))
l1 = Dense(1)(input1)
d2 = Dense(1, activation='sigmoid')(l1)
model = Model(inputs=[input1], outputs=[d2])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

history = model.fit(x1_train, y_train, epochs=5, verbose=1)
# In the all cases below warning occures too
# history = Model.fit(train, epochs=5, verbose=1) 
# ret = model.predict(np.arange(10)[np.newaxis,:])
# ret = model.predict(tf.constant([[0,1,2,3,4,5,6,7,8,9]]))
```


### Relevant log output

_No response_","['type:bug', 'comp:keras', 'TF 2.13']",2024-12-06T03:47:02Z,8,0,https://github.com/tensorflow/tensorflow/issues/82372
tensorflow/tensorflow,[XLA] TF XLA outputs abnormal value when compiling `Embedding`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

For `Embedding` operator, when I set the `input_dim=1` (which means it indexed from 0 to 0), the output always returns **0 without XLA**.

After compilation, the outputs are usually some random tensors.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.random.set_seed(42)
x = tf.constant([1])


# uncompiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()

output1 = m(x)


# compiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    @tf.function(jit_compile=True)
    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()
output2 = m(x)

print(output1)
print(output2)
```


### Relevant log output

```shell
tf.Tensor([[0.]], shape=(1, 1), dtype=float32)
tf.Tensor([[-0.00567592]], shape=(1, 1), dtype=float32)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.18']",2024-12-05T13:58:40Z,4,0,https://github.com/tensorflow/tensorflow/issues/82317
tensorflow/tensorflow,Are checkpoints broken in >= 2.16?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16, 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The example given in https://www.tensorflow.org/guide/checkpoint does not seem to work as expected in 2.16 and 2.17, while working fine in 2.15. After restoring and restarting the training process, it starts training from the very beginning.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1n76Mu5BhdBJBSXc7cXYJMr0lMDER2JRa?usp=sharing
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', '2.17']",2024-12-04T15:11:53Z,2,0,https://github.com/tensorflow/tensorflow/issues/82209
tensorflow/tensorflow,TPU not support TensorFlow 2.18 and 2.17.1,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.18 and tf. 2.17.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`import tensorflow as tf` results `segmentation fault core dumped`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:tpus', 'TF 2.18']",2024-12-04T14:56:53Z,7,0,https://github.com/tensorflow/tensorflow/issues/82208
tensorflow/tensorflow,Clarify the `constant_op.constant(2)` statement,"It would be helpful to clarify the `constant_op.constant(2)` statement by explaining the corresponding import statement.

https://github.com/tensorflow/tensorflow/blob/5bc9d26649cca274750ad3625bd93422617eed4b/tensorflow/python/ops/summary_ops_v2.py#L1062-L1066","['type:bug', 'comp:ops']",2024-12-02T18:05:41Z,4,0,https://github.com/tensorflow/tensorflow/issues/81954
tensorflow/tensorflow,MixedPrecision + XLA: Seen floating point types of different precisions,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

Google Colab default

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using bilinear interpolation + XLA + mixed_float16 policy issue raises during compilation.
Without bilinear interpolation or without XLA or without mixed_float16 there is no issue.

In Google Colab i have this issue only on CPU with TF 2.17 and both CPU & GPU with TF 2.18.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1joSiScbM7Stc9bn1C4R_4sFkDzakrTsJ?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-4-9cc273be2d5a> in <cell line: 28>()
     26 model.compile(loss='mse', optimizer='adam', run_eagerly=False, jit_compile=True)
     27 
---> 28 model.fit(dataset)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InternalError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>

  File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start

  File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 377, in dispatch_queue

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 250, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 748, in __init__

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code

  File ""<ipython-input-4-9cc273be2d5a>"", line 28, in <cell line: 28>

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 368, in fit

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 216, in function

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 129, in multi_step_on_iterator

during context [Unknown]: Seen floating point types of different precisions in %multiply.43589 = f32[2,8,8,1280]{3,2,1,0} multiply(f32[2,8,8,1280]{3,2,1,0} %add.43539, f16[2,8,8,1280]{3,2,1,0} %multiply.43588), metadata={op_type=""Mul"" op_name=""mul_9"" source_file=""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"" source_line=1196}, but mixed precision is disallowed.
	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_124474]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.18']",2024-11-29T07:11:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/81273
tensorflow/tensorflow,Some operators give different results on CPU and GPU when dealing with complex numbers that include `inf`.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The outputs of TensorFlow mathematical APIs (`sin, cos, tan, sinh, cosh, exp, and reduce_mean`) are inconsistent between the CPU and GPU when applied to complex inputs containing `inf`. 

### Standalone code to reproduce the issue

```shell
python
import tensorflow as tf

test_inputs = [
    tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128),
]

test_apis = [
    tf.math.sin, tf.math.cos, tf.math.tan,
    tf.math.sinh, tf.math.cosh, tf.math.exp, tf.math.reduce_mean
]

for api in test_apis:
    print(f""Testing {api.__name__}"")
    for x in test_inputs:
        try:
            with tf.device('/CPU'):
              cpu_out = api(x)
              print(f""CPU Output: {cpu_out}"")
            with tf.device('/GPU:0'):
              gpu_out = api(x)
              print(f""GPU Output: {gpu_out}"")
        except Exception as e:
            print(f""Error in {api.__name__}: {e}"")
```


### Relevant log output

```shell
Testing sin
CPU Output: [nan +0.j  0.+infj nan+infj]
GPU Output: [nan+nanj nan+infj nan+nanj]
Testing cos
CPU Output: [nan +0.j inf -0.j inf+nanj]
GPU Output: [nan+nanj inf+nanj nan+nanj]
Testing tan
CPU Output: [nan+0.j  0.+1.j  0.+1.j]
GPU Output: [nan+nanj  0. +1.j  0. +1.j]
Testing sinh
CPU Output: [inf +0.j  0.+nanj inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing cosh
CPU Output: [inf +0.j nan +0.j inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing exp
CPU Output: [inf +0.j nan+nanj inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing reduce_mean
CPU Output: (inf+infj)
GPU Output: (nan+nanj)
```
","['stat:awaiting response', 'type:bug', 'stale', 'comp:ops', '2.17']",2024-11-27T13:13:05Z,4,0,https://github.com/tensorflow/tensorflow/issues/80947
tensorflow/tensorflow,"When using `tf.math.log1p` and NumPy's `np.log1p` with the same complex input, the outputs are inconsistent.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow's `tf.math.log1p` produces inconsistent results with NumPy's `np.log1p` for complex inputs containing `inf`, such as `[inf+0.j, 0+inf.j, inf+inf.j]`. TensorFlow outputs `[inf+0.j, nan+nanj, nan+nanj]`, while NumPy returns `[inf+0.j, inf+1.57079633j, inf+0.78539816j]`.

### Standalone code to reproduce the issue

```shell
python
import tensorflow as tf
import numpy as np

test_input = tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128)

# TensorFlow computation
with tf.device('/CPU:0'):
    cpu_out = tf.math.log1p(test_input)

# NumPy computation
numpy_out = np.log1p(test_input.numpy())

print(f""CPU Output: {cpu_out}"")
print(f""NumPy Output: {numpy_out}"")
```


### Relevant log output

```shell
CPU Output: [inf +0.j nan+nanj nan+nanj]
NumPy Output: [inf+0.j         inf+1.57079633j inf+0.78539816j]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-26T13:36:51Z,3,0,https://github.com/tensorflow/tensorflow/issues/80850
tensorflow/tensorflow,Heap-buffer-overflow in `SparseMatrixSparseCholesky`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When batch_index is scalar, `SparseMatrixSparseCholesky` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

indices = tf.constant([[0, 0], [1, 1], [2, 1], [2, 2], [3, 3]], dtype=tf.int64)
values = tf.constant([1.0, 2.0, 1.0, 3.0, 4.0], tf.float32)
dense_shape = tf.constant([4, 4], dtype=tf.int64)
input = tf.raw_ops.SparseTensorToCSRSparseMatrix(
    indices=indices, values=values, dense_shape=dense_shape, name=None
)
permutation = tf.constant([4,1,1,1], dtype=tf.int32)

tf.raw_ops.SparseMatrixSparseCholesky(
  input=input, permutation=permutation, type=tf.float32
)
```


### Relevant log output

```shell
=================================================================
==3331846==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60700049d1d0 at pc 0x7faa202d6d87 bp 0x7ffd09bf8fe0 sp 0x7ffd09bf8fd0
WRITE of size 4 at 0x60700049d1d0 thread T0
    #0 0x7faa202d6d86 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2d86)
    #1 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f373a)
    #2 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f84e9)
    #3 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4201d41)
    #4 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x26d8d71)
    #5 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2adab6)
    #6 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cdc14a)
    #7 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1862fe6)
    #8 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x173f306)
    #9 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x179cf74)
    #10 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x17aa8b1)
    #11 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c582eb)
    #12 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aedc2c)
    #13 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31af066a)
    #14 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c2a939)
    #15 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31addde4)
    #16 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31ae1dd4)
    #17 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aebd26)
    #18 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x20ad4c33)
    #19 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c1fe5e)
    #20 0x7fa9f268645b in TFE_Execute (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x1065245b)
    #21 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so+0x3c2274)
    #22 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0xb7ccb)
    #23 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #24 0x51ad66  (/usr/bin/python3.11+0x51ad66)
    #25 0x4e75db in _PyObject_MakeTpCall (/usr/bin/python3.11+0x4e75db)
    #26 0x4fb151 in _PyEval_EvalFrameDefault (/usr/bin/python3.11+0x4fb151)
    #27 0x531822 in _PyFunction_Vectorcall (/usr/bin/python3.11+0x531822)
    #28 0x541194 in PyObject_Call (/usr/bin/python3.11+0x541194)
    #29 0x4fefe0 in _PyEval_EvalFrameDefault (/usr/bin/python3.11+0x4fefe0)
    #30 0x62e1b3  (/usr/bin/python3.11+0x62e1b3)
    #31 0x4f3a66 in PyEval_EvalCode (/usr/bin/python3.11+0x4f3a66)
    #32 0x647c36  (/usr/bin/python3.11+0x647c36)
    #33 0x64534f  (/usr/bin/python3.11+0x64534f)
    #34 0x650d14  (/usr/bin/python3.11+0x650d14)
    #35 0x650a63 in _PyRun_SimpleFileObject (/usr/bin/python3.11+0x650a63)
    #36 0x650832 in _PyRun_AnyFileObject (/usr/bin/python3.11+0x650832)
    #37 0x64f786 in Py_RunMain (/usr/bin/python3.11+0x64f786)
    #38 0x61ee0c in Py_BytesMain (/usr/bin/python3.11+0x61ee0c)
    #39 0x7faaf80d1d8f  (/lib/x86_64-linux-gnu/libc.so.6+0x29d8f)
    #40 0x7faaf80d1e3f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x29e3f)
    #41 0x61ec94 in _start (/usr/bin/python3.11+0x61ec94)

0x60700049d1d0 is located 0 bytes to the right of 80-byte region [0x60700049d180,0x60700049d1d0)
allocated by thread T0 here:
    #0 0x7faaf84bf887 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cpp:145
    #1 0x7faa202d6803 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2803)
    #2 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f373a)
    #3 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f84e9)
    #4 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4201d41)
    #5 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x26d8d71)
    #6 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2adab6)
    #7 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cdc14a)
    #8 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1862fe6)
    #9 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x173f306)
    #10 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x179cf74)
    #11 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x17aa8b1)
    #12 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c582eb)
    #13 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aedc2c)
    #14 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31af066a)
    #15 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c2a939)
    #16 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31addde4)
    #17 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31ae1dd4)
    #18 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aebd26)
    #19 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x20ad4c33)
    #20 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c1fe5e)
    #21 0x7fa9f268645b in TFE_Execute (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x1065245b)
    #22 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so+0x3c2274)
    #23 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0xb7ccb)
    #24 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #25 0x51ad66  (/usr/bin/python3.11+0x51ad66)

SUMMARY: AddressSanitizer: heap-buffer-overflow (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2d86) in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const
Shadow bytes around the buggy address:
  0x0c0e8008b9e0: 00 00 00 00 00 00 fa fa fa fa fd fd fd fd fd fd
  0x0c0e8008b9f0: fd fd fd fd fa fa fa fa 00 00 00 00 00 00 00 00
  0x0c0e8008ba00: 00 fa fa fa fa fa 00 00 00 00 00 00 00 00 04 fa
  0x0c0e8008ba10: fa fa fa fa 00 00 00 00 00 00 00 00 04 fa fa fa
  0x0c0e8008ba20: fa fa 00 00 00 00 00 00 00 00 04 fa fa fa fa fa
=>0x0c0e8008ba30: 00 00 00 00 00 00 00 00 00 00[fa]fa fa fa fa fa
  0x0c0e8008ba40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba50: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba60: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba70: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
==3331846==ABORTING
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-26T12:42:50Z,1,0,https://github.com/tensorflow/tensorflow/issues/80847
tensorflow/tensorflow,Aborted (core dumped) in `RaggedBincount`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value of size is close to the maximum value for the dtype, an integer overflow error will occur in following multiplication calculations.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

splits = tf.constant([0, 3, 5, 9], dtype=tf.int64)
values = tf.constant(1, shape=[3,3], dtype=tf.int64)
size = tf.constant(6522107765268123892, dtype=tf.int64)
weights = tf.constant(1, shape=[3,3], dtype=tf.float32)
counts = tf.raw_ops.RaggedBincount(splits=splits, values=values, size=size, weights=weights)
```


### Relevant log output

```shell
Status: INVALID_ARGUMENT: Encountered overflow when multiplying 3 with 6522107765268123892, result: -1
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-26T03:18:31Z,3,0,https://github.com/tensorflow/tensorflow/issues/80812
tensorflow/tensorflow,This method creates a model with a 100% memory leak loop using model. fit(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

ubuntu 2.2 or mac m1

### Mobile device

ubuntu 2.2 or mac m1

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend

### Standalone code to reproduce the issue

```shell
import gc

import keras
import numpy as np
import psutil
from keras.optimizers import Adam
from keras.layers import Dense, Dropout, Input, LSTM
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import time
import json


num_samples = 6
num_features = 3
num_classes = 4
epochs = 50
batch_size = 2
identifier = ""test_model""
num_iterations = 500  

def build_model(X, num_classes):
    model = Sequential()
    model.add(Input(shape=(X.shape[1], X.shape[2])))
    model.add(LSTM(16, return_sequences=True))
    model.add(LSTM(16))
    model.add(Dropout(0.4))
    model.add(Dense(8, activation='tanh'))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    return model


data_X = np.random.rand(num_samples, num_features)
data_Y = np.random.randint(0, num_classes, size=(num_samples, 1))  


data_Y = np.eye(num_classes)[data_Y.flatten()]  
print(type(data_X))

scaler = MinMaxScaler()
data_X_scaled = scaler.fit_transform(data_X)


train_X, test_X, train_Y, test_Y = train_test_split(data_X_scaled, data_Y, train_size=0.6, random_state=42)


train_X = np.expand_dims(train_X, axis=1)
test_X = np.expand_dims(test_X, axis=1)

best_loss = np.inf
best_model_data = None
for iteration in range(num_iterations):
   
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()
    model = build_model(train_X, num_classes)
 
    model_name = f""model_{iteration}""
    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=0, restore_best_weights=True)
    print(f""Iteration {iteration + 1}/{num_iterations}"")
    process = psutil.Process()
    mem_info = process.memory_info()
    print(f""start Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size
    try:
        history = model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, shuffle=True,
                            validation_data=(test_X, test_Y), verbose=0)
        current_loss = history.history['loss'][-1]
        print(f""Training model: {model.name}"")
    
        del model
        tf.keras.backend.clear_session()
        gc.collect()
    except Exception as e:
        print(""err:"", e)
    finally:
        process = psutil.Process()
        mem_info = process.memory_info()
        print(f""end Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size

print(""end！"")


if best_model_data:
    model_json = best_model_data[""model_architecture""]
    model_weights = json.loads(best_model_data[""model_weights""], object_hook=lambda d: np.array(d))
    model = tf.keras.models.model_from_json(model_json)
    model.set_weights(model_weights)
    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    print(""ok"")
else:
    print(""not found"")
```


### Relevant log output

```shell
Iteration 1/500
start Current memory usage: 450.69 MB
Training model: sequential
end Current memory usage: 524.41 MB
Iteration 2/500
start Current memory usage: 524.52 MB
Training model: sequential
end Current memory usage: 564.97 MB
Iteration 3/500
start Current memory usage: 564.98 MB
Training model: sequential
end Current memory usage: 598.00 MB
Iteration 4/500
start Current memory usage: 598.03 MB
Training model: sequential
end Current memory usage: 624.69 MB
Iteration 5/500
start Current memory usage: 624.69 MB
Training model: sequential
end Current memory usage: 653.89 MB
Iteration 6/500
start Current memory usage: 653.91 MB
Training model: sequential
end Current memory usage: 679.45 MB
Iteration 7/500
start Current memory usage: 679.45 MB
Training model: sequential
end Current memory usage: 701.59 MB
Iteration 8/500
start Current memory usage: 701.59 MB
Training model: sequential
end Current memory usage: 726.83 MB
Iteration 9/500
start Current memory usage: 726.84 MB
Training model: sequential
end Current memory usage: 749.56 MB
Iteration 10/500
start Current memory usage: 749.56 MB
Training model: sequential
end Current memory usage: 782.56 MB
Iteration 11/500
start Current memory usage: 782.56 MB
Training model: sequential
end Current memory usage: 805.92 MB
Iteration 12/500
start Current memory usage: 805.92 MB
Training model: sequential
end Current memory usage: 833.17 MB
Iteration 13/500
start Current memory usage: 833.17 MB
Training model: sequential
end Current memory usage: 852.84 MB
Iteration 14/500
start Current memory usage: 852.84 MB
Training model: sequential
end Current memory usage: 875.05 MB
Iteration 15/500
start Current memory usage: 875.06 MB
Training model: sequential
end Current memory usage: 901.56 MB
Iteration 16/500
start Current memory usage: 901.56 MB
Training model: sequential
end Current memory usage: 930.62 MB
Iteration 17/500
start Current memory usage: 705.70 MB
Training model: sequential
end Current memory usage: 762.64 MB
Iteration 18/500
start Current memory usage: 762.70 MB
Training model: sequential
end Current memory usage: 798.06 MB
Iteration 19/500
start Current memory usage: 798.17 MB
Training model: sequential
end Current memory usage: 824.98 MB
Iteration 20/500
start Current memory usage: 824.98 MB
Training model: sequential
end Current memory usage: 850.34 MB
Iteration 21/500
start Current memory usage: 850.42 MB
Training model: sequential
end Current memory usage: 876.81 MB
Iteration 22/500
start Current memory usage: 876.81 MB
Training model: sequential
end Current memory usage: 904.02 MB
Iteration 23/500
start Current memory usage: 904.08 MB
Training model: sequential
end Current memory usage: 929.70 MB
Iteration 24/500
start Current memory usage: 929.73 MB
Training model: sequential
end Current memory usage: 952.33 MB
Iteration 25/500
start Current memory usage: 952.34 MB
Training model: sequential
end Current memory usage: 952.28 MB
Iteration 26/500
start Current memory usage: 952.47 MB
Training model: sequential
end Current memory usage: 980.39 MB
Iteration 27/500
start Current memory usage: 978.78 MB
Training model: sequential
end Current memory usage: 999.02 MB
Iteration 28/500
start Current memory usage: 999.05 MB
Training model: sequential
end Current memory usage: 1023.50 MB
Iteration 29/500
start Current memory usage: 1023.53 MB
Training model: sequential
end Current memory usage: 1047.80 MB
Iteration 30/500
start Current memory usage: 1047.83 MB
Training model: sequential
end Current memory usage: 1068.88 MB
Iteration 31/500
start Current memory usage: 1068.94 MB
Training model: sequential
end Current memory usage: 1095.78 MB
Iteration 32/500
start Current memory usage: 1095.78 MB
Training model: sequential
end Current memory usage: 1119.03 MB
Iteration 33/500
start Current memory usage: 1119.03 MB
Training model: sequential
end Current memory usage: 1039.41 MB
Iteration 34/500
start Current memory usage: 1022.78 MB
Training model: sequential
end Current memory usage: 1040.88 MB
Iteration 35/500
start Current memory usage: 1040.70 MB
Training model: sequential
end Current memory usage: 1054.58 MB
Iteration 36/500
start Current memory usage: 1054.58 MB
Training model: sequential
end Current memory usage: 1076.16 MB
Iteration 37/500
start Current memory usage: 1076.19 MB
Training model: sequential
end Current memory usage: 1097.02 MB
Iteration 38/500
start Current memory usage: 1097.03 MB
Training model: sequential
end Current memory usage: 1113.70 MB
Iteration 39/500
start Current memory usage: 1114.12 MB
Training model: sequential
end Current memory usage: 1140.30 MB
Iteration 40/500
start Current memory usage: 1140.33 MB
Training model: sequential
end Current memory usage: 1163.81 MB
Iteration 41/500
start Current memory usage: 1163.86 MB
Training model: sequential
end Current memory usage: 1195.83 MB
Iteration 42/500
start Current memory usage: 1195.83 MB
Training model: sequential
end Current memory usage: 1221.53 MB
Iteration 43/500
start Current memory usage: 1221.55 MB
Training model: sequential
end Current memory usage: 1231.09 MB
Iteration 44/500
start Current memory usage: 1231.14 MB
Training model: sequential
end Current memory usage: 1245.78 MB
Iteration 45/500
start Current memory usage: 1199.55 MB
Training model: sequential
end Current memory usage: 1221.59 MB
Iteration 46/500
start Current memory usage: 1221.59 MB
Training model: sequential
end Current memory usage: 1249.11 MB
Iteration 47/500
start Current memory usage: 1249.22 MB
Training model: sequential
end Current memory usage: 1275.50 MB
Iteration 48/500
start Current memory usage: 1259.83 MB
Training model: sequential
end Current memory usage: 1290.91 MB
Iteration 49/500
start Current memory usage: 1285.67 MB
Training model: sequential
end Current memory usage: 1296.75 MB
Iteration 50/500
start Current memory usage: 1296.75 MB
Training model: sequential
end Current memory usage: 1306.59 MB
Iteration 51/500
start Current memory usage: 1306.59 MB
Training model: sequential
end Current memory usage: 1287.53 MB
Iteration 52/500
start Current memory usage: 1287.53 MB
Training model: sequential
end Current memory usage: 1297.23 MB
Iteration 53/500
start Current memory usage: 1297.25 MB
Training model: sequential
end Current memory usage: 1285.45 MB
Iteration 54/500
start Current memory usage: 1285.45 MB
Training model: sequential
end Current memory usage: 1290.36 MB
Iteration 55/500
start Current memory usage: 1282.14 MB
Training model: sequential
end Current memory usage: 1302.14 MB
Iteration 56/500
start Current memory usage: 1302.14 MB
Training model: sequential
end Current memory usage: 1287.70 MB
Iteration 57/500
start Current memory usage: 1287.75 MB
Training model: sequential
end Current memory usage: 1282.77 MB
Iteration 58/500
start Current memory usage: 1271.38 MB
Training model: sequential
end Current memory usage: 1232.14 MB
Iteration 59/500
start Current memory usage: 1212.70 MB
Training model: sequential
end Current memory usage: 1201.16 MB
Iteration 60/500
start Current memory usage: 1200.53 MB
Training model: sequential
end Current memory usage: 1169.45 MB
Iteration 61/500
start Current memory usage: 1169.45 MB
Training model: sequential
end Current memory usage: 1209.73 MB
Iteration 62/500
start Current memory usage: 1207.19 MB
Training model: sequential
end Current memory usage: 1226.28 MB
Iteration 63/500
start Current memory usage: 1226.28 MB
Training model: sequential
end Current memory usage: 1231.45 MB
Iteration 64/500
start Current memory usage: 1210.11 MB
Training model: sequential
end Current memory usage: 1176.00 MB
Iteration 65/500
start Current memory usage: 1173.97 MB
Training model: sequential
end Current memory usage: 1201.42 MB
Iteration 66/500
start Current memory usage: 1201.42 MB
Training model: sequential
end Current memory usage: 1223.94 MB
Iteration 67/500
start Current memory usage: 1222.50 MB
Training model: sequential
end Current memory usage: 1229.80 MB
Iteration 68/500
start Current memory usage: 1227.14 MB
Training model: sequential
end Current memory usage: 1219.02 MB
Iteration 69/500
start Current memory usage: 1210.48 MB
Training model: sequential
end Current memory usage: 1247.17 MB
Iteration 70/500
start Current memory usage: 1245.94 MB
Training model: sequential
end Current memory usage: 1259.84 MB
Iteration 71/500
start Current memory usage: 1259.86 MB
Training model: sequential
end Current memory usage: 1286.39 MB
Iteration 72/500
start Current memory usage: 1286.53 MB
Training model: sequential
end Current memory usage: 1316.52 MB
Iteration 73/500
start Current memory usage: 1311.53 MB
Training model: sequential
end Current memory usage: 1338.72 MB
Iteration 74/500
start Current memory usage: 1338.75 MB
Training model: sequential
end Current memory usage: 1348.45 MB
Iteration 75/500
start Current memory usage: 1338.30 MB
Training model: sequential
end Current memory usage: 1354.97 MB
Iteration 76/500
start Current memory usage: 1353.83 MB
Training model: sequential
end Current memory usage: 1385.67 MB
Iteration 77/500
start Current memory usage: 1385.69 MB
Training model: sequential
end Current memory usage: 1408.83 MB
Iteration 78/500
start Current memory usage: 1408.88 MB
Training model: sequential
end Current memory usage: 1430.91 MB
Iteration 79/500
start Current memory usage: 1430.94 MB
Training model: sequential
end Current memory usage: 1443.62 MB
Iteration 80/500
start Current memory usage: 1428.00 MB
Training model: sequential
end Current memory usage: 1436.50 MB
Iteration 81/500
start Current memory usage: 1436.64 MB
Training model: sequential
end Current memory usage: 1454.66 MB
Iteration 82/500
start Current memory usage: 1440.91 MB
Training model: sequential
end Current memory usage: 1461.81 MB
Iteration 83/500
start Current memory usage: 1460.47 MB
Training model: sequential
end Current memory usage: 1481.19 MB
Iteration 84/500
start Current memory usage: 1481.19 MB
Training model: sequential
end Current memory usage: 1477.84 MB
Iteration 85/500
start Current memory usage: 1477.84 MB
Training model: sequential
end Current memory usage: 1493.55 MB
Iteration 86/500
start Current memory usage: 1493.58 MB
Training model: sequential
end Current memory usage: 1509.50 MB
Iteration 87/500
start Current memory usage: 1509.50 MB
Training model: sequential
end Current memory usage: 1543.94 MB
Iteration 88/500
start Current memory usage: 1542.83 MB
Training model: sequential
end Current memory usage: 1516.17 MB
Iteration 89/500
start Current memory usage: 1516.20 MB
Training model: sequential
end Current memory usage: 1470.17 MB
Iteration 90/500
start Current memory usage: 1470.22 MB
Training model: sequential
end Current memory usage: 1443.72 MB
Iteration 91/500
start Current memory usage: 1444.36 MB
Training model: sequential
end Current memory usage: 1486.23 MB
Iteration 92/500
start Current memory usage: 1476.41 MB
Training model: sequential
end Current memory usage: 1524.97 MB
Iteration 93/500
start Current memory usage: 1524.97 MB
Training model: sequential
end Current memory usage: 1534.94 MB
Iteration 94/500
start Current memory usage: 1551.98 MB
Training model: sequential
end Current memory usage: 1853.48 MB
Iteration 95/500
start Current memory usage: 1853.48 MB
Training model: sequential
end Current memory usage: 1790.12 MB
Iteration 96/500
start Current memory usage: 1792.27 MB
Training model: sequential
end Current memory usage: 1883.20 MB
Iteration 97/500
start Current memory usage: 1879.05 MB
Training model: sequential
end Current memory usage: 1759.69 MB
Iteration 98/500
start Current memory usage: 1669.66 MB
Training model: sequential
end Current memory usage: 1596.77 MB
Iteration 99/500
start Current memory usage: 1597.12 MB
Training model: sequential
end Current memory usage: 1568.83 MB
Iteration 100/500
start Current memory usage: 1532.98 MB
Training model: sequential
end Current memory usage: 1516.75 MB
Iteration 101/500
start Current memory usage: 1465.98 MB
Training model: sequential
end Current memory usage: 1486.66 MB
Iteration 102/500
start Current memory usage: 1483.34 MB
Training model: sequential
end Current memory usage: 1523.19 MB
Iteration 103/500
start Current memory usage: 1523.14 MB
Training model: sequential
end Current memory usage: 1532.77 MB
Iteration 104/500
start Current memory usage: 1531.14 MB
Training model: sequential
end Current memory usage: 1561.78 MB
Iteration 105/500
start Current memory usage: 1555.67 MB
Training model: sequential
end Current memory usage: 1586.70 MB
Iteration 106/500
start Current memory usage: 1586.75 MB
Training model: sequential
end Current memory usage: 1608.41 MB
Iteration 107/500
start Current memory usage: 1603.81 MB
Training model: sequential
end Current memory usage: 1629.00 MB
Iteration 108/500
start Current memory usage: 1629.05 MB
Training model: sequential
end Current memory usage: 1609.25 MB
Iteration 109/500
start Current memory usage: 1609.31 MB
Training model: sequential
end Current memory usage: 1630.09 MB
Iteration 110/500
start Current memory usage: 1629.20 MB
Training model: sequential
end Current memory usage: 1638.66 MB
Iteration 111/500
start Current memory usage: 1620.30 MB
Training model: sequential
end Current memory usage: 1642.81 MB
Iteration 112/500
start Current memory usage: 1642.94 MB
Training model: sequential
end Current memory usage: 1659.45 MB
Iteration 113/500
start Current memory usage: 1655.17 MB
Training model: sequential
end Current memory usage: 1687.80 MB
Iteration 114/500
start Current memory usage: 1673.33 MB
Training model: sequential
end Current memory usage: 1705.94 MB
Iteration 115/500
start Current memory usage: 1699.95 MB
Training model: sequential
end Current memory usage: 1708.22 MB
Iteration 116/500
start Current memory usage: 1707.88 MB
Training model: sequential
end Current memory usage: 1648.23 MB
Iteration 117/500
start Current memory usage: 1634.03 MB
Training model: sequential
end Current memory usage: 1670.97 MB
Iteration 118/500
start Current memory usage: 1671.97 MB
Training model: sequential
end Current memory usage: 1649.69 MB
Iteration 119/500
start Current memory usage: 1645.14 MB
Training model: sequential
end Current memory usage: 1698.64 MB
Iteration 120/500
start Current memory usage: 1699.69 MB
Training model: sequential
end Current memory usage: 1737.67 MB
Iteration 121/500
start Current memory usage: 1737.67 MB
Training model: sequential
end Current memory usage: 1738.05 MB
Iteration 122/500
start Current memory usage: 1721.47 MB
Training model: sequential
end Current memory usage: 1730.64 MB
Iteration 123/500
start Current memory usage: 1729.53 MB
Training model: sequential
end Current memory usage: 1766.12 MB
Iteration 124/500
start Current memory usage: 1761.22 MB
Training model: sequential
end Current memory usage: 1796.58 MB
Iteration 125/500
start Current memory usage: 1796.73 MB
Training model: sequential
end Current memory usage: 1709.02 MB
Iteration 126/500
start Current memory usage: 1721.67 MB
Training model: sequential
end Current memory usage: 1771.50 MB
Iteration 127/500
start Current memory usage: 1771.50 MB
Training model: sequential
end Current memory usage: 1777.38 MB
Iteration 128/500
start Current memory usage: 1757.58 MB
Training model: sequential
end Current memory usage: 1806.50 MB
Iteration 129/500
start Current memory usage: 1758.81 MB
Training model: sequential
end Current memory usage: 1812.45 MB
Iteration 130/500
start Current memory usage: 1812.86 MB
Training model: sequential
end Current memory usage: 1811.14 MB
Iteration 131/500
start Current memory usage: 1799.61 MB
Training model: sequential
end Current memory usage: 1835.33 MB
Iteration 132/500
start Current memory usage: 1716.38 MB
Training model: sequential
end Current memory usage: 1759.75 MB
Iteration 133/500
start Current memory usage: 1752.44 MB
Training model: sequential
end Current memory usage: 1818.41 MB
Iteration 134/500
start Current memory usage: 1811.42 MB
Training model: sequential
end Current memory usage: 1853.58 MB
Iteration 135/500
start Current memory usage: 1853.70 MB
Training model: sequential
end Current memory usage: 1858.50 MB
Iteration 136/500
start Current memory usage: 1858.56 MB
Training model: sequential
end Current memory usage: 1874.84 MB
Iteration 137/500
start Current memory usage: 1862.92 MB
Training model: sequential
end Current memory usage: 1768.23 MB
Iteration 138/500
start Current memory usage: 1762.73 MB
Training model: sequential
end Current memory usage: 1843.39 MB
Iteration 139/500
start Current memory usage: 1843.52 MB
Training model: sequential
end Current memory usage: 1885.88 MB
Iteration 140/500
start Current memory usage: 1885.95 MB
Training model: sequential
end Current memory usage: 1924.86 MB
Iteration 141/500
start Current memory usage: 1925.05 MB
Training model: sequential
end Current memory usage: 1946.80 MB
Iteration 142/500
start Current memory usage: 1946.69 MB
Training model: sequential
end Current memory usage: 1977.53 MB
Iteration 143/500
start Current memory usage: 1974.27 MB
Training model: sequential
end Current memory usage: 1995.17 MB
Iteration 144/500
start Current memory usage: 1992.41 MB
Training model: sequential
end Current memory usage: 1984.45 MB
Iteration 145/500
start Current memory usage: 1963.42 MB
Training model: sequential
end Current memory usage: 1947.31 MB
Iteration 146/500
start Current memory usage: 1944.47 MB
Training model: sequential
end Current memory usage: 1996.00 MB
Iteration 147/500
start Current memory usage: 1996.08 MB
Training model: sequential
end Current memory usage: 2008.41 MB
Iteration 148/500
start Current memory usage: 1999.69 MB
Training model: sequential
end Current memory usage: 1951.30 MB
Iteration 149/500
start Current memory usage: 1942.98 MB
Training model: sequential
end Current memory usage: 1992.28 MB
Iteration 150/500
start Current memory usage: 1982.86 MB
Training model: sequential
end Current memory usage: 2008.83 MB
Iteration 151/500
start Current memory usage: 2008.83 MB
Training model: sequential
end Current memory usage: 1946.42 MB
Iteration 152/500
start Current memory usage: 1946.92 MB
Training model: sequential
end Current memory usage: 1992.48 MB
Iteration 153/500
start Current memory usage: 1979.52 MB
Training model: sequential
end Current memory usage: 2035.66 MB
Iteration 154/500
start Current memory usage: 2023.91 MB
Training model: sequential
end Current memory usage: 2030.31 MB
Iteration 155/500
start Current memory usage: 1974.39 MB
Training model: sequential
end Current memory usage: 2029.30 MB
Iteration 156/500
start Current memory usage: 1997.42 MB
Training model: sequential
end Current memory usage: 2000.31 MB
Iteration 157/500
start Current memory usage: 1964.38 MB
Training model: sequential
end Current memory usage: 1979.45 MB
Iteration 158/500
start Current memory usage: 1973.12 MB
Training model: sequential
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF 2.18']",2024-11-25T12:12:55Z,7,1,https://github.com/tensorflow/tensorflow/issues/80753
tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.SparseConcat`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, `SparseConcat` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

indices1 = tf.constant(2, shape=[3,3], dtype=tf.int64)
values1 = tf.constant(""aaaabaaacaaadaaaeaaafaaagaaahaaaiaaajaaakaaalaaamaaanaaaoaaapaaaqaaaraaasaaataaauaaavaaawaaaxaaayaaazaabbaabcaabdaabeaabfaabgaabhaabiaabjaabkaablaabmaabnaaboaabpaabqaabraabsaabtaabuaabvaabwaabxaabyaabzaacbaaccaacdaaceaacfaacgaachaaciaacjaackaaclaacmaacnaacoaacpaacqaacraacsaactaacuaacvaacwaacxaacyaac"",
    shape=[3], dtype=tf.string)
shapes1 = tf.constant([5, 2, 2147483647], dtype=tf.int64)

indices2 = tf.constant(-2, shape=[4,3], dtype=tf.int64)
values2 = tf.constant("" "", shape=[4], dtype=tf.string)
shapes2 = tf.constant([5,1879048192,536870912], dtype=tf.int64)

concat_dim = 1
tf.raw_ops.SparseConcat(
    indices=[indices1, indices2], values=[values1, values2], shapes=[shapes1, shapes2], concat_dim=concat_dim, name=None
)
```


### Relevant log output

```shell
2024-11-24 06:36:10.994508: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements() status: INVALID_ARGUMENT: Shape [5,1879048194,2147483647] results in overflow when computing number of elements
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-24T06:40:09Z,3,0,https://github.com/tensorflow/tensorflow/issues/80669
tensorflow/tensorflow,Floating point exception (core dumped) in `tf.raw_ops.Reshape`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs,  `tf.raw_ops.Reshape` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tensor = tf.constant(-3.5e+35, shape=[5], dtype=tf.float32)
shape = tf.constant([0, 1879048192, 100000000, 1610612736, -1], dtype=tf.int32)

tf.raw_ops.Reshape(tensor=tensor, shape=shape)
```


### Relevant log output

```shell
Floating point exception (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-22T02:58:01Z,2,0,https://github.com/tensorflow/tensorflow/issues/80529
tensorflow/tensorflow,The documentation in `data_performance.ipynb` uses `py_function()` without an explanation,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.18

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the [""Better performance with the tf.data API"" guide](https://www.tensorflow.org/guide/data_performance), `tf.py_function()` is used several times in the mapping function. There are various performance boosts demonstrated by the guide with said mapping functions. However, there does not seem to be any practical reason why `tf.py_function()` is used inside the mapping functions. In fact, if you remove those, the behavior is the same; in other words, there doesn't seem to be a need for them at all.

Curiously, if you remove them from the examples and then perform the time measurements, the speedup goes away. For example, consider the following mapping function from [the guide](https://github.com/tensorflow/docs/blob/bbc0b9c70fc0bd4411793d1b0bcc56ef1dbc2405/site/en/guide/data_performance.ipynb#L447-L450):

```python
def mapped_function(s):
    # Do some hard pre-processing
    tf.py_function(lambda: time.sleep(0.03), [], ())
    return s
```

In the sequential case, it is shown that the following mapping code results in a time measurement of 0.49222556600034295:

```python
benchmark(
    ArtificialDataset()
    .map(mapped_function)
)
```

And, the optimized (parallel) version as follows results in a time measurement of 0.36392719900049997:

```python
benchmark(
    ArtificialDataset()
    .map(
        mapped_function,
        num_parallel_calls=tf.data.AUTOTUNE
    )
)
```

But, if I remove the `tf.py_function()` from the mapping function, I get comparable measurements from both examples, namely, 0.22448736599994845 and 0.2266392660001202:

```python
def mapped_function(s):
    # Do some hard pre-processing
    lambda: time.sleep(0.03), [], ()
    return s
```

In fact, that measurement is even better, which makes me believe that this example is contrived to show a performance benefit by using `num_parallel_calls` when in fact TF is already optimizing the code without it. Frivolously wrapping the function in `tf.py_function()` is most likely  causing TensorFlow *not* to optimize the function. Thus, is `num_parallel_calls` even needed to achieve better performance?

### Standalone code to reproduce the issue

```shell
Mentioned in the above description.
```


### Relevant log output

_No response_","['type:docs-bug', 'stat:awaiting tensorflower', 'type:bug']",2024-11-20T16:01:24Z,3,0,https://github.com/tensorflow/tensorflow/issues/80365
tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.MatrixSolve`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the matrix argument is empty and the gpu is available, tf.raw_ops.MatrixSolve triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.MatrixSolve(matrix=tf.random.uniform([], dtype=tf.dtypes.double, maxval=1000000000), rhs=tf.random.uniform([1, 2], dtype=tf.dtypes.double, maxval=1000000000), adjoint=True)
```


### Relevant log output

```shell
2024-11-20 14:51:08.714846: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 14:51:08.775383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 14:51:08.852267: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 14:51:08.876168: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 14:51:08.931206: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 14:51:16.385650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 14:51:16.387914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 14:51:16.542383: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-20T06:53:21Z,2,0,https://github.com/tensorflow/tensorflow/issues/80331
tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.MatrixInverse`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the input argument is empty and the gpu is available, tf.raw_ops.MatrixInverse triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.MatrixInverse(input=tf.cast(tf.random.uniform([], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex128),adjoint=True)
```


### Relevant log output

```shell
2024-11-20 10:46:15.940818: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 10:46:16.001155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 10:46:16.076386: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 10:46:16.100080: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 10:46:16.154057: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 10:46:23.889652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21903 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 10:46:23.891964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 10:46:24.756574: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-20T02:47:58Z,2,0,https://github.com/tensorflow/tensorflow/issues/80316
tensorflow/tensorflow,model.fit fails when the number of rows exceeds Int32.MaxValue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19.0-dev20241117

### Custom code

Yes

### OS platform and distribution

MacOS 15.1.0

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I would expect model.fit to handle training on extremely large NumPy arrays without limitations.

### Standalone code to reproduce the issue

```shell
import numpy as np
from keras import Sequential
from keras.layers import Dense

n = 2_147_483_648
x = np.zeros(n).astype(np.float32)
y = x

model = Sequential([
    Dense(64, activation=""relu"", input_shape=(1,)),
    Dense(1, activation=""sigmoid"")
])
model.compile(optimizer=""adam"", loss=""binary_crossentropy"")
model.fit(x=x,y=y, epochs=1, batch_size=1024, verbose=1)
```


### Relevant log output

```shell
ValueError: Invalid value in tensor used for shape: -2147483648
```
","['type:bug', 'TF 2.18']",2024-11-19T09:24:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/80241
tensorflow/tensorflow,tf.range still miss some dtypes support,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

No

### Python version

Google Colab default

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Same issue as in https://github.com/tensorflow/tensorflow/issues/72365 but now with unsigned dtypes

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.range(10, delta=1, dtype='uint8')
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-7b6ccd0e0a16> in <cell line: 3>()
      1 import tensorflow as tf
      2 
----> 3 tf.range(10, delta=1, dtype='uint8')

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6000 def raise_from_not_ok_status(e, name) -> NoReturn:
   6001   e.message += ("" name: "" + str(name if name is not None else """"))
-> 6002   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   6003 
   6004 

InvalidArgumentError: Value for attr 'Tidx' of uint8 is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64, uint16, uint32
	; NodeDef: {{node Range}}; Op<name=Range; signature=start:Tidx, limit:Tidx, delta:Tidx -> output:Tidx; attr=Tidx:type,default=DT_INT32,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT16, DT_UINT32]> [Op:Range] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-11-14T17:40:39Z,1,0,https://github.com/tensorflow/tensorflow/issues/80039
tensorflow/tensorflow,tf.cast to int8 produce wrong number,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.15 - 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

input is a long list include random numbers.
tf.cast to int8 output is  different from numpy cast.

The strange thing is if you truncate the long input list to a short list then things works fine.

The code is straight forward, you can reproduce it in the colab

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/18dYjvY6JQk79hVq8JsjcWEwIG5KBuF1v?usp=sharing
```


### Relevant log output

```shell
NumPy Casted values (int8): 
 [   0    0    0    0    1   -1    0    3   -3    2   -2    0   50   21
 -125   62   25   31  127   -9  117   61  123   47  -20   28   52   36
  -43  -45  -84  118   37  -17   -6  -79    1   75  -45  -60 -103  -63
   85 -112   76   96  -56   86  -32 -108 -105 -121   -2  121   86  -54
   91  -55   36 -119   -3  -36   95  127 -105  -60   37   -9 -106    7
  -31  105   13 -103 -123   79   17  -48 -108  -56  -87 -128   35  -94
  -45  118  -91   86  -63  -43   77    1 -127  -16  -71  -73  -76  -15
  -11]
TensorFlow Casted values (int8): 
 [   0 -128    0    0    1   -1    0    3   -3    2   -2    0  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  -76  -15
  -11]
```
```
","['type:bug', 'comp:apis', '2.17']",2024-11-08T11:33:35Z,2,0,https://github.com/tensorflow/tensorflow/issues/79689
tensorflow/tensorflow,Unable to register CUDA plug-ins runnung docker image latest-gpu-jypyter,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

docker desktop 4.35.1 , ubuntu 24.04.1 LTS, WSL 2.3.24.0

### Mobile device

None

### Python version

Python 3.11.0rc1 (provided by tensorflow docker image)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.7

### GPU model and memory

NVIDIA GeForce RTX 4060 Ti 16G

### Current behavior?

I Strictly followed the instructions provided in:
https://www.tensorflow.org/install/docker 
https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
Got correct results running a sample workload (as suggested in nvidia contaner toolkit installation manual)

![image](https://github.com/user-attachments/assets/6f8c74e6-e23d-4395-8ffc-314f69af5bc7)

Downloaded tensorflow/tensorflow latest-gpu-jupyter image and ran the container.
Opend a new jupyter notebook (http://127.0.0.1:8888/tree?token=...)
Importing tensorflow I wanted to check the GPU support.
Got error messages and empy available gpu list.
`2024-11-06 10:31:50.143673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-06 10:31:50.712357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.__version__

print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```


### Relevant log output

```shell
2024-11-06 10:31:50.143673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-06 10:31:50.712357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
'2.18.0'

Num GPUs Available:  0
2024-11-06 10:31:54.748888: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (34)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.18']",2024-11-06T11:15:27Z,7,0,https://github.com/tensorflow/tensorflow/issues/79520
tensorflow/tensorflow,`tf.math.floormod` not throwing `Integer division by zero` error on GPU for tensor of int64 dtype,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

_No response_

### Current behavior?

Running `tf.math.floormod` with a tensor containing zeroes in the denominator tensor on GPU does not throw a division by zero error when the tensor has a dtype of `int64`.

[colab](https://colab.research.google.com/drive/1e053_hcmu0sHQcMPop-_WZR6ya1bw6dy?usp=sharing)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
A = tf.constant([[2,2],[2,2]], dtype=tf.int64)
B = tf.constant([[0,0],[0,0]], dtype=tf.int64)

with tf.device(""/gpu:0""):
    output_gpu = tf.math.floormod(A, B) # No error
    print(f""\nGPU: {output_gpu}\n"") # GPU: [[2 2] [2 2]]

with tf.device(""/cpu:0""):
    output_cpu = tf.math.floormod(A, B) 
    # InvalidArgumentError: Integer division by zero
```


### Relevant log output

```shell
GPU: [[2 2]
 [2 2]]

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-2-ba17e88e7095> in <cell line: 9>()
      8 
      9 with tf.device(""/cpu:0""):
---> 10     output_cpu = tf.math.floormod(A, B)
     11     # InvalidArgumentError: Integer division by zero

2 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   5981 def raise_from_not_ok_status(e, name) -> NoReturn:
   5982   e.message += ("" name: "" + str(name if name is not None else """"))
-> 5983   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   5984 
   5985 

InvalidArgumentError: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:CPU:0}} Integer division by zero [Op:FloorMod] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-11-01T00:42:37Z,1,0,https://github.com/tensorflow/tensorflow/issues/79162
tensorflow/tensorflow,`tf.linalg.lstsq` producing outputs with large inconsistencies between CPU and GPU with `float32` tensors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

_No response_

### Current behavior?

Passing tensors of `float32` to 'tf.linalg.lstsq' is producing very different output from CPU and GPU.

### Standalone code to reproduce the issue
[colab](https://colab.research.google.com/drive/1Fyh3HQs39NhGlOLEdzpqPKVkznBe6Fe9?usp=sharing)
```shell
import tensorflow as tf
import numpy as np

A = tf.constant([[[[0.37454012, 0.9507143, 0.7319939, 0.5986585, 0.15601864], [0.15599452, 0.05808361, 0.8661761, 0.601115, 0.7080726,], [0.02058449, 0.96990985, 0.83244264,
                0.21233912, 0.18182497], [0.1834045, 0.30424225, 0.52475643, 0.43194503, 0.29122913], [0.6118529, 0.13949387, 0.29214466, 0.36636186, 0.45606998]]]], dtype=tf.float32)

B = tf.constant([[[[0.59241456, 0.04645041], [0.94888556, 0.965632,], [0.684233, 0.4401525,], [
                0.9093204, 0.25877997], [0.54671025, 0.18485446]]]], dtype=tf.float32)

with tf.device(""/gpu:0""):
    output_gpu = tf.linalg.lstsq(A, B)
    
with tf.device( ""/cpu:0""):
    output_cpu = tf.linalg.lstsq(A, B)

np.testing.assert_allclose(output_cpu, output_gpu.cpu(), atol=100) # AssertionError
```


### Relevant log output

```shell
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=100

Mismatched elements: 2 / 10 (20%)
Max absolute difference: 112.95715
Max relative difference: 0.2963447
 x: array([[[[-170.69518 ,   28.379017],
         [ 164.85085 ,  -28.04222 ],
         [-273.4264  ,   46.952198],...
 y: array([[[[-241.07059 ,   40.25023 ],
         [ 232.80626 ,  -39.505226],
         [-386.38354 ,   66.00629 ],...
```
","['type:bug', 'comp:ops', 'TF 2.18']",2024-10-31T22:52:23Z,1,0,https://github.com/tensorflow/tensorflow/issues/79157
tensorflow/tensorflow,Thread ID in TensorBoard Profiler Trace Viewer Could Be Negative,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17

### Custom code

No

### OS platform and distribution

Linux Mint 21.2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

6.5.0

### GCC/compiler version

clang 14.0.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

While using TensorFlow's profiler, I found that some thread IDs displayed in the Trace Viewer are negative. As shown in the image below, thread names such as `tf_Compute/-1030865605`, `tf_data_private_threadpool/-102013...`, etc., have negative thread IDs.

![image](https://github.com/user-attachments/assets/71d32241-d617-4e52-bcbf-d3140ebef440)

My log file is also attached.

[20241030-161104.zip](https://github.com/user-attachments/files/17587809/20241030-161104.zip)

### Standalone code to reproduce the issue

```shell
(Just the profiler demo code) https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras
```


### Relevant log output

_No response_","['type:bug', 'awaiting PR merge', '2.17']",2024-10-31T13:36:36Z,2,0,https://github.com/tensorflow/tensorflow/issues/79128
tensorflow/tensorflow,Significant Discrepancy in `tf.linalg.triangular_solve` Results Between CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.4 LTS x86_64

### Mobile device

_No response_

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using `tf.linalg.triangular_solve` with large matrices or specific triangular matrix conditions (e.g., `upper=True`, `transpose=True`, `unitriangular=True`), the GPU results significantly differ from the CPU results. 

The discrepancy includes extremely large Mean Absolute Error (MAE) values and infinite Mean Squared Error (MSE) values of the results, indicating a possible issue in the GPU implementation of the function.

### Standalone code to reproduce the issue

[colab](https://colab.research.google.com/drive/1zXcOAkxHV6snaMyWGMeKIukf5IVBd0Bh?usp=sharing)
[Safe Tensors](https://drive.google.com/file/d/1n1JyugXNAS9M2wmnk9u9vCVuHz0uE3R6/view?usp=sharing)

```python
import tensorflow as tf
import numpy as np
from safetensors.torch import load_file

def set_seed(seed_value=42):
    """"""Sets the random seed for reproducibility.""""""
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)

def tensorflow_version(input, cpu=True):
    set_seed()
    if cpu:
        device_string = ""/cpu:0""
    else:
        device_string = ""/gpu:0""

    with tf.device(device_string):
        b_tensor = tf.constant(input[""b""])
        A_tensor = tf.constant(input[""A""])

        upper = input.get(""upper"", True)
        transpose = input.get(""transpose"", False)
        unitriangular = input.get(""unitriangular"", False)

        solution = tf.linalg.triangular_solve(
            A_tensor, b_tensor, lower=not upper, adjoint=transpose
        )

        return {""triangular_solve_solution"": solution.numpy()}

def load_safe_tensor(file_path):
    safe_tensor_data = load_file(file_path)
    A_tensor = safe_tensor_data[""A""]
    b_tensor = safe_tensor_data[""b""]

    return {
        ""A"": tf.convert_to_tensor(A_tensor.numpy()),
        ""b"": tf.convert_to_tensor(b_tensor.numpy()),
    }

def calculate_differences(cpu_result, gpu_result):
    diff = np.abs(cpu_result - gpu_result)
    mae = np.mean(diff)
    mse = np.mean(diff**2)
    rmse = np.sqrt(mse)
    max_diff = np.max(diff)
    mean_relative_diff = np.mean(diff / (np.abs(cpu_result) + 1e-10))

    return {
        ""Mean Absolute Error"": mae,
        ""Mean Squared Error"": mse,
        ""Root Mean Squared Error"": rmse,
        ""Maximum Absolute Difference"": max_diff,
        ""Mean Relative Difference"": mean_relative_diff,
    }

def main():
    file_path = ""tensorflow_triangular_solve_3.safetensors""
    input_data = load_safe_tensor(file_path)
    input_data[""upper""] = True
    input_data[""transpose""] = True
    input_data[""unitriangular""] = True

    result_cpu = tensorflow_version(input_data, cpu=True)
    print(""CPU Result:"")
    print(result_cpu)

    if tf.config.list_physical_devices(""GPU""):
        result_gpu = tensorflow_version(input_data, cpu=False)
        print(""GPU Result:"")
        print(result_gpu)

        if result_gpu:
            cpu_solution = result_cpu[""triangular_solve_solution""]
            gpu_solution = result_gpu[""triangular_solve_solution""]
            differences = calculate_differences(cpu_solution, gpu_solution)
            for key, value in differences.items():
                print(f""{key}: {value}"")
    else:
        print(""GPU not available."")

if __name__ == ""__main__"":
    main()
```

#### **Issue Summary:**
- The `tf.linalg.triangular_solve` function produces vastly different results on the GPU compared to the CPU. The differences include an enormous Mean Absolute Error (MAE) and an infinite Mean Squared Error (MSE), indicating a severe discrepancy between the CPU and GPU results.
- The issue appears to be related to the use of specific arguments like `upper=True`, `transpose=True`, and `unitriangular=True`, suggesting that there might be a numerical precision or stability issue in the GPU implementation.
- It is unclear why these large discrepancies occur, but they point to a critical inconsistency that could impact numerical reliability for users depending on TensorFlow’s triangular solve functionality.


### Relevant log output

```shell
Mean Absolute Error: 1.566790629150662e+21
Mean Squared Error: inf
Root Mean Squared Error: inf
Maximum Absolute Difference: 1.4265324671452624e+26
```
","['type:bug', 'comp:ops', 'TF 2.18']",2024-10-31T04:34:40Z,1,0,https://github.com/tensorflow/tensorflow/issues/79090
tensorflow/tensorflow,`lookup_ops.StaticVocabularyTable` and `lookup_ops.StaticVocabularyTableV1`  can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `segmentation fault issue` in TensorFlow when I used API `lookup_ops.StaticVocabularyTable` or `lookup_ops.StaticVocabularyTableV1` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1NGnFQqFl6Jy_Xt7wBL3ynmSVs9AiFw_l?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import os
from tensorflow.python.ops import lookup_ops

def _createVocabFile(basename, values=('brain', 'salad', 'surgery')):
    vocabulary_file = os.path.join(""/tmp"", basename)
    with open(vocabulary_file, 'w') as f:
        f.write('\n'.join(values) + '\n')
    return vocabulary_file

vocab_file = _createVocabFile('feat_to_id_1.txt')
vocab_size = 9223372036854775807
oov_buckets = 1
table = lookup_ops.StaticVocabularyTable(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)
#lookup_ops.StaticVocabularyTableV1(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-10-26T07:19:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/78831
tensorflow/tensorflow,"`gen_list_ops.tensor_list_concat_v2` aborts with ""Check failed: size >= 0 (0 vs. -1)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

Linux Ubuntu 20.04.3 LTS

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)

Please find the [[gist](https://colab.research.google.com/drive/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing)](https://colab.research.google.com/drive/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import gen_list_ops
from tensorflow.python.ops import list_ops

l = list_ops.tensor_list_reserve(element_dtype=dtypes.float32, element_shape=None, num_elements=3)
t = gen_list_ops.tensor_list_concat_v2(l, element_dtype=dtypes.float32, element_shape=list_ops._build_element_shape((None, 3)), leading_dims=[-1, 3, 5])
```


### Relevant log output

```shell
F tensorflow/core/framework/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-10-26T07:15:16Z,1,0,https://github.com/tensorflow/tensorflow/issues/78830
tensorflow/tensorflow,"`data_flow_ops.Barrier` aborts with ""Check failed: i >= 0 (0 vs. -100)"" ","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)

Please find the [gist](https://colab.research.google.com/drive/1OOOIvZ7brRDjRqshv36CA_55BlPbgI4e?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import data_flow_ops
from tensorflow.python.eager import context
import tensorflow as tf

with context.graph_mode():
    sess = tf.compat.v1.Session()
    with sess.as_default():
        b = data_flow_ops.Barrier((dtypes.float32, dtypes.float32), shapes=((), ()), name='B')
        keys = [b'a', b'b', b'c', b'd']
        values_0 = [10.0, 20.0, 30.0, 40.0]
        values_1 = [100.0, 200.0, 300.0, 400.0]
        insert_1_1_op = b.insert_many(-100, keys[0:2], values_1[0:2]) 
        insert_1_1_op.run()
```


### Relevant log output

```shell
F tensorflow/core/kernels/barrier_ops.cc:286] Check failed: i >= 0 (0 vs. -100)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-10-26T07:04:16Z,1,0,https://github.com/tensorflow/tensorflow/issues/78828
tensorflow/tensorflow,Does TFLite dequantize opertor support constant buffer input,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
<img width=""724"" alt=""image"" src=""https://github.com/user-attachments/assets/849a0952-90ce-42c4-b2dc-9a9e7333fb0b"">
The [tflite model for user input](https://github.com/fujunwei/tensorflow/blob/tflite_label_image/tensorflow/lite/examples/python/dequantize_input.tflite) can work as expected

<img width=""713"" alt=""image"" src=""https://github.com/user-attachments/assets/43cafc84-0ea6-4a06-84b4-4772c6cea8e8"">

The [tflite model with constant buffer](https://github.com/fujunwei/tensorflow/blob/tflite_label_image/tensorflow/lite/examples/python/dequantize_constant.tflite) can't compute, so does [dequantize](https://www.tensorflow.org/mlir/tfl_ops#tfldequantize_tfldequantizeop) support constant input?

**Any other info / logs**

You can also modify [dequantize_tester in the Repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/dequantize_tester.cc#L110) to reproduce this issue like below code:

1, Create a constant buffer
```
  const auto buffer_data = builder.CreateVector( reinterpret_cast<const uint8_t*>(constant_buffer.data()),
                                                                              constant_buffer.size());
  const auto buffer_index = base::checked_cast<uint32_t>(buffers.size());
  buffers.emplace_back(::tflite::CreateBuffer(builder, buffer_data));
```

2, Use the `buffer_index ` when creating tensor:
```
const std::array<flatbuffers::Offset<::tflite::Tensor>, 2> tensors{{
      ::tflite::CreateTensor(
          builder,
          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
          Unsigned() ? ::tflite::TensorType_UINT8 : ::tflite::TensorType_INT8,
          /*buffer=*/buffer_index , /*name=*/0,
          ::tflite::CreateQuantizationParameters(
              builder, /*min=*/0, /*max=*/0,
              builder.CreateVector<float>({InputScale()}),
              builder.CreateVector<int64_t>({InputZeroPoint()}))),
      ::tflite::CreateTensor(
          builder,
          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
          ::tflite::TensorType_FLOAT32,
          /*buffer=*/0, /*name=*/builder.CreateString(""dequantizeLinearOutput"")),
  }};

```","['stat:awaiting tensorflower', 'type:bug', 'comp:lite', 'ModelOptimizationToolkit', '2.17']",2024-10-25T08:28:42Z,4,0,https://github.com/tensorflow/tensorflow/issues/78748
tensorflow/tensorflow,No kernels registered for op `Conv2DBackpropInputV2`,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The existence of operator `Conv2DBackpropInputV2` is described in the official website document.https://www.tensorflow.org/api_docs/python/tf/raw_ops/Conv2DBackpropInputV2.
However, during my actual execution, the following error message appears:

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_sizes = tf.constant(1, shape=[4], dtype=tf.int32)
filter_tensor = tf.constant(2, shape=[4], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInputV2(
    input=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)
```


### Relevant log output

```shell
2024-10-21 12:30:18.492624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File ""/mnt/tests/Conv2DBackpropInput.py"", line 10, in <module>
    tf.raw_ops.Conv2DBackpropInputV2(
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/util/tf_export.py"", line 377, in wrapper
    return f(**kwargs)
           ^^^^^^^^^^^
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 2030, in conv2d_backprop_input_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Conv2DBackpropInputV2}} = Conv2DBackpropInputV2[T=DT_INT32, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true]
All kernels registered for op Conv2DBackpropInputV2:
  <no registered kernels>
 [Op:Conv2DBackpropInputV2] name:
```
","['type:docs-bug', 'stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-10-21T12:34:35Z,1,0,https://github.com/tensorflow/tensorflow/issues/78431
tensorflow/tensorflow,Overflow and Check fail in `tf.raw_ops.Conv2DBackpropInput`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Overflow : `input_sizes` is a one-dimensional tensor and contains maximum values
Check fail：`input_sizes`'s shape is [2] and The dimension of `filter` is less than 2

### Standalone code to reproduce the issue

```shell
Overflow:

import tensorflow as tf

input_sizes = tf.constant([1, 5, 5, 999999999999], shape=[4], dtype=tf.int32)
filter_tensor = tf.constant(3, shape=[3, 3, 3, 2], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInput(
    input_sizes=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)

Check fail:
```python
import tensorflow as tf

input_sizes = tf.constant(1, shape=[2], dtype=tf.int32)
filter_tensor = tf.constant(2, shape=[1], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInput(
    input_sizes=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)
```
```


### Relevant log output

```shell
Overflow:

2024-10-21 11:40:49.417611: F tensorflow/core/kernels/mkl/mkl_conv_grad_input_ops.cc:578] Non-OK-status: tensor::MakeShape(input_tensor, &input_tf_shape)
Status: INVALID_ARGUMENT: Dimension -727379969 must be >= 0
Aborted (core dumped)
```

Check fail:
```
2024-10-21 11:16:29.937265: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 1)
Aborted (core dumped)
```
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-21T12:17:20Z,3,0,https://github.com/tensorflow/tensorflow/issues/78428
tensorflow/tensorflow,Overflow in `tf.raw_ops.Fill`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Overflow in `tf.raw_ops.Fill` when there are too large values in `dims`.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1GDBN4lheNUIW704hsXVt3lW55UJue97S?usp=sharing
```


### Relevant log output

```shell
Kill
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-21T11:10:34Z,3,0,https://github.com/tensorflow/tensorflow/issues/78427
tensorflow/tensorflow,Multi-threaded execution throws an exception (using GPU).,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0-dev20241018

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Multi-threaded execution throws an exception (using GPU).

### Standalone code to reproduce the issue

```shell
import concurrent

import numpy as np
import tensorflow as tf
print(tf.__version__)

executor = concurrent.futures.ThreadPoolExecutor()


def sum(x, axis):
    return tf.reduce_sum(x, axis=axis)


futures = []

for i in range(1000):
    futures.clear()
    for _ in range(4):
        x = tf.convert_to_tensor(np.random.rand(100, 100))
        futures.append(executor.submit(sum, x, 1))
        x = tf.convert_to_tensor(np.random.rand(100))
        futures.append(executor.submit(sum, x, 0))
    concurrent.futures.wait(
        futures, return_when=concurrent.futures.ALL_COMPLETED
    )
    [future.result() for future in futures]
```


### Relevant log output

```shell
W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at reduction_ops_common.h:147 : INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)
I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)

```
```
","['type:bug', 'comp:gpu']",2024-10-19T13:02:32Z,5,0,https://github.com/tensorflow/tensorflow/issues/78338
tensorflow/tensorflow,[Incorrect Result] `tf.math.reciprocal` returns `NaN` on `inf` input on Linux.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17.0

### Custom code

No

### OS platform and distribution

AlmaLinux 9.4

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.math.reciprocal` returns `NaN` on Linux when input is `inf` or `-inf` and has dtype=complex128, shape >= 2.
The output is expected to be 0, since:
1. This behavior is not consistent with dtype=float64, where the output will be 0.
2. When input tensor contains only one value, the output will be 0.
3. The same code snippet will return different result on macOS, where the output is also 0.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

input = tf.constant(np.inf, dtype=tf.float64)
out = tf.math.reciprocal(input)
# tf.Tensor(0.0, shape=(), dtype=float64)
print(out)

input = tf.constant(np.inf, dtype=tf.complex128)
out = tf.math.reciprocal(input)
# tf.Tensor(0j, shape=(), dtype=complex128)
print(out)

input = tf.constant([np.inf, np.inf], dtype=tf.complex128)
out = tf.math.reciprocal(input)
# On Linux: tf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)
# On macOS: tf.Tensor([0.+0.j 0.+0.j], shape=(2,), dtype=complex128)
print(out)
```


### Relevant log output

```shell
AttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'
tf.Tensor(0.0, shape=(), dtype=float64)
tf.Tensor(0j, shape=(), dtype=complex128)
tf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-19T09:41:36Z,2,0,https://github.com/tensorflow/tensorflow/issues/78298
tensorflow/tensorflow,"tf.linalg.expm fails to support half/float16 data type, which is inconsistent with doc","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

According to the documentation: https://www.tensorflow.org/api_docs/python/tf/linalg/expm
tf.linalg.expm is expected to accept float16 input, but it fails on float16 when actually running the following code.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
input = tf.constant(np.random.randn(1,1), dtype='float16')
out = tf.linalg.expm(input)
```
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node MatrixSolve}} = MatrixSolve[T=DT_HALF, adjoint=false]
All kernels registered for op MatrixSolve:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
 [Op:MatrixSolve] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-17T13:09:08Z,3,0,https://github.com/tensorflow/tensorflow/issues/78107
tensorflow/tensorflow,DLPack with Int32 tensor on the GPU: inconsistent eager mode / graph mode / XLA,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-117097-gecf05620570 2.19.0-dev20241016

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello,

I realize that `int32` is a special dtype in TensorFlow for historical reasons. It seems that the handling of GPU int32-typed tensors has evolved over time.

Currently, the `device` field of a tensor created with:
```py
with tf.device('gpu'):
    x = tf.constant([0,1,2], tf.int32)
```
*does* indicate it's a GPU tensor: `/job:localhost/replica:0/task:0/device:GPU:0`.

However, when exporting and re-importing it via DLPack, it comes back as a CPU tensor.
There even seems to be a unit test validating this:
https://github.com/tensorflow/tensorflow/blob/d3de971a7348ecaefdbb920e580c37ebde10d780/tensorflow/python/dlpack/dlpack_test.py#L75-L78


However, @jhoydis found that this is *not* consistent between modes. In particular, if the tensor goes through an XLA-compiled function, it will correctly live on the GPU even after a round-trip through DLPack. (See reproducer below).

Would it please be possible to revisit this behavior, so that **exporting an int32 GPU tensor via DLPack does result in a GPU DLPack capsule in all modes, not just XLA?**

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


def f_eager(x):
    return x
f_graph = tf.function()(f_eager)
f_xla = tf.function(jit_compile=True)(f_eager)


with tf.device('gpu'):
    x = tf.constant([0,1,2], tf.int32)
    print(""Original tensor:"", x.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(x)
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Default:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_eager(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Eager:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_graph(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Graph:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_xla(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""XLA:"", x_.device)
```


### Relevant log output

```shell
Original tensor: /job:localhost/replica:0/task:0/device:GPU:0
Default: /job:localhost/replica:0/task:0/device:CPU:0
Eager: /job:localhost/replica:0/task:0/device:CPU:0
Graph: /job:localhost/replica:0/task:0/device:CPU:0
XLA: /job:localhost/replica:0/task:0/device:GPU:0
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'comp:gpu']",2024-10-17T08:59:51Z,1,0,https://github.com/tensorflow/tensorflow/issues/78091
tensorflow/tensorflow,tf.math.special.bessel_* has inconsistent result with scipy,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Based on the documentation, special function such as bessel_y0 should have consistent result with scipy. However, when receiving `-inf`, it has inconsistent results with scipy. Please check the reproducible for details.

### Standalone code to reproduce the issue

```shell
import scipy
import numpy as np
import tensorflow as tf
x = tf.constant(-np.inf, dtype='float64')
print(""TF:"", tf.math.special.bessel_y1(x))
print(""Scipy: "", scipy.special.y1(x))
print(""TF:"", tf.math.special.bessel_y0(x))
print(""Scipy: "", scipy.special.y0(x))
print(""TF:"", tf.math.special.bessel_k0(x))
print(""Scipy: "", scipy.special.k0(x))
print(""TF:"", tf.math.special.bessel_k1(x))
print(""Scipy: "", scipy.special.k1(x))
```


### Relevant log output

```shell
TF: tf.Tensor(-inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(-inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(inf, shape=(), dtype=float64)
Scipy:  nan
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-14T15:45:19Z,1,0,https://github.com/tensorflow/tensorflow/issues/77864
tensorflow/tensorflow,"tf.math.is_strictly_increasing's behavior is not clear on a (2,2) matrix","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When receiving this input:
```
x = tf.constant([[1,2],[2,3]])
```
`tf.math.is_strictly_increasing` outputs `False` instead of `True`.
Also, for a tensor with shape (1,3,3):
```
x = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],
  [-0.5704009,  -0.2167283,   0.2548743 ],
  [-0.14944994,  2.0107825,  -0.09678416]]])
```
It's output is still `False` instead of `True` even when x's first dimension only has one element.
Based on the description ""Elements of x are compared in row-major order."", it seems that elements in x are compared along row (i.e., the first dimension).
Therefore, to my understanding, if the first dimension contains only one element (such as 1x3x3 shape tensor), the output should be True. If the input is [[1,2],[3,4]], the output should also be `True` since the value in the first dimension is increasing (from `[1,2]` to `[3,4]`)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([[1,2],[2,3]])
print(tf.math.is_strictly_increasing(x))  # False
x = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],
  [-0.5704009,  -0.2167283,   0.2548743 ],
  [-0.14944994,  2.0107825,  -0.09678416]]])
print(tf.math.is_strictly_increasing(x))  # False
```
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-14T15:28:21Z,2,0,https://github.com/tensorflow/tensorflow/issues/77863
tensorflow/tensorflow,argmax returns incorrect result for input containing Minimum number (TensorFlow 2.x),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Current Behavior:
When using tf.math.argmax on an input array that contains -0.0, the result is incorrect. Specifically, the function returns 1 (the index of -0.0) as the position of the maximum value, while the actual maximum value is 1.401298464324817e-45 at index 2.

The same behavior is observed in Keras and JAX, as both use TensorFlow internally for the argmax function.

Expected Behavior:
tf.math.argmax should return 2, as the value at index 2 (1.401298464324817e-45) is greater than both -1.0 and -0.0.
```
import numpy as np
import torch
import tensorflow as tf
import jax.numpy as jnp
from tensorflow import keras

def test_argmax():
    # Input data
    input_data = np.array([-1.0, -0.0, 1.401298464324817e-45], dtype=np.float32)

    # PyTorch argmax
    pytorch_result = torch.argmax(torch.tensor(input_data, dtype=torch.float32)).item()
    print(f""PyTorch argmax result: {pytorch_result}"")

    # TensorFlow argmax
    tensorflow_result = tf.math.argmax(input_data).numpy()
    print(f""TensorFlow argmax result: {tensorflow_result}"")

    # Keras argmax (Keras internally uses TensorFlow, so should be the same)
    keras_result = keras.backend.argmax(input_data).numpy()
    print(f""Keras argmax result: {keras_result}"")

    # JAX argmax
    jax_result = jnp.argmax(input_data)
    print(f""JAX argmax result: {jax_result}"")

if __name__ == ""__main__"":
    test_argmax()

```


### Standalone code to reproduce the issue

```shell
PyTorch argmax result: 2
TensorFlow argmax result: 1
Keras argmax result: 1
JAX argmax result: 1

```
```


### Relevant log output

_No response_","['type:bug', 'comp:ops', 'TF 2.16']",2024-10-14T10:07:59Z,3,0,https://github.com/tensorflow/tensorflow/issues/77853
tensorflow/tensorflow,argsort incorrectly handles very small floating-point numbers and -0.0 compared to other libraries (PyTorch and JAX),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using TensorFlow's argsort function on an array containing small floating-point numbers and both 0.0 and -0.0, the sort order is incorrect compared to other deep learning libraries such as PyTorch and JAX. TensorFlow incorrectly places 1.401298464324817e-45 (a very small positive number) before 0.0 and -0.0.

Expected behavior is that both 0.0 and -0.0 should be treated as equivalent and placed before any positive number, including very small ones like 1.401298464324817e-45. However, TensorFlow does not follow this behavior, whereas PyTorch correctly handles this.
```
import numpy as np
import torch
import tensorflow as tf
import jax.numpy as jnp

def test_argsort():
    # Input data, hardcoded as float32
    input_data = np.array([
        -0.0, 1.401298464324817e-45, 1.100000023841858, -0.0,
        5.960464477539063e-08, -2.0000100135803223, 1000000.0,
        722801.375, 0.0, -1.100000023841858
    ], dtype=np.float32)

    # PyTorch argsort
    pytorch_result = torch.argsort(torch.tensor(input_data, dtype=torch.float32)).numpy()
    print(f""PyTorch argsort result: {pytorch_result}"")

    # TensorFlow argsort
    tensorflow_result = tf.argsort(input_data).numpy().astype(np.int32)
    print(f""TensorFlow argsort result: {tensorflow_result}"")

    # JAX argsort
    jax_result = jnp.argsort(input_data).astype(np.int32)
    print(f""JAX argsort result: {jax_result}"")

if __name__ == ""__main__"":
    test_argsort()

```

### Standalone code to reproduce the issue

```shell
PyTorch argsort result: [5 9 0 3 8 1 4 2 7 6]
TensorFlow argsort result: [5 9 0 1 3 8 4 2 7 6]
JAX argsort result: [5 9 0 1 3 8 4 2 7 6]
```
Expected Behavior:
TensorFlow's argsort should place 0.0 and -0.0 before any positive number, including very small values like 1.401298464324817e-45. PyTorch demonstrates the correct behavior by treating 0.0 and -0.0 as equal and placing them in the correct order relative to other values.

Standalone Code to Reproduce the Issue:
The above Python code demonstrates the issue. It uses the same input data for PyTorch, TensorFlow, and JAX to show the difference in behavior. TensorFlow and JAX produce incorrect results by misplacing the small positive value before 0.0, while PyTorch produces the correct order.

Relevant Log Output:
No error logs are generated, but the incorrect behavior is clearly shown in the sorting results.
```


### Relevant log output

_No response_","['type:bug', 'TF 2.16']",2024-10-14T09:02:22Z,2,0,https://github.com/tensorflow/tensorflow/issues/77849
tensorflow/tensorflow,Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT3D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If the value contained in fft_length is the maximum value, it will cause an abort

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant(0, shape=[2,0,0,0] ,dtype=tf.complex64)
fft_length = tf.constant(1879048192, shape=[3], dtype=tf.int32)

tf.raw_ops.IRFFT3D(input=input, fft_length=fft_length)
```


### Relevant log output

```shell
2024-10-13 13:04:53.308156: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()
Status: INVALID_ARGUMENT: Shape [2,1879048192,1879048192,1879048192] results in overflow when computing number of elements
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-13T13:07:08Z,1,0,https://github.com/tensorflow/tensorflow/issues/77824
tensorflow/tensorflow,Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT2D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Since the value in fft_length is a maximum value, it will cause abort

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant(0, shape=[1,4,10,0,0] ,dtype=tf.complex64)
fft_length = tf.constant(2147483647, shape=[2], dtype=tf.int32)

tf.raw_ops.IRFFT2D(input=input, fft_length=fft_length)
```


### Relevant log output

```shell
2024-10-13 12:59:11.295197: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()
Status: INVALID_ARGUMENT: Shape [1,4,10,2147483647,2147483647] results in overflow when computing number of elements
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-13T13:04:05Z,1,0,https://github.com/tensorflow/tensorflow/issues/77823
tensorflow/tensorflow,Gradients of tf.linalg.expm not supported with JIT compilation,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tested 2.17 and 2.10, both have the issue

### Custom code

Yes

### OS platform and distribution

Ubuntu

### Mobile device

_No response_

### Python version

tested 3.9 and 3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Gradients of `tf.linalg.expm` can not be computed with JIT compilation. 

This is an issue, because tf 2.17 seems to have activated jit compilation for compiled models per default whereas earlier versions did not, breaking existing code.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

A = tf.Variable([[.4, 1.5], [.6, .1]], dtype=tf.float32)

@tf.function(jit_compile=True) #set jit_compile=False to make it work
def f(A):
    with tf.GradientTape() as tape:
        B = tf.linalg.expm(A)
    return tape.gradient(B, A)

f(A)
```


### Relevant log output

```shell
2024-10-11 11:17:27.281304: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: XLA compilation requires a fixed tensor list size. Set the max number of elements. This could also happen if you're using a TensorArray in a while loop that does not have its maximum_iteration set, you can fix this by setting maximum_iteration to a suitable value.

Stack trace for op definition: 
File ""<frozen runpy>"", line 198, in _run_module_as_main
File ""<frozen runpy>"", line 88, in _run_code
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel_launcher.py"", line 18, in <module>
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/traitlets/config/application.py"", line 1075, in launch_instance
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelapp.py"", line 739, in start
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tornado/platform/asyncio.py"", line 205, in start
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py"", line 641, in run_forever
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py"", line 1986, in _run_once
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/events.py"", line 88, in _run
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 534, in process_one
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py"", line 362, in execute_request
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"",
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-11T12:31:35Z,1,0,https://github.com/tensorflow/tensorflow/issues/77693
tensorflow/tensorflow,tf.custom_gradient for function with kwarg shows unexpected behavior,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

3.12.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3/8

### GPU model and memory

_No response_

### Current behavior?

I have a function that takes two tensors as inputs, one as argument and one as keyword argument.
The function has a custom gradient.

When ``tape.gradient`` for both input tensors with respect to the output of the function is called, TensorFlow throws an error, saying that only one gradient is expect and not two.

When the function is called with both inputs as arguments (and not one of them as kwarg), no error is thrown.


### Standalone code to reproduce the issue

```shell
@tf.custom_gradient
def func(x, y=0):
    z = 2*x + y
    def grad(dz):
        dx = 2*dz
        dy = dz
        return dx, dy
    return z, grad
x = tf.constant(2.)
y = tf.constant(3.)
with tf.GradientTape() as tape:
    tape.watch([x, y])
    z = func(x, y=y) #func(x, y) does not generate the error
grads = tape.gradient(z, [x, y])
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[129], line 14
     12     tape.watch([x, y])
     13     z = func(x, y=y)
---> 14 grads = tape.gradient(z, [x, y])

File ~/.local/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py:1066, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients)
   1060   output_gradients = (
   1061       composite_tensor_gradient.get_flat_tensors_for_gradients(
   1062           output_gradients))
   1063   output_gradients = [None if x is None else ops.convert_to_tensor(x)
   1064                       for x in output_gradients]
-> 1066 flat_grad = imperative_grad.imperative_grad(
   1067     self._tape,
   1068     flat_targets,
   1069     flat_sources,
   1070     output_gradients=output_gradients,
   1071     sources_raw=flat_sources_raw,
   1072     unconnected_gradients=unconnected_gradients)
   1074 if not self._persistent:
   1075   # Keep track of watched variables before setting tape to None
   1076   self._watched_variables = self._tape.watched_variables()

File ~/.local/lib/python3.12/site-packages/tensorflow/python/eager/imperative_grad.py:67, in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
     63 except ValueError:
     64   raise ValueError(
     65       ""Unknown value for unconnected_gradients: %r"" % unconnected_gradients)
---> 67 return pywrap_tfe.TFE_Py_TapeGradient(
     68     tape._tape,  # pylint: disable=protected-access
     69     target,
     70     sources,
     71     output_gradients,
     72     sources_raw,
     73     compat.as_str(unconnected_gradients.value))

File ~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:588, in _eager_mode_decorator.<locals>.actual_grad_fn(*result_grad_components)
    585 flat_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(
    586     nest.flatten(input_grads))
    587 if len(flat_grads) != arg_count:
--> 588   raise ValueError(
    589       f""custom_gradient function expected to return {arg_count} ""
    590       f""gradients, but returned {len(flat_grads)} instead."")
    591 return flat_grads + variable_grads

ValueError: custom_gradient function expected to return 1 gradients, but returned 2 instead.
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-10T10:18:54Z,1,0,https://github.com/tensorflow/tensorflow/issues/77559
tensorflow/tensorflow,Backward compatibility issue: failure to load models saved in TensorFlow format (Keras 2) in TensorFlow 2.17,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.9.1 (model saved), 2.17.0 (model loaded)

### Custom code

Yes

### OS platform and distribution

(Official Docker Image) Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.8.10 (model saved),  3.11.0rc1 (model loaded)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

## Description

I have encountered a backward compatibility issue when loading models saved with Keras 2 in TensorFlow 2.9 into TensorFlow 2.17, which now uses Keras 3 API. This issue impacts various loading methods, and there does not appear to be a straightforward solution to resolve the errors.

## Steps to reproduce

1. **Train and export a model in TensorFlow 2.9 with Keras 2 API**:
   - A simple Keras sequential model is created and trained on random data.
   - The model is saved using both `tf.saved_model.save` and `tf.keras.models.save_model` with `tf` save format (which is unsupported in Keras 3).
   
2. **Attempt to load the models in TensorFlow 2.17 with Keras 3 API**:
   - The models are loaded using TensorFlow’s `tf.saved_model.load`, `keras.layers.TFSMLayer`, and `tf.keras.models.load_model`.

3. **Observe the errors**:
   - When loading using `tf.saved_model.load`, the error `'_UserObject' object has no attribute 'add_slot'` occurs.
   - When loading using `keras.layers.TFSMLayer`, the same `'_UserObject' object has no attribute 'add_slot'` error is triggered.
   - When loading using `tf.keras.models.load_model`, a different error appears: `File format not supported.` Because Keras 3 has dropped support for the default `tf` save format in version 2!

## Expected behavior

While I understand that issues related to loading legacy Keras models saved with the `tf` save format using Keras are out of scope for TensorFlow and should be addressed by the Keras team, the functionality surrounding TensorFlow's `tf.saved_model`, which uses the `SavedModel` bundle, is part of TensorFlow Core. Since this format is shared across different runtimes, it should remain backward compatible. Therefore, models saved in earlier versions of TensorFlow using the `SavedModel` format should load seamlessly in newer TensorFlow versions, without requiring users to rebuild their models or encountering compatibility errors.


### Standalone code to reproduce the issue

## Minimal example to reproduce the issue

A minimal code example to reproduce the issue is available in this repository: [Reproduce TF Model Compatibility Issue.](https://github.com/arianmaghsoudnia/reproduce-tf-model-compat-issue)

Please follow the steps in the README file.



### Relevant log output

_No response_","['type:bug', 'comp:keras', '2.17']",2024-10-09T10:50:39Z,3,0,https://github.com/tensorflow/tensorflow/issues/77356
tensorflow/tensorflow,tf.nn.conv2d terminates process with invalid input shape instead of raising an exception,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow terminates the process when passing an invalid input shape to `tf.nn.conv2d`. Instead of raising a Python exception that can be caught with a try-except block.

I expected TensorFlow to raise a catchable Python exception indicating that the input tensor shape is invalid. This would allow the error to be handled in a try-except block, instead of terminating the process. The error message should clearly explain the shape mismatch issue.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Define invalid input tensor and kernel
input_tensor = [[1.0, 2.0, 3.0]]
kernel = [[0.5, 0.5], [0.5, 0.5]]

try:
    # Create TensorFlow constants
    input_tf = tf.constant(input_tensor, dtype=tf.float32)
    kernel_tf = tf.constant(kernel, dtype=tf.float32)
    
    # Attempt to perform convolution, expecting an error
    output_tf = tf.nn.conv2d(
        tf.expand_dims(input_tf, axis=0), 
        tf.expand_dims(kernel_tf, axis=0), 
        strides=[1, 1, 1, 1], 
        padding='VALID'
    )
    
    print(""TensorFlow Output:"", output_tf.numpy())
except Exception as e:
    print(""TensorFlow Error:"", e)
```


### Relevant log output

```shell
2024-10-09 14:53:31.118589: F ./tensorflow/core/util/tensor_format.h:427] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-09T07:52:38Z,5,0,https://github.com/tensorflow/tensorflow/issues/77336
tensorflow/tensorflow,RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Google Colab with Python 3.10.12
- TensorFlow installation (pip package or built from source):
pip package
- TensorFlow library (version, if pip package or github SHA, if built from source):
v2.17.0

### 2. Code

```
import tensorflow as tf

saved_model_dir = '/content/saved_model'

num_calibration_steps = 100

input = tf.cast(tf.random.normal((1, 640, 640, 3)), tf.float32)
dummy_input = tf.cast(tf.random.normal((1, 2)), tf.int64)

def representative_dataset_gen():
    for _ in range(num_calibration_steps):
        yield [dummy_input, input] #model has 2 input tensors

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
  tf.lite.OpsSet.SELECT_TF_OPS
]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_quant_model = converter.convert()

# Save the quantized model to a local file
with open('quantized_model.tflite', 'wb') as f:
    f.write(tflite_quant_model)
```

### 3. Failure after conversion

After converting the model from ONNX using onnx2tf, I got saved_model, which I tried to convert to int8 quantized model using the code above. When trying to inference the model, after reading the model by the interpreter and calling the function `allocate_tensors()` 
```
interpreter = tf.lite.Interpreter(model_path=""/content/quantized_model.tflite"")
interpreter.allocate_tensors()
```
I get the following error:

```
RuntimeError                              Traceback (most recent call last)
[<ipython-input-7-b6b80a3bdf94>](https://localhost:8080/#) in <cell line: 6>()
      4 interpreter = tf.lite.Interpreter(model_path=""/content/quantized_model.tflite"")
      5 print(interpreter.get_input_details())
----> 6 interpreter.allocate_tensors()

[/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py](https://localhost:8080/#) in allocate_tensors(self)
    535   def allocate_tensors(self):
    536     self._ensure_safe()
--> 537     return self._interpreter.AllocateTensors()
    538 
    539   def _safe_to_run(self):

RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.
```

Could someone give me some advice, suggestions on how to solve this error? I couldn't even find that anyone has solved the same problem. 
The closest to this error is this [issue](https://github.com/tensorflow/tensorflow/issues/61395), but the workaround is to convert the ONNX model to Keras and for my complex model it is not possible to fix.
","['stat:awaiting tensorflower', 'type:bug', 'comp:lite', 'TFLiteConverter', '2.17']",2024-10-08T22:02:42Z,12,0,https://github.com/tensorflow/tensorflow/issues/77293
tensorflow/tensorflow,"tensorflow.python.ops.signal.dct_ops.dct aborts with ""Assertion failure no zero-sized FFTs""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241007

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)
Please find the [gist](https://colab.research.google.com/drive/1oBjZoqp6WZn_VU-CTxZ3bspUc6v9D51s?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.eager import def_function
from tensorflow.python.framework import tensor_spec
from tensorflow.python.ops.signal import dct_ops

def test_with_dynamic_dimensions(dct_type, norm, shape, dtype):
    @def_function.function
    def func(signals):
        return dct_ops.dct(signals, n=norm, type=dct_type, norm=None)
    signals_spec = tensor_spec.TensorSpec([None] * len(shape), dtype)
    f = func.get_concrete_function(signals_spec)
    f(np.zeros([0], dtype=dtype))
test_with_dynamic_dimensions(3, None, [3], np.float32)
```


### Relevant log output

```shell
DUCC FFT c2r failed: 
bazel-out/k8-opt/bin/external/ducc/_virtual_includes/fft/ducc/src/ducc0/fft/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):

Assertion failure
no zero-sized FFTs

Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-08T06:57:49Z,1,0,https://github.com/tensorflow/tensorflow/issues/77211
tensorflow/tensorflow,tensorflow.python.ops.parsing_ops.parse_single_sequence_example can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241007

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

Linux Ubuntu 20.04.3 LTS

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)

Please find the [gist](https://colab.research.google.com/drive/17PzKxkDEr3N8E9D9Kk_mT2A1LyPpoZZe?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.core.example import example_pb2
from tensorflow.core.example import feature_pb2
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.ops import parsing_ops
example = example_pb2.Example
feature = feature_pb2.Feature
features = lambda d: feature_pb2.Features(feature=d)
bytes_feature = lambda v: feature(bytes_list=feature_pb2.BytesList(value=v))
int64_feature = lambda v: feature(int64_list=feature_pb2.Int64List(value=v))
float_feature = lambda v: feature(float_list=feature_pb2.FloatList(value=v))
feature_list = lambda l: feature_pb2.FeatureList(feature=l)
feature_lists = lambda d: feature_pb2.FeatureLists(feature_list=d)
sequence_example = example_pb2.SequenceExample

def testSequenceExampleListWithWrongShapeFails():
    original = sequence_example(feature_lists=feature_lists({'a': feature_list([int64_feature([2, 3]), int64_feature([2, 3, 4])])}))
    serialized = original.SerializeToString()
    parsing_ops.parse_single_sequence_example(**
        ({
            'example_name': 'in1',
            'serialized': ops.convert_to_tensor(serialized),
            'sequence_features': {'a': parsing_ops.FixedLenSequenceFeature((0, 0), dtypes.int64)}
        }))
testSequenceExampleListWithWrongShapeFails()
```


### Relevant log output

```shell
Floating point exception (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops']",2024-10-08T06:52:07Z,1,0,https://github.com/tensorflow/tensorflow/issues/77210
tensorflow/tensorflow,`SimpleDynamicBuffer::AddString` is calling `memcpy` with null data,"I've noticed this hitting on our ubsan builds recently:

```
../../third_party/tflite/src/tensorflow/compiler/mlir/lite/utils/string_utils.cc:32:10: runtime error: null pointer passed as argument 1, which is declared to never be null
../../build/linux/debian_bullseye_amd64-sysroot/usr/include/string.h:44:28: note: nonnull attribute specified here
    #0 0x5a36de826450 in mlir::TFL::SimpleDynamicBuffer::AddString(char const*, unsigned long) third_party/tflite/src/tensorflow/compiler/mlir/lite/utils/string_utils.cc:32:3
    #1 0x5a36de825d3e in tflite::DynamicBuffer::AddString(char const*, unsigned long) third_party/tflite/src/tensorflow/lite/string_util.cc:37:28
    #2 0x5a36de82924d in PopulateTensor<std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char> > > third_party/tflite_support/src/tensorflow_lite_support/cc/task/core/task_utils.h:125:13
    #3 0x5a36de82924d in tflite::task::processor::UniversalSentenceEncoderPreprocessor::Preprocess(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/processor/universal_sentence_encoder_preprocessor.cc:58:3
    #4 0x5a36de81d3f7 in tflite::task::text::TextEmbedder::Preprocess(std::__Cr::vector<TfLiteTensor*, std::__Cr::allocator<TfLiteTensor*>> const&, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/text/text_embedder.cc:174:25
    #5 0x5a36de81cd8c in tflite::task::core::BaseTaskApi<tflite::task::processor::EmbeddingResult, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&>::InferWithFallback(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/core/base_task_api.h:146:5
    #6 0x5a36de81cc40 in tflite::task::text::TextEmbedder::Embed(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/text/text_embedder.cc:169:10
    #7 0x5a36d2728c24 in ai_chat::TextEmbedder::EmbedText(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&, tflite::task::processor::EmbeddingResult&) brave/components/ai_chat/core/browser/text_embedder.cc:271:49
    #8 0x5a36d2728073 in ai_chat::TextEmbedder::EmbedSegments() brave/components/ai_chat/core/browser/text_embedder.cc:287:19
    #9 0x5a36c8c67059 in ai_chat::TextEmbedderUnitTest::EmbedSegments(ai_chat::TextEmbedder*)::'lambda'()::operator()() const brave/components/ai_chat/core/browser/text_embedder_unittest.cc:67:58
    #10 0x5a36c6762969 in base::OnceCallback<void ()>::Run() && base/functional/callback.h:156:12
    #11 0x5a36d4977df2 in base::TaskAnnotator::RunTaskImpl(base::PendingTask&) base/task/common/task_annotator.cc:202:34
    #12 0x5a36d49dcfe9 in RunTask<(lambda at ../../base/task/thread_pool/task_tracker.cc:678:35)> base/task/common/task_annotator.h:90:5
    #13 0x5a36d49dcfe9 in base::internal::TaskTracker::RunTaskImpl(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base/task/thread_pool/task_tracker.cc:677:19
    #14 0x5a36d49dd0f1 in base::internal::TaskTracker::RunSkipOnShutdown(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base/task/thread_pool/task_tracker.cc:662:3
    #15 0x5a36d49dc1f5 in base::internal::TaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base/task/thread_pool/task_tracker.cc:520:5
    #16 0x5a36d4af81fb in base::test::TaskEnvironment::TestTaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base/test/task_environment.cc:1028:46
    #17 0x5a36d49db5a5 in base::internal::TaskTracker::RunAndPopNextTask(base::internal::RegisteredTaskSource) base/task/thread_pool/task_tracker.cc:415:5
    #18 0x5a36d4a0cabd in base::internal::WorkerThread::RunWorker() base/task/thread_pool/worker_thread.cc:493:36
    #19 0x5a36d4a0c100 in base::internal::WorkerThread::RunPooledWorker() base/task/thread_pool/worker_thread.cc:379:3
    #20 0x5a36d4a0bc86 in base::internal::WorkerThread::ThreadMain() base/task/thread_pool/worker_thread.cc:359:7
    #21 0x5a36d4a3e1ec in base::(anonymous namespace)::ThreadFunc(void*) base/threading/platform_thread_posix.cc:101:13
    #22 0x7695b109ca93 in start_thread nptl/pthread_create.c:447:8
    #23 0x7695b1129c3b in clone3 misc/../sysdeps/unix/sysv/linux/x86_64/clone3.S:78
```","['type:bug', 'comp:lite', 'TFLiteConverter', 'awaiting PR merge']",2024-10-07T18:57:35Z,2,0,https://github.com/tensorflow/tensorflow/issues/77168
tensorflow/tensorflow,NotImplementedError from tf.constant in trivial case,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Trying to make a tensor that has the same value for all items in the batch, see the following bare minimum code. 
I get `NotImplementedError: cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array.`
I am not trying to use numpy, this is an internal error.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import keras
import numpy as np

class CustomModel(keras.models.Model):
    def call(self, inputs):
        inputs_shape = tf.shape(inputs)
        return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # NotImplementedError
        #return 3.0 * tf.ones(shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # OK

model = CustomModel()
model.compile(run_eagerly=False, loss=""mse"")  # OK if run_eagerly=True
model.fit(np.array([[0.0]]), np.array([[0.0]]))
```
```


### Relevant log output

```shell
{
	""name"": ""NotImplementedError"",
	""message"": ""Exception encountered when calling CustomModel.call().

Cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.

Arguments received by CustomModel.call():
  • inputs=tf.Tensor(shape=(None, 1), dtype=float32)"",
	""stack"": ""---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In[6], line 13
     11 model = CustomModel()
     12 model.compile(run_eagerly=False, loss=\""mse\"")  # OK if run_eagerly=True
---> 13 model.fit(np.array([[0.0]]), np.array([[0.0]]))

File /usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

Cell In[6], line 8, in CustomModel.call(self, inputs)
      6 def call(self, inputs):
      7     inputs_shape = tf.shape(inputs)
----> 8     return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)

File /usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3100, in prod(a, axis, dtype, out, keepdims, initial, where)
   2979 @array_function_dispatch(_prod_dispatcher)
   2980 def prod(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,
   2981          initial=np._NoValue, where=np._NoValue):
   2982     \""\""\""
   2983     Return the product of array elements over a given axis.
   2984 
   (...)
   3098     10
   3099     \""\""\""
-> 3100     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
   3101                           keepdims=keepdims, initial=initial, where=where)

File /usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88, in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     85         else:
     86             return reduction(axis=axis, out=out, **passkwargs)
---> 88 return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

NotImplementedError: Exception encountered when calling CustomModel.call().

Cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.

Arguments received by CustomModel.call():
  • inputs=tf.Tensor(shape=(None, 1), dtype=float32)""
}
```
","['type:bug', 'TF 2.16']",2024-10-04T13:38:25Z,3,0,https://github.com/tensorflow/tensorflow/issues/77045
tensorflow/tensorflow,TFlite compilation crashes on MacOS (error: _Float16 is not supported on this target),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-rc0

### Custom code

No

### OS platform and distribution

MacOS 15.0

### Mobile device

_No response_

### Python version

3.12

### Bazel version

6.5

### GCC/compiler version

Apple clang version 16.0.0 (clang-1600.0.26.3)
XCode 16.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Compiling TF_lite v2.18.0-rc0 from source, using the command suggested in the documentation leads to crash (log below):

```PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native```

### Standalone code to reproduce the issue

```shell
PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native
```


### Relevant log output

```shell
TF2_BEHAVIOR=1 \
    XCODE_VERSION_OVERRIDE=16.0.0.16A242d \
    ZERO_AR_DATE=1 \
  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' 'DEBUG_PREFIX_MAP_PWD=.' -iquote external/XNNPACK -iquote bazel-out/darwin-opt/bin/external/XNNPACK -iquote external/pthreadpool -iquote bazel-out/darwin-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/darwin-opt/bin/external/FXdiv -iquote external/cpuinfo -iquote bazel-out/darwin-opt/bin/external/cpuinfo -iquote external/FP16 -iquote bazel-out/darwin-opt/bin/external/FP16 -Ibazel-out/darwin-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/darwin-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/darwin-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -Ibazel-out/darwin-opt/bin/external/FP16/_virtual_includes/FP16 -isystem external/XNNPACK/include -isystem bazel-out/darwin-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/darwin-opt/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/darwin-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/darwin-opt/bin/external/FXdiv/include -isystem external/cpuinfo/include -isystem bazel-out/darwin-opt/bin/external/cpuinfo/include -isystem external/cpuinfo/src -isystem bazel-out/darwin-opt/bin/external/cpuinfo/src -isystem external/FP16/include -isystem bazel-out/darwin-opt/bin/external/FP16/include -MD -MF bazel-out/darwin-opt/bin/external/XNNPACK/_objs/microkernel_configs/cmul-config.d -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_LOG_LEVEL=0' '-DXNN_ENABLE_CPUINFO=1' '-DXNN_ENABLE_MEMOPT=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=1' '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_SPARSE=1' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_ARM_I8MM=0' '-DXNN_ENABLE_RISCV_FP16_VECTOR=0' '-DXNN_ENABLE_AVX512VNNIGFNI=1' '-DXNN_ENABLE_AVX512AMX=1' '-DXNN_ENABLE_AVX512FP16=1' '-DXNN_ENABLE_AVXVNNI=1' '-DXNN_ENABLE_AVXVNNIINT8=1' '-DXNN_ENABLE_AVX256SKX=1' '-DXNN_ENABLE_AVX256VNNI=1' '-DXNN_ENABLE_AVX256VNNIGFNI=1' '-DXNN_ENABLE_HVX=0' '-DXNN_ENABLE_KLEIDIAI=0' '-DBAZEL_CURRENT_REPOSITORY=""XNNPACK""' '-frandom-seed=bazel-out/darwin-opt/bin/external/XNNPACK/_objs/microkernel_configs/cmul-config.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/MacOSX.platform/Developer/Library/Frameworks -no-canonical-prefixes -pthread -DGRPC_BAZEL_BUILD -w -O3 '-march=native' -Iinclude -Isrc '-DXNN_ENABLE_CPUINFO=1' '-std=c99' -O2 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -target x86_64-apple-macosx15.0 -c external/XNNPACK/src/configs/cmul-config.c -o bazel-out/darwin-opt/bin/external/XNNPACK/_objs/microkernel_configs/cmul-config.o)
# Configuration: 7ffafbfebd31d6f3229fc3b4603178937ffcc0387347731c7b47fcb97e2cd76d
# Execution platform: @local_execution_config_platform//:platform
ERROR: /private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/XNNPACK/BUILD.bazel:803:36: Compiling external/XNNPACK/sse_prod_microkernels.c failed: (Exit 1): wrapped_clang failed: error executing command (from target @XNNPACK//:sse_prod_microkernels) external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' ... (remaining 109 arguments skipped)
In file included from bazel-out/darwin-opt/bin/external/XNNPACK/sse_prod_microkernels.c:1:
In file included from external/XNNPACK/src/xnnpack/avgpool.h:15:
In file included from external/XNNPACK/src/xnnpack/microparams.h:12:
In file included from external/XNNPACK/src/xnnpack/math.h:21:
In file included from bazel-out/darwin-opt/bin/external/FP16/_virtual_includes/FP16/fp16/fp16.h:10:
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:614:27: error: _Float16 is not supported on this target
  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:614:8: error: _Float16 is not supported on this target
  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:28: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:38: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                                      ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:8: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:616:27: error: _Float16 is not supported on this target
  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:616:8: error: _Float16 is not supported on this target
  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:617:27: error: _Float16 is not supported on this target
  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:617:8: error: _Float16 is not supported on this target
  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:618:28: error: _Float16 is not supported on this target
  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:618:8: error: _Float16 is not supported on this target
  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:619:27: error: _Float16 is not supported on this target
  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:619:8: error: _Float16 is not supported on this target
  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:620:28: error: _Float16 is not supported on this target
  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:620:8: error: _Float16 is not supported on this target
  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:621:28: error: _Float16 is not supported on this target
  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:621:8: error: _Float16 is not supported on this target
  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:622:31: error: _Float16 is not supported on this target
  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                               ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:622:41: error: _Float16 is not supported on this target
  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                                         ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated.
Error in child process '/usr/bin/xcrun'. 1
Target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 119.893s, Critical Path: 3.17s
INFO: 342 processes: 313 internal, 29 local.
FAILED: Build did NOT complete successfully
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'comp:lite', '2.17']",2024-10-02T18:20:47Z,10,0,https://github.com/tensorflow/tensorflow/issues/76976
tensorflow/tensorflow,Jit-compiling `tf.while_loop` inside `tf.vectorized_map` raises `InvalidArgumentError`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

Yes

### OS platform and distribution

Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

MRE
-------
The following mock-up of `cumsum` attempts to JIT compile a `tf.vectorized_map`ped function containing a `tf.scan`:
```python
import tensorflow as tf

def cumsum(xs):
    return tf.scan(
        lambda a, x: a + x, elems=xs
    )

@tf.function(jit_compile=True)
def vec_cumsum(xs):
    return tf.vectorized_map(cumsum, elems=xs)

xs_batched = tf.reshape(tf.range(30), (3, 10))
vec_cumsum(xs_batched)
```

__Expected behaviour__: `vec_cumsum(xs_batched)` returns a batch of cumulative sums.

__Actual behaviour__: Even though all data structures are known statically at JIT compile time, an InvalidArgumentError is raised with ""No registered 'TensorListReserve'"".  The fault is clearly related to `tf.scan`'s use of `tf.while_loop`, as a (longer) example using naked `tf.while_loop(..., max_iterations=n)` will confirm.

In JAX, it is possible to jit-compile a `vmap`ped function containing a `lax.while_loop` indicating that this is possible in HLO.  It seems the `tf.function(jit_compile=True)` machinery may be mis-transpiling to HLO somehow.

May be related to #73367 also involving `tf.vectorized_map` and `tf.while_loop` (albeit with reversed scope)?

### Standalone code to reproduce the issue

```shell
Colab MRE: https://colab.research.google.com/drive/1bmq1t3PdtebCSlNd0t-iEFrXX7Q0qqZp?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-26ea491cd046> in <cell line: 13>()
     11 
     12 # Fails with ""No registered 'TensorListReserve'""""
---> 13 vec_cumsum(xs_batched)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_vec_cumsum_462[_XlaMustCompile=true,config_proto=13561319589895757934,executor_type=11160318154034397263] on XLA_CPU_JIT: TensorListReserve (No registered 'TensorListReserve' OpKernel for XLA_CPU_JIT devices compatible with node {{function_node __inference_while_fn_428}}{{node while_init/TensorArrayV2_4}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: element_dtype=DT_VARIANT, shape_type=DT_INT32){{function_node __inference_while_fn_428}}{{node while_init/TensorArrayV2_4}}
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'comp:ops', '2.17']",2024-10-01T17:01:36Z,1,0,https://github.com/tensorflow/tensorflow/issues/76891
tensorflow/tensorflow,Multithreading is not working with teansorflow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow==2.15.0.post1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

python:3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am using bert model for classification and serving the model with gunicorn worker_class=gthreads, tf.config.threading.set_intra_op_parallelism_threads(1)
tf.config.threading.set_inter_op_parallelism_threads(1)

when I using above two line of code the code is working fine as expected and if increase the number to more than 1, the code getting blocked at the below line of code

# Make predictions
outputs = model_obj(inputs)

and also inorder to reduce the docker image size I am using 
RUN pip3 install torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch_stable.html


before installing all the dependencies
Flask==2.2.5
g2p-en==2.1.0
gunicorn==21.2.0
jellyfish==1.0.3
kenlm==0.2.0
nltk==3.8.1
numpy==1.26.3
pandas==2.2.0
python-dotenv==1.0.1
requests==2.31.0
scikit-learn==1.4.0
semantic-router==0.0.17
semantic-router[fastembed]
sentence-transformers==2.3.1
tensorflow==2.15.0.post1
tensorflow-hub==0.16.0
theano==1.0.5
transformers==4.37.2
Werkzeug==2.2.2


please tell me why is my code is getting blocked if I use more than 1 thread.

### Standalone code to reproduce the issue

```shell
def intent_prediction(self, sentence, thresold_score):
        logger.info(f""Threshold Score: {thresold_score}"")
        try:
            model_obj = intent_object_dict[self.model]
        except KeyError:
            logger.error(""Model not found in intent_object_dict"")
            model_obj = self.load_model()

        if INTENT_MODEL == ""cohere"":
            score, intent = self.cohere_intent_prediction(sentence, model_obj)
        else:
            score, intent = self.Bert_intent_prediction(
                sentence, model_obj, thresold_score
            )
        return score, intent

def Bert_intent_prediction(self, sentence, model_obj, thresold_score):
        inputs = self.load_BERT_tokenizer(sentence)
        # Make predictions
        outputs = model_obj(inputs)
        # Get predicted class
        probabilities = tf.nn.softmax(outputs.logits, axis=1)
        predicted_class = tf.argmax(probabilities, axis=1).numpy()[0]
        matching_score = probabilities[0][predicted_class].numpy()
        try:
            intent_data = intent_label_dict[self.model]
        except Exception as e:
            logger.error(f""error while getting label: {e}"")
            intent_data = self.get_intent_labels()

        if matching_score >= thresold_score:
            logger.info(
                f""Matched Main Intent:\
    {intent_data[predicted_class]},\
    SCORE :{matching_score}""
            )
            logger.info(f""Matched Sentence: {intent_data[predicted_class]}"")
        else:
            logger.info(f""Intent not matched, score is {matching_score}"")

        intent = intent_data[predicted_class]

        return str(matching_score), intent
```


### Relevant log output

```shell
30-Sep-2024 15:50:42.761|INFO    |__init__|I want my sofa get cleaned|
    __init__.py:171|Enter into PUNC for intent...
30-Sep-2024 15:50:42.761|INFO    |phrase_sim|I want my sofa get cleaned|
    phrase_sim.py:72|Threshold Score: 0.7


after this the code is blocked
```
","['stat:awaiting tensorflower', 'type:bug', 'TF 2.15']",2024-09-30T10:37:06Z,3,0,https://github.com/tensorflow/tensorflow/issues/76794
tensorflow/tensorflow,Segmentation fault (core dumped) in `tf.data.experimental.SqlDataset`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the illegal input to tf.data.experimental.SqlDataset triggered when a crash, and will only come when iteration data.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

data_source_name = ""sqlite:///path/to/correct_database.db""

query = ""SELECT id, name FROM my_table""
output_types = (tf.int64, tf.string)
dataset = tf.data.experimental.SqlDataset(
    'sqlite', data_source_name, query, output_types)

for element in dataset:
    print(element)
```


### Relevant log output

```shell
2024-09-28 21:18:33.844482: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:18:33.907260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:18:33.986019: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:18:34.009755: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:18:34.068897: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:18:38.768599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:18:38.769172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:18:39.132534: W tensorflow/core/kernels/data/experimental/sql_dataset_op.cc:209] Failed to connect to database: INVALID_ARGUMENT: Sqlite::Open(sqlite:///path/to/correct_database.db) failed: unable to open database file
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T13:20:59Z,1,0,https://github.com/tensorflow/tensorflow/issues/76731
tensorflow/tensorflow,Aborted (core dumped) in `tf.linalg.det/slogdet/logdet/cholesky/inv`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.linalg.det/slogdet/logdet/cholesky/inv triggered a crash when the input is empty. Note that this will only be triggered if the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

invalid_input = tf.zeros([])
tf.linalg.det(invalid_input)    # crash
tf.linalg.slogdet(invalid_input)  # crash
tf.linalg.cholesky(invalid_input)  # crash
tf.linalg.logdet(invalid_input)  # crash
tf.linalg.inv(invalid_input)  # crash
```


### Relevant log output

```shell
2024-09-28 21:11:10.188752: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:11:10.199880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:11:10.213635: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:11:10.221654: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:11:10.279720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:11:17.015480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:11:17.015957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:11:17.154391: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T13:15:48Z,1,0,https://github.com/tensorflow/tensorflow/issues/76730
tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.ResourceScatterNdop`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the type of resource_handle is inconsistent with that of updates,tf.raw_ops.ResourceScatterNdop triggers the crash. As follows:
tf.raw_ops.ResourceScatterNdUpdate
tf.raw_ops.ResourceScatterNdAdd
tf.raw_ops.ResourceScatterNdSub
tf.raw_ops.ResourceScatterNdMax
tf.raw_ops.ResourceScatterNdMin

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

resource_var = tf.Variable(initial_value=tf.zeros([2, 2], dtype=tf.int32), trainable=False)
resource_handle = resource_var.handle

indices = np.array([[2, 1], [1, 2]], dtype=np.int32)
updates = np.array([10, 20], dtype=np.float32)
tf.raw_ops.ResourceScatterNdUpdate(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)

tf.raw_ops.ResourceScatterNdAdd(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdSub(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdMax(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdMin(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
```


### Relevant log output

```shell
2024-09-28 21:06:23.445185: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:06:23.508056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:06:23.583640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:06:23.607538: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:06:23.664877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:06:31.527466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:06:31.527985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:06:31.782114: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (3 vs. 1) float expected, got int32
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T13:09:06Z,1,0,https://github.com/tensorflow/tensorflow/issues/76729
tensorflow/tensorflow,Aborted (core dumped) in `tf.io.encode_png`/`tf.compat.v1.image.encode_png`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The crash was triggered when an illegal image was passed to tf.io.encode_png/tf.compat.v1.image.encode_png

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

image = tf.cast(tf.tile([[[0, 0, 0, 1]], [[0, 0, 1, 0]]], [0, 0, 1]), tf.uint8)

encoded_image = tf.compat.v1.image.encode_png(image) # crash
tf.io.encode_png(image, compression=-1, name=None) #crash
```


### Relevant log output

```shell
2024-09-28 20:48:36.270008: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:48:36.332972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:48:36.411391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:48:36.428306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:48:36.438336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-09-28 20:48:41.296886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.92024-09-28 20:48:41.297450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 20:48:41.475588: F tensorflow/core/lib/png/png_io.cc:350] 'image' Must be non NULL
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T12:49:42Z,3,0,https://github.com/tensorflow/tensorflow/issues/76726
tensorflow/tensorflow,Floating point exception (core dumped) in `tf.nn.depth_to_space`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.nn.depth_to_space triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
try:
    # Create an empty tensor
    arg_0_tensor = tf.zeros([0, 2, 3, 12], dtype=tf.float32)
    # arg_0 = tf.identity(arg_0_tensor)
    arg_1 = 536870912
    out = tf.nn.depth_to_space(arg_0_tensor, arg_1)
except Exception as e:
    print(""Error:"", str(e))
```


### Relevant log output

```shell
2024-09-28 20:41:05.888017: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:41:05.950498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:41:06.028236: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:41:06.052072: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:41:06.111011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 20:41:11.970896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2704 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 20:41:11.973176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
Floating point exception (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T12:41:49Z,1,0,https://github.com/tensorflow/tensorflow/issues/76724
tensorflow/tensorflow,Aborted (core dumped) in `tf.nn.max_pool/tf.nn.max_pool1d`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.nn.max_pool triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

invalid_kernel_size = -1
invalid_operation = tf.nn.max_pool(
    tf.random.normal([1, 32, 32, 3]),
    ksize=[1, invalid_kernel_size, invalid_kernel_size, 1],
    strides=[1, 2, 2, 1],
    padding='SAME'
)
```

```
import tensorflow as tf
import sys

ksize = sys.maxsize + 100  # Set to a value larger than sys.maxsize
input_tensor = tf.random.normal(shape=(2, 10, 4))
result = tf.nn.max_pool1d(input=input_tensor, ksize=ksize, strides=1, padding='SAME')

```


### Relevant log output

```shell
2024-09-28 20:26:47.491907: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:26:47.554171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:26:47.606570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:26:47.610539: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:26:47.639739: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 20:26:54.579839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21471 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 20:26:54.582099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 20:26:55.563477: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
F0000 00:00:1727526415.563805  147227 cuda_dnn.cc:1107] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0) 
*** Check failure stack trace: ***
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T12:28:21Z,1,0,https://github.com/tensorflow/tensorflow/issues/76722
tensorflow/tensorflow,Segmentation fault (core dumped) in `tf.profiler.experimental.Profile`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.profiler.experimental.Profile triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

profiler_options = tf.profiler.experimental.ProfilerOptions(
    host_tracer_level=999,
    python_tracer_level=-1,
    device_tracer_level=10,
    delay_ms=None
)

with tf.profiler.experimental.Profile(None, options=profiler_options):
    a = tf.constant(1)
    b = tf.constant(2)
    c = a + b
    print(c.numpy())
```


### Relevant log output

```shell
2024-09-28 20:07:36.902909: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-09-28 20:07:36.966049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:07:36.998027: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:07:37.002984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered2024-09-28 20:07:37.055864: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T12:11:00Z,2,1,https://github.com/tensorflow/tensorflow/issues/76718
tensorflow/tensorflow,TensorFlow keeps creating threads when multi-GPU training （thread leak）,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.4

### GPU model and memory

Nvidia A800 

### Current behavior?

I was using this machine to train a GPT-2 example from (the data I used can also be found in this link, also here https://github.com/chinese-poetry/chinese-poetry.git) https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/

Before we start, I would give a baseline amount of this machine's threads :
![10](https://github.com/user-attachments/assets/53e16381-e50f-4054-8c2a-b790fe2e077b)

When starting with the multi-GPU training of tensorflow by calling the tf.distribute.MirroredStrategy, the training process worked fine as usual. 

But with the time went by, I found the amount of threads increased with the training process going, here is the evidence of thread increasing when processing 3354th batch (I used cat /proc/""this programs' pid""/status to check the number of threads):
![3](https://github.com/user-attachments/assets/56bbfaf0-f59c-48f1-ae19-a98d563ad000)
![tf](https://github.com/user-attachments/assets/c546ea4b-855a-44e5-84c1-95d6d3f3aba4)
![2](https://github.com/user-attachments/assets/0d07b24d-e93e-451e-9e1e-51a26f7db60d)

Then, the evidence of thread increasing when processing 3791st batch (the amount of threads reached 22178):
![6](https://github.com/user-attachments/assets/508fed5c-8aec-4deb-8e68-74dbf6aa5613)
![4](https://github.com/user-attachments/assets/77507790-70fd-4abf-b72c-e9d831565879)
![5](https://github.com/user-attachments/assets/d8945e53-23d9-4fb7-b50d-37962b93a692)

When calculating  the 5054th batch, the training program got an error, and I captured a count of threads before the error (achieved around 31120 threads):
![8](https://github.com/user-attachments/assets/15b433cc-eefb-451f-a31c-2a286e17afec)
![8](https://github.com/user-attachments/assets/813835d2-3834-400a-b8fe-ec27f15d89ad)

I checked an similar issue in https://github.com/tensorflow/tensorflow/issues/62466, but I cannot find a solution, moreover, I have run other examples like diffusion model using this machine and the same tensorflow env with multi-GPU training, which worked fine and no any problems. So, could you please give me a help for this problem, very appreciated.





### Standalone code to reproduce the issue

```shell
Here is my code

import os

os.environ[""KERAS_BACKEND""] = ""tensorflow""  # or ""tensorflow"" or ""torch""

import keras_nlp
import keras
import tensorflow as tf
import time

keras.mixed_precision.set_global_policy(""mixed_float16"")

import os
import json
import datetime

train_ds = (
    tf.data.Dataset.from_tensor_slices(paragraphs)
    .batch(36)
    .cache()
    .prefetch(tf.data.AUTOTUNE)
)

strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1"", ""GPU:2"", ""GPU:3"", ""GPU:4"", ""GPU:5""])
print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))

preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(
    ""gpt2_base_en"",
    sequence_length=128,
)

# Open a strategy scope.
with strategy.scope():
# To speed up training and generation, we use preprocessor of length 128
# instead of full length 1024.
    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(
        ""gpt2_base_en"", preprocessor=preprocessor
    )
    num_epochs = 5

    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_path,
        save_weights_only=True,
        monitor=""accuracy"",
        # monitor=""i_loss"",
        mode=""min"",
        save_best_only=True,
        save_freq=""epoch""
    )
    learning_rate = 5e-4
    
    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    gpt2_lm.compile(
        optimizer=keras.optimizers.Adam(learning_rate),
        loss=loss,
        weighted_metrics=[""accuracy""],
    )

    gpt2_lm.fit(train_ds, epochs=num_epochs, callbacks=[
        checkpoint_callback,
        tensorboard_callback,
    ],
                )
```


### Relevant log output

```shell
2024-09-21 00:24:25.840848: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:57] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. gpt2_causal_lm/gpt2_backbone/embeddings_dropout/dropout/random_uniform/RandomUniform
2024-09-21 00:24:25.846135: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-09-21 00:24:26.559075: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.572007: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.573183: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.581128: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.581197: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.588190: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
12024-09-21 00:25:01.986918: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:02.215951: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2024-09-21 00:25:02.420703: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:02.542130: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:03.595774: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

12024-09-21 00:25:04.071526: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:04.130504: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:05.333297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
5054/8663 [================>.............] - ETA: 9:14 - loss: 11.8715 - accuracy: 2.2702terminate called after throwing an instance of 'std::system_error'
terminate called recursively
  what():  Resource temporarily unavailable
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:dist-strat', 'TF 2.11']",2024-09-20T17:14:03Z,2,0,https://github.com/tensorflow/tensorflow/issues/76157
tensorflow/tensorflow,"tf.python.ops.array_ops.transpose aborts with ""Check failed: d >= 0 (0 vs. -1)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `array_ops.transpose`

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.ops import array_ops

x =np.arange(0, 8).reshape([2, 4]).astype(np.float32)
y = np.array([-1, 0]).astype(np.int32)
array_ops.transpose(x, y,conjugate = False)
```


### Relevant log output

```shell
2024-09-19 16:16:30.137164: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-19T08:31:38Z,3,0,https://github.com/tensorflow/tensorflow/issues/76036
tensorflow/tensorflow,Code error when feature name has multiple `_`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

5.15.149-99.162.amzn2.x86_64

### Mobile device

_No response_

### Python version

Python 3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Expect there shouldn't be errors just by changing feature name. 

### Standalone code to reproduce the issue
This is a code sample that will work normally

```
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Concatenate
from tensorflow.keras import Model

import pandas as pd
import numpy as np

df = pd.DataFrame()
numeric_feature_name = 'a' * 27
categorical_feature_name = 'b' * 11
df[numeric_feature_name] = range(1000)
df[categorical_feature_name] = 'a'
df['label'] = 1

numeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')
categorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=""string"")
encoding_layer = get_category_encoding_layer(vocab=['a'])
encoded_categorical_feature = encoding_layer(categorical_feature_layer)

all_inputs = [numeric_feature_layer, categorical_feature_layer]
encoded_features = [numeric_feature_layer, encoded_categorical_feature]
concat_features = Concatenate()(encoded_features)
output = Dense(units=1, activation='sigmoid')(concat_features)
model = Model(inputs=all_inputs, outputs=output)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

dataframe_x = df[[numeric_feature_name, categorical_feature_name]]
dataframe_y = df['label']
df2 = ((dict(dataframe_x), dataframe_y))
ds = tf.data.Dataset.from_tensor_slices(df2)
ds = ds.batch(32)
ds_train = ds

model.fit(
    ds_train,
    epochs=10,
    batch_size=300,
    verbose=1
)
```

However, if I change the feature name, the same code will throw error

```
df = pd.DataFrame()
## Just change the feature name here
numeric_feature_name = 'a_b_c_d_e_f_g' 
categorical_feature_name = 'a_b_c_d_e_f'
df[numeric_feature_name] = range(1000)
df[categorical_feature_name] = 'a'
df['label'] = 1

numeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')
categorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=""string"")
encoding_layer = get_category_encoding_layer(vocab=['a'])
encoded_categorical_feature = encoding_layer(categorical_feature_layer)

all_inputs = [numeric_feature_layer, categorical_feature_layer]
encoded_features = [numeric_feature_layer, encoded_categorical_feature]
concat_features = Concatenate()(encoded_features)
output = Dense(units=1, activation='sigmoid')(concat_features)
model = Model(inputs=all_inputs, outputs=output)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

dataframe_x = df[[numeric_feature_name, categorical_feature_name]]
dataframe_y = df['label']
df2 = ((dict(dataframe_x), dataframe_y))
ds = tf.data.Dataset.from_tensor_slices(df2)
ds = ds.batch(32)
ds_train = ds

model.fit(
    ds_train,
    epochs=10,
    batch_size=300,
    verbose=1
)
```

We have tested that this error is on 2.17.0 and if we are using 2.15 tensorflow, both codes will run smoothly.
```


### Relevant log output

```shell
Epoch 1/10
2024-09-18 05:28:05.240962: W tensorflow/core/framework/op_kernel.cc:1817] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported
---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
Cell In[14], line 8
      5 ds = ds.batch(32)
      6 ds_train = ds
----> 8 model.fit(
      9     ds_train,
     10     epochs=10,
     11     batch_size=300,
     12     verbose=1
     13 )

File ~/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51 try:
     52   ctx.ensure_initialized()
---> 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                       inputs, attrs, num_outputs)
     55 except core._NotOkStatusException as e:
     56   if name is not None:

UnimplementedError: Graph execution error:

Detected at node functional_5_1/Cast defined at (most recent call last):
  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel_launcher.py"", line 18, in <module>

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/traitlets/config/application.py"", line 1075, in launch_instance

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 739, in start

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 205, in start

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 534, in process_one

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 362, in execute_request

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 778, in execute_request

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 449, in do_execute

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 549, in run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3075, in run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3130, in _run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 128, in _pseudo_sync_runner

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3334, in run_cell_async

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3517, in run_ast_nodes

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3577, in run_code

  File ""/tmp/ipykernel_37779/4021243845.py"", line 8, in <module>

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 320, in fit

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 121, in one_step_on_iterator

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 108, in one_step_on_data

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 51, in train_step

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/layers/layer.py"", line 901, in __call__

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/ops/operation.py"", line 46, in __call__

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 167, in call

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 258, in _standardize_inputs

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 218, in _convert_inputs_to_tensors

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/ops/core.py"", line 822, in convert_to_tensor

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py"", line 132, in convert_to_tensor

Cast string to float is not supported
	 [[{{node functional_5_1/Cast}}]] [Op:__inference_one_step_on_iterator_6125]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', '2.17']",2024-09-18T17:06:56Z,10,0,https://github.com/tensorflow/tensorflow/issues/75996
tensorflow/tensorflow,`resource_create_op` operation can cause TensorFlow to crash.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 A `Segmentation fault`  could be raised in TensorFlow when using `test_ops.resource_create_op` . The code is as follows:

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.framework import test_ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import array_ops_stack


sess = tf.compat.v1.Session()

@tf.function
def func():
    r1 = test_ops.stub_resource_handle_op(container='a', shared_name='b')
    r2 = test_ops.stub_resource_handle_op(container='a', shared_name='c')
    c = array_ops_stack.stack([r1, r2])
    s = array_ops.strided_slice(c, [1], [2 ** 32])
    with sess.as_default():
        test_ops.resource_create_op(s)

func()
```


### Relevant log output

```shell
> Segmentation fault (core dumped)

The above code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-16T01:45:51Z,3,0,https://github.com/tensorflow/tensorflow/issues/75825
tensorflow/tensorflow,Encountering a `Segmentation fault` when using `data_flow_ops.FIFOQueue` in TensorFlow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 A `Segmentation fault`  could be raised in TensorFlow when using `data_flow_ops.FIFOQueue` . The following code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes as dtypes_lib
from tensorflow.python.ops import data_flow_ops
import tensorflow as tf

q = data_flow_ops.FIFOQueue(10, dtypes_lib.float32, ())
elems = [10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]
tmp_var64 = elems[4:8]

sess = tf.compat.v1.Session()
with sess.as_default():
    q.dequeue_up_to([])
```


### Relevant log output

```shell
> Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-15T04:17:25Z,1,0,https://github.com/tensorflow/tensorflow/issues/75814
