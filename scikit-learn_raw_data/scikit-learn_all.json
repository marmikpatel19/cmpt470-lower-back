[{"issue_number":1,"repository":"scikit-learn\/scikit-learn","title":"StandardScaler is `stateless`","description":"### Describe the bug\n\nThe StandardScaler seems to be stateless in version 1.6.1. But fit changes the state of the StandardScaler if I got it correctly. \n\n### Steps\/Code to Reproduce\n\n```\nStandardScaler()._get_tags()[\"stateless\"]\n```\n\n### Expected Results\n\nFalse\n\n### Actual Results\n\nTrue\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.14 (main, Jul 18 2024, 22:40:44) [Clang 15.0.0 (clang-1500.1.0.2.5)]\nexecutable: ****\/python\n   machine: macOS-15.2-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.1.2\n   setuptools: 71.0.3\n        numpy: 1.26.4\n        scipy: 1.13.1\n       Cython: 3.0.11\n       pandas: 2.2.3\n   matplotlib: 3.9.2\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: ****.dylib\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: armv8\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: *****\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: neoversen1\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: *****\n        version: None\n```","labels":["Bug"],"created_at":"2025-02-15T18:58:02Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30840"},{"issue_number":2,"repository":"scikit-learn\/scikit-learn","title":"Bug: AttributeError in `str_escape` when handling `numpy.int64` in `sklearn.tree._export.py` in `\/sklearn\/tree\/_export.py`","description":"### Describe the bug\n\n\nWhen exporting a decision tree using `sklearn.tree.export_text()` (or other related functions), an AttributeError occurs if a `numpy.int64` value is passed to `_export.py` instead of a string.\n\n```\n  File \"venv\/lib\/python3.10\/site-packages\/sklearn\/tree\/_export.py\", line 311, in node_to_str\n    feature = self.str_escape(feature)\n  File \"venv\/lib\/python3.10\/site-packages\/sklearn\/tree\/_export.py\", line 581, in str_escape\n    return string.replace('\"', r\"\\\"\")\nAttributeError: 'numpy.int64' object has no attribute 'replace'\n```\nCauses:\nThe function `str_escape(feature)` expects a string but receives a `numpy.int64` value.\n`numpy.int64` does not have a .replace() method, causing an AttributeError. \n\n## Possible Fix:\nConvert feature to a string before passing it to `str_escape()` in `_export.py`.\nModify line 581 in `_export.py`:\n\nBefore (causing error):\n```\nreturn string.replace('\"', r\"\\\"\")\n```\n## After (fixing error):\n\n```\nreturn str(string).replace('\"', r\"\\\"\")\n```\nThis ensures that `feature` is always a string before calling `.replace()`.\n\n\n\n### Steps\/Code to Reproduce\n\nThis piece of code triggers the error:\n\n```\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nX = np.array([[0, 1], [1, 0], [1, 1], [0, 0]])\ny = np.array([0, 1, 1, 0])\n\nclf = DecisionTreeClassifier().fit(X, y)\n\nfeature_names = np.array([10, 20], dtype=np.int64)  # numpy.int64 feature names\n\n# Debugging prints\nprint(\"Feature Names:\", feature_names)\nprint(\"Feature Name Types:\", [type(name) for name in feature_names])\n\n# Attempt to trigger the error\nexport_graphviz(clf, out_file=None, feature_names=feature_names)\n```\n\n### Expected Results\n\nA graph in PNG format. \n\n### Actual Results\n\n\n```\nAttributeError: 'numpy.int64' object has no attribute 'replace'\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]\nexecutable: \/usr\/bin\/python3\n   machine: Linux-6.8.0-52-generic-x86_64-with-glibc2.35\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 22.0.2\n   setuptools: 59.6.0\n        numpy: 2.2.3\n        scipy: 1.15.1\n       Cython: 3.0.11\n       pandas: None\n   matplotlib: 3.5.1\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: lib\/python3.10\/site-packages\/numpy.libs\/libscipy_openblas64_-6bb31eeb.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: SkylakeX\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: lib\/python3.10\/site-packages\/scipy.libs\/libscipy_openblas-68440149.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: SkylakeX\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libgomp\n       filepath: lib\/python3.10\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\n        version: None\n```","labels":["Bug","Needs Triage"],"created_at":"2025-02-14T13:54:26Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30834"},{"issue_number":3,"repository":"scikit-learn\/scikit-learn","title":"Numpy Array Error when Training MultioutputClassifer with LogisticRegressionCV with classes underrepresented","description":"### Describe the bug\n\nWhen I train the MultioutputClassifer with LogisticRegressionCV with classes underrepresented, I get the following numpy error.\nI think this is connected to the issue #28178 and #26401.\n\n### Steps\/Code to Reproduce\n\n```python\nimport sklearn\nprint(sklearn.__version__)\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.multioutput import MultiOutputClassifier\nimport numpy as np\n\n\nn, m = 20, 5\nmodel = MultiOutputClassifier(LogisticRegressionCV())\nX = np.random.randn(n, m)\ny = np.concatenate([[np.random.randint(0, 2, n),\n                     np.random.randint(0, 5, n)]], axis=0).T\ny[-3:, 0] = [3, 4, 5]\nmodel.fit(X, y)\n```\n\n### Expected Results\n\n1.6.1\n\n### Actual Results\n\n1.6.1\n\n```pytb\n.venv\/lib\/python3.12\/site-packages\/sklearn\/model_selection\/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n  warnings.warn(\nTraceback (most recent call last):\n  File \"error_skitlearn.py\", line 14, in <module>\n    model.fit(X, y)\n  File \".venv\/lib\/python3.12\/site-packages\/sklearn\/multioutput.py\", line 543, in fit\n    super().fit(X, Y, sample_weight=sample_weight, **fit_params)\n  File \".venv\/lib\/python3.12\/site-packages\/sklearn\/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv\/lib\/python3.12\/site-packages\/sklearn\/multioutput.py\", line 274, in fit\n    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py\", line 77, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv\/lib\/python3.12\/site-packages\/joblib\/parallel.py\", line 1918, in __call__\n    return output if self.return_generator else list(output)\n                                                ^^^^^^^^^^^^\n  File \".venv\/lib\/python3.12\/site-packages\/joblib\/parallel.py\", line 1847, in _get_sequential_output\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \".venv\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv\/lib\/python3.12\/site-packages\/sklearn\/multioutput.py\", line 63, in _fit_estimator\n    estimator.fit(X, y, **fit_params)\n  File \".venv\/lib\/python3.12\/site-packages\/sklearn\/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv\/lib\/python3.12\/site-packages\/sklearn\/linear_model\/_logistic.py\", line 2038, in fit\n    coefs_paths = np.reshape(\n                  ^^^^^^^^^^^\n  File \".venv\/lib\/python3.12\/site-packages\/numpy\/_core\/fromnumeric.py\", line 324, in reshape\n    return _wrapfunc(a, 'reshape', shape, order=order)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv\/lib\/python3.12\/site-packages\/numpy\/_core\/fromnumeric.py\", line 54, in _wrapfunc\n    return _wrapit(obj, method, *args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \".venv\/lib\/python3.12\/site-packages\/numpy\/_core\/fromnumeric.py\", line 42, in _wrapit\n    conv = _array_converter(obj)\n           ^^^^^^^^^^^^^^^^^^^^^\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0]\nexecutable: .venv\/bin\/python\n   machine: Linux-6.8.0-52-generic-x86_64-with-glibc2.39\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 25.0.1\n   setuptools: 75.8.0\n        numpy: 2.2.3\n        scipy: 1.15.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: .venv\/lib\/python3.12\/site-packages\/numpy.libs\/libscipy_openblas64_-6bb31eeb.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: .venv\/lib\/python3.12\/site-packages\/scipy.libs\/libscipy_openblas-68440149.so\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libgomp\n       filepath: .venv\/lib\/python3.12\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\n        version: None\n```","labels":["Bug","Needs Triage"],"created_at":"2025-02-14T10:34:16Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30832"},{"issue_number":4,"repository":"scikit-learn\/scikit-learn","title":"UnsetMetadataPassedError can point towards the wrong method","description":"### Describe the bug\n\nWhen `enable_metadata_routing=True`, for a missing `set_score_request`, `UnsetMetadataPassedError` message states that a `set_fit_request` is missing.\n\n### Steps\/Code to Reproduce\n\n```python\nfrom sklearn import set_config\nfrom sklearn.exceptions import UnsetMetadataPassedError\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nrng = np.random.RandomState(22)\nn_samples, n_features = 10, 4\nX = rng.rand(n_samples, n_features)\ny = rng.randint(0, 2, size=n_samples)\nsw = rng.randint(0, 5, size=n_samples)\n\nset_config(enable_metadata_routing=True)\n# missing set_score_request\nlogreg = LogisticRegression().set_fit_request(sample_weight=True)\ntry:\n    cross_validate(\n        logreg, X, y, \n        params={\"sample_weight\":sw}, \n        error_score='raise'\n    )\nexcept UnsetMetadataPassedError as e:\n    print(e)\n```\n\n### Expected Results\n\nI would expect an error message pointing towards the missing `set_score_request`, and perhaps a less verbose message when only one metadata is passed. Something like:\n\n\n'sample_weight' are passed to cross validation but are not explicitly set as requested or not requested for cross_validate's estimator: LogisticRegression. Call `.set_score_request(sample_weight=True)` on the estimator for using 'sample_weight' or `sample_weight=False` for not using it. See the Metadata Routing User guide...\n\n### Actual Results\n\n['sample_weight'] are passed to cross validation but are not explicitly set as requested or not requested for cross_validate's estimator: LogisticRegression. Call `.set_fit_request({{metadata}}=True)` on the estimator for each metadata in ['sample_weight'] that you want to use and `metadata=False` for not using it. See the Metadata Routing User guide...\n\n### Versions\n\n```shell\nsklearn: 1.7.dev0\n```","labels":["Bug","Metadata Routing"],"created_at":"2025-02-12T16:16:09Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30818"},{"issue_number":5,"repository":"scikit-learn\/scikit-learn","title":"sample_weight is silently ignored in LogisticRegressionCV.score when metadata routing is enabled","description":"### Describe the bug\n\nI'm not sure if it is a proper bug, or my lack of understanding of the metadata routing API ;)\n\nWhen `enable_metadata_routing=True`, the `score` method of a `LogisticRegressionCV` estimator will ignore `sample_weight`.\n```python\nset_config(enable_metadata_routing=True)\nlogreg_cv = LogisticRegressionCV().fit(X, y)\nlogreg_cv.score(X, y, sample_weight=sw)==logreg_cv.score(X, y) #unweighted accuracy\n```\nI found it surprising, because the `score` method works fine when `enable_metadata_routing=False`, so the same piece of code behaves differently depending on the metadata routing config.\n```python\nset_config(enable_metadata_routing=False)\nlogreg_cv = LogisticRegressionCV().fit(X, y)\nlogreg_cv.score(X, y, sample_weight=sw) #weighted accuracy\n```\n\nIf I understood the metadata routing API correctly, to make the `score` method `sample_weight` aware we need to explicitly pass a scorer that request it:\n```python\nset_config(enable_metadata_routing=True)\nweighted_accuracy = make_scorer(accuracy_score).set_score_request(sample_weight=True)\nlogreg_cv = LogisticRegressionCV(scoring=weighted_accuracy).fit(X, y)\nlogreg_cv.score(X, y, sample_weight=sw) #weighted accuracy\n```\n\nIf it's the intended behavior of the metadata routing API, maybe we should warn the user or raise an error in the first case, instead of silently ignoring `sample_weight` ?\n\n### Steps\/Code to Reproduce\n\n```python\nfrom sklearn import set_config\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.linear_model import LogisticRegressionCV\nimport numpy as np\n\nrng = np.random.RandomState(22)\nn_samples, n_features = 10, 4\nX = rng.rand(n_samples, n_features)\ny = rng.randint(0, 2, size=n_samples)\nsw = rng.randint(0, 5, size=n_samples)\n\nset_config(enable_metadata_routing=True)\nlogreg_cv = LogisticRegressionCV()\nlogreg_cv.fit(X, y)\n# sample_weight is silently ignored in logreg_cv.score\nassert logreg_cv.score(X, y) == logreg_cv.score(X, y, sample_weight=sw) \nassert not logreg_cv.score(X, y, sample_weight=sw)==accuracy_score(logreg_cv.predict(X),y, sample_weight=sw)\n```\n\n### Expected Results\n\nEither `logreg_cv.score(X, y, sample_weight=sw)` raises an error\/warning or the assertions are false.\n\n### Actual Results\n\nThe assertions are true.\n\n### Versions\n\n```shell\nsklearn: 1.7.dev0\n```","labels":["Bug","Metadata Routing"],"created_at":"2025-02-12T15:49:01Z","comments":3,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30817"},{"issue_number":6,"repository":"scikit-learn\/scikit-learn","title":"Windows free-threaded CPython 3.13 ValueError: concurrent send_bytes() calls are not supported","description":"Noticed in [build log](https:\/\/github.com\/scikit-learn\/scikit-learn\/actions\/runs\/13233133978\/job\/36933421850#step:5:2813). An automated issue was opened in https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30801 and closed the next day.\n\nThis needs some investigation to figure out whether this can be reproduced locally and whether this is actually Windows-specific.\n\nThis may be a joblib issue as well.\n\n```\n================================== FAILURES ===================================\n  _____________________________ test_absolute_error _____________________________\n  \n      def test_absolute_error():\n          # For coverage only.\n          X, y = make_regression(n_samples=500, random_state=0)\n          gbdt = HistGradientBoostingRegressor(loss=\"absolute_error\", random_state=0)\n  >       gbdt.fit(X, y)\n  \n  ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\tests\\test_gradient_boosting.py:225: \n  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\base.py:1389: in wrapper\n      return fit_method(estimator, *args, **kwargs)\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:663: in fit\n      X_binned_train = self._bin_data(X_train, is_training_data=True)\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:1178: in _bin_data\n      X_binned = self._bin_mapper.fit_transform(X)  # F-aligned array\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319: in wrapped\n      data_to_wrap = f(self, X, *args, **kwargs)\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\base.py:918: in fit_transform\n      return self.fit(X, **fit_params).transform(X)\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\binning.py:234: in fit\n      non_cat_thresholds = Parallel(n_jobs=self.n_threads, backend=\"threading\")(\n  ..\\venv-test\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82: in __call__\n      return super().__call__(iterable_with_config_and_warning_filters)\n  ..\\venv-test\\Lib\\site-packages\\joblib\\parallel.py:2007: in __call__\n      return output if self.return_generator else list(output)\n  ..\\venv-test\\Lib\\site-packages\\joblib\\parallel.py:1711: in _get_outputs\n      self._terminate_and_reset()\n  ..\\venv-test\\Lib\\site-packages\\joblib\\parallel.py:1386: in _terminate_and_reset\n      self._backend.terminate()\n  ..\\venv-test\\Lib\\site-packages\\joblib\\_parallel_backends.py:262: in terminate\n      self._pool.close()\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\nuget-cpython\\python-freethreaded.3.13.0\\tools\\Lib\\multiprocessing\\pool.py:652: in close\n      self._change_notifier.put(None)\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\nuget-cpython\\python-freethreaded.3.13.0\\tools\\Lib\\multiprocessing\\queues.py:394: in put\n      self._writer.send_bytes(obj)\n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\nuget-cpython\\python-freethreaded.3.13.0\\tools\\Lib\\multiprocessing\\connection.py:200: in send_bytes\n      self._send_bytes(m[offset:offset + size])\n  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n  \n  self = <multiprocessing.connection.PipeConnection object at 0x00000243F43267C0>\n  buf = <memory at 0x00000243F4C65AC0>\n  \n      def _send_bytes(self, buf):\n          if self._send_ov is not None:\n              # A connection should only be used by a single thread\n  >           raise ValueError(\"concurrent send_bytes() calls \"\n                               \"are not supported\")\n  E           ValueError: concurrent send_bytes() calls are not supported\n  \n  ..\\..\\..\\..\\pypa\\cibuildwheel\\Cache\\nuget-cpython\\python-freethreaded.3.13.0\\tools\\Lib\\multiprocessing\\connection.py:287: ValueError\n```\n","labels":["Bug","Needs Investigation","free-threading","OS:Windows"],"created_at":"2025-02-11T14:44:27Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30810"},{"issue_number":7,"repository":"scikit-learn\/scikit-learn","title":"SequentialFeatureSelector fails on text features even though the estimator supports them","description":"### Describe the bug\n\nWhen a model can handle the data type (may it be text or NaN), `SequentialFeatureSelector` appears to be performing its own validation ignoring the capability of the model and apparently always insists that everything must be numbers. `cross_val_score` appears to be working so it's `SequentialFeatureSelector` that is rejecting the data.\n\n### Steps\/Code to Reproduce\n\n```python\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.feature_selection import SequentialFeatureSelector\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\n\nimport sklearn; print(F'{sklearn.__version__=}')\nimport xgboost; print(F'{xgboost.__version__=}')\n\nX, y = load_diabetes(return_X_y=True, as_frame=True, scaled=False)\nX['sex'] = X['sex'].apply(lambda x: 'M' if x==1.0 else 'F').astype('category')\nmodel = XGBRegressor(enable_categorical=True, random_state=0)\nprint('Testing cross_val_score begins')\ncross_val_score(model, X, y, error_score='raise') # no error\nprint('Testing cross_val_score ends')\nprint('Testing SequentialFeatureSelector begins')\nSequentialFeatureSelector(model, tol=0).fit(X, y)\nprint('Testing SequentialFeatureSelector ends')\n```\n\n### Expected Results\n\n```text\nsklearn.__version__='1.6.1'\nxgboost.__version__='2.1.4'\nTesting cross_val_score begins\nTesting cross_val_score ends\nTesting SequentialFeatureSelector begins\nTesting SequentialFeatureSelector ends\n```\n(No errors)\n\n### Actual Results\n\n```text\nsklearn.__version__='1.6.1'\nxgboost.__version__='2.1.4'\nTesting cross_val_score begins\nTesting cross_val_score ends\nTesting SequentialFeatureSelector begins\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-29-fb1642c5f9e7> in <cell line: 16>()\n     14 print('Testing cross_val_score ends')\n     15 print('Testing SequentialFeatureSelector begins')\n---> 16 SequentialFeatureSelector(model, tol=0).fit(X, y)\n     17 print('Testing SequentialFeatureSelector ends')\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/base.py in wrapper(estimator, *args, **kwargs)\n   1387                 )\n   1388             ):\n-> 1389                 return fit_method(estimator, *args, **kwargs)\n   1390 \n   1391         return wrapper\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/feature_selection\/_sequential.py in fit(self, X, y, **params)\n    280             process_routing(self, \"fit\", **params)\n    281         for _ in range(n_iterations):\n--> 282             new_feature_idx, new_score = self._get_best_new_feature_score(\n    283                 cloned_estimator, X, y, cv, current_mask, **params\n    284             )\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/feature_selection\/_sequential.py in _get_best_new_feature_score(self, estimator, X, y, cv, current_mask, **params)\n    311                 candidate_mask = ~candidate_mask\n    312             X_new = X[:, candidate_mask]\n--> 313             scores[feature_idx] = cross_val_score(\n    314                 estimator,\n    315                 X_new,\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/utils\/_param_validation.py in wrapper(*args, **kwargs)\n    214                     )\n    215                 ):\n--> 216                     return func(*args, **kwargs)\n    217             except InvalidParameterError as e:\n    218                 # When the function is just a wrapper around an estimator, we allow\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/model_selection\/_validation.py in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\n    682     scorer = check_scoring(estimator, scoring=scoring)\n    683 \n--> 684     cv_results = cross_validate(\n    685         estimator=estimator,\n    686         X=X,\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/utils\/_param_validation.py in wrapper(*args, **kwargs)\n    214                     )\n    215                 ):\n--> 216                     return func(*args, **kwargs)\n    217             except InvalidParameterError as e:\n    218                 # When the function is just a wrapper around an estimator, we allow\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/model_selection\/_validation.py in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\n    429     )\n    430 \n--> 431     _warn_or_raise_about_fit_failures(results, error_score)\n    432 \n    433     # For callable scoring, the return type is only know after calling. If the\n\n\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/model_selection\/_validation.py in _warn_or_raise_about_fit_failures(results, error_score)\n    515                 f\"Below are more details about the failures:\\n{fit_errors_summary}\"\n    516             )\n--> 517             raise ValueError(all_fits_failed_message)\n    518 \n    519         else:\n\nValueError: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/model_selection\/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/sklearn.py\", line 1143, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/sklearn.py\", line 603, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/sklearn.py\", line 1065, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 1573, in __init__\n    self._init(\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 1632, in _init\n    it.reraise()\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 569, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 550, in _handle_exception\n    return fn()\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 637, in <lambda>\n    return self._handle_exception(lambda: self.next(input_data), 0)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/data.py\", line 1402, in next\n    input_data(**self.kwargs)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 617, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/data.py\", line 1429, in _proxy_transform\n    data, _ = _ensure_np_dtype(data, data.dtype)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/data.py\", line 224, in _ensure_np_dtype\n    data = data.astype(dtype, copy=False)\nValueError: could not convert string to float: 'M'\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/model_selection\/_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/sklearn.py\", line 1143, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/sklearn.py\", line 603, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/sklearn.py\", line 1065, in _create_dmatrix\n    return QuantileDMatrix(\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 1573, in __init__\n    self._init(\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 1632, in _init\n    it.reraise()\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 569, in reraise\n    raise exc  # pylint: disable=raising-bad-type\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 550, in _handle_exception\n    return fn()\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 637, in <lambda>\n    return self._handle_exception(lambda: self.next(input_data), 0)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/data.py\", line 1402, in next\n    input_data(**self.kwargs)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 726, in inner_f\n    return func(**kwargs)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/core.py\", line 617, in input_data\n    new, cat_codes, feature_names, feature_types = _proxy_transform(\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/data.py\", line 1429, in _proxy_transform\n    data, _ = _ensure_np_dtype(data, data.dtype)\n  File \"\/usr\/local\/lib\/python3.10\/dist-packages\/xgboost\/data.py\", line 224, in _ensure_np_dtype\n    data = data.astype(dtype, copy=False)\nValueError: could not convert string to float: 'F'\n```\n\n### Versions\n\n```shell\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.1.2\n   setuptools: 75.1.0\n        numpy: 1.26.4\n        scipy: 1.13.1\n       Cython: 3.0.11\n       pandas: 2.2.2\n   matplotlib: 3.7.5\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: mkl\n    num_threads: 2\n         prefix: libmkl_rt\n       filepath: \/usr\/local\/lib\/libmkl_rt.so.2\n        version: 2025.0.1-Product\nthreading_layer: gnu\n\n       user_api: blas\n   internal_api: mkl\n    num_threads: 2\n         prefix: libmkl_rt\n       filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/mkl_fft.libs\/libmkl_rt-089e6a60.so.2\n        version: 2025.0.1-Product\nthreading_layer: not specified\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 4\n         prefix: libgomp\n       filepath: \/usr\/lib\/x86_64-linux-gnu\/libgomp.so.1.0.0\n        version: None\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 4\n         prefix: libopenblas\n       filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/scipy.libs\/libopenblasp-r0-01191904.3.27.so\n        version: 0.3.27\nthreading_layer: pthreads\n   architecture: Haswell\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 4\n         prefix: libgomp\n       filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\n        version: None\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 4\n         prefix: libgomp\n       filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/xgboost.libs\/libgomp-24e2ab19.so.1.0.0\n        version: None\n```","labels":["Bug","Needs Info"],"created_at":"2025-02-08T00:48:33Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30785"},{"issue_number":8,"repository":"scikit-learn\/scikit-learn","title":"_py_sort() returns ValueError on windows with numpy 1.26.4 but works correctly with numpy 2.x","description":"### Describe the bug\n\n_py_sort() returns ValueError with numpy 1.26.4 but works correctly with numpy 2.x. I have created 2 different conda envs with different numpy versions from conda-forge:\n```\nconda create -n numpy_1.26.4 numpy=1.26.4 scikit-learn=1.6.1 -c conda-forge --override-channels\n```\nand\n```\nconda create -n numpy_2 numpy=2 scikit-learn=1.6.1 -c conda-forge --override-channel\n```\nIn each of the envs, I essentially reproduced https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/tree\/tests\/test_tree.py#L2820 test_sort_log2_build test that shows different behavior. This works correctly with numpy 2, but with numpy 1.26.4 it returns: \n```\nValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long'\n```\n\n### Steps\/Code to Reproduce\n\nIn fact, this is just a copy of test_sort_log2_build test:\n```\n>>> import numpy as np\n>>> print(np.__version__)\n1.26.4\n>>> import sklearn\n>>> print(sklearn.__version__)\n1.6.1\n>>> from sklearn.tree._partitioner import _py_sort\n>>> rng = np.random.default_rng(75)\n>>> some = rng.normal(loc=0.0, scale=10.0, size=10).astype(np.float32)\n>>> feature_values = np.concatenate([some] * 5)\n>>> samples = np.arange(50)\n>>> _py_sort(feature_values, samples, 50)\n```\n\n### Expected Results\n\n```\n>>> _py_sort(feature_values, samples, 50)\n>>>\n```\nThis is the normal behavior of the test in case numpy 2:\n```\n>>> import numpy as np\n>>> print(np.__version__)\n2.1.2\n```\n\n### Actual Results\n\n```\n>>> _py_sort(feature_values, samples, 50)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"_partitioner.pyx\", line 705, in sklearn.tree._partitioner._py_sort\nValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long'\n```\nThis behavior is reproduced if the test is run with numpy 1.26.4\n\n### Versions\n\n```shell\n>>> import sklearn\n>>> print(sklearn.__version__)\n1.6.1\n```","labels":["Bug"],"created_at":"2025-02-07T14:47:55Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30782"},{"issue_number":9,"repository":"scikit-learn\/scikit-learn","title":"Wrong Mutual Information Calculation","description":"### Describe the bug\n\n#### Issue\nI encountered a bug unexpectedly while reviewing some metrics in a project.\nWhen calculating mutual information using the `mutual_info_classif`, I noticed values higher than entropy, which is [impossible](https:\/\/en.wikipedia.org\/wiki\/Mutual_information#\/media\/File:Figchannel2017ab.svg). There is no such issue with `mutual_info_regression` (although, there, the self-mi is far from entropy, which may be another interesting case).\n\n##### Implication\nAny algorithm sorting features based on `mutual_info_classif` or any metric based on this function may be affected.\n\nThanks a lot for putting time on this.\n\n\nP.S. In the minimal example, the feature is fixed (all one). However, I encountered the same issue in other scenarios as well. The example is just more simplified. The problem persists on both Linux and Mac. I attached personal computer session info.\n\n### Steps\/Code to Reproduce\n\n```python\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_selection import mutual_info_classif\n\nbig_n = 1_000_000\nbug_df = pd.DataFrame({\n    'feature': np.ones(big_n),\n    'target': (np.arange(big_n) < 100).astype(int),\n})\nbug_df\n\nmi = mutual_info_classif(bug_df[['feature']], bug_df['target'])\nentropy = mutual_info_classif(bug_df[['target']], bug_df['target'])\n\nprint(f\"mi: {mi[0] :.6f}\")\nprint(f\"self-mi (entropy): {entropy[0] :.6f}\")\n\nfrom scipy import stats\n\nscipy_entropy = stats.entropy([bug_df['target'].mean(), 1 - bug_df['target'].mean()])\n\nprint(f\"scipy entropy: {scipy_entropy :.6f}\")\n```\n\n### Expected Results\n\n```\nmi: 0.000000\nself-mi (entropy): 0.001023\nscipy entropy: 0.001021\n```\n\n### Actual Results\n\n```\nmi: 0.215495\nself-mi (entropy): 0.001023\nscipy entropy: 0.001021\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.10.13 (main, Sep 11 2023, 08:16:02) [Clang 14.0.6 ]\nexecutable: \/Users\/*\/miniconda3\/envs\/*\/bin\/python\n   machine: macOS-15.1.1-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 23.3.1\n   setuptools: 68.2.2\n        numpy: 1.26.1\n        scipy: 1.15.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.8.2\n       joblib: 1.3.2\nthreadpoolctl: 3.2.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: \/Users\/*\/miniconda3\/envs\/*\/lib\/python3.10\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: armv8\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: \/Users\/*\/miniconda3\/envs\/*\/lib\/python3.10\/site-packages\/sklearn\/.dylibs\/libomp.dylib\n        version: None\n```","labels":["Bug","Needs Investigation"],"created_at":"2025-02-05T16:46:04Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30772"},{"issue_number":10,"repository":"scikit-learn\/scikit-learn","title":"MiniBatchKMeans not handling sample weights as expected","description":"### Describe the bug\n\nFollowing up from PR #29907, we realised that when passing sample weights any resampling should be done with weights and replacement before passing through to other operations. \n\nMiniBatchKMeans has a similar bug where minibatch_indices are not resampled with weights but instead weights are passed on to the subsequent minibatch_step which returns resulting in sample weight equivalence not being respected (i.e., repeating and weighting a sample n times behave the same with similar outputs).\n\n### Steps\/Code to Reproduce\n\n```python\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kstest,ttest_ind\nfrom sklearn.datasets import make_blobs\nimport numpy as np\n\nrng = np.random.RandomState(0)\n    \ncentres = np.array([[0, 0, 0], [0, 5, 5], [3, 1, 1], [2, 4, 4], [100, 8, 800]])\nX, y = make_blobs(\n    n_samples=300,\n    cluster_std=1,\n    centers=centres,\n    random_state=10,\n)\n# Create dataset with repetitions and corresponding sample weights\nsample_weight = rng.randint(0, 10, size=X.shape[0])\nX_resampled_by_weights = np.repeat(X, sample_weight, axis=0)\ny_resampled_by_weights = np.repeat(y,sample_weight)\n\npredictions_sw = []\npredictions_dup = []\npredictions_sw_mini = []\npredictions_dup_mini = []\n\nprediction_rank = np.argsort(y)[-1:]\n\nfor seed in range(100):\n\n    ## Fit estimator\n    est_sw = KMeans(random_state=seed,n_clusters=5).fit(X,y,sample_weight=sample_weight)\n    est_dup = KMeans(random_state=seed,n_clusters=5).fit(X_resampled_by_weights,y_resampled_by_weights)\n    est_sw_mini = MiniBatchKMeans(random_state=seed,n_clusters=5).fit(X,y,sample_weight=sample_weight)\n    est_dup_mini = MiniBatchKMeans(random_state=seed,n_clusters=5).fit(X_resampled_by_weights,y_resampled_by_weights)\n    \n    ##Get predictions\n    predictions_sw.append(est_sw.predict(X[prediction_rank]))\n    predictions_dup.append(est_dup.predict(X[prediction_rank]))\n    predictions_sw_mini.append(est_sw_mini.predict(X[prediction_rank]))\n    predictions_dup_mini.append(est_dup_mini.predict(X[prediction_rank]))\n\nfig = plt.figure(figsize=(10,5))\nax1=fig.add_subplot(1,2,1)\nax2=fig.add_subplot(1,2,2)\n\npredictions_sw = np.asarray(predictions_sw).flatten()\npredictions_dup = np.asarray(predictions_dup).flatten()\nax1.hist(predictions_sw)\nax1.hist(predictions_dup,alpha=0.5)\nax1.set_title(\"KMeans: %.2f\"%(kstest(predictions_sw,predictions_dup).pvalue))\n\npredictions_sw_mini = np.asarray(predictions_sw_mini).flatten()\npredictions_dup_mini = np.asarray(predictions_dup_mini).flatten()\nax2.hist(predictions_sw_mini,label=\"weighted\")\nax2.hist(predictions_dup_mini,label=\"repeated\",alpha=0.5)\nax2.set_title(\"MiniBatchKMeans: %.2f\"%(kstest(predictions_sw_mini,predictions_dup_mini).pvalue))\nplt.legend()\n```\n\n### Expected Results\n\nKMeans and Minibatch KMeans return similar histograms\n\n### Actual Results\n\n![Image](https:\/\/github.com\/user-attachments\/assets\/141ecb6e-96a5-412b-a0fb-6197f7634ae9)\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:13:44) [Clang 16.0.6 ]\nexecutable: \/Users\/shrutinath\/micromamba\/envs\/scikit-learn\/bin\/python\n   machine: macOS-14.3-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.7.dev0\n          pip: 24.0\n   setuptools: 75.8.0\n        numpy: 2.0.0\n        scipy: 1.14.0\n       Cython: 3.0.10\n       pandas: 2.2.2\n   matplotlib: 3.9.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libopenblas\n       filepath: \/Users\/shrutinath\/micromamba\/envs\/scikit-learn\/lib\/libopenblas.0.dylib\n        version: 0.3.27\nthreading_layer: openmp\n   architecture: VORTEX\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: \/Users\/shrutinath\/micromamba\/envs\/scikit-learn\/lib\/libomp.dylib\n        version: None\n```","labels":["Bug"],"created_at":"2025-02-02T18:34:33Z","comments":0,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30750"},{"issue_number":11,"repository":"scikit-learn\/scikit-learn","title":"Edge case bug in metadata routing","description":"### Describe the bug\n\nHello, while using metadata routing I encountered what seems to be a bug. I do not have enough understanding of metadata routing to determine if it is actually a bug or an incorrect use.\n\nBelow is an example where I am using a meta estimator (`BaggingRegressor`) around a base estimator (`DecisionTreeRegressor`). In my use case, I need to dynamically wrap the base estimator in an `Adapter` to do some work before calling the fit method of the base estimator. This work is based on an extra parameter `extra_param`, which I request using the `set_fit_request` method. The parameter is passed sucessfully, but its type is altered from string to list on one edge case (when the string matches the number of samples of X).\n\n### Steps\/Code to Reproduce\n\n```python\nimport numpy as np\nimport sklearn\nfrom sklearn import base, ensemble, tree\n\nsklearn.set_config(enable_metadata_routing=True)\n\n\nclass Adapter(base.BaseEstimator):\n    def __init__(self, wrapped_estimator):\n        self.wrapped_estimator = wrapped_estimator\n\n    def fit(self, X, y, extra_param: str):\n        # Do some work before delegating to the wrapped_estimator's fit method\n        print(extra_param)\n        assert isinstance(extra_param, str)\n        return self.wrapped_estimator.fit(X, y)\n\n    # Delegate other methods\n    def __getattr__(self, name):\n        return getattr(self.wrapped_estimator, name)\n\n\nn, p = 10, 2\nrng = np.random.default_rng(0)\nx = rng.random((n, p))\ny = rng.integers(0, 2, n)\n\nestimator = tree.DecisionTreeRegressor()\nadapter = Adapter(estimator)\nadapter.set_fit_request(extra_param=True)\nmeta_estimator = ensemble.BaggingRegressor(adapter, n_estimators=1)\n\nmeta_estimator.fit(x, y, extra_param=\"a\" * (n - 1))  # Pass\nmeta_estimator.fit(x, y, extra_param=\"a\" * (n + 1))  # Pass\nmeta_estimator.fit(x, y, extra_param=\"a\" * n)  # Fail\n```\n\n### Expected Results\n\nNo error is thrown. The `extra_param` string parameter passed to `Adapter.fit` should always be a string and thus the assertion should not fail.\n\n### Actual Results\n\nWhen the string length matches the number of samples, the string becomes a list, and the assertion fails.\n\n```\naaaaaaaaa\naaaaaaaaaaa\n['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n\nTraceback (most recent call last):\n  File \"minimal.py\", line 35, in <module>\n    meta_estimator.fit(x, y, extra_param=\"a\" * n)  # Fail\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...\/miniforge3\/envs\/test\/lib\/python3.12\/site-packages\/sklearn\/utils\/validation.py\", line 63, in inner_f\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"...\/miniforge3\/envs\/test\/lib\/python3.12\/site-packages\/sklearn\/base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...\/miniforge3\/envs\/test\/lib\/python3.12\/site-packages\/sklearn\/ensemble\/_bagging.py\", line 389, in fit\n    return self._fit(X, y, max_samples=self.max_samples, **fit_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...\/miniforge3\/envs\/test\/lib\/python3.12\/site-packages\/sklearn\/ensemble\/_bagging.py\", line 532, in _fit\n    all_results = Parallel(\n                  ^^^^^^^^^\n  File \"...\/miniforge3\/envs\/test\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py\", line 77, in __call__\n    return super().__call__(iterable_with_config)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...\/miniforge3\/envs\/test\/lib\/python3.12\/site-packages\/joblib\/parallel.py\", line 1918, in __call__\n    return output if self.return_generator else list(output)\n                                                ^^^^^^^^^^^^\n  File \"...\/miniforge3\/envs\/test\/lib\/python3.12\/site-packages\/joblib\/parallel.py\", line 1847, in _get_sequential_output\n    res = func(*args, **kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^\n  File \"...\/miniforge3\/envs\/test\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...\/miniforge3\/envs\/test\/lib\/python3.12\/site-packages\/sklearn\/ensemble\/_bagging.py\", line 197, in _parallel_build_estimators\n    estimator_fit(X_, y_, **fit_params_)\n  File \"minimal.py\", line 15, in fit\n    assert isinstance(extra_param, str)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.8 | packaged by conda-forge | (main, Dec  5 2024, 14:19:53) [Clang 18.1.8 ]\nexecutable: \/Users\/alexandreperez\/dev\/lib\/miniforge3\/envs\/fun-ltm1\/bin\/python\n   machine: macOS-15.2-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.3.1\n   setuptools: 75.8.0\n        numpy: 1.26.4\n        scipy: 1.15.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 14\n         prefix: libopenblas\n       filepath: \/Users\/alexandreperez\/dev\/lib\/miniforge3\/envs\/fun-ltm1\/lib\/python3.12\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib\n        version: 0.3.23.dev\nthreading_layer: pthreads\n   architecture: armv8\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 14\n         prefix: libomp\n       filepath: \/Users\/alexandreperez\/dev\/lib\/miniforge3\/envs\/fun-ltm1\/lib\/python3.12\/site-packages\/sklearn\/.dylibs\/libomp.dylib\n        version: None\n```","labels":["Bug","Documentation","wontfix","Metadata Routing"],"created_at":"2025-01-30T12:27:35Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30739"},{"issue_number":12,"repository":"scikit-learn\/scikit-learn","title":"`randomized_svd` incorrect for complex valued matrices","description":"### Describe the bug\n\nThe `randomized_svd` utility function accepts complex valued inputs without error, but the result is inconsistent with `scipy.linalg.svd`.\n\n### Steps\/Code to Reproduce\n\n```python\nimport numpy as np\nfrom scipy import linalg\nfrom sklearn.utils.extmath import randomized_svd\n\nrng = np.random.RandomState(42)\nX = rng.randn(100, 20) + 1j * rng.randn(100, 20)\n\n_, s, _ = linalg.svd(X)\n_, s2, _ = randomized_svd(X, n_components=5)\n\nprint(\"s:\", s[:5])\nprint(\"s2:\", s2[:5])\n```\n\n### Expected Results\n\nI expected the singular values to be numerically close.\n\n### Actual Results\n\n```\ns: [19.81481515 18.69019042 17.62107998 17.23689681 16.3148512 ]\ns2: [11.25690754  9.97157079  9.01542947  8.06160863  7.54068744]\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.11.4 (main, Jul  5 2023, 08:40:20) [Clang 14.0.6 ]\nexecutable: \/Users\/clane\/miniconda3\/bin\/python\n   machine: macOS-13.7-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.7.dev0\n          pip: 25.0\n   setuptools: 65.5.0\n        numpy: 2.2.2\n        scipy: 1.15.1\n       Cython: 3.0.11\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: \/Users\/clane\/Projects\/misc\/scikit-learn\/.venv\/lib\/python3.11\/site-packages\/numpy\/.dylibs\/libscipy_openblas64_.dylib\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: neoversen1\n\n       user_api: blas\n   internal_api: openblas\n    num_threads: 8\n         prefix: libscipy_openblas\n       filepath: \/Users\/clane\/Projects\/misc\/scikit-learn\/.venv\/lib\/python3.11\/site-packages\/scipy\/.dylibs\/libscipy_openblas.dylib\n        version: 0.3.28\nthreading_layer: pthreads\n   architecture: neoversen1\n\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 8\n         prefix: libomp\n       filepath: \/opt\/homebrew\/Cellar\/libomp\/19.1.3\/lib\/libomp.dylib\n        version: None\n```","labels":["Bug"],"created_at":"2025-01-30T01:40:26Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30736"},{"issue_number":13,"repository":"scikit-learn\/scikit-learn","title":"Error in `d2_log_loss_score` multiclass when one of the classes is missing in `y_true`.","description":"### Describe the bug\n\nHello, I encountered an error with the `d2_log_loss_score` in the multiclass setting (i.e. when `y_pred` has shape (n, k) with k >= 3) when one of the classes is missing from the `y_true` labels, even when giving the labels through the `labels` argument. The error disappear when all the classes are present in `y_true`.\n\n### Steps\/Code to Reproduce\n\n```python\nfrom sklearn.metrics import d2_log_loss_score\n\ny_true = [0, 1, 1]\ny_pred = [[1, 0, 0], [1, 0, 0], [1, 0, 0]]\nlabels = [0, 1, 2]\n\nd2_log_loss_score(y_true, y_pred, labels=labels)\n```\n\n### Expected Results\n\nNo error is thrown.\n\n### Actual Results\n\n```\nTraceback (most recent call last):\n  File \"minimal.py\", line 7, in <module>\n    d2_log_loss_score(y_true, y_pred, labels=labels)\n  File \"...\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py\", line 216, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"...\/python3.12\/site-packages\/sklearn\/metrics\/_classification.py\", line 3407, in d2_log_loss_score\n    denominator = log_loss(\n                  ^^^^^^^^^\n  File \"...\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py\", line 189, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"...\/python3.12\/site-packages\/sklearn\/metrics\/_classification.py\", line 3023, in log_loss\n    raise ValueError(\nValueError: The number of classes in labels is different from that in y_pred. Classes found in labels: [0 1 2]\n```\n\n### Versions\n\n```shell\nSystem:\n    python: 3.12.8 | packaged by conda-forge | (main, Dec  5 2024, 14:19:53) [Clang 18.1.8 ]\nexecutable: \/Users\/alexandreperez\/dev\/lib\/miniforge3\/envs\/test\/bin\/python\n   machine: macOS-15.2-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.3.1\n   setuptools: 75.8.0\n        numpy: 2.2.2\n        scipy: 1.15.1\n       Cython: None\n       pandas: None\n   matplotlib: None\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 14\n         prefix: libomp\n       filepath: ...\/miniforge3\/envs\/test\/lib\/python3.12\/site-packages\/sklearn\/.dylibs\/libomp.dylib\n        version: None\n```","labels":["Bug","Needs Investigation"],"created_at":"2025-01-24T11:01:39Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30713"},{"issue_number":14,"repository":"scikit-learn\/scikit-learn","title":"Possible bug in sklearn 1.6.1 PartialDependenceDisplay.from_estimator when target and feature are both binary","description":"### Describe the bug\n\nPartialDependenceDisplay.from_estimator does not seem able to handle dummy variables when the response variable is binary. See example below. The example works fine in 1.5.2 but returns `ValueError: cannot reshape array of size 1 into shape (2)` in 1.6.1\n\n### Steps\/Code to Reproduce\n\n```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import PartialDependenceDisplay\n\nnp.random.seed(42)\nn_samples = 1000\nage = np.random.normal(35, 10, n_samples)\nsmoker = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\nprob_disease = 1 \/ (1 + np.exp(-(age - 35) \/ 10 - 2 * smoker))\nheart_disease = (np.random.random(n_samples) < prob_disease).astype(int)\ndf = pd.DataFrame({\"age\": age, \"smoker\": smoker, \"heart_disease\": heart_disease})\nX = df[[\"age\", \"smoker\"]]\ny = df[\"heart_disease\"]\n\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X, y)\n\npdp_age = PartialDependenceDisplay.from_estimator(rf_model, X, features=[0, 1])\n```\n\n### Expected Results\n\nPDP plots for age and smoker.\n\n### Actual Results\n\n```tb\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[1], [line 19](vscode-notebook-cell:?execution_count=1&line=19)\n     [16](vscode-notebook-cell:?execution_count=1&line=16) rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n     [17](vscode-notebook-cell:?execution_count=1&line=17) rf_model.fit(X, y)\n---> [19](vscode-notebook-cell:?execution_count=1&line=19) pdp_age = PartialDependenceDisplay.from_estimator(rf_model, X, features=[0, 1])\n\nFile ~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:707, in PartialDependenceDisplay.from_estimator(cls, estimator, X, features, sample_weight, categorical_features, feature_names, target, response_method, n_cols, grid_resolution, percentiles, method, n_jobs, verbose, line_kw, ice_lines_kw, pd_line_kw, contour_kw, ax, kind, centered, subsample, random_state)\n    [701](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:701)         raise ValueError(\n    [702](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:702)             f\"When a floating-point, subsample={subsample} should be in \"\n    [703](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:703)             \"the (0, 1) range.\"\n    [704](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:704)         )\n    [706](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:706) # compute predictions and\/or averaged predictions\n--> [707](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:707) pd_results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n    [708](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:708)     delayed(partial_dependence)(\n    [709](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:709)         estimator,\n    [710](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:710)         X,\n    [711](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:711)         fxs,\n    [712](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:712)         sample_weight=sample_weight,\n    [713](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:713)         feature_names=feature_names,\n    [714](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:714)         categorical_features=categorical_features,\n    [715](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:715)         response_method=response_method,\n    [716](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:716)         method=method,\n    [717](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:717)         grid_resolution=grid_resolution,\n    [718](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:718)         percentiles=percentiles,\n    [719](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:719)         kind=kind_plot,\n    [720](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:720)     )\n    [721](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:721)     for kind_plot, fxs in zip(kind_, features)\n    [722](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:722) )\n    [724](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:724) # For multioutput regression, we can only check the validity of target\n    [725](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:725) # now that we have the predictions.\n    [726](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:726) # Also note: as multiclass-multioutput classifiers are not supported,\n    [727](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:727) # multiclass and multioutput scenario are mutually exclusive. So there is\n    [728](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:728) # no risk of overwriting target_idx here.\n    [729](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_plot\/partial_dependence.py:729) pd_result = pd_results[0]  # checking the first result is enough\n\nFile ~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:77, in Parallel.__call__(self, iterable)\n     [72](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:72) config = get_config()\n     [73](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:73) iterable_with_config = (\n     [74](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:74)     (_with_config(delayed_func, config), args, kwargs)\n     [75](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:75)     for delayed_func, args, kwargs in iterable\n     [76](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:76) )\n---> [77](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:77) return super().__call__(iterable_with_config)\n\nFile ~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1918, in Parallel.__call__(self, iterable)\n   [1916](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1916)     output = self._get_sequential_output(iterable)\n   [1917](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1917)     next(output)\n-> [1918](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1918)     return output if self.return_generator else list(output)\n   [1920](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1920) # Let's create an ID that uniquely identifies the current call. If the\n   [1921](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1921) # call is interrupted early and that the same instance is immediately\n   [1922](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1922) # re-used, this id will be used to prevent workers that were\n   [1923](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1923) # concurrently finalizing a task from the previous call to run the\n   [1924](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1924) # callback.\n   [1925](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1925) with self._lock:\n\nFile ~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1847, in Parallel._get_sequential_output(self, iterable)\n   [1845](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1845) self.n_dispatched_batches += 1\n   [1846](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1846) self.n_dispatched_tasks += 1\n-> [1847](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1847) res = func(*args, **kwargs)\n   [1848](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1848) self.n_completed_tasks += 1\n   [1849](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1849) self.print_progress()\n\nFile ~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:139, in _FuncWrapper.__call__(self, *args, **kwargs)\n    [137](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:137)     config = {}\n    [138](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:138) with config_context(**config):\n--> [139](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:139)     return self.function(*args, **kwargs)\n\nFile ~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:216, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\n    [210](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:210) try:\n    [211](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:211)     with config_context(\n    [212](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:212)         skip_parameter_validation=(\n    [213](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:213)             prefer_skip_nested_validation or global_skip_validation\n    [214](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:214)         )\n    [215](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:215)     ):\n--> [216](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:216)         return func(*args, **kwargs)\n    [217](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:217) except InvalidParameterError as e:\n    [218](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:218)     # When the function is just a wrapper around an estimator, we allow\n    [219](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:219)     # the function to delegate validation to the estimator, but we replace\n    [220](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:220)     # the name of the estimator by the name of the function in the error\n    [221](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:221)     # message to avoid confusion.\n    [222](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:222)     msg = re.sub(\n    [223](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:223)         r\"parameter of \\w+ must be\",\n    [224](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:224)         f\"parameter of {func.__qualname__} must be\",\n    [225](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:225)         str(e),\n    [226](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/utils\/_param_validation.py:226)     )\n\nFile ~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_partial_dependence.py:682, in partial_dependence(estimator, X, features, sample_weight, categorical_features, feature_names, response_method, percentiles, grid_resolution, method, kind)\n    [676](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_partial_dependence.py:676)     averaged_predictions = _partial_dependence_recursion(\n    [677](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_partial_dependence.py:677)         estimator, grid, features_indices\n    [678](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_partial_dependence.py:678)     )\n    [680](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_partial_dependence.py:680) # reshape averaged_predictions to\n    [681](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_partial_dependence.py:681) # (n_outputs, n_values_feature_0, n_values_feature_1, ...)\n--> [682](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_partial_dependence.py:682) averaged_predictions = averaged_predictions.reshape(\n    [683](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_partial_dependence.py:683)     -1, *[val.shape[0] for val in values]\n    [684](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_partial_dependence.py:684) )\n    [685](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_partial_dependence.py:685) pdp_results = Bunch(grid_values=values)\n    [687](https:\/\/file+.vscode-resource.vscode-cdn.net\/Users\/vnijs\/gh\/pyrsm\/examples\/model\/~\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/inspection\/_partial_dependence.py:687) if kind == \"average\":\n\nValueError: cannot reshape array of size 1 into shape (2)\n```\n### Versions\n\n```shell\nSystem:\n    python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]\nexecutable: \/Users\/vnijs\/miniconda\/envs\/msba\/bin\/python\n   machine: macOS-14.2.1-arm64-arm-64bit\n\nPython dependencies:\n      sklearn: 1.6.1\n          pip: 24.3.1\n   setuptools: 75.1.0\n        numpy: 2.2.1\n        scipy: 1.15.1\n       Cython: None\n       pandas: 2.2.3\n   matplotlib: 3.10.0\n       joblib: 1.4.2\nthreadpoolctl: 3.5.0\n\nBuilt with OpenMP: True\n\nthreadpoolctl info:\n       user_api: openmp\n   internal_api: openmp\n    num_threads: 16\n         prefix: libomp\n       filepath: \/Users\/vnijs\/miniconda\/envs\/msba\/lib\/python3.12\/site-packages\/sklearn\/.dylibs\/libomp.dylib\n        version: None\n```","labels":["Bug","Regression"],"created_at":"2025-01-20T00:00:08Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30675"},{"issue_number":15,"repository":"scikit-learn\/scikit-learn","title":"average_precision_score produces unexpected output when scoring a single sample","description":"### Describe the bug\n\nWhen using `average_precision_score` and scoring a single sample, the metric ignores `y_score` and will always produce a score of 1.0 if `y_true = [1]` and otherwise will return a score of 0. I would have expected that it would instead raise an exception.\r\n\r\nPotentially related to #30147, however I'm focusing on the minimal example with just a single sample.\n\n### Steps\/Code to Reproduce\n\n```python\r\nfrom sklearn.metrics import average_precision_score\r\n\r\ny_score = [0]\r\ny_true = [1]\r\nscore = average_precision_score(y_true=y_true, y_score=y_score)\r\nprint(score)  # 1.0\r\n\r\ny_score = [1]\r\ny_true = [1]\r\nscore = average_precision_score(y_true=y_true, y_score=y_score)\r\nprint(score)  # 1.0\r\n\r\ny_score = [0.5]\r\ny_true = [1]\r\nscore = average_precision_score(y_true=y_true, y_score=y_score)\r\nprint(score)  # 1.0\r\n\r\ny_score = [0]\r\ny_true = [0]\r\nscore = average_precision_score(y_true=y_true, y_score=y_score)\r\nprint(score)  # 0.0\r\n\r\ny_score = [1]\r\ny_true = [0]\r\nscore = average_precision_score(y_true=y_true, y_score=y_score)\r\nprint(score)  # 0.0\r\n\r\ny_score = [0.5]\r\ny_true = [0]\r\nscore = average_precision_score(y_true=y_true, y_score=y_score)\r\nprint(score)  # 0.0\r\n```\r\n\r\nAdditionally, you can see that the average_precision_score returns a score opposite of what precision and recall return:\r\n\r\n```python\r\nfrom sklearn.metrics import average_precision_score, precision_score, recall_score\r\n\r\ny_score = [0]\r\ny_true = [1]\r\n\r\nscore = average_precision_score(y_true=y_true, y_score=y_score)\r\nprint(score)  # 1.0\r\nscore = precision_score(y_true=y_true, y_pred=y_score)\r\nprint(score)  # 0.0\r\nscore = recall_score(y_true=y_true, y_pred=y_score)\r\nprint(score)  # 0.0\r\n```\n\n### Expected Results\n\nI would have expected the metric to raise an exception, similar to what happens when ROC_AUC is called with a single sample:\r\n\r\n```python\r\nscore = roc_auc_score(y_true=y_true, y_score=y_score)\r\nprint(score)\r\n\r\n```\r\n\r\n```\r\nValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\r\n```\n\n### Actual Results\n\nRefer to code snippets above.\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.11.10 | packaged by conda-forge | (main, Sep 30 2024, 18:08:57) [GCC 13.3.0]\r\nexecutable: \/opt\/conda\/envs\/ag-311\/bin\/python\r\n   machine: Linux-5.15.0-1056-aws-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.1\r\n          pip: 24.2\r\n   setuptools: 60.2.0\r\n        numpy: 1.26.4\r\n        scipy: 1.12.0\r\n       Cython: None\r\n       pandas: 2.2.3\r\n   matplotlib: 3.9.2\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 128\r\n         prefix: libopenblas\r\n       filepath: \/opt\/conda\/envs\/ag-311\/lib\/libopenblasp-r0.3.28.so\r\n        version: 0.3.28\r\nthreading_layer: pthreads\r\n   architecture: SapphireRapids\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 64\r\n         prefix: libopenblas\r\n       filepath: \/opt\/conda\/envs\/ag-311\/lib\/python3.11\/site-packages\/scipy.libs\/libopenblasp-r0-23e5df77.3.21.dev.so\r\n        version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n   architecture: Cooperlake\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 192\r\n         prefix: libgomp\r\n       filepath: \/opt\/conda\/envs\/ag-311\/lib\/python3.11\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n```\n```\n","labels":["Bug","Needs Investigation"],"created_at":"2025-01-09T00:41:41Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30615"},{"issue_number":16,"repository":"scikit-learn\/scikit-learn","title":"ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part.","description":"Hello Scikit-learn team,\r\n\r\nI am encountering an issue while running inference VotingClassifier model with `voting=\"hard\"` argument, I found that this issue may related to [NEP 34](https:\/\/numpy.org\/neps\/nep-0034-infer-dtype-is-object.html) restriction of `dtype=object` in numpy and the solution is downgrading to numpy `1.23.1`. However, it doesn't work in my case due to dependency conflicts with pandas and other packages. I'd appreciate if you could analyze this issue and provide an update when possible.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/training.py\", line 135, in <module>\r\n    ensemble_model, trained_models, model_results, ensemble_results = main(sparse=False)\r\n                                                                      ^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/training.py\", line 127, in main\r\n    trained_ensemble, ensemble_results = train_ensemble_model(\r\n                                         ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/training.py\", line 89, in train_ensemble_model\r\n    ensemble_results, trained_ensemble = train_and_evaluate_ensemble(voting_clf, X_train, X_test, y_train, y_test)\r\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/training\/ensemble_trainer.py\", line 33, in train_and_evaluate_ensemble\r\n    y_pred_ensemble = voting_clf.predict(X_test)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/.venv\/lib\/python3.11\/site-packages\/sklearn\/ensemble\/_voting.py\", line 443, in predict\r\n    predictions = self._predict(X)\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/.venv\/lib\/python3.11\/site-packages\/sklearn\/ensemble\/_voting.py\", line 80, in _predict\r\n    return np.asarray([est.predict(X) for est in self.estimators_]).T\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part.\r\n```\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```\r\ntry:\r\n  main_logger.info(\"Training ensemble\")\r\n  voting_clf.fit(X_train, y_train)\r\n  \r\n  main_logger.info(\"Evaluating ensemble\")\r\n  y_pred_ensemble = voting_clf.predict(X_test)\r\n  results = classification_report(y_test, y_pred_ensemble, output_dict=True)\r\n  main_logger.info(f\"Ensemble Results:\\n{classification_report(y_test, y_pred_ensemble)}\")\r\n  \r\n  return results, voting_clf\r\n\r\nexcept Exception as e:\r\n    main_logger.error(f\"Error in ensemble training: {str(e)}\")\r\n    raise\r\n```\r\n\r\n### Expected Results\r\n\r\n```Finish training```\r\n\r\n### Actual Results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/training.py\", line 135, in <module>\r\n    ensemble_model, trained_models, model_results, ensemble_results = main(sparse=False)\r\n                                                                      ^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/training.py\", line 127, in main\r\n    trained_ensemble, ensemble_results = train_ensemble_model(\r\n                                         ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/training.py\", line 89, in train_ensemble_model\r\n    ensemble_results, trained_ensemble = train_and_evaluate_ensemble(voting_clf, X_train, X_test, y_train, y_test)\r\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/training\/ensemble_trainer.py\", line 33, in train_and_evaluate_ensemble\r\n    y_pred_ensemble = voting_clf.predict(X_test)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/.venv\/lib\/python3.11\/site-packages\/sklearn\/ensemble\/_voting.py\", line 443, in predict\r\n    predictions = self._predict(X)\r\n                  ^^^^^^^^^^^^^^^^\r\n  File \"\/home\/mtoan65\/Documents\/Sentiment_Analysis\/.venv\/lib\/python3.11\/site-packages\/sklearn\/ensemble\/_voting.py\", line 80, in _predict\r\n    return np.asarray([est.predict(X) for est in self.estimators_]).T\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\n1.5.2\r\n```\r\n","labels":["Bug","Needs Info"],"created_at":"2024-12-27T13:47:54Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30546"},{"issue_number":17,"repository":"scikit-learn\/scikit-learn","title":"AttributeError: 'super' object has no attribute '__sklearn_tags__'","description":"### Describe the bug\r\n\r\n```python\r\nAttributeError                            Traceback (most recent call last)\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/IPython\/core\/formatters.py](https:\/\/localhost:8080\/#) in __call__(self, obj, include, exclude)\r\n    968 \r\n    969             if method is not None:\r\n--> 970                 return method(include=include, exclude=exclude)\r\n    971             return None\r\n    972         else:\r\n\r\n4 frames\r\n[\/usr\/local\/lib\/python3.10\/dist-packages\/sklearn\/base.py](https:\/\/localhost:8080\/#) in __sklearn_tags__(self)\r\n    538 \r\n    539     def __sklearn_tags__(self):\r\n--> 540         tags = super().__sklearn_tags__()\r\n    541         tags.estimator_type = \"classifier\"\r\n    542         tags.classifier_tags = ClassifierTags()\r\n\r\nAttributeError: 'super' object has no attribute '__sklearn_tags__'\r\n```\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n.\r\n\r\n### Expected Results\r\n\r\nWorking XGBClassifier model\r\n\r\n### Actual Results\r\n\r\nNone\r\n\r\n### Versions\r\n\r\n```shell\r\n1.6\r\n```\r\n","labels":["Bug"],"created_at":"2024-12-26T20:46:53Z","comments":13,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30542"},{"issue_number":18,"repository":"scikit-learn\/scikit-learn","title":"Feature Selectors fail to route metadata when inside a Pipeline","description":"### Describe the bug\r\n\r\nAccording to the [metadata routing docs](https:\/\/scikit-learn.org\/1.6\/metadata_routing.html#metadata-routing-support-status), Feature Selectors only have four classes that support metadata routing (as of v1.6):\r\n- [sklearn.feature_selection.RFE](https:\/\/scikit-learn.org\/1.6\/modules\/generated\/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)\r\n- [sklearn.feature_selection.RFECV](https:\/\/scikit-learn.org\/1.6\/modules\/generated\/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV)\r\n- [sklearn.feature_selection.SelectFromModel](https:\/\/scikit-learn.org\/1.6\/modules\/generated\/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel)\r\n- [sklearn.feature_selection.SequentialFeatureSelector](https:\/\/scikit-learn.org\/1.6\/modules\/generated\/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector)\r\n\r\nEach of these classes fail to route metadata when used inside a Pipeline object. When `sample_weight` is provided in the Pipeline's `**fit_params`, the failure to pass `sample_weight` to the feature selector's estimator may result in incorrect feature selection (e.g., when the relationship between the features and the response are materially impacted by `sample_weight`).\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nimport sklearn\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.feature_selection import SelectFromModel\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.pipeline import Pipeline\r\n\r\nsklearn.set_config(enable_metadata_routing=True)\r\n\r\nX, y = load_iris(return_X_y=True, as_frame=True)\r\nw = np.arange(len(X)) + 1\r\n\r\nreg = LinearRegression().set_fit_request(sample_weight=True)\r\npipeline_reg = LinearRegression().set_fit_request(sample_weight=True)\r\n\r\npipeline_fs = SelectFromModel(\r\n    reg,\r\n    threshold=-np.inf,\r\n    prefit=False,\r\n    max_features=len(X.columns),\r\n)\r\n\r\npipeline = Pipeline(\r\n    [\r\n        (\"feature_selector\", pipeline_fs),\r\n        (\"regressor\", pipeline_reg),\r\n    ]\r\n)\r\n\r\npipeline.fit(X, y, sample_weight=w)\r\nreg.fit(X, y, sample_weight=w)\r\n\r\ntest_passed = (\r\n    pipeline[\"feature_selector\"].estimator_.coef_.tolist()\r\n    == reg.coef_.tolist()\r\n)\r\n```\r\n\r\n### Expected Results\r\n\r\nThe expected result is `test_passed = True`. \r\n\r\ni.e., the internal estimator of the pipeline's `feature_selector` should have `coef_` that exactly match the `coef_` from having a copied estimator fit on the same input (e.g., `(X, y, sample_weight)`). \r\n\r\n### Actual Results\r\n\r\nThe coefficients don't match between the pipeline's `feature_selector.estimator_` and the copied estimator trained on the same input `(X,y,sample_weight)`.\r\n```\r\n>>> pipeline[\"feature_selector\"].estimator_.coef_.tolist() == reg.coef_.tolist()\r\nFalse\r\n>>> pipeline[\"feature_selector\"].estimator_.coef_\r\narray([-0.11190585, -0.04007949,  0.22864503,  0.60925205])\r\n>>> reg.coef_\r\narray([-0.14681895, -0.07652903,  0.28196639,  0.5732906 ])\r\n```\r\n\r\nRather the coefficients of the pipeline's `feature_selector.estimator_` matches those of a copied estimator fit only on `(X,y)` without `sample_weight`.\r\n```\r\n>>> reg.fit(X,y).coef_\r\narray([-0.11190585, -0.04007949,  0.22864503,  0.60925205])\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]\r\nexecutable: \/Users\/kschluns\/Library\/Caches\/pypoetry\/virtualenvs\/ds-sbraf-edgrceiw-py3.11\/bin\/python\r\n   machine: macOS-15.1.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.6.0\r\n          pip: 23.1.2\r\n   setuptools: 75.6.0\r\n        numpy: 1.26.4\r\n        scipy: 1.14.1\r\n       Cython: None\r\n       pandas: 2.2.3\r\n   matplotlib: 3.10.0\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: \/Users\/kschluns\/Library\/Caches\/pypoetry\/virtualenvs\/ds-sbraf-edgrceiw-py3.11\/lib\/python3.11\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: armv8\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libomp\r\n       filepath: \/Users\/kschluns\/Library\/Caches\/pypoetry\/virtualenvs\/ds-sbraf-edgrceiw-py3.11\/lib\/python3.11\/site-packages\/sklearn\/.dylibs\/libomp.dylib\r\n        version: None\r\n```","labels":["Bug"],"created_at":"2024-12-22T17:35:04Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30527"},{"issue_number":19,"repository":"scikit-learn\/scikit-learn","title":"OPTICS.fit leaks memory when called under VS Code's built-in debugger","description":"### Describe the bug\r\n\r\nRunning clustering algorithm with n_jobs parameter set to more than 1 thread causes memory leak each time algorithm is run.\r\nThis simple code causes additional memory leak at each loop cycle. The issue will not occur if i replace manifold reduction algorithm with precomputed features.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport gc\r\nimport numpy as np\r\nfrom sklearn.manifold import TSNE\r\nfrom sklearn.cluster import OPTICS\r\nimport psutil\r\nprocess = psutil.Process()\r\n\r\n\r\ndef main():\r\n    data = np.random.random((100, 100))\r\n    for _i in range(1, 50):\r\n        points = TSNE().fit_transform(data)\r\n        prediction = OPTICS(n_jobs=2).fit_predict(points)  # n_jobs!=1\r\n        points = None\r\n        prediction = None\r\n        del prediction\r\n        del points\r\n        gc.collect()\r\n        print(f\"{process.memory_info().rss \/ 1e6:.1f} MB\")\r\n\r\n\r\nmain()\r\n```\r\n\r\n### Expected Results\r\n\r\nProgram's memory usage nearly constant between loop cycles\r\n\r\n### Actual Results\r\n\r\nProgram's memory usage increases infinitely\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.6 (tags\/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]\r\nexecutable: .venv\\Scripts\\python.exe\r\n   machine: Windows-10-10.0.26100-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.6.0\r\n          pip: 24.3.1\r\n   setuptools: 63.2.0\r\n        numpy: 1.25.2\r\n        scipy: 1.14.1\r\n       Cython: None\r\n       pandas: 2.2.3\r\n   matplotlib: 3.10.0\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 16\r\n         prefix: vcomp\r\n       filepath: .venv\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 16\r\n         prefix: libopenblas\r\n       filepath: .venv\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: Cooperlake\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 16\r\n         prefix: libscipy_openblas\r\n       filepath: .venv\\Lib\\site-packages\\scipy.libs\\libscipy_openblas-5b1ec8b915dfb81d11cebc0788069d2d.dll\r\n        version: 0.3.27.dev\r\nthreading_layer: pthreads\r\n   architecture: Cooperlake\r\n```\r\n","labels":["Bug","Performance","Needs Investigation"],"created_at":"2024-12-21T15:50:53Z","comments":18,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30525"},{"issue_number":20,"repository":"scikit-learn\/scikit-learn","title":"`remainder='passthrough'` block is missing from `ColumnTransformer` HTML repr since 1.5","description":"In the following example, the `repr` of `ColumnTransformer` does not seem to work as I expect it:\r\n\r\nhttps:\/\/scikit-learn.org\/dev\/auto_examples\/inspection\/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py\r\n\r\n<img width=\"809\" alt=\"image\" src=\"https:\/\/github.com\/user-attachments\/assets\/f3f258e6-11f6-49ee-a9df-8c6a464fe792\" \/>\r\n\r\nInstead I would expect a `passthrough` block and a `OneHotEncoder` block in the `ColumnTransformer`.\r\nI think that we should check the reason and see if we can improve the rendering.","labels":["Bug"],"created_at":"2024-12-17T16:28:45Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30498"},{"issue_number":21,"repository":"scikit-learn\/scikit-learn","title":"Gaussian Mixture: Diagonal covariance vectors might contain unreasonably negative values when the input datatype is np.float32","description":"### Describe the bug\r\n\r\nThe Gaussian Mixture implementation shows numerical instabilities on single-precision floating point input numbers, that even large values of the regularization parameter reg_covar (like 0.1) cannot mitigate.\r\n\r\nMore specifically, diagonal covariance elements must not be negative. However, due to the numerical instabilities intrinsic to floating point arithmetic, they might end up being tiny negative numbers that reg_covar must compensate.\r\nIt turns out that, for some input float32 , the covariance can reach the unreasonable value of -0.99999979.\r\nThis is because squaring float32 numbers significantly magnifies their precision errors.\r\n\r\nThe proposed solution consists in converting float32 values to float64 before squaring them.\r\nCare must be taken to not increase memory consumption in the overall process.\r\nHence, as avgX_means is equal to avg_means2, the return value can be simplified.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.mixture import GaussianMixture\r\n\r\nmodel = GaussianMixture(n_components=2, covariance_type=\"spherical\", reg_covar=0.1)\r\nmodel.fit(np.array([[9999.0], [0.0]], dtype=np.float32))\r\nmodel.covariances_\r\n```\r\n\r\n### Expected Results\r\n\r\n```python\r\narray([0.1, 0.1])\r\n```\r\n\r\n### Actual Results\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nInput In [132], in <cell line: 49>()\r\n     45 skgm._estimate_gaussian_covariances_diag = _optimized_estimate_gaussian_covariances_diag\r\n     48 model = GaussianMixture(n_components=2,covariance_type=\"spherical\", reg_covar=0.1)\r\n---> 49 model.fit(np.array([[9999.0], [0.0]], dtype=np.float32))\r\n     50 model.covariances_\r\n\r\nFile [~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\mixture\\_base.py:200](http:\/\/localhost:8888\/lab\/tree\/~\/AppData\/Local\/Programs\/Python\/Python39\/lib\/site-packages\/sklearn\/mixture\/_base.py#line=199), in BaseMixture.fit(self, X, y)\r\n    174 def fit(self, X, y=None):\r\n    175     \"\"\"Estimate model parameters with the EM algorithm.\r\n    176 \r\n    177     The method fits the model ``n_init`` times and sets the parameters with\r\n   (...)\r\n    198         The fitted mixture.\r\n    199     \"\"\"\r\n--> 200     self.fit_predict(X, y)\r\n    201     return self\r\n\r\nFile [~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\mixture\\_base.py:253](http:\/\/localhost:8888\/lab\/tree\/~\/AppData\/Local\/Programs\/Python\/Python39\/lib\/site-packages\/sklearn\/mixture\/_base.py#line=252), in BaseMixture.fit_predict(self, X, y)\r\n    250 self._print_verbose_msg_init_beg(init)\r\n    252 if do_init:\r\n--> 253     self._initialize_parameters(X, random_state)\r\n    255 lower_bound = -np.inf if do_init else self.lower_bound_\r\n    257 if self.max_iter == 0:\r\n\r\nFile [~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\mixture\\_base.py:160](http:\/\/localhost:8888\/lab\/tree\/~\/AppData\/Local\/Programs\/Python\/Python39\/lib\/site-packages\/sklearn\/mixture\/_base.py#line=159), in BaseMixture._initialize_parameters(self, X, random_state)\r\n    155 else:\r\n    156     raise ValueError(\r\n    157         \"Unimplemented initialization method '%s'\" % self.init_params\r\n    158     )\r\n--> 160 self._initialize(X, resp)\r\n\r\nFile [~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:723](http:\/\/localhost:8888\/lab\/tree\/~\/AppData\/Local\/Programs\/Python\/Python39\/lib\/site-packages\/sklearn\/mixture\/_gaussian_mixture.py#line=722), in GaussianMixture._initialize(self, X, resp)\r\n    721 if self.precisions_init is None:\r\n    722     self.covariances_ = covariances\r\n--> 723     self.precisions_cholesky_ = _compute_precision_cholesky(\r\n    724         covariances, self.covariance_type\r\n    725     )\r\n    726 elif self.covariance_type == \"full\":\r\n    727     self.precisions_cholesky_ = np.array(\r\n    728         [\r\n    729             linalg.cholesky(prec_init, lower=True)\r\n    730             for prec_init in self.precisions_init\r\n    731         ]\r\n    732     )\r\n\r\nFile [~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\mixture\\_gaussian_mixture.py:347](http:\/\/localhost:8888\/lab\/tree\/~\/AppData\/Local\/Programs\/Python\/Python39\/lib\/site-packages\/sklearn\/mixture\/_gaussian_mixture.py#line=346), in _compute_precision_cholesky(covariances, covariance_type)\r\n    345 else:\r\n    346     if np.any(np.less_equal(covariances, 0.0)):\r\n--> 347         raise ValueError(estimate_precision_error_message)\r\n    348     precisions_chol = 1.0 \/ np.sqrt(covariances)\r\n    349 return precisions_chol\r\n\r\nValueError: Fitting the mixture model failed because some components have ill-defined empirical covariance (for instance caused by singleton or collapsed samples). Try to decrease the number of components, or increase reg_covar.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.13 (tags\/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\r\nexecutable: C:\\Users\\leonce.mekinda\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\r\n   machine: Windows-10-10.0.19043-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.1\r\n          pip: 22.0.4\r\n   setuptools: 58.1.0\r\n        numpy: 1.22.4\r\n        scipy: 1.8.1\r\n       Cython: 0.29.30\r\n       pandas: 1.4.3\r\n   matplotlib: 3.5.2\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\leonce.mekinda\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\leonce.mekinda\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\leonce.mekinda\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\scipy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\r\n        version: 0.3.17\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n","labels":["Bug","Numerical Stability"],"created_at":"2024-12-02T01:02:22Z","comments":20,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30382"},{"issue_number":22,"repository":"scikit-learn\/scikit-learn","title":"HTML display rendering poorly in vscode \"Dark High Contrast\" color theme","description":"### Describe the bug\n\nWhen I use vscode, I use the \"Dark High Contrast\" theme, as my eyes are tired. In this mode, some of the estimator names are not visible in the HTML display\n\n### Steps\/Code to Reproduce\n\nExecute the following code in a vscode (for instance a cell)\r\n```python\r\n# %%\r\nfrom sklearn.pipeline import make_pipeline\r\nfrom sklearn.decomposition import PCA\r\nfrom sklearn.ensemble import HistGradientBoostingRegressor\r\n\r\npipe = make_pipeline(PCA(), HistGradientBoostingRegressor())\r\npipe\r\n```\n\n### Expected Results\n\nWith the \"Dark (Visual Studio)\" theme, the result is:\r\n![image](https:\/\/github.com\/user-attachments\/assets\/1c8d52d4-ce8c-4e8a-a217-fc68be2f2f70)\r\n\n\n### Actual Results\n\nHowever, with the \"Dark High Contrast\", the result is\r\n![image](https:\/\/github.com\/user-attachments\/assets\/a229f0dd-c71f-4744-9733-00a82d5258c0)\r\n\r\nNote that the title of the enclosing meta-estimator, here \"Pipeline\", is not visible\n\n### Versions\n\n```shell\ngit main of today (last commit: 426e6be923e34f68bc720ae625c8ca258f473265, merge of #30347)\r\n\r\nSystem:\r\n    python: 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]\r\nexecutable: \/bin\/python3\r\n   machine: Linux-6.8.0-49-generic-x86_64-with-glibc2.39\r\n\r\nPython dependencies:\r\n      sklearn: 1.7.dev0\r\n          pip: 24.0\r\n   setuptools: 68.1.2\r\n        numpy: 1.26.4\r\n        scipy: 1.11.4\r\n       Cython: 3.0.11\r\n       pandas: 2.1.4+dfsg\r\n   matplotlib: 3.6.3\r\n       joblib: 1.3.2\r\nthreadpoolctl: 3.1.0\r\n```\n```\n","labels":["Bug","frontend"],"created_at":"2024-11-27T20:10:36Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30357"},{"issue_number":23,"repository":"scikit-learn\/scikit-learn","title":"Hang when fitting `SVC` to a specific dataset","description":"### Describe the bug\r\n\r\nI am trying to fit an [`SVC`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html) to a specific dataset. The training process gets stuck, never finishing.\r\n\r\nscikit-learn uses a fork of LIBSVM [version 3.10.0](https:\/\/github.com\/scikit-learn\/scikit-learn\/blame\/caaa1f52a0632294bf951a9283d015f7b5dd5dd5\/sklearn\/svm\/src\/libsvm\/svm.h#L4) from [2011](https:\/\/github.com\/cjlin1\/libsvm\/releases\/tag\/v310). The equivalent code using a newer version of LIBSVM succeeds, suggesting that there is an upstream bug fix that scikit-learn could merge in.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n[libsvm_problematic_dataset.csv](https:\/\/github.com\/user-attachments\/files\/17927924\/libsvm_problematic_dataset.csv)\r\n\r\n```python\r\nimport logging\r\n\r\nfrom polars import read_csv\r\nfrom sklearn.svm import SVC\r\n\r\n_logger = logging.getLogger(__name__)\r\n\r\n\r\ndef main():\r\n    dataset = read_csv(\r\n        source='libsvm_problematic_dataset.csv'\r\n    )\r\n\r\n    x = dataset.select('feature').to_numpy()\r\n    y = dataset['label'].to_numpy()\r\n\r\n    _logger.info(\"Attempting to reproduce issue. If reproduced, the program will not exit.\")\r\n\r\n    SVC(\r\n        C=100,\r\n        kernel='poly',\r\n        degree=4,\r\n        gamma=0.9597420397825849,\r\n        tol=0.01,\r\n        cache_size=1000,\r\n        class_weight={\r\n            0: 1.04884106,\r\n            1: 0.95550528\r\n        },\r\n        verbose=True\r\n    ).fit(X=x, y=y)\r\n\r\n    _logger.error(\"The issue was not reproduced.\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    logging.basicConfig(level=logging.DEBUG)\r\n\r\n    main()\r\n```\r\n\r\n### Expected Results\r\n\r\n```\r\nINFO:__main__:Attempting to reproduce issue. If reproduced, the program will not exit.\r\n.................................................................................................\r\nWARNING: using -h 0 may be faster\r\n*..............................\r\nWARNING: using -h 0 may be faster\r\n*.............\r\nWARNING: using -h 0 may be faster\r\n*..................................................................\r\nWARNING: using -h 0 may be faster\r\n*..........................\r\nWARNING: using -h 0 may be faster\r\n*..........\r\nWARNING: using -h 0 may be faster\r\n*..............\r\nWARNING: using -h 0 may be faster\r\n*................\r\nWARNING: using -h 0 may be faster\r\n*................\r\nWARNING: using -h 0 may be faster\r\n*...........................\r\nWARNING: using -h 0 may be faster\r\n*..............\r\nWARNING: using -h 0 may be faster\r\n*.............\r\nWARNING: using -h 0 may be faster\r\n*.............\r\nWARNING: using -h 0 may be faster\r\n*.............................\r\nWARNING: using -h 0 may be faster\r\n*............................................\r\nWARNING: using -h 0 may be faster\r\n*.......\r\nWARNING: using -h 0 may be faster\r\n*......................\r\nWARNING: using -h 0 may be faster\r\n*.............\r\nWARNING: using -h 0 may be faster\r\n*.\r\nWARNING: using -h 0 may be faster\r\n*\r\noptimization finished, #iter = 460766\r\nobj = -245114.248664, rho = 1.000020\r\nnSV = 2452, nBSV = 2450\r\nTotal nSV = 2452\r\nERROR:__main__:The issue was not reproduced.\r\n```\r\n\r\nThis expected result was generated using LIBSVM [version 3.30.0](https:\/\/pypi.org\/project\/libsvm-official\/3.30.0\/) with the following code:\r\n```python\r\nimport logging\r\n\r\nfrom libsvm.svmutil import svm_train\r\nfrom polars import read_csv\r\n\r\n_logger = logging.getLogger(__name__)\r\n\r\n\r\ndef main():\r\n    dataset = read_csv(\r\n        source='libsvm_problematic_dataset.csv'\r\n    )\r\n\r\n    x = dataset.select('feature').to_numpy()\r\n    y = dataset['label'].to_numpy()\r\n\r\n    _logger.info(\"Attempting to reproduce issue. If reproduced, the program will not exit.\")\r\n\r\n    svm_train(\r\n        y, x,\r\n        [\r\n            '-s', 0,\r\n            '-t', 1,\r\n            '-d', 4,\r\n            '-g', 0.9597420397825849,\r\n            '-c', 100,\r\n            '-m', 1000,\r\n            '-e', 0.01,\r\n            '-w0', 1.04884106,\r\n            '-w1', 0.95550528\r\n        ]\r\n    )\r\n\r\n    _logger.error(\"The issue was not reproduced.\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    logging.basicConfig(level=logging.DEBUG)\r\n\r\n    main()\r\n```\r\n\r\n### Actual Results\r\n\r\n```\r\nINFO:__main__:Attempting to reproduce issue. If reproduced, the program will not exit.\r\n[LibSVM].....................................................\r\nWarning: using -h 0 may be faster\r\n*............\r\nWarning: using -h 0 may be faster\r\n*.....\r\nWarning: using -h 0 may be faster\r\n*.................\r\nWarning: using -h 0 may be faster\r\n*.....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\r\n```\r\n\r\nThe program never exits.\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.11.10 | packaged by conda-forge | (main, Oct 16 2024, 01:26:25) [Clang 17.0.6 ]\r\nexecutable: \/opt\/homebrew\/Caskroom\/miniforge\/base\/envs\/libsvm-debugging\/bin\/python\r\n   machine: macOS-14.7.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.2\r\n          pip: 24.3.1\r\n   setuptools: 75.6.0\r\n        numpy: 2.1.3\r\n        scipy: 1.14.1\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: None\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 10\r\n         prefix: libopenblas\r\n       filepath: \/opt\/homebrew\/Caskroom\/miniforge\/base\/envs\/libsvm-debugging\/lib\/libopenblas.0.dylib\r\n        version: 0.3.28\r\nthreading_layer: openmp\r\n   architecture: VORTEX\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 10\r\n         prefix: libomp\r\n       filepath: \/opt\/homebrew\/Caskroom\/miniforge\/base\/envs\/libsvm-debugging\/lib\/libomp.dylib\r\n        version: None\r\n```\r\n","labels":["Bug","Needs Investigation"],"created_at":"2024-11-27T02:41:11Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30353"},{"issue_number":24,"repository":"scikit-learn\/scikit-learn","title":"NuSVC argument `class_weight` is not used","description":"### Describe the bug\r\n\r\nLike `SVC`, the class `NuSVC` takes argument `class_weight`. However, it looks like this argument is not used. After a quick look at the libsvm C code within sklearn as well as [libsvm's original documentation](https:\/\/www.csie.ntu.edu.tw\/~cjlin\/libsvm\/), this seems to be expected: \"`wi` set the parameter C of class i to weight*C, for C-SVC\". I suggest that this argument should be removed from `NuSVC`'s constructor and from the documentation.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.svm import SVC, NuSVC\r\n\r\nX = [[1., 2, 3], [0, 5, 2]]\r\ny = [-1, 1]\r\n\r\nNuSVC(verbose=True).fit(X, y).dual_coef_\r\n\r\noptimization finished, #iter = 0\r\nC = 2.587063\r\nobj = 1.293532, rho = 0.000000\r\nnSV = 2, nBSV = 0\r\nTotal nSV = 2\r\nOut: [LibSVM]array([[-1.29353162,  1.29353162]])\r\n\r\nSVC(C=2.587063, verbose=True).fit(X, y).dual_coef_\r\n\r\noptimization finished, #iter = 1\r\nobj = -1.293532, rho = 0.000000\r\nnSV = 2, nBSV = 0\r\nTotal nSV = 2\r\nOut: [LibSVM]array([[-1.29353162,  1.29353162]])\r\n\r\nNuSVC(class_weight={-1:1.5, 1:.2}, verbose=True).fit(X, y).dual_coef_\r\n\r\noptimization finished, #iter = 0\r\nC = 2.587063\r\nobj = 1.293532, rho = 0.000000\r\nnSV = 2, nBSV = 0\r\nTotal nSV = 2\r\nOut: [LibSVM]array([[-1.29353162,  1.29353162]])\r\n\r\nSVC(C=2.587063, class_weight={-1:1.5, 1:.2}, verbose=True).fit(X, y).dual_coef_\r\n\r\noptimization finished, #iter = 1\r\nobj = -0.827860, rho = -0.600000\r\nnSV = 2, nBSV = 1\r\nTotal nSV = 2\r\nOut: [LibSVM]array([[-0.5174126,  0.5174126]])\r\n\r\n\r\nNuSVC(class_weight={-1:0, 1:0}).fit(X, y).dual_coef_\r\nOut: array([[-1.29353162,  1.29353162]])\r\n\r\nSVC(class_weight={-1:0, 1:0}).fit(X, y).dual_coef_\r\nOut: array([], shape=(1, 0), dtype=float64)\r\n```\r\n\r\n\r\n### Expected Results\r\n\r\nAs in the case of no `class_weight`, `NuSVM` should give the same `dual_coef_` as an `SVC` with the same `C`.\r\nAlso `class_weight={-1:0, 1:0}` should give the \"empty\" result.\r\n\r\n### Actual Results\r\n\r\nIn all cases above `NuSVM` with class weight behaves exactly as when no weights are given.\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03)  [GCC 11.3.0]\r\nexecutable: ...\/bin\/python3.9\r\n   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.2\r\n          pip: 23.0.1\r\n   setuptools: 67.6.0\r\n        numpy: 2.0.2\r\n        scipy: 1.13.1\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: 3.9.2\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: blis\r\n    num_threads: 1\r\n         prefix: libblis\r\n       filepath: ...\/lib\/libblis.so.4.0.0\r\n        version: 0.9.0\r\nthreading_layer: pthreads\r\n   architecture: skx\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libgomp\r\n       filepath: ...\/lib\/libgomp.so.1.0.0\r\n        version: None\r\n```\r\n","labels":["Bug","Needs Investigation"],"created_at":"2024-11-22T13:37:27Z","comments":15,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30332"},{"issue_number":25,"repository":"scikit-learn\/scikit-learn","title":"Error with set_output(transform='pandas') in ColumnTransformer when using OneHotEncoder with sparse output in intermediate steps","description":"### Describe the bug\r\n\r\n**Explanation**\r\n\r\nUsing the ColumnTransformer with set_output(transform='pandas') raises an error when there is a sparse intermediate output, even if the final output is dense. The error suggests setting sparse_output=False in OneHotEncoder, even though the intermediate sparse output should not impact the final dense output after transformations like TruncatedSVD.\r\n\r\n\r\nThe transformer raises this error even though the final output is dense due to the use of TruncatedSVD, which converts the intermediate sparse output to a dense matrix. The requirement to specify sparse_output=False for OneHotEncoder should not be enforced here, as the final output does not contain sparse data.\r\n\r\n**Suggested Fix**\r\n\r\nThis check should be modified to allow cases where the final output is dense, regardless of intermediate sparse representations.\r\n\r\n\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.pipeline import make_pipeline\r\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\r\nfrom sklearn.decomposition import TruncatedSVD\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.datasets import load_diabetes\r\nimport pandas as pd\r\n\r\nds = load_diabetes()\r\ndf = pd.DataFrame(ds['data'], columns=ds['feature_names'])\r\n\r\nct = ColumnTransformer([\r\n    ('ohe_tsvd', make_pipeline(OneHotEncoder(), TruncatedSVD()), ['sex']),\r\n    ('mm', MinMaxScaler(), ['age', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']),\r\n]).set_output(transform='pandas')\r\n\r\nct.fit_transform(df)\r\n```\r\n\r\n\r\n### Expected Results\r\n\r\npandas DataFrame as follow\r\n\r\n\r\nohe_mm__truncatedsvd0 | ohe_mm__truncatedsvd1 | mm__age | mm__bmi | mm__bp | mm__s1 | mm__s2 | mm__s3 | mm__s4 | mm__s5 | mm__s6\r\n-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --\r\n0.0 | 1.0 | 0.666667 | 0.582645 | 0.549296 | 0.294118 | 0.256972 | 0.207792 | 0.282087 | 0.562217 | 0.439394\r\n1.0 | 0.0 | 0.483333 | 0.148760 | 0.352113 | 0.421569 | 0.306773 | 0.623377 | 0.141044 | 0.222437 | 0.166667\r\n0.0 | 1.0 | 0.883333 | 0.516529 | 0.436620 | 0.289216 | 0.258964 | 0.246753 | 0.282087 | 0.496578 | 0.409091\r\n1.0 | 0.0 | 0.083333 | 0.301653 | 0.309859 | 0.495098 | 0.447211 | 0.233766 | 0.423131 | 0.572923 | 0.469697\r\n1.0 | 0.0 | 0.516667 | 0.206612 | 0.549296 | 0.465686 | 0.417331 | 0.389610 | 0.282087 | 0.362385 | 0.333333\r\n\r\n### Actual Results\r\n\r\n```pytb\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[19], line 10\r\n      5 df = pd.DataFrame(ds['data'], columns=ds['feature_names'])\r\n      6 ct = ColumnTransformer([\r\n      7     ('ohe_mm', make_pipeline(OneHotEncoder(), TruncatedSVD()), ['sex']),\r\n      8     ('mm', MinMaxScaler(), ['age', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']),\r\n      9 ]).set_output(transform='pandas')\r\n---> 10 ct.fit_transform(df)\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/utils\/_set_output.py:316](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/utils\/_set_output.py#line=315), in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)\r\n    314 @wraps(f)\r\n    315 def wrapped(self, X, *args, **kwargs):\r\n--> 316     data_to_wrap = f(self, X, *args, **kwargs)\r\n    317     if isinstance(data_to_wrap, tuple):\r\n    318         # only wrap the first output for cross decomposition\r\n    319         return_tuple = (\r\n    320             _wrap_data_with_container(method, data_to_wrap[0], X, self),\r\n    321             *data_to_wrap[1:],\r\n    322         )\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/base.py:1473](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/base.py#line=1472), in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\r\n   1466     estimator._validate_params()\r\n   1468 with config_context(\r\n   1469     skip_parameter_validation=(\r\n   1470         prefer_skip_nested_validation or global_skip_validation\r\n   1471     )\r\n   1472 ):\r\n-> 1473     return fit_method(estimator, *args, **kwargs)\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/compose\/_column_transformer.py:976](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/compose\/_column_transformer.py#line=975), in ColumnTransformer.fit_transform(self, X, y, **params)\r\n    973 else:\r\n    974     routed_params = self._get_empty_routing()\r\n--> 976 result = self._call_func_on_transformers(\r\n    977     X,\r\n    978     y,\r\n    979     _fit_transform_one,\r\n    980     column_as_labels=False,\r\n    981     routed_params=routed_params,\r\n    982 )\r\n    984 if not result:\r\n    985     self._update_fitted_transformers([])\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/compose\/_column_transformer.py:885](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/compose\/_column_transformer.py#line=884), in ColumnTransformer._call_func_on_transformers(self, X, y, func, column_as_labels, routed_params)\r\n    873             extra_args = {}\r\n    874         jobs.append(\r\n    875             delayed(func)(\r\n    876                 transformer=clone(trans) if not fitted else trans,\r\n   (...)\r\n    882             )\r\n    883         )\r\n--> 885     return Parallel(n_jobs=self.n_jobs)(jobs)\r\n    887 except ValueError as e:\r\n    888     if \"Expected 2D array, got 1D array instead\" in str(e):\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:74](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py#line=73), in Parallel.__call__(self, iterable)\r\n     69 config = get_config()\r\n     70 iterable_with_config = (\r\n     71     (_with_config(delayed_func, config), args, kwargs)\r\n     72     for delayed_func, args, kwargs in iterable\r\n     73 )\r\n---> 74 return super().__call__(iterable_with_config)\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1918](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/joblib\/parallel.py#line=1917), in Parallel.__call__(self, iterable)\r\n   1916     output = self._get_sequential_output(iterable)\r\n   1917     next(output)\r\n-> 1918     return output if self.return_generator else list(output)\r\n   1920 # Let's create an ID that uniquely identifies the current call. If the\r\n   1921 # call is interrupted early and that the same instance is immediately\r\n   1922 # re-used, this id will be used to prevent workers that were\r\n   1923 # concurrently finalizing a task from the previous call to run the\r\n   1924 # callback.\r\n   1925 with self._lock:\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/joblib\/parallel.py:1847](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/joblib\/parallel.py#line=1846), in Parallel._get_sequential_output(self, iterable)\r\n   1845 self.n_dispatched_batches += 1\r\n   1846 self.n_dispatched_tasks += 1\r\n-> 1847 res = func(*args, **kwargs)\r\n   1848 self.n_completed_tasks += 1\r\n   1849 self.print_progress()\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py:136](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/utils\/parallel.py#line=135), in _FuncWrapper.__call__(self, *args, **kwargs)\r\n    134     config = {}\r\n    135 with config_context(**config):\r\n--> 136     return self.function(*args, **kwargs)\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/pipeline.py:1310](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/pipeline.py#line=1309), in _fit_transform_one(transformer, X, y, weight, message_clsname, message, params)\r\n   1308 with _print_elapsed_time(message_clsname, message):\r\n   1309     if hasattr(transformer, \"fit_transform\"):\r\n-> 1310         res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\r\n   1311     else:\r\n   1312         res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\r\n   1313             X, **params.get(\"transform\", {})\r\n   1314         )\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/base.py:1473](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/base.py#line=1472), in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\r\n   1466     estimator._validate_params()\r\n   1468 with config_context(\r\n   1469     skip_parameter_validation=(\r\n   1470         prefer_skip_nested_validation or global_skip_validation\r\n   1471     )\r\n   1472 ):\r\n-> 1473     return fit_method(estimator, *args, **kwargs)\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/pipeline.py:533](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/pipeline.py#line=532), in Pipeline.fit_transform(self, X, y, **params)\r\n    490 \"\"\"Fit the model and transform with the final estimator.\r\n    491 \r\n    492 Fit all the transformers one after the other and sequentially transform\r\n   (...)\r\n    530     Transformed samples.\r\n    531 \"\"\"\r\n    532 routed_params = self._check_method_params(method=\"fit_transform\", props=params)\r\n--> 533 Xt = self._fit(X, y, routed_params)\r\n    535 last_step = self._final_estimator\r\n    536 with _print_elapsed_time(\"Pipeline\", self._log_message(len(self.steps) - 1)):\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/pipeline.py:406](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/pipeline.py#line=405), in Pipeline._fit(self, X, y, routed_params)\r\n    404     cloned_transformer = clone(transformer)\r\n    405 # Fit or load from cache the current transformer\r\n--> 406 X, fitted_transformer = fit_transform_one_cached(\r\n    407     cloned_transformer,\r\n    408     X,\r\n    409     y,\r\n    410     None,\r\n    411     message_clsname=\"Pipeline\",\r\n    412     message=self._log_message(step_idx),\r\n    413     params=routed_params[name],\r\n    414 )\r\n    415 # Replace the transformer of the step with the fitted\r\n    416 # transformer. This is necessary when loading the transformer\r\n    417 # from the cache.\r\n    418 self.steps[step_idx] = (name, fitted_transformer)\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/joblib\/memory.py:312](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/joblib\/memory.py#line=311), in NotMemorizedFunc.__call__(self, *args, **kwargs)\r\n    311 def __call__(self, *args, **kwargs):\r\n--> 312     return self.func(*args, **kwargs)\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/pipeline.py:1310](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/pipeline.py#line=1309), in _fit_transform_one(transformer, X, y, weight, message_clsname, message, params)\r\n   1308 with _print_elapsed_time(message_clsname, message):\r\n   1309     if hasattr(transformer, \"fit_transform\"):\r\n-> 1310         res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\r\n   1311     else:\r\n   1312         res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\r\n   1313             X, **params.get(\"transform\", {})\r\n   1314         )\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/utils\/_set_output.py:316](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/utils\/_set_output.py#line=315), in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)\r\n    314 @wraps(f)\r\n    315 def wrapped(self, X, *args, **kwargs):\r\n--> 316     data_to_wrap = f(self, X, *args, **kwargs)\r\n    317     if isinstance(data_to_wrap, tuple):\r\n    318         # only wrap the first output for cross decomposition\r\n    319         return_tuple = (\r\n    320             _wrap_data_with_container(method, data_to_wrap[0], X, self),\r\n    321             *data_to_wrap[1:],\r\n    322         )\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/base.py:1098](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/base.py#line=1097), in TransformerMixin.fit_transform(self, X, y, **fit_params)\r\n   1083         warnings.warn(\r\n   1084             (\r\n   1085                 f\"This object ({self.__class__.__name__}) has a `transform`\"\r\n   (...)\r\n   1093             UserWarning,\r\n   1094         )\r\n   1096 if y is None:\r\n   1097     # fit method of arity 1 (unsupervised transformation)\r\n-> 1098     return self.fit(X, **fit_params).transform(X)\r\n   1099 else:\r\n   1100     # fit method of arity 2 (supervised transformation)\r\n   1101     return self.fit(X, y, **fit_params).transform(X)\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/utils\/_set_output.py:316](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/utils\/_set_output.py#line=315), in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)\r\n    314 @wraps(f)\r\n    315 def wrapped(self, X, *args, **kwargs):\r\n--> 316     data_to_wrap = f(self, X, *args, **kwargs)\r\n    317     if isinstance(data_to_wrap, tuple):\r\n    318         # only wrap the first output for cross decomposition\r\n    319         return_tuple = (\r\n    320             _wrap_data_with_container(method, data_to_wrap[0], X, self),\r\n    321             *data_to_wrap[1:],\r\n    322         )\r\n\r\nFile [~\/python312\/lib\/python3.12\/site-packages\/sklearn\/preprocessing\/_encoders.py:1012](http:\/\/26.2.133.182:8888\/lab\/tree\/jnote\/sunkusun9\/kaggle\/PGS4_ep11\/result\/python312\/lib\/python3.12\/site-packages\/sklearn\/preprocessing\/_encoders.py#line=1011), in OneHotEncoder.transform(self, X)\r\n   1010 if transform_output != \"default\" and self.sparse_output:\r\n   1011     capitalize_transform_output = transform_output.capitalize()\r\n-> 1012     raise ValueError(\r\n   1013         f\"{capitalize_transform_output} output does not support sparse data.\"\r\n   1014         f\" Set sparse_output=False to output {transform_output} dataframes or\"\r\n   1015         f\" disable {capitalize_transform_output} output via\"\r\n   1016         '` ohe.set_output(transform=\"default\").'\r\n   1017     )\r\n   1019 # validation of X happens in _check_X called by _transform\r\n   1020 warn_on_unknown = self.drop is not None and self.handle_unknown in {\r\n   1021     \"ignore\",\r\n   1022     \"infrequent_if_exist\",\r\n   1023 }\r\n\r\nValueError: Pandas output does not support sparse data. Set sparse_output=False to output pandas dataframes or disable Pandas output via` ohe.set_output(transform=\"default\").\r\n```\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.12.6 (main, Sep 30 2024, 02:19:13) [GCC 9.4.0]\r\nexecutable: \/home\/sun9sun9\/python312\/bin\/python3.12\r\n   machine: Linux-5.4.0-196-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.2\r\n          pip: 24.3.1\r\n   setuptools: 75.2.0\r\n        numpy: 1.26.4\r\n        scipy: 1.12.0\r\n       Cython: None\r\n       pandas: 2.2.3\r\n   matplotlib: 3.8.4\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: \/home\/sun9sun9\/python312\/lib\/python3.12\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: \/home\/sun9sun9\/python312\/lib\/python3.12\/site-packages\/scipy.libs\/libopenblasp-r0-23e5df77.3.21.dev.so\r\n        version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libgomp\r\n       filepath: \/home\/sun9sun9\/python312\/lib\/python3.12\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n```\r\n","labels":["Bug","Enhancement"],"created_at":"2024-11-20T05:43:16Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30310"},{"issue_number":26,"repository":"scikit-learn\/scikit-learn","title":"ColumnTransformer does not validate sparse formats for X","description":"### Describe the bug\n\nIf the underlying transformers all accept sparse input data, `ColumnTransformer` should also be able to accept sparse input data. That's indeed the case for the `csr`, `csc`, `lil` and `dok` formats but it raises errors for the `bsr`, `coo`, `dia` formats because those are not \"subscriptable\". \r\n\r\nAs a possible fix, we could validate sparse input data by using `accept_sparse=(\"csr\", \"csc\", \"lil\", \"dok\")` which will then convert to a \"subscriptable\" sparse format. Currently it is not done as `ColumnTransformer` relies on its own `_check_X` which often entirely bypasses the validation, maybe for performance reasons ?\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nfrom scipy.sparse import dia_array\r\nfrom sklearn.compose import ColumnTransformer\r\n\r\nrng = np.random.RandomState(1)\r\nX = rng.uniform(size=(10, 3))\r\ny = rng.randint(0, 3, size=10)\r\nX = dia_array(X)\r\n\r\nest = ColumnTransformer(transformers=[('trans1','passthrough',[0,1])])\r\nest.fit(X, y)\r\n```\n\n### Expected Results\n\nNo error is thrown.\n\n### Actual Results\n\n```python-traceback\r\nTypeError: 'dia_array' object is not subscriptable\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:32:50) [Clang 16.0.6 ]\r\nexecutable: \/Users\/abaker\/miniforge3\/envs\/sklearn-dev\/bin\/python\r\n   machine: macOS-14.5-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.6.dev0\r\n          pip: 24.2\r\n   setuptools: 73.0.1\r\n        numpy: 2.1.0\r\n        scipy: 1.14.1\r\n       Cython: 3.0.11\r\n       pandas: 2.2.2\r\n   matplotlib: 3.9.2\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: \/Users\/abaker\/miniforge3\/envs\/sklearn-dev\/lib\/libopenblas.0.dylib\r\n        version: 0.3.27\r\nthreading_layer: openmp\r\n   architecture: VORTEX\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libomp\r\n       filepath: \/Users\/abaker\/miniforge3\/envs\/sklearn-dev\/lib\/libomp.dylib\r\n        version: None\n```\n","labels":["Bug"],"created_at":"2024-11-14T16:40:29Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30275"},{"issue_number":27,"repository":"scikit-learn\/scikit-learn","title":"Changelog check on towncrier false positive case","description":"Observed on this PR: https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/30209\r\nThis run: https:\/\/github.com\/scikit-learn\/scikit-learn\/actions\/runs\/11681055082\/job\/32525320042?pr=30209\r\n\r\nThe PR needs to add PR number to existing changelog, and changes another affected changelog, therefore there are 3 changelog files affected in the PR. However, the changelog checker complains with:\r\n\r\n```\r\n Not all changelog file number(s) match this pull request number (30209):\r\ndoc\/whats_new\/upcoming_changes\/sklearn.calibration\/30171.api.rst\r\ndoc\/whats_new\/upcoming_changes\/sklearn.frozen\/29705.major-feature.rst\r\ndoc\/whats_new\/upcoming_changes\/sklearn.frozen\/30209.major-feature.rst\r\n```\r\n\r\nWhich I'd say is a false positive.\r\n\r\ncc @lesteve ","labels":["Bug","Build \/ CI"],"created_at":"2024-11-05T09:28:21Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30222"},{"issue_number":28,"repository":"scikit-learn\/scikit-learn","title":"OneVsRestClassifier cannot be used with TunedThresholdClassifierCV","description":"https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/d5082d32de2797f9594c9477f2810c743560a1f1\/sklearn\/model_selection\/_classification_threshold.py#L386\r\n\r\nWhen predict is called on `OneVsRestClassifier`, it calls `predict_proba` on the underlying classifier.\r\n\r\nIf the underlying is a `TunedThresholdClassifierCV`, it redirects to the underlying estimator instead.\r\n\r\nOn the line referenced, I think that `OneVsRestClassifier` should check if the estimator is `TunedThresholdClassifierCV`, and if so use the `best_threshold_` instead of 0.5","labels":["Bug","Needs Decision"],"created_at":"2024-10-09T07:31:21Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30036"},{"issue_number":29,"repository":"scikit-learn\/scikit-learn","title":"SGDOneClassSVM model does not converge with default stopping criteria(stops prematurely)","description":"### Describe the bug\r\n\r\nSGDOneClassSVM does not converge with default early stopping criteria, because the used loss is not actual loss, but only error, which can be easily 0.0 and then increase as the model converges to adequate solution. That is, the used for stopping and reported with verbose \"loss\" value doesn't accout for the full model formula\/regularization. Also, pay attention to bias term to gauge convergence.\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/c7839c48363d1531af9a00abfcb9d911ecfcb2b2\/sklearn\/linear_model\/_sgd_fast.pyx.tp#L482\r\nThe optimization almost always stops after 6 epochs, the initial epoch, plus the 5 for stopping tolerance (can't change the number of epochs for the stopping tolerance btw).\r\nThe problem does not manifest with toy data(small dimensiaonal), becasue 6 epochs is likely enough for convergence to satisfactory solution.\r\nIn the reproduction code, mind the console output and comments. Possible workaround at the end of reproduction code, is to use tol=None with manual epoch limit(max_iter), but that slows the optimization down by a lot, since forbids the use of learning_rate=\"adaptive\".\r\n\r\n### Steps\/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom matplotlib import pyplot as plt\r\nimport pandas as pd\r\n#from sklearn.linear_model import Ridge\r\nfrom sklearn.datasets import make_regression\r\nfrom timeit import timeit\r\nfrom sklearn.linear_model import SGDOneClassSVM\r\nfrom sklearn.svm import OneClassSVM\r\nfrom sklearn.pipeline import make_pipeline\r\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\r\n\r\nnp.random.seed(123)\r\n\r\n#no matter the feature count, optimization stops in 6 epochs\r\nprint(\"fitting different feature counts:\")\r\nfeatCnts = [10, 1000, 25000]\r\nfor featCnt in featCnts:\r\n    print(\"\\n10k samples, {} features\".format(featCnt))\r\n    x, y = make_regression(10000, featCnt, n_informative=featCnt \/\/ 10)\r\n    x = MinMaxScaler().fit_transform(x) + 1.0 #make positive\r\n    model = SGDOneClassSVM(nu=0.01, verbose=10)#, tol=1e-10)\r\n    model.fit(x) #see console output\r\n\r\n#model reports 0 \"loss\" even after just 1 sample\r\nprint(\"\\n\\npartial fit test:\")\r\nx, y = make_regression(10000, 2500\/\/4, n_informative=250)\r\nxPos = StandardScaler().fit_transform(x) + 100 #highly positive data\r\n#Note: linear one class svm has to run with e.g. positive data for correct function(not centered about 0), otherwise data is not separable from origin\r\nprint(\"xPos data mean\", xPos.mean())\r\nmodel = SGDOneClassSVM(nu=0.01, verbose=10)#, tol=1e-10)\r\nprint(\"fit 1 sample\")\r\nmodel.partial_fit(xPos[:1], y) #gives avg loss 0.0\r\nprint(\"fit 100 samples\")\r\nmodel.partial_fit(xPos[:100], y)\r\n\r\n#demonstrate that model output(decision function) is far from expected\r\n# by comparing with a slow reference model\r\nprint(\"\\n\\nreference comparison:\")\r\ntestNu = 0.5\r\nmodelSgd = make_pipeline( MinMaxScaler(), SGDOneClassSVM(nu=testNu, verbose=10) )\r\nmodelSgd.fit(x)\r\nmodelSgdDecFn = modelSgd.decision_function(x)\r\nmodelSgdClass = modelSgd.predict(x)\r\n\r\nrefMinMaxModel = make_pipeline( MinMaxScaler(), OneClassSVM(nu=testNu, verbose=10, tol=1e-10, kernel=\"linear\", shrinking=False) ).fit(x)\r\nrefMinMax = refMinMaxModel.decision_function(x)\r\nrefMinMaxClass = refMinMaxModel.predict(x)\r\n\r\n#slow sgd, manually tuned learning rate model\r\ninvScalingPower = np.emath.logn( len(x) * 9000, 100 ) #target lr reduction coefficient after N samples (len(x) * epoch count to get sample count)\r\n#invScalingPower =0.2\r\nprint(\"invScalingPower\", invScalingPower )\r\nsgdMinMaxModel = make_pipeline( MinMaxScaler(), SGDOneClassSVM(nu=testNu, verbose=10, max_iter=10000\/\/10, tol=None,\r\n    learning_rate=\"constant\", eta0=0.001, power_t=invScalingPower, average=True) ).fit(x)#, sgdoneclasssvm__coef_init=np.random.normal(-100.0, 1.0, x.shape[1])) # sgdoneclasssvm__offset_init=1000)\r\nsgdMinMax = sgdMinMaxModel.decision_function(x)\r\nsgdMinMaxClass = sgdMinMaxModel.predict(x)\r\n\r\n\r\ndf = pd.DataFrame({\"sklearn default modelSgd\": modelSgdDecFn,  \"reference ocsvm refMinMax\": refMinMax, \"manual stopping sgdMinMax\": sgdMinMax, })\r\nprint(\"decision function:\\n\", df)\r\nprint(\"correlation between decision_function:\\n\", df.corr())\r\n\r\ndf = pd.DataFrame({\"sklearn default modelSgd\": modelSgdClass,  \"reference ocsvm refMinMax\": refMinMaxClass, \"manual stopping sgdMinMax\": sgdMinMaxClass, })\r\nprint(\"correlation between predict:\\n\", df.corr())\r\n```\r\n\r\n### Expected Results\r\n\r\nModel convergence criteria works adequately (especially important for the most efficient learning_rate=\"adaptive\"), and model reaches convergence.\r\n\r\n### Actual Results\r\n\r\nOptimization stops prematurely, usually after 6 epochs:\r\n```\r\nfitting different feature counts:\r\n\r\n10k samples, 10 features\r\n-- Epoch 1\r\nNorm: 1.41, NNZs: 10, Bias: -5.067461, T: 10000, Avg. loss: 0.000088\r\nTotal training time: 0.00 seconds.\r\n-- Epoch 2\r\nNorm: 1.66, NNZs: 10, Bias: -6.148603, T: 20000, Avg. loss: 0.000187\r\nTotal training time: 0.00 seconds.\r\n-- Epoch 3\r\nNorm: 1.80, NNZs: 10, Bias: -6.766193, T: 30000, Avg. loss: 0.000267\r\nTotal training time: 0.00 seconds.\r\n-- Epoch 4\r\nNorm: 1.87, NNZs: 10, Bias: -7.203003, T: 40000, Avg. loss: 0.000191\r\nTotal training time: 0.00 seconds.\r\n-- Epoch 5\r\nNorm: 1.97, NNZs: 10, Bias: -7.528345, T: 50000, Avg. loss: 0.000193\r\nTotal training time: 0.00 seconds.\r\n-- Epoch 6\r\nNorm: 2.01, NNZs: 10, Bias: -7.798230, T: 60000, Avg. loss: 0.000285\r\nTotal training time: 0.00 seconds.\r\nConvergence after 6 epochs took 0.01 seconds\r\n\r\n10k samples, 1000 features\r\n-- Epoch 1\r\nNorm: 0.95, NNZs: 1000, Bias: -5.741972, T: 10000, Avg. loss: 0.000000\r\nTotal training time: 0.01 seconds.\r\n-- Epoch 2\r\nNorm: 0.47, NNZs: 1000, Bias: -7.123019, T: 20000, Avg. loss: 0.000000\r\nTotal training time: 0.02 seconds.\r\n-- Epoch 3\r\nNorm: 0.32, NNZs: 1000, Bias: -7.932197, T: 30000, Avg. loss: 0.000000\r\nTotal training time: 0.03 seconds.\r\n-- Epoch 4\r\nNorm: 0.24, NNZs: 1000, Bias: -8.506685, T: 40000, Avg. loss: 0.000000\r\nTotal training time: 0.05 seconds.\r\n-- Epoch 5\r\nNorm: 0.38, NNZs: 1000, Bias: -8.948081, T: 50000, Avg. loss: 0.000001\r\nTotal training time: 0.06 seconds.\r\n-- Epoch 6\r\nNorm: 0.32, NNZs: 1000, Bias: -9.312374, T: 60000, Avg. loss: 0.000000\r\nTotal training time: 0.07 seconds.\r\nConvergence after 6 epochs took 0.07 seconds\r\n\r\n10k samples, 25000 features\r\n-- Epoch 1\r\nNorm: 4.73, NNZs: 25000, Bias: -5.741972, T: 10000, Avg. loss: 0.000000\r\nTotal training time: 0.24 seconds.\r\n-- Epoch 2\r\nNorm: 2.37, NNZs: 25000, Bias: -7.123019, T: 20000, Avg. loss: 0.000000\r\nTotal training time: 0.48 seconds.\r\n-- Epoch 3\r\nNorm: 1.58, NNZs: 25000, Bias: -7.932197, T: 30000, Avg. loss: 0.000000\r\nTotal training time: 0.71 seconds.\r\n-- Epoch 4\r\nNorm: 1.19, NNZs: 25000, Bias: -8.506685, T: 40000, Avg. loss: 0.000000\r\nTotal training time: 0.95 seconds.\r\n-- Epoch 5\r\nNorm: 0.95, NNZs: 25000, Bias: -8.952446, T: 50000, Avg. loss: 0.000000\r\nTotal training time: 1.19 seconds.\r\n-- Epoch 6\r\nNorm: 0.79, NNZs: 25000, Bias: -9.316738, T: 60000, Avg. loss: 0.000000\r\nTotal training time: 1.43 seconds.\r\nConvergence after 6 epochs took 1.43 seconds\r\n\r\n\r\npartial fit test:\r\nxPos data mean 100.00000000000011\r\nfit 1 sample\r\n-- Epoch 1\r\nNorm: 9397.76, NNZs: 625, Bias: 4.722997, T: 1, Avg. loss: 0.000000\r\nTotal training time: 0.00 seconds.\r\nfit 100 samples\r\n-- Epoch 1\r\nNorm: 3262.77, NNZs: 625, Bias: 2.619430, T: 100, Avg. loss: 0.000000\r\nTotal training time: 0.00 seconds.\r\n\r\n\r\nreference comparison:\r\n-- Epoch 1\r\nNorm: 1.21, NNZs: 625, Bias: -13.904809, T: 10000, Avg. loss: 0.001472\r\nTotal training time: 0.01 seconds.\r\n-- Epoch 2\r\nNorm: 1.32, NNZs: 625, Bias: -15.210391, T: 20000, Avg. loss: 0.001823\r\nTotal training time: 0.02 seconds.\r\n-- Epoch 3\r\nNorm: 1.38, NNZs: 625, Bias: -15.971154, T: 30000, Avg. loss: 0.002099\r\nTotal training time: 0.03 seconds.\r\n-- Epoch 4\r\nNorm: 1.43, NNZs: 625, Bias: -16.509796, T: 40000, Avg. loss: 0.002299\r\nTotal training time: 0.04 seconds.\r\n-- Epoch 5\r\nNorm: 1.46, NNZs: 625, Bias: -16.926977, T: 50000, Avg. loss: 0.002480\r\nTotal training time: 0.04 seconds.\r\n-- Epoch 6\r\nNorm: 1.49, NNZs: 625, Bias: -17.267392, T: 60000, Avg. loss: 0.002436\r\nTotal training time: 0.05 seconds.\r\nConvergence after 6 epochs took 0.05 seconds\r\n..*\r\noptimization finished, #iter = 2497\r\nobj = 1924865835.187691, rho = 776455.657200\r\nnSV = 5001, nBSV = 4999\r\n[LibSVM]invScalingPower 0.25143814733164016\r\n-- Epoch 1\r\nNorm: 0.40, NNZs: 625, Bias: -3.925000, T: 10000, Avg. loss: 0.000117\r\nTotal training time: 0.01 seconds.\r\n-- Epoch 2\r\nNorm: 0.79, NNZs: 625, Bias: -8.770000, T: 20000, Avg. loss: 0.000492\r\nTotal training time: 0.02 seconds.\r\n-- Epoch 3\r\n\r\n...\r\n\r\n-- Epoch 998\r\nNorm: 24.84, NNZs: 625, Bias: -309.593000, T: 9980000, Avg. loss: 1.322528\r\nTotal training time: 14.50 seconds.\r\n-- Epoch 999\r\nNorm: 24.80, NNZs: 625, Bias: -309.597000, T: 9990000, Avg. loss: 1.315051\r\nTotal training time: 14.52 seconds.\r\n-- Epoch 1000\r\nNorm: 24.80, NNZs: 625, Bias: -309.597000, T: 10000000, Avg. loss: 1.310013\r\nTotal training time: 14.53 seconds.\r\ndecision function:\r\n       sklearn default modelSgd  reference ocsvm refMinMax  manual stopping sgdMinMax\r\n0                     0.152997               -8169.563012                  -2.922611\r\n1                     0.208675               -5888.609526                  -2.068050\r\n2                     0.460575                4671.620928                   1.890327\r\n3                     0.324515                -993.308774                  -0.232038\r\n4                     0.376079                1109.737641                   0.558291\r\n...                        ...                        ...                        ...\r\n9995                  0.203837               -5984.511457                  -2.103481\r\n9996                  0.263156               -3678.767938                  -1.238047\r\n9997                  0.408527                2547.241215                   1.097869\r\n9998                  0.408359                2477.709831                   1.068493\r\n9999                  0.534379                7887.142835                   3.098472\r\n\r\n[10000 rows x 3 columns]\r\ncorrelation between decision_function:\r\n                            sklearn default modelSgd  reference ocsvm refMinMax  manual stopping sgdMinMax\r\nsklearn default modelSgd                   1.000000                   0.999918                   0.999919\r\nreference ocsvm refMinMax                  0.999918                   1.000000                   1.000000\r\nmanual stopping sgdMinMax                  0.999919                   1.000000                   1.000000\r\ncorrelation between predict:\r\n                            sklearn default modelSgd  reference ocsvm refMinMax  manual stopping sgdMinMax\r\nsklearn default modelSgd                   1.000000                   0.195700                   0.203691\r\nreference ocsvm refMinMax                  0.195700                   1.000000                   0.960769\r\nmanual stopping sgdMinMax                  0.203691                   0.960769                   1.000000\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: E:\\Program Files\\Python\\Python37\\python.exe\r\n   machine: Windows-10-10.0.17763-SP0\r\n\r\nPython dependencies:\r\n          pip: 24.0\r\n   setuptools: 68.0.0\r\n      sklearn: 1.0.2\r\n        numpy: 1.21.6\r\n        scipy: 1.7.3\r\n       Cython: 3.0.11\r\n       pandas: 1.3.5\r\n   matplotlib: 3.5.3\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n```\r\n","labels":["Bug"],"created_at":"2024-10-07T18:11:39Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/30027"},{"issue_number":30,"repository":"scikit-learn\/scikit-learn","title":"ClassifierChain does not accept NaN values even when base estimator supports them","description":"### Describe the bug\r\n\r\nI am working on a multilabel classification problem using ClassifierChain with RandomForestClassifier as the base estimator.\r\nI have encountered an issue where ClassifierChain raises a ValueError when the input data X contains np.nan values, even though RandomForestClassifier can handle np.nan values natively.\r\nWhen I use RandomForestClassifier alone, it processes np.nan values without any problems, thanks to its internal tree splitting mechanism that supports missing values. Similarly, when I use MultiOutputClassifier with RandomForestClassifier, I do not encounter any errors with np.nan values.\r\nHowever, when I use ClassifierChain, I receive an error during hyperparameter tuning.\r\nSince the base estimator can handle np.nan values, I expected ClassifierChain to pass the data through without additional checks. It seems inconsistent that ClassifierChain does not support missing values when the base estimator does. \r\nI'm wondering if this is the intended behavior of ClassifierChain. If not, could it be updated to support missing values when the base estimator does? Alternatively, is there a recommended workaround that doesn't involve imputing or dropping missing values?\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```py\r\nimport numpy as np\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.multioutput import ClassifierChain, MultiOutputClassifier\r\n\r\n\r\nX = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1) # Input data with NaN values\r\ny = np.array([[0, 1], [0, 0], [1, 0], [1, 1]])\r\n\r\nbase_clf = RandomForestClassifier() # Base classifier\r\n\r\nclf_br = MultiOutputClassifier(base_clf) # MultiOutputClassifier (Binary Relevance) - works fine with NaN\r\nclf_br.fit(X, y)  # No error\r\n\r\nclf_chain = ClassifierChain(base_clf) # ClassifierChain - raises error\r\nclf_chain.fit(X, y)  # Raises ValueError about NaNs\r\n```\r\n\r\n### Expected Results\r\n\r\nNo error is thrown when using ClassifierChain with RandomForestClassifier as the base estimator, since RandomForestClassifier handles np.nan values natively without any issues. The model should fit and train successfully, just as it does with MultiOutputClassifier.\r\n\r\n### Actual Results\r\n\r\nInstead, when fitting the ClassifierChain, I receive the following error message:\r\n```py-tb\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[2], line 5\r\n      2 clf_chain = ClassifierChain(base_clf)\r\n      4 # Fitting the model (this will likely raise an error due to NaN in X)\r\n----> 5 clf_chain.fit(X, y)\r\n\r\nFile <env_path>\/python3.12\/site-packages\/sklearn\/base.py, line=1472, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\r\n   1466     estimator._validate_params()\r\n   1468 with config_context(\r\n   1469     skip_parameter_validation=(\r\n   1470         prefer_skip_nested_validation or global_skip_validation\r\n   1471     )\r\n   1472 ):\r\n-> 1473     return fit_method(estimator, *args, **kwargs)\r\n\r\nFile <env_path>\/python3.12\/site-packages\/sklearn\/multioutput.py, line=1029, in ClassifierChain.fit(self, X, Y, **fit_params)\r\n   1005 \"\"\"Fit the model to data matrix X and targets Y.\r\n   1006 \r\n   1007 Parameters\r\n   (...)\r\n   1026     Class instance.\r\n   1027 \"\"\"\r\n   1028 _raise_for_params(fit_params, self, \"fit\")\r\n-> 1030 super().fit(X, Y, **fit_params)\r\n   1031 self.classes_ = [estimator.classes_ for estimator in self.estimators_]\r\n   1032 return self\r\n\r\nFile <env_path>\/python3.12\/site-packages\/sklearn\/multioutput.py, line=720, in _BaseChain.fit(self, X, Y, **fit_params)\r\n    699 @abstractmethod\r\n    700 def fit(self, X, Y, **fit_params):\r\n    701     \"\"\"Fit the model to data matrix X and targets Y.\r\n    702 \r\n    703     Parameters\r\n   (...)\r\n    719         Returns a fitted instance.\r\n    720     \"\"\"\r\n--> 721     X, Y = self._validate_data(X, Y, multi_output=True, accept_sparse=True)\r\n    723     random_state = check_random_state(self.random_state)\r\n    724     self.order_ = self.order\r\n\r\nFile <env_path>\/python3.12\/site-packages\/sklearn\/base.py, line=649, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\r\n    648         y = check_array(y, input_name=\"y\", **check_y_params)\r\n    649     else:\r\n--> 650         X, y = check_X_y(X, y, **check_params)\r\n    651     out = X, y\r\n    653 if not no_val_X and check_params.get(\"ensure_2d\", True):\r\n\r\nFile <env_path>\/python3.12\/site-packages\/sklearn\/utils\/validation.py, line=1300, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\r\n   1296         estimator_name = _check_estimator_name(estimator)\r\n   1297     raise ValueError(\r\n   1298         f\"{estimator_name} requires y to be passed, but the target y is None\"\r\n   1299     )\r\n-> 1301 X = check_array(\r\n   1302     X,\r\n   1303     accept_sparse=accept_sparse,\r\n   1304     accept_large_sparse=accept_large_sparse,\r\n   1305     dtype=dtype,\r\n   1306     order=order,\r\n   1307     copy=copy,\r\n   1308     force_writeable=force_writeable,\r\n   1309     force_all_finite=force_all_finite,\r\n   1310     ensure_2d=ensure_2d,\r\n   1311     allow_nd=allow_nd,\r\n   1312     ensure_min_samples=ensure_min_samples,\r\n   1313     ensure_min_features=ensure_min_features,\r\n   1314     estimator=estimator,\r\n   1315     input_name=\"X\",\r\n   1316 )\r\n   1318 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\r\n   1320 check_consistent_length(X, y)\r\n\r\nFile <env_path>\/python3.12\/site-packages\/sklearn\/utils\/validation.py, line=1063, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\r\n   1058     raise ValueError(\r\n   1059         \"Found array with dim %d. %s expected <= 2.\"\r\n   1060         % (array.ndim, estimator_name)\r\n   1061     )\r\n   1063 if force_all_finite:\r\n-> 1064     _assert_all_finite(\r\n   1065         array,\r\n   1066         input_name=input_name,\r\n   1067         estimator_name=estimator_name,\r\n   1068         allow_nan=force_all_finite == \"allow-nan\",\r\n   1069     )\r\n   1071 if copy:\r\n   1072     if _is_numpy_namespace(xp):\r\n   1073         # only make a copy if `array` and `array_orig` may share memory`\r\n\r\nFile <env_path>\/python3.12\/site-packages\/sklearn\/utils\/validation.py, line=122, in _assert_all_finite(X, allow_nan, msg_dtype, estimator_name, input_name)\r\n    120 if first_pass_isfinite:\r\n    121     return\r\n--> 123 _assert_all_finite_element_wise(\r\n    124     X,\r\n    125     xp=xp,\r\n    126     allow_nan=allow_nan,\r\n    127     msg_dtype=msg_dtype,\r\n    128     estimator_name=estimator_name,\r\n    129     input_name=input_name,\r\n    130 )\r\n\r\nFile <env_path>\/python3.12\/site-packages\/sklearn\/utils\/validation.py, line=171, in _assert_all_finite_element_wise(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\r\n    155 if estimator_name and input_name == \"X\" and has_nan_error:\r\n    156     # Improve the error message on how to handle missing values in\r\n    157     # scikit-learn.\r\n    158     msg_err += (\r\n    159         f\"\\n{estimator_name} does not accept missing values\"\r\n    160         \" encoded as NaN natively. For supervised learning, you might want\"\r\n   (...)\r\n    170         \"#estimators-that-handle-nan-values\"\r\n    171     )\r\n--> 172 raise ValueError(msg_err)\r\n\r\nValueError: Input X contains NaN.\r\nClassifierChain does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https:\/\/scikit-learn.org\/stable\/modules\/impute.html You can find a list of all estimators that handle NaN values at the following page: https:\/\/scikit-learn.org\/stable\/modules\/impute.html#estimators-that-handle-nan-values\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem and Versions:\r\n\r\nPython version: 3.12.5 (packaged by conda-forge)\r\nscikit-learn version: 1.5.1\r\n```\r\n","labels":["Bug"],"created_at":"2024-09-16T07:54:41Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29856"},{"issue_number":31,"repository":"scikit-learn\/scikit-learn","title":"Discrepancy between .fit_transform() and .transform() methods in the LLE module","description":"### Describe the bug\r\n\r\nA user would expect the same result from  \r\n- `.fit(X)` and then `.transform(X)`\r\n- `.fit_transformX()`\r\n\r\nBut this is not the case in the current code for `LocallyLinearEmbedding`. \r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.manifold import LocallyLinearEmbedding\r\nfrom sklearn.datasets import make_s_curve\r\nimport numpy as np\r\n\r\nX,_ = make_s_curve(100)\r\nmethods = [\"standard\", \"hessian\", \"ltsa\", \"modified\"]\r\nfor method in methods:\r\n    lle = LocallyLinearEmbedding(method=method, n_neighbors=12)\r\n    fit_transform = lle.fit_transform(X)  \r\n    fit_then_transform = lle.transform(X)\r\n    equal = np.any(fit_transform == fit_then_transform)\r\n    close_count = np.isclose(fit_transform ,fit_then_transform).sum()\r\n    print(f\"For {method} it is {equal} that f_t and f_then_t are equal.\")\r\n    print(f\"Only {close_count} are close.\\n\" )\r\n```\r\n\r\n### Expected Results\r\n\r\n```text\r\nFor {method} it is True that `fit_transform(X) == transform(x)`.\r\n```\r\n\r\n### Actual Results\r\n\r\n```text\r\nFor standard, it is False that `fit_transform(X) == transform(x)`.\r\nOnly 2 are close.\r\n\r\nFor hessian, it is False that `fit_transform(X) == transform(x)`.\r\nOnly 1 are close.\r\n\r\nFor ltsa, it is False that `fit_transform(X) == transform(x)`.\r\nOnly 1 are close.\r\n\r\nFor modified, it is False that `fit_transform(X) == transform(x)`.\r\nOnly 0 are close.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ]\r\nexecutable: \/Users\/wonderman\/miniforge3\/envs\/sklearn_dev_env\/bin\/python\r\n   machine: macOS-14.6.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.6.dev0\r\n          pip: 24.2\r\n   setuptools: 72.2.0\r\n        numpy: 2.1.0\r\n        scipy: 1.14.1\r\n       Cython: 3.0.11\r\n       pandas: None\r\n   matplotlib: None\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: \/Users\/wonderman\/miniforge3\/envs\/sklearn_dev_env\/lib\/libopenblas.0.dylib\r\n        version: 0.3.27\r\nthreading_layer: openmp\r\n   architecture: VORTEX\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libomp\r\n       filepath: \/Users\/wonderman\/miniforge3\/envs\/sklearn_dev_env\/lib\/libomp.dylib\r\n        version: None\r\n```\r\n","labels":["Bug"],"created_at":"2024-09-05T21:35:17Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29792"},{"issue_number":32,"repository":"scikit-learn\/scikit-learn","title":"spin docs --no-plot runs the examples","description":"Seen at the EuroScipy sprint\r\n\r\nCommands run by spin:\r\n```\r\n$ export SPHINXOPTS=-W -D plot_gallery=0 -j auto\r\n$ cd doc\r\n$ make html\r\n```\r\n\r\nLooks like our Makefile does not use SPHINXOPTS the same way as expected:\r\nProbably we have a slightly different way of building the doc\r\n\r\n```\r\n\u276f make html-noplot -n\r\nsphinx-build -D plot_gallery=0 -b html -d _build\/doctrees  -T  . -jauto \\\r\n    _build\/html\/stable\r\necho\r\necho \"Build finished. The HTML pages are in _build\/html\/stable.\"\r\n```","labels":["Bug","Sprint"],"created_at":"2024-08-30T08:31:28Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29742"},{"issue_number":33,"repository":"scikit-learn\/scikit-learn","title":"Default argument pos_label=1 is not ignored in f1_score metric for multiclass classification","description":"### Describe the bug\r\n\r\nI get a `ValueError` for `pos_label=1` default argument value to `f1_score` metric with argument `average='micro'` for the iris flower classification problem:\r\n\r\n```pytb\r\nValueError: pos_label=1 is not a valid label: It should be one of ['setosa' 'versicolor' 'virginica']\r\n```\r\n\r\nAccording to the documentation, the `pos_label` argument should be ignored for the multiclass problem:\r\n\r\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html#f1-score\r\n\r\n_The class to report if `average='binary'` and the data is binary, otherwise this parameter is ignored._\r\n\r\nSetting `pos_label` explicitly to None solves the problem and produces the expected output, see below.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\n# Import necessary libraries\r\nimport numpy as np\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.model_selection import cross_val_score\r\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\nfrom sklearn.metrics import make_scorer, f1_score\r\n\r\n# Load the Iris dataset\r\ndata = load_iris()\r\nX = data.data  # Features\r\ny = data.target  # Labels\r\n\r\n# Convert labels to string type\r\ny = np.array([data.target_names[label] for label in data.target])\r\n\r\n# Initialize the Linear Discriminant Analysis classifier\r\nclassifier = LinearDiscriminantAnalysis()\r\n\r\n# Define a custom scorer using F1 score with average='micro'\r\nf1_scorer = make_scorer(f1_score, average='micro', pos_label=1)\r\n\r\n# Perform cross-validation with cross_val_score\r\ntry:\r\n    scores = cross_val_score(classifier, X, y, cv=5, scoring=f1_scorer)\r\n    print(f\"Cross-validated F1 Scores (micro average): {scores}\")\r\n    print(f\"Mean F1 Score: {np.mean(scores)}\")\r\nexcept ValueError as e:\r\n    print(f\"Error: {e}\")\r\n```\r\n\r\n### Expected Results\r\n\r\n```\r\nCross-validated F1 Scores (micro average): [1.         1.         0.96666667 0.93333333 1.        ]\r\nMean F1 Score: 0.9800000000000001\r\n```\r\n\r\n### Actual Results\r\n\r\n```pytb\r\nCross-validated F1 Scores (micro average): [nan nan nan nan nan]\r\nMean F1 Score: nan\r\n[C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1000](file:\/\/\/C:\/Users\/rgt0227\/AppData\/Local\/anaconda3\/Lib\/site-packages\/sklearn\/model_selection\/_validation.py#line=999): UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \r\nTraceback (most recent call last):\r\n  File \"[C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 139](file:\/\/\/C:\/Users\/rgt0227\/AppData\/Local\/anaconda3\/Lib\/site-packages\/sklearn\/metrics\/_scorer.py#line=138), in __call__\r\n    score = scorer._score(\r\n            ^^^^^^^^^^^^^^\r\n  File \"[C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 371](file:\/\/\/C:\/Users\/rgt0227\/AppData\/Local\/anaconda3\/Lib\/site-packages\/sklearn\/metrics\/_scorer.py#line=370), in _score\r\n    y_pred = method_caller(\r\n             ^^^^^^^^^^^^^^\r\n  File \"[C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 89](file:\/\/\/C:\/Users\/rgt0227\/AppData\/Local\/anaconda3\/Lib\/site-packages\/sklearn\/metrics\/_scorer.py#line=88), in _cached_call\r\n    result, _ = _get_response_values(\r\n                ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"[C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_response.py\", line 204](file:\/\/\/C:\/Users\/rgt0227\/AppData\/Local\/anaconda3\/Lib\/site-packages\/sklearn\/utils\/_response.py#line=203), in _get_response_values\r\n    raise ValueError(\r\nValueError: pos_label=1 is not a valid label: It should be one of ['setosa' 'versicolor' 'virginica']\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\python.exe\r\n   machine: Windows-10-10.0.19045-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.0\r\n          pip: 23.2.1\r\n   setuptools: 68.0.0\r\n        numpy: 1.26.2\r\n        scipy: 1.11.4\r\n       Cython: None\r\n       pandas: 2.1.1\r\n   matplotlib: 3.8.0\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: mkl\r\n    num_threads: 8\r\n         prefix: mkl_rt\r\n       filepath: C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Library\\bin\\mkl_rt.2.dll\r\n        version: 2023.1-Product\r\nthreading_layer: intel\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\rgt0227\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n```\r\n","labels":["Bug"],"created_at":"2024-08-29T07:45:11Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29734"},{"issue_number":34,"repository":"scikit-learn\/scikit-learn","title":"LocallyLinearEmbedding : n_neighbors <= n_samples","description":"### Describe the bug\n\nMinor bug in `LocallyLinearEmbedding`'s parameter validation:\r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/70fdc843a4b8182d97a3508c1a426acc5e87e980\/sklearn\/manifold\/_locally_linear.py#L226-L230\r\n\r\nThe `if` condition contradicts the error message in the case that `n_neighbors == N`. So you get a message like\r\n\r\n```python-traceback\r\nValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3\"\r\n```\r\n\r\nwhich doesn't make sense.\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nimport sklearn.manifold\r\n\r\nX = np.random.randn(3, 5)\r\n\r\nembedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])\r\n\r\nembedder.fit_transform(X)\r\n```\n\n### Expected Results\n\nn\/a\n\n### Actual Results\n\n```python-traceback\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[1119], line 8\r\n      4 X = np.random.randn(3, 5)\r\n      6 embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])\r\n----> 8 embedder.fit_transform(X)\r\n\r\nFile ~\/Library\/Python\/3.12\/lib\/python\/site-packages\/sklearn\/utils\/_set_output.py:313, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)\r\n    311 @wraps(f)\r\n    312 def wrapped(self, X, *args, **kwargs):\r\n--> 313     data_to_wrap = f(self, X, *args, **kwargs)\r\n    314     if isinstance(data_to_wrap, tuple):\r\n    315         # only wrap the first output for cross decomposition\r\n    316         return_tuple = (\r\n    317             _wrap_data_with_container(method, data_to_wrap[0], X, self),\r\n    318             *data_to_wrap[1:],\r\n    319         )\r\n\r\nFile ~\/Library\/Python\/3.12\/lib\/python\/site-packages\/sklearn\/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\r\n   1466     estimator._validate_params()\r\n   1468 with config_context(\r\n   1469     skip_parameter_validation=(\r\n   1470         prefer_skip_nested_validation or global_skip_validation\r\n   1471     )\r\n   1472 ):\r\n-> 1473     return fit_method(estimator, *args, **kwargs)\r\n\r\nFile ~\/Library\/Python\/3.12\/lib\/python\/site-packages\/sklearn\/manifold\/_locally_linear.py:848, in LocallyLinearEmbedding.fit_transform(self, X, y)\r\n    831 @_fit_context(prefer_skip_nested_validation=True)\r\n    832 def fit_transform(self, X, y=None):\r\n    833     \\\"\\\"\\\"Compute the embedding vectors for data X and transform X.\r\n    834 \r\n    835     Parameters\r\n   (...)\r\n    846         Returns the instance itself.\r\n    847     \\\"\\\"\\\"\r\n--> 848     self._fit_transform(X)\r\n    849     return self.embedding_\r\n\r\nFile ~\/Library\/Python\/3.12\/lib\/python\/site-packages\/sklearn\/manifold\/_locally_linear.py:795, in LocallyLinearEmbedding._fit_transform(self, X)\r\n    793 X = self._validate_data(X, dtype=float)\r\n    794 self.nbrs_.fit(X)\r\n--> 795 self.embedding_, self.reconstruction_error_ = _locally_linear_embedding(\r\n    796     X=self.nbrs_,\r\n    797     n_neighbors=self.n_neighbors,\r\n    798     n_components=self.n_components,\r\n    799     eigen_solver=self.eigen_solver,\r\n    800     tol=self.tol,\r\n    801     max_iter=self.max_iter,\r\n    802     method=self.method,\r\n    803     hessian_tol=self.hessian_tol,\r\n    804     modified_tol=self.modified_tol,\r\n    805     random_state=random_state,\r\n    806     reg=self.reg,\r\n    807     n_jobs=self.n_jobs,\r\n    808 )\r\n    809 self._n_features_out = self.embedding_.shape[1]\r\n\r\nFile ~\/Library\/Python\/3.12\/lib\/python\/site-packages\/sklearn\/manifold\/_locally_linear.py:227, in _locally_linear_embedding(X, n_neighbors, n_components, reg, eigen_solver, tol, max_iter, method, hessian_tol, modified_tol, random_state, n_jobs)\r\n    223     raise ValueError(\r\n    224         \\\"output dimension must be less than or equal to input dimension\\\"\r\n    225     )\r\n    226 if n_neighbors >= N:\r\n--> 227     raise ValueError(\r\n    228         \\\"Expected n_neighbors <= n_samples,  but n_samples = %d, n_neighbors = %d\\\"\r\n    229         % (N, n_neighbors)\r\n    230     )\r\n    232 M_sparse = eigen_solver != \\\"dense\\\"\r\n    234 if method == \\\"standard\\\":\r\n\r\nValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3\"\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:47) [Clang 13.0.0 (clang-1300.0.29.30)]\r\nexecutable: \/usr\/local\/bin\/python3\r\n   machine: macOS-14.5-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.0\r\n          pip: 24.0\r\n   setuptools: 70.0.0\r\n        numpy: 1.26.4\r\n        scipy: 1.13.0\r\n       Cython: 3.0.10\r\n       pandas: 2.2.2\r\n   matplotlib: 3.8.4\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 11\r\n         prefix: libopenblas\r\n       filepath: \/Users\/gabriel.kissin\/Library\/Python\/3.12\/lib\/python\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: armv8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 11\r\n         prefix: libopenblas\r\n       filepath: \/Users\/gabriel.kissin\/Library\/Python\/3.12\/lib\/python\/site-packages\/scipy\/.dylibs\/libopenblas.0.dylib\r\n        version: 0.3.26.dev\r\nthreading_layer: pthreads\r\n   architecture: neoversen1\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 11\r\n         prefix: libomp\r\n       filepath: \/Users\/gabriel.kissin\/Library\/Python\/3.12\/lib\/python\/site-packages\/sklearn\/.dylibs\/libomp.dylib\r\n        version: None\r\n```\n```\n","labels":["Bug"],"created_at":"2024-08-25T21:22:56Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29715"},{"issue_number":35,"repository":"scikit-learn\/scikit-learn","title":"GaussianProcessRegressor: wrong std and cov results when n_features>1 and no y normalization","description":"### Describe the bug\n\nWhen `n_features > 1` and `normalization_y` is `False`, the `GaussianProcessRegressor.predict` seems to return bad std and cov results, as it doesn't consider the scale of the different features (while it seems to be ok when `n_features > 1` and `normalization_y` is `True`).\r\n\r\nBy taking a look at the code, we can see that `GaussianProcessRegressor.predict` uses the `_y_train_std` attribute to compute the variance and covariance but this attribute is set to `ones(n_features)` when `normalize_y` is set to `False` (default value), giving equal scale to all features.\r\n\r\nTo fix this bug, one should always compute `_y_train_std` from the training data and use the boolean attribute `normalize_y` to undo the normalization of `y_mean` if necessary.\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport pytest\r\nfrom numpy import array\r\nfrom numpy import hstack\r\nfrom sklearn.gaussian_process import GaussianProcessRegressor\r\n\r\nx = array([[0.], [0.5], [1.]])\r\ny = hstack((x**2, 10*x**2))\r\n# Note that the second output is equal to 10 times the first one.\r\n\r\n# With output normalization\r\ngpr = GaussianProcessRegressor(normalize_y=True)\r\ngpr.fit(x, y)\r\nstd = gpr.predict(array([[0.25]]), return_std=True)[1][0]\r\nassert std[0] != std[1]\r\nassert std[0] == pytest.approx(std[1]\/10, rel=1e-9)\r\n# As expected, the variance of the second output is 10 times larger than the first output.\r\n\r\n# Without output normalization\r\ngpr = GaussianProcessRegressor(normalize_y=False)\r\ngpr.fit(x, y)\r\nstd = gpr.predict(array([[0.25]]), return_std=True)[1][0]\r\nassert std[0] == std[1]\r\n# The variance of the second output is equal to the variance of the first output.\r\n```\n\n### Expected Results\n\nWithout output normalization, the variance of the second output should be 10 times larger than the first output.\n\n### Actual Results\n\nWithout output normalization, the variance of the second output is equal to the variance of the first output.\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.9.12 (tags\/v3.9.12:b28265d, Mar 23 2022, 23:52:46) [MSC v.1929 64 bit (AMD64)]\r\nexecutable: C:\\Users\\matthias.delozzo\\workspace\\GEMSEO\\gemseo\\.tox\\py39\\.venv\\Scripts\\python.exe\r\n   machine: Windows-10-10.0.19045-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.1\r\n          pip: None\r\n   setuptools: None\r\n        numpy: 1.26.4\r\n        scipy: 1.13.1\r\n       Cython: None\r\n       pandas: 2.2.2\r\n   matplotlib: 3.9.2\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\matthias.delozzo\\workspace\\GEMSEO\\gemseo\\.tox\\py39\\.venv\\Lib\\site-packages\\numpy.libs\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\matthias.delozzo\\workspace\\GEMSEO\\gemseo\\.tox\\py39\\.venv\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\matthias.delozzo\\workspace\\GEMSEO\\gemseo\\.tox\\py39\\.venv\\Lib\\site-packages\\scipy.libs\\libopenblas_v0.3.27--3aa239bc726cfb0bd8e5330d8d4c15c6.dll\r\n        version: 0.3.27\r\nthreading_layer: pthreads\r\n   architecture: Haswell\n```\n","labels":["Bug"],"created_at":"2024-08-21T10:39:55Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29697"},{"issue_number":36,"repository":"scikit-learn\/scikit-learn","title":"test_svm fails on i386 with scipy 1.13","description":"### Describe the bug\r\n\r\nscipy 1.13 is triggering test failure in test_svc_ovr_tie_breaking[NuSVC] on i386 architecture.\r\n\r\nThe error can be seeing in debian CI tests, https:\/\/ci.debian.net\/packages\/s\/scikit-learn\/unstable\/i386\/\r\nFull test log at https:\/\/ci.debian.net\/packages\/s\/scikit-learn\/unstable\/i386\/50043538\/\r\nor https:\/\/ci.debian.net\/packages\/s\/scikit-learn\/testing\/i386\/50043537\/\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\nOn an i386 system with scipy 1.13 installed\r\n```\r\n$ pytest-3 \/usr\/lib\/python3\/dist-packages\/sklearn\/svm\/tests\/test_svm.py -k test_svc_ovr_tie_breaking\r\n```\r\n\r\n### Expected Results\r\n\r\ntest should pass \r\n\r\n### Actual Results\r\n\r\n```\r\n1333s _______________________ test_svc_ovr_tie_breaking[NuSVC] _______________________\r\n1333s \r\n1333s SVCClass = <class 'sklearn.svm._classes.NuSVC'>\r\n1333s \r\n1333s     @pytest.mark.parametrize(\"SVCClass\", [svm.SVC, svm.NuSVC])\r\n1333s     def test_svc_ovr_tie_breaking(SVCClass):\r\n1333s         \"\"\"Test if predict breaks ties in OVR mode.\r\n1333s         Related issue: https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8277\r\n1333s         \"\"\"\r\n1333s         X, y = make_blobs(random_state=0, n_samples=20, n_features=2)\r\n1333s     \r\n1333s         xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\r\n1333s         ys = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\r\n1333s         xx, yy = np.meshgrid(xs, ys)\r\n1333s     \r\n1333s         common_params = dict(\r\n1333s             kernel=\"rbf\", gamma=1e6, random_state=42, decision_function_shape=\"ovr\"\r\n1333s         )\r\n1333s         svm = SVCClass(\r\n1333s             break_ties=False,\r\n1333s             **common_params,\r\n1333s         ).fit(X, y)\r\n1333s         pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])\r\n1333s         dv = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\r\n1333s >       assert not np.all(pred == np.argmax(dv, axis=1))\r\n1333s E       assert not True\r\n1333s E        +  where True = <function all at 0xf689d5e0>(array([1, 1, 1, ..., 1, 1, 1]) == array([1, 1, ..., dtype=int32)\r\n1333s E        +    where <function all at 0xf689d5e0> = np.all\r\n1333s E           \r\n1333s E           Full diff:\r\n1333s E           - array([1, 1, 1, ..., 1, 1, 1], dtype=int32)\r\n1333s E           ?                              -------------\r\n1333s E           + array([1, 1, 1, ..., 1, 1, 1]))\r\n1333s \r\n1333s \/usr\/lib\/python3\/dist-packages\/sklearn\/svm\/tests\/test_svm.py:1225: AssertionError\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\n$ python3 -c \"import sklearn; sklearn.show_versions()\"\r\n# (on amd64, edited manually for i386)\r\n\r\nSystem:\r\n    python: 3.12.4 (main, Jul 15 2024, 12:17:32) [GCC 13.3.0]\r\nexecutable: \/usr\/bin\/python3\r\n   machine: Linux-6.9.11-i386-i686-with-glibc2.39\r\n\r\nPython dependencies:\r\n      sklearn: 1.4.2\r\n          pip: 24.1.1\r\n   setuptools: 70.3.0\r\n        numpy: 1.26.4\r\n        scipy: 1.13.1\r\n       Cython: 3.0.10\r\n       pandas: 2.2.2+dfsg\r\n   matplotlib: 3.8.3\r\n       joblib: 1.3.2\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/usr\/lib\/i386-linux-gnu\/openblas-pthread\/libopenblasp-r0.3.27.so\r\n        version: 0.3.27\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 8\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/usr\/lib\/i386-linux-gnu\/libgomp.so.1.0.0\r\n        version: None\r\n```\r\n","labels":["Bug"],"created_at":"2024-08-06T21:45:19Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29633"},{"issue_number":37,"repository":"scikit-learn\/scikit-learn","title":"mirrors-prettier pre-commit has been archived so maybe should be replaced","description":"### Describe the bug\n\nNoticed your [mirrors-prettier pre-commit](https:\/\/github.com\/pre-commit\/mirrors-prettier) has been archived. I was going to suggest you remove and\/or look for alternative linters for the scss \/ js files.\n\n### Steps\/Code to Reproduce\n\nNoticed this in the .pre-commit-config.yaml\r\n\r\n```yaml\r\n-   repo: https:\/\/github.com\/pre-commit\/mirrors-prettier\r\n    rev: v2.7.1\r\n    hooks:\r\n    -   id: prettier\r\n        files: ^doc\/scss\/|^doc\/js\/scripts\/\r\n        exclude: ^doc\/js\/scripts\/vendor\/\r\n        types_or: [\"scss\", \"javascript\"]\r\n```\n\n### Expected Results\n\nN\/A\n\n### Actual Results\n\nN\/A\n\n### Versions\n\n```shell\n1.5.1\n```\n","labels":["Bug"],"created_at":"2024-08-04T07:29:35Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29621"},{"issue_number":38,"repository":"scikit-learn\/scikit-learn","title":"PowerTransformer's standardize sensitive to small differences in data, with MinMaxScaler unable to scale","description":"### Describe the bug\r\n\r\nEdit - when I wrote \"sensitive to small differences in data\",  I meant the different outputs when a value is changed from 4.61 to 4.62 as shown below.   \r\nEdit2 - I'm not sure this should be even flagged as a bug, rather it's an unexpected result that arises when you really shouldn't use PowerTransformer on certain data. However, users might apply the PowerTransformer on hundreds of features at once, and perhaps the situation described below could raise a warning?\r\n\r\nHello,  \r\nI'm trying to test how a Yeo-Johnson transformation might affect a model I'm working on (however, this issue is related to standardization rather than to the Yeo-Johnson transformation itself).  \r\nFor certain features, which aren't well suited for a Yeo-Johnson or Box-Cox transformation, the algorithm returns extremely similar values (when standardize=False), with differences only in the last few significant digits. This is consistent with scipy's `yeojohnson` and behaving as intended. However, then the standardization (standardize=True) can either yield unexpected results, or not, depending on small differences in the original data. \r\n\r\nIn one of such cases, the standardization (standardize=True) returns values that maintain the original trend, but are very small, in the order of 10^-17. This behavior of the standardization algorithm is very sensitive to minimal differences in the original data, as if I change just a 4.62 value to 4.61, it succeeds and creates a standardized array with a reasonable value range, I would say masking these very small differences in the original transformed array; while sometimes it doesn't succeed and yields a standardized array with values of ~10^-17, as mentioned above.  \r\nFurthermore, when the standardized data is in the 10^-17 range, even MinMaxScaler can't scale properly these values, which remain in the order of 10^-17.   \r\n\r\n\r\n\r\n\r\nI have included a simple array of 10 observations that can reproduce the issue.  \r\n\r\nIn the example, the initial value of data[0, 0] is 4.61, which superficially doesn't show issues when using `standardize=True`. Here, PowerTransformer returns a range of values from -1.04 to 2.58, which follow the original trend and can be further transformed by the MinMaxScaler to the range (0, 1).    \r\n\r\nWith minimal differences to the original data, i.e. if data[0, 0] is 4.62, PowerTransformer's standardization returns values in the range of -10^-18 to 0, so extremely close to 0; and MinMaxScaler retains the proportionalities, but doesn't scale them properly.  \r\n\r\nWith `standardize=False`, the method returns the transformed values, which as mentioned above are very close, down to almost the last significant digit, and MinMaxScaler is unable to scale them.  \r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.preprocessing import PowerTransformer, MinMaxScaler\r\n\r\ndata = np.array([[4.61],\r\n                 [4.56],\r\n                 [4.59],\r\n                 [4.56],\r\n                 [4.71],\r\n                 [5.63],\r\n                 [4.66],\r\n                 [4.67],\r\n                 [4.70],\r\n                 [4.61]])\r\n\r\nfor value in range(2):\r\n    # Tests with 4.61 and 4.62\r\n    data[0, 0] += value * 0.01\r\n    print(f\"\\nWhen data[0, 0] is: {data[0, 0]}\")\r\n    pt = PowerTransformer(standardize=True) # ALSO TRY WITH: `standardize=False`\r\n    data_pt = pt.fit_transform(data)\r\n    print(\"\\nAfter Powertransformer:\")\r\n    print(data_pt)\r\n    mms = MinMaxScaler(feature_range=(0,1))\r\n    data_mms = mms.fit_transform(data_pt)\r\n    print(\"\\nAfter MinMaxScaler:\")\r\n    print(data_mms)\r\n```\r\n\r\n### Expected Results\r\n\r\nPower-transformed data, and scaled data in the range (0, 1)\r\n\r\n### Actual Results\r\n\r\n```\r\nWhen data[0, 0] is: 4.61\r\n\r\nAfter Powertransformer:\r\n[[-0.45266018]\r\n [-1.04111841]\r\n [-0.67899027]\r\n [-1.04111841]\r\n [ 0.40739416]\r\n [ 2.58016302]\r\n [ 0.04526602]\r\n [ 0.09053204]\r\n [ 0.36212814]\r\n [-0.45266018]]\r\n\r\nAfter MinMaxScaler:\r\n[[0.1625]\r\n [0.    ]\r\n [0.1   ]\r\n [0.    ]\r\n [0.4   ]\r\n [1.    ]\r\n [0.3   ]\r\n [0.3125]\r\n [0.3875]\r\n [0.1625]]\r\n\r\nWhen data[0, 0] is: 4.62\r\n\r\nAfter Powertransformer:\r\n[[-4.16333634e-17]\r\n [-1.17961196e-16]\r\n [-7.63278329e-17]\r\n [-1.17961196e-16]\r\n [ 4.16333634e-17]\r\n [ 2.77555756e-16]\r\n [ 0.00000000e+00]\r\n [ 6.93889390e-18]\r\n [ 4.16333634e-17]\r\n [-5.55111512e-17]]\r\n\r\nAfter MinMaxScaler:\r\n[[7.63278329e-17]\r\n [0.00000000e+00]\r\n [4.16333634e-17]\r\n [0.00000000e+00]\r\n [1.59594560e-16]\r\n [3.95516953e-16]\r\n [1.17961196e-16]\r\n [1.24900090e-16]\r\n [1.59594560e-16]\r\n [6.24500451e-17]]\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:04:44) [MSC v.1940 64 bit (AMD64)]\r\nexecutable: C:\\Users\\Roberto\\miniconda3\\envs\\ml2\\python.exe\r\n   machine: Windows-10-10.0.19045-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.1\r\n          pip: 24.1.2\r\n   setuptools: 72.1.0\r\n        numpy: 2.0.1\r\n        scipy: 1.14.0\r\n       Cython: None\r\n       pandas: 2.2.2\r\n   matplotlib: 3.9.1\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libscipy_openblas\r\n       filepath: C:\\Users\\Roberto\\miniconda3\\envs\\ml2\\Lib\\site-packages\\numpy.libs\\libscipy_openblas64_-fb1711452d4d8cee9f276fd1449ee5c7.dll\r\n        version: 0.3.27\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\Roberto\\miniconda3\\envs\\ml2\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libscipy_openblas\r\n       filepath: C:\\Users\\Roberto\\miniconda3\\envs\\ml2\\Lib\\site-packages\\scipy.libs\\libscipy_openblas-5b1ec8b915dfb81d11cebc0788069d2d.dll\r\n        version: 0.3.27.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n```\r\n","labels":["Bug"],"created_at":"2024-07-31T10:35:55Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29595"},{"issue_number":39,"repository":"scikit-learn\/scikit-learn","title":"BUG Problem when `CalibratedClassifierCV` train contains 2 classes but data contains more","description":"### Describe the bug\n\nIn `CalibratedClassifierCV` when a train split contains 2 classes (binary) but the data contains more (>=3) classes, we assume the data is binary:\r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/d20e0b9abc4a4798d1fd839db50b19c01723094e\/sklearn\/calibration.py#L605-L607\r\n\r\nand we only end up fitting one calibrator:\r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/d20e0b9abc4a4798d1fd839db50b19c01723094e\/sklearn\/calibration.py#L620-L621\r\n\r\nContext: noticed when looking #29545 and trying to update [`test_calibration_less_classes`](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/d20e0b9abc4a4798d1fd839db50b19c01723094e\/sklearn\/tests\/test_calibration.py#L441)\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport numpy as np\r\n\r\nfrom sklearn.model_selection import KFold\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.calibration import CalibratedClassifierCV\r\n\r\nX = np.random.randn(12, 5)\r\ny = [0, 0, 0, 0] + [1, 1, 1, 1] + [2, 2, 2, 2]\r\nclf = DecisionTreeClassifier(random_state=7)\r\ncal_clf = CalibratedClassifierCV(\r\n    clf, method=\"sigmoid\", cv=KFold(3), ensemble=True\r\n)\r\ncal_clf.fit(X, y)\r\nfor i in range(3):\r\n    print(f'Fold: {i}')\r\n    proba = cal_clf.calibrated_classifiers_[i].predict_proba(X)\r\n    print(proba)\r\n```\n\n### Expected Results\n\nExpect proba to be 0 ONLY for the class not present in the train subset.\n\n### Actual Results\n\n```\r\nFold: 0  # train contains class 1 and 2, we take the first `pos_class_indices` (1) to be the positive class\r\n[[0. 1. 0.]\r\n [0. 1. 0.]\r\n [0. 1. 0.]\r\n [0. 1. 0.]\r\n [0. 1. 0.]\r\n [0. 1. 0.]\r\n [0. 1. 0.]\r\n [0. 1. 0.]\r\n [0. 1. 0.]\r\n [0. 1. 0.]\r\n [0. 1. 0.]\r\n [0. 1. 0.]]\r\nFold: 1  # train contains class 0 and 2, 0 is the first `pos_class_indices`\r\n[[1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]]\r\nFold: 2  # train contains class 0 and 1, `0` is the first `pos_class_indices`\r\n[[1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]\r\n [1. 0. 0.]]\r\n```\r\n\r\nA reasonable fix is to check when `CalibratedClassifierCV.classes_` is greater than `estimator.classes_` and output both proba and 1 - proba (assuming we can know which class the estimator deemed to be the positive class).\r\n\r\nIt does raise the question of whether we should warn when this happens..?\r\n\n\n### Versions\n\n```shell\nUsed main\n```\n","labels":["Bug"],"created_at":"2024-07-24T06:40:05Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29551"},{"issue_number":40,"repository":"scikit-learn\/scikit-learn","title":"decomposition.PCA(svd_solver='covariance_eigh') is less stable with numpy==2.0","description":"### Describe the bug\n\n`decomposition.PCA(svd_solver='covariance_eigh')` is less stable with numpy==2.0\r\n\r\nI noticed this issue as some tests started failing at the downstream [dask-ml\/#997](https:\/\/github.com\/dask\/dask-ml\/pull\/997)\r\n\r\nFor a certain data input, `pca.transform` gives incredibly large values, which are not seen with numpy==1.24.3\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nfrom sklearn import datasets, decomposition\r\nX = datasets.make_low_rank_matrix(1000, 10, effective_rank=2, random_state=0, tail_strength=0)\r\npca = decomposition.PCA(n_components=None, whiten=True, svd_solver='auto').fit(X)\r\nnp.max(np.abs(pca.transform(X)))\r\n```\r\n\n\n### Expected Results\n\n```python\r\n>>> 3.9065841446726326  # with numpy==1.24.3\r\n```\r\n\n\n### Actual Results\n\n```python\r\n>>> np.float64(874957.7078303652)  # with numpy==2.0\r\n```\r\n\r\nBoth uses sklearn==1.5.1, so probably an upstream issue.\r\n\r\nIndeed, the singular values from numpy==2.0 contains zero\r\n```python\r\npca.singular_values_\r\n# with numpy==1.24.3\r\n>>> array([9.99821683e-01, 7.78610978e-01, 3.67623087e-01, 1.05238902e-01,\r\n       1.83047062e-02, 1.92838400e-03, 1.23400336e-04, 4.77881459e-06,\r\n       1.12514225e-07, 2.93106569e-09])\r\n# with numpy==2.0\r\n>>> array([9.99821683e-01, 7.78610978e-01, 3.67623087e-01, 1.05238902e-01,\r\n       1.83047062e-02, 1.92838400e-03, 1.23400336e-04, 4.77880939e-06,\r\n       1.12582366e-07, 0.00000000e+00])\r\n```\r\nProbably this zero makes something wrong?\n\n### Versions\n\n```shell\n### numpy==2.0 configuration\r\n\r\nIn [10]: sklearn.show_versions()\r\n\r\nSystem:\r\n    python: 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:03:56) [MSC v.1929 64 bit (AMD64)]\r\nexecutable: C:\\Users\\oqf\\AppData\\Local\\anaconda3\\envs\\numpy2\\python.exe\r\n   machine: Windows-10-10.0.19045-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.1\r\n          pip: 24.0\r\n   setuptools: 69.5.1\r\n        numpy: 2.0.0\r\n        scipy: 1.14.0\r\n       Cython: None\r\n       pandas: 2.2.2\r\n   matplotlib: None\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 12\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\oqf\\AppData\\Local\\anaconda3\\envs\\numpy2\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 12\r\n         prefix: libscipy_openblas\r\n       filepath: C:\\Users\\oqf\\AppData\\Local\\anaconda3\\envs\\numpy2\\Lib\\site-packages\\numpy.libs\\libscipy_openblas64_-fb1711452d4d8cee9f276fd1449ee5c7.dll    \r\n        version: 0.3.27\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 12\r\n         prefix: libscipy_openblas\r\n       filepath: C:\\Users\\oqf\\AppData\\Local\\anaconda3\\envs\\numpy2\\Lib\\site-packages\\scipy.libs\\libscipy_openblas-5b1ec8b915dfb81d11cebc0788069d2d.dll       \r\n        version: 0.3.27.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n\r\n### numpy==1.24.3 configuration\r\n```python\r\nSystem:\r\n    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\Users\\oqf\\AppData\\Local\\anaconda3\\python.exe\r\n   machine: Windows-10-10.0.19045-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.5.1\r\n          pip: 23.2.1\r\n   setuptools: 68.0.0\r\n        numpy: 1.24.3\r\n        scipy: 1.11.1\r\n       Cython: 3.0.10\r\n       pandas: 2.0.3\r\n   matplotlib: 3.7.2\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.5.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: mkl\r\n    num_threads: 10\r\n         prefix: mkl_rt\r\n       filepath: C:\\Users\\oqf\\AppData\\Local\\anaconda3\\Library\\bin\\mkl_rt.2.dll\r\n        version: 2023.1-Product\r\nthreading_layer: intel\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 12\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\oqf\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 12\r\n         prefix: libiomp\r\n       filepath: C:\\Users\\oqf\\AppData\\Local\\anaconda3\\Library\\bin\\libiomp5md.dll\r\n        version: None\r\n```\n```\n","labels":["Bug"],"created_at":"2024-07-22T00:40:46Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29534"},{"issue_number":41,"repository":"scikit-learn\/scikit-learn","title":"RFE results are inconsistent between machines with ties in feature importances at threshold","description":"### Describe the bug\r\n\r\nRFE uses np.argsort on the feature_importances from the estimator, this is not repeatable across machines. This only matters when there are ties in the feature importances that overlap with the threshold. For example if the importances are [0, 2, 0, 1] and it needs to eliminate 1 feature, it may not choose the same feature to eliminate on all machines.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\nIt is difficult to write code that will show this as it requires multiple machines to test; however, the following test could be added to the _rfe unit tests to check if the sort is stable, though the sort could be consistent but not stable, so this isn't enough to prove an issue.\r\n```\r\ndef test_rfe_ties_around_threshold():\r\n    X, y = make_classification(n_features=47, random_state=0)\r\n    clf = MockClassifier() # mock classifier returns constant feature_importances\r\n    rfe = RFE(estimator=clf, n_features_to_select=4, step=2)\r\n    rfe.fit(X, y)\r\n\r\n    assert_array_equal(rfe.support_ ,np.array([*[False]*43, *[True]*4]))\r\n```\r\n\r\n### Expected Results\r\n\r\nExpected to get the same results on all machines, being stable is not necessary though it is an easy way to enforce testable consistency.\r\n\r\n### Actual Results\r\n\r\nCurrent sort does not result in a stable sort and fails the previously provided unit test.\r\n\r\n### Versions\r\n\r\n```shell\r\nmain branch\r\n```\r\n\r\n\r\nThis issue is most likely to occur with zero importance features, especially at the initial steps of RFE.","labels":["Bug"],"created_at":"2024-07-21T20:29:03Z","comments":14,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29531"},{"issue_number":42,"repository":"scikit-learn\/scikit-learn","title":"NDCG in case of abscence of relevant items","description":"### Describe the bug\n\nIn `sklearn.metrics._ndcg_sample_scores`, there is a counterintuitive handling of the case where all true relevances are equal to zero for some samples. In this case, DCG = 0, IDCG = 0, and the whole NDCG is not defined. In `sklearn` implementation it is defined as 0 and included in the averaged NDCG calculation.  The least leads to strange effects, like `ndcg_score(y,y) != 1`; moreover, it affects the metric value in non-trivial cases too.\r\n\r\nIn the original 2002 paper where NDCG is proposed, it is not stated how to handle such situations, but it is clearly mentioned that \r\n```\r\nThe (D)CG vectors for each IR technique can be normalized by dividing them\r\nby the corresponding ideal (D)CG vectors, component by component. In this way,\r\nfor any vector position, the normalized value 1 represents ideal performance,\r\nand values in the range [0, 1) the share of ideal performance cumulated by each\r\ntechnique.\r\n```\r\nmeaning that NDCG(y,y) must always be 1. \r\n\r\n\r\nI suggest excluding observations without relevant items and\/or throwing a warning.\n\n### Steps\/Code to Reproduce\n\n```\r\n>>> from sklearn.metrics import ndcg_score\r\n>>> y = np.array([[1.0, 0.0, 1.0], [0.0, 0.0, 0.0]])\r\n>>> ndcg_score(y, y)\r\n```\r\n\n\n### Expected Results\n\n1.\n\n### Actual Results\n\n0.5\n\n### Versions\n\n```shell\nThis code was not changed in 1.5, so I guess for newer versions the issue also is actual.\r\n\r\n\r\n\r\nSystem:\r\n    python: 3.11.8 (main, Feb 12 2024, 14:50:05) [GCC 13.2.1 20230801]\r\nexecutable: \/usr\/bin\/python3\r\n   machine: Linux-6.6.19-1-MANJARO-x86_64-with-glibc2.39\r\n\r\nPython dependencies:\r\n      sklearn: 1.3.1\r\n          pip: 24.0\r\n   setuptools: 69.0.3\r\n        numpy: 1.26.4\r\n        scipy: 1.10.1\r\n       Cython: 3.0.9\r\n       pandas: 1.5.3\r\n   matplotlib: 3.7.1\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/home\/arabella\/.local\/lib\/python3.11\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/arabella\/.local\/lib\/python3.11\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/arabella\/.local\/lib\/python3.11\/site-packages\/scipy.libs\/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 8\n```\n","labels":["Bug","help wanted"],"created_at":"2024-07-19T01:23:38Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29521"},{"issue_number":43,"repository":"scikit-learn\/scikit-learn","title":"GroupKFold inconsistent under ties in group sizes.","description":"### Describe the bug\n\nDue to the use of argsort (a non stable sort withthout the stable parameter introduced in numpy 2.0), GroupKFold is not always reproducible when there are ties in group sizes.\n\n### Steps\/Code to Reproduce\n\nYou may need to run this on different machines, but you can reproduce this issue with even less code than GroupKFold by simply testing `np.argsort`.\r\n```\r\nimport numpy as np\r\n\r\nx = np.array([0.37454012, 0.95071431, 0.73199394, 0.        , 0.15601864,\r\n       0.        , 0.05808361, 0.86617615, 0.60111501, 0.70807258,\r\n       0.02058449, 0.96990985, 0.83244264, 0.21233911, 0.18182497,\r\n       0.18340451, 0.30424224, 0.52475643, 0.43194502, 0.29122914,\r\n       0.61185289, 0.13949386, 0.29214465, 0.36636184, 0.45606998,\r\n       0.78517596, 0.19967378, 0.51423444, 0.59241457, 0.        ,\r\n       0.60754485, 0.17052412, 0.06505159, 0.        , 0.96563203,\r\n       0.80839735, 0.        , 0.        , 0.68423303, 0.44015249,\r\n       0.12203823])\r\n\r\nprint(np.argsort(x))\r\n```\r\nBoth times with numpy version: 1.26.4\r\nmachine a: `array(3,5,29,33,36,37,10,6,32,40...`\r\nmachine b: `array(37,36,3,29,5,33,10,6,32,40...`\n\n### Expected Results\n\nTechnically a stable sort isn't required, no need to maintain the original ordering. However a consistent sort result is required for reproducibility.  Stable sort is an easy way to achieve this that is already built into numpy.\n\n### Actual Results\n\nBoth times with numpy version: 1.26.4\r\nmachine a: `array(3,5,29,33,36,37,10,6,32,40...`\r\nmachine b: `array(37,36,3,29,5,33,10,6,32,40...`\n\n### Versions\n\n```shell\nsklearn versions are mostly irrelevant here as the issue is with calling a numpy method that requires additional changes.\n```\n","labels":["Bug"],"created_at":"2024-07-15T19:59:57Z","comments":7,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29495"},{"issue_number":44,"repository":"scikit-learn\/scikit-learn","title":"KernelDensity(bandwidth='silverman') doesn't throw proper error for 1d X","description":"Essentially the bandwidth estimation codepath is not covered in the common tests, but it should be :)","labels":["Bug"],"created_at":"2024-07-10T01:32:02Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29443"},{"issue_number":45,"repository":"scikit-learn\/scikit-learn","title":"kernel_approximation.Nystroem with precomputed kernel","description":"### Describe the bug\n\nI am trying to get a Nystroem approximation of a pre computed kernel but it throws an error if I use n_components anything less than the number of datapoints. Unless my understanding is wrong, does this not defeat the point of the approximation? Please advise, code below:\r\n\r\n\r\n\r\nI have come from this resolved issue https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/14641 \n\n### Steps\/Code to Reproduce\n\n```python\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.kernel_approximation import Nystroem\r\n \r\n# data shape (3000,50)\r\n# kernel matrix shape (3000,3000)\r\nclf = SVC()\r\nfeature_map_nystroem = Nystroem(\r\n    kernel = 'precomputed',\r\n    random_state=1,\r\n    n_components=300\r\n)\r\nkernel_transformed = feature_map_nystroem.fit_transform(kernel)\r\nclf.fit(kernel_transformed, y)\r\n```\r\n\r\n\n\n### Expected Results\n\nI expect this to work. \n\n### Actual Results\n\nInstead it gives error:\r\n\r\n```python\r\nin check_pairwise_arrays(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\r\n    153 if precomputed:\r\n    154     if X.shape[1] != Y.shape[0]:\r\n--> 155 raise ValueError(\"Precomputed metric requires shape \"\r\n\r\n    156                 \"(n_queries, n_indexed). Got (%d, %d) \"\r\n    157                          \"for %d indexed.\" %\r\n    158                          (X.shape[0], X.shape[1], Y.shape[0]))\r\n    159 elif X.shape[1] != Y.shape[1]:\r\n    160     raise ValueError(\"Incompatible dimension for X and Y matrices: \"\r\n    161                      \"X.shape[1] == %d while Y.shape[1] == %d\" % (\r\n    162                          X.shape[1], Y.shape[1]))\r\n \r\nValueError: Precomputed metric requires shape (n_queries, n_indexed). Got (300, 3000) for 300 indexed.\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.8.19 (default, Mar 20 2024, 19:58:24)  [GCC 11.2.0]\r\nexecutable: \/opt\/conda\/miniconda3\/envs\/python3.8\/bin\/python\r\n   machine: Linux-6.1.0-21-cloud-amd64-x86_64-with-glibc2.17\r\n \r\nPython dependencies:\r\n          pip: 24.0\r\n   setuptools: 63.1.0\r\n      sklearn: 0.24.2\r\n        numpy: 1.21.6\r\n        scipy: 1.10.1\r\n       Cython: 3.0.10\r\n       pandas: 1.2.5\r\n   matplotlib: 3.4.3\r\n       joblib: 1.4.2\r\nthreadpoolctl: 3.5.0\n```\n","labels":["Bug"],"created_at":"2024-06-26T15:50:45Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29353"},{"issue_number":46,"repository":"scikit-learn\/scikit-learn","title":"`SimpleImputer` Fails with pyarrow String Types in sklearn","description":"### Describe the bug\n\nWhen using SimpleImputer from sklearn with pyarrow string types, the imputer fails with an error. This issue occurs when attempting to impute missing values in a DataFrame containing pyarrow string columns.\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.impute import SimpleImputer\r\n\r\n# Create a DataFrame with pyarrow string types\r\ndata = {\r\n    'mpg': [18, 25, 120, 120],\r\n    'make': ['Ford', 'Chevy', np.nan, 'Tesla']\r\n}\r\ndf = (pd.DataFrame(data)\r\n       .astype({'make':'string[pyarrow]'})\r\n)\r\n\r\n# Initialize SimpleImputer\r\nimputer = SimpleImputer(strategy='most_frequent')\r\n\r\n# Attempt to fit and transform the DataFrame\r\nimputer.fit_transform(df[[\"make\"]])\r\n```\r\n\n\n### Expected Results\n\nThe SimpleImputer should handle pyarrow string types and impute the missing values without raising an error.\n\n### Actual Results\n\n```\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[158], line 18\r\n     15 imputer = SimpleImputer(strategy='most_frequent')\r\n     17 # Attempt to fit and transform the DataFrame\r\n---> 18 imputer.fit_transform(df[[\\\"make\\\"]])\r\n\r\nFile ~\/.envs\/menv\/lib\/python3.10\/site-packages\/sklearn\/utils\/_set_output.py:295, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)\r\n    293 @wraps(f)\r\n    294 def wrapped(self, X, *args, **kwargs):\r\n--> 295     data_to_wrap = f(self, X, *args, **kwargs)\r\n    296     if isinstance(data_to_wrap, tuple):\r\n    297         # only wrap the first output for cross decomposition\r\n    298         return_tuple = (\r\n    299             _wrap_data_with_container(method, data_to_wrap[0], X, self),\r\n    300             *data_to_wrap[1:],\r\n    301         )\r\n\r\nFile ~\/.envs\/menv\/lib\/python3.10\/site-packages\/sklearn\/base.py:1098, in TransformerMixin.fit_transform(self, X, y, **fit_params)\r\n   1083         warnings.warn(\r\n   1084             (\r\n   1085                 f\\\"This object ({self.__class__.__name__}) has a `transform`\\\"\r\n   (...)\r\n   1093             UserWarning,\r\n   1094         )\r\n   1096 if y is None:\r\n   1097     # fit method of arity 1 (unsupervised transformation)\r\n-> 1098     return self.fit(X, **fit_params).transform(X)\r\n   1099 else:\r\n   1100     # fit method of arity 2 (supervised transformation)\r\n   1101     return self.fit(X, y, **fit_params).transform(X)\r\n\r\nFile ~\/.envs\/menv\/lib\/python3.10\/site-packages\/sklearn\/base.py:1474, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\r\n   1467     estimator._validate_params()\r\n   1469 with config_context(\r\n   1470     skip_parameter_validation=(\r\n   1471         prefer_skip_nested_validation or global_skip_validation\r\n   1472     )\r\n   1473 ):\r\n-> 1474     return fit_method(estimator, *args, **kwargs)\r\n\r\nFile ~\/.envs\/menv\/lib\/python3.10\/site-packages\/sklearn\/impute\/_base.py:427, in SimpleImputer.fit(self, X, y)\r\n    423     self.statistics_ = self._sparse_fit(\r\n    424         X, self.strategy, self.missing_values, fill_value\r\n    425     )\r\n    426 else:\r\n--> 427     self.statistics_ = self._dense_fit(\r\n    428         X, self.strategy, self.missing_values, fill_value\r\n    429     )\r\n    431 return self\r\n\r\nFile ~\/.envs\/menv\/lib\/python3.10\/site-packages\/sklearn\/impute\/_base.py:510, in SimpleImputer._dense_fit(self, X, strategy, missing_values, fill_value)\r\n    503 elif strategy == \\\"most_frequent\\\":\r\n    504     # Avoid use of scipy.stats.mstats.mode due to the required\r\n    505     # additional overhead and slow benchmarking performance.\r\n    506     # See Issue 14325 and PR 14399 for full discussion.\r\n    507 \r\n    508     # To be able access the elements by columns\r\n    509     X = X.transpose()\r\n--> 510     mask = missing_mask.transpose()\r\n    512     if X.dtype.kind == \\\"O\\\":\r\n    513         most_frequent = np.empty(X.shape[0], dtype=object)\r\n\r\nAttributeError: 'bool' object has no attribute 'transpose'\"\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.10.14 (main, Mar 19 2024, 21:46:16) [Clang 15.0.0 (clang-1500.3.9.4)]\r\nexecutable: \/Users\/matt\/.envs\/menv\/bin\/python\r\n   machine: macOS-14.2.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.4.1.post1\r\n          pip: 24.0\r\n   setuptools: 67.6.1\r\n        numpy: 1.23.5\r\n        scipy: 1.10.0\r\n       Cython: 0.29.37\r\n       pandas: 2.2.2\r\n   matplotlib: 3.6.2\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/Users\/matt\/.envs\/menv\/lib\/python3.10\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: armv8\r\n    num_threads: 8\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libomp\r\n       filepath: \/Users\/matt\/.envs\/menv\/lib\/python3.10\/site-packages\/sklearn\/.dylibs\/libomp.dylib\r\n        version: None\r\n    num_threads: 8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/Users\/matt\/.envs\/menv\/lib\/python3.10\/site-packages\/scipy\/.dylibs\/libopenblas.0.dylib\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: armv8\r\n    num_threads: 8\n```\n","labels":["Bug","New Feature","Pandas compatibility"],"created_at":"2024-06-19T00:03:26Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29291"},{"issue_number":47,"repository":"scikit-learn\/scikit-learn","title":"`_BaseEncoder` with boolean `categories_` that include `nan` fails on `transform` when `X` is boolean","description":"### Describe the bug\n\nAn `Encoder` that was fitted on a `DataFrame` with boolean columns that include `NaN` will fail when transforming a boolean `X` due to a mismatch in the `dtype`s when calling `_check_unknown`. Since `X` has no `object` `dtype`, there is an attempt to call `np.isnan(known_values)`, which fails because `known_values` _does_ have an `object` `dtype`.\r\n\r\nAs far as I can tell, this can be fixed by casting the `dtype` of `values` in [`_check_unknown`](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/7e8ad632ff\/sklearn\/utils\/_encode.py#L243) to the `dtype` of `known_values`:\r\n```python\r\nif values.dtype != known_values.dtype:\r\n     values = values.astype(known_values.dtype)\r\n```\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport pandas as pd\r\n\r\nfrom sklearn.preprocessing import OrdinalEncoder\r\n\r\nx = pd.DataFrame({'a': [True, False, np.nan]})\r\no = OrdinalEncoder()\r\no.fit_transform(x)\r\n\r\ny = pd.DataFrame({'a': [True, True, False]})\r\no.transform(y)\r\n```\n\n### Expected Results\n\nI expect the array to be transformed according to the known classes:\r\n```python\r\narray([[1.],\r\n       [1.],\r\n       [0.]])\r\n```\n\n### Actual Results\n\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[1], line 10\r\n      7 o.fit_transform(x)\r\n      9 y = pd.DataFrame({'a': [True, True, False]})\r\n---> 10 o.transform(y)\r\n\r\nFile ~\/miniconda3\/envs\/analytics-models-v2\/lib\/python3.11\/site-packages\/sklearn\/utils\/_set_output.py:295, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)\r\n    293 @wraps(f)\r\n    294 def wrapped(self, X, *args, **kwargs):\r\n--> 295     data_to_wrap = f(self, X, *args, **kwargs)\r\n    296     if isinstance(data_to_wrap, tuple):\r\n    297         # only wrap the first output for cross decomposition\r\n    298         return_tuple = (\r\n    299             _wrap_data_with_container(method, data_to_wrap[0], X, self),\r\n    300             *data_to_wrap[1:],\r\n    301         )\r\n\r\nFile ~\/miniconda3\/envs\/analytics-models-v2\/lib\/python3.11\/site-packages\/sklearn\/preprocessing\/_encoders.py:1578, in OrdinalEncoder.transform(self, X)\r\n   1564 \"\"\"\r\n   1565 Transform X to ordinal codes.\r\n   1566 \r\n   (...)\r\n   1575     Transformed input.\r\n   1576 \"\"\"\r\n   1577 check_is_fitted(self, \"categories_\")\r\n-> 1578 X_int, X_mask = self._transform(\r\n   1579     X,\r\n   1580     handle_unknown=self.handle_unknown,\r\n   1581     force_all_finite=\"allow-nan\",\r\n   1582     ignore_category_indices=self._missing_indices,\r\n   1583 )\r\n   1584 X_trans = X_int.astype(self.dtype, copy=False)\r\n   1586 for cat_idx, missing_idx in self._missing_indices.items():\r\n\r\nFile ~\/miniconda3\/envs\/analytics-models-v2\/lib\/python3.11\/site-packages\/sklearn\/preprocessing\/_encoders.py:206, in _BaseEncoder._transform(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\r\n    204 Xi = X_list[i]\r\n    205 breakpoint()\r\n--> 206 diff, valid_mask = _check_unknown(Xi, self.categories_[i], return_mask=True)\r\n    208 if not np.all(valid_mask):\r\n    209     if handle_unknown == \"error\":\r\n\r\nFile ~\/miniconda3\/envs\/analytics-models-v2\/lib\/python3.11\/site-packages\/sklearn\/utils\/_encode.py:307, in _check_unknown(values, known_values, return_mask)\r\n    304         valid_mask = np.ones(len(values), dtype=bool)\r\n    306 # check for nans in the known_values\r\n--> 307 if np.isnan(known_values).any():\r\n    308     diff_is_nan = np.isnan(diff)\r\n    309     if diff_is_nan.any():\r\n    310         # removes nan from valid_mask\r\n\r\nTypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.11.8 (main, Feb 26 2024, 15:43:17) [Clang 14.0.6 ]\r\nexecutable: ~\/miniconda3\/envs\/analytics-models-v2\/bin\/python\r\n   machine: macOS-10.16-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.4.1.post1\r\n          pip: 23.3.1\r\n   setuptools: 68.2.2\r\n        numpy: 1.26.4\r\n        scipy: 1.13.0\r\n       Cython: None\r\n       pandas: 2.1.4\r\n   matplotlib: 3.8.4\r\n       joblib: 1.4.0\r\nthreadpoolctl: 3.4.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 6\r\n         prefix: libopenblas\r\n       filepath: ~\/miniconda3\/envs\/analytics-models-v2\/lib\/libopenblasp-r0.3.21.dylib\r\n        version: 0.3.21\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 12\r\n         prefix: libomp\r\n       filepath: ~\/miniconda3\/envs\/analytics-models-v2\/lib\/python3.11\/site-packages\/sklearn\/.dylibs\/libomp.dylib\r\n        version: None\n```\n","labels":["Bug"],"created_at":"2024-06-11T21:02:29Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29241"},{"issue_number":48,"repository":"scikit-learn\/scikit-learn","title":"classification_report with output_dict=True leads to brittle output","description":"### Describe the workflow you want to enable\n\nWhen `output_dict` is `True`, the returned `dict` structure is brittle and breaks if one of the class name is the same as one of the average metrics.  \r\nHere for example one of the class is named \"accuracy\" so that it doesn't appear in the returned `dict` .\r\n\r\n``` python\r\nfrom sklearn.metrics import classification_report\r\nfrom pprint import pprint\r\n\r\nclassification_report(\r\n    [\"chat\", \"accuracy\"],\r\n    [\"chat\", \"accuracy\"],\r\n    output_dict=True,\r\n)\r\n```\r\n\r\n``` python\r\n{'accuracy': 1.0,\r\n 'chat': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},\r\n 'macro avg': {'f1-score': 1.0,\r\n               'precision': 1.0,\r\n               'recall': 1.0,\r\n               'support': 2.0},\r\n 'weighted avg': {'f1-score': 1.0,\r\n                  'precision': 1.0,\r\n                  'recall': 1.0,\r\n                  'support': 2.0}}\r\n```\n\n### Describe your proposed solution\n\nAny unambiguous output structure, such as separating between class-wise and average metrics:\r\n``` python\r\n{\r\n    'class': {\r\n        'accuracy': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},\r\n        'chat': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},\r\n    },\r\n    'average': {\r\n        'accuracy': 1.0,\r\n        'macro avg': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},\r\n        'weighted avg': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},\r\n    },\r\n}\r\n```\r\n\r\nOr:\r\n``` python\r\n{\r\n    'class': {\r\n        'accuracy': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},\r\n        'chat': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},\r\n    },\r\n    'accuracy': 1.0,\r\n    'macro avg': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},\r\n    'weighted avg': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},\r\n}\r\n```\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_","labels":["Bug"],"created_at":"2024-06-06T12:11:50Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29205"},{"issue_number":49,"repository":"scikit-learn\/scikit-learn","title":"FIX TunedThresholdClassifierCV error or warn with informative message on invalid metrics","description":"This PR fixes two usability problems with the new `TunedThresholdClassifierCV` when using it with invalid values for the `scoring` parameter:\r\n\r\n- the first case, is passing a scoring name or scorer object that expects metrics defined for unthresholded predictions (e.g. ROC-AUC). This is clearly invalid and we can raise a `ValueError` with a meaning error message.\r\n- the second case is passing an under-specified scoring function that would return a constant prediction on a given dataset. In this case I chose to warn the user but leave the dummy threshold value that results from this case.\r\n\r\nFor the second point we could instead warn the user and keep on using 0.5 as the threshold which is probably less pathological\/arbitrary.\r\n\r\nAlternatively we could also raise a `ValueError` but I am worried that this error could be triggered for bad reasons when doing a grid search or an other resampling procedure around the `TunedThresholdClassifierCV` instance, hence I thought that a warning would be less disruptive.\r\n\r\n\/cc @glemaitre @lorentzenchr ","labels":["Bug","module:model_selection","To backport"],"created_at":"2024-05-22T16:57:58Z","comments":20,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/29082"},{"issue_number":50,"repository":"scikit-learn\/scikit-learn","title":"MultiOutputClassifier does not rely on estimator to provide pairwise tag","description":"### Describe the bug\r\n\r\nI use the `MultiOutputClassifier` function to make `SVC` multilabel. \r\n\r\nThen, if I use the linear or rbf kernel the cross_validation function works perfectly fine.\r\n\r\nHowever, when I use `SVC` with precomputed kernel is having an `ValueError: Precomputed matrix must be a square matrix`. \r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.metrics import pairwise_distances\r\nfrom sklearn.multioutput import MultiOutputClassifier\r\nfrom sklearn.model_selection import cross_val_score, cross_validate\r\n\r\nsvm = SVC(kernel='precomputed', C=100, random_state=42)\r\nmultilabel_classifier = MultiOutputClassifier(svm, n_jobs=-1)\r\n\r\nX = np.random.rand(1000, 1000)\r\ny = np.random.randint(0, 2, size=(1000, 6))\r\n\r\nkernel_eucl = pairwise_distances(X, metric='euclidean')\r\n\r\ncross_validate(\r\n    multilabel_classifier, kernel_eucl, y, cv=10, scoring='f1_weighted', n_jobs=-1\r\n)\r\n```\r\n\r\n### Expected Results\r\n\r\nAn weighted f1-score.\r\n\r\n### Actual Results\r\n\r\n```pytb\r\nValueError: \r\nAll the 10 fits failed.\r\nIt is very likely that your model is misconfigured.\r\nYou can try to debug the error by setting error_score='raise'.\r\n\r\nBelow are more details about the failures:\r\n--------------------------------------------------------------------------------\r\n10 fits failed with the following error:\r\n\r\n File \"C:\\Users\\bscuser\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 217, in fit\r\n    raise ValueError(\r\nValueError: Precomputed matrix must be a square matrix. Input is a 900x1000 matrix.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\Users\\bscuser\\anaconda3\\python.exe\r\n   machine: Windows-10-10.0.19045-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.2\r\n          pip: 23.3.1\r\n   setuptools: 68.2.2\r\n        numpy: 1.26.4\r\n        scipy: 1.11.4\r\n       Cython: None\r\n       pandas: 2.1.4\r\n   matplotlib: 3.8.0\r\n       joblib: 1.2.0\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       filepath: C:\\Users\\bscuser\\anaconda3\\Library\\bin\\mkl_rt.2.dll\r\n         prefix: mkl_rt\r\n       user_api: blas\r\n   internal_api: mkl\r\n        version: 2023.1-Product\r\n    num_threads: 4\r\nthreading_layer: intel\r\n\r\n       filepath: C:\\Users\\bscuser\\anaconda3\\vcomp140.dll\r\n         prefix: vcomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 4\r\n\r\n       filepath: C:\\Users\\bscuser\\anaconda3\\Library\\bin\\libiomp5md.dll\r\n         prefix: libiomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 4\r\n```\r\n","labels":["Bug"],"created_at":"2024-05-14T10:55:28Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/29016"},{"issue_number":51,"repository":"scikit-learn\/scikit-learn","title":"Yeo-Johnson inverse_transform fails silently on extreme skew data","description":"### Describe the bug\r\n\r\nThe Yeo-Johnson is not a surjective transformation for negative lambdas. Therefore, the inverse transformation returns `np.nan` when inverse transforming values outside the range of the transform. This failure is silent, so it took me quite a while of debugging to understand this behavior.\r\n\r\nThe problematic lines are\r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/8721245511de2f225ff5f9aa5f5fadce663cd4a3\/sklearn\/preprocessing\/_data.py#L3390\r\n\r\nand \r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/8721245511de2f225ff5f9aa5f5fadce663cd4a3\/sklearn\/preprocessing\/_data.py#L3386\r\n\r\nin which we might compute `np.power(something_negative, not_integral_value)`, which of course returns `np.nan` as per https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.power.html\r\n\r\n### Steps\/Code to Reproduce\r\n\r\nTo reproduce for positive values (there is a similar problem for negative values):\r\n\r\n```python\r\nimport numpy as np\r\nimport sklearn.preprocessing\r\ntrans = sklearn.preprocessing.PowerTransformer(method='yeo-johnson')\r\nx = np.array([1,1,1e10]).reshape(-1, 1) # extreme skew\r\ntrans.fit(x)\r\nlmbda = trans.lambdas_[0] \r\nprint(lmbda)\r\nassert lmbda < 0 # == -0.096 negative value\r\n\r\n# any value `psi` for which lambda*psi+1 <= 0 will result in nan due to lacking support, since the forwards transformation \r\n# is not surjective on negative lambdas. In this specific case, 10*-0.096 < 1\r\npsi = np.array([10]).reshape(-1, 1)\r\nx = trans.inverse_transform(psi).item()\r\nprint(x)\r\nassert np.isnan(x)\r\n```\r\n\r\n### Expected Results\r\n\r\n The code should either:\r\n\r\n1) validate its inputs and raise an exception\r\n2) validate its inputs and raise a warning\r\n3) fail silently, but have it documented behavior\r\n\r\n### Actual Results\r\n\r\nIt just prints\r\n\r\n```\r\n-0.0962322261004418\r\nnan\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.11.3 (main, Jan 18 2024, 19:07:12) [Clang 18.0.0 (https:\/\/github.com\/llvm\/llvm-project 75501f53624de92aafce2f1da698\r\nexecutable: \/home\/pyodide\/this.program\r\n   machine: Emscripten-3.1.46-wasm32-32bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.3.1\r\n          pip: None\r\n   setuptools: None\r\n        numpy: 1.26.1\r\n        scipy: 1.11.2\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: None\r\n       joblib: 1.3.2\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: False\r\n```\r\n","labels":["Bug"],"created_at":"2024-05-04T08:23:07Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28946"},{"issue_number":52,"repository":"scikit-learn\/scikit-learn","title":"BUG internal indexing tools trigger error with pandas < 2.0.0","description":"[#28375](https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/28375#issuecomment-2088926826) triggers errors for pandas < 2.0.0, despite just using scikit-learn internal functionalities.\r\n\r\nAs documented in https:\/\/scikit-learn.org\/dev\/install.html, we have pandas >= 1.1.3.","labels":["Bug","Pandas compatibility"],"created_at":"2024-05-02T09:58:49Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28931"},{"issue_number":53,"repository":"scikit-learn\/scikit-learn","title":"`FunctionTransformer` need `feature_names_out` even if `func` returns DataFrame","description":"### Describe the bug\r\n\r\nTrying to call `transform` for `FunctionTransformer` for which `feature_names_out` is configured raises error that advises to use `set_output(transform='pandas')`. But this doesn't change anything.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```py\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.preprocessing import FunctionTransformer\r\n\r\nmy_transformer = FunctionTransformer(\r\n    lambda X : pd.concat(\r\n        [\r\n            X[col].rename(f\"{col} {str(power)}\")**power\r\n            for col in X\r\n            for power in range(2,4)\r\n        ],\r\n        axis=1\r\n    ),\r\n    feature_names_out = (\r\n        lambda transformer, input_features: [\r\n            f\"{feature} {power_str}\"\r\n            for feature in input_features\r\n            for power_str in [\"square\", \"cubic\"]\r\n        ]\r\n    )\r\n)\r\n# I specified transform=pandas\r\nmy_transformer.set_output(transform='pandas')\r\nsample_size = 10\r\nX = pd.DataFrame({\r\n    \"feature 1\" : [1,2,3,4,5],\r\n    \"feature 2\" : [3,4,5,6,7]\r\n})\r\nmy_transformer.fit(X)\r\nmy_transformer.transform(X)\r\n```\r\n\r\n### Expected Results\r\n\r\n`pandas.DataFrame` like following\r\n\r\n|    |   feature 1 square |   feature 1 cubic |   feature 2 square |   feature 2 cubic |\r\n|---:|-------------------:|------------------:|-------------------:|------------------:|\r\n|  0 |                  1 |                 1 |                  9 |                27 |\r\n|  1 |                  4 |                 8 |                 16 |                64 |\r\n|  2 |                  9 |                27 |                 25 |               125 |\r\n|  3 |                 16 |                84 |                 36 |               216 |\r\n|  4 |                 25 |               125 |                 49 |               343 |\r\n\r\n### Actual Results\r\n\r\n```\r\nValueError: The output generated by `func` have different column names than the ones provided by `get_feature_names_out`. Got output with columns names: ['feature 1 2', 'feature 1 3', 'feature 2 2', 'feature 2 3'] and `get_feature_names_out` returned: ['feature 1 square', 'feature 1 cubic', 'feature 2 square', 'feature 2 cubic']. The column names can be overridden by setting `set_output(transform='pandas')` or `set_output(transform='polars')` such that the column names are set to the names provided by `get_feature_names_out`.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\r\nexecutable: \/usr\/bin\/python3\r\n   machine: Linux-6.5.0-14-generic-x86_64-with-glibc2.35\r\n\r\nPython dependencies:\r\n      sklearn: 1.4.1.post1\r\n          pip: 24.0\r\n   setuptools: 68.2.2\r\n        numpy: 1.24.2\r\n        scipy: 1.11.1\r\n       Cython: None\r\n       pandas: 2.2.1\r\n   matplotlib: 3.7.1\r\n       joblib: 1.3.1\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/fedor\/.local\/lib\/python3.10\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so\r\n        version: 0.3.21\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/home\/fedor\/.local\/lib\/python3.10\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/fedor\/.local\/lib\/python3.10\/site-packages\/scipy.libs\/libopenblasp-r0-23e5df77.3.21.dev.so\r\n        version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n```\r\n```\r\n","labels":["Bug"],"created_at":"2024-04-06T10:17:51Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28780"},{"issue_number":54,"repository":"scikit-learn\/scikit-learn","title":"Missing _ZdlPv symbol in _argkmin_classmode for manylinux wheels produced by meson","description":"The current work-around is to use `-fno-sized-deallocation` see https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/28506#discussion_r1512897297 for more details.\r\n\r\nThis can be reproduced locally with cibuildwheel.\r\n```\r\npython -m cibuildwheel --only cp312-manylinux_x86_64\r\n```\r\nwill produced a manylinux wheel is in the wheelhouse folder which you can install through something like this:\r\n```\r\npip install wheelhouse\/scikit_learn-1.5.dev0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n```\r\n\r\nTraceback from build log:\r\n```\r\n\u276f python -c 'import sklearn.metrics'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"\/home\/lesteve\/micromamba\/envs\/del\/lib\/python3.12\/site-packages\/sklearn\/metrics\/__init__.py\", line 7, in <module>\r\n    from . import cluster\r\n  File \"\/home\/lesteve\/micromamba\/envs\/del\/lib\/python3.12\/site-packages\/sklearn\/metrics\/cluster\/__init__.py\", line 25, in <module>\r\n    from ._unsupervised import (\r\n  File \"\/home\/lesteve\/micromamba\/envs\/del\/lib\/python3.12\/site-packages\/sklearn\/metrics\/cluster\/_unsupervised.py\", line 23, in <module>\r\n    from ..pairwise import _VALID_METRICS, pairwise_distances, pairwise_distances_chunked\r\n  File \"\/home\/lesteve\/micromamba\/envs\/del\/lib\/python3.12\/site-packages\/sklearn\/metrics\/pairwise.py\", line 43, in <module>\r\n    from ._pairwise_distances_reduction import ArgKmin\r\n  File \"\/home\/lesteve\/micromamba\/envs\/del\/lib\/python3.12\/site-packages\/sklearn\/metrics\/_pairwise_distances_reduction\/__init__.py\", line 94, in <module>\r\n    from ._dispatcher import (\r\n  File \"\/home\/lesteve\/micromamba\/envs\/del\/lib\/python3.12\/site-packages\/sklearn\/metrics\/_pairwise_distances_reduction\/_dispatcher.py\", line 17, in <module>\r\n    from ._argkmin_classmode import (\r\nImportError: \/home\/lesteve\/micromamba\/envs\/del\/lib\/python3.12\/site-packages\/sklearn\/metrics\/_pairwise_distances_reduction\/_argkmin_classmode.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZdlPv\r\n```\r\n","labels":["Bug","Build \/ CI"],"created_at":"2024-03-08T09:24:56Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28596"},{"issue_number":55,"repository":"scikit-learn\/scikit-learn","title":"Inaccurate Attribute Listing with dir(obj) for Classes Using available_if Conditional Method Decorator","description":"### Describe the bug\r\n\r\nWhen utilizing the `available_if` decorator from SciKit Learn to conditionally expose methods based on specific object state or conditions, we observe that the `dir(obj)` function may return inaccurate results. Specifically, `dir(obj)` continues to list methods that should be conditionally hidden based on the `available_if` decorator's logic. This discrepancy arises because the `__dir__` method on the affected classes does not dynamically account for this conditional availability. As a result, users and consuming code may be misled about the actual methods available for use on instances of the class at runtime, potentially leading to unexpected `AttributeErrors` when accessing supposedly available methods.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\nI will test with the SVC, but it can apply to other classes.\r\n\r\n```python\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.datasets import load_iris\r\n\r\nX, y = load_iris(return_X_y=True)\r\n\r\nmodel = SVC(probability=False)\r\nmodel.fit(X[:100], y[:100])\r\n\r\n# Check if 'predict_proba' is listed by dir()\r\nprint(\"'predict_proba' in dir(model):\", \"predict_proba\" in dir(model))\r\n\r\n# Attempt to call 'predict_proba'\r\ntry:\r\n    prob_predictions = model.predict_proba(X[:2])\r\n    print(\"Predict_proba called successfully.\")\r\nexcept AttributeError as e:\r\n    print(\"Attempting to call 'predict_proba' raised an AttributeError:\", e)\r\n\r\n```\r\n\r\n### Expected Results\r\n\r\nIt should print out the following:\r\n```\r\n'predict_proba' in dir(model): False\r\nAttempting to call 'predict_proba' raised an AttributeError: predict_proba is not available when probability=False\r\n```\r\n\r\nMethods decorated with `available_if` and whose conditions raise an `AttributeError` should not appear in the list returned by `dir(obj)`.\r\n\r\n### Actual Results\r\n\r\nIt should print out the following:\r\n```\r\n'predict_proba' in dir(model): True\r\nAttempting to call 'predict_proba' raised an AttributeError: predict_proba is not available when probability=False\r\n```\r\n\r\nMethods decorated with `available_if` and whose conditions raise an `AttributeError` appears in the list returned by `dir(obj)`.\r\n\r\nTraceback when you show the error itself (comes from the check in `available_if`.\r\n```\r\nTraceback (most recent call last):\r\n  File \"\/Users\/kmcgrady\/Projects\/test-scripts\/xg_example.py\", line 48, in <module>\r\n    prob_predictions = model.predict_proba(X[:2])\r\n  File \"\/Users\/kmcgrady\/.local\/share\/virtualenvs\/test-scripts-fLg6VU9n\/lib\/python3.10\/site-packages\/sklearn\/utils\/_available_if.py\", line 31, in __get__\r\n    if not self.check(obj):\r\n  File \"\/Users\/kmcgrady\/.local\/share\/virtualenvs\/test-scripts-fLg6VU9n\/lib\/python3.10\/site-packages\/sklearn\/svm\/_base.py\", line 827, in _check_proba\r\n    raise AttributeError(\r\nAttributeError: predict_proba is not available when probability=False\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.10 (main, May 13 2023, 16:09:51) [Clang 14.0.3 (clang-1403.0.22.14.1)]\r\nexecutable: \/Users\/kmcgrady\/.local\/share\/virtualenvs\/test-scripts-fLg6VU9n\/bin\/python\r\n   machine: macOS-14.3.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.3.2\r\n          pip: 23.3.1\r\n   setuptools: 68.2.2\r\n        numpy: 1.26.4\r\n        scipy: 1.11.4\r\n       Cython: None\r\n       pandas: 2.2.1\r\n   matplotlib: 3.8.2\r\n       joblib: 1.3.2\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 10\r\n         prefix: libomp\r\n       filepath: \/Users\/kmcgrady\/.local\/share\/virtualenvs\/test-scripts-fLg6VU9n\/lib\/python3.10\/site-packages\/sklearn\/.dylibs\/libomp.dylib\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 10\r\n         prefix: libopenblas\r\n       filepath: \/Users\/kmcgrady\/.local\/share\/virtualenvs\/test-scripts-fLg6VU9n\/lib\/python3.10\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: armv8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 10\r\n         prefix: libopenblas\r\n       filepath: \/Users\/kmcgrady\/.local\/share\/virtualenvs\/test-scripts-fLg6VU9n\/lib\/python3.10\/site-packages\/scipy\/.dylibs\/libopenblas.0.dylib\r\n        version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n   architecture: armv8\r\n```\r\n","labels":["Bug"],"created_at":"2024-03-01T07:06:00Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28558"},{"issue_number":56,"repository":"scikit-learn\/scikit-learn","title":"Two different versions for weighted lorenz curve calculation in the examples","description":"### Describe the issue linked to the documentation\r\n\r\nThere are 2 definitions of (weighted) `lorenz_curve()` functions [here](https:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_tweedie_regression_insurance_claims.html) and [here](https:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_poisson_regression_non_normal_loss.html)\r\n\r\nThe difference is in the X coordinates that these functions returns. Both return X coordinates between 0 and 1, but the first example returns **equally spaced** X coordinates:\r\n```python\r\ncumulated_samples = np.linspace(0, 1, len(cumulated_claim_amount))\r\n```\r\nand the second example return **un-equally spaced** X coordinates (spaced using the samples weights):\r\n```python\r\ncumulated_exposure = np.cumsum(ranked_exposure)\r\ncumulated_exposure \/= cumulated_exposure[-1]\r\n```\r\n\r\n### Suggest a potential alternative\/fix\r\n\r\n_No response_","labels":["Bug","Documentation"],"created_at":"2024-02-26T06:29:54Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28534"},{"issue_number":57,"repository":"scikit-learn\/scikit-learn","title":"Improve \"polars\" integration (error, warning & linting examples)","description":"### Describe the workflow you want to enable\r\n\r\nusing polars data (DataFrame, Series) is already supported in many places which is awesome, thank you!!\r\n\r\nBut in many places there are still\r\n- errors \/ crashes -> required conversion to numpy\/pandas\r\n- warnings -> requires conversion to numpy\/pandas\r\n- linting\/type problems -> requires updates to typing signalture?\r\n\r\n# Examples\r\n\r\n## Code\r\n\r\n```python\r\nimport pandas as pd\r\nimport polars as pl\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.inspection import permutation_importance\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.model_selection import cross_val_score, cross_validate\r\n\r\nX, y = make_classification()\r\n\r\n# X = pd.DataFrame(X)\r\n# y = pd.Series(y)\r\n\r\n# X = pl.DataFrame(X)\r\n# y = pl.Series(y)\r\n\r\nclf = LogisticRegression()\r\n\r\nclf.fit(\r\n    X=X,\r\n    y=y,\r\n)\r\n\r\nclf.score(\r\n    X=X,  # Lint\/Type Problem\r\n    y=y,\r\n)\r\n\r\ncross_val_score(\r\n    estimator=clf,\r\n    X=X,  # Lint\/Type Problem\r\n    y=y,  # ERROR with polars\r\n)\r\n\r\ncross_validate(\r\n    estimator=clf,\r\n    X=X,  # Lint\/Type Problem\r\n    y=y,  # ERROR with polars\r\n)\r\n\r\npermutation_importance(\r\n    estimator=clf,\r\n    X=X,  # WARNING with polars + Lint\/Type Problem\r\n    y=y,\r\n)\r\n\r\nclf.predict(X)\r\n```\r\n\r\n## Errors \/ crashes using polars\r\n\r\nBoth `cross_val_score` and `cross_validate` crash using polars Series with message:\r\n- `TypeError: cannot use `__getitem__` on Series of dtype Int32 with argument (array([18, 21, 22, 23, 24, 25,...`\r\n\r\n### Temporary solution\r\n\r\nuse\r\n- `to_numpy()` -> works with numpy array\r\n- `to_pandas()` -> works with pandas Series\r\n\r\n## Warnings\r\n\r\n`permutation_importance` creates \"UserWarnings\" using polars DataFrame with message:\r\n- `UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names`\r\n\r\n### Temporary Solution\r\n\r\nuse\r\n- `to_pandas()` -> works with pandas DataFrame\r\n\r\n## Linting \/ Type problems\r\n\r\nnumpy arrays and pandas DataFrame have \"full support\" while polars looks a little sad \ud83d\ude06 \r\n\r\n![image](https:\/\/github.com\/scikit-learn\/scikit-learn\/assets\/25177421\/b0aa54c2-52e0-4d4c-8c7c-448a9fbb978e)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Describe your proposed solution\r\n\r\nwould love to see further support for polars DataFrame \/ Series types.\r\n\r\nNot sure why some things error, because I thought polars provides all the required apis.\r\n\r\nAny way to assist \/ help?\r\n\r\n### Describe alternatives you've considered, if relevant\r\n\r\n_No response_\r\n\r\n### Additional context\r\n\r\n_No response_","labels":["Bug"],"created_at":"2024-02-20T16:22:18Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28488"},{"issue_number":58,"repository":"scikit-learn\/scikit-learn","title":"Crash in T-SNE","description":"### Describe the bug\r\n\r\nI got a crash using the (external) [hdbscan package](https:\/\/github.com\/scikit-learn-contrib\/hdbscan) in some special situation. I [debugged it](https:\/\/github.com\/scikit-learn-contrib\/hdbscan\/issues\/623) and found out that it happens in the scikit-learn package, specifically its T-SNE implementation. The hdbscan maintainer ([Leland McInnes](https:\/\/github.com\/lmcinnes)!) suggested to report it here.\r\n\r\nIs this something that should be guarded for in scikit-learn?\r\n\r\n### Steps\/Code to Reproduce\r\n\r\nThe simplest way to reproduce it (using the hdbscan package) is:\r\n```py\r\nimport numpy as np\r\nimport hdbscan\r\nmodel = hdbscan.HDBSCAN(gen_min_span_tree=True)\r\ndata = np.zeros((91, 3))\r\nclustering = model.fit(data)\r\nclustering.minimum_spanning_tree_.plot()\r\n```\r\nNote that it also happens when only a relative small proportion of points are equal (but only sometimes?), this is just the easiest way to show it. By default some warnings are displayed:\r\n> ...\\sklearn\\decomposition\\_pca.py:685: RuntimeWarning: invalid value encountered in divide\r\n>   self.explained_variance_ratio_ = self.explained_variance_ \/ total_var\r\n> ...\\sklearn\\manifold\\_t_sne.py:1002: RuntimeWarning: invalid value encountered in divide\r\n>   X_embedded = X_embedded \/ np.std(X_embedded[:, 0]) * 1e-4 \r\n\r\nIn the end it appears to be a problem in `sklearn.manifold._t_sne._barnes_hut_tsne.gradient()`, not (always?) being able to handle `nan` values. For example, this reproduces the crash:\r\n```py\r\nimport numpy as np\r\nfrom sklearn.manifold._t_sne import _barnes_hut_tsne\r\nneighbors = np.array([1, 2, 0, 2, 0, 1], dtype='int64')\r\nval_P = np.full_like(neighbors, 2 \/ 45, dtype='float32')\r\npos_output = np.full((3, 2), np.nan, dtype='float32')\r\nforces = np.zeros_like(pos_output)\r\nindptr = np.arange(7, step=2, dtype='int64')\r\n_barnes_hut_tsne.gradient(val_P, pos_output, neighbors, indptr, forces, 0.5, 2, 11)\r\n```\r\nOne layer deeper, the crash occurs inside `sklearn.neighbors._quad_tree._QuadTree.build_tree()`, as follows:\r\n```py\r\nimport numpy as np\r\nfrom sklearn.neighbors._quad_tree import _QuadTree\r\nqt = _QuadTree(2, 11)\r\nX = np.full((3, 2), np.nan, dtype='float32')\r\nqt.build_tree(X)\r\n```\r\nThe output of this (due to `verbose=11`) up to the crash is:\r\n> [QuadTree] bounding box axis 0 : [nan, nan]\r\n> [QuadTree] bounding box axis 1 : [nan, nan]\r\n> [QuadTree] Inserting depth 0\r\n> [QuadTree] inserted point 0 in cell 0\r\n> [QuadTree] Inserting depth 0\r\n> [QuadTree] inserted point 0 in new child 1\r\n> [QuadTree] Inserting depth 0\r\n> [QuadTree] Inserting depth 1\r\n> ...\r\n> [QuadTree] Inserting depth 6271\r\n> [QuadTree] inserted point 0 in new child 6272\r\n> [QuadTree] Inserting depth 6271\r\n> [QuadTree] Inserting depth 6272\r\n\r\nI didn't dig into the QuadTree code.\r\n\r\n### Expected Results\r\n\r\nI would expect anything but Python crashing.\r\n\r\n### Actual Results\r\n\r\nPython crashed: its console window just went away silently (on Windows).\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n\u00a0 \u00a0 python: 3.12.1 (tags\/v3.12.1:2305ca5, Dec \u00a07 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)]\r\nexecutable: C:\\Users\\Public\\Software\\Python\\python.exe\r\n\u00a0 \u00a0machine: Windows-11-10.0.22631-SP0\r\n\r\nPython dependencies:\r\n\u00a0 \u00a0 \u00a0 sklearn: 1.4.0\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pip: 23.3.2\r\n\u00a0 \u00a0setuptools: 69.0.3\r\n\u00a0 \u00a0 \u00a0 \u00a0 numpy: 1.26.3\r\n\u00a0 \u00a0 \u00a0 \u00a0 scipy: 1.12.0\r\n\u00a0 \u00a0 \u00a0 \u00a0Cython: 3.0.8\r\n\u00a0 \u00a0 \u00a0 \u00a0pandas: 2.2.0\r\n\u00a0 \u00a0matplotlib: 3.8.2\r\n\u00a0 \u00a0 \u00a0 \u00a0joblib: 1.3.2\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n\u00a0 \u00a0 \u00a0 \u00a0user_api: blas\r\n\u00a0 \u00a0internal_api: openblas\r\n\u00a0 \u00a0 num_threads: 16\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0prefix: libopenblas\r\n\u00a0 \u00a0 \u00a0 \u00a0filepath: C:\\Users\\Public\\Software\\Python\\Lib\\site-packages\\numpy.libs\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll\r\n\u00a0 \u00a0 \u00a0 \u00a0 version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n\u00a0 \u00a0architecture: Cooperlake\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0user_api: openmp\r\n\u00a0 \u00a0internal_api: openmp\r\n\u00a0 \u00a0 num_threads: 16\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0prefix: vcomp\r\n\u00a0 \u00a0 \u00a0 \u00a0filepath: C:\\Users\\Public\\Software\\Python\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n\u00a0 \u00a0 \u00a0 \u00a0 version: None\r\n\r\n\u00a0 \u00a0 \u00a0 \u00a0user_api: blas\r\n\u00a0 \u00a0internal_api: openblas\r\n\u00a0 \u00a0 num_threads: 16\r\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0prefix: libopenblas\r\n\u00a0 \u00a0 \u00a0 \u00a0filepath: C:\\Users\\Public\\Software\\Python\\Lib\\site-packages\\scipy.libs\\libopenblas_v0.3.20-571-g3dec11c6-gcc_10_3_0-c2315440d6b6cef5037bad648efc8c59.dll\r\n\u00a0 \u00a0 \u00a0 \u00a0 version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n\u00a0 \u00a0architecture: Cooperlake\r\n```\r\n","labels":["Bug","help wanted","module:manifold","cython"],"created_at":"2024-02-05T23:03:32Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28368"},{"issue_number":59,"repository":"scikit-learn\/scikit-learn","title":"Special characters (e.g. &) are not escaped by sklearn.tree.export_graphviz","description":"### Describe the bug\n\nExporting a decision tree where the `feature_names` or `class_names` contain special characters (particularly `&<>`) results in invalid graphviz output, as those characters have specific meanings to graphviz. Escaping to `&amp;`, `&lt;` and `&gt;` results in correct output. This can of course be done by the user but it's something I think scikit-learn should handle internally.\n\n### Steps\/Code to Reproduce\n\n```python\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn import tree\r\niris = load_iris()\r\nclf = tree.DecisionTreeClassifier()\r\nclf = clf.fit(iris.data, iris.target)\r\n\r\ntarget_names = [\"setosa & 123\", \"versicolor\", \"virginca\"]\r\n# target_names = [\"setosa &amp; 123\", \"versicolor\", \"virginca\"]  # This one works\r\n\r\ntree.export_graphviz(\r\n\t\tclf,\r\n\t\tout_file=\"tree.dot\",\r\n\t\tfeature_names=iris.feature_names,\r\n\t\tclass_names=target_names,\r\n\t\tfilled=True,\r\n\t\tspecial_characters=True,\r\n\t\t)\r\n\r\n```\r\n\r\nThen run graphviz\r\n\r\n```bash\r\ndot tree.dot -Tsvg -o tree.svg \r\n```\n\n### Expected Results\n\nGraphviz successfully converts to SVG without error.\n\n### Actual Results\n\n```\r\nError: not well-formed (invalid token) in line 1 \r\n... <br\/>class = setosa & 123 ...\r\nin label of node 0\r\nError: not well-formed (invalid token) in line 1 \r\n... <br\/>class = setosa & 123 ...\r\nin label of node 1\r\n```\r\n\r\nAlthough SVG output is written to disk it is not correct.\r\n![image](https:\/\/github.com\/scikit-learn\/scikit-learn\/assets\/8050853\/8aa517f5-9764-4d9a-93f9-d20864f6085c)\r\n\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.8.10 (default, Nov 22 2023, 10:22:35)  [GCC 9.4.0]\r\nexecutable: \/home\/domdf\/Python\/01 GitHub Repos\/13 GunShotMatch\/gunshotmatch-cli\/venv\/bin\/python3\r\n   machine: Linux-5.15.0-92-generic-x86_64-with-glibc2.29\r\n\r\nPython dependencies:\r\n      sklearn: 1.3.2\r\n          pip: 23.3.2\r\n   setuptools: 69.0.3\r\n        numpy: 1.24.4\r\n        scipy: 1.10.1\r\n       Cython: None\r\n       pandas: 2.0.3\r\n   matplotlib: 3.7.4\r\n       joblib: 1.3.2\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 16\r\n         prefix: libgomp\r\n       filepath: \/home\/domdf\/Python\/01 GitHub Repos\/13 GunShotMatch\/gunshotmatch-cli\/venv\/lib\/python3.8\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 16\r\n         prefix: libopenblas\r\n       filepath: \/home\/domdf\/Python\/01 GitHub Repos\/13 GunShotMatch\/gunshotmatch-cli\/venv\/lib\/python3.8\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so\r\n        version: 0.3.21\r\nthreading_layer: pthreads\r\n   architecture: Zen\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 16\r\n         prefix: libopenblas\r\n       filepath: \/home\/domdf\/Python\/01 GitHub Repos\/13 GunShotMatch\/gunshotmatch-cli\/venv\/lib\/python3.8\/site-packages\/scipy.libs\/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Zen\n```\n","labels":["Bug"],"created_at":"2024-02-01T10:21:36Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28339"},{"issue_number":60,"repository":"scikit-learn\/scikit-learn","title":"Tests failing when cuda installed but no GPU is present","description":"after doing `conda install pytorch cupy`, my tests fail with:\r\n\r\n```\r\nFAILED sklearn\/metrics\/tests\/test_common.py::test_array_api_compliance[\r\naccuracy_score-check_array_api_binary_classification_metric-cupy-None-None] \r\n- cupy_backends.cuda.api.runtime.CUDARuntimeError: cudaErrorNoDevice: \r\n- no CUDA-capable device is detected\r\n```\r\n\r\nI don't think tests should ever fail for this, should they?\r\n\r\ncc @ogrisel @betatim ","labels":["Bug"],"created_at":"2024-01-26T15:37:06Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28280"},{"issue_number":61,"repository":"scikit-learn\/scikit-learn","title":"Exception in LogisticRegressionCV","description":"### Describe the bug\r\n\r\nThe code provided below raises ValueError. I guess that the problem is that minor classes may not be included in **train** or **val** sets for some folds during internal cross-validation, even with stratified split. This produces errors with some metrics other than default (accuracy).\r\n\r\nOne solution may be setting log-proba to -inf for classes not present in the train set, as well as providing label argument. How can I fix this in the most simple way?\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```\r\nimport numpy as np\r\nfrom sklearn.linear_model import LogisticRegressionCV\r\nX = np.zeros((10, 1))\r\ny = [1, 1, 1, 1, 1, 2, 2, 2, 2, 3]\r\nlogreg = LogisticRegressionCV(cv=5, scoring='neg_log_loss')\r\nlogreg.fit(X, y)\r\n```\r\n\r\n### Expected Results\r\n\r\nNo exception thrown\r\n\r\n### Actual Results\r\n\r\nValueError: y_true and y_pred contain different number of classes 2, 3. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [0 1]\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.8 (main, Nov  2 2023, 15:57:09) [GCC 9.4.0]\r\nexecutable: \/data\/osedukhin\/tabular-models\/venv\/bin\/python\r\n   machine: Linux-5.4.0-123-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.4.0\r\n          pip: 22.2.2\r\n   setuptools: 63.2.0\r\n        numpy: 1.26.2\r\n        scipy: 1.11.3\r\n       Cython: None\r\n       pandas: 2.1.3\r\n   matplotlib: 3.8.1\r\n       joblib: 1.3.2\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 32\r\n         prefix: libopenblas\r\n       filepath: \/data\/osedukhin\/tabular-models\/venv\/lib\/python3.10\/site-packages\/numpy.libs\/libopenblas64_p-r0-0cf96a72.3.23.dev.so\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: SkylakeX\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 32\r\n         prefix: libgomp\r\n       filepath: \/data\/osedukhin\/tabular-models\/venv\/lib\/python3.10\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 32\r\n         prefix: libopenblas\r\n       filepath: \/data\/osedukhin\/tabular-models\/venv\/lib\/python3.10\/site-packages\/scipy.libs\/libopenblasp-r0-23e5df77.3.21.dev.so\r\n        version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n   architecture: SkylakeX\r\n```\r\n","labels":["Bug"],"created_at":"2024-01-18T18:36:29Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28178"},{"issue_number":62,"repository":"scikit-learn\/scikit-learn","title":"Inconsistency in `DecisionTreeClassifier` Threshold Behavior","description":"### Describe the bug\n\nI've encountered an unexpected behavior in `DecisionTreeClassifier` when using a decision stump (a tree with one root node and two leaf children). My assumption is based on the standard decision tree logic where a feature value `x` is classified to the left child if `x <= threshold`. Therefore, I expect the following assertion to always be true:\r\n```python\r\nassert clf.apply([[clf.tree_.threshold[0]]]) == 1\r\n```\r\n\r\nThis assertion is meant to test that a feature value equal to the root node's threshold is classified to the left child node, in accordance with the `x <= threshold` rule. However, I have observed that in approximately 25% of the cases, the data point is unexpectedly classified to the right child node instead of the left.\r\n\r\nThe issue persists regardless of whether the `threshold[0]` is explicitly converted to `float32` or not. Given scikit-learn's documentation stating that\r\n> All decision trees use `np.float32` arrays internally. If training data is not in this format, a copy of the dataset will be made\r\n\r\nthis behavior is puzzling. Precise thresholding is crucial for my application. Thank you for your time and effort in maintaining this important library and for any insights you can provide regarding this issue.\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nfrom sklearn.tree import DecisionTreeClassifier\r\n\r\ncorrect, wrong = [], []\r\nn_trials = 500\r\nfor seed in range(n_trials):\r\n    rand = np.random.RandomState(seed)\r\n    X = rand.normal(size=(100, 1))\r\n    y = rand.randint(0, 2, size=X.shape[0])\r\n    clf = DecisionTreeClassifier(max_depth=1, random_state=0).fit(X, y)\r\n    \r\n    thres = clf.tree_.threshold[0]\r\n    # thres = np.float32(thres) # gives the same correct-wrong distribution\r\n    if clf.apply([[thres]]) == 1:\r\n        correct.append(thres)\r\n    else:\r\n        wrong.append(thres)\r\n\r\n    assert clf.apply([[np.nextafter(np.float32(thres), np.float32(-np.inf))]]) == 1 # this is true though\r\nprint(f'{len(correct) \/ n_trials:%}') # 75%\r\nprint(f'{len(wrong) \/ n_trials:%}') # 25%\r\n```\n\n### Expected Results\n\n```\r\n100.0000%\r\n0.0000%\r\n```\n\n### Actual Results\n\n```\r\n75.0000%\r\n25.0000%\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\r\nexecutable: \/usr\/bin\/python3\r\n   machine: Linux-6.1.58+-x86_64-with-glibc2.35\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.2\r\n          pip: 23.1.2\r\n   setuptools: 67.7.2\r\n        numpy: 1.23.5\r\n        scipy: 1.11.4\r\n       Cython: 3.0.8\r\n       pandas: 1.5.3\r\n   matplotlib: 3.7.1\r\n       joblib: 1.3.2\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 2\r\n         prefix: libopenblas\r\n       filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/numpy.libs\/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 2\r\n         prefix: libgomp\r\n       filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 2\r\n         prefix: libopenblas\r\n       filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/scipy.libs\/libopenblasp-r0-23e5df77.3.21.dev.so\r\n        version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\n```\n","labels":["Bug"],"created_at":"2024-01-18T18:17:14Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/28175"},{"issue_number":63,"repository":"scikit-learn\/scikit-learn","title":"Bug in utils\/multiclass.py\/_ovr_decision_function","description":"### Describe the workflow you want to enable\n\nDear scikit learn developpers,\r\n\r\nI think the implementation of `_ovr_decision_function` in utils\r\n\/multiclass.py doesn't work properly when the parameter `confidences` is probability. While as the documentation suggests, it can be a probability .\r\n\r\n```\r\nconfidences : array-like of shape (n_samples, n_classifiers)\r\n        Decision functions or predicted probabilities for positive class\r\n        for each binary classifier.\r\n```\r\n\r\nThe problem is the following two lines of codes\r\n\r\n```\r\nsum_of_confidences[:, i] -= confidences[:, k]\r\nsum_of_confidences[:, j] += confidences[:, k]\r\n```\r\n\r\nIn this context, there is a binary classifier for class `i` vs `j`. And `j` is the positive class. \r\n\r\nIf `confidences` is \"decision_function\", then it works. Because if \"decision funtion\" is negative, it means the classifier thinks the negatve class `i` is more possible. And the `-=` will increase the `sum_of_confidences` of `i`, and decrease the `sum_of_confidences` of `j`.\r\n\r\nHowever, if  `confidences` is \"probability\", it doesn't work. Because probability is always greater than zero. So the `sum_of_confidences` of `i` will always decrease, even when `i` is more likely to happend (prob of `j` < 0.5).\r\n\r\n\r\n\n\n### Describe your proposed solution\n\n\"decision_function\" is centered at 0, while \"probability\" is centerd at 0.5. These two cases should be handled seperately.\n\n### Describe alternatives you've considered, if relevant\n\n_No response_\n\n### Additional context\n\n_No response_","labels":["Bug"],"created_at":"2023-12-17T13:50:59Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27973"},{"issue_number":64,"repository":"scikit-learn\/scikit-learn","title":"Pandas Copy-on-Write mode should be enabled in all tests","description":"### Describe the bug\r\n\r\nPandas COW will be enabled by default in version 3.0.\r\nFor example, today I just found that `TargetEncoder` doesn't work properly with it enabled.\r\nThere are probably many other examples that could be uncovered by testing.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport pandas as pd\r\nfrom sklearn.preprocessing import TargetEncoder\r\npd.options.mode.copy_on_write = True\r\n\r\ndf = pd.DataFrame({\r\n    \"x\": [\"a\", \"b\", \"c\", \"c\"],\r\n    \"y\": [4., 5., 6., 7.]\r\n})\r\nt = TargetEncoder(target_type=\"continuous\")\r\nt.fit(df[[\"x\"]], df[\"y\"])\r\n```\r\n\r\n### Expected Results\r\n\r\nNo error.\r\n\r\n### Actual Results\r\n```\r\nValueError                                Traceback (most recent call last)\r\nCell In[2], line 10\r\n      5 df = pd.DataFrame({\r\n      6     \"x\": [\"a\", \"b\", \"c\", \"c\"],\r\n      7     \"y\": [4., 5., 6., 7.]\r\n      8 })\r\n      9 t = TargetEncoder(target_type=\"continuous\")\r\n---> 10 t.fit(df[[\"x\"]], df[\"y\"])\r\n\r\nFile ~\/.conda\/envs\/jhop311\/lib\/python3.11\/site-packages\/sklearn\/base.py:1152, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\r\n   1145     estimator._validate_params()\r\n   1147 with config_context(\r\n   1148     skip_parameter_validation=(\r\n   1149         prefer_skip_nested_validation or global_skip_validation\r\n   1150     )\r\n   1151 ):\r\n-> 1152     return fit_method(estimator, *args, **kwargs)\r\n\r\nFile ~\/.conda\/envs\/jhop311\/lib\/python3.11\/site-packages\/sklearn\/preprocessing\/_target_encoder.py:203, in TargetEncoder.fit(self, X, y)\r\n    186 @_fit_context(prefer_skip_nested_validation=True)\r\n    187 def fit(self, X, y):\r\n    188     \"\"\"Fit the :class:`TargetEncoder` to X and y.\r\n    189 \r\n    190     Parameters\r\n   (...)\r\n    201         Fitted encoder.\r\n    202     \"\"\"\r\n--> 203     self._fit_encodings_all(X, y)\r\n    204     return self\r\n\r\nFile ~\/.conda\/envs\/jhop311\/lib\/python3.11\/site-packages\/sklearn\/preprocessing\/_target_encoder.py:332, in TargetEncoder._fit_encodings_all(self, X, y)\r\n    330 if self.smooth == \"auto\":\r\n    331     y_variance = np.var(y)\r\n--> 332     self.encodings_ = _fit_encoding_fast_auto_smooth(\r\n    333         X_ordinal, y, n_categories, self.target_mean_, y_variance\r\n    334     )\r\n    335 else:\r\n    336     self.encodings_ = _fit_encoding_fast(\r\n    337         X_ordinal, y, n_categories, self.smooth, self.target_mean_\r\n    338     )\r\n\r\nFile sklearn\/preprocessing\/_target_encoder_fast.pyx:82, in sklearn.preprocessing._target_encoder_fast._fit_encoding_fast_auto_smooth()\r\n\r\nFile stringsource:660, in View.MemoryView.memoryview_cwrapper()\r\n\r\nFile stringsource:350, in View.MemoryView.memoryview.__cinit__()\r\n\r\nValueError: buffer source array is read-only\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.11.3 | packaged by conda-forge | (main, Apr  6 2023, 08:57:19) [GCC 11.3.0]\r\nexecutable: \/home\/jhopfens\/.conda\/envs\/jhop311\/bin\/python\r\n   machine: Linux-3.10.0-1160.99.1.el7.x86_64-x86_64-with-glibc2.17\r\n\r\nPython dependencies:\r\n      sklearn: 1.3.2\r\n          pip: 23.0.1\r\n   setuptools: 67.6.1\r\n        numpy: 1.25.2\r\n        scipy: 1.11.2\r\n       Cython: 3.0.0\r\n       pandas: 2.1.0\r\n   matplotlib: 3.7.2\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/jhopfens\/.conda\/envs\/jhop311\/lib\/python3.11\/site-packages\/numpy.libs\/libopenblas64_p-r0-5007b62f.3.23.dev.so\r\n        version: 0.3.23.dev\r\nthreading_layer: pthreads\r\n   architecture: SkylakeX\r\n    num_threads: 64\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/home\/jhopfens\/.conda\/envs\/jhop311\/lib\/python3.11\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 128\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/jhopfens\/.conda\/envs\/jhop311\/lib\/python3.11\/site-packages\/scipy.libs\/libopenblasp-r0-23e5df77.3.21.dev.so\r\n        version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n   architecture: SkylakeX\r\n    num_threads: 64\r\n```\r\n","labels":["Bug"],"created_at":"2023-11-30T16:54:28Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27879"},{"issue_number":65,"repository":"scikit-learn\/scikit-learn","title":"HalvingGridSearchCV should not care about parameter-grid layout but apparently does","description":"### Describe the bug\r\n\r\nI have two parameter-grid layouts, both specifying the exact same set of configurations. The difference is that the choices are listed in a different order, e.g., [False, True] versus [True, False]. My understanding is that this should not matter given that HalvingGridSearchCV tries all configurations on the first step. However, I get different results when using first grid versus the second.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.experimental import enable_halving_search_cv\r\nfrom sklearn.model_selection import HalvingGridSearchCV\r\n\r\n#three grids with identical configuration space\r\n\r\nsvc_grid_1 = dict(C=[1,2,3],\r\n                gamma=['auto', 'scale'],\r\n                shrinking=(True, False),\r\n                kernel=['sigmoid', 'linear', 'poly', 'rbf'],\r\n                max_iter = [5000, 10000, -1]\r\n                )\r\n\r\nsvc_grid_2 = dict(C=[1,2,3],\r\n                gamma=['auto', 'scale'],\r\n                shrinking=(True, False),\r\n                kernel=['rbf', 'poly', 'linear',  'sigmoid'],  #reordered choices\r\n                max_iter = [5000, 10000, -1]\r\n                )\r\n\r\nsvc_grid_3 = dict(kernel=['sigmoid', 'linear', 'poly', 'rbf'],  #reordered keys\r\n                  C=[1,2,3],\r\n                  shrinking=(True, False),\r\n                  max_iter = [5000, 10000, -1],\r\n                  gamma=['auto', 'scale'],\r\n                )\r\n\r\n###How many different combinations?\r\n\r\nfrom sklearn.model_selection import ParameterGrid\r\nparam_grid_1 = ParameterGrid(svc_grid_1)  #a list of dictionaries, one for each combo\r\nprint(f'{len(param_grid_1)=}')  #144\r\nparam_grid_2 = ParameterGrid(svc_grid_2)  #a list of dictionaries, one for each combo\r\nprint(f'{len(param_grid_1)=}')  #144\r\nparam_grid_3 = ParameterGrid(svc_grid_3)  #a list of dictionaries, one for each combo\r\nprint(f'{len(param_grid_1)=}')  #144\r\n\r\nall([d in param_grid_2 and d in param_grid_3 for d in param_grid_1])  #True\r\n\r\n#random data\r\n\r\nnp.random.seed(0)\r\nn_samples, n_features = 1313, 6\r\nx_train = np.random.rand(n_samples, n_features)\r\ny_train = np.random.randint(0, 2, n_samples)  # binary target with values in {0, 1}\r\n\r\n#Try Grid 1\r\n\r\nsvc_model = SVC(probability=True, random_state=1)  #base model\r\nmin_res = 30\r\n\r\n#do the grid search\r\nhalving_cv = HalvingGridSearchCV(\r\n    svc_model, svc_grid_1,  \r\n    scoring=\"roc_auc\",  \r\n    n_jobs=1,  \r\n    min_resources=min_res,\r\n    factor=2,  \r\n    cv=5, random_state=1234,\r\n    refit=True,  \r\n)\r\n\r\ngrid_result_1 = halving_cv.fit(x_train, y_train)\r\ngrid_result_1.best_params_ \r\n#{'C': 1,\r\n# 'gamma': 'scale',\r\n# 'kernel': 'sigmoid',\r\n# 'max_iter': -1,\r\n# 'shrinking': True}\r\n\r\n#Try grid 2\r\n\r\nsvc_model = SVC(probability=True, random_state=1)  #base model\r\n\r\nmin_res = 30\r\n\r\n#do the grid search\r\nhalving_cv = HalvingGridSearchCV(\r\n    svc_model, svc_grid_2,  \r\n    scoring=\"roc_auc\", \r\n    n_jobs=1,  \r\n    min_resources=min_res,\r\n    factor=2, \r\n    cv=5, random_state=1234,\r\n    refit=True, \r\n)\r\n\r\ngrid_result_2 = halving_cv.fit(x_train, y_train)\r\ngrid_result_2.best_params_ \r\n#{'C': 1,\r\n #'gamma': 'scale',\r\n #'kernel': 'sigmoid',\r\n #'max_iter': 5000,\r\n #'shrinking': True}\r\n\r\nset(grid_result_1.best_params_.values()) == set(grid_result_2.best_params_.values())  #False\r\n```\r\n### Expected Results\r\n\r\nbest_params_  would be same for both grids\r\n\r\n### Actual Results\r\n\r\nThey are different.\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\r\nexecutable: \/usr\/bin\/python3\r\n   machine: Linux-5.15.120+-x86_64-with-glibc2.35\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.2\r\n          pip: 23.1.2\r\n   setuptools: 67.7.2\r\n        numpy: 1.23.5\r\n        scipy: 1.11.3\r\n       Cython: 3.0.4\r\n       pandas: 1.5.3\r\n   matplotlib: 3.7.1\r\n       joblib: 1.3.2\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/numpy.libs\/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libgomp\r\n       filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: \/usr\/local\/lib\/python3.10\/dist-packages\/scipy.libs\/libopenblasp-r0-23e5df77.3.21.dev.so\r\n        version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n```\r\n","labels":["Bug","Needs Investigation"],"created_at":"2023-11-07T17:36:38Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27740"},{"issue_number":66,"repository":"scikit-learn\/scikit-learn","title":"sklearn PCA rotates a single vector","description":"### Describe the bug\r\n\r\nThe issue we recently discovered is that sklearn PCA rotates the input when only a single variable is fed into the model.  \r\n\r\nI am aware there are infinite rotations when there is a single vector fed into the model, however, the output from PCA should intuitively make sense. So I suggest hard coding this scenario (which I guess already done in R)\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom sklearn.decomposition import PCA\r\n\r\nx = np.array([1,2,3,4,5,6,7,8,9,10.]).reshape(-1,1).astype('float64')\r\n\r\np = PCA().fit_transform(x)\r\np\r\n```\r\n### output:\r\n```\r\narray([[ 4.5],\r\n       [ 3.5],\r\n       [ 2.5],\r\n       [ 1.5],\r\n       [ 0.5],\r\n       [-0.5],\r\n       [-1.5],\r\n       [-2.5],\r\n       [-3.5],\r\n       [-4.5]])\r\n```\r\n\r\nas we see here the value 4.5 is corresponding to the smallest value in the set (1) and the value -4.5 is corresponding to the highest value in the input set (10).\r\n\r\n### Expected Results:\r\n```\r\narray([[-4.5],\r\n       [-3.5],\r\n       [-2.5],\r\n       [-1.5],\r\n       [-0.5],\r\n       [ 0.5],\r\n       [ 1.5],\r\n       [ 2.5],\r\n       [ 3.5],\r\n       [ 4.5]])\r\n```\r\n\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```\r\nfrom sklearn.decomposition import PCA\r\n\r\nx= np.array([1,2,3,4,5,6,7,8,9,10.]).reshape(-1,1).astype('float64')\r\n\r\np = PCA().fit_transform(x)\r\np\r\n```\r\n\r\n### Expected Results\r\n\r\n```\r\narray([[-4.5],\r\n       [-3.5],\r\n       [-2.5],\r\n       [-1.5],\r\n       [-0.5],\r\n       [ 0.5],\r\n       [ 1.5],\r\n       [ 2.5],\r\n       [ 3.5],\r\n       [ 4.5]])\r\n```\r\n\r\n### Actual Results\r\n\r\n```\r\narray([[ 4.5],\r\n       [ 3.5],\r\n       [ 2.5],\r\n       [ 1.5],\r\n       [ 0.5],\r\n       [-0.5],\r\n       [-1.5],\r\n       [-2.5],\r\n       [-3.5],\r\n       [-4.5]])\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.11.4 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 13:47:18) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: e:\\Users\\***\\miniconda3\\python.exe\r\n   machine: Windows-10-10.0.22621-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.3.0\r\n          pip: 23.2.1\r\n   setuptools: 68.0.0\r\n        numpy: 1.24.3\r\n        scipy: 1.11.1\r\n       Cython: None\r\n       pandas: 2.0.3\r\n   matplotlib: 3.7.1\r\n       joblib: 1.2.0\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       filepath: E:\\Users\\***\\miniconda3\\Library\\bin\\mkl_rt.2.dll\r\n         prefix: mkl_rt\r\n       user_api: blas\r\n   internal_api: mkl\r\n        version: 2023.1-Product\r\n    num_threads: 8\r\nthreading_layer: intel\r\n\r\n       filepath: E:\\Users\\***\\miniconda3\\vcomp140.dll\r\n         prefix: vcomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 16\r\n\r\n       filepath: E:\\Users\\***\\miniconda3\\Library\\bin\\libiomp5md.dll\r\n         prefix: libiomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 16\r\n```\r\n","labels":["Bug"],"created_at":"2023-10-19T09:09:38Z","comments":9,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27620"},{"issue_number":67,"repository":"scikit-learn\/scikit-learn","title":"Non-metric MDS gives nonsense results on the Digits dataset","description":"### Describe the bug\r\n\r\nI am running metric and nonmetric MDS on the Digits dataset, and obtain nonsense results with `metric=False`. At the same time, my understanding is that non-metric MDS is more flexible and should not yield worse results compared to metric MDS. Is my understanding wrong, or does it suggest some problems with the non-metric MDS implementation?\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```Python\r\n%matplotlib notebook\r\n\r\nimport pylab as plt\r\nimport numpy as np\r\nfrom sklearn.datasets import load_digits\r\nfrom sklearn.manifold import MDS\r\n\r\ndigits = load_digits()\r\nX, y = digits.data, digits.target\r\n\r\nZ1 = MDS(n_components=2, n_init=1, random_state=42).fit_transform(X)\r\nZ2 = MDS(n_components=2, n_init=1, random_state=42, metric=False).fit_transform(X)\r\n\r\ntitles = ['Metric MDS', 'Non-metric MDS']\r\n\r\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(6, 3), layout='constrained')\r\n\r\nfor i,Z in enumerate([Z1, Z2]):\r\n    ax = axs.ravel()[i]\r\n    \r\n    ax.set_aspect('equal', 'datalim')\r\n    \r\n    ind = np.random.permutation(X.shape[0])\r\n    ax.scatter(*Z[ind].T, s=3, color=plt.cm.Dark2(y)[ind])\r\n\r\n    ax.set_xticks([])\r\n    ax.set_yticks([])\r\n    for sp in ax.spines:\r\n        ax.spines[sp].set_visible(False)\r\n        \r\n    ax.set_title(titles[i])\r\n    \r\nfig.savefig('digits-embed-mds-nonmetric.png', dpi=300)\r\n```\r\n\r\n### Expected Results\r\n\r\nNon-metric MDS giving something sensible.\r\n\r\n### Actual Results\r\n\r\n![digits-embed-mds-nonmetric](https:\/\/github.com\/scikit-learn\/scikit-learn\/assets\/8970231\/aa6af88a-75d0-4ef2-ab6e-6482c4df5ae8)\r\n\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.7.9 (default, Aug 31 2020, 12:42:55)  [GCC 7.3.0]\r\nexecutable: \/home\/dmitry\/anaconda3\/bin\/python\r\n   machine: Linux-5.15.0-76-generic-x86_64-with-debian-bullseye-sid\r\n\r\nPython dependencies:\r\n          pip: 22.3.1\r\n   setuptools: 65.6.3\r\n      sklearn: 0.24.1\r\n        numpy: 1.21.5\r\n        scipy: 1.7.3\r\n       Cython: 0.29.32\r\n       pandas: 1.3.5\r\n   matplotlib: 3.5.2\r\n       joblib: 1.1.1\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True\r\n```\r\n","labels":["Bug","Needs Investigation"],"created_at":"2023-08-07T15:03:55Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27028"},{"issue_number":68,"repository":"scikit-learn\/scikit-learn","title":"PoissonRegressor lbfgs solver giving coefficients of 0 and Runtime Warning","description":"### Describe the bug\r\n\r\nSee the following [stack exchange](https:\/\/stats.stackexchange.com\/questions\/622085\/sklearn-poissonregressor-giving-all-coefficients-zero) post (the solution to my original issue was to use newton-cholesky solver)\r\n\r\nWhen fitting a Poisson Regression (without regularization) to some dummy data I encounter:\r\n\r\n- with lbfgs solver, a Runtime Warning, a non-zero intercept and all coefficients as zero\r\n- with newton-cholesky solver, coefficients as expected\r\n\r\nSome people on StackExchange have mentioned it is worth submitting an issue (there was a similar [one](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24752) faced with Logistic Regression).\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport statsmodels.api as sm\r\nimport statsmodels.formula.api as smf\r\nimport pandas as pd\r\nfrom sklearn.linear_model import PoissonRegressor\r\nfrom sklearn.preprocessing import (\r\n    OneHotEncoder,\r\n)\r\ndata = sm.datasets.get_rdataset('Insurance', package='MASS').data\r\n# Fit Poisson regression using formula interface\r\nformula = \"Claims ~ C(District, Treatment(1)) + C(Group, Treatment('<1l')) + C(Age, Treatment('<25')) + Holders\"\r\nmodel_smf = smf.poisson(formula=formula, data=data).fit()\r\nprint(type(model_smf))\r\nprint(model_smf.summary())\r\n# with sklearn OneHotEncoder\r\n\r\nX_train_ohe = OneHotEncoder(sparse_output=False, drop=[1, \"<1l\", \"<25\"]).fit(data[[\"District\", \"Group\", \"Age\"]])\r\nX_train_ohe = pd.DataFrame(X_train_ohe.transform(data[[\"District\", \"Group\", \"Age\"]]), columns=X_train_ohe.get_feature_names_out())\r\n\r\nX_train = pd.concat([X_train_ohe, data[[\"Holders\"]]], axis=1)\r\ny_train = data[\"Claims\"]\r\n\r\n# one-hot encode the categorical columns, and drop the baseline column\r\n# with lbfgs solver\r\n\r\nmodel_sklearn_lbfgs = PoissonRegressor(alpha=0).fit(X_train, y_train)\r\n\r\nprint(model_sklearn_lbfgs.intercept_)\r\nprint(model_sklearn_lbfgs.coef_)\r\n# with newton-cholesky solver\r\n\r\nmodel_sklearn_nc = PoissonRegressor(alpha=0, solver='newton-cholesky').fit(X_train, y_train)\r\n\r\nprint(model_sklearn_nc.intercept_)\r\nprint(model_sklearn_nc.coef_)\r\n```\r\n\r\n### Expected Results\r\n\r\nExpected intercept\/coefficients (from statsmodels and sklearn with newton-cholesky solver):\r\n```\r\n2.8456859090224444\r\n[-4.47436243e-01 -9.30629212e-01 -1.46547942e+00  1.00603354e+00\r\n  4.71328611e-01 -5.98213625e-01  5.69685530e-01  6.85301286e-01\r\n  2.22504699e+00 -1.58161172e-05]\r\n```\r\n\r\n### Actual Results\r\n\r\nResult with lbfgs:\r\n```\r\n3.896592058397603\r\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\r\n\/home\/akaash\/Downloads\/statsmodels-play\/venv\/lib\/python3.10\/site-packages\/sklearn\/linear_model\/_linear_loss.py:290: RuntimeWarning: invalid value encountered in matmul\r\n  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0]\r\nexecutable: \/home\/akaash\/Downloads\/statsmodels-play\/venv\/bin\/python\r\n   machine: Linux-5.19.0-46-generic-x86_64-with-glibc2.35\r\n\r\nPython dependencies:\r\n      sklearn: 1.3.0\r\n          pip: 23.2.1\r\n   setuptools: 68.0.0\r\n        numpy: 1.25.1\r\n        scipy: 1.11.1\r\n       Cython: None\r\n       pandas: 2.0.3\r\n   matplotlib: 3.7.2\r\n       joblib: 1.3.1\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 6\r\n         prefix: libopenblas\r\n       filepath: \/home\/akaash\/Downloads\/statsmodels-play\/venv\/lib\/python3.10\/site-packages\/numpy.libs\/libopenblas64_p-r0-7a851222.3.23.so\r\n        version: 0.3.23\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 6\r\n         prefix: libopenblas\r\n       filepath: \/home\/akaash\/Downloads\/statsmodels-play\/venv\/lib\/python3.10\/site-packages\/scipy.libs\/libopenblasp-r0-23e5df77.3.21.dev.so\r\n        version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 6\r\n         prefix: libgomp\r\n       filepath: \/home\/akaash\/Downloads\/statsmodels-play\/venv\/lib\/python3.10\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n```\r\n","labels":["Bug","help wanted"],"created_at":"2023-08-04T22:18:48Z","comments":16,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27016"},{"issue_number":69,"repository":"scikit-learn\/scikit-learn","title":"inverse_transform of SimpleImputer with empty features changes order of columns","description":"### Describe the bug\n\nWhen one uses a SimpleImputer with `keep_empty_features=False` and `add_indicator=True`, then if a column has only missing values during fit, but has values during transform, then the inverse transform will shift all columns to the right of that column one to the left. \r\n\r\n(Note that I found this while experimenting and is not something that we rely on being fixed)\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nfrom sklearn.impute import SimpleImputer\r\n\r\nX1 = np.array(\r\n    [\r\n        [np.nan, 2.0, 3.0],\r\n        [np.nan, 2.0, 3.0],\r\n    ]\r\n)\r\n\r\nX2 = np.array(\r\n    [\r\n        [1.0, 2.0, 3.0],\r\n        [1.0, 2.0, 3.0],\r\n    ]\r\n)\r\n\r\nimputer = SimpleImputer(add_indicator=True)\r\nimputer.fit(X1)\r\nprint(imputer.inverse_transform(imputer.transform(X2)))\r\n```\n\n### Expected Results\n\nI think the best way to go is to fill the missing columns with the value from `missing_values`.\r\n\r\n```python\r\n[[nan 2. 3.]\r\n [nan 2. 3.]]\r\n```\n\n### Actual Results\n\n```python\r\n[[2. 3. 0.]\r\n [2. 3. 0.]]\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.11.4\r\nexecutable: [redacted]\r\n   machine: macOS-13.4.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.3.0\r\n          pip: 23.2\r\n   setuptools: 68.0.0\r\n        numpy: 1.25.1\r\n        scipy: 1.11.1\r\n       Cython: None\r\n       pandas: 2.0.3\r\n   matplotlib: 3.7.2\r\n       joblib: 1.3.0\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 12\r\n         prefix: libopenblas\r\n       filepath: [redacted]\r\n        version: 0.3.23\r\nthreading_layer: openmp\r\n   architecture: VORTEX\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 12\r\n         prefix: libomp\r\n       filepath: [redacted]\r\n        version: None\n```\n","labels":["Bug","help wanted"],"created_at":"2023-08-04T09:38:19Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/27012"},{"issue_number":70,"repository":"scikit-learn\/scikit-learn","title":"`ValueError: Input contains NaN.` in `sklearn.manifold.smacof`","description":"### Describe the bug\r\n\r\nI accidentally stumbled onto a `ValueError` when executing `smacof`. I hacked into `_mds.py` to save both the offending `dissimilarities` as well as the randomly generated `X`, then cut them down to minimal shape that still exhibits the error. This data is attached below in the MCVE.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```\r\nimport numpy as np                          \r\nimport sklearn.manifold                     \r\n                                            \r\ndis = np.array([\r\n    [0.0, 1.732050807568877, 1.7320508075688772], \r\n    [1.732050807568877, 0.0, 6.661338147750939e-16],\r\n    [1.7320508075688772, 6.661338147750939e-16, 0.0]\r\n])  \r\ninit = np.array([\r\n    [0.08665881585055124, 0.7939114643387546],\r\n    [0.9959834154297658, 0.7555546025640025],\r\n    [0.8766008278401566, 0.4227358815811242]\r\n])  \r\nsklearn.manifold.smacof(dis, init=init, normalized_stress=\"auto\", metric=False, n_init=1)\r\n```\r\n\r\n### Expected Results\r\n\r\nNo errors\r\n\r\n### Actual Results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"...\/rep_error.py\", line 14, in <module>\r\n    sklearn.manifold.smacof(dis, init=init, normalized_stress=\"auto\", metric=False, n_init=1)\r\n  File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/manifold\/_mds.py\", line 329, in smacof\r\n    pos, stress, n_iter_ = _smacof_single(\r\n  File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/manifold\/_mds.py\", line 128, in _smacof_single\r\n    dis = euclidean_distances(X)\r\n  File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/metrics\/pairwise.py\", line 310, in euclidean_distances\r\n    X, Y = check_pairwise_arrays(X, Y)\r\n  File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/metrics\/pairwise.py\", line 156, in check_pairwise_arrays\r\n    X = Y = check_array(\r\n  File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/utils\/validation.py\", line 959, in check_array\r\n    _assert_all_finite(\r\n  File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/utils\/validation.py\", line 124, in _assert_all_finite\r\n    _assert_all_finite_element_wise(\r\n  File \"...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/sklearn\/utils\/validation.py\", line 173, in _assert_all_finite_element_wise\r\n    raise ValueError(msg_err)\r\nValueError: Input contains NaN.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]\r\nexecutable: ...\/.direnv\/python-3.9.5\/bin\/python\r\n   machine: Linux-5.4.0-148-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.3.0\r\n          pip: 23.2.1\r\n   setuptools: 44.0.0\r\n        numpy: 1.25.1\r\n        scipy: 1.11.1\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: None\r\n       joblib: 1.3.1\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 32\r\n         prefix: libgomp\r\n       filepath: ...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 32\r\n         prefix: libopenblas\r\n       filepath: ...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/numpy.libs\/libopenblas64_p-r0-7a851222.3.23.so\r\n        version: 0.3.23\r\nthreading_layer: pthreads\r\n   architecture: Zen\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 32\r\n         prefix: libopenblas\r\n       filepath: ...\/.direnv\/python-3.9.5\/lib\/python3.9\/site-packages\/scipy.libs\/libopenblasp-r0-23e5df77.3.21.dev.so\r\n        version: 0.3.21.dev\r\nthreading_layer: pthreads\r\n   architecture: Zen\r\n```\r\n","labels":["Bug","Needs Investigation"],"created_at":"2023-08-03T04:24:30Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26999"},{"issue_number":71,"repository":"scikit-learn\/scikit-learn","title":"Agglomerative clustering training error for seuclidean\/mahalanobis affinity and single linkage","description":"### Describe the bug\n\nWhen trying Agglomerative clustering model training with the affinity as 'seuclidean' or 'mahalanobis' and the linkage as 'single' the training fails. The same affinity values along with other linkage such as 'average' options executes for model training. There's no specification given for this issue. Also, in the code I can see the handling for the single linkage is different and there is some cython code which is not accessible.\n\n### Steps\/Code to Reproduce\n\n```\r\n\r\nfrom sklearn.cluster import AgglomerativeClustering\r\nfrom sklearn.datasets import load_iris\r\nmodel = AgglomerativeClustering(affinity='mahalanobis', linkage='average')\r\ndata = load_iris(as_frame=True)['data']\r\nmodel.fit(data)\r\n\r\n```\n\n### Expected Results\n\nNo error should be thrown sig\n\n### Actual Results\n\n![image](https:\/\/github.com\/scikit-learn\/scikit-learn\/assets\/69189799\/c3da5d93-b215-4fd8-8610-a4c8ce632f8d)\r\n\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.8.17 (default, Jul  5 2023, 21:04:15)  [GCC 11.2.0]\r\nexecutable: \/home\/albint\/miniconda3\/envs\/myenv\/bin\/python\r\n   machine: Linux-5.15.0-78-generic-x86_64-with-glibc2.17\r\n\r\nPython dependencies:\r\n      sklearn: 1.3.0\r\n          pip: 23.1.2\r\n   setuptools: 67.8.0\r\n        numpy: 1.24.4\r\n        scipy: 1.10.1\r\n       Cython: 3.0.0\r\n       pandas: 2.0.3\r\n   matplotlib: 3.7.2\r\n       joblib: 1.3.1\r\nthreadpoolctl: 3.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: \/home\/albint\/miniconda3\/envs\/myenv\/lib\/python3.8\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so\r\n        version: 0.3.21\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n    num_threads: 8\r\n         prefix: libopenblas\r\n       filepath: \/home\/albint\/miniconda3\/envs\/myenv\/lib\/python3.8\/site-packages\/scipy.libs\/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n    num_threads: 8\r\n         prefix: libgomp\r\n       filepath: \/home\/albint\/miniconda3\/envs\/myenv\/lib\/python3.8\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\n```\n","labels":["Bug","help wanted"],"created_at":"2023-08-01T11:50:21Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26961"},{"issue_number":72,"repository":"scikit-learn\/scikit-learn","title":"Balanced Accuracy Score is NOT equal to Recall Score ","description":"### Describe the bug\r\n\r\nBy definition balanced accuracy should be equal to recall averaged over all the classes. Current implementation gives different answers. Please see the example below. \r\n\r\n```\r\nimport scikit.metrics as skm\r\n\r\ny_true = [1,1]\r\ny_pred = [1,2]\r\n\r\nskm.recall_score(y_true, y_pred, average='macro')  # 0.25\r\nskm.balanced_accuracy_score(y_true, y_pred)  # 0.5\r\n\r\n```\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```\r\nimport scikit.metrics as skm\r\n\r\ny_true = [1,1]\r\ny_pred = [1,2]\r\n\r\nrecall = skm.recall_score(y_true, y_pred, average='macro')  \r\nbalanced_acc = skm.balanced_accuracy_score(y_true, y_pred) \r\n```\r\n\r\n### Expected Results\r\n\r\n```\r\nrecall = balaced_acc = 0.25\r\n```\r\n\r\n### Actual Results\r\n\r\n```\r\nrecall = 0.25\r\nbalanced_acc = 0.5\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nimport sklearn; sklearn.show_versions()\r\n```\r\n","labels":["Bug","New Feature"],"created_at":"2023-07-24T21:52:02Z","comments":5,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26892"},{"issue_number":73,"repository":"scikit-learn\/scikit-learn","title":"Handling `pd.NA` in encoders","description":"It seems that we don't handle it properly `pd.NA` in the encoder and thus differently than `np.nan`.\r\n`pd.NA` will raise an error as in the following reproducible:\r\n\r\n```python\r\ndf = pd.DataFrame({\"col_1\": [\"A\", \"B\", pd.NA]})\r\nOneHotEncoder(sparse_output=False).fit_transform(df)\r\n```\r\n\r\nor \r\n\r\n```python\r\ndf = pd.DataFrame({\"col_1\": [\"A\", \"B\", pd.NA]})\r\nOrdinalEncoder().fit_transform(df)\r\n```\r\n\r\nleading to:\r\n\r\n<details>\r\n\r\n```pytb\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nFile [~\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py:174](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/glemaitre\/Documents\/packages\/scikit-learn\/~\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py:174), in _unique_python(values, return_inverse, return_counts)\r\n    [172](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=171) uniques_set, missing_values = _extract_missing(uniques_set)\r\n--> [174](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=173) uniques = sorted(uniques_set)\r\n    [175](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=174) uniques.extend(missing_values.to_list())\r\n\r\nFile [~\/miniconda3\/envs\/dev\/lib\/python3.10\/site-packages\/pandas\/_libs\/missing.pyx:388](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/glemaitre\/Documents\/packages\/scikit-learn\/~\/miniconda3\/envs\/dev\/lib\/python3.10\/site-packages\/pandas\/_libs\/missing.pyx:388), in pandas._libs.missing.NAType.__bool__()\r\n\r\nTypeError: boolean value of NA is ambiguous\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n[\/home\/glemaitre\/Documents\/packages\/scikit-learn\/examples\/model_selection\/plot_tuned_threshold_classifier_with_metadata_routing.py](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/glemaitre\/Documents\/packages\/scikit-learn\/examples\/model_selection\/plot_tuned_threshold_classifier_with_metadata_routing.py) in line 3\r\n      [188](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/examples\/model_selection\/plot_tuned_threshold_classifier_with_metadata_routing.py?line=187) # %%\r\n      [189](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/examples\/model_selection\/plot_tuned_threshold_classifier_with_metadata_routing.py?line=188) df = pd.DataFrame({\"col_1\": [\"A\", \"B\", pd.NA]})\r\n----> [190](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/examples\/model_selection\/plot_tuned_threshold_classifier_with_metadata_routing.py?line=189) OneHotEncoder(sparse_output=False).fit_transform(df)\r\n\r\nFile [~\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_set_output.py:140](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/glemaitre\/Documents\/packages\/scikit-learn\/~\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_set_output.py:140), in _wrap_method_output..wrapped(self, X, *args, **kwargs)\r\n    [138](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_set_output.py?line=137) @wraps(f)\r\n    [139](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_set_output.py?line=138) def wrapped(self, X, *args, **kwargs):\r\n--> [140](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_set_output.py?line=139)     data_to_wrap = f(self, X, *args, **kwargs)\r\n    [141](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_set_output.py?line=140)     if isinstance(data_to_wrap, tuple):\r\n    [142](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_set_output.py?line=141)         # only wrap the first output for cross decomposition\r\n    [143](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_set_output.py?line=142)         return_tuple = (\r\n    [144](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_set_output.py?line=143)             _wrap_data_with_container(method, data_to_wrap[0], X, self),\r\n    [145](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_set_output.py?line=144)             *data_to_wrap[1:],\r\n    [146](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_set_output.py?line=145)         )\r\n\r\nFile [~\/Documents\/packages\/scikit-learn\/sklearn\/base.py:948](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/glemaitre\/Documents\/packages\/scikit-learn\/~\/Documents\/packages\/scikit-learn\/sklearn\/base.py:948), in TransformerMixin.fit_transform(self, X, y, **fit_params)\r\n    [933](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=932)         warnings.warn(\r\n    [934](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=933)             (\r\n    [935](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=934)                 f\"This object ({self.__class__.__name__}) has a `transform`\"\r\n   (...)\r\n    [943](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=942)             UserWarning,\r\n    [944](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=943)         )\r\n    [946](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=945) if y is None:\r\n    [947](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=946)     # fit method of arity 1 (unsupervised transformation)\r\n--> [948](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=947)     return self.fit(X, **fit_params).transform(X)\r\n    [949](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=948) else:\r\n    [950](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=949)     # fit method of arity 2 (supervised transformation)\r\n    [951](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=950)     return self.fit(X, y, **fit_params).transform(X)\r\n\r\nFile [~\/Documents\/packages\/scikit-learn\/sklearn\/base.py:1215](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/glemaitre\/Documents\/packages\/scikit-learn\/~\/Documents\/packages\/scikit-learn\/sklearn\/base.py:1215), in _fit_context..decorator..wrapper(estimator, *args, **kwargs)\r\n   [1208](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=1207)     estimator._validate_params()\r\n   [1210](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=1209) with config_context(\r\n   [1211](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=1210)     skip_parameter_validation=(\r\n   [1212](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=1211)         prefer_skip_nested_validation or global_skip_validation\r\n   [1213](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=1212)     )\r\n   [1214](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=1213) ):\r\n-> [1215](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/base.py?line=1214)     return fit_method(estimator, *args, **kwargs)\r\n\r\nFile [~\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py:982](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/glemaitre\/Documents\/packages\/scikit-learn\/~\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py:982), in OneHotEncoder.fit(self, X, y)\r\n    [972](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=971)     warnings.warn(\r\n    [973](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=972)         (\r\n    [974](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=973)             \"`sparse` was renamed to `sparse_output` in version 1.2 and \"\r\n   (...)\r\n    [978](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=977)         FutureWarning,\r\n    [979](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=978)     )\r\n    [980](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=979)     self.sparse_output = self.sparse\r\n--> [982](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=981) self._fit(\r\n    [983](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=982)     X,\r\n    [984](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=983)     handle_unknown=self.handle_unknown,\r\n    [985](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=984)     force_all_finite=\"allow-nan\",\r\n    [986](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=985) )\r\n    [987](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=986) self._set_drop_idx()\r\n    [988](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=987) self._n_features_outs = self._compute_n_features_outs()\r\n\r\nFile [~\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py:97](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/glemaitre\/Documents\/packages\/scikit-learn\/~\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py:97), in _BaseEncoder._fit(self, X, handle_unknown, force_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\r\n     [94](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=93) Xi = X_list[i]\r\n     [96](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=95) if self.categories == \"auto\":\r\n---> [97](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=96)     result = _unique(Xi, return_counts=compute_counts)\r\n     [98](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=97)     if compute_counts:\r\n     [99](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/preprocessing\/_encoders.py?line=98)         cats, counts = result\r\n\r\nFile [~\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py:42](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/glemaitre\/Documents\/packages\/scikit-learn\/~\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py:42), in _unique(values, return_inverse, return_counts)\r\n     [11](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=10) \"\"\"Helper function to find unique values with support for python objects.\r\n     [12](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=11) \r\n     [13](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=12) Uses pure python method for object dtype, and numpy method for\r\n   (...)\r\n     [39](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=38)     array. Only provided if `return_counts` is True.\r\n     [40](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=39) \"\"\"\r\n     [41](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=40) if values.dtype == object:\r\n---> [42](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=41)     return _unique_python(\r\n     [43](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=42)         values, return_inverse=return_inverse, return_counts=return_counts\r\n     [44](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=43)     )\r\n     [45](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=44) # numerical\r\n     [46](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=45) return _unique_np(\r\n     [47](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=46)     values, return_inverse=return_inverse, return_counts=return_counts\r\n     [48](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=47) )\r\n\r\nFile [~\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py:179](https:\/\/file+.vscode-resource.vscode-cdn.net\/home\/glemaitre\/Documents\/packages\/scikit-learn\/~\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py:179), in _unique_python(values, return_inverse, return_counts)\r\n    [177](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=176) except TypeError:\r\n    [178](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=177)     types = sorted(t.__qualname__ for t in set(type(v) for v in values))\r\n--> [179](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=178)     raise TypeError(\r\n    [180](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=179)         \"Encoders require their input to be uniformly \"\r\n    [181](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=180)         f\"strings or numbers. Got {types}\"\r\n    [182](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=181)     )\r\n    [183](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=182) ret = (uniques,)\r\n    [185](file:\/\/\/home\/glemaitre\/Documents\/packages\/scikit-learn\/sklearn\/utils\/_encode.py?line=184) if return_inverse:\r\n\r\nTypeError: Encoders require their input to be uniformly strings or numbers. Got ['NAType', 'str']\r\n```\r\n\r\n<\/details>\r\n\r\nIf `np.nan` is used, then we are fine:\r\n\r\n```python\r\ndf = pd.DataFrame({\"col_1\": [\"A\", \"B\", np.nan]})\r\nOneHotEncoder(sparse_output=False).fit_transform(df)\r\n```\r\n\r\n```\r\narray([[1., 0., 0.],\r\n       [0., 1., 0.],\r\n       [0., 0., 1.]])\r\n```\r\n\r\n```python\r\ndf = pd.DataFrame({\"col_1\": [\"A\", \"B\", np.nan]})\r\nOrdinalEncoder().fit_transform(df)\r\n```\r\n\r\n```\r\narray([[ 0.],\r\n       [ 1.],\r\n       [nan]])\r\n```","labels":["Bug","module:preprocessing"],"created_at":"2023-07-24T18:33:03Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26890"},{"issue_number":74,"repository":"scikit-learn\/scikit-learn","title":"Ridge and RidgeCV has different result even if alpha is the same","description":"### Describe the bug\r\n\r\n(From discussion in https:\/\/github.com\/scikit-learn\/scikit-learn\/discussions\/26733)\r\n\r\nIn one of my dataset, I try to specify only one value in alphas in RidgeCV, and expects RidgeCV should give me the same result as using Ridge with same alpha value. But it would not (see the code and output below). And the RidgeCV result seems abnormal.\r\n\r\nI found that the problem is related to some large values we used in sample_weight. I can reproduce it with make_regression.\r\n\r\nI guess the large value in sample_weight may caused some overflow in RidgeCV intermediate calculation?\r\nGiven that RidgeCV and Ridge are using different solvers under the hood, I think it is not surprise that they may behave differently in this case? I just scaled down the weight and the problem is gone.\r\n\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.linear_model import Ridge, RidgeCV\r\nfrom sklearn.datasets import make_regression\r\n\r\nX, y = make_regression(n_features=2)\r\nwgt = np.random.randint(2e5, 2e11, size=100)\r\n\r\nprint('=== RidgeCV ===')\r\nclf = RidgeCV(alphas=[0.001],fit_intercept=True).fit(\r\n    X=X,\r\n    y=y,\r\n    sample_weight=wgt,\r\n)\r\nprint(f'{clf.coef_ = }')\r\nprint(f'{clf.intercept_ = }')\r\nprint(f'{clf.alpha_ = }')\r\n\r\nprint('=== Ridge ===')\r\nclf = Ridge(alpha=0.001,fit_intercept=True).fit(\r\n    X=X,\r\n    y=y,\r\n    sample_weight=wgt\r\n)\r\nprint(f'{clf.coef_ = }')\r\nprint(f'{clf.intercept_ = }')\r\n```\r\n\r\n### Expected Results\r\n\r\nThe output of RidgeCV and Ridge is the same.\r\n\r\n### Actual Results\r\n\r\n```\r\n=== RidgeCV ===\r\n'clf.coef_ = array([-95.63781897,  31.72140137])'\r\n'clf.intercept_ = 2.6375015138569937'\r\n'clf.alpha_ = 0.001'\r\n=== Ridge ===\r\n'clf.coef_ = array([41.8887401, 33.131992 ])'\r\n'clf.intercept_ = -4.440892098500626e-16'\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nI am using scikit-learn 1.2.1 on python 3.8 on Linux.\r\n```\r\n","labels":["Bug","module:linear_model","Needs Investigation","Numerical Stability"],"created_at":"2023-07-17T18:59:24Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26848"},{"issue_number":75,"repository":"scikit-learn\/scikit-learn","title":"AttributeError: This 'LabelEncoder' has no attribute 'set_output'","description":"### Describe the bug\n\nI tried to call **'set_output'** from LabelEncoder object and got the AttributeError.\r\n\r\n[The document](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) says sklearn.preprocessing.LabelEncoder has **'set_output'** method, but it was not working.\r\n\r\nSoon I found most of other **'set_output'** available estimators inherits both of sklearn.base.OneToOneFeatureMixin and sklearn.base.TransformerMinxin\r\n\r\nHowerver, [LabelEncoder](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/preprocessing\/_label.py) only inherits the TransformerMinxin.\r\n\r\n```python\r\nclass LabelEncoder(TransformerMixin, BaseEstimator):\r\n```\r\n\r\n<\/br>\r\n\r\nFunction **'set_output'** seems available when **'_auto_wrap_is_configured'** is True. [(utils._set_output.py)](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/utils\/_set_output.py)\r\n\r\n```python\r\n    @available_if(_auto_wrap_is_configured)\r\n    def set_output(self, *, transform=None):\r\n        \"\"\"Set output container.\r\n\r\n        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`\r\n        for an example on how to use the API.\r\n\r\n        Parameters\r\n        ----------\r\n        transform : {\"default\", \"pandas\"}, default=None\r\n            Configure output of `transform` and `fit_transform`.\r\n\r\n            - `\"default\"`: Default output format of a transformer\r\n            - `\"pandas\"`: DataFrame output\r\n            - `None`: Transform configuration is unchanged\r\n\r\n        Returns\r\n        -------\r\n        self : estimator instance\r\n            Estimator instance.\r\n        \"\"\"\r\n        if transform is None:\r\n            return self\r\n\r\n        if not hasattr(self, \"_sklearn_output_config\"):\r\n            self._sklearn_output_config = {}\r\n\r\n        self._sklearn_output_config[\"transform\"] = transform\r\n        return self\r\n```\r\n\r\n<\/br>\r\n\r\nThen estimator should have **'get_feature_names_out'** to make **'_auto_wrap_is_configured'** returns True. [(utils._set_output.py)](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/utils\/_set_output.py)\r\n\r\n```python\r\ndef _auto_wrap_is_configured(estimator):\r\n    \"\"\"Return True if estimator is configured for auto-wrapping the transform method.\r\n\r\n    `_SetOutputMixin` sets `_sklearn_auto_wrap_output_keys` to `set()` if auto wrapping\r\n    is manually disabled.\r\n    \"\"\"\r\n    auto_wrap_output_keys = getattr(estimator, \"_sklearn_auto_wrap_output_keys\", set())\r\n    return (\r\n        hasattr(estimator, \"get_feature_names_out\")\r\n        and \"transform\" in auto_wrap_output_keys\r\n    )\r\n```\r\n\r\n<\/br>\r\n\r\n To have **'get_feature_names_out'** attr, estimator should inherit OneToOneFeatureMixin as I think. [(base.py)](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/base.py)\r\n\r\n```python\r\nclass OneToOneFeatureMixin:\r\n    \"\"\"Provides `get_feature_names_out` for simple transformers.\r\n\r\n    This mixin assumes there's a 1-to-1 correspondence between input features\r\n    and output features, such as :class:`~preprocessing.StandardScaler`.\r\n    \"\"\"\r\n\r\n    def get_feature_names_out(self, input_features=None):\r\n        \"\"\"Get output feature names for transformation.\r\n\r\n        Parameters\r\n        ----------\r\n        input_features : array-like of str or None, default=None\r\n            Input features.\r\n\r\n            - If `input_features` is `None`, then `feature_names_in_` is\r\n              used as feature names in. If `feature_names_in_` is not defined,\r\n              then the following input feature names are generated:\r\n              `[\"x0\", \"x1\", ..., \"x(n_features_in_ - 1)\"]`.\r\n            - If `input_features` is an array-like, then `input_features` must\r\n              match `feature_names_in_` if `feature_names_in_` is defined.\r\n\r\n        Returns\r\n        -------\r\n        feature_names_out : ndarray of str objects\r\n            Same as input features.\r\n        \"\"\"\r\n        check_is_fitted(self, \"n_features_in_\")\r\n        return _check_feature_names_in(self, input_features)\r\n```\r\n\r\n<\/br>\r\n\r\nI want to know that it is kind of a bug, or the document says wrong information.\n\n### Steps\/Code to Reproduce\n\n```python\r\nfrom sklearn.preprocessing import LabelEncoder\r\n\r\nLabelEncoder().set_output()\r\n```\n\n### Expected Results\n\nNo error is thrown\n\n### Actual Results\n\n```python\r\n--------------------------------------------------------------------------------\r\nAttributeError                                 Traceback (most recent call last)                        \r\n\/tmp\/ipykernel_16765\/3091610596.py in <module>\r\n---> 1 LabelEncoder().set_output()\r\n\r\n~\/.local\/lib\/python3.8\/site-packages\/sklearn\/utils\/_available_if.py in __get__(self, obj, owner)\r\n      31        # this is to allow access to the docstrings.\r\n      32        if not self.check(obj):\r\n--->  33            raise attr_err\r\n      34        out = MethodType(self.fn, obj)\r\n      35\r\nAttributeError: This 'LabelEncoder' has no attribute 'set_output'\r\n```\r\n            \r\n             \r\n            \n\n### Versions\n\n```shell\n1.2.2\n```\n","labels":["Bug","Documentation"],"created_at":"2023-06-27T07:08:20Z","comments":10,"reactions":6,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26711"},{"issue_number":76,"repository":"scikit-learn\/scikit-learn","title":"Automatic bandwidth calculation valid only for normalized data","description":"### Describe the bug\r\n\r\n`sklearn.neighbors.KernelDensity` supports automatic (optimal) bandwidth calculation via `bandwidth = 'silverman'` and `bandwidth = 'scott'`. The algorithm computes the appropriate observation-weighted bandwidth factors (proportional to nobs^0.2) but does not adjust for the standard deviation or interquartile range of the dataset. Roughly, the algorithm should scale the dataset's standard error by the algorithmic bandwidth factors.\r\n\r\nSee, e.g., [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Kernel_density_estimation#A_rule-of-thumb_bandwidth_estimator). The implementation in `scipy.stats._kde` is correct.\r\n\r\n### Steps\/Code to Reproduce\r\n```python\r\nimport matplotlib.pyplot as plot\r\nimport numpy as np\r\nfrom sklearn.neighbors import KernelDensity\r\nfrom scipy.stats import gaussian_kde\r\n\r\ndata = np.random.normal( scale = 0.01, size = 100 )\r\n\r\n#\r\n# 1. sklearn (auto)\r\n#\r\nkd_sklearn_auto = KernelDensity( kernel = 'gaussian', bandwidth = 'silverman' )\r\nkd_sklearn_auto.fit( np.reshape( data, ( -1, 1 ) ) )\r\n\r\n#\r\n# 2. sklearn (manual)\r\n#\r\nkd_sklearn_manual = KernelDensity( kernel = 'gaussian', bandwidth = 0.9 * np.std( data ) \/ len( data ) ** ( 1 \/ 5 ) )\r\nkd_sklearn_manual.fit( np.reshape( data, ( -1, 1 ) ) )\r\n\r\n#\r\n# 3. scipy\r\n#\r\nkd_scipy = gaussian_kde( data, bw_method = 'silverman' )\r\n\r\n#\r\n# 4. show the difference\r\n#\r\nxs = np.arange( start = -0.05, stop = 0.05, step = 1e-4 )\r\nplot.plot( xs, np.exp( kd_sklearn_auto.score_samples( np.reshape( xs, ( -1, 1 ) ) ) ), label = 'KDE SKLearn (auto)' )\r\nplot.plot( xs, np.exp( kd_sklearn_manual.score_samples( np.reshape( xs, ( -1, 1 ) ) ) ), label = 'KDE SKLearn (manual)' )\r\nplot.plot( xs, kd_scipy.pdf( xs ), label = 'KDE SciPy' )\r\nplot.hist( data, label = 'Data' )\r\nplot.legend()\r\nplot.show()\r\n```\r\n\r\n### Expected Results\r\n\r\nAutomatic SKLearn bandwidth curve should approximately match SciPy bandwidth curve, roughly the shape of the underlying data histogram.\r\n\r\n### Actual Results\r\n\r\nAutomatic SKLearn bandwidth curve generates a flat PDF.\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]\r\nexecutable: \/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-67c47d19-3f15-49a2-ab8f-bcf25a2bc29f\/bin\/python\r\n   machine: Linux-5.15.0-1038-azure-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.2\r\n          pip: 21.2.4\r\n   setuptools: 58.0.4\r\n        numpy: 1.20.3\r\n        scipy: 1.7.1\r\n       Cython: 0.29.24\r\n       pandas: 1.3.4\r\n   matplotlib: 3.4.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       filepath: \/databricks\/python3\/lib\/python3.9\/site-packages\/numpy.libs\/libopenblasp-r0-5bebc122.3.13.dev.so\r\n         prefix: libopenblas\r\n       user_api: blas\r\n   internal_api: openblas\r\n        version: 0.3.13.dev\r\n    num_threads: 6\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n\r\n       filepath: \/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-67c47d19-3f15-49a2-ab8f-bcf25a2bc29f\/lib\/python3.9\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n         prefix: libgomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 6\r\n\r\n       filepath: \/databricks\/python3\/lib\/python3.9\/site-packages\/scipy.libs\/libopenblasp-r0-085ca80a.3.9.so\r\n         prefix: libopenblas\r\n       user_api: blas\r\n   internal_api: openblas\r\n        version: 0.3.9\r\n    num_threads: 6\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n```\r\n","labels":["Bug"],"created_at":"2023-06-21T21:41:09Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26658"},{"issue_number":77,"repository":"scikit-learn\/scikit-learn","title":"TransformedTargetRegressor forces 1d y shape to regressor","description":"### Describe the bug\r\n\r\nI experience the following error when using TransformedTargetRegressor with my skorch model:\r\nValueError: The target data shouldn't be 1-dimensional but instead have 2 dimensions, with the second dimension having the same size as the number of regression targets (usually 1). Please reshape your target data to be 2-dimensional (e.g. y = y.reshape(-1, 1).\r\n\r\n#### After checking the Source Code this lead me the the following unexpected behaivor which makes little sense:\r\n\r\nIf TransformedTargetRegressor is fitted with with a 2d dimensional y, it will still be transformed to a 1d dimensional output\r\n\r\ny should have the same input and output shapes with a TransformedTargetRegressor or there should be an init argument to disable the change of the input shape\r\n(Yes, internally it gets casted to 2d, but I\u2019m talking about the In and Outputs) \r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/364c77e04\/sklearn\/compose\/_target.py#L20\r\nTransformedTargetRegressor-->fit\r\n\r\n```python\r\n        if y.ndim == 1:\r\n            y_2d = y.reshape(-1, 1)\r\n        else:\r\n            y_2d = y\r\n        self._fit_transformer(y_2d)\r\n\r\n[...]\r\n\r\n        if y_trans.ndim == 2 and y_trans.shape[1] == 1:\r\n            y_trans = y_trans.squeeze(axis=1)\r\n```\r\nBut in the end we squeeze it back into a 1d which causes issues for models which expect a 2d input of y\r\ny was 2d in the beginning for a reason\r\n\r\n### **The following code would solve this:**\r\n```\r\n        if y_trans.ndim == 2 and y_trans.shape[1] == 1 and y.ndim==1:  #only squeeze back to 1d if y is 1d\r\n            y_trans = y_trans.squeeze(axis=1)\r\n```\r\n\r\nThis could only create an issue where the y input was for some reason 2d but should be 1d for the regressor. \r\nIn this case an attribute would be nice\r\n```\r\n        if y_trans.ndim == 2 and y_trans.shape[1] == 1 and self.output_dim == 1:\r\n            y_trans = y_trans.squeeze(axis=1)\r\n```\r\n\r\nAlso in TransformedTargetRegressor-->predict the results dont get squeezed after the prediction of the estimator - only if the original input shape was 1, in that case it is squeezed\r\n\r\nSo the result looks as expected, but only if the regressor takes a 1d y\r\nIf the estimator expects a 2d y the code fails\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nregressor = TransformedTargetRegressor(\r\n    transformer=MinMaxScaler()\r\n)\r\nX, y = np.random.rand(10, 10), np.expand_dims(np.random.rand(10), 1)\r\nregressor.fit(X, y)\r\n```\r\n\r\n### Expected Results\r\n\r\nThe shape of y stays the same as the input OR there is a attribute which allows the choice of  (1d or original)  or (1d or 2d)\r\n\r\ninput | internal | output\r\n2d \u2014> 2d \u2014> 2d\r\n1d \u2014> 2d \u2014> 1d\r\n\r\n### Actual Results\r\n\r\nthe regressor gets just a 1d array even through y was specifically set to 2d\r\n(I don't know how to extract these results without an debugger)\r\n\r\nIt works for this example because the default regressor is used, but when using it with other models they might need the 2nd dimention of y, because it was specifically reshaped  (-1,1)\r\n\r\ninput | internal | output\r\n2d \u2014> 2d \u2014> 1d    THIS creates issues for the regressive which is passed to the Transformer if it expects a 2d array because a 2d y was given \r\n1d \u2014> 2d \u2014> 1d\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]\r\nexecutable: \/anaconda\/envs\/azureml_py310_sdkv2\/bin\/python\r\n   machine: Linux-5.15.0-1017-azure-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.3\r\n          pip: 22.1.2\r\n   setuptools: 61.2.0\r\n        numpy: 1.23.2\r\n        scipy: 1.9.0\r\n       Cython: None\r\n       pandas: 1.4.3\r\n   matplotlib: 3.6.2\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n```\r\n","labels":["Bug"],"created_at":"2023-06-07T13:58:14Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26530"},{"issue_number":78,"repository":"scikit-learn\/scikit-learn","title":"Inconsitency between C-contiguous and F-contiguous arrays","description":"### No consistency between C-contiguous and F-contiguous arrays for LinearRegression()\r\n\r\nAt least for LinearRegression() : In some edge case (when X is almost singular), there is huge difference between C-contiguous and F-contiguous arrays predictions.\r\n\r\n- This is due to the fact that array product gives different results between contiguous and F-contiguous arrays ([cf this Stack Overflow questions that I posted](https:\/\/stackoverflow.com\/questions\/76388886\/python-rounding-errors-between-c-contiguous-and-f-contiguous-arrays-for-matrix))\r\n- These \"edge cases\" can actually be quite common in time-series predictions, where a lot of auto-regressive features can easily be correlated\r\n- I would strongly advise parsing all arrays to C-contiguous before doing the predictions\/fitting.\r\n- Please also note that **fitting** with F-contiguous or C-contiguous can also give different results.\r\n- The worst is not that this is happening, it is that no warning are being raised whatsoever.\r\n- Also, F-contiguous arrays are extremely common in pandas DataFrames, which is what a lot of developers are using in this context...\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np; print(np.__version__) # 1.23.5\r\nimport scipy; print(scipy.__version__) # 1.10.0\r\nimport sklearn as sk; print(sk.__version__) # 1.2.1\r\n\r\nfrom sklearn.linear_model import LinearRegression\r\nimport pandas as pd\r\n\r\n# Parameters \r\nseed, N_obs, N_feat, mu_x, sigma_x, mu_y, sigma_y = 0, 100, 1000, 100, 0.1, 100, 1\r\n\r\n# 1) Creating a weird edge-case X, y :\r\nnp.random.seed(seed)\r\ns = pd.Series(np.random.normal(mu_x, sigma_x, N_obs))\r\nX = np.stack([s.ewm(com=com).mean() for com in np.arange(N_feat)]).T\r\ny = np.random.normal(mu_y, sigma_y, N_obs)\r\n\r\n# 2) Showing that there is different results for C-cont vs F-cont arrays :\r\nmodel = LinearRegression()\r\nmodel.fit(X, y)\r\ny_pred = model.predict(X)\r\ny_pred_c = model.predict(np.ascontiguousarray(X))\r\n\r\n# Either just plot it and see :\r\nimport matplotlib.pyplot as plt\r\nplt.scatter(y_pred, y_pred_c)\r\n\r\n# Or look at the data :\r\nnp.var(y_pred)\r\nnp.var(y_pred - y_pred_c)\r\nnp.corrcoef(y_pred, y_pred_c)[0,1] # == 0.40295584536349216\r\n# --> y_pred EXTREMELY different than y_pred_c\r\n```\r\n\r\n### Expected Results\r\n\r\nWe expect y_pred to be fully equal to y_pred_c.\r\nOr at least `np.corrcoef(y_pred, y_pred_c)[0,1] > .99`\r\n\r\n\r\n### Actual Results\r\n\r\n`np.corrcoef(y_pred, y_pred_c)[0,1] # == 0.40295584536349216`\r\n- ypred and y_pred_c are totally different.\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0]\r\nexecutable: \/opt\/anaconda3\/bin\/python\r\n   machine: Linux-5.10.0-23-cloud-amd64-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.1\r\n          pip: 22.3.1\r\n   setuptools: 65.6.3\r\n        numpy: 1.23.5\r\n        scipy: 1.10.0\r\n       Cython: None\r\n       pandas: 1.5.3\r\n   matplotlib: 3.7.0\r\n       joblib: 1.1.1\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       filepath: \/opt\/anaconda3\/lib\/libmkl_rt.so.1\r\n         prefix: libmkl_rt\r\n       user_api: blas\r\n   internal_api: mkl\r\n        version: 2021.4-Product\r\n    num_threads: 64\r\nthreading_layer: intel\r\n\r\n       filepath: \/opt\/anaconda3\/lib\/libiomp5.so\r\n         prefix: libiomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 128\r\n\r\n       filepath: \/opt\/anaconda3\/lib\/libgomp.so.1.0.0\r\n         prefix: libgomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 128\r\n```\r\n","labels":["Bug"],"created_at":"2023-06-02T15:05:37Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26493"},{"issue_number":79,"repository":"scikit-learn\/scikit-learn","title":"Numpy Array Error when Training LogisticRegressionCV","description":"### Describe the bug\r\n\r\nWhen I attempt to train LogisticRegressionCV, I get the error: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.\r\n\r\nThe inputs to LogisticRegressionCV are:\r\n- X = output of StandardScaler.fit_transform, with a shape of (24, 12) and dtype of float64\r\n- y = array with shape (24, ) and dtype of int64\r\n\r\nI checked that there are not null or infinite values in either array.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```\r\ndata_for_color_training = training_data.loc[training_data[\"IdentCode\"] == color]\r\nx_train = data_for_color_training.loc[:, (data_for_color_training.columns.str.startswith(\"dx\") | data_for_color_training.columns.str.startswith(\"dy\") | data_for_color_training.columns.str.startswith(\"dz\"))]\r\ny_train = data_for_color_training[\"Grade\"]\r\n\t\t\t\r\n#--------------Edit specific pre-processing and model here-----------------------------\r\nscaler = StandardScaler()\r\nX_train = scaler.fit_transform(x_train)\r\nmodelTrained = LogisticRegressionCV(max_iter=10000).fit(X_train, y_train)\r\n```\r\n\r\n### Expected Results\r\n\r\nThe variable modelTrained will output a LogisticRegressionCV model.\r\n\r\n### Actual Results\r\n```traceback\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\...\\PythonScripts\\UserDefinedModel.py\", line 22, in train_predict_by_colorcode\r\n    modelTrained = LogisticRegressionCV(max_iter=10000).fit(X_train, y_train)\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1912, in fit\r\n    coefs_paths = np.reshape(\r\n                  ^^^^^^^^^^^\r\n  File \"<__array_function__ internals>\", line 200, in reshape\r\n  File \"C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 298, in reshape\r\n    return _wrapfunc(a, 'reshape', newshape, order=order)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 54, in _wrapfunc\r\n    return _wrapit(obj, method, *args, **kwds)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 43, in _wrapit\r\n    result = getattr(asarray(obj), method)(*args, **kwds)\r\n                     ^^^^^^^^^^^^\r\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 298, in reshape\r\n    return _wrapfunc(a, 'reshape', newshape, order=order)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 54, in _wrapfunc\r\n    return _wrapit(obj, method, *args, **kwds)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py\", line 43, in _wrapit\r\n    result = getattr(asarray(obj), method)(*args, **kwds)\r\n                     ^^^^^^^^^^^^\r\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.\r\n```\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.11.1 (tags\/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]\r\nexecutable: C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\r\n   machine: Windows-10-10.0.19044-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.2\r\n          pip: 22.3.1\r\n   setuptools: 65.5.0\r\n        numpy: 1.24.1\r\n        scipy: 1.10.0\r\n       Cython: None\r\n       pandas: 1.5.3\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\r\n        version: 0.3.21\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 16\r\n\r\n       user_api: openmp\r\ninternal_api: openmp\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n    num_threads: 16\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\...\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\scipy.libs\\libopenblas-802f9ed1179cb9c9b03d67ff79f48187.dll\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 16\r\n```\r\n","labels":["Bug","module:linear_model"],"created_at":"2023-05-19T02:51:59Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26401"},{"issue_number":80,"repository":"scikit-learn\/scikit-learn","title":"SplineTransformer(extrapolate=\"periodic\") outputs nan values for constant features","description":"While reviewing #24145 I discovered the following bug:\r\n\r\n```python\r\nIn [1]: import numpy as np\r\n\r\nIn [2]: from sklearn.preprocessing import SplineTransformer\r\n\r\nIn [3]: SplineTransformer(extrapolation=\"periodic\").fit_transform(np.ones(shape=(5, 1)))\r\nOut[3]: \r\narray([[nan, nan, nan, nan],\r\n       [nan, nan, nan, nan],\r\n       [nan, nan, nan, nan],\r\n       [nan, nan, nan, nan],\r\n       [nan, nan, nan, nan]])\r\n```\r\n\r\nBatman!\r\n\r\nThis is caused by:\r\n\r\n- https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/ea046f024694b9a558c882b8c2610c52dad95e29\/sklearn\/preprocessing\/_polynomial.py#L962-L971\r\n\r\n`spl.t[n] - spl.t[spl.k]` is typically zero for constant features.\r\n\r\nWhile fixing it for constant features is probably easy (just use `x = X[:, i]`), I wonder if other related numerical stability problems can be triggered for nearly constant features.\r\n\r\nBut maybe we can solve this problem in two stages: first the nan problem caused by modulus by exact zero and then investigate behavior on nearly constant data.\r\n","labels":["Bug"],"created_at":"2023-05-17T13:56:20Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26390"},{"issue_number":81,"repository":"scikit-learn\/scikit-learn","title":"SequentialFeatureSelector in backward auto mode will always remove one feature","description":"https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/364c77e047ca08a95862becf40a04fe9d4cd2c98\/sklearn\/feature_selection\/_sequential.py#L273\r\n\r\nThe initial value of `old_score` is incorrect if `direction == 'backward'`. With the current initial value of `-np.inf, ((new_score - old_score) < self.tol)` would always be `False` no matter what `new_score` is returned by `self._get_best_new_feature_score()`, so that at least one feature would be removed in the first iteration.\r\n\r\n`old_score` needs to be set to an initial value that is the `cross_val_score` with all the features in use if `direction == 'backward'`.","labels":["Bug","module:feature_selection"],"created_at":"2023-05-14T19:30:13Z","comments":2,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26369"},{"issue_number":82,"repository":"scikit-learn\/scikit-learn","title":"Duality gap computation in covariance.GraphicalLasso yields negative values.","description":"### Describe the bug\r\n\r\nThe computation of the duality gap in `_dual_gap(emp_cov, precision_, alpha)` of `GraphicalLasso` uses the definition from `Duchi et al., 2012`. \r\nHowever, their duality gap is expressed given a _feasible_ dual variable. In the implementation, it is applied to a primal variable that doesn't necessarily satisfy the dual variable's feasibility constraints.\r\nThis results in potentially negative values for the duality gap.\r\n\r\nThe primal problem reads\r\n\r\n$$P(\\Theta) = \\min_\\Theta \\varphi(\\Theta) + \\nu(\\Theta)$$\r\nwith\r\n$$\\varphi(\\Theta) =  -\\log \\det (\\Theta)  + \\langle S, \\Theta \\rangle \\text{ and } \\nu(\\Theta) =  \\Vert \\Lambda \\odot \\Theta \\Vert_1$$\r\nThe Fenchel-Rockafellar dual problem is then given by\r\n\r\n$$D(W) = \\min_{W}  \\varphi^{\\star} (W) + \\nu^{\\star} (-W)$$\r\n$$= \\min_{W} -\\log \\det(S - W) - p \\quad \\text{s.t. } \\quad \\vert W_{ij} \\vert < \\lambda_{ij}$$\r\nWe can then compute the duality gap as follows:\r\n$$G(\\Theta, W) = P(\\Theta) + D(W)$$\r\nOnly for a *feasible* $W$ can we actually take $\\theta = (S - W)^{-1}$ and thus simplify the duality gap expression and fall back on the implemented formula:\r\n$$G(\\Theta) = \\langle S, \\Theta \\rangle + \\Vert \\Lambda \\odot \\Theta \\Vert_1 - p$$ \r\n@mathurinm \r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.covariance import GraphicalLasso\r\n\r\nnp.random.seed(0)\r\nX = np.random.randn(100, 50)\r\nemp_cov = X.T @ X \/ len(X)\r\n\r\nclf = GraphicalLasso(verbose=True, tol=1e-20).fit(emp_cov)\r\n```\r\n\r\n### Expected Results\r\n\r\nThe expected results would be positive values for the duality gap at all iterations.\r\n\r\n### Actual Results\r\n\r\nHere are the duality gaps for the first 4 iterations : \r\n```\r\n[graphical_lasso] Iteration   0, cost  5.24e+01, dual gap 6.745e-01\r\n[graphical_lasso] Iteration   1, cost  5.24e+01, dual gap -3.808e-05\r\n[graphical_lasso] Iteration   2, cost  5.24e+01, dual gap -4.379e-09\r\n[graphical_lasso] Iteration   3, cost  5.24e+01, dual gap 5.049e-09\r\n\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.6 (main, Oct 24 2022, 16:07:47) [GCC 11.2.0]\r\nexecutable: \/home\/cpouliquen\/anaconda3\/envs\/these-can\/bin\/python3.10\r\n   machine: Linux-5.4.0-148-generic-x86_64-with-glibc2.27\r\n\r\nPython dependencies:\r\n          pip: 22.2.2\r\n   setuptools: 64.0.2\r\n      sklearn: 1.0.2\r\n        numpy: 1.22.3\r\n        scipy: 1.8.1\r\n       Cython: 0.29.33\r\n       pandas: 1.5.1\r\n   matplotlib: 3.5.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n```\r\n```\r\n","labels":["Bug","module:covariance"],"created_at":"2023-05-03T14:53:33Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26321"},{"issue_number":83,"repository":"scikit-learn\/scikit-learn","title":"Cloned estimators have identical randomness but different RNG instances","description":"### Describe the bug\n\nCloned estimators have identical randomness but different RNG instances. According to documentation, it should be the other way around: different randomness but identical RNG instances.\r\n\r\nRelated #25395 \r\n\r\nThe User Guide [says](https:\/\/scikit-learn.org\/stable\/common_pitfalls.html#controlling-randomness):\r\n> For an optimal robustness of cross-validation (CV) results, pass RandomState instances when creating estimators\r\n\r\n> ```python\r\n> rf_inst = RandomForestClassifier(random_state=np.random.RandomState(0))\r\n> cross_val_score(rf_inst, X, y)\r\n> ```\r\n> ...\r\n> Since `rf_inst` was passed a `RandomState` instance, each call to `fit` starts from a different RNG. As a result, the random subset of features will be different for each folds\r\n\r\nIn regards to cloning, the same reference says:\r\n> ```python\r\n> rng = np.random.RandomState(0)\r\n> a = RandomForestClassifier(random_state=rng)\r\n> b = clone(a)\r\n> ```\r\n> Moreover, `a` and `b` will influence each-other since they share the same internal RNG: calling `a.fit` will consume `b`\u2019s RNG, and calling `b.fit` will consume `a`\u2019s RNG, since they are the same. \r\n\r\nThe actual behaviour does not follow this description. \n\n### Steps\/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nfrom sklearn import clone\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.model_selection import cross_validate\r\nfrom sklearn.ensemble import RandomForestClassifier\r\n\r\nrng = np.random.RandomState(0)\r\nX, y = make_classification(random_state=rng)\r\nrf = RandomForestClassifier(random_state=rng)\r\n\r\nd = cross_validate(rf, X, y, return_estimator=True, cv=2)\r\nrngs = [e.random_state for e in d['estimator']]\r\n# estimators corresponding to different CV runs have different but identical RNGs:\r\nprint(rngs[0] is rngs[1]) # False\r\nprint(all(rngs[0].randint(10, size=10) == rngs[1].randint(10, size=10))) # True\r\n\r\nrf_clone = clone(rf)\r\nrngs = [rf.random_state, rf_clone.random_state]\r\nprint(rngs[0] is rngs[1]) # False\r\nprint(all(rngs[0].randint(10, size=10) == rngs[1].randint(10, size=10))) # True\r\n```\n\n### Expected Results\n\nTrue False True False\n\n### Actual Results\n\nFalse True False True\n\n### Versions\n\n```shell\nTested on a two-week-old dev build and also on the following version (Kaggle)\r\n\r\nSystem:\r\n    python: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53)  [GCC 9.4.0]\r\nexecutable: \/opt\/conda\/bin\/python3.7\r\n   machine: Linux-5.15.90+-x86_64-with-debian-bullseye-sid\r\n\r\nPython dependencies:\r\n          pip: 22.3.1\r\n   setuptools: 59.8.0\r\n      sklearn: 1.0.2\r\n        numpy: 1.21.6\r\n        scipy: 1.7.3\r\n       Cython: 0.29.34\r\n       pandas: 1.3.5\r\n   matplotlib: 3.5.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\n```\n","labels":["Bug","Documentation","Needs Decision"],"created_at":"2023-04-11T16:36:07Z","comments":25,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26148"},{"issue_number":84,"repository":"scikit-learn\/scikit-learn","title":"RandomForest not passing feature names to trees and creating warnings.","description":"### Describe the bug\r\n\r\nI fit a decision forest with training data that includes feature names. When I call predict_proba on the forest everything is fine. When I call rf.estimators_[0].predict_proba it will warn that it was not trained with feature names.\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom sklearn.ensemble import RandomForestClassifier\r\n\r\nprint(sklearn.__version__)\r\n\r\ndata = np.random.normal([1,2,3,4,5,6,7,8,9,10], size=(1000, 10))\r\nfeature_names = [f'F{i}' for i in range(10)]\r\ndf = pd.DataFrame(data=data, columns=feature_names)\r\ny = np.ones(1000)\r\ny[500:] = 0\r\n\r\nrf = RandomForestClassifier()\r\nrf.fit(df, y)\r\n\r\nprint(f\"Feature names in Forest: {rf.feature_names_in_} \")\r\n\r\nprint('Classing pred proba on forest')\r\nrf.predict_proba(df)\r\nprint('Done calling pred proba on forest')\r\n\r\n# THIS GIVES A WARNING\r\nrf.estimators_[0].predict_proba(df)\r\n```\r\n\r\n### Expected Results\r\n\r\nNo warnings\r\n\r\n### Actual Results\r\n```\r\nFeature names in Forest: ['F0' 'F1' 'F2' 'F3' 'F4' 'F5' 'F6' 'F7' 'F8' 'F9'] \r\nClassing pred proba on forest\r\nDone calling pred proba on forest\r\n.....lib\/python3.8\/site-packages\/sklearn\/base.py:432: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\r\n  warnings.warn(\r\n```\r\n### Versions\r\n\r\n```shell\r\nsk>>> sklearn.show_versions()\r\n\r\nSystem:\r\n    python: 3.8.16 (default, Mar  2 2023, 03:21:46)  [GCC 11.2.0]\r\nexecutable: ......\/bin\/python\r\n   machine: Linux-5.15.0-60-generic-x86_64-with-glibc2.17\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.2\r\n          pip: 23.0.1\r\n   setuptools: 65.6.3\r\n        numpy: 1.23.1\r\n        scipy: 1.9.0\r\n       Cython: None\r\n       pandas: 1.5.3\r\n   matplotlib: None\r\n       joblib: 1.1.1\r\nthreadpoolctl: 2.2.0\r\n```\r\n","labels":["Bug","Moderate","module:ensemble"],"created_at":"2023-04-10T22:29:00Z","comments":16,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/26140"},{"issue_number":85,"repository":"scikit-learn\/scikit-learn","title":"1.2.1: cannot build documentation without installing module","description":"### Describe the bug\n\nLooks like something is wrong and I cannot build docuemtation without installing module.\r\n\n\n### Steps\/Code to Reproduce\n\nN\/A\n\n### Expected Results\n\nIt should be possible to build documentation out of only what is in source tree a d withoit have installed module.\n\n### Actual Results\n\n<details>\r\n\r\n```console\r\n++ ls -1d lib.linux-x86_64-cpython-38\r\n+ PYTHONPATH=\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/build\/lib.linux-x86_64-cpython-38\r\n+ PBR_VERSION=1.2.1\r\n+ PDM_PEP517_SCM_VERSION=1.2.1\r\n+ SETUPTOOLS_SCM_PRETEND_VERSION=1.2.1\r\n+ \/usr\/bin\/sphinx-build -n -T -b man doc build\/sphinx\/man\r\nRunning Sphinx v6.1.3\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build\/__init__.py\", line 48, in <module>\r\n    from ._check_build import check_build  # noqa\r\nModuleNotFoundError: No module named 'sklearn.__check_build._check_build'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/sphinx\/config.py\", line 351, in eval_config_file\r\n    exec(code, namespace)  # NoQA: S102\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/doc\/conf.py\", line 20, in <module>\r\n    from sklearn.externals._packaging.version import parse\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__init__.py\", line 81, in <module>\r\n    from . import __check_build  # noqa: F401\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build\/__init__.py\", line 50, in <module>\r\n    raise_build_error(e)\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build\/__init__.py\", line 31, in raise_build_error\r\n    raise ImportError(\r\nImportError: No module named 'sklearn.__check_build._check_build'\r\n___________________________________________________________________________\r\nContents of \/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build:\r\n__init__.py               _check_build.pyx          _check_build.c\r\n__pycache__\r\n___________________________________________________________________________\r\nIt seems that scikit-learn has not been built correctly.\r\n\r\nIf you have installed scikit-learn from source, please do not forget\r\nto build the package before using it: run `python setup.py install` or\r\n`make` in the source directory.\r\n\r\nIf you have used an installer, please check that it is suited for your\r\nPython version, your operating system and your platform.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/sphinx\/cmd\/build.py\", line 279, in build_main\r\n    app = Sphinx(args.sourcedir, args.confdir, args.outputdir,\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/sphinx\/application.py\", line 202, in __init__\r\n    self.config = Config.read(self.confdir, confoverrides or {}, self.tags)\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/sphinx\/config.py\", line 173, in read\r\n    namespace = eval_config_file(filename, tags)\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/sphinx\/config.py\", line 364, in eval_config_file\r\n    raise ConfigError(msg % traceback.format_exc()) from exc\r\nsphinx.errors.ConfigError: There is a programmable error in your configuration file:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build\/__init__.py\", line 48, in <module>\r\n    from ._check_build import check_build  # noqa\r\nModuleNotFoundError: No module named 'sklearn.__check_build._check_build'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/sphinx\/config.py\", line 351, in eval_config_file\r\n    exec(code, namespace)  # NoQA: S102\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/doc\/conf.py\", line 20, in <module>\r\n    from sklearn.externals._packaging.version import parse\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__init__.py\", line 81, in <module>\r\n    from . import __check_build  # noqa: F401\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build\/__init__.py\", line 50, in <module>\r\n    raise_build_error(e)\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build\/__init__.py\", line 31, in raise_build_error\r\n    raise ImportError(\r\nImportError: No module named 'sklearn.__check_build._check_build'\r\n___________________________________________________________________________\r\nContents of \/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build:\r\n__init__.py               _check_build.pyx          _check_build.c\r\n__pycache__\r\n___________________________________________________________________________\r\nIt seems that scikit-learn has not been built correctly.\r\n\r\nIf you have installed scikit-learn from source, please do not forget\r\nto build the package before using it: run `python setup.py install` or\r\n`make` in the source directory.\r\n\r\nIf you have used an installer, please check that it is suited for your\r\nPython version, your operating system and your platform.\r\n\r\n\r\nConfiguration error:\r\nThere is a programmable error in your configuration file:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build\/__init__.py\", line 48, in <module>\r\n    from ._check_build import check_build  # noqa\r\nModuleNotFoundError: No module named 'sklearn.__check_build._check_build'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/usr\/lib\/python3.8\/site-packages\/sphinx\/config.py\", line 351, in eval_config_file\r\n    exec(code, namespace)  # NoQA: S102\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/doc\/conf.py\", line 20, in <module>\r\n    from sklearn.externals._packaging.version import parse\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__init__.py\", line 81, in <module>\r\n    from . import __check_build  # noqa: F401\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build\/__init__.py\", line 50, in <module>\r\n    raise_build_error(e)\r\n  File \"\/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build\/__init__.py\", line 31, in raise_build_error\r\n    raise ImportError(\r\nImportError: No module named 'sklearn.__check_build._check_build'\r\n___________________________________________________________________________\r\nContents of \/home\/tkloczko\/rpmbuild\/BUILD\/scikit-learn-1.2.1\/sklearn\/__check_build:\r\n__init__.py               _check_build.pyx          _check_build.c\r\n__pycache__\r\n___________________________________________________________________________\r\nIt seems that scikit-learn has not been built correctly.\r\n\r\nIf you have installed scikit-learn from source, please do not forget\r\nto build the package before using it: run `python setup.py install` or\r\n`make` in the source directory.\r\n\r\nIf you have used an installer, please check that it is suited for your\r\nPython version, your operating system and your platform.\r\n```\r\n<\/details>\n\n### Versions\n\n```shell\n1.2.1\n```\n","labels":["Bug","Documentation","Needs Decision - Include Feature"],"created_at":"2023-03-06T13:10:05Z","comments":22,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25766"},{"issue_number":86,"repository":"scikit-learn\/scikit-learn","title":"Differences in scalar vs vectorized predictions with `GaussianProcessRegressor`","description":"### Describe the bug\r\n\r\nI would expect that calling `GaussianProcessRegressor.predict(X)` with a single X matrix, or with repeated scalar evaluations rows of X should would give nearly the same result, but they don't.\r\n\r\nAn example is given below.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.gaussian_process import GaussianProcessRegressor\r\nfrom sklearn.gaussian_process.kernels import DotProduct, RBF\r\nfrom sklearn.model_selection import train_test_split\r\n\r\n\r\ndata_url = \"http:\/\/lib.stat.cmu.edu\/datasets\/boston\"\r\nraw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\r\ndata = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\r\ntarget = raw_df.values[1::2, 2]\r\n\r\n\r\ndef evaluate_gpr_vec_and_scalar(gpr, X):\r\n    res_vector = gpr.predict(X).squeeze()\r\n    res_scalar = np.array([gpr.predict(xi.reshape(1, -1)) for xi in X]).squeeze()\r\n    return res_scalar, res_vector\r\n\r\n# load\/split data\r\nX_train, X_test, y_train, y_test = train_test_split(data, target, random_state=0)\r\n\r\n# train\r\ngpr = GaussianProcessRegressor(DotProduct() + RBF(), alpha=1.)\r\ngpr.fit(X_train, y_train)\r\n\r\n# predict scalar and vector\r\npred_scalar, pred_vector = evaluate_gpr_vec_and_scalar(gpr, X_test)\r\nerr = pred_scalar - pred_vector\r\nprint(err.max())\r\n```\r\n\r\n### Expected Results\r\n\r\nI would expect the error to be 0 or small - perhaps around machine `eps`, if anything.\r\n\r\n### Actual Results\r\n\r\n`2.1609594114124775e-08`\r\n\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:54) [Clang 13.0.1 ]\r\nexecutable: \/Users\/phil\/miniconda3\/envs\/skl\/bin\/python\r\n   machine: macOS-12.5.1-x86_64-i386-64bit\r\nPython dependencies:\r\n      sklearn: 1.1.1\r\n          pip: 22.3\r\n   setuptools: 65.5.0\r\n        numpy: 1.23.4\r\n        scipy: 1.9.3\r\n       Cython: None\r\n       pandas: 1.5.1\r\n   matplotlib: 3.6.1\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\nBuilt with OpenMP: True\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/Users\/phil\/miniconda3\/envs\/skl\/lib\/libopenblasp-r0.3.21.dylib\r\n        version: 0.3.21\r\nthreading_layer: openmp\r\n   architecture: Nehalem\r\n    num_threads: 8\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libomp\r\n       filepath: \/Users\/phil\/miniconda3\/envs\/skl\/lib\/libomp.dylib\r\n        version: None\r\n    num_threads: 8\r\n```\r\n","labels":["Bug","Needs Investigation","Numerical Stability"],"created_at":"2023-03-03T01:20:51Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25750"},{"issue_number":87,"repository":"scikit-learn\/scikit-learn","title":"Is the check of strict convergence in KMeans too expensive for the benefits ?","description":"### Describe the bug\r\n\r\nIn `KMeans` scikit-learn defines [`strict_convergence`](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/cluster\/_kmeans.py#L701) as the event of producing the same label assignments at two successive iterations.\r\n\r\nWhen this happens, it means convergence for both labels and centroids (set aside possible oscillations due to numerical instability, were the iterations to continue).\r\n\r\nBut checking for strict convergence seems to be somewhat expensive (one loop over the last two label assignments of each sample per iteration), and if the user properly set `tol` it doesn't seem necessary at all ?\r\n\r\nChecking for strict convergence seems to really help when `tol==0`. With `tol==0` I've seen cases of endless oscillations around 0 because of numerical instability, but never reaching 0, and finally terminating at `max_iter` iterations.\r\n\r\nFor the general case, isn't it detrimental to performance though ? one can expect the performance cost to be significant for small dimensions of data, for which an additional pass on a column is marginally more expensive.\r\n\r\nSo I would maybe suggest the following improvements:\r\n\r\n- [ ] enable automatically the strict convergence checks only if `tol==0` (or if `tol` is \"very small\")\r\n- [ ] maybe expose to the user the choice of enabling strict convergence at each iteration ?\r\n\r\n\r\n### Versions\r\n\r\n```shell\r\n1.3\r\n```\r\n","labels":["Bug","module:cluster","Needs Investigation"],"created_at":"2023-02-27T15:53:14Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25716"},{"issue_number":88,"repository":"scikit-learn\/scikit-learn","title":"SequentialFeatureSelector is not working with ColumnTransformer","description":"### Describe the bug\r\n\r\nPlease see the code.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.impute import SimpleImputer\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.compose import ColumnTransformer\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\n\r\n# dummy data\r\nN = 100\r\ndummy_x = pd.DataFrame(\r\n    np.random.randn(N,3),\r\n    columns = list('abc'),\r\n)\r\n\r\ndummy_y = pd.DataFrame(\r\n    np.random.choice([0,1], size= (N,1)),\r\n    columns = ['label'],\r\n)\r\n\r\n\r\nnum_pipeline = Pipeline([\r\n        ('imputer', SimpleImputer(strategy=\"median\")),\r\n        ('std_scaler', StandardScaler()),\r\n    ])\r\n\r\nct_parts = [\r\n                ('num', num_pipeline, [0,1,2]),\r\n]\r\n\r\ndata_preparation_pipe = ColumnTransformer(ct_parts, remainder='passthrough')\r\n\r\n\r\nfrom sklearn.feature_selection import SequentialFeatureSelector\r\nfrom sklearn.ensemble import GradientBoostingClassifier\r\n\r\nmodel = Pipeline(\r\n    [\r\n        # ('data_prep', num_pipeline),\r\n        ('data_prep', data_preparation_pipe),\r\n        ('ML', GradientBoostingClassifier()),\r\n    ]\r\n)\r\n\r\nsfs = SequentialFeatureSelector(\r\n    model,\r\n)\r\n\r\nsfs.fit(dummy_x, dummy_y)\r\n```\r\n\r\n\r\n\r\n\r\n### Expected Results\r\n\r\nNo error\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\nTraceback (most recent call last):\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/IPython\/core\/interactiveshell.py\", line 3460, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"\/tmp\/ipykernel_3433582\/3663298101.py\", line 1, in <module>\r\n    sfs.fit(dummy_x, dummy_y)\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/feature_selection\/_sequential.py\", line 268, in fit\r\n    new_feature_idx, new_score = self._get_best_new_feature_score(\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/feature_selection\/_sequential.py\", line 299, in _get_best_new_feature_score\r\n    scores[feature_idx] = cross_val_score(\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/model_selection\/_validation.py\", line 515, in cross_val_score\r\n    cv_results = cross_validate(\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/model_selection\/_validation.py\", line 285, in cross_validate\r\n    _warn_or_raise_about_fit_failures(results, error_score)\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/model_selection\/_validation.py\", line 367, in _warn_or_raise_about_fit_failures\r\n    raise ValueError(all_fits_failed_message)\r\nValueError: \r\nAll the 5 fits failed.\r\nIt is very likely that your model is misconfigured.\r\nYou can try to debug the error by setting error_score='raise'.\r\n\r\nBelow are more details about the failures:\r\n--------------------------------------------------------------------------------\r\n5 fits failed with the following error:\r\nTraceback (most recent call last):\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/utils\/__init__.py\", line 416, in _get_column_indices\r\n    idx = _safe_indexing(np.arange(n_columns), key)\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/utils\/__init__.py\", line 356, in _safe_indexing\r\n    return _array_indexing(X, indices, indices_dtype, axis=axis)\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/utils\/__init__.py\", line 185, in _array_indexing\r\n    return array[key] if axis == 0 else array[:, key]\r\nIndexError: index 1 is out of bounds for axis 0 with size 1\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/model_selection\/_validation.py\", line 686, in _fit_and_score\r\n    estimator.fit(X_train, y_train, **fit_params)\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/pipeline.py\", line 401, in fit\r\n    Xt = self._fit(X, y, **fit_params_steps)\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/pipeline.py\", line 359, in _fit\r\n    X, fitted_transformer = fit_transform_one_cached(\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/joblib\/memory.py\", line 349, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/pipeline.py\", line 893, in _fit_transform_one\r\n    res = transformer.fit_transform(X, y, **fit_params)\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/utils\/_set_output.py\", line 142, in wrapped\r\n    data_to_wrap = f(self, X, *args, **kwargs)\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/compose\/_column_transformer.py\", line 724, in fit_transform\r\n    self._validate_column_callables(X)\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/compose\/_column_transformer.py\", line 426, in _validate_column_callables\r\n    transformer_to_input_indices[name] = _get_column_indices(X, columns)\r\n  File \"\/software\/anaconda3\/envs\/TOSC_ML\/lib\/python3.10\/site-packages\/sklearn\/utils\/__init__.py\", line 418, in _get_column_indices\r\n    raise ValueError(\r\nValueError: all features must be in [0, 0] or [-1, 0]\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0]\r\nexecutable: \/software\/anaconda3\/envs\/TOSC_ML\/bin\/python\r\n   machine: Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.28\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.1\r\n          pip: 23.0\r\n   setuptools: 67.3.2\r\n        numpy: 1.23.5\r\n        scipy: 1.10.0\r\n       Cython: None\r\n       pandas: 1.5.3\r\n   matplotlib: 3.7.0\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/software\/anaconda3\/envs\/TOSC_ML\/lib\/libopenblasp-r0.3.21.so\r\n        version: 0.3.21\r\nthreading_layer: pthreads\r\n   architecture: Zen\r\n    num_threads: 128\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/software\/anaconda3\/envs\/TOSC_ML\/lib\/libgomp.so.1.0.0\r\n        version: None\r\n    num_threads: 128\r\n```\r\n","labels":["Bug","Moderate","help wanted"],"created_at":"2023-02-27T04:35:40Z","comments":5,"reactions":4,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25711"},{"issue_number":89,"repository":"scikit-learn\/scikit-learn","title":"Multioutput regressors raise ValueError when scoring with `multioutput=\"raw_values\"`","description":"### Describe the bug\r\n\r\nThe goal of the `multioutput=\"raw_values\"` parameter in the regression metrics is to be able to inspect the individual scores of a multioutput metaestimator, but the `_score` function in `_validation.py` expects a number, not the array it actually outputs.\r\n\r\nWe should add an exception to take into account this scenario.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.datasets import make_regression\r\nfrom sklearn.ensemble import HistGradientBoostingRegressor\r\nfrom sklearn.metrics import make_scorer, mean_absolute_error\r\nfrom sklearn.model_selection import cross_validate\r\nfrom sklearn.multioutput import MultiOutputRegressor\r\n\r\nX, Y = make_regression(n_features=10, n_targets=2, random_state=0)\r\nhist_gbdt = HistGradientBoostingRegressor(random_state=0)\r\nmodel = MultiOutputRegressor(hist_gbdt) # RegressorChain fails as well\r\nscoring = {\"MO_MAE\": make_scorer(mean_absolute_error, multioutput=\"raw_values\")}\r\n\r\ncv_results = cross_validate(model, X, Y, scoring=scoring)\r\ncv_results[\"test_MO_MAE\"]\r\n```\r\n\r\n### Expected Results\r\n\r\nArray of shape (`n_outputs`, `n_splits`), in this case shape (2, 5) as using the default KFold cross-validation.\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\nFile ~\/miniforge3\/envs\/joblib-benchmark\/lib\/python3.9\/site-packages\/sklearn\/model_selection\/_validation.py:708, in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\r\n    705 result[\"fit_error\"] = None\r\n    707 fit_time = time.time() - start_time\r\n--> 708 test_scores = _score(estimator, X_test, y_test, scorer, error_score)\r\n    709 score_time = time.time() - start_time - fit_time\r\n    710 if return_train_score:\r\n\r\nFile ~\/miniforge3\/envs\/joblib-benchmark\/lib\/python3.9\/site-packages\/sklearn\/model_selection\/_validation.py:809, in _score(estimator, X_test, y_test, scorer, error_score)\r\n    807                 score = score.item()\r\n    808         if not isinstance(score, numbers.Number):\r\n--> 809             raise ValueError(error_msg % (score, type(score), name))\r\n    810         scores[name] = score\r\n    811 else:  # scalar\r\n\r\nValueError: scoring must return a number, got [110.4080722   83.27384784] (<class 'numpy.ndarray'>) instead. (scorer=MO_MAE)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03)  [GCC 11.3.0]\r\nexecutable: \/home\/arturoamor\/miniforge3\/envs\/joblib-benchmark\/bin\/python3.9\r\n   machine: Linux-5.14.0-1057-oem-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.1\r\n          pip: 23.0.1\r\n   setuptools: 67.3.2\r\n        numpy: 1.24.2\r\n        scipy: 1.10.1\r\n       Cython: None\r\n       pandas: 1.5.3\r\n   matplotlib: 3.7.0\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/home\/arturoamor\/miniforge3\/envs\/joblib-benchmark\/lib\/python3.9\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/arturoamor\/miniforge3\/envs\/joblib-benchmark\/lib\/python3.9\/site-packages\/numpy.libs\/libopenblas64_p-r0-15028c96.3.21.so\r\n        version: 0.3.21\r\nthreading_layer: pthreads\r\n   architecture: SkylakeX\r\n    num_threads: 8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/arturoamor\/miniforge3\/envs\/joblib-benchmark\/lib\/python3.9\/site-packages\/scipy.libs\/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: SkylakeX\r\n    num_threads: 8\r\n```\r\n","labels":["Bug","module:multioutput"],"created_at":"2023-02-23T11:09:26Z","comments":2,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25666"},{"issue_number":90,"repository":"scikit-learn\/scikit-learn","title":"Inflated results on random-data with SVM","description":"### Describe the bug\r\n\r\nWhen trying to train\/evaluate a support vector machine in scikit-learn, I am experiencing some unexpected behaviour and I am wondering whether I am doing something wrong or that this is a possible bug.\r\n\r\nIn a very specific subset of circumstances, namely:\r\n\r\n- `LeaveOneOut() `is used as cross-validation procedure\r\n- The SVM is used, with `probability = True` and a small `C`  such as` 0.01`\r\n- The y labels are balanced (i.e. the mean of y is 0.5)\r\n\r\nThe results of the trained SVM are very good on randomly generated data - while they should be near chance. If the y labels are a bit different, or the SVM is swapped out for a ``LogisticRegression``, it gives expected results (Brier of 0.25, AUC near 0.5). \r\nBut for the named circumstances, the Brier is roughly 0.10 - 0.15 and AUC > 0.9 if the y labels are balanced.\r\n\r\n### Steps\/Code to Reproduce\r\n```python\r\n\r\nfrom sklearn import svm\r\nfrom sklearn.linear_model import LogisticRegression\r\nimport numpy as np\r\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, LeaveOneOut, KFold\r\nfrom sklearn.metrics import roc_auc_score, brier_score_loss\r\nfrom tqdm import tqdm\r\nimport pandas as pd\r\n\r\n\r\nN = 20\r\nN_FEATURES = 50\r\n\r\n\r\nscores = []\r\nfor z in tqdm(range(500)):\r\n    X = np.random.normal(0, 1, size=(N, N_FEATURES))\r\n    y = np.random.binomial(1, 0.5, size=N)\r\n    \r\n    if z < 10:\r\n        y = np.array([0, 1] * int(N\/2))\r\n        y = np.random.permutation(y)\r\n\r\n    y_real, y_pred = [], []\r\n    skf_outer = LeaveOneOut()\r\n    for train_index, test_index in skf_outer.split(X, y):\r\n        X_train, X_test = X[train_index], X[test_index, :]\r\n        y_train, y_test = y[train_index], y[test_index]\r\n\r\n        clf = svm.SVC(probability=True, C=0.01)\r\n\r\n        clf.fit(X_train, y_train)\r\n        predictions = clf.predict_proba(X_test)[:, 1]\r\n\r\n        y_pred.extend(predictions)\r\n        y_real.extend(y_test)\r\n\r\n    scores.append([np.mean(y), \r\n                   brier_score_loss(np.array(y_real), np.array(y_pred)), \r\n                   roc_auc_score(np.array(y_real), np.array(y_pred))])\r\n\r\ndf_scores = pd.DataFrame(scores)\r\ndf_scores.columns = ['y_label', 'brier', 'auc']\r\ndf_scores['y_0.5'] = df_scores['y_label'] == 0.5\r\ndf_scores = df_scores.groupby(['y_0.5']).mean()\r\nprint(df_scores)\r\n```\r\n\r\n### Expected Results\r\n\r\nI would expect that all results would be somewhat similar, with a Brier ~0.25 and AUC ~0.5.\r\n\r\n### Actual Results\r\n\r\n```\r\n        y_label     brier       auc\r\ny_0.5                              \r\nFalse  0.514649  0.298204  0.216884\r\nTrue   0.500000  0.159728  0.999080\r\n```\r\n\r\nHere, you can see that if the ``np.mean`` of the ``y_labels`` is 0.5, the results are actually really really good.\r\nWhile the data is randomly generated for 500 times\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\ProgramData\\Anaconda3\\envs\\test\\python.exe\r\n   machine: Windows-10-10.0.19044-SP0\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.2.2\r\n   setuptools: 61.2.0\r\n        numpy: 1.19.5\r\n        scipy: 1.10.0\r\n       Cython: 0.29.14\r\n       pandas: 1.4.4\r\n   matplotlib: 3.6.3\r\n       joblib: 1.2.0\r\nthreadpoolctl: 2.2.0\r\nBuilt with OpenMP: True\r\nthreadpoolctl info:\r\n       filepath: C:\\ProgramData\\Anaconda3\\envs\\test\\Library\\bin\\mkl_rt.1.dll\r\n         prefix: mkl_rt\r\n       user_api: blas\r\n   internal_api: mkl\r\n        version: 2021.4-Product\r\n    num_threads: 8\r\nthreading_layer: intel\r\n       filepath: C:\\Users\\manuser\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy.libs\\libopenblas-802f9ed1179cb9c9b03d67ff79f48187.dll\r\n         prefix: libopenblas\r\n       user_api: blas\r\n   internal_api: openblas\r\n        version: 0.3.18\r\n    num_threads: 16\r\nthreading_layer: pthreads\r\n   architecture: Prescott\r\n       filepath: C:\\ProgramData\\Anaconda3\\envs\\test\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n         prefix: vcomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 8\r\n       filepath: C:\\ProgramData\\Anaconda3\\envs\\test\\Library\\bin\\libiomp5md.dll\r\n         prefix: libiomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 8\r\n       filepath: C:\\Users\\manuser\\AppData\\Roaming\\Python\\Python38\\site-packages\\mxnet\\libopenblas.dll\r\n         prefix: libopenblas\r\n       user_api: blas\r\n   internal_api: openblas\r\n        version: None\r\n    num_threads: 16\r\nthreading_layer: pthreads\r\n   architecture: Prescott\r\n       filepath: C:\\ProgramData\\Anaconda3\\envs\\test\\Lib\\site-packages\\torch\\lib\\libiomp5md.dll\r\n         prefix: libiomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 16\r\n       filepath: C:\\ProgramData\\Anaconda3\\envs\\test\\Lib\\site-packages\\torch\\lib\\libiompstubs5md.dll\r\n         prefix: libiomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 1\r\n```\r\n","labels":["Bug","module:svm","Needs Investigation"],"created_at":"2023-02-17T14:45:57Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25631"},{"issue_number":91,"repository":"scikit-learn\/scikit-learn","title":"KernelDensity incorrect handling of bandwidth","description":"### Describe the bug\r\n\r\nI was using kernel density estimator\r\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KernelDensity.html\r\nusing 'silverman' or 'scott' as the bandwidth argument. Then I found that the bandwidth automatically adjusted by the algorithm is independent of the actual scale of the dataset. In fact, I was shocked to find that the calculation of a bandwidth in https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/neighbors\/_kde.py for 'silverman' and 'scott' does not check the scales of data at all. \r\n\r\nSuppose I fit the model `kde` to some 2D data `X` and get the bandwidth as `kde.bandwidth_`.\r\nNext, I fit the model `kde` to the same 2D data `X` but with all elements multiplied by, say, 20 and get the bandwidth as `kde.bandwidth_`. \r\nI found that these two values of `kde.bandwidth_` are equal (it is calculated from the shape of `X`, see the source code). But obviously they should differ by a factor of 20 if the bandwidth is really computed in a truly adaptive manner.\r\n\r\nFor your reference, I want to mention that scipy's KDE https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.gaussian_kde.html calculates the covariance of data to extract the scale of data. I think this is the right thing to do.\r\n\r\nNote that if the bandwidth is incorrect, everything else is incorrect too, including probablities of samples, etc.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.neighbors import KernelDensity\r\nX = np.random.randn(1000, 2)\r\nkde = KernelDensity(bandwidth='scott')\r\nkde.fit(X)\r\nprint(kde.bandwidth_)\r\n\r\nkde.fit(X * 20)\r\nprint(kde.bandwidth_)\r\n```\r\n\r\n### Expected Results\r\n\r\nDifferent bandwidths for data sets with different scales.\r\n\r\n### Actual Results\r\n\r\n0.31622776601683794\r\n0.31622776601683794\r\n\r\n### Versions\r\n\r\n```shell\r\n1.2.1\r\n```\r\n","labels":["Bug","module:neighbors"],"created_at":"2023-02-16T06:16:04Z","comments":6,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25623"},{"issue_number":92,"repository":"scikit-learn\/scikit-learn","title":"KBinsDiscretizer creates wrong bins.","description":"### Describe the bug\r\n\r\n`KBinsDiscretizer` gives wrong bins and wrong transformed data, when the inputs contains only 2 distinct values, and `n_bins=3`. It only produces 1 bin, which is not expected.  The warning shows some bins are too small so they are merged.\r\n\r\nNote that when setting `n_bins=2`, `KBinsDiscretizer` gives more reasonable bins and correct results. However, from the `bin_edges_`, it seems `n_bins=2` is handled separately.\r\n\r\n### My guess about the cause\r\nI think the root cause is inconsistent handling of `-inf` and `+inf` in fitting and transforming. \r\n\r\nIn fitting, `-inf` and `+inf` are not considered when removing duplicated bin edges. But in transforming data, the first and last bin edges are replaces with `-inf` and `+inf`. Such differences obviously removes some necessary bins.\r\n\r\nTaking the case I mentioned as a example, current implementation deduplicates bin edges from `[-1.0, -1.0, 0.0, 0.0]` to `[-1.0, 0.0]`.  Then in transforming, all real values are actually in one bin, since the bin edges are replaced with `-inf` and `+inf`.\r\n\r\nA more reasonable conversion is replacing first and last bin edges with `-inf` and `+inf` in fitting, i.e., `[-inf, -1.0, 0.0, inf]`, then removing duplicated edges, which gives `[-inf, -1.0, 0.0, inf]`. This will give expected results.\r\n\r\n\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```\r\nfrom sklearn.preprocessing import KBinsDiscretizer\r\nX = [[-1], [-1], [ 0], [ 0]]\r\ndiscretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')\r\ndiscretizer.fit(X)\r\n\r\nprint(discretizer.bin_edges_)\r\nXt = discretizer.transform(X)\r\nprint(Xt)\r\n```\r\n\r\n### Expected Results\r\n\r\nThe last `print` should give \r\n```\r\n[[0.]\r\n [0.]\r\n [1.]\r\n [1.]]\r\n```\r\nor \r\n\r\n```\r\n[[1.]\r\n [1.]\r\n [2.]\r\n [2.]]\r\n```\r\n. It depends on how to implement it correctly.\r\n\r\n### Actual Results\r\n\r\nThe last `print` gives\r\n```\r\n[[0.]\r\n [0.]\r\n [0.]\r\n [0.]]\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.8.16 (default, Jan 17 2023, 16:42:09)  [Clang 14.0.6 ]\r\nexecutable: \/opt\/anaconda3\/envs\/sklearn\/bin\/python3\r\n   machine: macOS-10.16-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.1\r\n          pip: 22.3.1\r\n   setuptools: 65.6.3\r\n        numpy: 1.24.2\r\n        scipy: 1.10.0\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: None\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libomp\r\n       filepath: \/opt\/anaconda3\/envs\/sklearn\/lib\/python3.8\/site-packages\/sklearn\/.dylibs\/libomp.dylib\r\n        version: None\r\n    num_threads: 10\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/opt\/anaconda3\/envs\/sklearn\/lib\/python3.8\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib\r\n        version: 0.3.21\r\nthreading_layer: pthreads\r\n   architecture: Nehalem\r\n    num_threads: 10\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/opt\/anaconda3\/envs\/sklearn\/lib\/python3.8\/site-packages\/scipy\/.dylibs\/libopenblas.0.dylib\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Nehalem\r\n    num_threads: 10\r\n```\r\n```\r\n","labels":["Bug","Needs Decision","module:preprocessing"],"created_at":"2023-02-13T02:51:44Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25594"},{"issue_number":93,"repository":"scikit-learn\/scikit-learn","title":"Importing BaseEstimator leads to unnecessary memory usage","description":"### Describe the bug\n\nImporting `BaseEstimator` from `sklearn.base` causes a cascade of imports that leads to unnecessary memory usage (500MiB of stuff at peak, see screenshot below).\r\n\r\n![Screenshot from 2023-02-10 19-23-53](https:\/\/user-images.githubusercontent.com\/1296726\/218169206-d8e940e9-40ba-458d-8047-deacf2f30521.png)\r\n\n\n### Steps\/Code to Reproduce\n\n```python\r\nfrom sklearn.base import BaseEstimator\r\n```\n\n### Expected Results\n\nI would expect such a basic class to import some utilities around, but nothing major.\n\n### Actual Results\n\nThe code ends up importing a large amount of unnecessary and large modules, the full chain to some point is:\r\n - code -> `from sklearn.base import BaseEstimator`\r\n - sklearn\/\\_\\_init\\_\\_.py:82 -> `from .base import clone`\r\n - sklearn\/base.py:17 -> `from .utils import _IS_32BIT`\r\n - sklearn\/utils\/\\_\\_init\\_\\_.py:21 -> `from scipy.sparse import issparse` (This is the most useless import, as the issparse function is a oneliner that calls isinstance)\r\n - scipy\/sparse\/\\_\\_init\\_\\_.py:283 -> `from . import csgraph`\r\n - scipy\/sparse\/csgraph\/\\_\\_init\\_\\_.py:185 -> `from ._laplacian import laplacian`\r\n - scipy\/sparse\/csgraph\/_laplacian.py:7 -> `from scipy.sparse.linalg import LinearOperator`\r\n - scipy\/sparse\/linalg\/\\_\\_init\\_\\_.py:120 -> `from ._isolve import *`\r\n - scipy\/sparse\/linalg\/_isolve\/\\_\\_init\\_\\_.py:4 -> `from .iterative import *`\r\n - scipy\/sparse\/linalg\/_isolve\/iterative.py:9 -> `from . import _iterative`\r\n\r\nI guess the issue I am reporting is that importing a simple estimator base class should not cause so many unrelated imports and spike memory usage by doing so.\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]\r\nexecutable: \/home\/jjan\/dev\/sec-certs-page\/virt\/bin\/python\r\n   machine: Linux-6.0.12-arch1-1-x86_64-with-glibc2.36\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.1\r\n          pip: 23.0\r\n   setuptools: 61.2.0\r\n        numpy: 1.22.3\r\n        scipy: 1.10.0\r\n       Cython: None\r\n       pandas: 1.4.2\r\n   matplotlib: 3.5.1\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/home\/jjan\/dev\/sec-certs-page\/virt\/lib\/python3.10\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 16\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/jjan\/dev\/sec-certs-page\/virt\/lib\/python3.10\/site-packages\/numpy.libs\/libopenblas64_p-r0-2f7c42d4.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Zen\r\n    num_threads: 16\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/jjan\/dev\/sec-certs-page\/virt\/lib\/python3.10\/site-packages\/scipy.libs\/libopenblasp-r0-41284840.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Zen\r\n    num_threads: 16\n```\n","labels":["Bug","Needs Investigation"],"created_at":"2023-02-10T18:42:11Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25590"},{"issue_number":94,"repository":"scikit-learn\/scikit-learn","title":"set_output API do not preserve original dtypes for pandas","description":"### Describe the bug\n\nFollowing issue #24182,\r\n\r\nWhen using the set_output with expected output to be a pandas' data frame, while converting tougher columns with different dtypes the output does not preserve the original dtype but the \"common type\" by numpy.\r\n\r\nPossible workaround is to create transformer for each column, but it doesn't feel like the right approach.\r\n\r\nIn collaboration with @BenEfrati\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.impute import SimpleImputer\r\n\r\n\r\n\r\nX = pd.DataFrame({\r\n                  \"age\": [1, 2, 4],\r\n                  \"float\": [1.5, 2.5, None],\r\n                  })\r\nX[\"age\"] = X[\"age\"].astype(np.uint8)\r\nX.dtypes\r\n\r\n# age        uint8\r\n# float    float64\r\n# dtype: object\r\n\r\nsimple_imputer = SimpleImputer(strategy='constant', fill_value=1).set_output(transform=\"pandas\")\r\nX_trans_partial = simple_imputer.fit_transform(X)\r\nX_trans_partial.dtypes\r\n\r\n# age      float64\r\n# float    float64\r\n# dtype: object\r\n```\n\n### Expected Results\n\nWe'd expect the age column to keep being an unsigned 8-bit integer.\n\n### Actual Results\n\nthe age column is now a 64-bit floating point.\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\New folder\\venv\\Scripts\\python.exe\r\n   machine: Windows-10-10.0.19044-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.1\r\n          pip: 23.0\r\n   setuptools: 58.1.0\r\n        numpy: 1.24.2\r\n        scipy: 1.10.0\r\n       Cython: None\r\n       pandas: 1.5.3\r\n   matplotlib: None\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\New folder\\venv\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\r\n        version: 0.3.21\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: vcomp\r\n       filepath: C:\\New folder\\venv\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\New folder\\venv\\Lib\\site-packages\\scipy.libs\\libopenblas-802f9ed1179cb9c9b03d67ff79f48187.dll\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\n```\n","labels":["Bug","module:impute","Needs Decision - Include Feature"],"created_at":"2023-02-07T08:52:18Z","comments":6,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25560"},{"issue_number":95,"repository":"scikit-learn\/scikit-learn","title":"Enable feature selectors to pass pandas DataFrame to estimator","description":"### Describe the workflow you want to enable\r\n\r\nWhen running SequentialFeatureSelector (or, presumably, other feature selection methods) with a pandas DataFrame input, the reduced-feature input is passed to the estimator as a numpy array. This seems inconsistent with the other transformers that successfully maintain the pandas indices.\r\n\r\nMy actual use case is the following: Select the best k features that complement a set of fixed features. I was going to implement it like this: (happy to take suggestions for alternatives).\r\n\r\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\nimport sklearn\r\nfrom sklearn.feature_selection import SequentialFeatureSelector\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.preprocessing import FunctionTransformer\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.pipeline import Pipeline\r\n\r\nX=pd.DataFrame(np.random.rand(10,3))\r\ny=pd.Series(np.random.rand(10))\r\n\r\nfixed_features=pd.DataFrame(np.random.rand(X.shape[0],2))\r\nappender= FunctionTransformer(lambda X:pd.concat([X,fixed_features.reindex(X.index)],axis=1))\r\nX.shape,appender.fit_transform(X).shape\r\n```\r\n\r\n    ((10, 3), (10, 5))\r\n\r\n\r\n```python\r\nLR_with_fixed=LR=Pipeline(steps=[\r\n    ('append',appender),\r\n    ('LR',LinearRegression())\r\n])\r\nSFS_with_fixed = SequentialFeatureSelector(\r\n    LR_with_fixed,n_features_to_select=1, direction=\"forward\"\r\n)\r\n_=SFS_with_fixed.fit(X,y)\r\n```\r\n\r\n    ValueError: \r\n    All the 5 fits failed.\r\n    .....\r\n      File \"\/tmp\/ipykernel_275\/897617596.py\", line 2, in <lambda>\r\n        appender= FunctionTransformer(lambda X:pd.concat([X,fixed_features.reindex(X.index)],axis=1))\r\n    AttributeError: 'numpy.ndarray' object has no attribute 'index'\r\n\r\n\r\n\r\n### Describe your proposed solution\r\n\r\nI believe that it should be possible to pass the reduced matrix to the estimator as a pandas DataFrame, and this would be consistent with the way other Transformers work currently.\r\n\r\n### Describe alternatives you've considered, if relevant\r\n\r\nAn alternative that would suffice for my particular use case is to directly pass a set of fixed features as a parameter to the feature selector.\r\n\r\n### Additional context\r\n\r\n_No response_","labels":["Bug"],"created_at":"2023-01-26T17:11:58Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25492"},{"issue_number":96,"repository":"scikit-learn\/scikit-learn","title":"SVC and OneClassSVM fails to fit or have wrong fitted attributes with null sample weights","description":"### Describe the bug\n\n*SVC().fit(X, y, w)* fails when the targets *y* are multiclass and the sample_weights *w* zero out one of the classes.\r\n* Dense *X* produces incorrect arrays for *support_*, *n_support_*, and *dual_coef_* attributes, some with wrong dimension.\r\n* Sparse *X* errors out when trying to construct sparse *dual_coef_* from incompatible arguments.\r\n\r\nA warning is emitted (*e.g.*, \"class label 0 specified in weight is not found\"), but it does not indicate that the arrays on the trained SVC object are incorrect.\r\n\r\nSeems to be a case that was not tested by PR #14286.\r\n\r\n### Workaround\r\n\r\nReplace the zero weights (or negative weights) with very small values like 1e-16. \n\n### Steps\/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nfrom sklearn.svm import SVC\r\n\r\nX = np.array([[0., 0.], [1., 0.], [0., 1.]])   # or sp.csr_matrix(...)\r\ny = [0, 1, 2]\r\nw = [0., 1., 1.]                               # class 0 has zero weight\r\nclf = SVC().fit(X, y, w)\r\n```\n\n### Expected Results\n\nThe fitted attributes should be\r\n```\r\nclasses_: [0 1 2]\r\nsupport_: [1 2]\r\nsupport_vectors_: [[1. 0.]\r\n                   [0. 1.]]\r\nn_support_: [0 1 1]\r\ndual_coef_: [[0. 0. 0.]\r\n             [0. 1. -1.]]\r\n```\r\nassuming the 'arbitrary' values in *dual_coef_* are set to zero.\n\n### Actual Results\n\nFor dense *X*, the fitted attributes are actually\r\n```\r\nclasses_: [0 1 2]\r\nsupport_: [0 1]              <-- should be [1 2]\r\nsupport_vectors_: [[1. 0.]\r\n                   [0. 1.]]\r\nn_support_: [1 1]            <-- should be [0 1 1]\r\ndual_coef_: [[ 1. -1.]]      <-- should be [[0. 0. 0.] [0. 1. -1.]]\r\n```\r\nFor sparse *X* it raises\r\n```\r\nValueError: indices and data should have the same size\r\n```\r\nwith traceback\r\n```\r\nsklearn\/svm\/_base.py:252, in fit(self, X, y, sample_weight)\r\n--> 252 fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)\r\n\r\nsklearn\/svm\/_base.py:413, in _sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed)\r\n--> 413     self.dual_coef_ = sp.csr_matrix(\r\n    414         (dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV)\r\n    415     )\r\n\r\nscipy\/sparse\/_compressed.py:106, in _cs_matrix.__init__(self, arg1, shape, dtype, copy)\r\n--> 106 self.check_format(full_check=False)\r\n\r\nscipy\/sparse\/_compressed.py:176, in _cs_matrix.check_format(self, full_check)\r\n    174 # check index and data arrays\r\n    175 if (len(self.indices) != len(self.data)):\r\n--> 176     raise ValueError(\"indices and data should have the same size\")\r\n```\r\n\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:55:37)  [Clang 14.0.6 ]\r\nexecutable: miniconda3\/bin\/python\r\n   machine: macOS-13.1-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 22.3.1\r\n   setuptools: 65.6.3\r\n        numpy: 1.24.1\r\n        scipy: 1.10.0\r\n       Cython: 0.29.33\r\n       pandas: 1.5.2\r\n   matplotlib: 3.6.2\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: mkl\r\n         prefix: libmkl_rt\r\n       filepath: miniconda3\/lib\/libmkl_rt.dylib\r\n        version: 2020.0.4\r\nthreading_layer: intel\r\n    num_threads: 8\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libomp\r\n       filepath: miniconda3\/lib\/libomp.dylib\r\n        version: None\r\n    num_threads: 16\n```\n","labels":["Bug","help wanted","Hard"],"created_at":"2023-01-12T19:40:35Z","comments":12,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25380"},{"issue_number":97,"repository":"scikit-learn\/scikit-learn","title":"Segmentation fault occurs when KernelPCA is loaded after torchvision","description":"### Describe the bug\n\nIf KernelPCA is loaded after torchvision, then I get a segmentation fault. If I load them in the opposite order then I get no segmentaton fault.\n\n### Steps\/Code to Reproduce\n\n```\r\nimport torchvision\r\nfrom sklearn.decomposition import KernelPCA\r\n\r\nimport numpy as np\r\n\r\ntransformer = KernelPCA(n_components=7, kernel='rbf')\r\nX_transformed = transformer.fit_transform(np.zeros([100,100]))\r\n```\n\n### Expected Results\n\nNo segmentation fault\n\n### Actual Results\n\nSegmentation fault (core dumped)\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0]\r\nexecutable: \/usr\/bin\/python3\r\n   machine: Linux-5.4.0-77-generic-x86_64-with-glibc2.29\r\n\r\nPython dependencies:\r\n          pip: 22.1\r\n   setuptools: 45.2.0\r\n      sklearn: 0.24.2\r\n        numpy: 1.22.4\r\n        scipy: 1.4.1\r\n       Cython: 0.29.23\r\n       pandas: 1.4.3\r\n   matplotlib: 3.5.2\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\n\r\ntorchvision version: 0.10.0+cu102\n```\n","labels":["Bug","module:decomposition"],"created_at":"2022-12-19T17:04:31Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25213"},{"issue_number":98,"repository":"scikit-learn\/scikit-learn","title":"Sparse data representations results in worse models than dense data for some classifiers","description":"### Describe the bug\r\n\r\nUsing scipy sparse matrices with sklearn LogisticRegression greatly improves speed and therefore is desirable in many scenarios.\r\n\r\nHowever, it appears that sparse versus dense data representations yield different (worse) results for some sklearn classifiers.\r\n\r\nMy perhaps naive assumption is that sparse versus dense is just a method of representing the data and operations performed on the sparse or dense data (including model training) should yield identical or nearly identical results.\r\n\r\nA notebook gist looking at sparse versus dense results for nine solvers can be found here: https:\/\/gist.github.com\/mmulthaup\/db619d8b5ea4baf4a00153b055a7e9a8\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```py\r\n#Minimal example\r\nimport sklearn\r\nimport scipy\r\nimport numpy as np\r\n \r\n#Artificial data\r\ny = np.repeat(1,100).tolist()+np.repeat(0,100).tolist()\r\nX = np.concatenate([scipy.stats.poisson.rvs(0.2,size=[100,1000]),scipy.stats.poisson.rvs(0.1,size=[100,1000])])\r\n \r\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\r\n    X, y, test_size=0.5, random_state=42)\r\nX_train_sparse = scipy.sparse.bsr_array(X_train)\r\nX_test_sparse = scipy.sparse.bsr_array(X_test)\r\n \r\n#Modeling\r\nmodel = sklearn.linear_model.LogisticRegression(solver=\"saga\",random_state=42,max_iter=4000)\r\ndense_scores = model.fit(X_train,y_train).predict_proba(X_test)[:,1]\r\nsparse_scores = model.fit(X_train_sparse,y_train).predict_proba(X_test_sparse)[:,1]\r\n\r\nprint(f\"Dense AUC: {round(sklearn.metrics.roc_auc_score(y_test,dense_scores),3)}\") #Dense AUC: 1.0\r\nprint(f\"Sparse AUC: {round(sklearn.metrics.roc_auc_score(y_test,sparse_scores),3)}\") #Sparse AUC: 0.584\r\n```\r\n\r\n### Expected Results\r\n\r\nDense AUC: 1.0\r\nSparse AUC: 1.0\r\n\r\n### Actual Results\r\n\r\nDense AUC: 1.0\r\nSparse AUC: 0.584\r\n\r\n\r\n### Versions\r\n\r\n```shell\r\nException ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7fc557a3f310>\r\nTraceback (most recent call last):\r\n  File \"\/databricks\/python\/lib\/python3.9\/site-packages\/threadpoolctl.py\", line 400, in match_module_callback\r\n    self._make_module_from_path(filepath)\r\n  File \"\/databricks\/python\/lib\/python3.9\/site-packages\/threadpoolctl.py\", line 515, in _make_module_from_path\r\n    module = module_class(filepath, prefix, user_api, internal_api)\r\n  File \"\/databricks\/python\/lib\/python3.9\/site-packages\/threadpoolctl.py\", line 606, in __init__\r\n    self.version = self.get_version()\r\n  File \"\/databricks\/python\/lib\/python3.9\/site-packages\/threadpoolctl.py\", line 646, in get_version\r\n    config = get_config().split()\r\nAttributeError: 'NoneType' object has no attribute 'split'\r\n\r\nSystem:\r\n    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]\r\nexecutable: \/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-0c18486f-171e-4bd6-9fb6-691f3a2da533\/bin\/python\r\n   machine: Linux-5.4.0-1088-aws-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.0\r\n          pip: 21.2.4\r\n   setuptools: 61.2.0\r\n        numpy: 1.23.5\r\n        scipy: 1.9.3\r\n       Cython: 0.29.28\r\n       pandas: 1.5.2\r\n   matplotlib: 3.5.1\r\n       joblib: 1.2.0\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       filepath: \/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-0c18486f-171e-4bd6-9fb6-691f3a2da533\/lib\/python3.9\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n         prefix: libgomp\r\n       user_api: openmp\r\n   internal_api: openmp\r\n        version: None\r\n    num_threads: 16\r\n\r\n       filepath: \/local_disk0\/.ephemeral_nfs\/envs\/pythonEnv-0c18486f-171e-4bd6-9fb6-691f3a2da533\/lib\/python3.9\/site-packages\/scipy.libs\/libopenblasp-r0-41284840.3.18.so\r\n         prefix: libopenblas\r\n       user_api: blas\r\n   internal_api: openblas\r\n        version: 0.3.18\r\n    num_threads: 16\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n```\r\n","labels":["Bug","module:linear_model"],"created_at":"2022-12-16T14:53:20Z","comments":5,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25198"},{"issue_number":99,"repository":"scikit-learn\/scikit-learn","title":"Instability in fastica for float32","description":"Originally seen in https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24131#issuecomment-1208091119 for more details on a single random seed and only for Atlas but I have managed to reproduce with OpenBLAS see below.\r\n\r\nFor now we have a work-around for the CI in #24198 but it is still an issue and in an ideal world, someone would investigate and try to see whether it is fixable.\r\n\r\nOther possibly related issues and PRs are mentioned in https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/24198#issuecomment-1252616127: https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/2735 and https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/2738.\r\n\r\nHere is a snippet that reproduces the same problem on OpenBLAS:\r\n\r\n```py\r\nimport numpy as np\r\nfrom scipy import stats\r\nfrom sklearn.decomposition import fastica\r\nimport sys\r\n\r\nglobal_random_seed = 20\r\nglobal_dtype = np.float32\r\n\r\n\r\ndef center_and_norm(x, axis=-1):\r\n    x = np.rollaxis(x, axis)\r\n    x -= x.mean(axis=0)\r\n    x \/= x.std(axis=0)\r\n\r\n\r\nrng = np.random.RandomState(global_random_seed)\r\nn_samples = 1000\r\n# Generate two sources:\r\ns1 = (2 * np.sin(np.linspace(0, 100, n_samples)) > 0) - 1\r\ns2 = stats.t.rvs(1, size=n_samples, random_state=global_random_seed)\r\ns = np.c_[s1, s2].T\r\ncenter_and_norm(s)\r\ns = s.astype(global_dtype)\r\ns1, s2 = s\r\n\r\n# Mixing angle\r\nphi = 0.6\r\nmixing = np.array([[np.cos(phi), np.sin(phi)], [np.sin(phi), -np.cos(phi)]])\r\nmixing = mixing.astype(global_dtype)\r\nm = np.dot(mixing, s)\r\ncenter_and_norm(m)\r\n\r\nalgo = 'deflation'\r\nnl = 'logcosh'\r\nwhiten = 'arbitrary-variance'\r\n\r\nproblematic_random_state = 13441\r\n\r\nk_, mixing_, s_ = fastica(\r\n    m.T, fun=nl, whiten=whiten, algorithm=algo, random_state=problematic_random_state\r\n)\r\n```\r\n\r\nYou get a warning with a division by zero:\r\n```\r\n\/home\/lesteve\/dev\/scikit-learn\/sklearn\/decomposition\/_fastica.py:89: RuntimeWarning: invalid value encountered in divide\r\n  w1 \/= np.sqrt((w1**2).sum())\r\n```\r\n\r\nAnd then the following traceback because NaNs are passed into a BLAS routine eventually:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In [1], line 39\r\n     35 whiten = 'arbitrary-variance'\r\n     37 problematic_random_state = 13441 # both for openblas and atlas (also numpy < and >= 1.23)\r\n---> 39 k_, mixing_, s_ = fastica(\r\n     40     m.T, fun=nl, whiten=whiten, algorithm=algo, random_state=problematic_random_state\r\n     41 )\r\n\r\nFile ~\/dev\/scikit-learn\/sklearn\/decomposition\/_fastica.py:322, in fastica(X, n_components, algorithm, whiten, fun, fun_args, max_iter, tol, w_init, whiten_solver, random_state, return_X_mean, compute_sources, return_n_iter)\r\n    174 \"\"\"Perform Fast Independent Component Analysis.\r\n    175 \r\n    176 The implementation is based on [1]_.\r\n   (...)\r\n    308        pp. 411-430.\r\n    309 \"\"\"\r\n    310 est = FastICA(\r\n    311     n_components=n_components,\r\n    312     algorithm=algorithm,\r\n   (...)\r\n    320     random_state=random_state,\r\n    321 )\r\n--> 322 S = est._fit_transform(X, compute_sources=compute_sources)\r\n    324 if est._whiten in [\"unit-variance\", \"arbitrary-variance\"]:\r\n    325     K = est.whitening_\r\n\r\nFile ~\/dev\/scikit-learn\/sklearn\/decomposition\/_fastica.py:683, in FastICA._fit_transform(self, X, compute_sources)\r\n    680 else:\r\n    681     self.components_ = W\r\n--> 683 self.mixing_ = linalg.pinv(self.components_, check_finite=False)\r\n    684 self._unmixing = W\r\n    686 return S\r\n\r\nFile ~\/miniconda3\/lib\/python3.9\/site-packages\/scipy\/linalg\/_basic.py:1304, in pinv(a, atol, rtol, return_rank, check_finite, cond, rcond)\r\n   1231 \"\"\"\r\n   1232 Compute the (Moore-Penrose) pseudo-inverse of a matrix.\r\n   1233 \r\n   (...)\r\n   1301 \r\n   1302 \"\"\"\r\n   1303 a = _asarray_validated(a, check_finite=check_finite)\r\n-> 1304 u, s, vh = _decomp_svd.svd(a, full_matrices=False, check_finite=False)\r\n   1305 t = u.dtype.char.lower()\r\n   1306 maxS = np.max(s)\r\n\r\nFile ~\/miniconda3\/lib\/python3.9\/site-packages\/scipy\/linalg\/_decomp_svd.py:133, in svd(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\r\n    131     raise LinAlgError(\"SVD did not converge\")\r\n    132 if info < 0:\r\n--> 133     raise ValueError('illegal value in %dth argument of internal gesdd'\r\n    134                      % -info)\r\n    135 if compute_uv:\r\n    136     return u, s, v\r\n\r\nValueError: illegal value in 4th argument of internal gesdd\r\n```","labels":["Bug","module:decomposition"],"created_at":"2022-11-25T10:00:12Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/25038"},{"issue_number":100,"repository":"scikit-learn\/scikit-learn","title":"forced threading joblib backend in pairwise_distances","description":"### Describe the bug\r\n\r\nThe function pairwise_distances in pairwise.py comes with a forced \"threading\" joblib backend.\r\nThis slows down the system if one uses a python callable as distance function, due to the global interpreter lock - in practice there is no parallelism.\r\n\r\nIs there a reason for this choice?\r\n\r\nIn my opition the backend should be different if the metric is a custom callable, or at least, the user should be able to force a different one.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\nInvoke pairwise_distance with a callable as metric and n_jobs > 1\r\n\r\n```\r\ndef myCustomDistanceWrittenInPython(p0, p1):\r\n    return myhexoticmetric\r\n\r\nwith parallel_backend('loky', n_jobs=n_jobs):\r\n    pairwise_distances(X, metric=myCustomDistanceWrittenInPython)\r\n```\r\n\r\n### Expected Results\r\n\r\njoblib creates n_jobs processes and runs parallel\r\n\r\n### Actual Results\r\n\r\nthe global interpreter lock of python prevents a correct parllelization\r\n\r\n### Versions\r\n\r\n```shell\r\nsklearn main branch after commit e947074f63c6602ae15cd57b1fa4f6658040cff7 of Fri Nov 11 05:10:31 2022 -0500\r\n```\r\n","labels":["Bug","module:metrics"],"created_at":"2022-11-11T13:39:48Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24896"},{"issue_number":101,"repository":"scikit-learn\/scikit-learn","title":"Gaussian process `log_marginal_likelihood()` `theta` parameter: pass as `log(theta)`?","description":"### Discussed in https:\/\/github.com\/scikit-learn\/scikit-learn\/discussions\/24765\r\n\r\nI opened this as a discussion, but it is probably more suited for an issue, as it may lead to a documentation fix.\r\n\r\nSystem Info\r\n\r\n```\r\nSystem:\r\n    python: 3.10.7 (main, Sep  8 2022, 14:34:29) [GCC 12.2.0]\r\nexecutable: \/usr\/bin\/python3\r\n   machine: Linux-5.19.0-1-amd64-x86_64-with-glibc2.35\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.2\r\n          pip: 22.2\r\n   setuptools: 59.6.0\r\n        numpy: 1.21.5\r\n        scipy: 1.8.1\r\n       Cython: 0.29.32\r\n       pandas: 1.3.5\r\n   matplotlib: 3.5.2\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/usr\/lib\/x86_64-linux-gnu\/libgomp.so.1.0.0\r\n        version: None\r\n    num_threads: 16\r\n```\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **elcorto** October 26, 2022<\/sup>\r\n## Preliminaries\r\n\r\nI'm a bit confused by `GaussianProcess{Regressor,Classifier}`'s internal hyperparameter representation and the nature of the `theta` argument of the `log_marginal_likelihood()` method.\r\n\r\nThe doc strings of these methods say\r\n\r\n```\r\ntheta : array-like of shape (n_kernel_params,) default=None\r\n    Kernel hyperparameters for which the log-marginal likelihood is\r\n    evaluated. If None, the precomputed log_marginal_likelihood\r\n    of ``self.kernel_.theta`` is returned.\r\n```\r\n\r\nHowever, the GP's internal hyper optimizer code path seems to work with `log(theta)`.\r\n\r\n```py\r\n>>> from sklearn.gaussian_process import GaussianProcessRegressor\r\n>>> from sklearn.gaussian_process.kernels import RBF, WhiteKernel\r\n\r\n>>> gp=GaussianProcessRegressor(kernel=RBF()+WhiteKernel())\r\n>>> gp.fit(rand(100,3), rand(100))\r\n\r\n>>> gp.kernel_\r\nRBF(length_scale=59.8) + WhiteKernel(noise_level=0.0812)\r\n\r\n>>> log_theta=gp.kernel_.theta\r\n>>> log_theta\r\narray([ 4.09017207, -2.51094817])\r\n\r\n>>> exp(log_theta)\r\narray([59.75017175,  0.08119122])\r\n```\r\n\r\n\r\nThe value of the `kernel_.theta` attribute is actually `log(theta)`. And indeed, the [`theta` getter][getter] returns `log(theta)`, so they are not *stored* in log format but only returned that way.\r\n\r\nThe getter's docs say\r\n\r\n```\r\nReturns the (flattened, log-transformed) non-fixed hyperparameters.\r\n\r\nNote that theta are typically the log-transformed values of the\r\nkernel's hyperparameters as this representation of the search space\r\nis more amenable for hyperparameter search, as hyperparameters like\r\nlength-scales naturally live on a log-scale.\r\n\r\nReturns\r\n-------\r\ntheta : ndarray of shape (n_dims,)\r\n    The non-fixed, log-transformed hyperparameters of the kernel\r\n```\r\n\r\nOK.\r\n\r\n## Question\r\n\r\nWhen calling `log_marginal_likelihood(theta)`, [the code sets `kernel.theta = theta`][call_setter], which calls [the `theta` setter][setter] which does essentially `exp(theta)`. So from that I'd assume that we need to call `log_marginal_likelihood(log(theta))`. Indeed, continuing with above's code\r\n\r\n```py\r\n>>> gp.log_marginal_likelihood_value_\r\n-19.58130329439676\r\n\r\n>>> gp.log_marginal_likelihood(log_theta)\r\n-19.58130329439676\r\n```\r\n\r\nshows that we need to pass `log(theta)` to `log_marginal_likelihood()`. Also when trying to plot `log_marginal_likelihood` on a grid of hyperparameters, I only get what looks like correct results if I pass in `log(theta)`. If that is true, should the documentation of the `log_marginal_likelihood()` methods be adapted accordingly?\r\n\r\nThanks.\r\n\r\n\r\n[setter]: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/gaussian_process\/kernels.py#L290\r\n[getter]: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/gaussian_process\/kernels.py#L266\r\n[call_setter]: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/gaussian_process\/_gpr.py#L543\r\n<\/div>","labels":["Bug","module:gaussian_process"],"created_at":"2022-10-29T11:43:20Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24786"},{"issue_number":102,"repository":"scikit-learn\/scikit-learn","title":"PCA crashes when fit to large array","description":"### Describe the bug\r\n\r\nWhen trying to fit a `PCA` model to a large dataset with `shape=(30000, 28000)`, the model fits for a bit more than 30 minutes using nearly all cores and then crashes out of python. I've confirmed that a dataset with `shape=(20000, 28000)` succeeds without issue. I watched both in `htop` and found memory usage to never exceed 30% available memory.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn import decomposition\r\n\r\ndata = np.random.randn(30000, 28000)\r\npca_model = decomposition.PCA(0.95, whiten=True)\r\npca_model.fit(data)\r\n```\r\n\r\n### Expected Results\r\n\r\nThe model should be fit without crashing out of python.\r\n\r\n### Actual Results\r\n\r\n```\r\nfree(): invalid size\r\nAborted (core dumped)\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0]\r\nexecutable: \/usr\/local\/bin\/python\r\n   machine: Linux-4.15.0-136-generic-x86_64-with-glibc2.29\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.2\r\n          pip: 20.2.4\r\n   setuptools: 63.2.0\r\n        numpy: 1.22.3\r\n        scipy: 1.9.0\r\n       Cython: None\r\n       pandas: 1.4.2\r\n   matplotlib: None\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/home\/container-user\/.local\/lib\/python3.8\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 72\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/container-user\/.local\/lib\/python3.8\/site-packages\/numpy.libs\/libopenblas64_p-r0-2f7c42d4.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: SkylakeX\r\n    num_threads: 64\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/container-user\/.local\/lib\/python3.8\/site-packages\/scipy.libs\/libopenblasp-r0-9f9f5dbc.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: SkylakeX\r\n    num_threads: 64\r\n```\r\n","labels":["Bug","module:decomposition"],"created_at":"2022-10-25T21:36:20Z","comments":5,"reactions":3,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24757"},{"issue_number":103,"repository":"scikit-learn\/scikit-learn","title":"Improvement for Gaussian NB by rethinking the variance smoothing","description":"### Describe the workflow you want to enable\r\n\r\n## Problem background\r\n\r\nIn [sklearn.naive_bayes.GaussianNB](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html?highlight=gaussian+nb), a genarative probability model is given by \r\n\r\nP(c| **x** ) = P(c) P( **x** |c)\/P( **x** ) , where x<sub>i<\/sub> ~ N(u<sub>i<\/sub>, v<sub>i<\/sub>)\r\n\r\nHowever, sometimes the variance for P( x<sub>i<\/sub> |c) is zero. In other words, with label c, x<sub>i<\/sub> is a constant value in the dataset.  \r\n\r\n## Current solution in `sklearn.naive_bayes.GaussianNB`\r\n\r\nIt 's seen from the source that \r\n\r\n```python\r\nself.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()\r\n```\r\n\r\nand that\r\n\r\n```python\r\nself.var_ = np.zeros((n_classes, n_features))\r\nself.var_[:, :] -= self.epsilon_ \r\nself.var_[i, :] = new_sigma\r\nself.var_[:, :] += self.epsilon_\r\n```\r\n\r\n\r\n### Describe your proposed solution\r\n\r\n## The problem of the current implementation\r\nYou can see, the current implementation would add a  portion of the max variance of all the features(without considering the class), which makes the updated variance never being zero (unless all features are constants).  \r\nThis is questionable in two cases:\r\n\r\n1.  When there is no feature that has a zero variance. \r\n Well, the current algs still adds a small variance to each feature. This seems to be fair(all features are considered), but it is actually dangerous. Not all features are preprossed using something like `StdScaler`, which means their variances can differ dramatically. Adding a big value(coming from the feature with largest variance) blindly would damage the probability computation of the feature with small variance greatly. For example, the variance of the biggest feature is 100, while other features have a variance of 1, let var_smoothing be 1, then the updated variance by current implementation would be 200 and 101-s, making the significant wrong probability calculation of other features. \r\n\r\n2. When there is indeed a feature x1 that has a zero variance under some class c1.\r\n In this case, adding the max variance exaggerated the instability of this feature. You see, the ground-truth variance of x1|c1 may be something like 0.0001, but due to the max-variance feature x2, it is added by 100. which is obviously unreasonable.  \r\n\r\n## My implementation \r\n1. If there is no variable that is zero-variance, do not apply var-smoothing. \r\n2. If a feature x1 under some class c1 has a zero variance, use the variance of x1 without knowing the class to be the smoothing variance, instead of using the max variance of all features. \r\n3. If the variance of x1, even without knowing the class, is zero. throw a warning and ignore this feature for decision. \r\n\r\n\r\n## Experiment\r\n\r\nI tested my implementation on the famous [ling-spam dataset](https:\/\/www.kaggle.com\/datasets\/mandygu\/lingspam-dataset)\r\n\r\n- model: I use the \u201cbag of words\u201d model to first do a feature extraction, then `sklearn.naive_bayes.GaussianNB` is applied for both the current implementation and my new implementation\r\n- hyperparameter tuning\r\n  -  to make the competition fair, I applied a parameter tuning to both implementations on validation set\r\n  - I used GridSearchCV to search 'var_smoothing' in [1e-13, 1e-11, 1e-9, 1e-7, 1e-5, 1e-3]\r\n  - I also used `geatpy`(a genetic algorithm lib) to optimize the cross validation loss by tuning var_smoothing\r\n\r\n- experiment result on test set\r\n  - current implementation\r\n     ```python\r\n     accuracy=0.9615384615384616\r\n     recall=0.9307692307692308\r\n      f1=0.9603174603174605\r\n     ```\r\n    ![image](https:\/\/user-images.githubusercontent.com\/41530341\/197404136-3b6cb6b1-93c0-4b86-ae47-8af897d83af3.png)\r\n  - my implementation\r\n     ```python\r\n     accuracy=0.9807692307692307\r\n     recall=0.9615384615384616\r\n     f1=0.9803921568627451\r\n     ```\r\n     ![image](https:\/\/user-images.githubusercontent.com\/41530341\/197404183-7e687a01-ecd3-4986-9e38-8507c5aa9cb2.png)\r\n     ![image](https:\/\/user-images.githubusercontent.com\/41530341\/197404176-19e82c70-b97a-4122-b5f9-08eef5b89f94.png)\r\n\r\n\r\n\r\n\r\n### Describe alternatives you've considered, if relevant\r\n\r\nI know that this implentation includes a partial fit algorithm that can further add more samples to train the model furtherly. I've roughly read the report Stanford CS tech report STAN-CS-79-773, but I am still not sure whether my solution would break the formulation of the partial fit problem or whether the current implementation is necessary for doing an accurate partial fit. Currently I think it doesn't matters, but if you find something wrong please give me some insight. \r\n\r\nI will make a PR if my solution is indeed theoretically and experimentally better. \r\n\r\n### Additional context\r\n\r\n_No response_","labels":["Bug","New Feature","module:naive_bayes"],"created_at":"2022-10-23T16:36:23Z","comments":12,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24732"},{"issue_number":104,"repository":"scikit-learn\/scikit-learn","title":"AttributeError: 'MLPRegressor' object has no attribute '_best_coefs'","description":"The following parameters were working fine with another dataset. When I switched to a new dataset, for some reason an `AttributeError` is occurring. Any ideas?\r\n\r\n```python\r\nmodel = MLPRegressor(hidden_layer_sizes=(10), activation='tanh', batch_size=1000,\r\n  learning_rate_init=0.1, max_iter=5000, momentum=.9, solver='adam', early_stopping=True,\r\n  random_state=1, verbose=1)\r\nX = [[0.,0.,0.,0.2173913,0.,0.5,\r\n  0.,0.46666667,0.76351351,0.41140777,0.55555556,0.03361345,\r\n  0.09022556,0.78107607,0.13250518,1.,0.,0.,\r\n  0.,0.,1.],\r\n [0.80769231,1.,0.,0.69565217,0.20942029,0.5,\r\n  0.,0.13333333,0.,0.,0.88888889,0.95424837,\r\n  0.96491228,0.83766234,0.97584541,0.,0.,0.,\r\n  0.,0.,0.]] \r\ny = [1.0, 1.0]\r\nmodel.fit(X, y)\r\n```\r\n\r\nOutput:\r\n```\r\n...\r\nIteration 4997, loss = 0.00001391\r\nValidation score: nan\r\nIteration 4998, loss = 0.00002568\r\nValidation score: nan\r\nIteration 4999, loss = 0.00001822\r\nValidation score: nan\r\nIteration 5000, loss = 0.00001470\r\nValidation score: nan\r\n\r\nAttributeError: 'MLPRegressor' object has no attribute '_best_coefs'\r\n```\r\n\r\nThe error occurs here:\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/7c2a58d51f4528827e9bfe9c43d06c5c1716bfb8\/sklearn\/neural_network\/_multilayer_perceptron.py#L687\r\n\r\nThe error also seems to completely disappear when there are more than around 10-25 training examples.","labels":["Bug","module:neural_network"],"created_at":"2022-10-21T03:10:32Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24713"},{"issue_number":105,"repository":"scikit-learn\/scikit-learn","title":"Exit Code -1073741819 when doing K-means++ clustering","description":"### Describe the bug\r\n\r\nUnfortunately I am getting an exit code in Pycharm when doing clustering with k-means++.\r\nI tried nearly everything. Setup new Pycharm project try using different versions of numpy or sklearn.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\ndef __cluster(num_clusters: int, data: list[list[float]]):\r\n    km = KMeans(n_clusters=num_clusters)\r\n    return km.fit_predict(data)\r\n\r\n\r\ndistributions_filename: str = \"distributions.json\"\r\nwith open(distributions_filename) as f:\r\n    distributions: list[list[float]] = json.load(f)\r\n    buckets = __cluster(num_clusters=200, data=distributions)\r\n```\r\n### Expected Results\r\n\r\nclustered buckets\r\n\r\n### Actual Results\r\n\r\nbefore the clustering is finished I get an error: \"Process finished with exit code -1073741819 (0xC0000005)\"\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.7 (tags\/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]\r\nexecutable: C:\\Users\\maron\\Desktop\\test\\venv\\Scripts\\python.exe\r\n   machine: Windows-10-10.0.19044-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.2\r\n          pip: 21.3.1\r\n   setuptools: 60.2.0\r\n        numpy: 1.23.3\r\n        scipy: 1.9.1\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: None\r\n       joblib: 1.2.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\maron\\Desktop\\test\\venv\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n    num_threads: 32\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\maron\\Desktop\\test\\venv\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Zen\r\n    num_threads: 24\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\maron\\Desktop\\test\\venv\\Lib\\site-packages\\scipy\\.libs\\libopenblas.PZA5WNOTOH6FZLB2KBVKAURAKVTFSNNU.gfortran-win_amd64.dll\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Zen\r\n    num_threads: 24\r\n```\r\n","labels":["Bug","module:cluster","Needs Investigation"],"created_at":"2022-09-29T10:53:39Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24540"},{"issue_number":106,"repository":"scikit-learn\/scikit-learn","title":"GroupShuffleSplit chokes on pd.Int16Dtype() with a cryptic error","description":"### Describe the bug\r\n\r\n`GroupShuffleSplit` chokes on `pd.Int16Dtype()` with a cryptic error.\r\nIt looks like internally the data series gets converted to a list, and list comparison returns a scalar, while an iterable is expected\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\ndata = pd.DataFrame({\"clusters\": [1, 2, 3, pd.NA, pd.NA],\r\n                    \"x\" : [0,1,2,3,4]} )\r\n\r\nsplitter = GroupShuffleSplit(test_size=.2, n_splits=2, random_state = 7)\r\nsplit = splitter.split(data, groups=data['clusters'])\r\ntrain_inds, test_inds = next(split)\r\n```\r\n\r\n### Expected Results\r\n\r\ne.g. `[0,1,3]` and `[2,4]`\r\n\r\n### Actual Results\r\n\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\nInput In [23], in <cell line: 3>()\r\n      1 splitter = GroupShuffleSplit(test_size=.2, n_splits=2, random_state = 7)\r\n      2 split = splitter.split(data, groups=data['clusters'])\r\n----> 3 train_inds, test_inds = next(split)\r\n\r\nFile \/opt\/conda\/lib\/python3.8\/site-packages\/sklearn\/model_selection\/_split.py:1600, in BaseShuffleSplit.split(self, X, y, groups)\r\n   1570 \"\"\"Generate indices to split data into training and test set.\r\n   1571 \r\n   1572 Parameters\r\n   (...)\r\n   1597 to an integer.\r\n   1598 \"\"\"\r\n   1599 X, y, groups = indexable(X, y, groups)\r\n-> 1600 for train, test in self._iter_indices(X, y, groups):\r\n   1601     yield train, test\r\n\r\nFile \/opt\/conda\/lib\/python3.8\/site-packages\/sklearn\/model_selection\/_split.py:1805, in GroupShuffleSplit._iter_indices(self, X, y, groups)\r\n   1803 if groups is None:\r\n   1804     raise ValueError(\"The 'groups' parameter should not be None.\")\r\n-> 1805 groups = check_array(groups, ensure_2d=False, dtype=None)\r\n   1806 classes, group_indices = np.unique(groups, return_inverse=True)\r\n   1807 for group_train, group_test in super()._iter_indices(X=classes):\r\n   1808     # these are the indices of classes in the partition\r\n   1809     # invert them into data indices\r\n\r\nFile \/opt\/conda\/lib\/python3.8\/site-packages\/sklearn\/utils\/validation.py:805, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\r\n    799         raise ValueError(\r\n    800             \"Found array with dim %d. %s expected <= 2.\"\r\n    801             % (array.ndim, estimator_name)\r\n    802         )\r\n    804     if force_all_finite:\r\n--> 805         _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\r\n    807 if ensure_min_samples > 0:\r\n    808     n_samples = _num_samples(array)\r\n\r\nFile \/opt\/conda\/lib\/python3.8\/site-packages\/sklearn\/utils\/validation.py:126, in _assert_all_finite(X, allow_nan, msg_dtype)\r\n    122     raise ValueError(\"Input contains NaN\")\r\n\r\nAttributeError: 'bool' object has no attribute 'any'\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.8.8 (default, Apr 13 2021, 19:58:26)  [GCC 7.3.0]\r\nexecutable: \/opt\/conda\/bin\/python\r\n   machine: Linux-5.4.162-86.275.amzn2.x86_64-x86_64-with-glibc2.10\r\n\r\nPython dependencies:\r\n          pip: 22.1.2\r\n   setuptools: 59.5.0\r\n      sklearn: 1.0.2\r\n        numpy: 1.22.3\r\n        scipy: 1.6.3\r\n       Cython: 0.29.23\r\n       pandas: 1.2.4\r\n   matplotlib: 3.4.2\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n```\r\n","labels":["Bug","module:model_selection"],"created_at":"2022-09-21T03:28:21Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24486"},{"issue_number":107,"repository":"scikit-learn\/scikit-learn","title":"MLPRegressor - Validation score wrongly defined","description":"### Describe the bug\r\n\r\nIn MLPRegressor, if the option early_stopping is set as True, the model will monitor the loss calculated on the validation set in stead of the training set, using the same loss formulation which is the mean squared error. However, as implemented in the line 719 in the source code:\r\n```\r\nself.validation_scores_.append(self.score(X_val, y_val))\r\n```\r\nThe function \"score\", which returns (to confirm) the coefficient of determination, is used.  This is not correct. It should be something like:\r\n```\r\nself.validation_scores_.append(mean_squared_error(self.predict(X_val), y_val))\r\n```\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\nSorry, I don't have time to write a simple code. But the error is quite clear.\r\n\r\n### Expected Results\r\n\r\nThe validation score must be mean squared error.\r\n\r\n### Actual Results\r\n\r\nCoefficient of determination\r\n\r\n### Versions\r\n\r\n```shell\r\n1.1.1\r\n```\r\n","labels":["Bug","Enhancement","Needs Decision","module:neural_network"],"created_at":"2022-09-09T18:12:16Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24411"},{"issue_number":108,"repository":"scikit-learn\/scikit-learn","title":"GridSearchCV does not seem to recognize whether estimators from StackingClassifier are fitted or not","description":"### Describe the bug\r\n\r\nThere seems to be a bug with the combination of `GridSearchCV` and `StackingClassifier` when the parameter `cv` of  `StackingClassifier` is set to 'prefit'. With this option, the estimators of the `StackingClassifier` should be fitted before fitting the stacked model, and only the final_estimator would then be fitted. When including the `StackingClassifier` within `GridSearchCV` however, the fact that estimators have already been fitted does not seem to be recognized.\r\n\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.naive_bayes import GaussianNB\r\nfrom sklearn.ensemble import GradientBoostingClassifier, StackingClassifier\r\nfrom sklearn.model_selection import GridSearchCV\r\n\r\n# Creating toy data set\r\nn_features = 3\r\nn_instances = 40\r\ntrain = np.random.rand(n_instances, n_features)\r\nlabel = np.random.randint(0,2,n_instances)\r\n\r\n# Declaring estimators\r\nlog_clf = LogisticRegression()\r\ngau_clf = GaussianNB()\r\n\r\n# Fitting estimators\r\nlog_clf.fit(train, label)\r\ngau_clf.fit(train, label)\r\n\r\n# Creating stacked model\r\nestimators = [\r\n    (\"log\", log_clf),\r\n    (\"gau\", gau_clf)\r\n]\r\n\r\nboost = GradientBoostingClassifier()\r\nstack = StackingClassifier(estimators=estimators,\r\n                          final_estimator=boost,\r\n                          cv = 'prefit')\r\n\r\n# Creating the Grid CV\r\nparam_search = {\r\n    'final_estimator__max_depth': [1]\r\n    }\r\n\r\ngridcv = GridSearchCV(stack, \r\n                      param_grid=param_search)\r\n\r\n# Fitting the stack and gridcv models\r\nstack.fit(train, label) # works fine \r\ngridcv.fit(train, label) # sklearn.exceptions.NotFittedError: This LogisticRegression instance is not fitted yet. \r\n                         # Call 'fit' with appropriate arguments before using this estimator\r\n\r\n```\r\n\r\n### Expected Results\r\n\r\nIn the above code, the `stack` model works fine and no error related to the estimators' previous fitting is thrown.\r\n\r\nHowever, when included in the GridSearchCV, the fact that estimator models have been fitted already does not seem to be recognized and a `sklearn.exceptions.NotFittedError` error is prompted. \r\n\r\n\r\n\r\n### Actual Results\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"c:\\Users\\levesque\\Documents\\Python\\AMEX\\errorExample.py\", line 39, in <module>\r\n    gridcv.fit(train, label) # sklearn.exceptions.NotFittedError: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate\r\narguments before using this estimator\r\n  File \"C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 875, in fit\r\n    self._run_search(evaluate_candidates)\r\n  File \"C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 1379, in _run_search\r\n    evaluate_candidates(ParameterGrid(self.param_grid))\r\n  File \"C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 852, in evaluate_candidates\r\n    _warn_or_raise_about_fit_failures(out, self.error_score)\r\n  File \"C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 367, in _warn_or_raise_about_fit_failures\r\n    raise ValueError(all_fits_failed_message)\r\nValueError:\r\nAll the 5 fits failed.\r\nIt is very likely that your model is misconfigured.\r\nYou can try to debug the error by setting error_score='raise'.\r\n\r\nBelow are more details about the failures:\r\n--------------------------------------------------------------------------------\r\n5 fits failed with the following error:\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\r\n    estimator.fit(X_train, y_train, **fit_params)\r\n  File \"C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\", line 584, in fit\r\n    return super().fit(X, self._le.transform(y), sample_weight)\r\n  File \"C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\", line 183, in fit\r\n    check_is_fitted(estimator)\r\n  File \"C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 1345, in check_is_fitted\r\n    raise NotFittedError(msg % {\"name\": type(estimator).__name__})\r\nsklearn.exceptions.NotFittedError: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.0 (tags\/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]\r\nexecutable: C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\Scripts\\python.exe\r\n   machine: Windows-10-10.0.17763-SP0\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.2\r\n          pip: 22.2.2\r\n   setuptools: 57.4.0\r\n        numpy: 1.23.1\r\n        scipy: 1.9.0\r\n       Cython: None\r\n       pandas: 1.4.3\r\n   matplotlib: 3.5.2\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: vcomp\r\n       filepath: C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\Lib\\site-packages\\sklearn\\.libs\\vcomp140.dll\r\n        version: None\r\n    num_threads: 8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: C:\\Users\\levesque\\Documents\\Python\\AMEX\\env\\Lib\\site-packages\\scipy\\.libs\\libopenblas.PZA5WNOTOH6FZLB2KBVKAURAKVTFSNNU.gfortran-win_amd64.dll\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 8\r\n```\r\n","labels":["Bug","module:model_selection","module:base"],"created_at":"2022-09-09T13:50:18Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24409"},{"issue_number":109,"repository":"scikit-learn\/scikit-learn","title":"BayesianRidge prediction standard deviation affected by uniform sample weights","description":"### Describe the bug\r\n\r\nThe standard deviation of predictions obtained by setting return_std=True on predict(), is clearly affected by uniform sample_weight vectors on fit(). A uniform sample_weight vector will act as a constant on the likelihood function, and I am therefore questioning the effect it has on the standard deviation of predictions. As shown below by the example, the scale (1 vs. 20) of the uniform sample_weight vector directly affects the scale of the standard deviations. The example is based on the Curve Fitting with Bayesian Ridge Regression [example](https:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_bayesian_ridge_curvefit.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-curvefit-py)).\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```\r\nimport numpy as np\r\nfrom sklearn.linear_model import BayesianRidge\r\nimport matplotlib.pyplot as plt\r\n\r\ndef func(x):\r\n    return np.sin(2 * np.pi * x)\r\n\r\nsize = 25\r\nrng = np.random.RandomState(1234)\r\nx_train = rng.uniform(0.0, 1.0, size)\r\ny_train = func(x_train) + rng.normal(scale=0.1, size=size)\r\nx_test = np.linspace(0.0, 1.0, 100)\r\n\r\nn_order = 3\r\nX_train = np.vander(x_train, n_order + 1, increasing=True)\r\nX_test = np.vander(x_test, n_order + 1, increasing=True)\r\nreg = BayesianRidge(tol=1e-6, fit_intercept=False)\r\n\r\ninit = [1.0, 1e-3]\r\nreg.set_params(alpha_init=init[0], lambda_init=init[1])\r\n\r\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\r\nfor i, ax in enumerate(axes):\r\n    if i == 0:\r\n        reg.fit(X_train, y_train, sample_weight=np.ones(size))\r\n        ymean, ystd = reg.predict(X_test, return_std=True)\r\n\r\n        ax.plot(x_test, func(x_test), color=\"blue\", label=\"sin($2\\\\pi x$)\")\r\n        ax.scatter(x_train, y_train, s=50, alpha=0.5, label=\"observation\")\r\n        ax.plot(x_test, ymean, color=\"red\", label=\"predict mean\")\r\n        ax.fill_between(x_test, ymean - ystd, ymean + ystd, color=\"pink\", alpha=0.5, label=\"predict std\")\r\n        ax.set_title('sample_weight=np.ones(size)')\r\n        ax.set_ylim(-1.6, 1.6)\r\n        ax.legend()\r\n    elif i == 1:\r\n        reg.fit(X_train, y_train, sample_weight=20*np.ones(size))\r\n        ymean, ystd = reg.predict(X_test, return_std=True)\r\n\r\n        ax.plot(x_test, func(x_test), color=\"blue\", label=\"sin($2\\\\pi x$)\")\r\n        ax.scatter(x_train, y_train, s=50, alpha=0.5, label=\"observation\")\r\n        ax.plot(x_test, ymean, color=\"red\", label=\"predict mean\")\r\n        ax.fill_between(x_test, ymean - ystd, ymean + ystd, color=\"pink\", alpha=0.5, label=\"predict std\")\r\n        ax.set_title('sample_weight=20 * np.ones(size)')\r\n        ax.set_ylim(-1.6, 1.6)\r\n        ax.legend()\r\n\r\nplt.tight_layout()\r\nplt.show()\r\n```\r\n\r\n\r\n\r\n### Expected Results\r\n\r\n\r\n![expected](https:\/\/user-images.githubusercontent.com\/42994401\/187893778-bd076956-dd22-4b53-90ea-faec7c27c98f.png)\r\n\r\n\r\n\r\n### Actual Results\r\n\r\n\r\n![actual](https:\/\/user-images.githubusercontent.com\/42994401\/187893819-3654f021-67db-4703-a2b2-cad2ae87071d.png)\r\n\r\n\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\Users\\AAA\\anaconda3\\python.exe\r\n   machine: Windows-10-10.0.19041-SP0\r\n\r\nPython dependencies:\r\n          pip: 20.2.4\r\n   setuptools: 50.3.1.post20201107\r\n      sklearn: 0.24.0\r\n        numpy: 1.22.4\r\n        scipy: 1.5.2\r\n       Cython: 0.29.21\r\n       pandas: 1.1.3\r\n   matplotlib: 3.3.2\r\n       joblib: 0.17.0\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n```\r\n","labels":["Bug","module:linear_model","Needs Investigation"],"created_at":"2022-09-01T10:33:32Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24313"},{"issue_number":110,"repository":"scikit-learn\/scikit-learn","title":"sklearn.svm.SVR use more RAM memory on newer versions","description":"### Describe the bug\n\nHi, I've previously reported a bug that [RandomForestClassifier](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24124) consumes more memory on newer versions, and I've also noticed that the svm API also consumes more memory on newer versions. The detailed test information is as follows.\r\n|Memory| Version|\r\n|--|--|\r\n|274MB|1.0.1|\r\n|276MB|0.20.3|\r\n|180MB|0.19.2|\r\n\n\n### Steps\/Code to Reproduce\n\n[Download dataset](https:\/\/drive.google.com\/drive\/folders\/1tI0h0ZoL8bnvlZyVsa0-QPdKTcrZ_kVv?usp=sharing)\r\n\r\n```python\r\nimport math\r\nimport pandas as pd\r\nimport numpy as np\r\nimport random\r\nimport torch\r\nfrom sklearn.svm import SVR\r\nfrom sklearn.ensemble import BaggingRegressor\r\n\r\ndef random_init(**kwargs):\r\n    random.seed(kwargs['seed'])\r\n    np.random.seed(kwargs['seed'])\r\n    torch.manual_seed(kwargs['seed'])\r\n    torch.cuda.manual_seed(kwargs['seed'])\r\n    torch.backends.cudnn.deterministic = True\r\ndef load_data(df,cv=False,target=False,**kwargs):\r\n    num_samples = len(df)\r\n    sample_size = sum([len(args['categories'][c]) for c in args['categories']]) + len(args['num_feats'])\r\n    dataset = torch.zeros((num_samples,sample_size),dtype=torch.float)\r\n    idx = 0\r\n    for c in args['cat_feats']:\r\n        for i in range(len(args['categories'][c])):\r\n            dataset[np.array(df[c])==args['categories'][c][i],idx] = 1.0\r\n            idx += 1\r\n    for n in args['num_feats']:\r\n        dataset[:,idx] = torch.from_numpy(np.array(df[n]))\r\n        idx += 1\r\n    if target:\r\n        targets = torch.from_numpy(np.array(df['target']))\r\n    else:\r\n        targets = None\r\n    \r\n    if cv == False:\r\n        return dataset, targets\r\n\r\n    idx = [i for i in range(num_samples)]\r\n    random.shuffle(idx)\r\n    trainset = dataset[idx[0:int(num_samples*(1-kwargs['cv_percentage']))]]\r\n    traintargets = targets[idx[0:int(num_samples*(1-kwargs['cv_percentage']))]]\r\n    validset = dataset[idx[int(num_samples*(1-kwargs['cv_percentage'])):]]\r\n    validtargets = targets[idx[int(num_samples*(1-kwargs['cv_percentage'])):]]\r\n    return trainset, validset, traintargets, validtargets  \r\ndef get_stats(trainset):\r\n    mean = torch.mean(trainset,dim=0)\r\n    std = torch.std(trainset,dim=0)\r\n    for i in range(trainset.shape[1]):\r\n        if ((trainset[:,i]==0) | (trainset[:,i]==1)).all():\r\n            mean[i] = 0.5\r\n            std[i] = 1.0\r\n    return mean, std\r\n#Global arguments\r\nargs = {\r\n    'cv_percentage': 0.1,\r\n    'seed': 0,\r\n    }\r\n# Load data\r\nrandom_init(**args)\r\ntrain_data = pd.read_csv('train.csv')\r\ntest_data = pd.read_csv('test.csv')\r\nargs['cat_feats'] = [c for c in np.sort(train_data.columns) if 'cat' in c]\r\nargs['num_feats'] = [c for c in np.sort(train_data.columns) if 'cont' in c]\r\nargs['categories'] = {c: np.unique(train_data[c]) for c in args['cat_feats']}\r\ntestset, _ = load_data(test_data,cv=False,target=False,**args)\r\ntrainset, validset, traintargets, validtargets = load_data(train_data,cv=True,target=True,**args)\r\nargs['mean'],args['std'] = get_stats(trainset)\r\nval_pred = {}\r\ntest_pred = {}\r\n# Bagged SVRs\r\nimport tracemalloc\r\ntracemalloc.start()\r\nprint('Bagged SVRs...')\r\n# 0.19.2 180MB\r\n# 0.20.3 276\r\n# SVR arguments\r\nargs = {**args,**{\r\n    'kernel': 'linear',\r\n    'n_estimators': 3,\r\n    'max_samples': 20000,\r\n    'max_features':50\r\n    }}\r\nrandom_init(**args)\r\nsvrs = BaggingRegressor(SVR(kernel=args['kernel'],C=1.0, epsilon=0.2),\r\n                        n_estimators=args['n_estimators'],max_samples=args['max_samples'],max_features=args['max_features'])\r\nsvrs.fit(((trainset-args['mean'])\/args['std']).numpy(),traintargets.numpy())\r\nval_pred['svrs'] = svrs.predict(((validset-args['mean'])\/args['std']).numpy())\r\ntest_pred['svrs'] = svrs.predict(((testset-args['mean'])\/args['std']).numpy())\r\ncurrentSVR, peakSVR = tracemalloc.get_traced_memory()\r\nprint(\"SVR current memory usage is {\",currentSVR \/1024\/1024,\"}MB; SVR Peak memory was :{\",peakSVR \/ 1024\/1024,\"}MB\")\r\n```\n\n### Expected Results\n\nsame memory usage\n\n### Actual Results\n\nnew version use more memory.\n\n### Versions\n\n```shell\n1.0.1, 0.20.3, 0.19.2\n```\n","labels":["Bug","module:svm","Needs Investigation"],"created_at":"2022-08-06T16:44:58Z","comments":3,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24138"},{"issue_number":111,"repository":"scikit-learn\/scikit-learn","title":"Gaussian Process regression get_params set_params does not yield same predictor? ","description":"### Describe the bug\r\n\r\nThis is on version 1.1.1\r\n\r\nI *think* the example below is the correct way of setting GPR params saved from a previous .fit() tuning via the optimizer. Reading through the code I don't yet understand why the parameters `self.L_` and `self.alpha_` are not being updated but maybe am missing something in my reading.\r\n\r\nSee below for the example. Everything passes except the last line. \r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport sys\r\n\r\nimport numpy as np\r\nfrom scipy.linalg import cholesky\r\nfrom sklearn.gaussian_process import GaussianProcessRegressor, kernels\r\nfrom sklearn.gaussian_process.kernels import *\r\n\r\nnp.random.seed(0)\r\n\r\nX = np.random.randn(100, 3)\r\ny = np.random.randn(100, 1)\r\n\r\nKERNEL = Matern()\r\n# make it once, fit and get the params\r\ngp_ = GaussianProcessRegressor(kernel=KERNEL, normalize_y=True, alpha=1e-5)\r\ngp_.fit(X, y)\r\nparams = gp_.kernel_.get_params(deep=False)  # tried deep=True but doesn't matter\r\n\r\n# make it again, don't fit, but set the params\r\ngp = GaussianProcessRegressor(kernel=KERNEL, normalize_y=True, alpha=1e-5)\r\ngp.optimizer = None  # this should make fit do very little\r\ngp.fit(X, y)  # this creates kernele\r\n\r\n# do it again for good measure? Maybe we need to do this several times to trigger it?\r\ngp.kernel_.set_params(**params)\r\ngp.fit(X, y)  # do this again to now recreate L_\r\ngp.kernel_.set_params(**params)\r\n\r\n\r\ndef get_K_L_(self):\r\n    K = self.kernel_(self.X_train_)\r\n    K[np.diag_indices_from(K)] += self.alpha\r\n    L_ = cholesky(K, lower=True)\r\n    return K, L_\r\n\r\n\r\nKa, La = get_K_L_(gp_)\r\nKb, Lb = get_K_L_(gp)\r\n\r\na = gp_.__getstate__()\r\nb = gp.__getstate__()\r\n\r\n# here you will see alpha_ and L_ are not the same. Something is not updating them correctly?\r\n# see https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/gaussian_process\/_gpr.py#L322\r\nfor k in a:\r\n    if isinstance(a[k], np.ndarray):\r\n        print(k, type(a[k]), np.all(a[k] == b[k]))\r\n\r\n\r\nassert np.all(gp_.X_train_ == gp.X_train_)  # passes\r\nassert gp_.alpha == gp.alpha  # passes\r\nassert np.allclose(Ka, Kb), 'Ka Kb do not match'  # passes\r\nassert np.allclose(La, Lb), 'La Lb do not match'  # passes\r\n# it is bizarre that the output of the kernels matchs but not L_????\r\nassert np.allclose(gp.L_, gp_.L_), 'L_ do not match'  # FAILS\r\n```\r\n\r\n### Expected Results\r\n\r\nNo exceptions. The gp and gp_ should return the same results, have same state (except for the optimizer).\r\n\r\n### Actual Results\r\n\r\n```\r\n_y_train_mean <class 'numpy.ndarray'> True\r\n_y_train_std <class 'numpy.ndarray'> True\r\nX_train_ <class 'numpy.ndarray'> True\r\ny_train_ <class 'numpy.ndarray'> True\r\nL_ <class 'numpy.ndarray'> False\r\nalpha_ <class 'numpy.ndarray'> False\r\n---------------------------------------------------------------------------\r\nAssertionError                            Traceback (most recent call last)\r\n<ipython-input-5-71be2b0a00fc> in <module>\r\n     53 assert np.allclose(La, Lb), 'La Lb do not match'  # passes\r\n     54 # it is bizarre that the output of the kernels matchs but not L_????\r\n---> 55 assert np.allclose(gp.L_, gp_.L_), 'L_ do not match'  # FAILS\r\n\r\nAssertionError: L_ do not match\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]\r\nexecutable: \/home\/cottrell\/anaconda3\/envs\/3.10.4\/bin\/python\r\n   machine: Linux-5.15.0-43-generic-x86_64-with-glibc2.35\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.1\r\n          pip: 22.1.2\r\n   setuptools: 61.2.0\r\n        numpy: 1.23.1\r\n        scipy: 1.8.1\r\n       Cython: 0.29.30\r\n       pandas: 1.4.3\r\n   matplotlib: 3.5.2\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/cottrell\/anaconda3\/envs\/3.10.4\/lib\/python3.10\/site-packages\/numpy.libs\/libopenblas64_p-r0-742d56dc.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/home\/cottrell\/anaconda3\/envs\/3.10.4\/lib\/python3.10\/site-packages\/scikit_learn.libs\/libgomp-a34b3233.so.1.0.0\r\n        version: None\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/cottrell\/anaconda3\/envs\/3.10.4\/lib\/python3.10\/site-packages\/scipy.libs\/libopenblasp-r0-8b9e111f.3.17.so\r\n        version: 0.3.17\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 12\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/cottrell\/anaconda3\/envs\/3.10.4\/lib\/python3.10\/site-packages\/opencv_python_headless.libs\/libopenblas-r0-f650aae0.3.3.so\r\n        version: None\r\nthreading_layer: disabled\r\n   architecture: Haswell\r\n    num_threads: 1\r\n```\r\n","labels":["Bug","module:gaussian_process","Needs Investigation"],"created_at":"2022-08-04T08:59:03Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24107"},{"issue_number":112,"repository":"scikit-learn\/scikit-learn","title":"Weights are being normalized using number of samples as opposed to sum in GaussianMixture","description":"### Describe the bug\n\nWeights are being normalized at Line https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/mixture\/_gaussian_mixture.py#L718 using `n_samples`. It should be done using `weights.sum()` as\r\ndone in `_m_step()` here: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/mixture\/_gaussian_mixture.py#L756.\n\n### Steps\/Code to Reproduce\n\nWeights are being normalized at Line https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/mixture\/_gaussian_mixture.py#L718 using `n_samples`. It should be done using `weights.sum()` as\r\ndone in `_m_step()` here: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/main\/sklearn\/mixture\/_gaussian_mixture.py#L756.\n\n### Expected Results\n\nCorrect weights.\n\n### Actual Results\n\nIncorrect weights.\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.9.13 (main, May 24 2022, 21:28:31)  [Clang 13.1.6 (clang-1316.0.21.2)]\r\nexecutable: \/Users\/kshitijgoel\/Documents\/main\/code.nosync\/self_organizing_gmm\/.venv\/bin\/python\r\n   machine: macOS-12.4-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.1\r\n          pip: 22.2.1\r\n   setuptools: 62.3.2\r\n        numpy: 1.23.1\r\n        scipy: 1.8.1\r\n       Cython: 0.29.30\r\n       pandas: 1.4.3\r\n   matplotlib: 3.5.2\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: False\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/Users\/kshitijgoel\/Documents\/main\/code.nosync\/self_organizing_gmm\/.venv\/lib\/python3.9\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/Users\/kshitijgoel\/Documents\/main\/code.nosync\/self_organizing_gmm\/.venv\/lib\/python3.9\/site-packages\/scipy\/.dylibs\/libopenblas.0.dylib\r\n        version: 0.3.17\r\nthreading_layer: pthreads\r\n   architecture: Haswell\r\n    num_threads: 8\n```\n","labels":["Bug","Moderate","module:mixture"],"created_at":"2022-08-02T16:18:04Z","comments":7,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/24085"},{"issue_number":113,"repository":"scikit-learn\/scikit-learn","title":"Build time test issue on arm64 and ppc64el","description":"### Describe the bug\r\n\r\nThe Debian autobuilders catched a build time test error for the architectures [arm64](https:\/\/buildd.debian.org\/status\/fetch.php?pkg=scikit-learn&arch=arm64&ver=1.1.1-1&stamp=1653341247&raw=0) and [ppc64el](https:\/\/buildd.debian.org\/status\/fetch.php?pkg=scikit-learn&arch=ppc64el&ver=1.1.1-1&stamp=1653347532&raw=0).\r\n\r\nThis relates to issue #22503 but that was reported against version 1.0.2.  I created a new issue since it now links to the recent build logs of your latest version. \r\n\r\n### Steps\/Code to Reproduce\r\n\r\nIf you have access to arm64 or ppc64el architectures try to build the code and run the test suite.\r\n\r\n### Expected Results\r\n\r\nSee linked logs.\r\n\r\n### Actual Results\r\n\r\nIf you browse the end of the logs I've linked above both architectures are ending up with the following test suite error:\r\n\r\n    =================================== FAILURES ===================================\r\n    ________ [doctest] sklearn.ensemble._weight_boosting.AdaBoostRegressor _________\r\n    1026     Examples\r\n    1027     --------\r\n    1028     >>> from sklearn.ensemble import AdaBoostRegressor\r\n    1029     >>> from sklearn.datasets import make_regression\r\n    1030     >>> X, y = make_regression(n_features=4, n_informative=2,\r\n    1031     ...                        random_state=0, shuffle=False)\r\n    1032     >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\r\n    1033     >>> regr.fit(X, y)\r\n    1034     AdaBoostRegressor(n_estimators=100, random_state=0)\r\n    1035     >>> regr.predict([[0, 0, 0, 0]])\r\n    Expected:\r\n        array([4.7972...])\r\n    Got:\r\n        array([5.74049295])\r\n\r\nThe fact that the error is absolutely identical for am64 and ppc64el lets me assume that there is really an issue on non-amd64 64 bit architectures. \r\n\r\n### Versions\r\n\r\n```shell\r\nThe packaged version ofscikit-learn which belongs to the linked autobuilder logs is 1.1.1\r\n```\r\n","labels":["Bug","arch:arm","module:test-suite"],"created_at":"2022-07-15T07:42:21Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23901"},{"issue_number":114,"repository":"scikit-learn\/scikit-learn","title":"Issue with affinity propagation and dtype np.float32","description":"While working on https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/23838, it seems that the issue observed in https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/10832 was not solved since it breaks for different random seeds.\r\n\r\nTherefore, we should revisit the original issue.","labels":["Bug","module:cluster"],"created_at":"2022-07-14T14:43:37Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23897"},{"issue_number":115,"repository":"scikit-learn\/scikit-learn","title":"check_estimator() not respecting custom estimator constraints via check_estimators_fit_returns_self()","description":"### Describe the bug\n\n# check_estimators_fit_returns_self() does not pass with n_feature constraints\r\nBuilding a custom estimator extending BaseEstimator with certain constraints such as:\r\n\r\n## Example of constraints in the test.\r\n```\r\nX = check_array(\r\n    X, \r\n    accept_sparse = False,\r\n    accept_large_sparse = False,\r\n    ensure_min_samples = 2,\r\n    ensure_min_features = 3,\r\n    force_all_finite = 'allow-nan')\r\n```\r\nThe input and output of the estimator results in error thrown within `check_estimator`.\r\n\r\nIt appears that the tests run by `check_estimator` still check with input data with n_features = 2 for instance. Thus, the check_array with the above constraints will throw an error. This is not limited to the number of features but also whether force_all_finite allows nan or not. Some tests check with this value as True.\r\n\r\nThe test `check_estimators_fit_returns_self()` generates blobs with n_features = 2. Thus, this test will fail for any estimator that has feature_constraints != 2. \r\n\r\nSee: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/baf0ea25d\/sklearn\/utils\/estimator_checks.py#L2581-L2595\r\n\r\n## TL:DR\r\nIt appears that despite setting certain constraints of the input X,y. Such as `force_all_finite` and `ensure_min_features`. Some tests still use the default values for the `check_array` function and fail the check_estimator test due to this.\r\n\r\n## The full error\r\n<details>\r\n<summary> Full error <\/summary>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nInput In [5], in <cell line: 1>()\r\n----> 1 check_estimator(model)\r\n\r\nFile ~\/miniforge3\/lib\/python3.9\/site-packages\/sklearn\/utils\/estimator_checks.py:623, in check_estimator(estimator, generate_only, Estimator)\r\n    621 for estimator, check in checks_generator():\r\n    622     try:\r\n--> 623         check(estimator)\r\n    624     except SkipTest as exception:\r\n    625         # SkipTest is thrown when pandas can't be imported, or by checks\r\n    626         # that are in the xfail_checks tag\r\n    627         warnings.warn(str(exception), SkipTestWarning)\r\n\r\nFile ~\/miniforge3\/lib\/python3.9\/site-packages\/sklearn\/utils\/_testing.py:318, in _IgnoreWarnings.__call__.<locals>.wrapper(*args, **kwargs)\r\n    316 with warnings.catch_warnings():\r\n    317     warnings.simplefilter(\"ignore\", self.category)\r\n--> 318     return fn(*args, **kwargs)\r\n\r\nFile ~\/miniforge3\/lib\/python3.9\/site-packages\/sklearn\/utils\/estimator_checks.py:2595, in check_estimators_fit_returns_self(name, estimator_orig, readonly_memmap)\r\n   2592     X, y = create_memmap_backed_data([X, y])\r\n   2594 set_random_state(estimator)\r\n-> 2595 assert estimator.fit(X, y) is estimator\r\n\r\nFile ~\/Documents\/VOIDEV\/tandem-riding-detection-service\/service\/source\/models\/physics\/_base.py:42, in PhysicsRegressor.fit(self, X, y)\r\n     20 \"\"\"A fitting function which only checks the format of X and y.\r\n     21 \r\n     22 About the expected input data\r\n   (...)\r\n     38     Returns self.\r\n     39 \"\"\"\r\n     40 self.n_features_in_ = 3\r\n---> 42 X, y = check_X_y(\r\n     43     X,\r\n     44     y,\r\n     45     accept_sparse = False,\r\n     46     accept_large_sparse = False,\r\n     47     ensure_min_samples = 2,\r\n     48     ensure_min_features = 3,\r\n     49     force_all_finite = True)\r\n     51 #assert X.shape[-1] == 3, f\"Expected 3 features. Received {X.shape[-1]}\"\r\n     53 return self\r\n\r\nFile ~\/miniforge3\/lib\/python3.9\/site-packages\/sklearn\/utils\/validation.py:1074, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\r\n   1069         estimator_name = _check_estimator_name(estimator)\r\n   1070     raise ValueError(\r\n   1071         f\"{estimator_name} requires y to be passed, but the target y is None\"\r\n   1072     )\r\n-> 1074 X = check_array(\r\n   1075     X,\r\n   1076     accept_sparse=accept_sparse,\r\n   1077     accept_large_sparse=accept_large_sparse,\r\n   1078     dtype=dtype,\r\n   1079     order=order,\r\n   1080     copy=copy,\r\n   1081     force_all_finite=force_all_finite,\r\n   1082     ensure_2d=ensure_2d,\r\n   1083     allow_nd=allow_nd,\r\n   1084     ensure_min_samples=ensure_min_samples,\r\n   1085     ensure_min_features=ensure_min_features,\r\n   1086     estimator=estimator,\r\n   1087     input_name=\"X\",\r\n   1088 )\r\n   1090 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\r\n   1092 check_consistent_length(X, y)\r\n\r\nFile ~\/miniforge3\/lib\/python3.9\/site-packages\/sklearn\/utils\/validation.py:918, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\r\n    916     n_features = array.shape[1]\r\n    917     if n_features < ensure_min_features:\r\n--> 918         raise ValueError(\r\n    919             \"Found array with %d feature(s) (shape=%s) while\"\r\n    920             \" a minimum of %d is required%s.\"\r\n    921             % (n_features, array.shape, ensure_min_features, context)\r\n    922         )\r\n    924 if copy and np.may_share_memory(array, array_orig):\r\n    925     array = np.array(array, dtype=dtype, order=order)\r\n\r\nValueError: Found array with 2 feature(s) (shape=(21, 2)) while a minimum of 3 is required.\r\n```\r\n<\/details>\r\n\r\n## Reproduction\r\nLet's create a very basic estimator which only takes 3 features. Then run check_estimator on this.\r\n\r\n```\r\nclass DummyEstimator(BaseEstimator):\r\n    def __init__(self):\r\n        pass\r\n\r\n    def fit(self, X, y=None):\r\n        X, y = check_X_y(\r\n            X,\r\n            y,\r\n            accept_sparse = False,\r\n            accept_large_sparse = False,\r\n            ensure_min_features = 3,\r\n            force_all_finite = 'allow-nan')\r\n        return self\r\n\r\n    def predict(self, X):\r\n        X  = check_array(\r\n            X,\r\n            accept_sparse = False,\r\n            accept_large_sparse = False,\r\n            ensure_min_features = 3,\r\n            force_all_finite = 'allow-nan')\r\n        \r\n        # very dumb example but it does not matter\r\n        y_pred = X[:,0] + X[:,1] + X[:,3]\r\n        \r\n        y_pred  = check_array(\r\n            y_pred,\r\n            accept_sparse = False,\r\n            accept_large_sparse = False,\r\n            ensure_min_features = 3,\r\n            ensure_2d = False,\r\n            force_all_finite = 'allow-nan')\r\n        \r\n        return y_pred\r\n```\r\n\r\nError produced:\r\n```ValueError: Found array with 2 feature(s) (shape=(21, 2)) while a minimum of 3 is required.```\n\n### Steps\/Code to Reproduce\n\nRun a check_estimator on an instance of this dummy estimator\r\n\r\n```\r\nclass DummyEstimator(BaseEstimator):\r\n    def __init__(self):\r\n        pass\r\n\r\n    def fit(self, X, y=None):\r\n        X, y = check_X_y(\r\n            X,\r\n            y,\r\n            accept_sparse = False,\r\n            accept_large_sparse = False,\r\n            ensure_min_features = 3,\r\n            force_all_finite = 'allow-nan')\r\n        return self\r\n\r\n    def predict(self, X):\r\n        X  = check_array(\r\n            X,\r\n            accept_sparse = False,\r\n            accept_large_sparse = False,\r\n            ensure_min_features = 3,\r\n            force_all_finite = 'allow-nan')\r\n        \r\n        # very dumb example but it does not matter\r\n        y_pred = X[:,0] + X[:,1] + X[:,3]\r\n        \r\n        y_pred  = check_array(\r\n            y_pred,\r\n            accept_sparse = False,\r\n            accept_large_sparse = False,\r\n            ensure_min_features = 3,\r\n            ensure_2d = False,\r\n            force_all_finite = 'allow-nan')\r\n        \r\n        return y_pred\r\n```\n\n### Expected Results\n\nPassing all tests.\n\n### Actual Results\n\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nInput In [5], in <cell line: 1>()\r\n----> 1 check_estimator(DummyEstimator())\r\n\r\nFile ~\/miniforge3\/lib\/python3.9\/site-packages\/sklearn\/utils\/estimator_checks.py:623, in check_estimator(estimator, generate_only, Estimator)\r\n    621 for estimator, check in checks_generator():\r\n    622     try:\r\n--> 623         check(estimator)\r\n    624     except SkipTest as exception:\r\n    625         # SkipTest is thrown when pandas can't be imported, or by checks\r\n    626         # that are in the xfail_checks tag\r\n    627         warnings.warn(str(exception), SkipTestWarning)\r\n\r\nFile ~\/miniforge3\/lib\/python3.9\/site-packages\/sklearn\/utils\/_testing.py:318, in _IgnoreWarnings.__call__.<locals>.wrapper(*args, **kwargs)\r\n    316 with warnings.catch_warnings():\r\n    317     warnings.simplefilter(\"ignore\", self.category)\r\n--> 318     return fn(*args, **kwargs)\r\n\r\nFile ~\/miniforge3\/lib\/python3.9\/site-packages\/sklearn\/utils\/estimator_checks.py:2595, in check_estimators_fit_returns_self(name, estimator_orig, readonly_memmap)\r\n   2592     X, y = create_memmap_backed_data([X, y])\r\n   2594 set_random_state(estimator)\r\n-> 2595 assert estimator.fit(X, y) is estimator\r\n\r\nFile ~\/Documents\/VOIDEV\/tandem-riding-detection-service\/service\/source\/models\/physics\/_base.py:15, in DummyEstimator.fit(self, X, y)\r\n     14 def fit(self, X, y=None):\r\n---> 15     X, y = check_X_y(\r\n     16         X,\r\n     17         y,\r\n     18         accept_sparse = False,\r\n     19         accept_large_sparse = False,\r\n     20         ensure_min_features = 3,\r\n     21         force_all_finite = 'allow-nan')\r\n     22     return self\r\n\r\nFile ~\/miniforge3\/lib\/python3.9\/site-packages\/sklearn\/utils\/validation.py:1074, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\r\n   1069         estimator_name = _check_estimator_name(estimator)\r\n   1070     raise ValueError(\r\n   1071         f\"{estimator_name} requires y to be passed, but the target y is None\"\r\n   1072     )\r\n-> 1074 X = check_array(\r\n   1075     X,\r\n   1076     accept_sparse=accept_sparse,\r\n   1077     accept_large_sparse=accept_large_sparse,\r\n   1078     dtype=dtype,\r\n   1079     order=order,\r\n   1080     copy=copy,\r\n   1081     force_all_finite=force_all_finite,\r\n   1082     ensure_2d=ensure_2d,\r\n   1083     allow_nd=allow_nd,\r\n   1084     ensure_min_samples=ensure_min_samples,\r\n   1085     ensure_min_features=ensure_min_features,\r\n   1086     estimator=estimator,\r\n   1087     input_name=\"X\",\r\n   1088 )\r\n   1090 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\r\n   1092 check_consistent_length(X, y)\r\n\r\nFile ~\/miniforge3\/lib\/python3.9\/site-packages\/sklearn\/utils\/validation.py:918, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\r\n    916     n_features = array.shape[1]\r\n    917     if n_features < ensure_min_features:\r\n--> 918         raise ValueError(\r\n    919             \"Found array with %d feature(s) (shape=%s) while\"\r\n    920             \" a minimum of %d is required%s.\"\r\n    921             % (n_features, array.shape, ensure_min_features, context)\r\n    922         )\r\n    924 if copy and np.may_share_memory(array, array_orig):\r\n    925     array = np.array(array, dtype=dtype, order=order)\r\n\r\nValueError: Found array with 2 feature(s) (shape=(21, 2)) while a minimum of 3 is required.\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.9.7 (default, Sep 16 2021, 23:53:23)  [Clang 12.0.0 ]\r\nexecutable: \/Users\/alexandervialabellander\/miniforge3\/bin\/python\r\n   machine: macOS-12.2.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.1\r\n          pip: 21.2.4\r\n   setuptools: 58.0.4\r\n        numpy: 1.22.3\r\n        scipy: 1.7.3\r\n       Cython: None\r\n       pandas: 1.3.5\r\n   matplotlib: 3.5.0\r\n       joblib: 1.1.0\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libomp\r\n       filepath: \/Users\/alexandervialabellander\/miniforge3\/lib\/python3.9\/site-packages\/sklearn\/.dylibs\/libomp.dylib\r\n        version: None\r\n    num_threads: 8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/Users\/alexandervialabellander\/miniforge3\/lib\/python3.9\/site-packages\/numpy\/.dylibs\/libopenblas64_.0.dylib\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: armv8\r\n    num_threads: 8\r\n\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/Users\/alexandervialabellander\/miniforge3\/lib\/libopenblasp-r0.3.17.dylib\r\n        version: 0.3.17\r\nthreading_layer: pthreads\r\n   architecture: armv8\r\n    num_threads: 8\n```\n","labels":["Bug","module:test-suite","Developer API"],"created_at":"2022-07-12T09:22:08Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23885"},{"issue_number":116,"repository":"scikit-learn\/scikit-learn","title":"`OneVsOneClassifier` does not accept custom input types","description":"### Describe the bug\n\nDue to the additional validation in #6626, `OneVsOneClassifier` cannot be used with custom types that work like arrays but cannot be converted to arrays. In my case I am the maintainer of [scikit-fda](https:\/\/fda.readthedocs.io\/en\/latest\/), a functional data library that attempts to be compatible with scikit-learn. The metaestimators `OneVsRestClassifier` and `OneVsOneClassifier` should be trivially compatible with our data. Currently `OneVsRestClassifier` works fine, while `OneVsOneClassifier` doesn't.\r\n\r\nI think that scikit-learn has sometimes very aggressive validation that makes it difficult to extend it for custom objects, as in this case.\n\n### Steps\/Code to Reproduce\n\nI have no example using only scikit-learn, as you need custom types and classifiers \/ transformers to trigger it\r\nThe following code is adapted from https:\/\/fda.readthedocs.io\/en\/latest\/auto_tutorial\/plot_skfda_sklearn.html#multiclass-and-multioutput-classification-utilities\r\nNote that the code works if you replace OneVsOneClassifier with OneVsRestClassifier, as the later does not have that aggressive validation.\r\n\r\n```python\r\nimport skfda\r\nimport skfda.preprocessing.dim_reduction.variable_selection as vs\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.multiclass import OneVsOneClassifier\r\n\r\nX, y = skfda.datasets.fetch_phoneme(return_X_y=True)\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\r\n\r\npipeline = Pipeline([\r\n    (\"dim_reduction\", vs.RKHSVariableSelection(n_features_to_select=3)),\r\n    (\"classifier\", SVC()),\r\n])\r\n\r\nmulticlass = OneVsOneClassifier(pipeline)\r\n\r\nmulticlass.fit(X_train, y_train)\r\nmulticlass.score(X_test, y_test)\r\n```\n\n### Expected Results\n\nThe result of the classification.\n\n### Actual Results\n\n```\r\nTypeError                                 Traceback (most recent call last)\r\nTypeError: float() argument must be a string or a number, not 'FDataGrid'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-2-66e12b97fe3e> in <module>\r\n     17 multiclass = OneVsOneClassifier(pipeline)\r\n     18 \r\n---> 19 multiclass.fit(X_train, y_train)\r\n     20 multiclass.score(X_test, y_test)\r\n\r\n~\/Programas\/Utilidades\/Lenguajes\/miniconda3\/envs\/fda38\/lib\/python3.8\/site-packages\/sklearn\/multiclass.py in fit(self, X, y)\r\n    726         \"\"\"\r\n    727         # We need to validate the data because we do a safe_indexing later.\r\n--> 728         X, y = self._validate_data(\r\n    729             X, y, accept_sparse=[\"csr\", \"csc\"], force_all_finite=False\r\n    730         )\r\n\r\n~\/Programas\/Utilidades\/Lenguajes\/miniconda3\/envs\/fda38\/lib\/python3.8\/site-packages\/sklearn\/base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)\r\n    579                 y = check_array(y, **check_y_params)\r\n    580             else:\r\n--> 581                 X, y = check_X_y(X, y, **check_params)\r\n    582             out = X, y\r\n    583 \r\n\r\n~\/Programas\/Utilidades\/Lenguajes\/miniconda3\/envs\/fda38\/lib\/python3.8\/site-packages\/sklearn\/utils\/validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\r\n    962         raise ValueError(\"y cannot be None\")\r\n    963 \r\n--> 964     X = check_array(\r\n    965         X,\r\n    966         accept_sparse=accept_sparse,\r\n\r\n~\/Programas\/Utilidades\/Lenguajes\/miniconda3\/envs\/fda38\/lib\/python3.8\/site-packages\/sklearn\/utils\/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\r\n    744                     array = array.astype(dtype, casting=\"unsafe\", copy=False)\r\n    745                 else:\r\n--> 746                     array = np.asarray(array, order=order, dtype=dtype)\r\n    747             except ComplexWarning as complex_warning:\r\n    748                 raise ValueError(\r\n\r\nValueError: setting an array element with a sequence.\r\n\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.8.0 | packaged by conda-forge | (default, Nov 22 2019, 19:11:38)  [GCC 7.3.0]\r\nexecutable: \/home\/carlos\/Programas\/Utilidades\/Lenguajes\/miniconda3\/envs\/fda38\/bin\/python3.8\r\n   machine: Linux-5.4.0-92-generic-x86_64-with-glibc2.10\r\n\r\nPython dependencies:\r\n          pip: 21.3.1\r\n   setuptools: 60.5.0\r\n      sklearn: 1.0.2\r\n        numpy: 1.22.0\r\n        scipy: 1.7.3\r\n       Cython: 0.29.26\r\n       pandas: 1.4.0\r\n   matplotlib: 3.5.1\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.0.0\r\n\r\nBuilt with OpenMP: True\n```\n","labels":["Bug","module:multiclass"],"created_at":"2022-06-28T12:39:29Z","comments":6,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23779"},{"issue_number":117,"repository":"scikit-learn\/scikit-learn","title":"fetch_lfw_pairs issue","description":"### Describe the bug\r\n\r\nOn two different machines for exactly same code, I am getting different result.\r\n\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```py\r\nfrom sklearn.datasets import fetch_lfw_pairs\r\nfrom sklearn.utils import resample\r\n\r\nX = fetch_lfw_pairs(\r\n    subset=\"test\",\r\n    # funneled=False,\r\n    # slice_=(slice(0, 250), slice(0, 250)),\r\n    resize=1,\r\n    color=True\r\n  )\r\n\r\nn_samples = 1\r\nimgs, y = resample(X.pairs, X.target, n_samples=n_samples, random_state=101)\r\n\r\nprint(imgs[0][0])\r\n```\r\n\r\n### Expected Results\r\n\r\nBoth on both machines should be the same.\r\n\r\n### Actual Results\r\n\r\nResult on Machine 1:\r\n\r\n```\r\narray([[[ 77.,  77.,  77.],\r\n        [ 94.,  88.,  76.],\r\n        [107., 101.,  77.],\r\n        ...,\r\n        [159., 124.,  97.],\r\n        [141., 110.,  84.],\r\n        [136., 109.,  82.]],\r\n\r\n       [[ 78.,  78.,  80.],\r\n        [108., 101.,  91.],\r\n        [128., 121.,  99.],\r\n        ...,\r\n        [159., 124.,  97.],\r\n        [151., 120.,  94.],\r\n        [149., 124.,  98.]],\r\n\r\n       [[ 82.,  80.,  85.],\r\n        [111., 104.,  96.],\r\n        [130., 122., 103.],\r\n        ...,\r\n        [163., 131., 105.],\r\n        [157., 129., 104.],\r\n        [159., 138., 110.]],\r\n\r\n       ...,\r\n\r\n       [[ 27.,  27.,  27.],\r\n        [ 27.,  27.,  27.],\r\n        [ 24.,  24.,  24.],\r\n        ...,\r\n        [151., 172., 235.],\r\n        [142., 163., 228.],\r\n        [128., 150., 210.]],\r\n\r\n       [[ 26.,  24.,  25.],\r\n        [ 33.,  33.,  33.],\r\n        [ 30.,  29.,  27.],\r\n        ...,\r\n        [150., 171., 236.],\r\n        [136., 157., 222.],\r\n        [112., 133., 194.]],\r\n\r\n       [[ 31.,  30.,  26.],\r\n        [ 36.,  35.,  33.],\r\n        [ 37.,  36.,  34.],\r\n        ...,\r\n        [146., 167., 230.],\r\n        [124., 145., 208.],\r\n        [ 93., 112., 171.]]], dtype=float32)\r\n```\r\n\r\nResult on Machine 2:\r\n```\r\narray([[[0.        , 0.00392157, 0.        ],\r\n        [0.00392157, 0.01176471, 0.04705882],\r\n        [0.01960784, 0.07450981, 0.16470589],\r\n        ...,\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.00392157]],\r\n\r\n       [[0.        , 0.        , 0.        ],\r\n        [0.00392157, 0.01176471, 0.05098039],\r\n        [0.01960784, 0.07843138, 0.16862746],\r\n        ...,\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.00392157]],\r\n\r\n       [[0.        , 0.00392157, 0.        ],\r\n        [0.00392157, 0.01568628, 0.05490196],\r\n        [0.01568628, 0.07450981, 0.16470589],\r\n        ...,\r\n        [0.        , 0.        , 0.00392157],\r\n        [0.        , 0.00392157, 0.        ],\r\n        [0.        , 0.00392157, 0.        ]],\r\n\r\n       ...,\r\n\r\n       [[0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ],\r\n        ...,\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ]],\r\n\r\n       [[0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ],\r\n        ...,\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ]],\r\n\r\n       [[0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ],\r\n        ...,\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ],\r\n        [0.        , 0.        , 0.        ]]], dtype=float32)\r\n\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nMachine 1:\r\n\r\nSystem:\r\n    python: 3.8.5 (default, Sep  4 2020, 07:30:14)  [GCC 7.3.0]\r\nexecutable: \/home\/anks\/miniconda3\/envs\/stealth\/bin\/python\r\n   machine: Linux-5.17.9-1-MANJARO-x86_64-with-glibc2.10\r\n\r\nPython dependencies:\r\n          pip: 21.2.4\r\n   setuptools: 58.0.4\r\n      sklearn: 1.0.2\r\n        numpy: 1.21.2\r\n        scipy: 1.7.3\r\n       Cython: 0.29.30\r\n       pandas: 1.3.0\r\n   matplotlib: 3.5.1\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nMachine 2:\r\n\r\nSystem:\r\n    python: 3.8.5 | packaged by conda-forge | (default, Sep 24 2020, 16:55:52)  [GCC 7.5.0]\r\nexecutable: \/home\/aimluser\/miniconda3\/envs\/application_env\/bin\/python3.8\r\n   machine: Linux-4.4.0-042stab145.3-x86_64-with-glibc2.10\r\n\r\nPython dependencies:\r\n          pip: 22.1.2\r\n   setuptools: 62.1.0\r\n      sklearn: 1.0.2\r\n        numpy: 1.22.3\r\n        scipy: 1.8.0\r\n       Cython: None\r\n       pandas: 1.3.0\r\n   matplotlib: 3.5.2\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n```\r\n","labels":["Bug","module:datasets","Needs Investigation"],"created_at":"2022-06-24T13:15:12Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23750"},{"issue_number":118,"repository":"scikit-learn\/scikit-learn","title":"BUG unpenalized GLM with LBFGS solver does not yield minimum norm solution with fit_intercept=True","description":"### Summary\r\nWhile unpenalized GLMs like `PoissonRegressor(alpha=0, fit_intercept=False)` yield a minimum norm solution on wide data, `PoissonRegressor(alpha=0, fit_intercept=True)` does not.\r\n\r\nDetected in #23314.\r\n\r\n#### Minimal Reproducible Example\r\n```python\r\nimport numpy as np\r\nfrom numpy.testing import assert_allclose\r\nimport pytest\r\nfrom sklearn.datasets import make_low_rank_matrix\r\nfrom sklearn.linear_model import PoissonRegressor\r\nfrom sklearn.linear_model._linear_loss import LinearModelLoss\r\n\r\n\r\nalpha = 0  # unpenalized, even tiny values like 1e-10 does not help.\r\nfit_intercept = True  # if set to False, all tests pass.\r\nmodel = PoissonRegressor(\r\n    alpha = alpha,\r\n    fit_intercept=fit_intercept,\r\n    tol=1e-12,\r\n    max_iter=1000,\r\n)\r\n\r\n# same as glm_dataset in test_glm.py for \"wide\"\r\nn_samples, n_features = 4, 12\r\nk = n_samples\r\nrng = np.random.RandomState(42)\r\nX = make_low_rank_matrix(\r\n    n_samples=n_samples,\r\n    n_features=n_features,\r\n    effective_rank=k,\r\n    tail_strength=0.1,\r\n    random_state=rng,\r\n)\r\nX[:, -1] = 1  # last columns acts as intercept\r\nU, s, Vt = np.linalg.svd(X, full_matrices=False)\r\n\r\nraw_prediction = rng.uniform(low=-3, high=3, size=n_samples)\r\n# minimum norm solution min ||w||_2 such that raw_prediction = X w:\r\n# w = X'(XX')^-1 raw_prediction = V s^-1 U' raw_prediction\r\ncoef_unpenalized = Vt.T @ np.diag(1 \/ s) @ U.T @ raw_prediction\r\n\r\nlinear_loss = LinearModelLoss(base_loss=model._get_loss(), fit_intercept=True)\r\nsw = np.full(shape=n_samples, fill_value=1 \/ n_samples)\r\ny = linear_loss.base_loss.link.inverse(raw_prediction)\r\n\r\nif fit_intercept:\r\n    X = X[:, :-1]  # remove intercept\r\n    intercept = coef_unpenalized[-1]\r\n    coef = coef_unpenalized[:-1]\r\nelse:\r\n    intercept = 0\r\n    coef = coef_unpenalized\r\n\r\nmodel.fit(X, y)\r\n\r\n# This is true. So we have a solution.\r\nassert_allclose(model.predict(X), y, rtol=1e-6)\r\n\r\n# But it is NOT the minimum norm solution\r\nnorm_solution = np.linalg.norm(np.r_[intercept, coef])\r\nnorm_model = np.linalg.norm(np.r_[model.intercept_, model.coef_])\r\nprint(f\"norm_solution = {norm_solution}\")\r\nprint(f\"norm_model    = {norm_model}\")\r\nassert norm_model == pytest.approx(norm_solution, rel=1e-4)\r\n```\r\nOutput\r\n```\r\nnorm_solution = 2.494710738642327\r\nnorm_model    = 2.5295744931923707\r\nAssertionError\r\n```\r\n\r\nThe strange thing is that setting `fit_intercept=False`, norms and coefficients are equal, i.e. the minimum norm solution is returned by the LBFGS solver.\r\nOn top, even setting a tiny positive penalty like `alpha=1e-10` does not give the minimum norm solution.","labels":["Bug","module:linear_model"],"created_at":"2022-06-17T09:41:36Z","comments":4,"reactions":2,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23670"},{"issue_number":119,"repository":"scikit-learn\/scikit-learn","title":"`distance_threshold` not respected in `AgglomerativeClustering` with sparse `connectivity`","description":"### Describe the bug\r\n\r\nWhen passing a sparse `connectivity` to `AgglomerativeClustering` constrained by a given `distance_threshold`, the current implementation may return a clustering not respecting this constraint.\r\n\r\nThis is at least true for the 'complete' and 'average' linkage criteria. I found the origin of the problem in [`linkage_tree`](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/80598905e517759b4696c74ecc35c6e2eb508cff\/sklearn\/cluster\/_agglomerative.py#L384), because I saw the only moment the inter-point distances were calculated was here:\r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/80598905e517759b4696c74ecc35c6e2eb508cff\/sklearn\/cluster\/_agglomerative.py#L565-L568\r\n\r\nSo the only distance info we have is for connected points. Then, for `linkage='complete'`, in the situation where you have clustered together two neighbours A and B, and in the next step you consider aggregating C, which is B's neighbour but not A's, if `d(A, C) > distance_threshold` but at the same time `d(B, C)` is the minimum remaining distance (at the top of the heap), then C will be aggregated with these two.\r\n\r\nNow I'm not sure at all on how to fix this properly. One option is to compute all pairwise distances in the connection matrix `A` when `A` and the heap `inertia` are initialised:\r\n\r\n https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/80598905e517759b4696c74ecc35c6e2eb508cff\/sklearn\/cluster\/_agglomerative.py#L596-L604\r\n\r\nor to somehow compute the ones needed on the fly when two nodes with different neighbourhoods are merged. Both solutions seem quite expensive though.\r\n\r\nTo give you some context, I encountered this while trying to do a spatial aggregation of administrative boundaries (England's LSOAs), in such a way that an indicator (let's say average income) does not vary too much within the aggregated regions. In this kind of geographic context, this is a rather common task, the [Ward variant](https:\/\/pysal.org\/spopt\/generated\/spopt.region.WardSpatial.html#spopt.region.WardSpatial) is even implemented in the `pysal` package.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nimport pandas as pd\r\nfrom sklearn.feature_extraction.image import grid_to_graph\r\nfrom sklearn.cluster import AgglomerativeClustering\r\n\r\nrng = np.random.RandomState(0)\r\nn_samples = 100\r\nconn = np.ones([n_samples, n_samples], dtype=bool)\r\n# Add some random sparsity, 70% at most\r\nmasked = (rng.random((n_samples, 70)) * n_samples).astype(int)\r\nfor i, nbh in enumerate(masked):\r\n    conn[i, nbh] = False\r\n    conn[nbh, i] = False\r\nX = rng.randn(n_samples, 1)\r\nclustering = AgglomerativeClustering(\r\n    n_clusters=None,\r\n    distance_threshold=1,\r\n    connectivity=conn,\r\n    linkage='complete',\r\n    affinity='l1',\r\n)\r\nclustering.fit(X)\r\ndf = pd.DataFrame({'labels': clustering.labels_, 'x': X.flatten()})\r\nclustered_df = df.groupby(\"labels\")[\"x\"].agg([\"min\", \"max\"])\r\n\r\n# This should be < distance_threshold\r\nprint((clustered_df['max'] - clustered_df['min']).max())\r\n# 1.20248995732655\r\n```\r\n\r\n### Expected Results\r\n\r\nA float lower than 1, meaning the linkage criterion was respected.\r\n\r\n### Actual Results\r\n\r\n1.20248995732655\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21)  [GCC 10.3.0]\r\nexecutable: \/home\/thomaslouf\/.conda\/envs\/ses-ling\/bin\/python\r\n   machine: Linux-4.15.0-144-generic-x86_64-with-glibc2.27\r\n\r\nPython dependencies:\r\n      sklearn: 1.1.1\r\n          pip: 22.1.2\r\n   setuptools: 62.3.2\r\n        numpy: 1.22.4\r\n        scipy: 1.8.1\r\n       Cython: None\r\n       pandas: 1.4.2\r\n   matplotlib: 3.5.2\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/thomaslouf\/.conda\/envs\/ses-ling\/lib\/libopenblasp-r0.3.20.so\r\n        version: 0.3.20\r\nthreading_layer: pthreads\r\n   architecture: Bulldozer\r\n    num_threads: 32\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/home\/thomaslouf\/.conda\/envs\/ses-ling\/lib\/libgomp.so.1.0.0\r\n        version: None\r\n    num_threads: 1\r\n```\r\n","labels":["Bug","module:cluster","Needs Investigation"],"created_at":"2022-06-06T15:48:21Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23550"},{"issue_number":120,"repository":"scikit-learn\/scikit-learn","title":"RFECV race condition on estimator","description":"In RFECV, at\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/fb3ed90fb501a755ce2938fb566bd0f6e2235054\/sklearn\/feature_selection\/_rfe.py#L723-L726\r\nthe estimator is passed as-is to the fit function. Since `fit` modifies the object without copying, this is prone to race conditions (see example below).\r\n\r\nContrast this to BaseSearchCV, where the estimator is properly cloned:\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/fb3ed90fb501a755ce2938fb566bd0f6e2235054\/sklearn\/model_selection\/_search.py#L823-L833\r\n\r\n---\r\n\r\n<details>\r\n<summary>On my system, with parameter `n_jobs=-1`, I got the following error:<\/summary>\r\n\r\n```\r\n5 fits failed with the following error:\r\nTraceback (most recent call last):\r\n  File \"...\/site-packages\/sklearn\/model_selection\/_validation.py\", line 686, in _fit_and_score\r\n    estimator.fit(X_train, y_train, **fit_params)\r\n  File \"...\/site-packages\/sklearn\/feature_selection\/_rfe.py\", line 723, in fit\r\n    scores = parallel(\r\n  File \"...\/site-packages\/joblib\/parallel.py\", line 1056, in __call__\r\n    self.retrieve()\r\n  File \"...\/site-packages\/joblib\/parallel.py\", line 935, in retrieve\r\n    self._output.extend(job.get(timeout=self.timeout))\r\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/pool.py\", line 771, in get\r\n    raise self._value\r\n  File \"\/usr\/lib\/python3.10\/multiprocessing\/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"...\/site-packages\/joblib\/_parallel_backends.py\", line 595, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"...\/site-packages\/joblib\/parallel.py\", line 262, in __call__\r\n    return [func(*args, **kwargs)\r\n  File \"...\/site-packages\/joblib\/parallel.py\", line 262, in <listcomp>\r\n    return [func(*args, **kwargs)\r\n  File \"...\/site-packages\/sklearn\/utils\/fixes.py\", line 117, in __call__\r\n    return self.function(*args, **kwargs)\r\n  File \"...\/site-packages\/sklearn\/feature_selection\/_rfe.py\", line 37, in _rfe_single_fit\r\n    return rfe._fit(\r\n  File \"...\/site-packages\/sklearn\/feature_selection\/_rfe.py\", line 327, in _fit\r\n    self.scores_.append(step_score(self.estimator_, features))\r\n  File \"...\/site-packages\/sklearn\/feature_selection\/_rfe.py\", line 40, in <lambda>\r\n    lambda estimator, features: _score(\r\n  File \"...\/site-packages\/sklearn\/model_selection\/_validation.py\", line 767, in _score\r\n    scores = scorer(estimator, X_test, y_test)\r\n  File \"...\/site-packages\/sklearn\/metrics\/_scorer.py\", line 219, in __call__\r\n    return self._score(\r\n  File \"...\/site-packages\/sklearn\/metrics\/_scorer.py\", line 261, in _score\r\n    y_pred = method_caller(estimator, \"predict\", X)\r\n  File \"...\/site-packages\/sklearn\/metrics\/_scorer.py\", line 71, in _cached_call\r\n    return getattr(estimator, method)(*args, **kwargs)\r\n  File \"...\/site-packages\/sklearn\/ensemble\/_forest.py\", line 835, in predict\r\n    return self.classes_.take(np.argmax(proba, axis=1), axis=0)\r\nAttributeError: 'list' object has no attribute 'take'\r\n```\r\n<\/details>\r\n\r\nIt is generated from the following snippet:\r\n```py\r\n  rf = RandomForestClassifier()\r\n  rfecv = RFECV(rf, scoring='accuracy', n_jobs=-1)\r\n  rfecv.fit(X_train, y_train)\r\n```\r\n\r\nThe error appears to happen because `n_outputs_` is not constant between runs. The error does not happen without parallelism.","labels":["Bug","module:feature_selection"],"created_at":"2022-06-03T09:07:46Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23533"},{"issue_number":121,"repository":"scikit-learn\/scikit-learn","title":"Poor performance of KNeighborsClassifier for sklearn version >=0.24.2","description":"### Describe the bug\r\n\r\nHi. I used **sklearn.neighbors.KNeighborsClassifier** in my program but I noticed that its runtime increased when adopting higher versions of  sklearn.\r\n\r\nAs shown in the following experiment results, this model performs best if we use sklearn <0.24.2.\r\n\r\n**My question is that why does such a library version perform better? Are there any issues in newer versions?**\r\n\r\nThe detailed information is as follows:\r\nRuntime | Memory | Version\r\n-- | -- | --\r\n**451.2495478** | **2337.8830499649** | **1.0.1**\r\n**441.4340003** | **2338.7937555313** | **0.24.2**\r\n26.25025550 | 414.1812286377 | 0.23.2\r\n26.42619310 | 408.6705312729 | 0.22.1\r\n26.42619310 | 408.6052923203 | 0.22\r\n31.0837617 | 409.0471181870 | 0.21.3\r\n29.0283316 | 408.5676021576 | 0.20.3\r\n22.6652508 | 409.7873554230 | 0.19.2\r\n\r\nMy program only used the apis of  KNeighborsClassifier provided by sklearn.\r\n```Python\r\nknn = KNeighborsClassifier(n_neighbors = 3)\r\n```\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\nI create a minor program that used KNeighborsClassifier and you can test it on [colab](https:\/\/colab.research.google.com\/drive\/1GU9Hw8jKObcO5a54-7cNXAA-t9BJuKFi?usp=sharing). The runtime is still higher when adopting the newer sklearn version on this example.\r\n\r\n[train.csv](https:\/\/drive.google.com\/file\/d\/1f77gSB472ouhZ5bK0ipocBVa3SweGKLc\/view?usp=sharing)\r\n\r\nUsing the above `train.csv` and the following snippet:\r\n\r\n```python\r\n# %%\r\n# %%\r\nimport pandas as pd\r\n\r\ntrain_data = pd.read_csv(\"train.csv\")\r\ntotal = train_data.isnull().sum().sort_values(ascending=False)\r\npercent_1 = train_data.isnull().sum() \/ train_data.isnull().count() * 100\r\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\r\nmissing_data = pd.concat([total, percent_2], axis=1, keys=[\"Total\", \"%\"])\r\ncor = train_data.corr()\r\ncor_target = abs(cor[\"target\"])\r\nrelevant_features = cor_target[cor_target > 0.5]\r\ns = train_data.dtypes == \"object\"\r\ntrain_data_cat_var = list(s[s].index)\r\ntrain_data.drop(\r\n    [\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"ord_3\", \"ord_4\", \"ord_5\"],\r\n    axis=1,\r\n    inplace=True,\r\n)\r\ntrain_data_cat_var = [\r\n    ele\r\n    for ele in train_data_cat_var\r\n    if ele\r\n    not in [\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"ord_3\", \"ord_4\", \"ord_5\"]\r\n]\r\nfinal_train_data = pd.get_dummies(\r\n    train_data, columns=train_data_cat_var, drop_first=True\r\n)\r\nfeatures = final_train_data.drop([\"target\"], axis=1).columns\r\ntarget = final_train_data[\"target\"]\r\n\r\n# %%\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(\r\n    final_train_data[features], target, test_size=0.2\r\n)\r\n\r\n# %%\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\n\r\n# %%\r\n%%time\r\nknn = KNeighborsClassifier(n_neighbors=3, algorithm=\"brute\")\r\nknn.fit(X_train, y_train)\r\nY_pred_knn = knn.predict(X_test)\r\n```\r\n\r\n\r\n![image-20220520110041914](https:\/\/user-images.githubusercontent.com\/26241319\/169444004-c50c2e22-489c-4ce9-8771-99bec82693bc.png)\r\n\r\n\r\n### Expected Results\r\n\r\nSimilar runtime for the different sklearn versions or better performance for higher sklearn versions.\r\n\r\n### Actual Results\r\n\r\nAccording to the above experiment results, when sklearn version is 0.24.2 or 1.0.1, the performance of the model is worse.\r\n\r\n### Versions\r\n\r\n\r\nI test my program with sklearn 1.0.1, 0.24.2, 0.23.2, 0.22.1, 0.22, 0.21.3, 0.20.3 and 0.19.2.\r\n\r\n","labels":["Bug","module:neighbors","Needs Investigation"],"created_at":"2022-05-20T03:30:52Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23427"},{"issue_number":122,"repository":"scikit-learn\/scikit-learn","title":"The data type of input data for LinearRegression class will affect the results","description":"### Describe the bug\r\n\r\nOur team just used the class sklearn.linear_model.LinearRegression to do multi-linear regression. And, we found out that the same data, which means the values are identical for each element, with different data formats will return different coefficients. The array we use are all integer values but saved as float32 and float64, separately. However, the array in float64 as input get us the expected result, but float32 doesn't.\r\n\r\nThe code below is a simple reproducible example.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nIn [1]: from sklearn.linear_model import LinearRegression\r\n    ...: import numpy as np\r\n    ...: rng = np.random.default_rng(0)\r\n    ...: x_1_array = rng.integers(low=1000, high=10000, size=10_000_000)\r\n    ...: x_2_array = rng.integers(low=1000, high=10000, size=10_000_000)\r\n    ...: x_3_array = rng.integers(low=1000, high=10000, size=10_000_000)\r\n    ...: y_array = 3 * x_1_array + x_2_array - 5 * x_3_array\r\n    ...: intercept_data = rng.integers(low=-3, high=3)\r\n    ...: print(f'True intercept {intercept_data}')\r\n    ...: y_array += intercept_data\r\n    ...: X_train = np.stack([x_1_array,x_2_array,x_3_array],axis=-1)\r\n    ...: y_train = y_array\r\n    ...: linreg = LinearRegression()\r\n    ...: model = linreg.fit(X_train.astype(np.float64), y_train)\r\n    ...: print(f'Float 64 Intercept: {linreg.intercept_}')\r\n    ...: print(f'Float 64 Coefficient: {linreg.coef_}')\r\n    ...: model = linreg.fit(X_train.astype(np.float32), y_train)\r\n    ...: print(f'Float 32 Intercept: {linreg.intercept_}')\r\n    ...: print(f'Float 32 Coefficient: {linreg.coef_}')\r\n```\r\n\r\n### Expected Results\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/51532833\/168492124-d98f9663-4899-47ee-83b2-4952462c37a6.png)\r\n\r\n### Actual Results\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/51532833\/168492136-4e0f4100-9e48-4f58-aa2a-97e826f06c61.png)\r\n\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.7.9 (v3.7.9:13c94747c7, Aug 15 2020, 01:31:08)  [Clang 6.0 (clang-600.0.57)]\r\nexecutable: \/Users\/lazy\/PycharmProjects\/projects\/bin\/python\r\n   machine: Darwin-21.4.0-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n          pip: 21.3\r\n   setuptools: 58.2.0\r\n      sklearn: 1.0.2\r\n        numpy: 1.21.4\r\n        scipy: 1.7.2\r\n       Cython: None\r\n       pandas: 1.3.4\r\n   matplotlib: 3.4.3\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n```\r\n","labels":["Bug","module:linear_model","float32"],"created_at":"2022-05-15T20:17:09Z","comments":8,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23376"},{"issue_number":123,"repository":"scikit-learn\/scikit-learn","title":"SelfTrainingClassifier on a Pipeline","description":"### Describe the bug\r\n\r\nSelfTrainingClassifier cannot be fit on text data even if the base_estimator parameter is an estimator that can accept text data (e.g. a pipeline with text preprocessing). In particular, it seems that SelfTrainingClassifier validates the data on the classifier within the pipeline (hence being unable to accept text) instead of validating on the entire pipeline (which can accept text).\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport pandas as pd\r\nfrom sklearn.datasets import fetch_20newsgroups\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.naive_bayes import MultinomialNB\r\nfrom sklearn.semi_supervised import SelfTrainingClassifier\r\n\r\n#pulling text data\r\ncategories = ['alt.atheism', 'soc.religion.christian']\r\ntwenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\r\n\r\nX = pd.DataFrame({'text': twenty_train.data})\r\ny = twenty_train.target\r\n#changing some labels to -1 for semi supervised\r\ny[len(y) - 100:len(y)] = -1\r\n\r\n#building pipeline\r\npreprocessor = ColumnTransformer(\r\n    transformers =  [\r\n        ('vectorizer', CountVectorizer(), 'text')\r\n    ])\r\n\r\nclassifier = MultinomialNB()\r\n\r\npipe = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', classifier)])\r\n\r\n#fitting SelfTrainingClassifier with the pipeline as the base estimator\r\nstc = SelfTrainingClassifier(pipe)\r\nstc.fit(X,y)\r\n\r\n#note that fitting pipe does work\r\n#pipe.fit(X,y)\r\n```\r\n\r\n### Expected Results\r\n\r\nNo error is thrown.\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\nValueError                                Traceback (most recent call last)\r\nInput In [27], in <cell line: 30>()\r\n     28 #fitting SelfTrainingClassifier with the pipeline as the base estimator\r\n     29 stc = SelfTrainingClassifier(pipe)\r\n---> 30 stc.fit(X,y)\r\n\r\nFile ~\/code\/scikit-learn\/sklearn\/semi_supervised\/_self_training.py:181, in SelfTrainingClassifier.fit(self, X, y)\r\n    162 \"\"\"\r\n    163 Fit self-training classifier using `X`, `y` as training data.\r\n    164 \r\n   (...)\r\n    177     Fitted estimator.\r\n    178 \"\"\"\r\n    179 # we need row slicing support for sparce matrices, but costly finiteness check\r\n    180 # can be delegated to the base estimator.\r\n--> 181 X, y = self._validate_data(\r\n    182     X, y, accept_sparse=[\"csr\", \"csc\", \"lil\", \"dok\"], force_all_finite=False\r\n    183 )\r\n    185 if self.base_estimator is None:\r\n    186     raise ValueError(\"base_estimator cannot be None!\")\r\n\r\nFile ~\/code\/scikit-learn\/sklearn\/base.py:596, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, **check_params)\r\n    594         y = check_array(y, input_name=\"y\", **check_y_params)\r\n    595     else:\r\n--> 596         X, y = check_X_y(X, y, **check_params)\r\n    597     out = X, y\r\n    599 if not no_val_X and check_params.get(\"ensure_2d\", True):\r\n\r\nFile ~\/code\/scikit-learn\/sklearn\/utils\/validation.py:1074, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\r\n   1069         estimator_name = _check_estimator_name(estimator)\r\n   1070     raise ValueError(\r\n   1071         f\"{estimator_name} requires y to be passed, but the target y is None\"\r\n   1072     )\r\n-> 1074 X = check_array(\r\n   1075     X,\r\n   1076     accept_sparse=accept_sparse,\r\n   1077     accept_large_sparse=accept_large_sparse,\r\n   1078     dtype=dtype,\r\n   1079     order=order,\r\n   1080     copy=copy,\r\n   1081     force_all_finite=force_all_finite,\r\n   1082     ensure_2d=ensure_2d,\r\n   1083     allow_nd=allow_nd,\r\n   1084     ensure_min_samples=ensure_min_samples,\r\n   1085     ensure_min_features=ensure_min_features,\r\n   1086     estimator=estimator,\r\n   1087     input_name=\"X\",\r\n   1088 )\r\n   1090 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\r\n   1092 check_consistent_length(X, y)\r\n\r\nFile ~\/code\/scikit-learn\/sklearn\/utils\/validation.py:856, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\r\n    854         array = array.astype(dtype, casting=\"unsafe\", copy=False)\r\n    855     else:\r\n--> 856         array = np.asarray(array, order=order, dtype=dtype)\r\n    857 except ComplexWarning as complex_warning:\r\n    858     raise ValueError(\r\n    859         \"Complex data not supported\\n{}\\n\".format(array)\r\n    860     ) from complex_warning\r\n\r\nFile ~\/mambaforge\/envs\/dev\/lib\/python3.10\/site-packages\/pandas\/core\/generic.py:2064, in NDFrame.__array__(self, dtype)\r\n   2063 def __array__(self, dtype: npt.DTypeLike | None = None) -> np.ndarray:\r\n-> 2064     return np.asarray(self._values, dtype=dtype)\r\n\r\nValueError: could not convert string to float: 'From: [...]'\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.10.4 \r\n\r\nPython dependencies:\r\n          pip: 22.0.4\r\n   setuptools: 62.1.0\r\n      sklearn: 1.0.2\r\n        numpy: 1.21.5\r\n        scipy: 1.7.3\r\n       Cython: None\r\n       pandas: 1.4.2\r\n   matplotlib: None\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n```\r\n","labels":["Bug","module:semi_supervised"],"created_at":"2022-05-10T16:04:11Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23323"},{"issue_number":124,"repository":"scikit-learn\/scikit-learn","title":"Yeo-Johnson Power Transformer gives Numpy warning (and raises scipy.optimize._optimize.BracketError in some cases)","description":"### Describe the bug\r\n\r\nWhen I use a power transformer with yeo-johnson method I get this warning in numpy:\r\n\r\n`..\/lib\/python3.10\/site-packages\/numpy\/core\/_methods.py:235: RuntimeWarning: overflow encountered in multiply`\r\n\r\nStrangely I can't surpress this warning with filter.warnings()\r\n\r\n**Tl;dr: The eroor seems to appear when multiple instances of the transformer are called, in this example because n_jobs=2 is set in the preprocessor. But the bug also appears with n_jobs=1 when you call this preprocessor in a grid search for example.**\r\n\r\n### Steps\/Code to Reproduce\r\n\r\nThe following code snippet will raise a warning:\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.preprocessing import PowerTransformer\r\n\r\nrng = np.random.default_rng(0)\r\nX = rng.exponential(size=(50,))\r\nX -= X.max()\r\nX = -X\r\nX += 100\r\nX = X.reshape(-1, 1)\r\n\r\nmethod = \"yeo-johnson\"\r\ntransformer = PowerTransformer(method=method, standardize=False)\r\ntransformer.fit_transform(X)\r\n```\r\n\r\n### Expected Results\r\n\r\nNo Warnings\r\n\r\n### Actual Results\r\n```pytb\r\n\/Users\/glemaitre\/mambaforge\/envs\/dev\/lib\/python3.8\/site-packages\/numpy\/core\/_methods.py:233: RuntimeWarning: overflow encountered in multiply\r\n  x = um.multiply(x, x, out=x)\r\n\r\n```\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.8.12 | packaged by conda-forge | (default, Sep 16 2021, 01:38:21)  [Clang 11.1.0 ]\r\nexecutable: \/Users\/glemaitre\/mambaforge\/envs\/dev\/bin\/python\r\n   machine: macOS-12.3.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n      sklearn: 1.2.dev0\r\n          pip: 21.3\r\n   setuptools: 58.2.0\r\n        numpy: 1.21.6\r\n        scipy: 1.8.0\r\n       Cython: 0.29.24\r\n       pandas: 1.4.2\r\n   matplotlib: 3.4.3\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/Users\/glemaitre\/mambaforge\/envs\/dev\/lib\/libopenblas_vortexp-r0.3.18.dylib\r\n        version: 0.3.18\r\nthreading_layer: openmp\r\n   architecture: VORTEX\r\n    num_threads: 8\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libomp\r\n       filepath: \/Users\/glemaitre\/mambaforge\/envs\/dev\/lib\/libomp.dylib\r\n        version: None\r\n    num_threads: 8\r\n```\r\n","labels":["Bug","module:preprocessing"],"created_at":"2022-05-10T09:40:28Z","comments":28,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23319"},{"issue_number":125,"repository":"scikit-learn\/scikit-learn","title":"Investigate SAG\/SAGA solver","description":"#### Description\r\nThe newly introduced tight tests for `Ridge` in #22910 together with the random seed fixture in #22749 revealed some shortcomings of the sag and saga solver:\r\n\r\n1. ~~It shows some random behavior even with fixed random seed.~~\r\n2. The `tol` needs to be set much smaller to receive comparable results with the other solvers of `Ridge`.\r\n\r\nIdeally, the cause for both issues can be identified and fixed.\r\n\r\n#### Some links for context\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23014\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/23017\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/23026\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/23152\r\n#23177\r\n\r\n","labels":["Bug","Moderate","help wanted","module:linear_model"],"created_at":"2022-04-21T15:42:38Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23180"},{"issue_number":126,"repository":"scikit-learn\/scikit-learn","title":"Wrong normalization for pseudolikelihood in Restricted Boltzmann Machine.","description":"### Describe the bug\n\nPseudolikelihood of a sample under Restricted Boltzmann machine includes  the number of features  as a multiplier. \r\nThus pseudolikelihood reflects the dimensionality of visible units, apart from the sample probability. \r\nI believe this multiplier should be removed. \n\n### Steps\/Code to Reproduce\n\nfrom sklearn.neural_network import BernoulliRBM\r\nimport numpy as np\r\n \r\nRBM_= BernoulliRBM(verbose=1) \r\nX_10= np.ones([10,10])\r\nX_1000= np.ones([10,1000])\r\n                  \r\n\r\nprint('Learning RBM with 10 visible units:')\r\nRBM_.fit(X_small)\r\nprint(100*'_')\r\nprint('Learning RBM with 1000 visible units')\r\nRBM_.fit(X_large)\r\n                  \n\n### Expected Results\n\nThe pseudolikelihoods for 10 and 1000 visible units should be of the same order at corresponding learning stages.\n\n### Actual Results\n\nLearning RBM with 10 visible units:\r\n[BernoulliRBM] Iteration 1, pseudo-likelihood = -0.19, time = 0.00s\r\n[BernoulliRBM] Iteration 2, pseudo-likelihood = -0.28, time = 0.00s\r\n[BernoulliRBM] Iteration 3, pseudo-likelihood = -0.22, time = 0.00s\r\n[BernoulliRBM] Iteration 4, pseudo-likelihood = -0.12, time = 0.00s\r\n[BernoulliRBM] Iteration 5, pseudo-likelihood = -0.10, time = 0.00s\r\n[BernoulliRBM] Iteration 6, pseudo-likelihood = -0.07, time = 0.00s\r\n[BernoulliRBM] Iteration 7, pseudo-likelihood = -0.07, time = 0.00s\r\n[BernoulliRBM] Iteration 8, pseudo-likelihood = -0.08, time = 0.00s\r\n[BernoulliRBM] Iteration 9, pseudo-likelihood = -0.10, time = 0.01s\r\n[BernoulliRBM] Iteration 10, pseudo-likelihood = -0.06, time = 0.00s\r\n____________________________________________________________________________________________________\r\nLearning RBM with 1000 visible units\r\n[BernoulliRBM] Iteration 1, pseudo-likelihood = -3.80, time = 0.01s\r\n[BernoulliRBM] Iteration 2, pseudo-likelihood = -0.76, time = 0.02s\r\n[BernoulliRBM] Iteration 3, pseudo-likelihood = -0.94, time = 0.01s\r\n[BernoulliRBM] Iteration 4, pseudo-likelihood = -1.97, time = 0.01s\r\n[BernoulliRBM] Iteration 5, pseudo-likelihood = -2.01, time = 0.01s\r\n[BernoulliRBM] Iteration 6, pseudo-likelihood = -0.61, time = 0.01s\r\n[BernoulliRBM] Iteration 7, pseudo-likelihood = -1.06, time = 0.01s\r\n[BernoulliRBM] Iteration 8, pseudo-likelihood = -1.09, time = 0.01s\r\n[BernoulliRBM] Iteration 9, pseudo-likelihood = -0.28, time = 0.01s\r\n[BernoulliRBM] Iteration 10, pseudo-likelihood = -1.01, time = 0.01s\r\n\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]\r\nexecutable: \/home\/x\/PycharmProjects\/RBM\/env\/bin\/python\r\n   machine: Linux-4.15.0-142-generic-x86_64-with-debian-stretch-sid\r\n\r\nPython dependencies:\r\n          pip: 22.0.4\r\n   setuptools: 40.8.0\r\n      sklearn: 1.0.2\r\n        numpy: 1.21.5\r\n        scipy: 1.7.3\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: 3.5.1\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\n```\n","labels":["Bug","module:neural_network","Needs Investigation"],"created_at":"2022-04-21T15:30:40Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23179"},{"issue_number":127,"repository":"scikit-learn\/scikit-learn","title":"Cache final transformer in pipeline with memory setting","description":"### Describe the bug\n\nWhen setting the `memory` parameter of a transformer `Pipeline` (i.e., one whose last step is a transformer), the final transformer is not cached.\r\n\r\nDiscovered at https:\/\/stackoverflow.com\/q\/71812869\/10495893.\n\n### Steps\/Code to Reproduce\n\n```\r\nfrom sklearn.base import BaseEstimator, TransformerMixin\r\nfrom sklearn.pipeline import Pipeline\r\nimport time\r\n\r\nclass Test(BaseEstimator, TransformerMixin):\r\n    def __init__(self, col):\r\n        self.col = col\r\n\r\n    def fit(self, X, y=None):\r\n        print(self.col)\r\n        return self\r\n\r\n    def transform(self, X, y=None):\r\n        for t in range(5):\r\n            # just to slow it down \/ check caching.\r\n            print(\".\")\r\n            time.sleep(1)\r\n        #print(self.col)\r\n        return X\r\n\r\npipline = Pipeline(\r\n    [\r\n        (\"test\", Test(col=\"this_column\")),\r\n        (\"test2\", Test(col=\"that_column\"))\r\n    ],\r\n    memory=\"tmp\/cache\",\r\n)\r\n\r\npipline.fit(None)\r\npipline.fit(None)\r\npipline.fit(None)\r\n```\n\n### Expected Results\n\n```\r\nthis_column\r\n.\r\n.\r\n.\r\n.\r\n.\r\nthat_column\r\n```\n\n### Actual Results\n\n```\r\nthis_column\r\n.\r\n.\r\n.\r\n.\r\n.\r\nthat_column\r\nthat_column\r\nthat_column\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.7.13 (default, Mar 16 2022, 17:37:17)  [GCC 7.5.0]\r\nexecutable: \/usr\/bin\/python3\r\n   machine: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic\r\n\r\nPython dependencies:\r\n          pip: 21.1.3\r\n   setuptools: 57.4.0\r\n      sklearn: 1.0.2\r\n        numpy: 1.21.5\r\n        scipy: 1.4.1\r\n       Cython: 0.29.28\r\n       pandas: 1.3.5\r\n   matplotlib: 3.2.2\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\n```\n","labels":["Bug","module:pipeline"],"created_at":"2022-04-11T14:55:57Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23112"},{"issue_number":128,"repository":"scikit-learn\/scikit-learn","title":"`SequentialFeatureSelector` is not passing pandas df to estimator\/pipeline","description":"### Describe the bug\n\n`SequentialFeatureSelector` cannot be used with a pipeline that expects a pandas dataframe (e.g. a one containing a `ColumnTransformer`) as an input.\r\n\r\n\r\nAt the same time the same pipeline can be used in `cross_val_score`.\n\n### Steps\/Code to Reproduce\n\n```\r\nfrom sklearn.feature_selection import SequentialFeatureSelector\r\nfrom sklearn.compose import ColumnTransformer\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.compose import make_column_selector\r\nfrom sklearn.pipeline import Pipeline\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.model_selection import cross_val_score\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\ndf = pd.DataFrame(data=np.random.rand(10,4), columns=[\"1\",\"2\", \"3\", \"4\"])\r\ny = np.random.randint(0,2, size=(10,))\r\n\r\nct = ColumnTransformer(\r\n    transformers=[\r\n        (\"numerical\", StandardScaler(), make_column_selector(pattern=\"1\")),\r\n    ],\r\n    remainder=\"passthrough\",\r\n)\r\n\r\npipeline = Pipeline([(\"ct\", ct), (\"lr\", LogisticRegression())])\r\n\r\n# this works\r\nprint(\"cross_val_score\", cross_val_score(pipeline, df, y, cv=2))\r\n\r\nsfs = SequentialFeatureSelector(pipeline, cv=2)\r\nsfs.fit(df, y=y)\r\n```\n\n### Expected Results\n\nNo error is thrown and the features are selected.\n\n### Actual Results\n\n```\r\n....sklearn\/model_selection\/_validation.py:372: FitFailedWarning: \r\n2 fits failed out of a total of 2.\r\n...\r\nValueError: make_column_selector can only be applied to pandas dataframes\r\n```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]\r\nexecutable: \/root\/.cache\/pypoetry\/virtualenvs\/science-yd9zGFjU-py3.9\/bin\/python\r\n   machine: Linux-5.13.0-35-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n          pip: 22.0.3\r\n   setuptools: 60.9.3\r\n      sklearn: 1.0.2\r\n        numpy: 1.19.5\r\n        scipy: 1.8.0\r\n       Cython: None\r\n       pandas: 1.4.1\r\n   matplotlib: 3.5.1\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n```\n```\n","labels":["Bug","module:feature_selection"],"created_at":"2022-04-11T11:15:18Z","comments":2,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/23107"},{"issue_number":129,"repository":"scikit-learn\/scikit-learn","title":"BUG unpenalized Ridge does not give minimum norm solution","description":"#### Describe the bug\r\nAs noted in #22910, `Ridge(alpha=0, fit_intercept=True)` does not give the minimal norm solution for wide data, i.e. `n_features > n_samples`.\r\n\r\nNote that we nowhere guarantee that we provide the **minimum norm solution**.\r\n\r\nEdit: Same seems to hold for `LinearRegression`, see #26164.\r\n\r\n#### Probable Cause\r\nFor wide X, the least squares problem reads a bit different: $\\mathrm{min} ||w||_2$ subject to $Xw = y$ with solution $w = X'(XX')^{-1} y$, see e.g. http:\/\/ee263.stanford.edu\/lectures\/min-norm.pdf.\r\nWith explicit intercept $w_0$, this reads $w = X'(XX' + 1 1')^{-1} y$, where 1 is a column vector of ones. $w_0 = 1'(XX' + 1 1')^{-1} y$.\r\n**This is incompatible with our mean centering approach.**\r\n\r\n#### Example\r\n\r\n<details>\r\n\r\n```python\r\nimport numpy as np\r\nfrom numpy.testing import assert_allclose\r\nfrom scipy import linalg\r\nfrom sklearn.datasets import make_low_rank_matrix\r\nfrom sklearn.linear_model import Ridge\r\n\r\nn_samples, n_features = 4, 12  # wide data\r\nk = min(n_samples, n_features)\r\nrng = np.random.RandomState(42)\r\nX = make_low_rank_matrix(\r\n    n_samples=n_samples, n_features=n_features, effective_rank=k\r\n)\r\nX[:, -1] = 1  # last columns acts as intercept\r\nU, s, Vt = linalg.svd(X)\r\nassert np.all(s) > 1e-3  # to be sure X is not singular\r\nU1, U2 = U[:, :k], U[:, k:]\r\nVt1, _ = Vt[:k, :], Vt[k:, :]\r\ny = rng.uniform(low=-10, high=10, size=n_samples)\r\n# w = X'(XX')^-1 y = V s^-1 U' y\r\ncoef_ols = Vt1.T @ np.diag(1 \/ s) @ U1.T @ y\r\n\r\nmodel = Ridge(alpha=0, fit_intercept=True)\r\nX = X[:, :-1]  # remove intercept\r\nintercept = coef_ols[-1]\r\ncoef = coef_ols[:-1]\r\nmodel.fit(X, y)\r\n\r\n# Check that we have found a solution => residuals = 0\r\nassert_allclose(model.predict(X), y)\r\n# Check that `coef`, `intercept` also provide a valid solution\r\nassert_allclose(X @ coef + intercept, y)\r\n\r\n# Ridge does not give the minimum norm solution. (This should be equal.)\r\nnp.linalg.norm(np.r_[model.intercept_, model.coef_]) > np.linalg.norm(\r\n    np.r_[intercept, coef]\r\n)\r\n```\r\nThis last statement should be be `False`. It proves that Ridge does not give the miminum norm solution. \r\n\r\n<\/details>","labels":["Bug","module:linear_model"],"created_at":"2022-03-25T13:55:07Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22947"},{"issue_number":130,"repository":"scikit-learn\/scikit-learn","title":"Birch is not finding clusters correctly","description":"### Describe the bug\r\n\r\nBirch do not correctly find clusters if the number of samples of one cluster is much larger than another cluster even if the distance between clusters is much larger than threshold.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.datasets import make_blobs\r\nfrom sklearn.cluster import Birch\r\nimport numpy as np\r\n\r\nX1, y1 = make_blobs(n_samples=10000, centers=[[0.6, 0.6]], cluster_std=0.01, random_state=0)\r\nX2, y2 = make_blobs(n_samples=100, centers=[[0.1, 0.1]], cluster_std=0.01, random_state=0)\r\n\r\nX = np.append(X1, X2, axis = 0)\r\n\r\nbrc = Birch(n_clusters=None, threshold = 0.1)\r\nbrc.fit(X)\r\n\r\nbrc.subcluster_centers_\r\n```\r\n\r\n### Expected Results\r\n\r\nExpected two subclusters  since the distance between clusters is much larger than used threshold.\r\n\r\n### Actual Results\r\n\r\nOnly one subcluster is detected\r\n\r\n`brc.subcluster_centers_`:\r\n\r\n```\r\narray([[0.59494558, 0.59509375]])\r\n```\r\n\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.8.6 | packaged by conda-forge | (default, Jan 25 2021, 23:21:18)  [GCC 9.3.0]\r\nexecutable: \/home\/envs\/env1_custom\/bin\/python\r\n   machine: Linux-4.14.252\r\n\r\nPython dependencies:\r\n          pip: 22.0.3\r\n   setuptools: 60.9.3\r\n      sklearn: 1.0.2\r\n        numpy: 1.20.2\r\n        scipy: 1.7.0\r\n       Cython: None\r\n       pandas: 1.2.4\r\n   matplotlib: 3.5.1\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n```\r\n","labels":["Bug","module:cluster"],"created_at":"2022-03-15T17:12:24Z","comments":6,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22854"},{"issue_number":131,"repository":"scikit-learn\/scikit-learn","title":"Verbosity option is not working in GridSearchCV (Jupyter notebook)","description":"### Describe the bug\n\nSo this issue has been addressed before [here ](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22291) by [darrencl](https:\/\/github.com\/darrencl), but the user didn't follow up with [lesteve](https:\/\/github.com\/lesteve) response.\r\n\r\nThe problem is that GridSearchCV doesn't show the elapsed time periodically, or any log, I am set`n_jobs = -1`, and `verbose = 1`. I tried setting `n_jobs` to other values, the same with `verbose`, but nothing happened.\r\nNote that this didn't happen until I updated scikit-learn from version 0.22.1 to 1.0.2.\r\n\r\n[lesteve](https:\/\/github.com\/lesteve) in his response assumed that this problem is due to `ipykernel <6`, which is not the case with me.\n\n### Steps\/Code to Reproduce\n\n```\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn import datasets\r\n\r\niris = datasets.load_iris()\r\nparams = {'n_estimators':[10,20,30,40,50,60],\r\n          'max_depth':[20,50,60,70,80]}\r\ngrid_obj = GridSearchCV(estimator=RandomForestClassifier(), param_grid=params, n_jobs=-1, verbose=1, cv=5)\r\ngrid_obj.fit(iris.data, iris.target)\r\n```\n\n### Expected Results\n\nThis is the output when using version 0.22.1\r\n![image](https:\/\/user-images.githubusercontent.com\/17684093\/158401635-31755363-2108-41e7-8f7f-08985abd03b8.png)\r\n\n\n### Actual Results\n\n```Fitting 5 folds for each of 30 candidates, totalling 150 fits```\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: D:\\Programs\\ApplicationsSetup\\anaconda3\\python.exe\r\n   machine: Windows-10-10.0.19041-SP0\r\n\r\nPython dependencies:\r\n          pip: 21.2.2\r\n   setuptools: 58.0.4\r\n      sklearn: 1.0.2\r\n        numpy: 1.19.2\r\n        scipy: 1.6.2\r\n       Cython: 0.29.25\r\n       pandas: 1.4.1\r\n   matplotlib: 3.5.1\r\n       joblib: 1.1.0\r\nthreadpoolctl: 2.2.0\r\n\r\n\r\n!jupyter --version\r\nSelected Jupyter core packages...\r\nIPython          : 7.31.1\r\nipykernel        : 6.4.1\r\nipywidgets       : 7.6.5\r\njupyter_client   : 6.1.12\r\njupyter_core     : 4.9.1\r\njupyter_server   : 1.13.5\r\njupyterlab       : 3.2.9\r\nnbclient         : 0.5.11\r\nnbconvert        : 6.1.0\r\nnbformat         : 5.1.3\r\nnotebook         : 6.4.9\r\nqtconsole        : 5.2.2\r\ntraitlets        : 5.1.1\n```\n","labels":["Bug","module:model_selection","Needs Investigation"],"created_at":"2022-03-15T14:48:31Z","comments":22,"reactions":6,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22849"},{"issue_number":132,"repository":"scikit-learn\/scikit-learn","title":"Formula for dual gap of elastic net in coordinate descent solver","description":"### Describe the bug\r\nThe computation of the dual gap for the elastic net in the coordinate descent solver (`enet_coordinate_descent`) might be wrong.\r\n\r\nThe elastic net minimizes\r\n```\r\nPrimal(w) = (1\/2) * ||y - X w||_2^2 + alpha * ||w||_1 + beta\/2 * ||w||_2^2\r\n```\r\n\r\n### Lasso\r\nFor the pure Lasso, i.e. `beta=0`, the dual becomes [1]\r\n```\r\nDual(nu) = -1\/2 * ||nu||_2^2 - nu'y   if ||X'nu||_\u221e = max(abs(X'nu)) <= alpha   else -\u221e\r\n```\r\nwith `nu = Xw - y` (=minus the residual `R`) as possible dual point. This yields the Lasso dual gap\r\n```\r\nPrimal(w) - Dual(nu)\r\n```\r\nIn case of `||X'nu||_\u221e > alpha`, one uses a down-scaled variable `nu` in the dual.\r\n\r\n### Elastic Net\r\nFor the elastic net, the dual, see section 5.2.3 in [2], becomes\r\n```\r\nDual(nu) = -1\/2 * ||nu||_2^2 - nu'y - 1\/(2*beta) * sum_j (|X_j'nu| - alpha)_+^2\r\n```\r\nwith `X_j` the j-th column of `X` and `x_+ = max(0, x)` the positive part.\r\n\r\n### References\r\n[1] Kim, Seung-Jean et al. \u201cAn Interior-Point Method for Large-Scale $\\ell_1$-Regularized Least Squares.\u201d IEEE Journal of Selected Topics in Signal Processing 1 (2007): 606-617. [pdf link](http:\/\/stanford.edu\/~boyd\/papers\/pdf\/l1_ls.pdf)\r\n[2] Mendler-D\u00fcnner, Celestine et al. \u201cPrimal-Dual Rates and Certificates.\u201d ICML (2016). [arxiv link](https:\/\/arxiv.org\/pdf\/1602.05205.pdf)\r\n\r\n### Expected Results\r\n\r\n```python\r\nif beta > =0:\r\n    R = y - X @ w  # note: R = -nu\r\n    R_norm2 = R @ R\r\n    gap = R_norm2 - R @ y + alpha * l1_norm + beta\/2 * l2_norm\r\n    gap += 1\/(2*beta) * np.sum(np.max(0, np.abs(X.T @ R) - alpha)**2)\r\n```\r\nAt least a test for it, similar to the pure Lasso case in https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/2c2f31d68c21b7647557b2f776e86c05954d80bf\/sklearn\/linear_model\/tests\/test_coordinate_descent.py#L292\r\n\r\n### Actual Results\r\n\r\nI start to be rattled by the `-beta * w` term in line 205.\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/2c2f31d68c21b7647557b2f776e86c05954d80bf\/sklearn\/linear_model\/_cd_fast.pyx#L205-L236\r\n\r\n\r\n","labels":["Bug","module:linear_model"],"created_at":"2022-03-14T18:48:33Z","comments":13,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22836"},{"issue_number":133,"repository":"scikit-learn\/scikit-learn","title":"Second build failure for s390x and ppc64","description":"### Describe the bug\n\nHi,\r\nbesides issue #22503 I've observed an _additional_ error in the test suite for the architectures s390x and ppc64.\n\n### Steps\/Code to Reproduce\n\n```\r\npytest -m \"not network\" -v --color=no -k \"not test_load_boston_alternative and not test_dump\"\r\n```\r\nYou can find a [full build log for arm64](https:\/\/buildd.debian.org\/status\/fetch.php?pkg=scikit-learn&arch=s390x&ver=1.0.2-1&stamp=1644952396&raw=0) where you can seek for the string above to see the start of the test procedure.\n\n### Expected Results\n\nPassing the test suite without any error.\n\n### Actual Results\n\n    ___________ [doctest] sklearn.feature_extraction._hash.FeatureHasher ___________\r\n    081     DictVectorizer : Vectorizes string-valued features using a hash table.\r\n    082     sklearn.preprocessing.OneHotEncoder : Handles nominal\/categorical features.\r\n    083\r\n    084     Examples\r\n    085     --------\r\n    086     >>> from sklearn.feature_extraction import FeatureHasher\r\n    087     >>> h = FeatureHasher(n_features=10)\r\n    088     >>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]\r\n    089     >>> f = h.transform(D)\r\n    090     >>> f.toarray()\r\n    Expected:\r\n        array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],\r\n               [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])\r\n    Got:\r\n        array([[ 0.,  0.,  0., -1.,  0.,  0.,  0.,  4.,  0.,  2.],\r\n               [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])\r\n\r\n    \/<<PKGBUILDDIR>>\/.pybuild\/cpython3_3.9\/build\/sklearn\/feature_extraction\/_hash.py:90: DocTestFailure\r\n    = 2 failed, 22017 passed, 404 skipped, 3 deselected, 59 xfailed, 38 xpassed, 2351 warnings in 686.07s (0:11:26) =\r\n\n\n### Versions\n\n```shell\n`1.0.2`\r\n\r\nAlso see the [full build log](https:\/\/buildd.debian.org\/status\/fetch.php?pkg=scikit-learn&arch=s390x&ver=1.0.2-1&stamp=1644952396&raw=0) what other components are used.\n```\n","labels":["Bug","module:feature_extraction","Needs Investigation"],"created_at":"2022-02-16T07:22:50Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22504"},{"issue_number":134,"repository":"scikit-learn\/scikit-learn","title":"`test_y_multioutput`in Gaussian Process is failing on Debian 32bit","description":"### Describe the bug\n\nOn 32bit systems on debian the test `test_y_multioutput` is failing.\r\n\r\nThe test will probably be just skipped during the build, this is not urgent, but maybe something underlying multioutput and Gaussian Process is hidden here (see #19995).\n\n### Steps\/Code to Reproduce\n\n```\r\n pytest sklearn\/gaussian_process\/tests\/test_gpr.py -k test_y_multioutput\r\n```\n\n### Expected Results\n\nThe test passes.\n\n### Actual Results\n\n```\r\n=================================== FAILURES ===================================\r\n______________________________ test_y_multioutput ______________________________\r\n    def test_y_multioutput():\r\n        # Test that GPR can deal with multi-dimensional target values\r\n        y_2d = np.vstack((y, y * 2)).T\r\n    \r\n        # Test for fixed kernel that first dimension of 2d GP equals the output\r\n        # of 1d GP and that second dimension is twice as large\r\n        kernel = RBF(length_scale=1.0)\r\n    \r\n        gpr = GaussianProcessRegressor(kernel=kernel, optimizer=None, normalize_y=False)\r\n        gpr.fit(X, y)\r\n    \r\n        gpr_2d = GaussianProcessRegressor(kernel=kernel, optimizer=None, normalize_y=False)\r\n        gpr_2d.fit(X, y_2d)\r\n    \r\n        y_pred_1d, y_std_1d = gpr.predict(X2, return_std=True)\r\n        y_pred_2d, y_std_2d = gpr_2d.predict(X2, return_std=True)\r\n        _, y_cov_1d = gpr.predict(X2, return_cov=True)\r\n        _, y_cov_2d = gpr_2d.predict(X2, return_cov=True)\r\n    \r\n        assert_almost_equal(y_pred_1d, y_pred_2d[:, 0])\r\n        assert_almost_equal(y_pred_1d, y_pred_2d[:, 1] \/ 2)\r\n    \r\n        # Standard deviation and covariance do not depend on output\r\n        assert_almost_equal(y_std_1d, y_std_2d)\r\n        assert_almost_equal(y_cov_1d, y_cov_2d)\r\n    \r\n        y_sample_1d = gpr.sample_y(X2, n_samples=10)\r\n        y_sample_2d = gpr_2d.sample_y(X2, n_samples=10)\r\n        assert_almost_equal(y_sample_1d, y_sample_2d[:, 0])\r\n    \r\n        # Test hyperparameter optimization\r\n        for kernel in kernels:\r\n            gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True)\r\n            gpr.fit(X, y)\r\n    \r\n            gpr_2d = GaussianProcessRegressor(kernel=kernel, normalize_y=True)\r\n            gpr_2d.fit(X, np.vstack((y, y)).T)\r\n    \r\n>           assert_almost_equal(gpr.kernel_.theta, gpr_2d.kernel_.theta, 4)\r\nE           AssertionError: \r\nE           Arrays are not almost equal to 4 decimals\r\nE           \r\nE           Mismatched elements: 2 \/ 3 (66.7%)\r\nE           Max absolute difference: 4.77758733\r\nE           Max relative difference: 27.70946804\r\nE            x: array([ -4.6052,  -0.3981, -11.5129])\r\nE            y: array([  0.1724,   0.4926, -11.5129])\r\nsklearn\/gaussian_process\/tests\/test_gpr.py:379: AssertionError\r\n```\n\n### Versions\n\n```shell\nscikit-learn 1.0.2\r\n\r\nPython dependencies:\r\n          pip: None\r\n   setuptools: 59.6.0\r\n      sklearn: 1.0.2\r\n        numpy: 1.21.5\r\n        scipy: 1.7.3\r\n       Cython: 0.29.24\r\n       pandas: 1.3.5\r\n   matplotlib: 3.5.1\r\n       joblib: 0.17.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\r\n```\n```\n","labels":["Bug","module:gaussian_process"],"created_at":"2022-02-11T02:41:23Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22446"},{"issue_number":135,"repository":"scikit-learn\/scikit-learn","title":"StandardScaler and PolynomialFeatures fail on zero-feature inputs during fit (should become passthrough)","description":"### Describe the bug\n\nIf you use StandardScaler or PolynomialFeatures (or other transformers, these are the two that hit me first) as elements in  a complex pipeline, an issue comes up if you ever fit these transformers on data that has no features (empty data). This happens a lot when you build a complex pipeline and you don't know what the features will be in advance. For example, in reinforcement learning problems, some problems will have context features while others won't, and we need pipelines for both the action and context features which are then concatenated (using FeatureUnion). My current workaround is that when I try to fit my pipeline, if it fails, it eliminates StandardScalers and PolynomialFeatures because they are a part of the context pipeline and we don't have context features for this problem, then fit again. \n\n### Steps\/Code to Reproduce\n\n\r\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\r\nfrom sklearn.pipeline import Pipeline\r\n\r\npipeline = Pipeline([\r\n    ('polynomial', PolynomialFeatures()),\r\n    ('fully_transformed_action_and_context', StandardScaler()),\r\n])\r\n\r\npipeline.fit([[]])\n\n### Expected Results\n\nNo error is thrown. Pipeline becomes equivalent to passthrough.\n\n### Actual Results\n\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.8\/lib\/python3.8\/site-packages\/sklearn\/pipeline.py\", line 390, in fit\r\n    Xt = self._fit(X, y, **fit_params_steps)\r\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.8\/lib\/python3.8\/site-packages\/sklearn\/pipeline.py\", line 348, in _fit\r\n    X, fitted_transformer = fit_transform_one_cached(\r\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.8\/lib\/python3.8\/site-packages\/joblib\/memory.py\", line 355, in __call__\r\n    return self.func(*args, **kwargs)\r\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.8\/lib\/python3.8\/site-packages\/sklearn\/pipeline.py\", line 893, in _fit_transform_one\r\n    res = transformer.fit_transform(X, y, **fit_params)\r\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.8\/lib\/python3.8\/site-packages\/sklearn\/base.py\", line 847, in fit_transform\r\n    return self.fit(X, **fit_params).transform(X)\r\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.8\/lib\/python3.8\/site-packages\/sklearn\/preprocessing\/_polynomial.py\", line 287, in fit\r\n    _, n_features = self._validate_data(X, accept_sparse=True).shape\r\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.8\/lib\/python3.8\/site-packages\/sklearn\/base.py\", line 561, in _validate_data\r\n    X = check_array(X, **check_params)\r\n  File \"\/Library\/Frameworks\/Python.framework\/Versions\/3.8\/lib\/python3.8\/site-packages\/sklearn\/utils\/validation.py\", line 806, in check_array\r\n    raise ValueError(\r\nValueError: Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required.\r\n\n\n### Versions\n\n```shell\nSystem:\r\n    python: 3.8.2 (v3.8.2:7b3ab5921f, Feb 24 2020, 17:52:18)  [Clang 6.0 (clang-600.0.57)]\r\nexecutable: \/usr\/local\/bin\/python3.8\r\n   machine: macOS-10.15.7-x86_64-i386-64bit\r\nPython dependencies:\r\n          pip: 20.0.2\r\n   setuptools: 41.2.0\r\n      sklearn: 1.0.1\r\n        numpy: 1.19.5\r\n        scipy: 1.4.1\r\n       Cython: 0.29.16\r\n       pandas: 1.4.0\r\n   matplotlib: 3.2.1\r\n       joblib: 0.14.1\r\nthreadpoolctl: 3.0.0\r\nBuilt with OpenMP: True\n```\n","labels":["Bug","module:preprocessing"],"created_at":"2022-02-10T19:26:04Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22442"},{"issue_number":136,"repository":"scikit-learn\/scikit-learn","title":"PCA hangs occasionally when applied to data that fits in memory","description":"### Describe the bug\n\nWhen trying to `fit` PCA with a large enough array that fits in memory (approximately `300 000 x 750`) with an aim to make about 3 times reduction, unstable results are obtained: in most cases the *fit succeeds*, but occasionaly it *hangs \"forever\"*.\r\n\r\nRandom seed is fixed, but unfortunalely it does not help in this case.\r\nThe `svd_solver` option is left at the default. Looks like explicitly specifying `svd_solver='arpack'` does not cause this issue.\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nimport random\r\n\r\nnp.random.seed(42)\r\nrandom.seed(42)\r\n\r\nfrom sklearn.decomposition import PCA\r\n\r\n\r\nif __name__ == '__main__':\r\n    embeddings = np.random.RandomState(42).normal(size=(328039, 768))\r\n    embeddings = embeddings.astype(np.float32)\r\n\r\n    for i in range(20):\r\n        print('Iteration', i)\r\n        pca = PCA(n_components=260, random_state=42)\r\n        pca = pca.fit(embeddings)\r\n\r\n```\n\n### Expected Results\n\nStable results: consistently successful fits over a finite time.\n\n### Actual Results\n\nSometimes the process hangs, in most cases in the first 10 iterations. I haven't found any consistency pattern at what time it fails.\n\n### Versions\n\n```shell\nSystem:\r\n   python: 3.9.7 | packaged by conda-forge \r\n   machine: Windows-10-10.0.19043-SP0\r\n   RAM: 32Gb\r\n\r\nPython dependencies:\r\n          pip: 21.3.1\r\n   setuptools: 60.8.1\r\n      sklearn: 1.0.1\r\n        numpy: 1.20.1\r\n        scipy: 1.7.1\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: None\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.1.0\r\n\r\nBuilt with OpenMP: True\n```\n","labels":["Bug","module:decomposition","Needs Investigation"],"created_at":"2022-02-10T16:53:43Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22434"},{"issue_number":137,"repository":"scikit-learn\/scikit-learn","title":"Y_trans is not equal to y_scores_ in PLSRegression","description":"### Describe the bug\n\nThe documentation that `Y_trans` obtained from `fit_transform` should be equal to `y_scores_` fitted attribute but this is not the case.\r\n\r\nWe should investigate if this is normal or not\n\n### Steps\/Code to Reproduce\n\n```python\r\nIn [1]: from sklearn.cross_decomposition import PLSRegression\r\n   ...: X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]\r\n   ...: Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]\r\n   ...: pls2 = PLSRegression(n_components=2)\r\n   ...: X_trans, Y_trans = pls2.fit_transform(X, Y)\r\n```\n\n### Expected Results\n\n`Y_trans` should be equal to `y_scores_`.\n\n### Actual Results\n\n```python\r\n\r\nIn [2]: Y_trans\r\nOut[2]: \r\narray([[-1.10089007,  2.64706486],\r\n       [-1.73668766, -5.50753532],\r\n       [ 1.05463999,  5.91255735],\r\n       [ 1.78293774, -3.05208689]])\r\n\r\nIn [3]: pls2.y_scores_\r\nOut[3]: \r\narray([[-1.40620988,  0.09323305],\r\n       [-1.10143324, -0.85118366],\r\n       [ 0.37266921,  1.64933181],\r\n       [ 2.13497391, -0.8913812 ]])\r\n```\n\n### Versions\n\n```shell\nIn [8]: sklearn.show_versions()\r\n\r\nSystem:\r\n    python: 3.8.12 | packaged by conda-forge | (default, Sep 16 2021, 01:38:21)  [Clang 11.1.0 ]\r\nexecutable: \/Users\/glemaitre\/mambaforge\/envs\/dev\/bin\/python\r\n   machine: macOS-12.1-arm64-arm-64bit\r\n\r\nPython dependencies:\r\n          pip: 21.3\r\n   setuptools: 58.2.0\r\n      sklearn: 1.1.dev0\r\n        numpy: 1.21.2\r\n        scipy: 1.8.0.dev0+1902.b795164\r\n       Cython: 0.29.24\r\n       pandas: 1.3.3\r\n   matplotlib: 3.4.3\r\n       joblib: 1.2.0.dev0\r\nthreadpoolctl: 3.0.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/Users\/glemaitre\/mambaforge\/envs\/dev\/lib\/libopenblas_vortexp-r0.3.18.dylib\r\n        version: 0.3.18\r\nthreading_layer: openmp\r\n   architecture: VORTEX\r\n    num_threads: 8\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libomp\r\n       filepath: \/Users\/glemaitre\/mambaforge\/envs\/dev\/lib\/libomp.dylib\r\n        version: None\r\n    num_threads: 8\r\n```\n```\n","labels":["Bug","module:cross_decomposition"],"created_at":"2022-02-08T17:36:42Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22420"},{"issue_number":138,"repository":"scikit-learn\/scikit-learn","title":"BUG? LinearSVC with hinge loss and L2 penalty, solver='liblinear' seems to get stuck compared to other solvers","description":"### Describe the bug\r\n\r\nCompared to a vanilla coordinate descent on the dual, liblinear plateaus at a higher objective value.\r\n\r\n### Steps\/Code to Reproduce\r\n```python\r\nimport warnings\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom numba import njit\r\nfrom numpy.linalg import norm\r\nfrom sklearn.svm import LinearSVC\r\n\r\nfrom sklearn.exceptions import ConvergenceWarning\r\nwarnings.filterwarnings('ignore', category=ConvergenceWarning)\r\n\r\nC = 0.1\r\nclf = LinearSVC(C=C, penalty='l2', dual=True, fit_intercept=False, tol=1e-12,\r\n                loss='hinge', random_state=0)\r\n\r\n\r\n@njit\r\ndef loss(w, yX, C):\r\n    return np.maximum(0, 1 - yX @ w).sum() + norm(w) ** 2 \/ (2 * C)\r\n\r\n\r\n@njit\r\ndef cd_dual(yX, C, n_iter):\r\n    \"\"\"Solve problem with coordinate descent in the dual, ie minimize:\r\n    norm(yX @ mu) ** 2 \/ 2 + mu.sum()    subject to 0 <= mu <= C\r\n\r\n    with link equation w = yX @ mu  (=sum_i mu_i * y_i * X[i]\"\"\"\r\n    n_samples, n_features = yX.shape\r\n    lipschitz = np.zeros(n_samples)\r\n    for j in range(n_samples):\r\n        lipschitz[j] = norm(yX[j]) ** 2\r\n\r\n    mu = np.zeros(n_samples)\r\n    w = np.zeros(n_features)\r\n    losses = np.zeros(n_iter)\r\n    for it in range(n_iter):\r\n        for j in range(n_samples):\r\n            old_mu_j = mu[j]\r\n            grad_f_j = yX[j] @ w - 1\r\n            new_mu_j = max(0, min(old_mu_j - grad_f_j \/ lipschitz[j], C))\r\n\r\n            if new_mu_j != old_mu_j:\r\n                w += yX[j] * (new_mu_j - old_mu_j)\r\n                mu[j] = new_mu_j\r\n        losses[it] = loss(w, yX, C)\r\n\r\n    return w, losses\r\n\r\n\r\nfig, axarr = plt.subplots(1, 2, figsize=(10, 4), constrained_layout=True)\r\nfor idx, (n_samples, n_features) in enumerate([[100, 500], [500, 100]]):\r\n    np.random.seed(0)\r\n    X = np.random.randn(n_samples, n_features)\r\n    y = X @ np.random.randn(n_features) + 0.1 * np.random.randn(n_samples)\r\n    y = 2 * (y > 0) - 1\r\n    yX = X * y[:, None]\r\n\r\n    max_iter = 3000 if n_samples > n_features else 300\r\n    losses_sk = []\r\n    iter_sk = np.geomspace(1, max_iter, num=50).astype(int)\r\n    for n_iter in iter_sk:\r\n        clf.max_iter = n_iter\r\n        clf.fit(X, y)\r\n        w = clf.coef_[0]\r\n        losses_sk.append(loss(w, yX, C))\r\n    losses_sk = np.array(losses_sk)\r\n\r\n    w, losses_cd = cd_dual(yX, C, max_iter)\r\n    loss_min = min(losses_cd.min(), losses_sk.min())\r\n\r\n    axarr[idx].semilogy(iter_sk, losses_sk - loss_min, label='sklearn')\r\n    axarr[idx].semilogy(losses_cd - loss_min, label='dual coordinate descent')\r\n    axarr[idx].set_xlabel(\"iteration\")\r\n    axarr[idx].set_ylabel(\"loss - min loss\")\r\n    axarr[idx].set_title(f'n_samples, n_features=({n_samples}, {n_features})')\r\n    axarr[idx].legend()\r\nplt.show(block=False)\r\n\r\n\r\n# no intercept is fitted\r\nnp.testing.assert_allclose(clf.decision_function(X), X @ clf.coef_[0])\r\n```\r\n\r\n### Expected Results\r\n\r\nI expect both orange and blue suboptimality curve to go to zero. The blue one (sklearn) does not. This is particularly visible when n_samples > features (right plot)\r\n\r\n### Actual Results\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/8993218\/150823358-ae25d607-1a0c-4685-95dc-6acb134e43b0.png)\r\n\r\n\r\n### Versions\r\n\r\n```shell\r\nSystem:\r\n    python: 3.9.7 (default, Sep 16 2021, 13:09:58)  [GCC 7.5.0]\r\nexecutable: \/home\/mathurin\/anaconda3\/bin\/python\r\n   machine: Linux-5.13.0-27-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n          pip: 21.2.4\r\n   setuptools: 58.0.4\r\n      sklearn: 1.0.1\r\n        numpy: 1.20.3\r\n        scipy: 1.7.1\r\n       Cython: 0.29.24\r\n       pandas: 1.3.4\r\n   matplotlib: 3.4.3\r\n       joblib: 1.1.0\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True\r\n```\r\n","labels":["Bug","module:svm"],"created_at":"2022-01-24T16:28:27Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22283"},{"issue_number":139,"repository":"scikit-learn\/scikit-learn","title":"Failing test test_quantile_estimates_calibration with q=.9","description":"### Describe the bug\r\n\r\nHello,\r\n\r\nI built the source of the main branch and ran the tests. The quantile estimates calibration test is failing after 2 mins 33 sec and a lot of processor usage. \r\n\r\nThanks!\r\n\r\n### Steps\/Code to Reproduce\r\n\r\nI launched the test in my dev folder with:\r\n\r\n```\r\npytest -v -k test_quantile_estimates_calibration\r\n```\r\n\r\n### Expected Results\r\n\r\nTest passed\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\n=========================================================================================== test session starts ===========================================================================================\r\nplatform linux -- Python 3.10.2, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- \/home\/francois\/mambaforge\/envs\/sklearn-env\/bin\/python\r\ncachedir: .pytest_cache\r\nrootdir: \/home\/francois\/Documents\/scikit-learn, configfile: setup.cfg, testpaths: sklearn\r\nplugins: xdist-2.5.0, forked-1.4.0\r\ncollected 26675 items \/ 26672 deselected \/ 3 selected                                                                                                                                                     \r\n\r\nsklearn\/linear_model\/tests\/test_quantile.py::test_quantile_estimates_calibration[0.5] PASSED                                                                                                        [ 33%]\r\nsklearn\/linear_model\/tests\/test_quantile.py::test_quantile_estimates_calibration[0.9] FAILED                                                                                                        [ 66%]\r\nsklearn\/linear_model\/tests\/test_quantile.py::test_quantile_estimates_calibration[0.05] PASSED                                                                                                       [100%]\r\n\r\n================================================================================================ FAILURES =================================================================================================\r\n________________________________________________________________________________ test_quantile_estimates_calibration[0.9] _________________________________________________________________________________\r\n\r\nq = 0.9\r\n\r\n    @pytest.mark.parametrize(\"q\", [0.5, 0.9, 0.05])\r\n    def test_quantile_estimates_calibration(q):\r\n        # Test that model estimates percentage of points below the prediction\r\n        X, y = make_regression(n_samples=1000, n_features=20, random_state=0, noise=1.0)\r\n        quant = QuantileRegressor(\r\n            quantile=q,\r\n            alpha=0,\r\n            solver_options={\"lstsq\": False},\r\n        ).fit(X, y)\r\n>       assert np.mean(y < quant.predict(X)) == approx(q, abs=1e-2)\r\nE       assert 0.721 == 0.9 \u00b1 1.0e-02\r\nE         +0.721\r\nE         -0.9 \u00b1 1.0e-02\r\n\r\nsklearn\/linear_model\/tests\/test_quantile.py:129: AssertionError\r\n================================================================ 1 failed, 2 passed, 26672 deselected, 1032 warnings in 153.11s (0:02:33) =================================================================\r\n\r\n```\r\n\r\n### Versions\r\n\r\n```\r\nSystem:\r\n    python: 3.10.2 | packaged by conda-forge | (main, Jan 14 2022, 08:02:09) [GCC 9.4.0]\r\nexecutable: \/home\/francois\/mambaforge\/envs\/sklearn-env\/bin\/python\r\n   machine: Linux-5.13.0-27-generic-x86_64-with-glibc2.31\r\n\r\nPython dependencies:\r\n          pip: 21.3.1\r\n   setuptools: 59.8.0\r\n      sklearn: 1.1.dev0\r\n        numpy: 1.22.0\r\n        scipy: 1.7.3\r\n       Cython: 0.29.26\r\n       pandas: 1.3.5\r\n   matplotlib: 3.5.1\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.0.0\r\n\r\nBuilt with OpenMP: True\r\n\r\nthreadpoolctl info:\r\n       user_api: blas\r\n   internal_api: openblas\r\n         prefix: libopenblas\r\n       filepath: \/home\/francois\/mambaforge\/envs\/sklearn-env\/lib\/libopenblasp-r0.3.18.so\r\n        version: 0.3.18\r\nthreading_layer: pthreads\r\n   architecture: SkylakeX\r\n    num_threads: 8\r\n\r\n       user_api: openmp\r\n   internal_api: openmp\r\n         prefix: libgomp\r\n       filepath: \/home\/francois\/mambaforge\/envs\/sklearn-env\/lib\/libgomp.so.1.0.0\r\n        version: None\r\n    num_threads: 8\r\n```","labels":["Bug","module:linear_model"],"created_at":"2022-01-21T15:13:45Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22262"},{"issue_number":140,"repository":"scikit-learn\/scikit-learn","title":"Sphinx docs don't build correctly on Windows 10","description":"### Describe the bug\n\nI'm on Windows 10. The scikit-learn contributor docs prescribe the following to build the docs (after necessary installs): \r\n![image](https:\/\/user-images.githubusercontent.com\/34613774\/149794271-b07390d7-5c60-4c21-b7c5-3f6a69565222.png)\r\n\r\nHowever, in practice, running `make` only results in: \r\n![image](https:\/\/user-images.githubusercontent.com\/34613774\/149794451-91c7aa44-2020-46eb-8dca-fbc87b50ea93.png)\r\n\r\nThen, I attempt to run `make html-noplot`\n\n### Steps\/Code to Reproduce\n\n```\r\ncd doc\r\nmake\r\nmake html-noplot\r\n```\n\n### Expected Results\n\nNo error is thrown and docs are properly built with no plots.\n\n### Actual Results\n\n![image](https:\/\/user-images.githubusercontent.com\/34613774\/149795038-dc2a2ee9-be86-452a-89fb-9b8ee321a922.png)\r\n![image](https:\/\/user-images.githubusercontent.com\/34613774\/149795120-447955de-2b2b-4daa-a7cc-543b7e8d357a.png)\r\nthis continues for a while, until\r\n![image](https:\/\/user-images.githubusercontent.com\/34613774\/149795516-9b51a639-af59-4e65-b5c1-20f71379f7de.png)\r\nwhich continues for a while, until it crashes at the end\r\n![image](https:\/\/user-images.githubusercontent.com\/34613774\/149795707-b0e696e1-4a73-4ec1-a26e-5828427ed08a.png)\r\n\r\nFinally, checking the actual directory I find only a `module\/generated` directory with several images inside.\n\n### Versions\n\n![image](https:\/\/user-images.githubusercontent.com\/34613774\/149796023-401770e6-3485-43fd-b44f-df4e0ff2c0fe.png)\r\n","labels":["Bug","Build \/ CI"],"created_at":"2022-01-17T15:19:09Z","comments":3,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22232"},{"issue_number":141,"repository":"scikit-learn\/scikit-learn","title":"power_t: does it make sense for this parameter to have negative values","description":"sklearn\/linear_model\/_stochastic_gradient.py\r\n\r\nRef #22115\r\n\r\n>I do not anticipate negative `power_t` to be mathematically meaningful but apparently our code accepts it without crashing... So ok with documenting it.\r\n\r\n_Originally posted by @ogrisel in https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/22115#r780109928_\r\n\r\n@thomasjpfan \r\n>I feel like this a case where documenting `-inf` will lead to more people trying out. If this is not mathematically meaningful, then we could be promoting a bad practice?\r\n\r\ncc:  @glemaitre","labels":["Bug","help wanted","module:linear_model"],"created_at":"2022-01-10T19:07:21Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/22178"},{"issue_number":142,"repository":"scikit-learn\/scikit-learn","title":"Cross endianness and bitness pickle issues with KNeighborsClassifier \/ KDTree","description":"Reported in https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/21237 (cross endianness). There is a similar issue for the cross bitness, to reproduce:\r\n\r\nGenerate a pickle on a 64bit machine:\r\n```py\r\nfrom sklearn.datasets import make_classification\r\nX, y = make_classification(random_state=0)\r\n\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nclf = KNeighborsClassifier(algorithm='kd_tree')\r\nclf.fit(X, y)\r\nimport pickle\r\npickle.dump(clf, open('\/tmp\/kneighbors.pkl', 'wb'))\r\n```\r\n\r\nOpen it on a 32bit machine:\r\n```\r\ndocker run -it -v \/tmp:\/io lesteve\/i386-scikit-learn python3 -c 'import pickle; pickle.load(open(\"\/io\/kneighbors.pkl\", \"rb\"))'\r\n```\r\nOutput:\r\n```\r\nWARNING: The requested image's platform (linux\/386) does not match the detected host platform (linux\/amd64) and no specific platform was requested\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"sklearn\/neighbors\/_binary_tree.pxi\", line 1062, in sklearn.neighbors._kd_tree.BinaryTree.__setstate__\r\n    self._update_memviews()\r\n  File \"sklearn\/neighbors\/_binary_tree.pxi\", line 1004, in sklearn.neighbors._kd_tree.BinaryTree._update_memviews\r\n    self.idx_array = self.idx_array_arr\r\nValueError: Buffer dtype mismatch, expected 'ITYPE_t' but got 'long long'\r\n```\r\n\r\nQuite likely solving the issue is rather similar to https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/21552 and https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/21539.","labels":["Bug","module:neighbors"],"created_at":"2021-11-04T11:17:12Z","comments":2,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/21553"},{"issue_number":143,"repository":"scikit-learn\/scikit-learn","title":"Ridge with cholesky solver returns NaN","description":"### Describe the bug\n\nWhen using Ridge with a big number of features and the Cholesky solver, the model gets full of NaNs.\r\nPlaying with the number of features we can see that the imprecision grows until it becomes NaN.\r\n\r\nThe bug is architecture-dependent.\r\nThe provided snippet works perfectly in an `Intel(R) Xeon(R) CPU E3-1585L v5 @ 3.00GHz`.\r\nIt fails when running in an `Intel(R) Xeon(R) Gold 6140 CPU @ 2.30GHz`.\r\n\r\nA workaround was found by blindly exporting OPENBLAS_CORETYPE=\"Broadwell\".\n\n### Steps\/Code to Reproduce\n\n```python\r\nimport numpy as np\r\nfrom sklearn.linear_model import Ridge\r\n\r\n# we need something big to reproduce:\r\nn_inputs = 1000\r\nn_targets = 1\r\nn_samples = 20000\r\n\r\nrng = np.random.default_rng(seed=42)\r\n\r\nb = rng.random((n_targets,))\r\nw = rng.random((n_targets, n_inputs)) \/ n_inputs\r\n\r\nX = rng.random((n_samples, n_inputs))\r\ny = rng.random((n_samples, n_targets))\r\n\r\nfor i in range(n_samples):\r\n    y[i] = (w @ X[i]) + b\r\n\r\n# \"lsqr\" does not have the same problem as \"cholesky\"\r\nmodel = Ridge(alpha=0.0, tol=1e-6, solver=\"cholesky\")\r\nmodel.fit(X, y)\r\n\r\nnp.testing.assert_almost_equal(model.coef_, w)\r\nnp.testing.assert_almost_equal(model.intercept_, b)\r\n\r\nerror = np.abs(model.predict(X) - y).sum()\r\nnp.testing.assert_almost_equal(error, 0.0, decimal=4)\r\n\r\nassert np.isfinite(model.coef_).all()\r\nassert np.isfinite(model.intercept_).all()\r\n\r\nprint(\"done!\")\r\n```\n\n### Expected Results\n\ndone!\n\n### Actual Results\n\nNumpy assertion errors as `model.coef_` is full of `NaN`.\n\n### Versions\n\nThe software is the same on both machines:\r\n```\r\nSystem:\r\n    python: 3.8.9 (default, Apr  2 2021, 11:20:07)  [GCC 10.3.0]\r\nexecutable: ***\/.venv\/bin\/python\r\n   machine: Linux-3.10.0-1062.9.1.el7.x86_64-x86_64-with-glibc2.2.5\r\n\r\nPython dependencies:\r\n          pip: 20.2.3\r\n   setuptools: 54.2.0.post0\r\n      sklearn: 0.24.1\r\n        numpy: 1.20.2\r\n        scipy: 1.6.1\r\n       Cython: 0.29.22\r\n       pandas: 1.2.3\r\n   matplotlib: 3.4.1\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n```","labels":["Bug","module:linear_model"],"created_at":"2021-10-24T19:49:12Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/21447"},{"issue_number":144,"repository":"scikit-learn\/scikit-learn","title":"Using a custom rbf kernel function for sklearn's SVC is way faster than built-in method","description":"### Describe the bug\r\n\r\nI've noticed a rather strange behavior when using **Scikit-Learn's SVC** implementation. Using the **built-in rbf kernel** with SVC is **slower** by magnitudes than passing a **custom rbf function to SVC()**. \r\n\r\nFrom what I could see and understand so far, the only difference between the two versions is that **in the built-in rbf case, not sklearn but libsvm will compute the kernel**. Passing a dedicated kernel function as hyperparameter to SVC() leads to the computation of the kernel inside sklearn, not in libsvm. The results are identical, but **the latter case takes only a fraction of the computation time**. \r\n\r\nI've created a toy dataset that mimics the data I am currently working on. This probably only becomes relevant with larger datasets. There, the discrepancy in computation time likely increases.\r\n\r\nDoes anyone know why this is happening? Is this the expected behavior?\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n```py\r\nimport numpy as np\r\nfrom time import time\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.metrics.pairwise import rbf_kernel\r\nfrom sklearn.metrics import accuracy_score\r\n\r\n# create toy data\r\nn_features = 1000\r\nn_samples = 10000\r\nn_informative = 10\r\nX, y = make_classification(n_samples, n_features, n_informative=n_informative)\r\ngamma = 1 \/ n_features\r\n\r\n# fit SVC with built-in rbf kernel\r\nsvc_built_in = SVC(kernel='rbf', gamma=gamma)\r\nnp.random.seed(13)\r\nt1 = time()\r\nsvc_built_in.fit(X, y)\r\nacc = accuracy_score(y, svc_built_in.predict(X))\r\nprint(\"Fitting SVC with built-in kernel took {:.1f} seconds\".format(time()-t1))\r\nprint(\"Accuracy: {}\".format(acc))\r\n\r\n# fit SVC with custom rbf kernel\r\nsvc_custom = SVC(kernel=rbf_kernel, gamma=gamma)\r\nnp.random.seed(13)\r\nt1 = time()\r\nsvc_custom.fit(X, y)\r\nacc = accuracy_score(y, svc_custom.predict(X))\r\nprint(\"Fitting SVC with a custom kernel took {:.1f} seconds\".format(time()-t1))\r\nprint(\"Accuracy: {}\".format(acc))\r\n```\r\n### Expected Results\r\n\r\nI would have assumed that both versions should run in the same time.\r\n\r\n### Actual Results\r\n\r\nFitting SVC with built-in kernel took 58.6 seconds\r\nAccuracy: 0.9846\r\nFitting SVC with a custom kernel took 3.2 seconds\r\nAccuracy: 0.9846\r\n\r\n### Versions\r\n\r\nSystem:\r\n    python: 3.8.12 (default, Oct 12 2021, 13:49:34)  [GCC 7.5.0]\r\nexecutable: \/home\/nwinter\/anaconda3\/envs\/mmll_gists\/bin\/python\r\n   machine: Linux-5.11.0-37-generic-x86_64-with-glibc2.17\r\n\r\nPython dependencies:\r\n          pip: 21.2.4\r\n   setuptools: 58.0.4\r\n      sklearn: 1.0\r\n        numpy: 1.21.3\r\n        scipy: 1.7.1\r\n       Cython: None\r\n       pandas: 1.3.4\r\n   matplotlib: 3.4.3\r\n       joblib: 1.1.0\r\nthreadpoolctl: 3.0.0\r\n\r\nBuilt with OpenMP: True","labels":["Bug","Performance","module:svm"],"created_at":"2021-10-22T18:43:34Z","comments":4,"reactions":2,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/21410"},{"issue_number":145,"repository":"scikit-learn\/scikit-learn","title":"LogisticRegression with SAGA using sample_weight does not converge","description":"### Describe the bug\r\n\r\nI am fitting\u00a0a logistic regression model on a sparse matrix and a binary response. Many of the rows of the matrix are repeated, so to speed things up I switched to a smaller sparse matrix with non-repeated rows and use the repetitions of the rows to calculate the\u00a0`sample_weight`\u00a0argument to\u00a0`fit`.\r\n\r\nThe issue is that when I work with the weighted model, `fit` produces a warning that it dit not converge because it took too many steps. \r\n\r\nI looked a bit under the hood of `fit` and\u00a0`sag_solver`\u00a0does some scaling of `alpha` and `beta` using `n_samples`. The resulting `alpha_scaled` and `beta_scaled` are different between the weighted and unweighted cases and they should not be (the loss function is the same). Perhaps the equivalent scaling for the weighted case should be the sum of the weights (if the intended 'unit' of the weights is 'count') and not just `n_samples`. Not sure if this is the issue, but it just it made me worry that the\u00a0`sample_weight`\u00a0argument is used in a bit naive way just as a multiplier for the loss function, while there might be scaling implications that are not accounted for when deciding when to stop. \r\n\r\nUPDATE: It seems that the issue is with the SAGA solver. I tried with liblinear and it seems to work. This will solve my immediate problem, because for now I only want the L1-reg. It is still good to look into this, because at the moment only SAGA offers elasticnet.\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n[vttrifonov\/logistic_sample_weights.ipynb](https:\/\/gist.github.com\/vttrifonov\/55ded32d11f93d26e64fdb2d4c7434d2)\r\n\r\n### Expected Results\r\n\r\nIn the above code I expect the second fit to run much faster than the first and to produce the same coefficients. \r\n\r\n### Actual Results\r\n\r\nIt fails in both.\r\n\r\n### Versions\r\n\r\nSystem:\r\n    python: 3.7.0 (default, Jun 28 2018, 07:39:16)  [Clang 4.0.1 (tags\/RELEASE_401\/final)]\r\nexecutable: \/Users\/vtrifonov\/projects\/tiny-proteins\/env\/bin\/python\r\n   machine: Darwin-19.6.0-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n          pip: 21.2.2\r\n   setuptools: 58.0.4\r\n      sklearn: 0.24.2\r\n        numpy: 1.20.3\r\n        scipy: 1.7.1\r\n       Cython: None\r\n       pandas: 1.3.2\r\n   matplotlib: 3.4.2\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True","labels":["Bug","Moderate","help wanted","module:linear_model"],"created_at":"2021-10-12T00:51:24Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/21305"},{"issue_number":146,"repository":"scikit-learn\/scikit-learn","title":"SpectralClustering breaks normalized Laplacian when the row sums are negative","description":"### Describe the bug\r\n\r\n_laplacian_dense from scipy\\sparse\\csgraph\\_laplacian.py breaks, crushing SpectralClustering\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.cluster import SpectralClustering\r\nx = np.random.randn(100, 10)\r\nx = x @ x.T\r\nspectral = SpectralClustering(n_clusters=4, affinity='precomputed',\r\n    random_state=0).fit(x)\r\nresult = list(spectral.labels_)\r\nprint(result)\r\n```\r\n\r\n### Expected Results\r\n\r\nNo error\r\n\r\n### Actual Results\r\n\r\n```python-traceback\r\n$ C:\/Users\/Name\/AppData\/Local\/Programs\/Python\/Python37\/python.exe c:\/Users\/Name\/Downloads\/tmp1.py\r\nC:\\Users\\Name\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\csgraph\\_laplacian.py:118: RuntimeWarning: invalid value encountered in sqrt\r\n  w = np.where(isolated_node_mask, 1, np.sqrt(w))\r\nTraceback (most recent call last):\r\n  File \"c:\/Users\/Name\/Downloads\/tmp1.py\", line 6, in <module>\r\n    random_state=0).fit(x)\r\n  File \"C:\\Users\\Name\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\cluster\\_spectral.py\", line 590, in fit\r\n    verbose=self.verbose,\r\n  File \"C:\\Users\\Name\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\cluster\\_spectral.py\", line 308, in spectral_clustering\r\n    drop_first=False,\r\n  File \"C:\\Users\\Name\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\manifold\\_spectral_embedding.py\", line 300, in spectral_embedding\r\n    laplacian, k=n_components, sigma=1.0, which=\"LM\", tol=eigen_tol, v0=v0\r\n  File \"C:\\Users\\Name\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py\", line 1645, in eigsh\r\n    hermitian=True, tol=tol)\r\n  File \"C:\\Users\\Name\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py\", line 1069, in get_OPinv_matvec\r\n    return LuInv(A).matvec\r\n  File \"C:\\Users\\Name\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py\", line 936, in __init__\r\n    self.M_lu = lu_factor(M)\r\n  File \"C:\\Users\\Name\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\scipy\\linalg\\decomp_lu.py\", line \r\n70, in lu_factor\r\n    a1 = asarray_chkfinite(a)\r\n  File \"C:\\Users\\Name\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 489, in asarray_chkfinite\r\n    \"array must not contain infs or NaNs\")\r\nValueError: array must not contain infs or NaNs\r\n```\r\n\r\n### Versions\r\n\r\nSystem:\r\n    python: 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\Users\\Name\\AppData\\Local\\Programs\\Python\\Python37\\python.exe\r\n   machine: Windows-10-10.0.19041-SP0\r\n\r\nPython dependencies:\r\n          pip: 21.2.4\r\n   setuptools: 58.1.0\r\n      sklearn: 1.0\r\n        numpy: 1.21.2\r\n        scipy: 1.7.1\r\n       Cython: 0.29.24\r\n       pandas: 1.3.3\r\n   matplotlib: 3.4.3\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.2.0","labels":["Bug","module:cluster"],"created_at":"2021-10-07T20:36:58Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/21274"},{"issue_number":147,"repository":"scikit-learn\/scikit-learn","title":"CountVectorizer(lowercase=True,strip_accents='unicode') may produce vocab that contains uppercase chars","description":"### Describe the bug\r\n\r\nSome characters are transformed to uppercase chars (eg \u2122 -> TM), despite lowercase=True.\r\nThis then gives warning messages \"UserWarning: Upper case characters found in vocabulary while 'lowercase' is True\"\r\n(OK, there are worse things happening in the world ;-). Thanks for scikit-learn)\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nx = ['This is Problematic\u2122.','THIS IS NOT']\r\n\r\ncv = CountVectorizer(\r\n    lowercase=True,\r\n    strip_accents='unicode',\r\n    ngram_range=(1,1)\r\n    )\r\n\r\nx_v = cv.fit_transform(x)\r\nprint(cv.get_feature_names_out()) # Contains \"problematicTM\"\r\n\r\n# then you can get a warning\r\n# \"UserWarning: Upper case characters found in vocabulary while 'lowercase' is True\"\r\n# if you both create a classifier AND run cv.fit_transform\r\n# (no message if you only call one)\r\n\r\nfrom sklearn.svm import LinearSVC\r\ny = [1,0]\r\nxtest = ['This is not']\r\nytest = [0]\r\n\r\n# comment either of the 2 following lines, no warning message displayed\r\nclf = LinearSVC(random_state=42).fit(x_v, y)\r\nxtest_v = cv.transform(xtest)\r\n```\r\n\r\n### Expected Results\r\n\r\nprint: ['is' 'not' 'problematictm' 'this']\r\n\r\n### Actual Results\r\n\r\nprint: ['is' 'not' 'problematicTM' 'this']\r\n\r\nwarning message:\r\n\/Users\/fps\/_fps\/DeveloperTools\/virtualenvs\/fps_env\/lib\/python3.9\/site-packages\/sklearn\/feature_extraction\/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\r\n  warnings.warn(\r\n\r\n### Versions\r\n\r\n\/Users\/fps\/_fps\/DeveloperTools\/virtualenvs\/fps_env\/lib\/python3.9\/site-packages\/setuptools\/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\r\n  warnings.warn(\r\n\r\nSystem:\r\n    python: 3.9.2 (v3.9.2:1a79785e3e, Feb 19 2021, 09:06:10)  [Clang 6.0 (clang-600.0.57)]\r\nexecutable: \/Users\/fps\/_fps\/DeveloperTools\/virtualenvs\/fps_env\/bin\/python3\r\n   machine: macOS-10.16-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n          pip: 21.2.4\r\n   setuptools: 49.2.1\r\n      sklearn: 1.0\r\n        numpy: 1.21.2\r\n        scipy: 1.7.1\r\n       Cython: None\r\n       pandas: 1.3.3\r\n   matplotlib: None\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True","labels":["Bug","module:feature_extraction"],"created_at":"2021-09-30T21:51:08Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/21207"},{"issue_number":148,"repository":"scikit-learn\/scikit-learn","title":"Is _generate_sample_indices working correctly ?","description":"### Describe the bug\r\n\r\nI fit two random forests (with just one tree each) with identical parameters, but for the second one I disable the row subsampling by setting `bootstrap=False` and instead pass the inbag data generated with _generate_sample_indices.\r\n\r\nI believe that the only stochastic elements of a random forest are the row and column subsampling. I disable the former by setting `bootstrap=False` and the latter as well by choosing `max_features = p`.\r\n\r\n\r\n### Steps\/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nfrom sklearn.ensemble._forest import _generate_sample_indices\r\nfrom sklearn.datasets import load_boston\r\nimport matplotlib.pyplot as plt\r\n\r\nboston = load_boston()\r\nxtrain, ytrain = boston.data, boston.target\r\nn_samples, p = xtrain.shape\r\n\r\nrf = RandomForestRegressor(n_estimators=1, \r\n    min_samples_leaf =10,max_features = p)\r\nrf.fit(xtrain, ytrain)\r\n\r\ntree=rf.estimators_[0]\r\nsampled_indices = _generate_sample_indices(tree.random_state, n_samples, n_samples)\r\n\r\nrf0 = RandomForestRegressor(n_estimators=1, \r\n    min_samples_leaf =10,max_features = p, bootstrap=False)\r\nrf0.fit(xtrain[sampled_indices,:], ytrain[sampled_indices])\r\n\r\np  =  rf.predict(xtrain)\r\np0 = rf0.predict(xtrain)\r\n\r\nplt.scatter(p,p0)\r\n```\r\n\r\n### Expected Results\r\n\r\nI would expect the two forests (consisting of a single tree) to be identical ?\r\n\r\n\r\n### Actual Results\r\n\r\nThe predictions are quite different as you can see in the output\r\n\r\n### Versions\r\n\r\nSystem:\r\n    python: 3.8.3 (default, Jul  2 2020, 11:26:31)  [Clang 10.0.0 ]\r\nexecutable: \/opt\/anaconda3\/bin\/python\r\n   machine: macOS-10.16-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n          pip: 20.1.1\r\n   setuptools: 49.2.0.post20200714\r\n      sklearn: 0.23.1\r\n        numpy: 1.19.5\r\n        scipy: 1.5.0\r\n       Cython: 0.29.21\r\n       pandas: 1.0.5\r\n   matplotlib: 3.2.2\r\n       joblib: 0.16.0\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n","labels":["Bug","module:ensemble","module:tree"],"created_at":"2021-09-09T20:16:26Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/21002"},{"issue_number":149,"repository":"scikit-learn\/scikit-learn","title":"HalvingGridSearchCV number of classes","description":"#### Describe the bug\r\n\r\nErrors:\r\nYou may have to run the code several times to see these errors. I think most of them is to do with the way the initial number of samples is selected; If the initial number of samples contains the same target then the fit function fails. ~I also seem to be running into problems with no samples being passed?~\r\n\r\nValueError: The number of classes has to be greater than one; got 1 class\r\n~ValueError: Found array with 0 sample(s) (shape=(0, 22)) while a minimum of 1 is required.~\r\nValueError: zero-size array to reduction operation maximum which has no identity\r\n\r\n\r\noblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\r\nERROR: The process \"8404\" not found.\r\nERROR: The process \"9492\" not found.\r\nERROR: The process \"28656\" not found.\r\nERROR: The process \"29236\" not found.\r\nERROR: The process \"29412\" not found.\r\nERROR: The process \"9004\" not found.\r\nERROR: The process \"28564\" not found.\r\nERROR: The process \"12072\" not found.\r\nERROR: The process with PID 3748 (child process of PID 11860) could not be terminated.\r\nReason: Access is denied.\r\nERROR: The process \"4372\" not found.\r\nERROR: The process \"2436\" not found.\r\nERROR: The process with PID 28212 (child process of PID 13280) could not be terminated.\r\nReason: Access is denied.\r\nERROR: The process \"25468\" not found.\r\nERROR: The process with PID 27876 (child process of PID 9884) could not be terminated.\r\nReason: Access is denied.\r\nERROR: The process with PID 27912 (child process of PID 19872) could not be terminated.\r\nReason: Access is denied.\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\nExample:\r\n```python\r\nimport numpy as np\r\nfrom sklearn.decomposition import KernelPCA\r\nfrom sklearn.experimental import enable_halving_search_cv\r\nfrom sklearn.model_selection import HalvingGridSearchCV\r\nfrom sklearn.pipeline import Pipeline\r\n\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.model_selection import ShuffleSplit\r\nfrom sklearn.metrics import make_scorer, f1_score\r\n\r\ndata_input = np.random.randn(4000,22)\r\ndata_output = np.round(np.random.random((4000,))).astype((int))\r\n\r\nsearch_points = 10\r\ngamma = np.logspace(-9, 1, search_points)\r\ncoef = np.linspace(-1, 1, search_points)\r\npoints = np.round(np.linspace(1, len(data_input[0, :]), search_points))\r\nFE_algorithm = KernelPCA(kernel='rbf')\r\nFE_grid = {'FE__n_components': points.astype(int), 'FE__gamma': gamma,'FE__coef0': coef,}\r\n\r\nregularisation = np.logspace(0,1,search_points)\r\nClass_algorithm = SVC(kernel='linear')\r\nCA_grid = {'CA__C': regularisation,}\r\n\r\npipe = Pipeline(steps=[('FE', FE_algorithm), ('CA', Class_algorithm)])\r\nparam_grid = {**FE_grid, **CA_grid} # concat dicts\r\n\r\nscorer = make_scorer(f1_score, average='weighted')\r\nsearch = HalvingGridSearchCV(pipe, param_grid, cv=ShuffleSplit(test_size=0.20, n_splits=1, random_state=0),\r\n                                      n_jobs=-1, verbose=3, scoring=scorer)\r\n\r\nsearch.fit(data_input, data_output)\r\n```\r\n\r\n#### Versions\r\n\r\nWindows-10-10.0.19041-SP0\r\nPython 3.7.4 (tags\/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]\r\nNumPy 1.19.5\r\nSciPy 1.6.3\r\nScikit-Learn 0.24.1\r\n\r\n\r\n\r\n","labels":["Bug","module:model_selection"],"created_at":"2021-08-19T13:43:05Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/20775"},{"issue_number":150,"repository":"scikit-learn\/scikit-learn","title":"RegressorChain support for Pipelines including ColumnTransformer","description":"#### Describe the bug\r\n\r\nI can't seem to get the `RegressorChain` working with pipelines that include a `ColumnTransformer`. I posted an issue on StackOverflow with more: https:\/\/stackoverflow.com\/questions\/68430993\/sklearn-using-regressorchain-with-columntransformer-in-pipelines .\r\n\r\nSomewhere in `__init__.py \/ _get_column_indices(X, key)` this call fails: `all_columns = X.columns`  saying `'numpy.ndarray' object has no attribute 'columns'`. Because this is a known issue with `ColumnTransformer`, I suspect the `RegressorChain` can't be used with it.\r\n\r\nI'm not sure if this is a supported scenario, but the documentation for RegressorChain (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.multioutput.RegressorChain.html), for `set_params`, includes this:\r\n\r\n\"The method works on simple estimators **as well as on nested objects (such as Pipeline)**. The latter have parameters of the form <component>__<parameter> so that it\u2019s possible to update each component of a nested object.\"\r\n\r\nSo I was led to assume it would also work with Pipelines including the column transformer.\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\nAny example with a Pipeline containing a ColumnTransformer and a Regressor. The StackOverflow link I included above has my code.\r\n\r\n#### Expected Results\r\n\r\nFitted pipeline.\r\n\r\n#### Actual Results\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nC:\\ProgramData\\Anaconda3\\envs\\py38aml\\lib\\site-packages\\sklearn\\utils\\__init__.py in _get_column_indices(X, key)\r\n    373         try:\r\n--> 374             all_columns = X.columns\r\n    375         except AttributeError:\r\n\r\nAttributeError: 'numpy.ndarray' object has no attribute 'columns'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-181-24da1e03388c> in <module>\r\n      3 \r\n      4 chain_regressor = RegressorChain(base_estimator=chain_pipeline) #, order=[1,0,2])\r\n----> 5 chain_regressor.fit(X, y)\r\n      6 \r\n      7 \r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py38aml\\lib\\site-packages\\sklearn\\multioutput.py in fit(self, X, Y, **fit_params)\r\n    840         self : object\r\n    841         \"\"\"\r\n--> 842         super().fit(X, Y, **fit_params)\r\n    843         return self\r\n    844 \r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py38aml\\lib\\site-packages\\sklearn\\multioutput.py in fit(self, X, Y, **fit_params)\r\n    507         for chain_idx, estimator in enumerate(self.estimators_):\r\n    508             y = Y[:, self.order_[chain_idx]]\r\n--> 509             estimator.fit(X_aug[:, :(X.shape[1] + chain_idx)], y,\r\n    510                           **fit_params)\r\n    511             if self.cv is not None and chain_idx < len(self.estimators_) - 1:\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py38aml\\lib\\site-packages\\sklearn\\compose\\_target.py in fit(self, X, y, **fit_params)\r\n    205             self.regressor_ = clone(self.regressor)\r\n    206 \r\n--> 207         self.regressor_.fit(X, y_trans, **fit_params)\r\n    208 \r\n    209         return self\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py38aml\\lib\\site-packages\\sklearn\\pipeline.py in fit(self, X, y, **fit_params)\r\n    339         \"\"\"\r\n    340         fit_params_steps = self._check_fit_params(**fit_params)\r\n--> 341         Xt = self._fit(X, y, **fit_params_steps)\r\n    342         with _print_elapsed_time('Pipeline',\r\n    343                                  self._log_message(len(self.steps) - 1)):\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py38aml\\lib\\site-packages\\sklearn\\pipeline.py in _fit(self, X, y, **fit_params_steps)\r\n    301                 cloned_transformer = clone(transformer)\r\n    302             # Fit or load from cache the current transformer\r\n--> 303             X, fitted_transformer = fit_transform_one_cached(\r\n    304                 cloned_transformer, X, y, None,\r\n    305                 message_clsname='Pipeline',\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py38aml\\lib\\site-packages\\joblib\\memory.py in __call__(self, *args, **kwargs)\r\n    350 \r\n    351     def __call__(self, *args, **kwargs):\r\n--> 352         return self.func(*args, **kwargs)\r\n    353 \r\n    354     def call_and_shelve(self, *args, **kwargs):\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py38aml\\lib\\site-packages\\sklearn\\pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)\r\n    752     with _print_elapsed_time(message_clsname, message):\r\n    753         if hasattr(transformer, 'fit_transform'):\r\n--> 754             res = transformer.fit_transform(X, y, **fit_params)\r\n    755         else:\r\n    756             res = transformer.fit(X, y, **fit_params).transform(X)\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py38aml\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py in fit_transform(self, X, y)\r\n    503         self._validate_transformers()\r\n    504         self._validate_column_callables(X)\r\n--> 505         self._validate_remainder(X)\r\n    506 \r\n    507         result = self._fit_transform(X, y, _fit_transform_one)\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py38aml\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py in _validate_remainder(self, X)\r\n    330         cols = []\r\n    331         for columns in self._columns:\r\n--> 332             cols.extend(_get_column_indices(X, columns))\r\n    333 \r\n    334         remaining_idx = sorted(set(range(self._n_features)) - set(cols))\r\n\r\nC:\\ProgramData\\Anaconda3\\envs\\py38aml\\lib\\site-packages\\sklearn\\utils\\__init__.py in _get_column_indices(X, key)\r\n    374             all_columns = X.columns\r\n    375         except AttributeError:\r\n--> 376             raise ValueError(\"Specifying the columns using strings is only \"\r\n    377                              \"supported for pandas DataFrames\")\r\n    378         if isinstance(key, str):\r\n\r\nValueError: Specifying the columns using strings is only supported for pandas DataFrames\r\n```\r\n\r\n#### Versions\r\n\r\nSystem:\r\n    python: 3.8.10 (default, May 19 2021, 13:12:57) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\ProgramData\\Anaconda3\\envs\\py38aml\\python.exe\r\n   machine: Windows-10-10.0.22000-SP0\r\n\r\nPython dependencies:\r\n          pip: 21.1.3\r\n   setuptools: 52.0.0.post20210125\r\n      sklearn: 0.24.2\r\n        numpy: 1.20.2\r\n        scipy: 1.6.2\r\n       Cython: None\r\n       pandas: 1.2.5\r\n   matplotlib: 3.3.4\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.2.0\r\n\r\nBuilt with OpenMP: True","labels":["Bug","help wanted","Hard","module:preprocessing"],"created_at":"2021-07-18T16:46:16Z","comments":4,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/20557"},{"issue_number":151,"repository":"scikit-learn\/scikit-learn","title":"AdaBoost's training error can increase with a larger number of trees","description":"We encountered the following behavior of AdaBoost when working on the [scikit-learn mooc](https:\/\/mooc-forums.inria.fr\/moocsl\/t\/quiz-m6-03-question-3\/3128\/15). Here is a script that reproduces the problem:\r\n\r\nWe compute the validation curves for 3 models with different numbers of leaf nodes in the weak learners and each time study the impact of the number of trees on the train and validation errors.\r\n\r\n\r\n```python\r\nfrom sklearn.ensemble import AdaBoostRegressor\r\nfrom sklearn.tree import DecisionTreeRegressor\r\nfrom sklearn.model_selection import validation_curve\r\nfrom sklearn.datasets import fetch_california_housing\r\nimport matplotlib.pyplot as plt\r\n\r\n\r\ndata, target = fetch_california_housing(return_X_y=True)\r\n\r\nfor max_leaf_nodes in [50, 100, 500]:\r\n    adaboost = AdaBoostRegressor(\r\n        learning_rate=1.,\r\n        base_estimator=DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes),\r\n    )\r\n    param_range = [1, 2, 5, 10, 20, 50, 100, 500]\r\n    train_scores, test_scores = validation_curve(\r\n        adaboost,\r\n        data,\r\n        target,\r\n        param_name=\"n_estimators\",\r\n        param_range=param_range,\r\n        scoring=\"neg_mean_absolute_error\",\r\n        n_jobs=4,\r\n    )\r\n    train_errors, test_errors = -train_scores, -test_scores\r\n    plt.figure()\r\n    plt.errorbar(param_range, train_errors.mean(axis=1),\r\n                 yerr=train_errors.std(axis=1), label=\"Training score\",\r\n                 alpha=0.7)\r\n    plt.errorbar(param_range, test_errors.mean(axis=1),\r\n                 yerr=test_errors.std(axis=1), label=\"Cross-validation score\",\r\n                 alpha=0.7)\r\n    plt.legend()\r\n    plt.xscale(\"log\")\r\n    plt.xticks(param_range)\r\n    plt.ylabel(\"Mean absolute error in k$\\n(smaller is better)\")\r\n    plt.xlabel(\"# estimators\")\r\n    _ = plt.title(\r\n        f\"AdaBoost regressor with {max_leaf_nodes=}\\n\"\r\n        f\"best CV score: {test_errors.mean(axis=1).min():.4f}\"\r\n    )\r\n```\r\n\r\nHere are the results:\r\n\r\n![adaboost_50_leaves](https:\/\/user-images.githubusercontent.com\/89061\/124276287-e0752f00-db43-11eb-9a45-ae83a6507218.png)\r\n![adaboost_100_leaves](https:\/\/user-images.githubusercontent.com\/89061\/124276290-e10dc580-db43-11eb-8848-32b8212019e6.png)\r\n![adaboost_500_leaves](https:\/\/user-images.githubusercontent.com\/89061\/124276292-e1a65c00-db43-11eb-8525-8f8da999477e.png)\r\n\r\nFor 500 leaves we get the expected behavior: both the training and validation errors decrease when increasing the number of trees. At some point we reach a plateau of diminishing returns (expected, at least for validation error).\r\n\r\nHowever for lower number of leaves (e.g. the second plot with `max_leaf_nodes=50`), the training set is growing with more trees! This is really something that I would not have expected. I wonder if this is not revealing a bug in our implementation.\r\n\r\nFor reference, for lower values of `max_leaf_nodes` both the training and validation errors stay do not decrease (and even increase again) and they stay close to one another (severe under fitting) with large error bars.","labels":["Bug","module:ensemble"],"created_at":"2021-07-02T12:49:24Z","comments":13,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/20443"},{"issue_number":152,"repository":"scikit-learn\/scikit-learn","title":"Numerical stability problems in GaussianProcessRegressor on 32 bit Python","description":"See the failing tests observed on 32 bit Linux with Python 3.7 or 3.8 in #20334 for details.\r\n\r\nThose tests are skipped for now to avoid blocking the nightly build and release processes.","labels":["Bug","Build \/ CI"],"created_at":"2021-06-23T13:00:50Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/20337"},{"issue_number":153,"repository":"scikit-learn\/scikit-learn","title":"Test failure: test_loadings_converges in test_pls.py","description":"Observed on travis last night, possibly caused by the upgrade of numpy \/ scipy:\r\n\r\n```python\r\n___________________________ test_loadings_converges ____________________________\r\n[gw1] linux -- Python 3.7.10 \/tmp\/tmp.J9NDQpQnRJ\/venv\/bin\/python\r\n\r\n    def test_loadings_converges():\r\n        \"\"\"Test that CCA converges. Non-regression test for #19549.\"\"\"\r\n        X, y = make_regression(n_samples=200, n_features=20, n_targets=20, random_state=20)\r\n        cca = CCA(n_components=10, max_iter=500)\r\n        with pytest.warns(None) as record:\r\n            cca.fit(X, y)\r\n        # ConvergenceWarning is not raised\r\n>       assert not record\r\nE       assert not WarningsChecker(record=True)\r\n\/tmp\/tmp.J9NDQpQnRJ\/venv\/lib\/python3.7\/site-packages\/sklearn\/cross_decomposition\/tests\/test_pls.py:583: AssertionError\r\n```\r\n\r\nI haven't investigated the cause yet. I can probably give it a look interactively in a linux arm64 docker container on my M1 laptop.","labels":["Bug","Build \/ CI"],"created_at":"2021-06-23T10:20:45Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/20335"},{"issue_number":154,"repository":"scikit-learn\/scikit-learn","title":"ElasticNet (and Lasso) for multiple outputs is not described correctly","description":"#### Describe the issue linked to the documentation\r\n\r\nThere are 2 ways to run ElasticNet with multiple outputs. The first is to run it for each output (method A) and the second is to use a different objective, using `||W||_21` as norm (method B). From what I see, ElasticNet is supposed to apply method A and MultiTaskElasticNet uses method B. enet_path and lasso_path apply method B.\r\n\r\nHowever, ElasticNet and Lasso's path method is exactly enet_path (`path = staticmethod(enet_path)`). This causes confusion, as\r\n\r\n1. fit and path don't apply the same algorithm\r\n2. method B is documented in by enet_path and hence auto-included on the pages of ElasticNet and Lasso\r\n\r\n#### Suggest a potential alternative\/fix\r\n\r\nChanging the API for `path` to match method A would cause compatibility issues. I propose to add a note somewhere to explain that `path` doesn't apply the same algorithm as `fit` in Lasso and ElasticNet, and add a link to MultiTaskLasso and MultiTaskElasticNet.\r\n","labels":["Bug","Documentation","module:linear_model"],"created_at":"2021-04-12T21:35:35Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19868"},{"issue_number":155,"repository":"scikit-learn\/scikit-learn","title":"ClassifierChain should only accept multilabel-indicator","description":"By trying to solve https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19357 and writing some test, it seems that `ClassifierChain` is expected to be fitted with `multilabel-indicator` target (each column should only contain 0\/1 classes).\r\n\r\nHowever, there is no check and one can fit a `multiclass-multioutput` target. The classifier will later fail if calling `decision_function` that would return an array of shape `n_samples, 3` while an array of `n_samples,` is expected. I assume a similar behaviour for `predict_proba`.\r\n\r\nI think that we should check the type of target to raise a proper error at `fit`.","labels":["Bug","module:multioutput"],"created_at":"2021-04-09T17:34:43Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19853"},{"issue_number":156,"repository":"scikit-learn\/scikit-learn","title":"Inconsistent behavior with bagging & base estimator class_weight","description":"#### Describe the bug\r\n\r\nBagging Classifier transforms y to indices between 0..N but does not impact the possible `base_estimator.class_weight` attribute making the fit method fails when using class_weight: the class could not be found.\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\nThis is working fine: \r\n\r\n```python\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.linear_model import PassiveAggressiveClassifier\r\n\r\nX, y = make_classification(n_samples=10000, n_features=5, n_redundant=0, n_clusters_per_class=1, weights=[0.5])\r\n# let's say y is not 0 or 1 but something else like 1 and 2 for instance:\r\ny += 1\r\n# now i fit a classifier with class_weight, works fine\r\nPassiveAggressiveClassifier(class_weight={1: 1, 2: 1}).fit(X, y)\r\n```\r\n\r\nNow i'm just wrapping it around BaggingClassifier:\r\n\r\n```python\r\nfrom sklearn.ensemble import BaggingClassifier\r\n\r\nBaggingClassifier(PassiveAggressiveClassifier(class_weight={1: 1, 2: 1})).fit(X, y)\r\n```\r\n\r\nI'm getting: \r\n\r\n```pytb\r\nValueError: Class label 2 not present.\r\n```\r\n\r\nIt seems it comes from here : https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/95119c13af77c76e150b753485c662b7c52a41a2\/sklearn\/ensemble\/_bagging.py#L654 \r\n\r\nBecause bagging transform y but then class_weight attribute of the base estimator is not aligned with the new labels.\r\n\r\nIs it expected or this is a bug ?\r\n\r\n#### Versions\r\n\r\n```\r\n System:\r\n    python: 3.7.9 (default, Feb 13 2021, 00:48:23)  [GCC 10.2.1 20210110]\r\nexecutable: \/home\/...\/.pyenv\/versions\/3.7.9\/bin\/python3.7\r\n   machine: Linux-5.10.0-3-amd64-x86_64-with-debian-bullseye-sid\r\n\r\nPython dependencies:\r\n          pip: 21.0.1\r\n   setuptools: 53.0.0\r\n      sklearn: 0.24.1\r\n        numpy: 1.20.1\r\n        scipy: 1.6.1\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: None\r\n       joblib: 1.0.1\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n```","labels":["Bug","module:ensemble"],"created_at":"2021-03-11T19:53:38Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19665"},{"issue_number":157,"repository":"scikit-learn\/scikit-learn","title":"sklearn.manifold.locally_linear_embedding() enforces use of euclidean distance when passing a NearestNeighbors instance with a precomputed distance matrix as parameter","description":"<!--\r\nBefore submitting a bug, please make sure the issue hasn't been already\r\naddressed by searching through the past issues.\r\n-->\r\n\r\n#### Describe the bug\r\n<!--\r\nA clear and concise description of what the bug is.\r\n-->\r\nWhen calling the `locally_linear_embedding()` method with a _NearestNeighbors_ instance as its `X` parameter, the euclidean distance is always used in the standard version of the algorithm (`method = 'standard'`) defeating the purpose of passing down a _NearestNeighbors_ instance with a precomputed distance matrix (`metric = 'precomputed'`).\r\n\r\n#### Steps\/Code to Reproduce\r\n<!--\r\nPlease add a minimal example that we can reproduce the error by running the\r\ncode. Be as succinct as possible, do not depend on external data. In short, we\r\nare going to copy-paste your code and we expect to get the same\r\nresult as you.\r\n\r\nExample:\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation\r\ndocs = [\"Help I have a bug\" for i in range(1000)]\r\nvectorizer = CountVectorizer(input=docs, analyzer='word')\r\nlda_features = vectorizer.fit_transform(docs)\r\nlda_model = LatentDirichletAllocation(\r\n    n_topics=10,\r\n    learning_method='online',\r\n    evaluate_every=10,\r\n    n_jobs=4,\r\n)\r\nmodel = lda_model.fit(lda_features)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```python\r\nfrom sklearn.neighbors import NearestNeighbors\r\nfrom sklearn.manifold import locally_linear_embedding\r\n\r\nD = distance_matrix(data) # Precomputation of the distance matrix of some data with a custom distance measure\r\n\r\nnbrs = NearestNeighbors(n_neighbors = 5,\r\n                        metric = 'precomputed')\r\nnbrs.fit(D) # NearestNeighbors instance with a precomputed distance matrix\r\n\r\nZ,_ =  locally_linear_embedding(nbrs,\r\n                                n_neighbors = 5,\r\n                                n_components = 2) # Passing down the NearestNeighbors instance with a precomputed distance matrix\r\n```\r\n\r\n#### Expected Results\r\n<!-- Example: No error is thrown. Please paste or describe the expected results.-->\r\nEmbeddings computed using the distance matrix passed down to `locally_linear_embedding()` through the _NearestNeighbors_ instance.\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback. -->\r\nEmbeddings are instead computed using the distance matrix passed down to `locally_linear_embedding()` through the _NearestNeighbors_ instance treating it as a dataset with shape _(n_samples, n_features)_. This happens when calling `barycenter_kneighbors_graph()` on line 320 which in turn calls the `kneighbors()` with a new instance of _NearestNeighbors_. This new instance has attribute `self.effective_metric_='euclidian'`. The problem happens when `kneighbors()` does a internal check on `self.effective_metric_` and decides to use euclidian metric.\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nFor scikit-learn >= 0.20:\r\nimport sklearn; sklearn.show_versions()\r\nFor scikit-learn < 0.20:\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\nimport imblearn; print(\"Imbalanced-Learn\", imblearn.__version__)\r\n-->\r\n\r\nSystem:\r\n    python: 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\ProgramData\\Anaconda3\\python.exe\r\n   machine: Windows-10-10.0.19041-SP0\r\n\r\nPython dependencies:\r\n          pip: 20.1.1\r\n   setuptools: 49.2.0.post20200714\r\n      sklearn: 0.23.1\r\n        numpy: 1.18.5\r\n        scipy: 1.5.0\r\n       Cython: 0.29.21\r\n       pandas: 1.0.5\r\n   matplotlib: 3.2.2\r\n       joblib: 0.16.0\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug"],"created_at":"2021-02-19T18:26:28Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19505"},{"issue_number":158,"repository":"scikit-learn\/scikit-learn","title":"SVC's predict_proba(...) predicts the exact same probability for wildly different inputs","description":"I'm using `scikit-learn` version 0.24.1.\r\n\r\nI suspect some form of memoization or extreme low sensitivity happening in the SVC `predict_proba(...)` implementation whereby once a SVC model is built from a Pipeline with inputs scaling preprocessing and calibration and when setting the `random_state` argument; I get the exact same predicted probability for wildly different inputs.\r\n\r\nI also checked the `decision_function(..)` result and it returns the exact same value for two wildly different `x_test` inputs.\r\n\r\nThis issue only happens when setting the argument `random_state` e.g. to zero. It is hard to create a MRE here also because of NDA and confidentiality of the dataset and work but I propose to export the pipeline like this:\r\n\r\n```\r\nfrom joblib import dump, load\r\ndump(model, '\/some\/where\/model.joblib')\r\n```\r\nand provide the two wildly different inputs leading to the same exact `predict_proba(...)` and `decision_function(...)` results. Would that be a viable solution to reproduce and fix the possible bug?\r\n\r\nA simplified relevant example of my pipeline is the following:\r\n```\r\npipeline = Pipeline(steps=[('preprocess', MaxAbsScaler()),\r\n                           ('svm', SVC(kernel='rbf', probability=True, \r\n                                       random_state=0, decision_function_shape='ovr', \r\n                                       break_ties=False, ...))]\r\nparams = [{...}]\r\nmodel = GridSearchCV(pipeline, params, ...).fit(x_train, y_train).best_estimator_\r\n\r\n# and now I get\r\nprob1 = model.predict_proba(x_test1)\r\nprob2 = model.predict_proba(x_test2_wildly_diff_from_test1)\r\nassert np.abs(prob1 - prob2) < 1e-10\r\n```\r\n\r\nFor example, using `random_seed=0`:\r\n```\r\n>>> import numpy as np\r\n>>> from scipy.spatial import distance\r\n# how far are the two x_test vectors from each other?\r\n>>> distance.cosine(x_test1, x_test2)  # EDIT: very far apart angle-wise e.g. 90\u00b0\r\n1.0280449512858494\r\n>>> distance.euclidean(x_test1, x_test2) # very far in euclidean distance\r\n30675.221284568033\r\n>>> model.predict_proba(x_test1)\r\narray([[0.86879653, 0.13120347]])\r\n>>> model.predict_proba(x_test2)\r\narray([[0.86879653, 0.13120347]])\r\n>>> model.decision_function(x_test1)\r\narray([-0.03474242])\r\n>>> model.decision_function(x_test2)\r\narray([-0.03474242])\r\n```\r\n\r\nUPDATE: what bothers me is not that they are close, what spooks me is that the two completely different vectors end up getting the exact same distance (to the 1e-10 decimal place) from the decision boundary `decision_function(...)` and therefore the exact probability too `predict_proba(...)`. I'm still thinking how to further validate and scrutinize this case ...\r\n","labels":["Bug","module:svm"],"created_at":"2021-02-12T10:03:43Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19447"},{"issue_number":159,"repository":"scikit-learn\/scikit-learn","title":"Interactive Imputer cannot accept PLSRegression() as an estimator due to \"shape mismatch\"","description":"<!--\r\nBefore submitting a bug, please make sure the issue hasn't been already\r\naddressed by searching through the past issues.\r\n-->\r\n\r\n#### Describe the bug\r\nWhen setting the estimator as PLSRegression(), a ValueError is triggered by module '_iteractive.py' in line 348, caused by \"shape mismatch\"\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\nExample:\r\n```python\r\nimport numpy as np\r\n\r\nfrom sklearn.datasets import fetch_california_housing\r\nfrom sklearn.cross_decomposition import PLSRegression\r\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\r\nfrom sklearn.impute import IterativeImputer\r\n\r\nrng = np.random.RandomState(42)\r\n\r\nX_california, y_california = fetch_california_housing(return_X_y=True)\r\nX_california = X_california[:400]\r\ny_california = y_california[:400]\r\n\r\ndef add_missing_values(X_full, y_full):\r\n    n_samples, n_features = X_full.shape\r\n\r\n    # Add missing values in 75% of the lines\r\n    missing_rate = 0.75\r\n    n_missing_samples = int(n_samples * missing_rate)\r\n\r\n    missing_samples = np.zeros(n_samples, dtype=bool)\r\n    missing_samples[: n_missing_samples] = True\r\n\r\n    rng.shuffle(missing_samples)\r\n    missing_features = rng.randint(0, n_features, n_missing_samples)\r\n    X_missing = X_full.copy()\r\n    X_missing[missing_samples, missing_features] = np.nan\r\n    y_missing = y_full.copy()\r\n\r\n    return X_missing, y_missing\r\n\r\nX_miss_california, y_miss_california = add_missing_values(\r\n    X_california, y_california)\r\n\r\nimputer = IterativeImputer(estimator=PLSRegression(n_components=2))\r\n\r\nX_imputed = imputer.fit_transform(X_miss_california)\r\nprint(X_imputed)\r\n```\r\n\r\n\r\n#### Expected Results: after applying the workaround below:\r\n```python\r\n[[   8.3252       41.            6.98412698 ...    2.55555556\r\n    37.88       -122.25930206]\r\n [   8.3014       21.            6.23813708 ...    2.10984183\r\n    37.86       -122.22      ]\r\n [   7.2574       52.            8.28813559 ...    2.80225989\r\n    37.85       -122.24      ]\r\n ...\r\n [   3.60438721   50.            5.33480176 ...    2.30396476\r\n    37.88       -122.29      ]\r\n [   5.1675       52.            6.39869281 ...    2.44444444\r\n    37.89       -122.29      ]\r\n [   5.1696       52.            6.11590296 ...    2.70619946\r\n    37.8709526  -122.29      ]]\r\n```\r\n\r\n#### Actual Results\r\n```python\r\nFile \"\/home\/hushsh\/py3\/lib\/python3.6\/site-packages\/sklearn\/impute\/_iterative.py\", line 348, in _impute_one_feature\r\n    X_filled[missing_row_mask, feat_idx] = imputed_values\r\nValueError: shape mismatch: value array of shape (27,1) could not be broadcast to indexing result of shape (27,) \r\n```\r\n\r\n#### Versions\r\nSystem:\r\n    python: 3.6.9 (default, Oct  8 2020, 12:12:24)  [GCC 8.4.0]\r\nexecutable: \/home\/hushsh\/raid_data\/py3\/bin\/python\r\n   machine: Linux-5.4.0-60-generic-x86_64-with-LinuxMint-19.3-tricia\r\n\r\nPython dependencies:\r\n          pip: 21.0.1\r\n   setuptools: 47.3.1\r\n      sklearn: 0.24.1\r\n        numpy: 1.18.1\r\n        scipy: 1.4.1\r\n       Cython: 0.29.15\r\n       pandas: 1.1.3\r\n   matplotlib: 3.1.2\r\n       joblib: 0.14.1\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\n#### My Workaround that fixed the bug: Insert the following three lines before line 348\r\n```python\r\nshape_imputed_values = imputed_values.shape\r\nif len(shape_imputed_values)>1:\r\n    # convert 2D array to 1D array fixes the bug:\r\n    imputed_values = imputed_values.reshape(shape_imputed_values[0])\r\n```\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","Hard","module:cross_decomposition"],"created_at":"2021-02-05T06:33:02Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19352"},{"issue_number":160,"repository":"scikit-learn\/scikit-learn","title":"ExpSineSquared GP Kernel is not PD","description":"Here's a bit of test code:\r\n\r\n```python\r\nimport numpy as np\r\nimport scipy\r\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\r\n\r\nL = 1.0\r\n\r\n# create some train\/test data on a grid\r\ntrain_len = 4\r\nr = np.linspace(0, L, train_len)\r\ntrain_x, train_y = np.meshgrid(r, r)\r\ntrain_in = np.stack((train_x.flatten(), train_y.flatten()), axis=-1)\r\n\r\n# get the kernel\r\nkernel = ExpSineSquared()\r\n\r\nK = kernel(train_in) + 1e-4 * np.eye(train_len**2)\r\n\r\nprint(np.sort(np.linalg.eigvals(K)))\r\n```\r\n\r\nThis rather popular kernel for modeling periodic functions doesn't seem to be a truly positive definite kernel, at least in the sklearn implementation, as some of the eigenvalues printed are negative. Is this an issue with the sklearn implementation, or is this a weird issue with the kernel itself?\r\n\r\nI think folks found something similar in #9824 ","labels":["Bug","module:gaussian_process"],"created_at":"2021-02-03T23:40:11Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19343"},{"issue_number":161,"repository":"scikit-learn\/scikit-learn","title":"sklearn Birch memory usage is over 3x higher than original BIRCH (+ Python overheads)","description":"The original BIRCH algorithm defined clustering features as CF=(N,LS,SS) where N is an integer, LS is a vector (\"linear sum\") and SS is a scalar value (sum of squared values). Assuming double precision, a clustering feature for d dimensional data should hence use `4+8*d+8` bytes.\r\nMemory efficiency is the key motivation of BIRCH.\r\n\r\nThe sklearn implementation will require WAY more memory, and its not just to boxing overhead:\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/8c6a045e46abe94e43a971d4f8042728addfd6a7\/sklearn\/cluster\/_birch.py#L286-L290\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/8c6a045e46abe94e43a971d4f8042728addfd6a7\/sklearn\/cluster\/_birch.py#L292-L297\r\nas you can see, it stores (N,LS,LS\/N,SS,||LS\/N||) instead, about `4+8*d*2+8*2` which is about twice the original memory requirements not even taking boxing overhead into account.\r\n\r\nThe CFTree in BIRCH is meant to be low level - largely a list of CFs and child pointers for inner nodes. Here, the current sklearn implementation uses substantially more memory, too:\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/8c6a045e46abe94e43a971d4f8042728addfd6a7\/sklearn\/cluster\/_birch.py#L139-L152\r\nWe first have several constants replicated: threshold, branching factor, is_leaf and n_features are redundantly stored in every node of the tree.\r\n`subclusters_` is okay, that is what you would expect: a list of `_CFSubcluster`; except that this one is not preallocated in size.\r\n`init_centroids_` is a *copy* of the centroids of the CFs, `init_sq_norm_` of their norms. Then there is a double-chaining of leaves in every node (leaf or not) via `prev_leaf_` and `next_leaf_`, but they are only navigated in forward order ever, and _get_leaves will likely be called only once and it would be possible to collect them by tree traversal more efficiently rather than maintaining this list throughout the tree construction. There even is an additional `dummy_leaf_` to access the double-linked list because of that, which is completely initialized with an empty matrix.\r\nThe additional copies of LS\/N and ||LS\/N|| increase the Birch memory consumption to be more than three times the original memory; not taking any additional Python overheads into account.","labels":["Bug","module:cluster"],"created_at":"2021-01-23T15:04:54Z","comments":0,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19258"},{"issue_number":162,"repository":"scikit-learn\/scikit-learn","title":"train_test_split stratify with a pandas Int32 dtype causes error","description":"<!--\r\nBefore submitting a bug, please make sure the issue hasn't been already\r\naddressed by searching through the past issues.\r\n-->\r\n\r\n#### Describe the bug\r\n<!--\r\nA clear and concise description of what the bug is.\r\n-->\r\n\r\nIt appears since 0.23.0 using multiple columns in train_test_split's stratify option results in an error if one column is the pandas nullable int `Int32Dtype()` type. Error does not occur in 0.22.2.post1\r\n\r\n#### Steps\/Code to Reproduce\r\n<!--\r\nPlease add a minimal example that we can reproduce the error by running the\r\ncode. Be as succinct as possible, do not depend on external data. In short, we\r\nare going to copy-paste your code and we expect to get the same\r\nresult as you.\r\n\r\nExample:\r\n\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation\r\ndocs = [\"Help I have a bug\" for i in range(1000)]\r\nvectorizer = CountVectorizer(input=docs, analyzer='word')\r\nlda_features = vectorizer.fit_transform(docs)\r\nlda_model = LatentDirichletAllocation(\r\n    n_topics=10,\r\n    learning_method='online',\r\n    evaluate_every=10,\r\n    n_jobs=4,\r\n)\r\nmodel = lda_model.fit(lda_features)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\nThe following code will work in 0.22.2.post1 but will give an error in 0.23.0 or later (including 0.24.1). If only column `a` or `c` is chosen for `stratify` it will work. It will also work if column `c` is a numpy `int32` type rather than the nullable pandas `Int32`.\r\n\r\n```\r\nfrom sklearn.model_selection import train_test_split\r\nimport pandas as pd\r\n\r\ndf = pd.DataFrame({'a':['a','b','b']*10,'b':list(range(30)),'c':[1,2,2]*10})\r\ndf['c']=df['c'].astype('Int32')\r\n\r\nx_train, x_test, y_train, y_test=train_test_split(df.drop(columns='a'),df['a'],test_size=.33, stratify=df[['a','c']])\r\n\r\n```\r\n\r\n#### Expected Results\r\n<!-- Example: No error is thrown. Please paste or describe the expected results.-->\r\nNo error is thrown and results are:\r\n```\r\n>>> x_train\r\n   b  c\r\n4  4  2\r\n1  1  2\r\n5  5  2\r\n3  3  1\r\n>>> x_test\r\n   b  c\r\n0  0  1\r\n2  2  2\r\n>>> y_train\r\n4    b\r\n1    b\r\n5    b\r\n3    a\r\nName: y_train, dtype: object\r\n>>> y_test\r\n0    a\r\n2    b\r\nName: y_test, dtype: object\r\n```\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback. -->\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/sklearn\/model_selection\/_split.py\", line 2197, in train_test_split\r\n    train, test = next(cv.split(X=arrays[0], y=stratify))\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/sklearn\/model_selection\/_split.py\", line 1793, in split\r\n    y = check_array(y, ensure_2d=False, dtype=None)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/sklearn\/utils\/validation.py\", line 63, in inner_f\r\n    return f(*args, **kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/sklearn\/utils\/validation.py\", line 560, in check_array\r\n    array = array.astype(dtype)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py\", line 5548, in astype\r\n    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors,)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/pandas\/core\/internals\/managers.py\", line 604, in astype\r\n    return self.apply(\"astype\", dtype=dtype, copy=copy, errors=errors)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/pandas\/core\/internals\/managers.py\", line 409, in apply\r\n    applied = getattr(b, f)(**kwargs)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/pandas\/core\/internals\/blocks.py\", line 595, in astype\r\n    values = astype_nansafe(vals1d, dtype, copy=True)\r\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/pandas\/core\/dtypes\/cast.py\", line 997, in astype_nansafe\r\n    return arr.astype(dtype, copy=True)\r\nValueError: could not convert string to float: 'a'\r\n```\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nFor scikit-learn >= 0.20:\r\nimport sklearn; sklearn.show_versions()\r\nFor scikit-learn < 0.20:\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\nimport imblearn; print(\"Imbalanced-Learn\", imblearn.__version__)\r\n-->\r\nSystem:\r\n    python: 3.7.8 (default, Jul  4 2020, 08:48:22)  [Clang 11.0.0 (clang-1100.0.33.17)]\r\nexecutable: \/usr\/local\/opt\/python\/bin\/python3.7\r\n   machine: Darwin-18.7.0-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n          pip: 20.0.2\r\n   setuptools: 46.0.0\r\n      sklearn: 0.24.1\r\n        numpy: 1.19.4\r\n        scipy: 1.4.1\r\n       Cython: None\r\n       pandas: 1.1.5\r\n   matplotlib: 3.2.1\r\n       joblib: 0.14.1\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","Regression","module:model_selection"],"created_at":"2021-01-22T22:40:46Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19250"},{"issue_number":163,"repository":"scikit-learn\/scikit-learn","title":"check_decision_proba_consistency fails with LinearDiscriminantAnalysis","description":"The following common check recently started to fail on a Windows CI job:\r\n\r\n```python-traceback\r\nname = 'LinearDiscriminantAnalysis'\r\nestimator_orig = LinearDiscriminantAnalysis()\r\n\r\n    @ignore_warnings(category=FutureWarning)\r\n    def check_decision_proba_consistency(name, estimator_orig):\r\n        # Check whether an estimator having both decision_function and\r\n        # predict_proba methods has outputs with perfect rank correlation.\r\n    \r\n        centers = [(2, 2), (4, 4)]\r\n        X, y = make_blobs(n_samples=100, random_state=0, n_features=4,\r\n                          centers=centers, cluster_std=1.0, shuffle=True)\r\n        X_test = np.random.randn(20, 2) + 4\r\n        estimator = clone(estimator_orig)\r\n    \r\n        if (hasattr(estimator, \"decision_function\") and\r\n                hasattr(estimator, \"predict_proba\")):\r\n    \r\n            estimator.fit(X, y)\r\n            # Since the link function from decision_function() to predict_proba()\r\n            # is sometimes not precise enough (typically expit), we round to the\r\n            # 10th decimal to avoid numerical issues.\r\n            a = estimator.predict_proba(X_test)[:, 1].round(decimals=10)\r\n            b = estimator.decision_function(X_test).round(decimals=10)\r\n>           assert_array_equal(rankdata(a), rankdata(b))\r\nE           AssertionError: \r\nE           Arrays are not equal\r\nE           \r\nE           Mismatched elements: 2 \/ 20 (10%)\r\nE           Max absolute difference: 0.5\r\nE           Max relative difference: 0.02631579\r\nE            x: array([ 7. ,  8. , 11. ,  9. , 17. , 10. ,  5. , 14. ,  6. ,  1. , 19.5,\r\nE                   4. ,  2. , 16. , 12. , 13. ,  3. , 15. , 19.5, 18. ])\r\nE            y: array([ 7.,  8., 11.,  9., 17., 10.,  5., 14.,  6.,  1., 20.,  4.,  2.,\r\nE                  16., 12., 13.,  3., 15., 19., 18.])\r\n```\r\n\r\nThis happened on this PR which should not have any impact on the behavior `LinearDiscriminantAnalysis.predict_proba`: #17743.\r\n\r\nhttps:\/\/dev.azure.com\/scikit-learn\/scikit-learn\/_build\/results?buildId=25474&view=logs&j=d32b16b6-cb9d-571b-e765-de83708fb8dd&t=b93f76c1-c2c9-579e-c2ec-c4f438af1261\r\n\r\n\r\nI suspect the test to be too brittle. Maybe using a test set more related to the original distribution (blobs) would avoid ties or caused by arbitrary rounding?","labels":["Bug","module:test-suite"],"created_at":"2021-01-21T08:39:53Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19224"},{"issue_number":164,"repository":"scikit-learn\/scikit-learn","title":"[MRG] Correct handling of missing_values and NaN in SimpleImputer for object arrays (closes #19071)","description":"#### Reference Issues\/PRs\r\n\r\nFixes #19071.\r\nIn `SimpleImputer` strategies mean\/median you can use input arrays with dtype is `object`. Imagine numeric data in the input array, except for missing values. In object arrays, you have a lot of flexibility on how to encode missing values. You can use `np.nan`, `None`, a string constant or anything else. There are two scenarios which are described in the issue #19071:\r\n\r\n1.  There is more than one type of missing value in the input object array, one of them is specified as the `missing_values` parameter of the `SimpleImputer`. In this case the mean\/median imputation should fail, because there are still non-numeric values in the input array other than `missing_values` which is masked. See #19071 for an example of not failing.\r\n2. There is just one type of missing value in the input object array and that is specified as the `missing_values` parameter of the `SimpleImputer`. In this case the mean\/median imputation must not fail and should calculate mean\/median from the remaining values of the input array. See #19071 for an example of this failing with `missing_values=None`.\r\n\r\n#### What does this implement\/fix? Explain your changes.\r\n\r\nIn the `_validate_input` method, I keep the dtype as object.\r\nIn the `_dense_fit` method I try to convert the object array into float64 to prepare for mean\/median calculation. If the not-masked values still contain some non-numeric values (scenario 1), ValueError is raised with the following informative error message:\r\n```\r\nNon-numeric values other than missing_values={missing_values}, showing {num_show}\/{num_notmasked_nan}: {examples}\r\n```\r\nwhere the number of examples is limited to three.\r\n\r\n#### Any other comments?\r\n\r\nA comprehensive set of unit tests is added, which covers both scenarios.","labels":["Bug","Stalled","help wanted","module:impute"],"created_at":"2020-12-29T19:58:11Z","comments":1,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/19079"},{"issue_number":165,"repository":"scikit-learn\/scikit-learn","title":"SimpleImputer, missing_values and None","description":"There are issues with `SimpleImputer` and None:\r\n\r\n1. None values are treated as NaNs even when the dtype is `object`. Unlike the `float` dtype, `object` can support both nans **and** None so there's no reason to treat them the same.\r\n\r\n```py\r\nimport numpy as np\r\nfrom sklearn.impute import SimpleImputer\r\n\r\na = np.arange(5).reshape(-1, 1).astype(object)\r\na[3] = None\r\n\r\n# This will not fail and impute None as if it were nan\r\nSimpleImputer(missing_values=np.nan).fit_transform(a)\r\n```\r\n\r\n2. Unlike what the doc says, `missing_values=None` is not supported. In fact, `missing_values` is expected to be of numerical dtype, not `NoneType`. The error message isn't super descriptive:\r\n\r\n```py\r\nimport numpy as np\r\nfrom sklearn.impute import SimpleImputer\r\n\r\na = np.arange(5).reshape(-1, 1).astype(object)\r\na[3] = None\r\n\r\nSimpleImputer(missing_values=None).fit_transform(a)\r\n```\r\nfails with:\r\n\r\n`ValueError: Input contains NaN, infinity or a value too large for dtype('float64').`\r\n\r\n(I haven't checked other imputers which may or may not have similar issues)","labels":["Bug","help wanted","module:impute"],"created_at":"2020-12-26T14:52:03Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19071"},{"issue_number":166,"repository":"scikit-learn\/scikit-learn","title":"BallTree query match time is O(n) not O(log(n))","description":"I've run performance analysis on matching NN with BallTree (same with KDTree), and the matching time is linear to number of elements, and should be O(log(n)). \r\n\r\nHere are the result of benchmark:\r\nnum_elements, match_time\r\n10000 0.09097146987915039\r\n20000 0.18194293975830078\r\n40000 0.3668830394744873\r\n80000 0.7527577877044678\r\n\r\nHere is my code:\r\n```\r\n\r\nfrom sklearn.neighbors import BallTree\r\nimport numpy as np \r\nimport time \r\n\r\n\r\ndef tree_perf(tree_size):\r\n    X = np.random.rand(tree_size, 512)\r\n    Y1 = np.random.rand(1, 512)\r\n    Y2 = np.random.rand(10, 512)\r\n\r\n    ts = time.time()\r\n    kdt = BallTree(X, leaf_size=30, metric='euclidean')\r\n    load_tree = time.time() - ts\r\n    num_nn = 1\r\n    ts = time.time()\r\n    vs = kdt.query(Y1, k=num_nn, return_distance=True)\r\n    match1 = time.time() - ts\r\n    ts = time.time()\r\n    vs = kdt.query(Y2, k=num_nn, return_distance=True)\r\n    match10 = time.time() - ts\r\n    print(tree_size, load_tree, match1, match10)\r\n\r\nprint(\"num_elements\", \"load_tree\", \"match_1\", \"match_10\")\r\n\r\nfor i in range(100):\r\n    tree_size = 10000 + i * 10000\r\n    tree_perf(tree_size)\r\n```","labels":["Bug","Performance","help wanted","module:neighbors"],"created_at":"2020-12-23T22:29:08Z","comments":18,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/19066"},{"issue_number":167,"repository":"scikit-learn\/scikit-learn","title":"t-SNE runs out of memory just before returning results and after finishing computation","description":"#### Describe the bug\r\n\r\nBased on the verbose output of TSNE it seems that the whole computation actually finishes fine and just when it is about to return the results it writes a large amount of data to RAM and then gets killed by the system.\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.manifold import TSNE\r\n\r\nlarge_N = 100_000  # for my system this is large enough, of course smaller N like 10_000 work\r\npca_result = np.random.normal(size=(large_N, 50))\r\ntsne = TSNE(n_components=2, verbose=10, perplexity=40, n_iter=300)\r\ntsne_pca_results = tsne.fit_transform(pca_result)\r\n```\r\n\r\n#### Expected Results\r\nRunning this should behave the same way as for smaller N like 10.000, or, run out of memory during execution, because the TSNE needs more memory to run for given N than provided by the system.\r\n\r\n#### Actual Results\r\nFollowing the verbose output and the runtime, even for the given large_N of 100.000 TSNE (which I believe uses BarnesHut implementation as default) is actually able to index the data and compute conditional probabilities. The verbose output also shows the running of 300 iterations with reduced error each time. The last line of output shows the achieved the KL divergence. At this point I believe TSNE has actually run successfully and should just return embedding matrix of shape (N,2) which is much smaller than (N,50) and should therefore have no problem to fit into memory. But when opening `htop` in a parallel terminal one can actually observe how the program writes data into the RAM within seconds, fills it completly up and then gets killed by the system. Below the output as observed on my terminal:\r\n\r\n```\r\nt-SNE] Computing 121 nearest neighbors...                                                                                                                                                  | 0\/202 [00:00<?, ?it\/s]\r\n[t-SNE] Indexed 535327 samples in 5.554s...                                                                                                                                                                        \r\n[t-SNE] Computed neighbors for 535327 samples in 57.435s...                                                                                                                                                        \r\n[t-SNE] Computed conditional probabilities for sample 1000 \/ 535327                                                                                                                                                \r\n[t-SNE] Computed conditional probabilities for sample 2000 \/ 535327                                                                                                                                                \r\n[t-SNE] Computed conditional probabilities for sample 3000 \/ 535327                                                                                                                                                \r\n[t-SNE] Computed conditional probabilities for sample 4000 \/ 535327                                                                                                                                                \r\n[t-SNE] Computed conditional probabilities for sample 5000 \/ 535327                                                                                                                                                \r\n[t-SNE] Computed conditional probabilities for sample 6000 \/ 535327                                                                                                                                                \r\n[t-SNE] Computed conditional probabilities for sample 7000 \/ 535327                                                                                                                                                \r\n[t-SNE] Computed conditional probabilities for sample 8000 \/ 535327\r\n...\r\n[t-SNE] Computed conditional probabilities for sample 534000 \/ 535327\r\n[t-SNE] Computed conditional probabilities for sample 535000 \/ 535327\r\n[t-SNE] Computed conditional probabilities for sample 535327 \/ 535327\r\n[t-SNE] Mean sigma: 0.000000\r\n[t-SNE] Computed conditional probabilities in 70.161s\r\n[t-SNE] Iteration 50: error = 123.6066971, gradient norm = 0.0000000 (50 iterations in 89.115s)\r\n[t-SNE] Iteration 50: gradient norm 0.000000. Finished.\r\n[t-SNE] KL divergence after 50 iterations with early exaggeration: 123.606697\r\n[t-SNE] Iteration 100: error = 7.7342658, gradient norm = 0.0000001 (50 iterations in 101.858s)\r\n[t-SNE] Iteration 150: error = 7.7342658, gradient norm = 0.0000067 (50 iterations in 110.148s)\r\n[t-SNE] Iteration 200: error = 7.7170248, gradient norm = 0.0012155 (50 iterations in 119.877s)\r\n[t-SNE] Iteration 250: error = 6.7275314, gradient norm = 0.0006462 (50 iterations in 91.746s)\r\n[t-SNE] Iteration 300: error = 4.7438197, gradient norm = 0.0004072 (50 iterations in 57.282s)\r\n[t-SNE] KL divergence after 300 iterations: 4.743820\r\nKilled\r\n```\r\n\r\n#### Versions\r\n\r\nOutput of `import sklearn; sklearn.show_versions()`:\r\n```\r\nSystem:\r\n    python: 3.6.9 (default, Apr 18 2020, 01:56:04)  [GCC 8.4.0]\r\nexecutable: \/lhome\/davidj2\/code\/sync\/pointset_attention_net\/.venv\/bin\/python3.6\r\n   machine: Linux-4.15.0-108-generic-x86_64-with-Ubuntu-18.04-bionic\r\n\r\nPython dependencies:\r\n          pip: 20.2.4\r\n   setuptools: 50.3.2\r\n      sklearn: 0.23.2\r\n        numpy: 1.19.4\r\n        scipy: 1.5.3\r\n       Cython: 0.29.21\r\n       pandas: 1.1.4\r\n   matplotlib: 3.3.3\r\n       joblib: 0.17.0\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n```","labels":["Bug","module:manifold"],"created_at":"2020-12-04T10:50:12Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18966"},{"issue_number":168,"repository":"scikit-learn\/scikit-learn","title":".fit.transform != .fit_transform inconsistency in PCA results","description":"PCA's fit_transform returns different results than the application of fit and transform methods individually. A piece of code that shows the inconsistency is given below.\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.decomposition import PCA\r\n\r\nnn = np.array([[0,1,2],[3,4,5],[6,7,8]])\r\npca = PCA(n_components=2, random_state=42)\r\nprint(pca.fit_transform(nn))\r\n\r\nnn = np.array([[0,1,2],[3,4,5],[6,7,8]])\r\npca = PCA(n_components=2, random_state=42)\r\npca.fit(nn)\r\nprint(pca.transform(nn))\r\n```\r\n\r\nPlease run the code with sklearn 0.23.2\r\n\r\n","labels":["Bug","module:decomposition"],"created_at":"2020-11-29T21:50:16Z","comments":12,"reactions":2,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18941"},{"issue_number":169,"repository":"scikit-learn\/scikit-learn","title":"MDS implementation has error in smacof algorithm for non-metric case","description":"Dear Scikit team,\r\nI really enjoy using the scikit learn library and I am very greatful for your very impactful work. I hope I can contribute with this bug report (if it is true) to this library.\r\nKeep on your good work!\r\n\r\nI checked existing issues that concern MDS it is not the same as #16846 but to solve it I would also address #11381\r\n\r\n#### Describe the bug\r\nEverything concerns the MDS implementation in `scikit-learn\/sklearn\/manifold\/_mds.py`. I have copied the relevant snipped of code below and added the line numbers I am referring to. All equations I am referring to are from [1]\r\n\r\n__Short version:__\r\n- The initialization of the disparities in [line 103](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/manifold\/_mds.py#L103) leads to an error when calculating the stress and the update of the configuration.\r\n- The standardization in [line 106](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/manifold\/_mds.py#L106) is wrong.\r\n- The library can not reproduce results from the [r library mass](https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/MASS\/html\/isoMDS.html)\r\n\r\n__Long version:__\r\nI found the following error in the implementation:\r\n- In `scikit-learn.sklearn.manifold._mds._smacof_single` in [line 103](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/manifold\/_mds.py#L103) the disparities get initialized with the distances. In [line 104](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/manifold\/_mds.py#L104) only the values of the upper triangle matrix get overwritten by the calculated disparities (`disparities_flat`). This results in a matrix that has disparities (similar to rank distances) in the upper triangle and euclidean distances in the lower triangle. Therefore the normalization factor in [line 106](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/manifold\/_mds.py#L106) is calculated based on a matrix that is half distances and half disparities\r\n- Line 106 standardized the disparities to \r\n![grafik](https:\/\/user-images.githubusercontent.com\/31069329\/100464473-f6160800-30cd-11eb-9118-94e754c2cc24.png)\r\nwhich corresponds to step 3 and 9 of the `Smacof algorithm with Admissibly Transformed Proximities`[2]. However, from equation 9.1 it seems like this corresponds only to the upper triangle matrix (which explains the `n(n-1)\/2`) in the implementation of line [106](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/manifold\/_mds.py#L106), all elements from the matrix are used which results in a wrong standardization factor.\r\n- Also [line 106](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/manifold\/_mds.py#L106) the term ```(disparities ** 2).sum()``` should not be in the square-root. This results from a transformation when we are looking for a normalization factor `a` that is applied to the disparities such that:  \r\n![grafik](https:\/\/user-images.githubusercontent.com\/31069329\/100475051-64fd5c00-30e2-11eb-87b5-45bdeb3e723f.png)\r\nWe then should calculate `a` as \r\n![grafik](https:\/\/user-images.githubusercontent.com\/31069329\/100475075-7b0b1c80-30e2-11eb-9e4a-b9e8b97b3f50.png)\r\n(equations 9.1 + 9.2) from [1]\r\n\r\n- When the stress is computed in [line 110](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/manifold\/_mds.py#L110), a difference between the dissimilarities and the disparities is computed. As stated above, the disparities had the dissimilarities in the lower triangle matrix (which I think is confusing) which would normally mean that the all differences in the lower triangle matrix are zero. However, as the disparities got scaled in [line 106](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/manifold\/_mds.py#L106), the differences in the lower triangle matrix are no longer zero but some (more or less) random values. \r\n\r\n\r\nThere are also some smaller issues that I noticed:\r\n- After the last update of the configuration, the stress is not updated. This means the old stress is returned (I think this is the same as #16846)\r\n- I think it would be good to return the normalized stress \r\n\r\n__References__\r\n[1]:  \u201cModern Multidimensional Scaling - Theory and Applications\u201d Borg, I.; Groenen P. Springer Series in Statistics (1997), Version from 2005\r\n[2]: \u201cModern Multidimensional Scaling - Theory and Applications\u201d Borg, I.; Groenen P. Springer Series in Statistics (1997), Version from 2005, Page 204 in Chapter _9. Metric and non-metric MDS_\r\n\r\nCode from [scikit MDS implementation](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/manifold\/_mds.py#L89) (for reference)\r\n```python\r\nir = IsotonicRegression()\r\n    for it in range(max_iter):\r\n        # Compute distance and monotonic regression\r\n        dis = euclidean_distances(X)\r\n\r\n        if metric:\r\n            disparities = dissimilarities\r\n        else:\r\n            dis_flat = dis.ravel()\r\n            # dissimilarities with 0 are considered as missing values\r\n            dis_flat_w = dis_flat[sim_flat != 0]\r\n\r\n            # Compute the disparities using a monotonic regression\r\n            disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)\r\n[103]    disparities = dis_flat.copy()\r\n[104]    disparities[sim_flat != 0] = disparities_flat\r\n[105]    disparities = disparities.reshape((n_samples, n_samples))\r\n[106]    disparities *= np.sqrt((n_samples * (n_samples - 1) \/ 2) \/\r\n                                   (disparities ** 2).sum())\r\n\r\n        # Compute stress\r\n[110]    stress = ((dis.ravel() - disparities.ravel()) ** 2).sum() \/ 2\r\n\r\n```\r\n\r\n#### Steps\/Code to Reproduce\r\nThe code and data to calculate the results is in this gist: (https:\/\/gist.github.com\/henrymartin1\/05a2aa5210e52216abd7ce61e6918e3a). The relevant file is`scikit_mds_bugreport.py`\r\n\r\nI honestly had a hard time to come up with code that proofs that there is a problem. This is what I did: \r\n- I created an array and its distance matrix in python and stored it as .csv\r\n- Using this data, I calculated the optimal configuration and the stress using the non-metric MDS implementation of the [r-library mass](https:\/\/www.rdocumentation.org\/packages\/MASS\/versions\/7.3-53\/topics\/isoMDS)\r\n- I calculate the optimal configuration and the stress using scikit-learns mds\r\n- I calculate the non-metric stress using my own implementation of non-metric stress\r\n- I plot the optimal scikit configuration and the optimal r configuration\r\n\r\n#### Expected Results\r\n- The two optimal configurations would be the same or very similar\r\n- The stress reported by r and the stress reported by scikit are the same\r\n- The configurations found by scikit and by r have the same stress when I calculate it using my own implementation of the stress function. (+ they match with what is reported by scikit and the r implementation)\r\n\r\n#### Actual Results\r\n- The optimal scikit configuration and the optimal r configuration are significantly different (see plots below)\r\n- My calculated stress for the r configuration and the scikit configuration are different\r\n- The scikit stress is not normalized (not necessarily a problem)\r\n##### Result mds scikit \r\n![grafik](https:\/\/user-images.githubusercontent.com\/31069329\/100474874-f8825d00-30e1-11eb-920d-c568ffe6e93c.png)\r\n##### Result mds r\r\n![grafik](https:\/\/user-images.githubusercontent.com\/31069329\/100474890-020bc500-30e2-11eb-8914-3ae4e61a486e.png)\r\n\r\n\r\n#### Versions\r\nSystem:\r\n    python: 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 18:22:52) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\Users\\henry\\.conda\\envs\\evhomepv\\python.exe\r\n   machine: Windows-10-10.0.19041-SP0\r\n\r\nPython dependencies:\r\n          pip: 20.2.4\r\n   setuptools: 49.6.0.post20201009\r\n      sklearn: 0.23.2\r\n        numpy: 1.19.4\r\n        scipy: 1.5.3\r\n       Cython: None\r\n       pandas: 1.1.4\r\n   matplotlib: 3.3.2\r\n       joblib: 0.17.0\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","module:manifold"],"created_at":"2020-11-27T18:11:35Z","comments":7,"reactions":3,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18933"},{"issue_number":170,"repository":"scikit-learn\/scikit-learn","title":"Duplicate check_finite when calling scipy.linalg functions","description":"Most functions in `scipy.linalg` functions (e.g. `svd`, `qr`, `eig`, `eigh`, `pinv`, `pinv2` ...) have a default kwarg `check_finite=True` that we  typically leave to the default value in scikit-learn.\r\n\r\nAs we already validate the input data for most estimators in scikit-learn, this check is redundant and can cause significant overhead, especially at predict \/ transform time. We should probably always call those method with an explicit `check_finite=False` in scikit-learn.\r\n\r\nThis issue shall probably be addressed in many PRs, probably one per module that imports  `scipy.linalg`.\r\n\r\nWe should still make sure that the estimators raise a `ValueError` with the expected error message when fed with numpy arrays with infinite some values (`-np.inf`, `np.inf` or `np.nan`). This can be done manually by calling `sklearn.utils.estimator_checks.check_estimators_nan_inf` on the estimator, which should be automatically be called by `sklearn.tests.test_common` but we need to check that it's actually the case when reviewing such PRs.","labels":["Bug","Moderate","Performance"],"created_at":"2020-11-13T17:35:09Z","comments":34,"reactions":2,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18837"},{"issue_number":171,"repository":"scikit-learn\/scikit-learn","title":"Inconsistency between Gaussian Process Regressor vs Classifier causing LinAlgError","description":"<!--\r\nBefore submitting a bug, please make sure the issue hasn't been already\r\naddressed by searching through the past issues.\r\n-->\r\n\r\n#### Describe the bug\r\nAs my current dataset was encountering with LinAlgErrors ('n-th leading minor of the array is not positive definite') while being fitted with GaussianProcessClassifier, I delved into the code a little bit and saw that the classifier, contrary to its regressor counterpart, does not contain an alpha parameter to ensure numerical stability. What is missing is basically one line, namely [this one](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/gaussian_process\/_gpr.py#L263) possibly just under [this one](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/gaussian_process\/_gpc.py#L414). \r\n\r\nFor a better understanding of the requirement here, see [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) and [here](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/0fb307bf3\/sklearn\/gaussian_process\/_gpr.py#L268).\r\n\r\n#### Steps\/Code to Reproduce\r\nThis is unfortunately not so easy, but one can try to reproduce this by using a dataset with feature count ~ sample count.\r\n\r\n#### Versions\r\nscikit-learn: 0.23.2\r\n\r\n","labels":["Bug","module:gaussian_process"],"created_at":"2020-10-25T17:28:16Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18683"},{"issue_number":172,"repository":"scikit-learn\/scikit-learn","title":"LinearDiscriminantAnalysis does not handle degenerate problems well","description":"As investigated when trying to solve unstable common tests in #18667, it seems that `LinearDiscriminantAnalysis` is bad a handling classification problems that are too trivial such as illustrated in the following (using MKL):\r\n\r\n```python\r\n>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\n>>> LinearDiscriminantAnalysis(solver=\"svd\").fit([[0], [1], [1]], [0, 1, 1])\r\nTraceback (most recent call last):\r\n  File \"<ipython-input-6-034ea174ac0e>\", line 1, in <module>\r\n    LinearDiscriminantAnalysis(solver=\"svd\").fit([[0], [1], [1]], [0, 1, 1])\r\n  File \"\/home\/ogrisel\/code\/scikit-learn\/sklearn\/discriminant_analysis.py\", line 553, in fit\r\n    self._solve_svd(X, y)\r\n  File \"\/home\/ogrisel\/code\/scikit-learn\/sklearn\/discriminant_analysis.py\", line 478, in _solve_svd\r\n    _, S, Vt = linalg.svd(X, full_matrices=0)\r\n  File \"\/home\/ogrisel\/miniforge3\/envs\/dev\/lib\/python3.8\/site-packages\/scipy\/linalg\/decomp_svd.py\", line 121, in svd\r\n    lwork = _compute_lwork(gesXd_lwork, a1.shape[0], a1.shape[1],\r\n  File \"\/home\/ogrisel\/miniforge3\/envs\/dev\/lib\/python3.8\/site-packages\/scipy\/linalg\/lapack.py\", line 950, in _compute_lwork\r\n    ret = routine(*args, **kwargs)\r\nValueError: On entry to DGESDD parameter number 10 had an illegal value\r\n\r\n>>> LinearDiscriminantAnalysis(solver=\"eigen\").fit([[0], [1], [1]], [0, 1, 1])\r\n\/home\/ogrisel\/code\/scikit-learn\/sklearn\/covariance\/_empirical_covariance.py:88: UserWarning: Only one sample available. You may want to reshape your data array\r\n  warnings.warn(\"Only one sample available. \"\r\nTraceback (most recent call last):\r\n  File \"<ipython-input-7-57ea859bdde2>\", line 1, in <module>\r\n    LinearDiscriminantAnalysis(solver=\"eigen\").fit([[0], [1], [1]], [0, 1, 1])\r\n  File \"\/home\/ogrisel\/code\/scikit-learn\/sklearn\/discriminant_analysis.py\", line 558, in fit\r\n    self._solve_eigen(X, y,\r\n  File \"\/home\/ogrisel\/code\/scikit-learn\/sklearn\/discriminant_analysis.py\", line 419, in _solve_eigen\r\n    evals, evecs = linalg.eigh(Sb, Sw)\r\n  File \"\/home\/ogrisel\/miniforge3\/envs\/dev\/lib\/python3.8\/site-packages\/scipy\/linalg\/decomp.py\", line 578, in eigh\r\n    raise LinAlgError('The leading minor of order {} of B is not '\r\nLinAlgError: The leading minor of order 1 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.\r\n\r\n>>> lda = LinearDiscriminantAnalysis(solver=\"lsqr\").fit([[0], [1], [1]], [0, 1, 1])\r\n\/home\/ogrisel\/code\/scikit-learn\/sklearn\/covariance\/_empirical_covariance.py:88: UserWarning: Only one sample available. You may want to reshape your data array\r\n  warnings.warn(\"Only one sample available. \"\r\nLinearDiscriminantAnalysis(solver='lsqr')\r\n```\r\n\r\nThe least squares solver does not fail (but still raises a warning) and the resulting model is unable to classifiy the training set correctly:\r\n\r\n```python\r\n>>> lda.predict([[0], [1]])\r\narray([1, 1])\r\n```\r\n\r\nThe above was done with scipy 1.5.2 with OpenBLAS from conda-forge under Linux.\r\n\r\nI am not sure what we could do about it. Maybe we could at least try to raise the same exception for all the solvers when the problem is degenerate. The original exception of the eigen cause could still be raised as the cause of the user friendly exception (`raise e1 from e2` in Python 3). For the SVD case this happens when the estimated rank is 0.","labels":["Bug"],"created_at":"2020-10-22T09:21:05Z","comments":3,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18671"},{"issue_number":173,"repository":"scikit-learn\/scikit-learn","title":"fit.transform not equal to fit_transform in NMF with solver 'mu' and init 'nndsvda'","description":"#### Describe the bug\r\nThe test verifying that `fit.transform == fit_transform` failed for the `NMF` estimator with `solver='mu'` and init 'nndsvda'\r\n\r\nDiscovered in #16948, see also #18505.\r\n\r\ncc @jeremiedbb @TomDLT , maybe @vene . Thanks!\r\n\r\n#### Steps\/Code to Reproduce\r\n```python\r\nfrom sklearn.utils.estimator_checks import check_transformer_general\r\nfrom sklearn.decomposition import NMF\r\n\r\ncheck_transformer_general('NMF', NMF(init='nndsvda', max_iter=1000, solver='mu'))\r\n```\r\n\r\n#### Expected Results\r\nThe test passes.\r\n\r\n#### Actual Results\r\n```pytr\r\nTraceback (most recent call last):\r\n  File \"debug_nmf.py\", line 4, in <module>\r\n    check_transformer_general(\r\n  File \"\/home\/cmarmo\/software\/scikit-learn\/sklearn\/utils\/_testing.py\", line 302, in wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"\/home\/cmarmo\/software\/scikit-learn\/sklearn\/utils\/estimator_checks.py\", line 1305, in check_transformer_general\r\n    _check_transformer(name, transformer, X, y)\r\n  File \"\/home\/cmarmo\/software\/scikit-learn\/sklearn\/utils\/estimator_checks.py\", line 1390, in _check_transformer\r\n    assert_allclose_dense_sparse(\r\n  File \"\/home\/cmarmo\/software\/scikit-learn\/sklearn\/utils\/_testing.py\", line 409, in assert_allclose_dense_sparse\r\n    assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)\r\n  File \"\/home\/cmarmo\/.skldevenv\/lib64\/python3.8\/site-packages\/numpy\/testing\/_private\/utils.py\", line 1532, in assert_allclose\r\n    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),\r\n  File \"\/home\/cmarmo\/.skldevenv\/lib64\/python3.8\/site-packages\/numpy\/testing\/_private\/utils.py\", line 846, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=1e-07, atol=0.01\r\nfit_transform and transform outcomes not consistent in NMF(init='nndsvda', max_iter=1000, random_state=0, solver='mu')\r\nMismatched elements: 18 \/ 90 (20%)\r\nMax absolute difference: 0.04219191\r\nMax relative difference: 1.\r\n x: array([[5.584139e-01, 5.114790e-01, 4.818158e-01],\r\n       [7.176271e-02, 9.486441e-02, 2.635424e-01],\r\n       [1.848232e-01, 1.241254e-01, 9.756835e-22],...\r\n y: array([[5.566442e-01, 5.129683e-01, 4.835650e-01],\r\n       [7.235885e-02, 9.501380e-02, 2.622291e-01],\r\n       [1.846617e-01, 1.244062e-01, 1.105227e-07],...\r\n```\r\n\r\n#### Versions\r\n```\r\nSystem:\r\n    python: 3.8.5 (default, Aug 12 2020, 00:00:00)  [GCC 10.2.1 20200723 (Red Hat 10.2.1-1)]\r\nexecutable: \/home\/cmarmo\/.skldevenv\/bin\/python\r\n   machine: Linux-5.8.13-200.fc32.x86_64-x86_64-with-glibc2.2.5\r\n\r\nPython dependencies:\r\n          pip: 20.2.3\r\n   setuptools: 41.6.0\r\n      sklearn: 0.24.dev0\r\n        numpy: 1.18.5\r\n        scipy: 1.5.2\r\n       Cython: 0.29.21\r\n       pandas: 1.1.0\r\n   matplotlib: 3.3.0\r\n       joblib: 0.16.0\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n```","labels":["Bug","module:decomposition"],"created_at":"2020-10-21T09:41:07Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18663"},{"issue_number":174,"repository":"scikit-learn\/scikit-learn","title":"Bug: VotingClassifier does not support estimators using class_weight","description":"When using `VotingClassifier`, the estimators used inside cannot use the  `class_weight` parameter. The reason for that, I believe, is that `VotingClassifier` is encoding the labels before passing them to the single estimators, but not encoding the labels in `class_weight`. This throws a `ValueError: Class label <> not present`.\r\n\r\nAs a matter of fact, I wonder why this encoding would even happen in the first place, as it is handled by each model individually.\r\n\r\nHere is a reproducible example:\r\n\r\n```\r\nfrom sklearn.ensemble import VotingClassifier\r\nfrom sklearn.linear_model import LogisticRegression\r\nimport numpy as np\r\n\r\nX = np.array([[1,2], [1,3], [2,1], [2,3], [2,3]])\r\ny = np.array([\"a\", \"b\", \"a\", \"b\", \"a\"])\r\n\r\nLR = LogisticRegression(class_weight={\"a\":1, \"b\": 2})\r\nVC = VotingClassifier([(\"LR\", LR)])\r\n\r\nVC.fit(X, y)\r\n```\r\n\r\nTraceback:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-1-3ae1f62ef011> in <module>\r\n      9 VC = VotingClassifier([(\"LR\", LR)])\r\n     10 \r\n---> 11 VC.fit(X, y)\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/sklearn\/ensemble\/_voting.py in fit(self, X, y, sample_weight)\r\n    220         transformed_y = self.le_.transform(y)\r\n    221 \r\n--> 222         return super().fit(X, transformed_y, sample_weight)\r\n    223 \r\n    224     def predict(self, X):\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/sklearn\/ensemble\/_voting.py in fit(self, X, y, sample_weight)\r\n     66                 delayed(_parallel_fit_estimator)(clone(clf), X, y,\r\n     67                                                  sample_weight=sample_weight)\r\n---> 68                 for clf in clfs if clf not in (None, 'drop')\r\n     69             )\r\n     70 \r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/parallel.py in __call__(self, iterable)\r\n   1002             # remaining jobs.\r\n   1003             self._iterating = False\r\n-> 1004             if self.dispatch_one_batch(iterator):\r\n   1005                 self._iterating = self._original_iterator is not None\r\n   1006 \r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/parallel.py in dispatch_one_batch(self, iterator)\r\n    833                 return False\r\n    834             else:\r\n--> 835                 self._dispatch(tasks)\r\n    836                 return True\r\n    837 \r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/parallel.py in _dispatch(self, batch)\r\n    752         with self._lock:\r\n    753             job_idx = len(self._jobs)\r\n--> 754             job = self._backend.apply_async(batch, callback=cb)\r\n    755             # A job can complete so quickly than its callback is\r\n    756             # called before we get here, causing self._jobs to\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/_parallel_backends.py in apply_async(self, func, callback)\r\n    207     def apply_async(self, func, callback=None):\r\n    208         \"\"\"Schedule a func to be run\"\"\"\r\n--> 209         result = ImmediateResult(func)\r\n    210         if callback:\r\n    211             callback(result)\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/_parallel_backends.py in __init__(self, batch)\r\n    588         # Don't delay the application, to avoid keeping the input\r\n    589         # arguments in memory\r\n--> 590         self.results = batch()\r\n    591 \r\n    592     def get(self):\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/parallel.py in __call__(self)\r\n    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):\r\n    255             return [func(*args, **kwargs)\r\n--> 256                     for func, args, kwargs in self.items]\r\n    257 \r\n    258     def __len__(self):\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/parallel.py in <listcomp>(.0)\r\n    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):\r\n    255             return [func(*args, **kwargs)\r\n--> 256                     for func, args, kwargs in self.items]\r\n    257 \r\n    258     def __len__(self):\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/sklearn\/ensemble\/_base.py in _parallel_fit_estimator(estimator, X, y, sample_weight)\r\n     34             raise\r\n     35     else:\r\n---> 36         estimator.fit(X, y)\r\n     37     return estimator\r\n     38 \r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/sklearn\/linear_model\/_logistic.py in fit(self, X, y, sample_weight)\r\n   1599                       penalty=penalty, max_squared_sum=max_squared_sum,\r\n   1600                       sample_weight=sample_weight)\r\n-> 1601             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\r\n   1602 \r\n   1603         fold_coefs_, _, n_iter_ = zip(*fold_coefs_)\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/parallel.py in __call__(self, iterable)\r\n   1002             # remaining jobs.\r\n   1003             self._iterating = False\r\n-> 1004             if self.dispatch_one_batch(iterator):\r\n   1005                 self._iterating = self._original_iterator is not None\r\n   1006 \r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/parallel.py in dispatch_one_batch(self, iterator)\r\n    833                 return False\r\n    834             else:\r\n--> 835                 self._dispatch(tasks)\r\n    836                 return True\r\n    837 \r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/parallel.py in _dispatch(self, batch)\r\n    752         with self._lock:\r\n    753             job_idx = len(self._jobs)\r\n--> 754             job = self._backend.apply_async(batch, callback=cb)\r\n    755             # A job can complete so quickly than its callback is\r\n    756             # called before we get here, causing self._jobs to\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/_parallel_backends.py in apply_async(self, func, callback)\r\n    207     def apply_async(self, func, callback=None):\r\n    208         \"\"\"Schedule a func to be run\"\"\"\r\n--> 209         result = ImmediateResult(func)\r\n    210         if callback:\r\n    211             callback(result)\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/_parallel_backends.py in __init__(self, batch)\r\n    588         # Don't delay the application, to avoid keeping the input\r\n    589         # arguments in memory\r\n--> 590         self.results = batch()\r\n    591 \r\n    592     def get(self):\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/parallel.py in __call__(self)\r\n    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):\r\n    255             return [func(*args, **kwargs)\r\n--> 256                     for func, args, kwargs in self.items]\r\n    257 \r\n    258     def __len__(self):\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/joblib\/parallel.py in <listcomp>(.0)\r\n    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):\r\n    255             return [func(*args, **kwargs)\r\n--> 256                     for func, args, kwargs in self.items]\r\n    257 \r\n    258     def __len__(self):\r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/sklearn\/linear_model\/_logistic.py in _logistic_regression_path(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\r\n    841     le = LabelEncoder()\r\n    842     if isinstance(class_weight, dict) or multi_class == 'multinomial':\r\n--> 843         class_weight_ = compute_class_weight(class_weight, classes, y)\r\n    844         sample_weight *= class_weight_[le.fit_transform(y)]\r\n    845 \r\n\r\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/sklearn\/utils\/class_weight.py in compute_class_weight(class_weight, classes, y)\r\n     63             i = np.searchsorted(classes, c)\r\n     64             if i >= len(classes) or classes[i] != c:\r\n---> 65                 raise ValueError(\"Class label {} not present.\".format(c))\r\n     66             else:\r\n     67                 weight[i] = class_weight[c]\r\n\r\nValueError: Class label a not present.\r\n```\r\n\r\nThis issue was noticed [here](https:\/\/stackoverflow.com\/questions\/60992466\/using-class-weights-with-sklearn-votingclassifier#comment108421729_60992466).\r\n\r\nSystem:\r\n    python: 3.7.6 \r\n   machine: macOS\r\n\r\n   sklearn: 0.22.1\r\n     numpy: 1.18.1\r\n     scipy: 1.4.1","labels":["Bug","module:ensemble"],"created_at":"2020-10-06T20:20:24Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18550"},{"issue_number":175,"repository":"scikit-learn\/scikit-learn","title":"Error on using None for missing values in SimpleImputer with boolean arrays","description":"```python\r\nfrom sklearn.impute import SimpleImputer\r\nimport pandas as pd\r\nX2 = pd.DataFrame({'a': [True, False, True, False, None]})\r\nSimpleImputer(strategy='most_frequent').fit_transform(X2)\r\n```\r\n\r\n> TypeError: '<' not supported between instances of 'NoneType' and 'bool'\r\n\r\nInterestingly this works when the remaining types are float, or when using ``np.NaN`` for the missing value.\r\n\r\nPotentially related to #17625, where I suggested treating ``None`` as ``np.NaN``. \r\n\r\nFYI ``X2.a.unique()`` gives the expected result, ``np.unique(X2.a)`` errors. I'm not sure if using the  ``__array_function__`` protocol might help us here, we might be casting to numpy arrays before hitting the unique.","labels":["Bug","module:impute"],"created_at":"2020-09-25T16:30:47Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18461"},{"issue_number":176,"repository":"scikit-learn\/scikit-learn","title":"feature_importance causes a BSOD on Windows 10","description":"\r\n#### Describe the bug\r\nRunning permutation_importance on a medium-sized data set results in a BSOD on Windows 10. The dataset is 470605 x 332, code is running in a Jupyter notebook, Python version 3.7.6, scikit version 0.22.1.\r\nThe BSOD is a KERNEL_SECURITY_CHECK_FAILURE, with ERROR_CODE: `(NTSTATUS) 0xc0000409 - The system detected an overrun of a stack-based buffer in this application. This overrun could potentially allow a malicious user to gain control of this application.`\r\nThe machine has a Ryzen 5 3600 with 16GB of RAM.\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.inspection import permutation_importance\r\nrf = RandomForestClassifier(n_estimators = 250,\r\n                           n_jobs = -1,\r\n                           oob_score = True,\r\n                           bootstrap = True,\r\n                           random_state = 42)\r\nrf.fit(X_train, y_train)\r\npermImp = permutation_importance(rf,\r\n                                 X_val,\r\n                                 y_val,\r\n                                 scoring='f1',\r\n                                 n_repeats=5,\r\n                                 n_jobs=-1,\r\n                                 random_state=42)\r\n```\r\n\r\n#### Expected Results\r\nNo BSOD, permutation importance computed.\r\n\r\n#### Actual Results\r\nBSOD after ~1-2 minutes\r\n\r\n#### Versions\r\n> sklearn.show_versions()\r\n\r\nSystem:\r\n    python: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\Users\\lucag\\anaconda3\\python.exe\r\n   machine: Windows-10-10.0.18362-SP0\r\n\r\nPython dependencies:\r\n       pip: 20.0.2\r\nsetuptools: 45.2.0.post20200210\r\n   sklearn: 0.22.1\r\n     numpy: 1.18.1\r\n     scipy: 1.4.1\r\n    Cython: 0.29.15\r\n    pandas: 1.0.1\r\nmatplotlib: 3.1.3\r\n    joblib: 0.14.1\r\n\r\nBuilt with OpenMP: True\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","Large Scale","OS:Windows"],"created_at":"2020-08-18T16:15:05Z","comments":23,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18187"},{"issue_number":177,"repository":"scikit-learn\/scikit-learn","title":"SpectralClustering with assign_labels='discretize' returns less clusters than specified in n_clusters","description":"I'm not sure if this is a bug or an undocumented feature (or at least I couldn't find any info about it).\r\n\r\nUsing sklearn version 0.22.1.\r\n\r\nThe following code is producing the behavior mentioned in the title:\r\n\r\n```python\r\nclustering = SpectralClustering(n_clusters=4, affinity='rbf', assign_labels='discretize', random_state=0).fit(X)\r\nnp.unique(clustering.labels_) shows just 3 labels. \r\n```\r\n\r\nX is a np.array of shape (258, 257).\r\nThis happens only with some random seeds. I was not able to reproduce it with assign_labels='kmeans' (running multiple times).\r\nAlso the labels of the clusters are not consecutive as if less than n_clusters where picked out of the n_components.","labels":["Bug","module:cluster"],"created_at":"2020-08-04T10:04:28Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18083"},{"issue_number":178,"repository":"scikit-learn\/scikit-learn","title":"Unexpected behavior when passing multiple parameter sets to RandomizedSearchCV","description":"#### Describe the bug\r\n\r\nHere is part of the documentation for the `param_distributions` parameter of [RandomizedSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html):\r\n\r\n> If a list of dicts is given, first a dict is sampled uniformly, and then a parameter is sampled using that dict as above.\r\n\r\nMy interpretation is that if I pass a list of two dictionaries, then at each iteration:\r\n\r\n- First, one of the two dictionaries will be chosen at random\r\n- Then, a set of parameters within that dictionary will be chosen at random\r\n\r\nI have found that that is **not** the case. Instead, I have found that if one of the two dictionaries has many more possible parameter combinations, then the larger dictionary will usually be chosen at each iteration.\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.model_selection import RandomizedSearchCV\r\n\r\nX, y = load_iris(return_X_y=True)\r\nrf = RandomForestClassifier()\r\n\r\n# 30 possible combinations\r\nparams1 = {}\r\nparams1['n_estimators'] = [10, 20, 30, 40, 50]\r\nparams1['min_samples_leaf'] = [1, 2, 3, 4, 5, 6]\r\n\r\n# 120 possible combinations\r\nparams2 = {}\r\nparams2['n_estimators'] = [60, 70, 80]\r\nparams2['min_samples_leaf'] = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\nparams2['max_features'] = ['auto', None]\r\nparams2['bootstrap'] = [True, False]\r\n\r\nparams_both = [params1, params2]\r\n\r\nrand = RandomizedSearchCV(rf, params_both, cv=5, scoring='accuracy', n_iter=50, random_state=1)\r\nrand.fit(X, y)\r\nprint(sorted(rand.cv_results_['param_n_estimators']))\r\n```\r\n\r\n#### Expected Results\r\n\r\nSince `n_iter=50`, I would expect that `params1` and `params2` would each be chosen about 25 times.\r\n\r\n#### Actual Results\r\n\r\nHere is the actual output of the last line:\r\n\r\n```\r\n[10, 20, 30, 40, 40, 50, 50, 50, 50, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 70, 70, 70, 70, 70, 70, 70, 70, 70, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\r\n```\r\n\r\nAs you can see, `params1` (which had values 10 through 50) was chosen only 9 times, and whereas `params2` (which had values 60 through 80) was chosen 41 times.\r\n\r\n#### Comments\r\n\r\nThere seem to be two possibilities:\r\n\r\n1. The current behavior of `RandomizedSearchCV` **is** the desired behavior. In that case, I would propose tweaking the documentation to make this behavior more clear.\r\n2. The current behavior of `RandomizedSearchCV` **is not** the desired behavior. In that case, I would propose fixing the behavior so that it matches the documentation.\r\n\r\nI don't have a strong feeling about which behavior is the \"optimal\" behavior.\r\n\r\nThis was implemented in #14549 by @amueller, so he may have some insight on this!\r\n\r\n#### Versions\r\n\r\n```\r\nSystem:\r\n    python: 3.8.2 | packaged by conda-forge | (default, Apr 24 2020, 07:56:27)  [Clang 9.0.1 ]\r\nexecutable: \/Users\/kevin\/miniconda3\/envs\/sk23\/bin\/python\r\n   machine: macOS-10.14.6-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n          pip: 20.1.1\r\n   setuptools: 46.4.0.post20200518\r\n      sklearn: 0.23.1\r\n        numpy: 1.18.4\r\n        scipy: 1.4.1\r\n       Cython: None\r\n       pandas: 1.0.3\r\n   matplotlib: 3.2.1\r\n       joblib: 0.15.1\r\nthreadpoolctl: 2.0.0\r\n\r\nBuilt with OpenMP: True\r\n```\r\n","labels":["Bug","Moderate","help wanted","module:model_selection"],"created_at":"2020-08-01T19:23:24Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/18057"},{"issue_number":179,"repository":"scikit-learn\/scikit-learn","title":"predict_proba() for linear models with log loss can return NaNs with large model coefficients","description":"<!--\r\nBefore submitting a bug, please make sure the issue hasn't been already\r\naddressed by searching through the past issues.\r\n-->\r\n\r\n#### Describe the bug\r\nThe predict_proba() function can return NaNs for linear models with log loss if\r\nthe training has resulted in a model with large coefficients. The linear equation\r\ncan result in decision_function() returning large negative values for every class\r\nfor a single input. This means that the normalization in predict_proba_lr()\r\n(line 327 in https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/linear_model\/_base.py )\r\ndivides by zero, resulting in NaNs being returned.\r\n\r\nI stumbled across this problem when using SGDClassifier with log loss and default\r\nalpha. I wanted to train the model using minibatches and monitor the loss after\r\neach batch (using metrics.log_loss and the outputs of predict_proba() ). I happened\r\nto hit values of the model coefficients that created this issue. Increasing alpha to\r\nincrease regularization helps prevent the issue, but it would be good if it could be\r\navoided somehow, or if a more pertinent warning were given to the user.\r\n\r\n#### Steps\/Code to Reproduce\r\nSee gist: https:\/\/gist.github.com\/richardtomsett\/8b814f30e1d665fae2b4085d3e4156f5\r\n\r\n#### Expected Results\r\npredict_proba() returns a valid categorical probability distribution over the classes\r\n\r\n#### Actual Results\r\npredict_proba() returns NaN for some inputs\r\n\r\n#### Versions\r\nSystem:\r\n    python: 3.8.3 (default, May 19 2020, 13:54:14)  [Clang 10.0.0 ]\r\nexecutable: [redacted]\r\n   machine: macOS-10.15.3-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n          pip: 20.0.2\r\n   setuptools: 47.1.1.post20200604\r\n      sklearn: 0.23.1\r\n        numpy: 1.18.5\r\n        scipy: 1.4.1\r\n       Cython: None\r\n       pandas: None\r\n   matplotlib: 3.3.0\r\n       joblib: 0.15.1\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\n","labels":["Bug","module:linear_model"],"created_at":"2020-07-23T16:37:57Z","comments":10,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17978"},{"issue_number":180,"repository":"scikit-learn\/scikit-learn","title":"Single linkage option in Agglomerative causes MemoryError for very large numbers","description":"#### Describe the bug\r\nUsing the single linkage option in Agglomerative clustering results in a MemoryError when the input data contains very large values, likely due to numeric overflows. \r\n\r\n#### Steps\/Code to Reproduce\r\nExample: \r\nRunning the code below with half of the sample data does not produce a MemoryError (although the same numpy RuntimeWarning is being thrown). However, if the data contains as many very large numbers as in my sample below my machine with 16Gb RAM runs out of memory and I get a MemoryError. To me this seems a bit disproportionate as the sample data is not very large.\r\n\r\n```python\r\nfrom sklearn.cluster import AgglomerativeClustering\r\n\r\nX = [[1.30830774e+307, 6.02217328e+307],\r\n     [1.54166067e+308, 1.75812744e+308],\r\n     [5.57938866e+307, 4.13840113e+307],\r\n     [1.36302835e+308, 1.07968131e+308],\r\n     [1.58772669e+308, 1.19380571e+307],\r\n     [2.20362426e+307, 1.58814671e+308],\r\n     [1.06216028e+308, 1.14258583e+308],\r\n     [7.18031911e+307, 1.69661213e+308],\r\n     [7.91182553e+307, 5.12892426e+307],\r\n     [5.58470885e+307, 9.13566765e+306],\r\n     [1.22366243e+308, 8.29427922e+307],\r\n     [4.39205961e+306, 1.26048413e+308],\r\n     [4.61599953e+306, 7.24075646e+307],\r\n     [1.66596896e+308, 1.65498552e+308],\r\n     [4.73958815e+307, 7.66412710e+307],\r\n     [1.57013390e+308, 1.03527051e+308],\r\n     [1.28464631e+308, 1.68216358e+308],\r\n     [1.07121506e+308, 3.11489418e+307],\r\n     [2.97524276e+307, 1.59238260e+306],\r\n     [1.24529964e+308, 5.18478922e+306],\r\n     [7.55088957e+307, 7.08726240e+307],\r\n     [8.38161061e+307, 1.50363727e+307],\r\n     [1.11502738e+308, 3.77564517e+307],\r\n     [1.33977566e+308, 4.21606136e+307],\r\n     [3.18410347e+306, 1.41182675e+308],\r\n     [1.37949806e+307, 1.33091975e+308],\r\n     [8.84125355e+307, 8.83334687e+307],\r\n     [4.16683700e+307, 4.38042780e+307],\r\n     [6.33989592e+307, 1.13127438e+308],\r\n     [3.30270525e+307, 5.82271903e+307],\r\n     [1.73464450e+308, 1.57922601e+308],\r\n     [8.50840567e+307, 8.34750980e+307],\r\n     [1.06389805e+308, 7.01361194e+307],\r\n     [1.20971776e+308, 1.42173002e+308],\r\n     [1.91271271e+307, 7.63302091e+307],\r\n     [1.46375293e+308, 3.27352625e+307],\r\n     [5.64255485e+307, 1.62562194e+308],\r\n     [5.96180877e+307, 4.31806469e+307],\r\n     [1.18773200e+308, 4.91146568e+307],\r\n     [1.19298612e+307, 1.54110558e+308],\r\n     [3.51302340e+307, 5.60375139e+307],\r\n     [5.14281492e+307, 9.76302343e+307],\r\n     [1.77408023e+308, 1.65760854e+308],\r\n     [1.46544741e+308, 1.47445640e+308],\r\n     [1.76109403e+308, 1.30923042e+308],\r\n     [1.64984146e+308, 2.58303609e+307],\r\n     [1.61558758e+307, 1.03985868e+308],\r\n     [1.37977676e+308, 4.90921157e+307],\r\n     [1.01105745e+308, 1.57678709e+307],\r\n     [1.24672794e+308, 5.96657664e+307]]\r\n\r\nclusterer = AgglomerativeClustering(linkage='single')\r\nclusterer.fit(X)\r\n```\r\n\r\n#### Expected Results\r\nNo error is thrown or ValueError that specifies the range of allowed values. GaussianMixture clustering is handling the same data like this:\r\n```\r\nValueError: Input contains NaN, infinity or a value too large for dtype('float64').\r\n```\r\n\r\n#### Actual Results\r\n\r\n```python-traceback\r\nC:\\Program Files\\Python37\\lib\\site-packages\\numpy\\core\\fromnumeric.py:90: RuntimeWarning: overflow encountered in reduce\r\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\r\n\r\nTraceback (most recent call last):\r\n  File \"C:\/Users\/thaar\/PycharmProjects\/sklearn-dev\/agglomerative_test.py\", line 58, in <module>\r\n    clusterer.fit(X)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py\", line 898, in fit\r\n    self.n_leaves_)\r\n  File \"C:\\Program Files\\Python37\\lib\\site-packages\\sklearn\\cluster\\_agglomerative.py\", line 674, in _hc_cut\r\n    label[_hierarchical._hc_get_descendent(-node, children, n_leaves)] = i\r\n  File \"sklearn\\cluster\\_hierarchical_fast.pyx\", line 96, in sklearn.cluster._hierarchical_fast._hc_get_descendent\r\nMemoryError\r\n```\r\n\r\n#### Versions\r\nSystem:\r\n    python: 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\Users\\thaar\\PycharmProjects\\sklearn-dev\\venv\\Scripts\\python.exe\r\n   machine: Windows-10-10.0.18362-SP0\r\n\r\nPython dependencies:\r\n          pip: 20.1.1\r\n   setuptools: 49.2.0\r\n      sklearn: 0.23.1\r\n        numpy: 1.18.4\r\n        scipy: 1.4.1\r\n       Cython: None\r\n       pandas: 1.0.3\r\n   matplotlib: 3.2.1\r\n       joblib: 0.14.1\r\nthreadpoolctl: 2.0.0\r\n\r\nBuilt with OpenMP: True\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","help wanted","module:cluster"],"created_at":"2020-07-20T20:40:21Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17960"},{"issue_number":181,"repository":"scikit-learn\/scikit-learn","title":"[0.23.1] AdaBoostRegressor failes on arm(rhel) processors","description":"#### Describe the bug\r\nOn arm processors (AWS:gravition2, rhel), I get the following failure in version 0.23.1\r\n```\r\n937     Examples\r\n938     --------\r\n939     >>> from sklearn.ensemble import AdaBoostRegressor\r\n940     >>> from sklearn.datasets import make_regression\r\n941     >>> X, y = make_regression(n_features=4, n_informative=2,\r\n942     ...                        random_state=0, shuffle=False)\r\n943     >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\r\n944     >>> regr.fit(X, y)\r\n945     AdaBoostRegressor(n_estimators=100, random_state=0)\r\n946     >>> regr.predict([[0, 0, 0, 0]])\r\nExpected:\r\n    array([4.7972...])\r\nGot:\r\n    array([5.57874246])\r\n\r\n\/home\/ec2-user\/github\/sklearn_master\/sklearn\/ensemble\/_weight_boosting.py:946: DocTestFailure\r\n```\r\n\r\n#### Steps\/Code to Reproduce\r\n`pytest -v sklearn\/ensemble\/_weight_boosting.py::sklearn.ensemble._weight_boosting.AdaBoostRegressor`\r\n\r\n#### Expected Results\r\nPASSED is thrown. \r\n\r\n#### Actual Results\r\nFAILED is thrown.\r\n```\r\n946     >>> regr.predict([[0, 0, 0, 0]])\r\nExpected:\r\n    array([4.7972...])\r\nGot:\r\n    array([5.57874246])\r\n```\r\n\r\n#### Versions\r\n```\r\nSystem:\r\n    python: 3.6.8 (default, Dec  5 2019, 16:02:25)  [GCC 8.3.1 20191121 (Red Hat 8.3.1-5)]\r\nexecutable: \/usr\/bin\/python3\r\n   machine: Linux-4.18.0-193.1.2.el8_2.aarch64-aarch64-with-redhat-8.2-Ootpa\r\n\r\nPython dependencies:\r\n          pip: 20.1.1\r\n   setuptools: 39.2.0\r\n      sklearn: 0.23.1\r\n        numpy: 1.14.3\r\n        scipy: 1.0.0\r\n       Cython: 0.29\r\n       pandas: 1.0.5\r\n   matplotlib: 3.2.1\r\n       joblib: 0.14.0\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True\r\nLinux-4.18.0-193.1.2.el8_2.aarch64-aarch64-with-redhat-8.2-Ootpa\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 3, in <module>\r\nNameError: name 'Python' is not defined\r\n```","labels":["Bug","help wanted","module:ensemble","arch:arm"],"created_at":"2020-07-01T00:55:28Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17798"},{"issue_number":182,"repository":"scikit-learn\/scikit-learn","title":"pandas dtypes \"boolean\" not supported in classification target","description":"<!--\r\nBefore submitting a bug, please make sure the issue hasn't been already\r\naddressed by searching through the past issues.\r\n-->\r\n\r\n#### Describe the bug\r\nPandas has extended NumPy's dtypes and these dtypes extensions are not all supported as targets in a sklearn classifier. In particular, if the target y is a Pandas \"boolean\" dtype, a classifier such as LogisticRegression fails whereas if the target is a numpy \"bool\" dtype, the classifier will not fail.\r\n\r\n#### Steps\/Code to Reproduce\r\n```python\r\nimport pandas as pd\r\nfrom sklearn.linear_model import LogisticRegression\r\n\r\ndf = pd.DataFrame({\"boolean_col\": pd.Series([False, True, False, True], dtype=\"boolean\"),\r\n                   \"bool_col\": pd.Series([False, True, False, True], dtype=\"bool\"),\r\n                   \"num_col\": pd.Series([1, 2, 3, 4])})\r\n\r\nclf = LogisticRegression()\r\n```\r\n\r\n\r\n\r\n\r\n#### Expected Results\r\n```python\r\nclf.fit(df[[\"num_col\"]], df.bool_col)\r\n--------------------------------------------------------------------------------------------------------------\r\nLogisticRegression()\r\n```\r\n#### Actual Results\r\n```python\r\nclf.fit(df[[\"num_col\"]], df.boolean_col)\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-4-9a0c5abf260e> in <module>\r\n----> 1 clf.fit(df[[\"num_col\"]], df.boolean_col)\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lognode\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py in fit(self, X, y, sample_weight)\r\n   1343                                    order=\"C\",\r\n   1344                                    accept_large_sparse=solver != 'liblinear')\r\n-> 1345         check_classification_targets(y)\r\n   1346         self.classes_ = np.unique(y)\r\n   1347 \r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lognode\\lib\\site-packages\\sklearn\\utils\\multiclass.py in check_classification_targets(y)\r\n    170     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\r\n    171                       'multilabel-indicator', 'multilabel-sequences']:\r\n--> 172         raise ValueError(\"Unknown label type: %r\" % y_type)\r\n    173 \r\n    174 \r\n\r\nValueError: Unknown label type: 'unknown'\r\n```\r\n#### Versions\r\nSystem:\r\n    python: 3.8.2 (default, Apr 14 2020, 19:01:40) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: ~\\AppData\\Local\\Continuum\\anaconda3\\envs\\lognode\\python.exe\r\n   machine: Windows-10-10.0.18362-SP0\r\n\r\nPython dependencies:\r\n          pip: 20.0.2\r\n   setuptools: 46.1.3.post20200330\r\n      sklearn: 0.23.1\r\n        numpy: 1.18.1\r\n        scipy: 1.4.1\r\n       Cython: None\r\n       pandas: 1.0.3\r\n   matplotlib: 3.2.1\r\n       joblib: 0.14.1\r\nthreadpoolctl: 2.1.0\r\n\r\nBuilt with OpenMP: True","labels":["Bug"],"created_at":"2020-06-23T15:25:30Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17675"},{"issue_number":183,"repository":"scikit-learn\/scikit-learn","title":"inconsistent treatment of None and np.NaN in SimpleImputer","description":"Doing constant imputation treats only the \"missing_value\" as missing, so a ``None`` by default stays there:\r\n```python\r\nfrom sklearn.impute import SimpleImputer\r\nimport numpy as np\r\nX = np.array([1, 2, np.NaN, None]).reshape(-1, 1)\r\nSimpleImputer(strategy='constant', fill_value=\"asdf\").fit_transform()\r\n```\r\n```\r\narray([[1],\r\n       [2],\r\n       ['asdf'],\r\n       [None]], dtype=object)\r\n```\r\nHowever, using strategy='mean' coerces the None to NaN and so both are replaced:\r\n```python\r\nSimpleImputer(strategy='mean').fit_transform(X)\r\n```\r\n```\r\narray([[1. ],\r\n       [2. ],\r\n       [1.5],\r\n       [1.5]])\r\n```\r\nI don't think the definition of what's missing should depend on the strategy. @thomasjpfan argues that the current constant behavior is inconvenient because it means you have to impute both values separately if you want to one-hot-encode.\r\n\r\nIt seems more safe to treat them differently but I'm not sure there's a use-case for that. This came up in #17317.\r\nI think this only matters in these two, as other imputers don't allow dtype object arrays.","labels":["Bug"],"created_at":"2020-06-17T20:23:13Z","comments":6,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17625"},{"issue_number":184,"repository":"scikit-learn\/scikit-learn","title":"Linear SVM does wrong prediction","description":"#### Describe the bug\r\nThe prediction for specific train data is wrong\r\n\r\n#### Steps\/Code to Reproduce\r\nYou can run the following code. While the first decision limit is correct, for the 2nd pair of points the decision limit is totally wrong placed\r\n\r\nExample:\r\n```python\r\nfrom sklearn import svm\r\nfrom matplotlib import pyplot as plt\r\nimport numpy as np\r\n\r\nlclf = svm.LinearSVC()\r\n\r\np1 = 0.25\r\np2 = -0.25\r\n\r\nx_min = -2\r\nx_max = 2\r\nx_h = 0.01\r\ny_min = -2\r\ny_max = 2\r\ny_h = 0.01\r\nxx, yy = np.meshgrid(np.arange(x_min, x_max, x_h),\r\n                    np.arange(y_min, y_max, y_h))\r\n\r\nlclf.fit([[p1,0],[p2,0]],[-1,1])\r\n\r\nplt.plot(p1, 0, \"ro\")\r\nplt.plot(p2, 0, \"go\")\r\n\r\nZ = lclf.predict(np.c_[xx.ravel(), yy.ravel()])\r\nZ = Z.reshape(xx.shape)\r\n\r\nplt.gca().contourf(xx, yy, Z, colors = [\"red\", \"green\"], alpha = 0.1)\r\nplt.show()\r\n\r\n\r\n#The following does not work\r\n\r\np3 = 0.025545042466768184\r\np4 = 0.0293294646367189\r\n\r\nx_min = 0\r\nx_max = 0.035\r\nx_h = 0.00001\r\ny_min = -0.01\r\ny_max = 0.01\r\ny_h = 0.001\r\nxx, yy = np.meshgrid(np.arange(x_min, x_max, x_h),\r\n                    np.arange(y_min, y_max, y_h))\r\n\r\nlclf.fit([[p3, 0], [p4,0]], [1,-1])\r\n\r\nplt.plot(p3, 0, \"go\")\r\nplt.plot(p4, 0, \"ro\")\r\n\r\nZ = lclf.predict(np.c_[xx.ravel(), yy.ravel()])\r\nZ = Z.reshape(xx.shape)\r\n\r\nplt.gca().contourf(xx, yy, Z, colors = [\"red\", \"green\"], alpha = 0.1)\r\nplt.show()\r\n```\r\n\r\n\r\n#### Expected Results\r\nThe decision boarder should be centered between the two train points\r\n\r\n#### Actual Results\r\nthe boarder is on the left of both points\r\n\r\n#### Versions\r\nSystem:\r\n    python: 3.7.5 (tags\/v3.7.5:5c02a39a0b, Oct 15 2019, 00:11:34) [MSC v.1916 64 bit (AMD64)]\r\nexecutable: C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python37_64\\python.exe\r\n   machine: Windows-10-10.0.18362-SP0\r\n\r\nPython dependencies:\r\n       pip: 20.0.2\r\nsetuptools: 46.1.3\r\n   sklearn: 0.22.2.post1\r\n     numpy: 1.18.3\r\n     scipy: 1.4.1\r\n    Cython: None\r\n    pandas: 1.0.3\r\nmatplotlib: 3.2.1\r\n    joblib: 0.14.1\r\n\r\nBuilt with OpenMP: True\r\n","labels":["Bug","module:linear_model","module:svm"],"created_at":"2020-06-10T13:37:27Z","comments":9,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17557"},{"issue_number":185,"repository":"scikit-learn\/scikit-learn","title":"LinearDiscriminantAnalysis cannot predict_proba if labels are constant","description":"I know that most of the times labels will not be constant, however I am using `check_estimator` to check a custom estimator which has a `base_estimator` attribute and scikit-learn chooses LinearDiscriminantAnalysis as base estimator.\r\n\r\n#### Describe the bug\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\nrnd = np.random.RandomState(0)\r\nX_train = rnd.uniform(size=(10, 3))\r\ny = np.ones(10)\r\nLinearDiscriminantAnalysis().fit(X_train, y).predict_proba(X_train)\r\n```\r\n\r\nThe code above will throw the following error:\r\n\r\n`AxisError: axis 1 is out of bounds for array of dimension 1`\r\n\r\nIt does not happen if I use the `predict` method. I also tried with a pair more classifiers (ensembles) and it actually works. I also tried the other discriminant analysis, the quadratic, and it does not let me fit because y is constant. This error makes sense to me, but I can't understand what is going on with the one in the linear discriminant analysis,.\r\n\r\n#### Versions\r\nLinux-4.4.0-179-generic-x86_64-with-glibc2.10\r\nPython 3.8.3 (default, May 19 2020, 18:47:26) \r\n[GCC 7.3.0]\r\nNumPy 1.18.1\r\nSciPy 1.4.1\r\nScikit-Learn 0.23.1\r\n\r\n","labels":["Bug"],"created_at":"2020-06-01T08:23:10Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17401"},{"issue_number":186,"repository":"scikit-learn\/scikit-learn","title":"_weighted_percentile does not lead to the same result than np.median","description":"While reviewing a test in https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/16937, it appears that our implementation of `_weighted_percentile` with unit `sample_weight` will lead to a different result than `np.median` which is a bit problematic for consistency.\r\n\r\nIn the gradient-boosting, it brakes the loss equivalence because the initial predictions are different. We could bypass this issue by always computing the median using `_weighted_percentile` there.\r\n\r\n```python\r\nimport pytest\r\nimport numpy as np\r\nfrom sklearn.utils.stats import _weighted_percentile\r\n\r\nrng = np.random.RandomState(42)\r\nX = rng.randn(10)\r\nX.sort()\r\nsample_weight = np.ones(X.shape)\r\n\r\nmedian_numpy = np.median(X)\r\nmedian_numpy_percentile = np.percentile(X, 50)\r\nmedian_sklearn = _weighted_percentile(X, sample_weight, percentile=50.0)\r\n\r\nassert median_numpy == pytest.approx(np.mean(X[[4, 5]]))\r\nassert median_sklearn == pytest.approx(X[4])\r\nassert median_numpy == median_numpy_percentile\r\nassert median_sklearn == pytest.approx(median_numpy)\r\n```","labels":["Bug"],"created_at":"2020-05-28T10:03:16Z","comments":16,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17370"},{"issue_number":187,"repository":"scikit-learn\/scikit-learn","title":"KMeans(init='k-means++') performance issue with OpenBLAS","description":"I open this issue to investigate a performance problem that might be related to #17230.\r\n\r\nI adapted the reproducer of #17230 to display more info and make it work on a medium-size ranom dataset.\r\n\r\n```python\r\nfrom sklearn import cluster\r\nfrom time import time\r\nfrom pprint import pprint\r\nfrom threadpoolctl import threadpool_info\r\nimport numpy as np\r\n\r\n\r\npprint(threadpool_info())\r\nrng = np.random.RandomState(0)\r\ndata = rng.randn(5000, 50)\r\nt0_global = time()\r\nfor k in range(1, 15):\r\n    t0 = time()\r\n    # print(f\"Running k-means with k={k}: \", end=\"\", flush=True)\r\n    cluster.KMeans(\r\n        n_clusters=k,\r\n        random_state=42,\r\n        n_init=10,\r\n        max_iter=2000,\r\n        algorithm='full',\r\n        init='k-means++').fit(data)\r\n    # print(f\"{time() - t0:.3f} s\")\r\n\r\nprint(f\"Total duration: {time() - t0_global:.3f} s\")\r\n```\r\n\r\nI tried to run this on Linux with scikit-learn master (therefore including the #16499 fix)  with 2 different builds of scipy (with openblas from pypi and MKL from anaconda) and various values for `OMP_NUM_THREADS` (unset, `OMP_NUM_THREADS=1`, `OMP_NUM_THREADS=2`, `OMP_NUM_THREADS=4`) on a laptop with 2 physical cpu cores (4 logical cpus).\r\n\r\nIn both cases, I use the same scikit-learn binaries (built with GCC in editable mode). I just change the env.\r\n\r\nThe summary is:\r\n\r\n- with MKL there is not problem: large or unset values of `OMP_NUM_THREADS` are faster than `OMP_NUM_THREADS=1`;\r\n- with OpenBLAS without explicit setting of `OMP_NUM_THREADS` or setting a large value for it is significanlty slower forced sequential run with `OMP_NUM_THREADS=1`.\r\n\r\nI will include my runs in the first comment. \r\n\r\n\/cc @jeremiedbb ","labels":["Bug","Performance","module:cluster"],"created_at":"2020-05-25T09:51:23Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17334"},{"issue_number":188,"repository":"scikit-learn\/scikit-learn","title":"sklearn.utils.resample stratify parameter is not working","description":"I am using \r\n\r\n```\r\nPython 3.6.8\r\nscikit-learn==0.23.1\r\n```\r\n\r\nI am using ```sklearn.utils,resample``` for stratified sampling. Here is my code:-\r\n\r\n```python\r\nfrom sklearn.utils import resample\r\ny=[1,1,2,3,2,2,1,3,1,1,2,3,2,2,1,3,4,4]\r\nsample = resample(y, n_samples=5, replace=False, stratify=y,\r\n         random_state=0)\r\nprint(sample)\r\n```\r\n\r\ngives me output:-\r\n```\r\n[3, 2, 2, 1, 1]\r\n```\r\n\r\nExpected output:-\r\noutput should contain all the distinct values in y. At least one \"4\" should be present in the output. For eg.\r\n```\r\n[3,2,2,1,4]\r\n```\r\n\r\nAm I missing something? Thank you for your help in advance.","labels":["Bug","module:utils"],"created_at":"2020-05-24T06:15:29Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17321"},{"issue_number":189,"repository":"scikit-learn\/scikit-learn","title":"GridsearchCV.score with multimetric scoring and callable refit","description":"#### Describe the bug\r\nWhen using GridsearchCV with multimetric scoring and a callable as refit, the `GridsearchCV.score` function doesn't works since `score = self.scorer_[self.refit] if self.multimetric_ else self.scorer_` seems to wait only for a string in case of multimetric scoring\r\n#### Steps\/Code to Reproduce\r\n```\r\nfrom sklearn.datasets import load_breast_cancer\r\nfrom sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\r\nfrom sklearn.linear_model import LogisticRegression\r\nimport numpy as np\r\n\r\ndef get_best_index(cv_results):\r\n    best_rank_mask = cv_results['rank_test_roc_auc'] == cv_results['rank_test_roc_auc'].min()\r\n    params_best_score = np.array(cv_results['params'])[best_rank_mask]\r\n    params_name = params_best_score[0].keys()\r\n    classifier_params_names = [_ for _ in params_name if 'classifier' in _]\r\n    if 'classifier__C' in classifier_params_names or 'classifier__base_estimator__C' in classifier_params_names:\r\n        classifier_params = [_['classifier__C' if 'classifier__C' in classifier_params_names else 'classifier__base_estimator__C'] for _ in params_best_score]\r\n        params_best_score = params_best_score[classifier_params == min(classifier_params)]\r\n    best_params = params_best_score[0]\r\n    best_index = int(np.where(np.array(cv_results['params']) == best_params)[0])\r\n    return best_index\r\n\r\nbreast = load_breast_cancer()\r\nX = breast.data\r\ny = breast.target\r\ncv = RepeatedStratifiedKFold(5, 2, random_state=111)\r\nparams_dic = {'C': np.arange(0.1, 1.1, 0.1)}\r\nclf = GridSearchCV(LogisticRegression(penalty='l2', max_iter=1e5, solver='saga'), params_dic, scoring=['roc_auc', 'accuracy'], cv=cv, refit=get_best_index, n_jobs=4)\r\nclf.fit(X,y)\r\nclf.score(X,y)\r\n```\r\n\r\n#### Actual Results\r\nFile \"C:\\Users\\Tim\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\", line 447, in score\r\n    score = self.scorer_[self.refit] if self.multimetric_ else self.scorer_\r\nKeyError: <function get_best_index at 0x0000028DD66BDBF8>\r\n\r\n#### Expected Results\r\nSince refit is a callable I don't know how he could know which metric to choose for scoring., However, if I give a string with the metric I chose, i.e. 'roc_auc', to refit argument the best index won't be chosen in the way I want. Maybe in case of multimetric scoring and callable refit, ask for dictionnary instead like {score: callable} and the score will be used in GridsearchCV.score ?\r\n\r\n#### Versions\r\nSystem:\r\n    python: 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\r\nexecutable: C:\\Users\\Tim\\Anaconda3\\python.exe\r\n   machine: Windows-10-10.0.18362-SP0\r\nPython dependencies:\r\n       pip: 20.0.2\r\nsetuptools: 39.1.0\r\n   sklearn: 0.22.1\r\n     numpy: 1.18.1\r\n     scipy: 1.4.1\r\n    Cython: 0.28.2\r\n    pandas: 1.0.0\r\nmatplotlib: 2.2.2\r\n    joblib: 0.14.1\r\nBuilt with OpenMP: True\r\n","labels":["Bug","module:model_selection"],"created_at":"2020-04-27T10:54:52Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/17058"},{"issue_number":190,"repository":"scikit-learn\/scikit-learn","title":"BUG: Test failure on ppc64le","description":"<!--\r\nBefore submitting a bug, please make sure the issue hasn't been already\r\naddressed by searching through the past issues.\r\n-->\r\n\r\n#### Describe the bug\r\n\r\nWhile working on https:\/\/github.com\/conda-forge\/scikit-learn-feedstock\/pull\/123, I found that `manifold\/tests\/test_t_sne.py::test_uniform_grid[barnes_hut]` fails on ppc64le; @rth [asked](https:\/\/github.com\/conda-forge\/scikit-learn-feedstock\/pull\/123#issuecomment-614625410) me to open a bug here.\r\n\r\n```\r\n=========================== short test summary info ============================\r\nFAILED manifold\/tests\/test_t_sne.py::test_uniform_grid[barnes_hut] - Assertio...\r\n= 1 failed, 13718 passed, 398 skipped, 6 xfailed, 1674 warnings in 382.15s (0:06:22) =\r\n```\r\n\r\nHere are the logs, the failure is the same across cpython 3.6-3.8:\r\n* [python 3.6](https:\/\/travis-ci.com\/github\/conda-forge\/scikit-learn-feedstock\/jobs\/319979948)\r\n* [python 3.7](https:\/\/travis-ci.com\/github\/conda-forge\/scikit-learn-feedstock\/jobs\/319979949)\r\n* [python 3.8](https:\/\/travis-ci.com\/github\/conda-forge\/scikit-learn-feedstock\/jobs\/319979950)\r\n\r\nInterestingly, [PyPy](https:\/\/travis-ci.com\/github\/conda-forge\/scikit-learn-feedstock\/jobs\/319979951) doesn't have that failure (but has some segfaults instead..., see #16956).\r\n\r\nA more detailed trace is below the fold. From what I can see from the log, the failure seems to be a question of \"only\" the quality of an embedding, and not a hard failure per se.\r\n\r\n<details>\r\n\r\n```\r\n=================================== FAILURES ===================================\r\n________________________ test_uniform_grid[barnes_hut] _________________________\r\n[gw2] linux -- Python 3.6.10 $PREFIX\/bin\/python\r\n\r\nmethod = 'barnes_hut'\r\n\r\n    @pytest.mark.parametrize('method', ['barnes_hut', 'exact'])\r\n    def test_uniform_grid(method):\r\n        \"\"\"Make sure that TSNE can approximately recover a uniform 2D grid\r\n    \r\n        Due to ties in distances between point in X_2d_grid, this test is platform\r\n        dependent for ``method='barnes_hut'`` due to numerical imprecision.\r\n    \r\n        Also, t-SNE is not assured to converge to the right solution because bad\r\n        initialization can lead to convergence to bad local minimum (the\r\n        optimization problem is non-convex). To avoid breaking the test too often,\r\n        we re-run t-SNE from the final point when the convergence is not good\r\n        enough.\r\n        \"\"\"\r\n        seeds = [0, 1, 2]\r\n        n_iter = 500\r\n        for seed in seeds:\r\n            tsne = TSNE(n_components=2, init='random', random_state=seed,\r\n                        perplexity=20, n_iter=n_iter, method=method)\r\n            Y = tsne.fit_transform(X_2d_grid)\r\n    \r\n            try_name = \"{}_{}\".format(method, seed)\r\n            try:\r\n>               assert_uniform_grid(Y, try_name)\r\n\r\n..\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla\/lib\/python3.6\/site-packages\/sklearn\/manifold\/tests\/test_t_sne.py:784: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nY = array([[ 52.326397  , -15.92225   ],\r\n       [ 46.679527  , -20.175953  ],\r\n       [ 40.870537  , -24.181147  ],\r\n       ...[-35.291374  ,  22.122814  ],\r\n       [-42.2738    ,  18.793724  ],\r\n       [-48.922283  ,  15.606232  ]], dtype=float32)\r\ntry_name = 'barnes_hut_1'\r\n\r\n    def assert_uniform_grid(Y, try_name=None):\r\n        # Ensure that the resulting embedding leads to approximately\r\n        # uniformly spaced points: the distance to the closest neighbors\r\n        # should be non-zero and approximately constant.\r\n        nn = NearestNeighbors(n_neighbors=1).fit(Y)\r\n        dist_to_nn = nn.kneighbors(return_distance=True)[0].ravel()\r\n        assert dist_to_nn.min() > 0.1\r\n    \r\n        smallest_to_mean = dist_to_nn.min() \/ np.mean(dist_to_nn)\r\n        largest_to_mean = dist_to_nn.max() \/ np.mean(dist_to_nn)\r\n    \r\n        assert smallest_to_mean > .5, try_name\r\n>       assert largest_to_mean < 2, try_name\r\nE       AssertionError: barnes_hut_1\r\nE       assert 6.67359409617653 < 2\r\n\r\n..\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla\/lib\/python3.6\/site-packages\/sklearn\/manifold\/tests\/test_t_sne.py:807: AssertionError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nmethod = 'barnes_hut'\r\n\r\n    @pytest.mark.parametrize('method', ['barnes_hut', 'exact'])\r\n    def test_uniform_grid(method):\r\n        \"\"\"Make sure that TSNE can approximately recover a uniform 2D grid\r\n    \r\n        Due to ties in distances between point in X_2d_grid, this test is platform\r\n        dependent for ``method='barnes_hut'`` due to numerical imprecision.\r\n    \r\n        Also, t-SNE is not assured to converge to the right solution because bad\r\n        initialization can lead to convergence to bad local minimum (the\r\n        optimization problem is non-convex). To avoid breaking the test too often,\r\n        we re-run t-SNE from the final point when the convergence is not good\r\n        enough.\r\n        \"\"\"\r\n        seeds = [0, 1, 2]\r\n        n_iter = 500\r\n        for seed in seeds:\r\n            tsne = TSNE(n_components=2, init='random', random_state=seed,\r\n                        perplexity=20, n_iter=n_iter, method=method)\r\n            Y = tsne.fit_transform(X_2d_grid)\r\n    \r\n            try_name = \"{}_{}\".format(method, seed)\r\n            try:\r\n                assert_uniform_grid(Y, try_name)\r\n            except AssertionError:\r\n                # If the test fails a first time, re-run with init=Y to see if\r\n                # this was caused by a bad initialization. Note that this will\r\n                # also run an early_exaggeration step.\r\n                try_name += \":rerun\"\r\n                tsne.init = Y\r\n                Y = tsne.fit_transform(X_2d_grid)\r\n>               assert_uniform_grid(Y, try_name)\r\n\r\n..\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla\/lib\/python3.6\/site-packages\/sklearn\/manifold\/tests\/test_t_sne.py:792: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nY = array([[-18.169476  ,   6.0802336 ],\r\n       [-18.278513  ,   2.8822129 ],\r\n       [-18.671782  ,  -0.4646889 ],\r\n       ...[ 22.550077  ,  19.698557  ],\r\n       [ 21.399723  ,  22.933178  ],\r\n       [ 16.22136   ,  28.22955   ]], dtype=float32)\r\ntry_name = 'barnes_hut_1:rerun'\r\n\r\n    def assert_uniform_grid(Y, try_name=None):\r\n        # Ensure that the resulting embedding leads to approximately\r\n        # uniformly spaced points: the distance to the closest neighbors\r\n        # should be non-zero and approximately constant.\r\n        nn = NearestNeighbors(n_neighbors=1).fit(Y)\r\n        dist_to_nn = nn.kneighbors(return_distance=True)[0].ravel()\r\n        assert dist_to_nn.min() > 0.1\r\n    \r\n        smallest_to_mean = dist_to_nn.min() \/ np.mean(dist_to_nn)\r\n        largest_to_mean = dist_to_nn.max() \/ np.mean(dist_to_nn)\r\n    \r\n        assert smallest_to_mean > .5, try_name\r\n>       assert largest_to_mean < 2, try_name\r\nE       AssertionError: barnes_hut_1:rerun\r\nE       assert 2.145051767903112 < 2\r\n\r\n..\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla\/lib\/python3.6\/site-packages\/sklearn\/manifold\/tests\/test_t_sne.py:807: AssertionError\r\n=============================== warnings summary ===============================\r\n[...]\r\n```\r\n\r\n<\/details>\r\n","labels":["Bug","help wanted","module:test-suite"],"created_at":"2020-04-18T09:53:47Z","comments":1,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16955"},{"issue_number":191,"repository":"scikit-learn\/scikit-learn","title":"In multifold.MDS  stress value doesn\u2019t correspond to returned coordinates","description":"<p><strong>Description<\/strong><\/p>\r\n<p>&nbsp;<\/p>\r\n<p>In multifold.MDS&nbsp; stress value doesn&rsquo;t correspond to returned coordinates.<\/p>\r\n<p>&nbsp;<\/p>\r\n<p><strong>How to reproduce:<\/strong><\/p>\r\n<p>&nbsp;<\/p>\r\n<p>Apply MDS to four points in 2d-plane:<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>X &nbsp;&nbsp;Y<\/p>\r\n<p>1 &nbsp;&nbsp;5<\/p>\r\n<p>1 &nbsp;&nbsp;4<\/p>\r\n<p>1&nbsp; &nbsp;1<\/p>\r\n<p>3&nbsp; &nbsp;3<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>If distances are Euclidean, then disparities matrix will be:<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>disparities =<\/p>\r\n<p>&nbsp;[ 0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2.82842712&nbsp; ]<\/p>\r\n<p>&nbsp;[ 1.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2.23606798&nbsp; ]<\/p>\r\n<p>&nbsp;[ 4.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2.82842712&nbsp; ]<\/p>\r\n<p>&nbsp;[ 2.82842712&nbsp; &nbsp; &nbsp; 2.23606798&nbsp; &nbsp; &nbsp;2.82842712&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ]<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>Perform multidimensional scaling:<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>mds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42,  metric = True)<\/p>\r\n<p>results = mds.fit(disparities)<\/p>\r\n<p>coords = results.embedding_<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>cords =<\/p>\r\n<p>&nbsp;[ -0.32503572&nbsp; &nbsp; &nbsp; 1.78399044 ]<\/p>\r\n<p>&nbsp;[&nbsp; 0.09843686&nbsp; &nbsp; &nbsp; 0.87890749 ]<\/p>\r\n<p>&nbsp;[&nbsp; 1.47842546&nbsp; &nbsp; &nbsp;-1.76761757 ]<\/p>\r\n<p>&nbsp;[ -1.2518266&nbsp; &nbsp; &nbsp; -0.89528037 ]<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>stress = results.stress_<\/p>\r\n<p>0.0045113518633979315<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>Now calculate stress by hand.<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>First calculate Euclidian distances which correspond coordinates returned:<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>dis = euclidean_distances(coords)<\/p>\r\n<p>&nbsp;[ 0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.9992518&nbsp; &nbsp; &nbsp; &nbsp;3.98326395&nbsp; &nbsp; &nbsp; 2.83503675&nbsp; ]<\/p>\r\n<p>&nbsp;[ 0.9992518&nbsp; &nbsp;&nbsp;&nbsp;0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2.98470492&nbsp; &nbsp; &nbsp; 2.22956362&nbsp; ]<\/p>\r\n<p>&nbsp;[ 3.98326395&nbsp; &nbsp;2.98470492&nbsp; &nbsp; &nbsp;0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2.86622548&nbsp; ]<\/p>\r\n<p>&nbsp;[ 2.83503675&nbsp; &nbsp;2.22956362&nbsp; &nbsp; &nbsp;2.86622548&nbsp; &nbsp; &nbsp; 0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ]<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>And now we can calculate stress value:<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>stress =<\/p>\r\n<p>((dis.ravel() - disparities.ravel()) ** 2).sum() \/ 2<\/p>\r\n<p>0.0020293041020245147<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>So, the real stress calculated using returned coordinates doesn&rsquo;t correspond to the stress value<\/p>\r\n<p>results.stress_ = 0.0045113518633979315<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>After some debugging it is clear that MDS returns coordinates for the current iteration and stress for the previous iteration.<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>&nbsp;<\/p>\r\n<p><strong>Here is the script to reproduce:<\/strong><\/p>\r\n<p>&nbsp;<\/p>\r\n<p>from sklearn import manifold<\/p>\r\n<p>from sklearn.metrics.pairwise import euclidean_distances<\/p>\r\n<p>import pandas as pd<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>data = [['A',1,5],['B',1,4],['C',1,1],['D',3,3]]<\/p>\r\n<p>df = pd.DataFrame(data, columns = ['Name', 'X','Y'])<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>#Distance matrix<\/p>\r\n<p>disparities = euclidean_distances(df[['X','Y']])<\/p>\r\n<p>mds = manifold.MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42,&nbsp; metric = True)<\/p>\r\n<p>results = mds.fit(disparities)<\/p>\r\n<p>coords = results.embedding_<\/p>\r\n<p>stress = results.stress_<\/p>\r\n<p>print ('returned stress=',stress)<\/p>\r\n<p>&nbsp;<\/p>\r\n<p># Calculate Stress by hand<\/p>\r\n<p>dis = euclidean_distances(coords)<\/p>\r\n<p>real_stress = ((dis.ravel() - disparities.ravel()) ** 2).sum() \/ 2<\/p>\r\n<p>print('real stress =',real_stress)<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>&nbsp;<\/p>\r\n<p><strong>Versions:<\/strong><\/p>\r\n<p>System:<\/p>\r\n<p>&nbsp;&nbsp;&nbsp; python: 3.7.6 (default, Dec 30 2019, 19:38:36)&nbsp; [Clang 10.0.0 (clang-1000.11.45.5)]<\/p>\r\n<p>executable: \/usr\/local\/opt\/python\/bin\/python3.7<\/p>\r\n<p>&nbsp;&nbsp; machine: Darwin-17.7.0-x86_64-i386-64bit<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>Python deps:<\/p>\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pip: 19.3.1<\/p>\r\n<p>setuptools: 42.0.2<\/p>\r\n<p>&nbsp;&nbsp; sklearn: 0.21.3<\/p>\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp; numpy: 1.17.4<\/p>\r\n<p>&nbsp;&nbsp;&nbsp;&nbsp; scipy: 1.3.2<\/p>\r\n<p>&nbsp;&nbsp;&nbsp; Cython: 0.29.15<\/p>\r\n<p>&nbsp;&nbsp;&nbsp; pandas: 1.0.1<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>&nbsp;<\/p>\r\n<p>&nbsp;<\/p>","labels":["Bug","help wanted","module:manifold"],"created_at":"2020-04-05T11:07:53Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16846"},{"issue_number":192,"repository":"scikit-learn\/scikit-learn","title":"MultiOutputClassifier missleading score","description":"https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/95d4f0841d57e8b5f6b2a570312e9d832e69debc\/sklearn\/multioutput.py#L421\r\n\r\nWell, just to let you know that when using a MultiOutputClassifier I was finding the score results to be surprisingly low. After digging a bit, I found that the score of a MultiOutputClassifier was being calculated as the mean of y being equal to y_pred for all targets: \r\n`np.mean(np.all(y == y_pred, axis=1))` \r\n\r\nFor me, that was not, at all, the expected behaviour when you state that score \"_Returns the mean accuracy on the given test data and labels_\"! Indeed, I was expecting the returned accuracy to consider all hits. Something like: \r\n`np.mean(np.mean(y == y_pred, axis=0))`\r\n\r\nA simple example:\r\n```\r\ny = [['low', 'medium', 'medium'], ['medium', 'medium', 'medium'], ['medium', 'high', 'high']]\r\ny_pred = [['medium', 'medium', 'medium'], ['medium', 'high', 'medium'], ['low', 'high', 'high']]\r\n```\r\nWould return an accuracy of 0 instead of 0.667!\r\nWhy not give the user the chance to choose?\r\n\r\nAnyway, if you find the current behaviour to be the expected one, feel free to close this issue.","labels":["Bug"],"created_at":"2020-04-03T00:40:49Z","comments":3,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16832"},{"issue_number":193,"repository":"scikit-learn\/scikit-learn","title":"Pipeline requires both fit and transform method to be available instead of only fit_transform","description":"\r\n#### Describe the bug\r\nCalling a pipeline with a nonparametric function causes an error since the function `transform()` is missing.  The pipeline itself calls the function `fit_transform()` if it's present.  For nonparametric functions (the most prominent being t-SNE) a regular `transform()` method does not exist since there is no projection or mapping that is learned.  It could still be used for dimensionality reduction.\r\n\r\n#### Steps\/Code to Reproduce\r\nExample:\r\n```python\r\nfrom sklearn.decomposition import PCA\r\nfrom sklearn.manifold import TSNE\r\nfrom sklearn.pipeline import make_pipeline\r\n\r\nmake_pipeline(TSNE(), PCA())\r\n```\r\n\r\n\r\n#### Expected Results\r\nA pipeline is created.\r\n\r\n#### Actual Results\r\nOutput:\r\n```\r\nTypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'TSNE(angle=0.5,...\r\n```\r\n#### Possible Solution\r\nEditing this https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/95d4f0841\/sklearn\/pipeline.py#L179\r\nto the following in order to reflect the change:\r\n```python\r\nif (not hasattr(t, \"fit_transform\")) or not (hasattr(t, \"fit\") and hasattr(t, \"transform\")):\r\n```\r\n\r\n#### Versions\r\n```\r\nimport sklearn; sklearn.show_versions()\r\nSystem:\r\n    python: 3.8.2 (default, Feb 26 2020, 22:21:03)  [GCC 9.2.1 20200130]\r\nexecutable: \/usr\/bin\/python3\r\n   machine: Linux-5.5.9-arch1-2-x86_64-with-glibc2.2.5\r\n\r\nPython dependencies:\r\n       pip: 20.0.2\r\nsetuptools: 46.0.0\r\n   sklearn: 0.22.2.post1\r\n     numpy: 1.18.1\r\n     scipy: 1.4.1\r\n    Cython: 0.29.15\r\n    pandas: 1.0.1\r\nmatplotlib: 3.2.0\r\n    joblib: 0.14.1\r\n\r\nBuilt with OpenMP: True\r\n```\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","Low Priority","module:pipeline"],"created_at":"2020-03-16T22:40:24Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16710"},{"issue_number":194,"repository":"scikit-learn\/scikit-learn","title":"SVC with ADABoosting","description":"\r\n\r\nI am trying to use SVC with ADAboosting. WIth SVC, but not other base estimators, the initial estimator does not seemed to be trained.\r\n\r\nComparing the initial estimator of the ensemble to a single estimator with the same hyper-parameters \r\n\r\ncreate an ADA ensemble\r\n\r\n<img width=\"986\" alt=\"Screen Shot 2020-03-04 at 9 15 10 PM\" src=\"https:\/\/user-images.githubusercontent.com\/19198429\/75944794-39ed2100-5e5e-11ea-9125-b06cfaf729d5.png\">\r\n\r\nCreate a single SVC\r\n\r\n<img width=\"553\" alt=\"Screen Shot 2020-03-04 at 9 15 20 PM\" src=\"https:\/\/user-images.githubusercontent.com\/19198429\/75944859-58ebb300-5e5e-11ea-9f3c-65c098b4a9f4.png\">\r\n\r\nCompare the accuracies\r\n\r\n<img width=\"415\" alt=\"Screen Shot 2020-03-04 at 9 15 49 PM\" src=\"https:\/\/user-images.githubusercontent.com\/19198429\/75944878-630db180-5e5e-11ea-9760-a38601e0d873.png\">\r\n\r\n\r\nI have tried different hyper-parameters and had the same issue. This issue does happen for RandomForests and DecisionTree Classifiers\r\n\r\n\r\n","labels":["Bug","module:ensemble"],"created_at":"2020-03-05T03:24:27Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16642"},{"issue_number":195,"repository":"scikit-learn\/scikit-learn","title":"OneHotEncoder.get_feature_names doesn't work with integer column names","description":"```python\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nX = pd.DataFrame({1: np.random.randint(0, 10, size=400)})\r\nOneHotEncoder().fit(X).get_feature_names([1])\r\n```\r\n```pythontb\r\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\r\n```","labels":["Bug","Needs Decision","module:preprocessing"],"created_at":"2020-02-28T21:04:42Z","comments":29,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16593"},{"issue_number":196,"repository":"scikit-learn\/scikit-learn","title":"categoricalNB fails with sparse matrix","description":"According to the docs CategoricalNB requires X to be \"array-like, sparse matrix}, shape = [n_samples, n_features]\". However if I try to fit with a sparse matrix I get an error saying it has to be dense. It works fine passing a sparse matrix to fit a MultinomialNB model:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-19-afb2039f4bc4> in <module>\r\n      1 from sklearn.naive_bayes import CategoricalNB\r\n      2 m = CategoricalNB()\r\n----> 3 m.fit(x, y)\r\n      4 preds = m.predict(x)\r\n      5 m.score(x, y)\r\n\r\n~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py in fit(self, X, y, sample_weight)\r\n   1101         self : object\r\n   1102         \"\"\"\r\n-> 1103         return super().fit(X, y, sample_weight=sample_weight)\r\n   1104 \r\n   1105     def partial_fit(self, X, y, classes=None, sample_weight=None):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py in fit(self, X, y, sample_weight)\r\n    607         self : object\r\n    608         \"\"\"\r\n--> 609         X, y = self._check_X_y(X, y)\r\n    610         _, n_features = X.shape\r\n    611         self.n_features_ = n_features\r\n\r\n~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py in _check_X_y(self, X, y)\r\n   1161         # X, y = check_array(X, y, dtype='int', accept_sparse=False,\r\n   1162         #                    force_all_finite=True)\r\n-> 1163         X, y = check_X_y(X, y, accept_sparse=False, force_all_finite=True)\r\n   1164         X, y = check_X_y(X, y, dtype='int')\r\n   1165         if np.any(X < 0):\r\n\r\n~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\r\n    753                     ensure_min_features=ensure_min_features,\r\n    754                     warn_on_dtype=warn_on_dtype,\r\n--> 755                     estimator=estimator)\r\n    756     if multi_output:\r\n    757         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\r\n\r\n~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\r\n    509                                       dtype=dtype, copy=copy,\r\n    510                                       force_all_finite=force_all_finite,\r\n--> 511                                       accept_large_sparse=accept_large_sparse)\r\n    512     else:\r\n    513         # If np.array(..) gives ComplexWarning, then we convert the warning\r\n\r\n~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py in _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\r\n    304 \r\n    305     if accept_sparse is False:\r\n--> 306         raise TypeError('A sparse matrix was passed, but dense '\r\n    307                         'data is required. Use X.toarray() to '\r\n    308                         'convert to a dense numpy array.')\r\n\r\nTypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.","labels":["Bug","module:naive_bayes"],"created_at":"2020-02-27T11:38:27Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16561"},{"issue_number":197,"repository":"scikit-learn\/scikit-learn","title":"LinearSVC multiclass is suboptimal compared to voting","description":"<!--\r\nBefore submitting a bug, please make sure the issue hasn't been already\r\naddressed by searching through the past issues.\r\n-->\r\n\r\n#### Description\r\nLinearSVC with multiclass data seems to do worse than creating multiple binary classifiers and voting using `decision_function()`. Since the `multiclass` default is `ovr`, this is probably not expected behavior.\r\n\r\n\r\n#### Steps\/Code to Reproduce\r\nAn [MWE](https:\/\/gist.github.com\/abhishek-ghose\/ff52ac298c82b70c6562bf7a29f3ef76) that demonstrates custom fitting of multiple `LinearSVC`and voting. This uses a toy dataset, where the data for a class is a vertical stripe. The code snippet generates the dataset.\r\n\r\n\r\nThe code tries the `LinearSVC()` as-is and compares it to a *custom* implementation - shown by functions `custom_ovr_fit()` and `custom_ovr_predict()`. \r\n\r\n#### Expected Results\r\nThere should be minor differences between the results of the two approaches.\r\n\r\n#### Actual Results\r\n\r\nI see something like the following on my machine, but results might vary across machines (I've tried, this is possibly because of different versions) - however the difference in the two approaches is always substantial (its `76.31%` below, but on other machines the gap closes to `~50%`.):\r\n\r\n```\r\n========Comparison summary========\r\n\tFor c=0.25000, scikit F1=0.47, custom F1=0.83, pct. improve. over scikit = 76.94%\r\n\tFor c=0.50000, scikit F1=0.48, custom F1=0.84, pct. improve. over scikit = 76.07%\r\n\tFor c=1.00000, scikit F1=0.49, custom F1=0.87, pct. improve. over scikit = 76.31%\r\n\tFor c=2.00000, scikit F1=0.49, custom F1=0.72, pct. improve. over scikit = 46.08%\r\n\tFor c=4.00000, scikit F1=0.41, custom F1=0.53, pct. improve. over scikit = 29.29%\r\n\r\n\t**scikit: best param=1.00000, score=0.49\r\n\t**custom: best param=1.00000, score=0.87\r\n\t**pct. improve. over scikit's best score = 76.31%\r\n```\r\n\r\nI am using the `F1-macro` score. Not only are the values for a given `C` often not close, but the best score, across the parameter search space - represented by `pct. improve. over scikit's best score` - is drastically different.\r\n\r\nAs one might expect, this is dataset dependent. For ex on the `digits` dataset I did not see much of difference, but on the `wine` dataset, I see a gap of `~11%`. \r\n\r\n\r\n#### Versions\r\nI don't have `imblearn` installed (please let me know if this is necessary) - here's the rest of the info.\r\n```\r\nWindows-10-10.0.17763-SP0\r\nPython 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\r\nNumPy 1.17.1\r\nSciPy 1.1.0\r\nScikit-Learn 0.19.1\r\n```","labels":["Bug","module:svm"],"created_at":"2020-02-24T22:46:20Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16538"},{"issue_number":198,"repository":"scikit-learn\/scikit-learn","title":"Decision Tree probabilities with balanced class weight","description":"<!--\r\nBefore submitting a bug, please make sure the issue hasn't been already\r\naddressed by searching through the past issues.\r\n-->\r\n\r\n#### Describe the bug\r\nI'm not sure if this is a bug or expected behaviour, but it's something that tripped me up and I thought I'd ask about it. The issue comes from fitting a decision tree model with `class_weight = 'balanced'`.\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\n```\r\nimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn import tree\r\nfrom sklearn.datasets import load_breast_cancer\r\n\r\n# Load data\r\ndata = load_breast_cancer()\r\nX = data.data\r\ny = data.target\r\n\r\n# Build Decision Tree\r\ndt = tree.DecisionTreeClassifier(\r\n    max_depth = 1,\r\n    class_weight = 'balanced',\r\n    random_state = 0\r\n).fit(\r\n    X = X,\r\n    y = y\r\n)\r\n\r\n# Output decision tree nodes and probabilities on data\r\nnode = dt.apply(X)\r\noutput_prob = dt.predict_proba(X)[:, 1]\r\nresults = pd.DataFrame({'y': y, 'node': node, 'output_prob': output_prob})\r\n\r\n# Compare output node probabilities with calculated probabilities\r\ndef f(x):\r\n    d = {}\r\n    d['output_prob'] = x['output_prob'].max()\r\n    d['calc_prob'] = sum(x['y']) \/ x['y'].count()\r\n    return pd.Series(d)\r\n\r\ncomparison = results.groupby('node').apply(f)\r\n```\r\n\r\n#### Expected Results\r\nI would expect that the probabilities output from `predict_proba` to match the calculated probabilities. \r\n\r\n#### Actual Results\r\nThe output from `predict_proba` does not match the calculated probabilities. I suspect what's happening is that the probabilities being output are calculated from the input data _after_ it's been balanced, and not from the input data itself. \r\n\r\nEven though the classes have been balanced during the building of the model, I wouldn't expect these balanced weightings to be used in the probabilities being generated, although maybe that's a problem with my expectations \ud83d\ude04. This showed up when I used the probabilities to calculate the expected versus actual target rate.\r\n\r\nIf this _is_ expected behaviour, then maybe it would be useful to have a parameter in the `predict_proba` function to allow someone to choose to output the probabilities either pre or post balancing?\r\n\r\n#### Versions\r\nSystem:\r\n    python: 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]\r\nexecutable: \/home\/ec2-user\/anaconda3\/envs\/test\/bin\/python\r\n   machine: Linux-4.14.165-102.185.amzn1.x86_64-x86_64-with-glibc2.10\r\n\r\nPython deps:\r\n       pip: 19.3.1\r\nsetuptools: 42.0.2.post20191201\r\n   sklearn: 0.21.3\r\n     numpy: 1.17.3\r\n     scipy: 1.3.2\r\n    Cython: 0.29.14\r\n    pandas: 0.25.3","labels":["Bug","module:tree"],"created_at":"2020-02-19T10:38:15Z","comments":4,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16479"},{"issue_number":199,"repository":"scikit-learn\/scikit-learn","title":"Support of passthrough=True in Stacking when X a dataframe of mixed type","description":"In `StackingClassifier` and `StackingRegressor`, we added an option `passthrough=True\/False` which will concatenate the internal predictions of the first layer model with the original dataset. While everything is going if `X` is only numerical, things start to be complicated when we are dealing with mixed types and dataframe.\r\n\r\nLet's illustrate the issue by tacking the Titanic dataset:\r\n\r\n## Workflow\r\n\r\n### Some imports\r\n\r\n```python\r\nimport numpy as np\r\nfrom pandas.api.types import CategoricalDtype\r\n\r\nfrom sklearn.base import clone\r\nfrom sklearn.compose import make_column_selector as selector\r\nfrom sklearn.compose import make_column_transformer\r\nfrom sklearn.datasets import fetch_openml\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.ensemble import StackingClassifier\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.linear_model import RidgeClassifierCV\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn.impute import SimpleImputer\r\nfrom sklearn.pipeline import make_pipeline\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nfrom sklearn.preprocessing import OrdinalEncoder\r\n\r\nnp.random.seed(0)\r\n```\r\n\r\n### Load the dataset\r\n\r\n```python\r\nX, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\r\nsubset_feature = ['embarked', 'sex', 'pclass', 'age', 'fare']\r\nX = X[subset_feature]\r\n\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\r\n```\r\n\r\n### Define first-layer learners\r\n\r\nSince we are dealing with mixed-types, the first-layer learners need to define some preprocessor.\r\nHere, we will stack a linear model and a tree-based model and then we defined 2 types of preprocessors.\r\n\r\n```python\r\ngradient_based_processor = make_column_transformer(\r\n    (make_pipeline(StandardScaler(), SimpleImputer(strategy=\"median\")),\r\n     selector(dtype_exclude=CategoricalDtype)),\r\n    (make_pipeline(SimpleImputer(strategy='constant', fill_value='missing'),\r\n                   OneHotEncoder(handle_unknown='ignore')),\r\n     selector(dtype_include=CategoricalDtype))\r\n)\r\n\r\ncategories = [\r\n    X[col].dtype.categories.tolist() + [\"missing\"] if X[col].isnull().any()\r\n    else X[col].dtype.categories.tolist()\r\n    for col in selector(dtype_include=CategoricalDtype)(X)\r\n]\r\n\r\ntree_based_processor = make_column_transformer(\r\n    (make_pipeline(SimpleImputer(strategy='constant', fill_value='missing'),\r\n                   OrdinalEncoder(categories=categories)),\r\n     selector(dtype_include=CategoricalDtype))\r\n)\r\n\r\nrf = make_pipeline(clone(tree_based_processor), RandomForestClassifier())\r\nlr = make_pipeline(clone(gradient_based_processor), LogisticRegression())\r\n\r\n# sanity check\r\nprint(lr.fit(X_train, y_train).score(X_test, y_test))\r\nprint(rf.fit(X_train, y_train).score(X_test, y_test))\r\n```\r\n```\r\n0.8048780487804879\r\n0.8048780487804879\r\n```\r\n\r\nUp to now, we have a standard learner. Let's try to stack them\r\n\r\n### Only stack the predictions of the first-layer models\r\n\r\nLet's use some stacking. By default, we will stack the predictions of the first-layer models\r\n\r\n```python\r\nnumerical_ridge = RidgeClassifierCV()\r\nmodel = StackingClassifier(\r\n    estimators=[(\"lr\", lr), (\"rf\", rf)],\r\n    final_estimator=numerical_ridge,\r\n    passthrough=False\r\n)\r\nprint(model.fit(X_train, y_train).score(X_test, y_test))\r\n```\r\n```\r\n0.8048780487804879\r\n```\r\n\r\nWe don't have any issues since the predictions are only numerical. The issue starts if we want to concatenate the original `X`\r\n\r\n### Concatenate `X` with the predictions of the first-layer models\r\n\r\nIf we try directly to pass through `X` we get an issue since the ridge classifier did not encode the data in `X`:\r\n\r\n```python\r\nmodel = StackingClassifier(\r\n    estimators=[(\"lr\", lr), (\"rf\", rf)],\r\n    final_estimator=numerical_ridge,\r\n    passthrough=True\r\n)\r\ntry:\r\n    print(model.fit(X_train, y_train).score(X_test, y_test))\r\nexcept Exception as e:\r\n    print(e)\r\n```\r\n```\r\ncould not convert string to float: 'S'\r\n```\r\n\r\nThus, one has to create a pipeline to deal with the data:\r\n\r\n```python\r\nridge = make_pipeline(clone(gradient_based_processor), RidgeClassifierCV())\r\nridge.named_steps[\"columntransformer\"].set_params(remainder=\"passthrough\")\r\n\r\nmodel = StackingClassifier(\r\n    estimators=[(\"lr\", lr), (\"rf\", rf)],\r\n    final_estimator=ridge,\r\n    passthrough=True\r\n)\r\ntry:\r\n    print(model.fit(X_train, y_train).score(X_test, y_test))\r\nexcept Exception as e:\r\n    print(e)\r\n```\r\n```\r\nmake_column_selector can only be applied to pandas dataframes\r\n```\r\n\r\nSo here we actually have the following issue:\r\n\r\n* the concatenation does not preserve dataframe. This could be solved by stacking all the data to `X` (which should manage without importing `pandas`). However, it means that we might want to generate some **feature names** for the predictions generated by the first-layers. So this is my first question, what shall we do?\r\n* if the above issue can be fixed, then we start to have another issue. When creating the pipeline, the user would need to specifically state to let pass through the remainder in the final estimator. Otherwise, the prediction columns will be dropped. So, we might want to interfere with the column transformer of the last steps to be sure to pass through the predictions. But API I don't know what is the best way to do so?\r\n\r\nping @jnothman @qinhanmin2014 @thomasjpfan \r\n\r\nSorry for the long narration.","labels":["Bug","New Feature","API"],"created_at":"2020-02-18T18:46:38Z","comments":2,"reactions":7,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16473"},{"issue_number":200,"repository":"scikit-learn\/scikit-learn","title":"mutual_info_regression is missing a factor?","description":"```\r\nfrom pylab import *\r\n\r\nfrom sklearn.feature_selection import mutual_info_regression\r\n\r\nmi = dict()\r\nfor n in np.logspace(2, 5, 10):\r\n    n = int(n)\r\n    x = np.random.randn(n)[:, None]\r\n    mi[n] = mutual_info_regression(x, x.squeeze(), n_neighbors=5).item()\r\nmi = pd.Series(mi)\r\nmi.name = 'mi'\r\nmi.index.names = ['n']\r\nmi = mi.reset_index()\r\n\r\nfigure()\r\nplot(mi.n, mi.mi, 'o')\r\nxscale('log')\r\ngrid()\r\nfigure()\r\nplot(mi.n, mi.mi - np.log(mi.n), 'o')\r\nxscale('log')\r\nylim(0, ylim()[1] * 2)\r\ngrid()\r\n```\r\n\r\n![image](https:\/\/user-images.githubusercontent.com\/223276\/74650392-873c7380-5179-11ea-95d4-34d8c004bd08.png)\r\n","labels":["Bug","module:feature_selection"],"created_at":"2020-02-17T11:35:02Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16457"},{"issue_number":201,"repository":"scikit-learn\/scikit-learn","title":"HistGradientBoostingClassifier slow in prediction mode","description":"While HistGradientBoostingClassifier is 100 faster than GradientBoostingClassifier when fitting the model, I found it to be very slow in case of predicting the class probabilities, in my case about 100 times slower :-(\r\n\r\nFor example: \r\nGradientBoostingClassifier: 3.2 min for training for 1 million examples. 32 ms for 1000 predictions.\r\nHistGradientBoostingClassifier: 7s for training. 1s for 1000 predictions","labels":["Bug","Performance","module:ensemble"],"created_at":"2020-02-11T15:02:02Z","comments":13,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16429"},{"issue_number":202,"repository":"scikit-learn\/scikit-learn","title":"Unable to Take Cartesian Products of Many Arrays","description":"\r\n#### Describe the bug\r\nWhen trying to run cartesian from sklearn.utils.extmath on more 32 arrays, the module crashes due to np.indices functioning rejecting the input shape in the source code.\r\n\r\n#### Steps\/Code to Reproduce\r\ncartesian(([(1, 2), (3, 4)], [(5, 6), (7, 8)], [(9, 10), (7, 11)], [(3, 12), (13, 14)], [(9, 15), (3, 16)], [(17, 18), (3, 19)], [(1, 20), (21, 22)], [(21, 23), (3, 24)], [(25, 26), (27, 28)], [(21, 29), (3, 30)], [(9, 31), (27, 32)], [(9, 33), (1, 34)], [(21, 35), (17, 36)], [(13, 37), (7, 38)], [(25, 39), (7, 40)], [(1, 41), (27, 42)], [(21, 43), (17, 44)], [(17, 45), (3, 46)], [(25, 47), (17, 48)], [(21, 49), (17, 50)], [(5, 51), (13, 52)], [(1, 53), (7, 54)], [(25, 55), (13, 56)], [(5, 57), (7, 58)], [(9, 59), (1, 60)], [(25, 61), (5, 62)], [(3, 63), (27, 64)], [(25, 65), (7, 66)], [(1, 67), (27, 68)], [(27, 69), (7, 70)], [(21, 71), (17, 72)], [(17, 73), (5, 74)], [(9, 75), (21, 76)], [(21, 77), (13, 78)], [(25, 79), (3, 80)], [(9, 81), (25, 82)], [(9, 83), (7, 84)], [(17, 85), (27, 86)], [(5, 87), (27, 88)], [(25, 89), (13, 90)]))\r\n\r\nExample:\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation\r\ndocs = [\"Help I have a bug\" for i in range(1000)]\r\nvectorizer = CountVectorizer(input=docs, analyzer='word')\r\nlda_features = vectorizer.fit_transform(docs)\r\nlda_model = LatentDirichletAllocation(\r\n    n_topics=10,\r\n    learning_method='online',\r\n    evaluate_every=10,\r\n    n_jobs=4,\r\n)\r\nmodel = lda_model.fit(lda_features)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https:\/\/gist.github.com\r\n-->\r\n\r\n```\r\nSample code to reproduce the problem\r\n```\r\n\r\n#### Expected Results\r\n<!-- Example: No error is thrown. Please paste or describe the expected results.-->\r\nThe cartesian product of the lists.\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback. -->\r\n\r\nTraceback (most recent call last):\r\n  File \"\/home\/hlc5v\/.conda\/envs\/graph_env\/lib\/python3.6\/runpy.py\", line 193, in _run_module_as_main\r\n    \"__main__\", mod_spec)\r\n  File \"\/home\/hlc5v\/.conda\/envs\/graph_env\/lib\/python3.6\/runpy.py\", line 85, in _run_code\r\n    exec(code, run_globals)\r\n  File \"\/home\/hlc5v\/.conda\/envs\/graph_env\/lib\/python3.6\/cProfile.py\", line 160, in <module>\r\n    main()\r\n  File \"\/home\/hlc5v\/.conda\/envs\/graph_env\/lib\/python3.6\/cProfile.py\", line 153, in main\r\n    runctx(code, globs, None, options.outfile, options.sort)\r\n  File \"\/home\/hlc5v\/.conda\/envs\/graph_env\/lib\/python3.6\/cProfile.py\", line 20, in runctx\r\n    filename, sort)\r\n  File \"\/home\/hlc5v\/.conda\/envs\/graph_env\/lib\/python3.6\/profile.py\", line 64, in runctx\r\n    prof.runctx(statement, globals, locals)\r\n  File \"\/home\/hlc5v\/.conda\/envs\/graph_env\/lib\/python3.6\/cProfile.py\", line 100, in runctx\r\n    exec(cmd, globals, locals)\r\n  File \"..\/..\/DNF_Approx\/GraphConvert\/graphtodnf.py\", line 191, in <module>\r\n    main()\r\n  File \"..\/..\/DNF_Approx\/GraphConvert\/graphtodnf.py\", line 176, in main\r\n    pi, vars, weights = createPathSet(curr, infected, args.susceptible[j], t=int(args.time_steps))\r\n  File \"..\/..\/DNF_Approx\/GraphConvert\/graphtodnf.py\", line 103, in createPathSet\r\n    PI = cartesian_product(*pathSet)\r\n  File \"..\/..\/DNF_Approx\/GraphConvert\/graphtodnf.py\", line 113, in cartesian_product\r\n    arr = np.empty([len(a) for a in arrays] + [la], dtype=tuple)\r\nValueError: sequence too large; cannot be greater than 32\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nFor scikit-learn >= 0.20:\r\nimport sklearn; sklearn.show_versions()\r\nFor scikit-learn < 0.20:\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\nimport imblearn; print(\"Imbalanced-Learn\", imblearn.__version__)\r\n-->\r\nSystem:\r\n    python: 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018, 14:57:15) [MSC v.1915 64 bit (AMD64)]\r\nexecutable: C:\\Users\\hcars\\PycharmProjects\\DNF_git\\GraphForecastApprox\\venv\\Scripts\\python.exe\r\n   machine: Windows-10-10.0.17763-SP0\r\nPython dependencies:\r\n       pip: 19.0.3\r\nsetuptools: 40.8.0\r\n   sklearn: 0.22.1\r\n     numpy: 1.17.2\r\n     scipy: 1.4.1\r\n    Cython: None\r\n    pandas: 0.25.1\r\nmatplotlib: 3.1.1\r\n    joblib: 0.14.1\r\nBuilt with OpenMP: True\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","module:utils"],"created_at":"2020-01-31T16:15:39Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16350"},{"issue_number":203,"repository":"scikit-learn\/scikit-learn","title":"In PassiveAggressiveClassifier, inconsistency between the parameter \"loss\" and the attribute \"loss_function_\"","description":"#### Describe the issue linked to the documentation\r\nThe two possible loss values for the PassiveAggressiveClassifier are \"hinge\" and \"squared_hinge\", but in both cases, the classifier attribute \"loss_function_\" is the hinge loss, and only the learning rate is adapted. \r\n``` python\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn import datasets\r\n\r\nfrom sklearn.linear_model import PassiveAggressiveClassifier\r\n\r\nheldout = [0.95, 0.90, 0.75, 0.50, 0.01]\r\nrounds = 20\r\nX, y = datasets.load_digits(return_X_y=True)\r\n\r\nclassifiers = [\r\n    (\"Passive-Aggressive I\", PassiveAggressiveClassifier(loss='hinge',\r\n                                                         C=1.0, tol=1e-4)),\r\n    (\"Passive-Aggressive II\", PassiveAggressiveClassifier(loss='squared_hinge',\r\n                                                          C=1.0, tol=1e-4))\r\n]\r\n\r\nxx = 1. - np.array(heldout)\r\n\r\nfor name, clf in classifiers:\r\n    print(\"training %s\" % name)\r\n    rng = np.random.RandomState(42)\r\n    yy = []\r\n    for i in heldout:\r\n        yy_ = []\r\n        for r in range(rounds):\r\n            X_train, X_test, y_train, y_test = \\\r\n                train_test_split(X, y, test_size=i, random_state=rng)\r\n            clf.fit(X_train, y_train)\r\n            y_pred = clf.predict(X_test)\r\n            yy_.append(1 - np.mean(y_pred == y_test))\r\n       print(clf.loss_function_)\r\n```\r\nreturns\r\n```\r\ntraining Passive-Aggressive I\r\n<sklearn.linear_model._sgd_fast.Hinge object at 0x1172efb30>\r\ntraining Passive-Aggressive II\r\n<sklearn.linear_model._sgd_fast.Hinge object at 0x1172efcd0>\r\n```\r\n\r\n\r\n\r\n#### Suggest a potential alternative\/fix\r\nshouldn't the signature of the classifier reflect the implemented algorithm, and allow the user to set the learning_rate parameter instead of the loss?\r\nIndeed, the user set loss parameter is not used, and loss=\"hinge\" is hardcoded instead of loss=self.loss ([here](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/40f668c03891caaeda231279fe12cbb95b198cbe\/sklearn\/linear_model\/_passive_aggressive.py#L251) and [here](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/40f668c03891caaeda231279fe12cbb95b198cbe\/sklearn\/linear_model\/_passive_aggressive.py#L223))","labels":["Bug","Documentation","module:linear_model"],"created_at":"2020-01-29T10:16:48Z","comments":1,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16270"},{"issue_number":204,"repository":"scikit-learn\/scikit-learn","title":"TSNE implementation finds variation where there is none","description":"#### Describe the bug\r\nManifold.TSNE gives back a lower-dimensional representation of the input data, which contains variation, even if the input data only contains zeros (it seems to do this in all cases where all input samples are the same).\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.manifold import TSNE\r\n\r\nf = TSNE(n_components=2)\r\n\r\nzero_array = np.zeros((993, 20))\r\ntsne_array = f.fit_transform(zero_array)\r\n\r\n```\r\n\r\n#### Expected Results\r\nA warning, or all data mapped to the same point.\r\n\r\n#### Actual Results\r\n\r\nA lower-dimensional array (here (993, 2) shape) with different vectors!\r\n```python\r\n[[-0.21821421 -0.16358966]\r\n [-0.21803701 -0.16340736]\r\n [-0.21847947 -0.16356741]\r\n ...\r\n [-0.07215934  0.15475279]\r\n [-0.4158346   0.2995374 ]\r\n [-0.21808246 -0.16424587]]\r\n```\r\n\r\n#### Versions\r\n\r\nPython 3.7.5\r\nNumPy 1.17.4\r\nSciPy 1.3.2\r\nScikit-Learn 0.21.3\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","module:manifold"],"created_at":"2020-01-26T12:31:23Z","comments":6,"reactions":3,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16238"},{"issue_number":205,"repository":"scikit-learn\/scikit-learn","title":"n_jobs greater than 1 causes segmentation fault","description":"#### Describe the bug\r\nSetting n_jobs for `sklearn.manifold.TSNE` to anything greater than 1 (including -1) causes a segmentation fault\r\n\r\n#### Steps\/Code to Reproduce\r\n<!--\r\nPlease add a minimal example that we can reproduce the error by running the\r\ncode. Be as succinct as possible, do not depend on external data. In short, we\r\nare going to copy-paste your code and we expect to get the same\r\nresult as you. -->\r\n\r\nExample:\r\n```python\r\nimport numpy as np\r\nfrom sklearn.manifold import TSNE\r\nTSNE(n_components=2,n_jobs=1).fit_transform(np.zeros((100,512)) # Runs fine\r\nTSNE(n_components=2,n_jobs=2).fit_transform(np.zeros((100,512)) # Segmentation fault\r\n```\r\n\r\n#### Expected Results\r\n<!-- Example: No error is thrown. Please paste or describe the expected results.-->\r\nWould expect it to work with any amount of `n_jobs`\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback. -->\r\nSegmentation fault\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nFor scikit-learn >= 0.22.1:\r\nimport sklearn; sklearn.show_versions()\r\nFor scikit-learn < 0.20:\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\nimport imblearn; print(\"Imbalanced-Learn\", imblearn.__version__)\r\n-->\r\nSystem:\r\n    python: 3.7.3 (default, Mar 26 2019, 01:59:45)  [GCC 5.4.0 20160609]\r\nexecutable: \/home\/<ME>\/.virtualenvs\/reid\/bin\/python3.7\r\n   machine: Linux-4.15.0-39-generic-x86_64-with-Ubuntu-16.04-xenial\r\n```\r\nPython dependencies:\r\n       pip: 20.0.1\r\nsetuptools: 41.0.1\r\n   sklearn: 0.22.1\r\n     numpy: 1.16.3\r\n     scipy: 1.2.1\r\n    Cython: 0.29.12\r\n    pandas: 0.25.1\r\nmatplotlib: 3.0.3\r\n    joblib: 0.13.2\r\n\r\nBuilt with OpenMP: True\r\n```\r\nN.B. Could very well be related to #12922 but I'm not sure.\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","module:manifold","segfault"],"created_at":"2020-01-23T14:03:05Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16184"},{"issue_number":206,"repository":"scikit-learn\/scikit-learn","title":"PERF predict_proba is slow when n_jobs > 1 for random forests","description":"The use of `n_jobs > 1` for small batch can slow down the prediction for forest models. This is probably due to the overhead incurred by using `joblib` (create thread, check system info, ...) which is dominate the runtime compared to the computations. This was reported originally in joblib\/joblib#982.\r\n\r\nA couple of ideas to solve this:\r\n- Set `n_jobs=1` when the size of the batch is small.\r\n- Introduce a `n_jobs_predict` parameters that would default to `1\/n_jobs` for forests but that can be set separately.\r\n\r\n","labels":["Bug","module:ensemble"],"created_at":"2020-01-17T10:37:50Z","comments":0,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16143"},{"issue_number":207,"repository":"scikit-learn\/scikit-learn","title":"Wrongly implemented test in RidgeCV","description":"The following test does not actually test anything:\r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/bdf1ae9e4a739abf06076e67dc453774ffc886a7\/sklearn\/linear_model\/tests\/test_ridge.py#L478-L502\r\n\r\n`gcv_ridge` will compute the error by aggregating all predictions and compute the `explained_variance` on all predictions.\r\n\r\nHowever, `loo_ridge` will make a `GridSearchCV` which will compute the score for each individual sample (because of a LOO CV) and then report the mean. However, the explained variance will always be 1.0 since we are computing the variance on a single sample.\r\n\r\nSo the test is passing by chance.","labels":["Bug","Moderate","help wanted","module:linear_model"],"created_at":"2020-01-07T17:24:00Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16041"},{"issue_number":208,"repository":"scikit-learn\/scikit-learn","title":"TfidfVectorizer ngrams does not work when vocabulary provided","description":"<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https:\/\/stackoverflow.com\/questions\/tagged\/scikit-learn\r\n- Mailing List: https:\/\/mail.python.org\/mailman\/listinfo\/scikit-learn\r\nFor more information, see User Questions: http:\/\/scikit-learn.org\/stable\/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n\r\nThe `TfidfVectorizer` does not honor the `ngram_range` argument when the `vocabulary` is provided.\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\nExample 1, vocabulary is *not* provided, this works as expected:\r\n\r\n```python\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\n\r\nX = ['abc',\r\n     'bcd',\r\n     'cde']\r\n\r\ntfidf = TfidfVectorizer(stop_words=None,\r\n                        analyzer='char',\r\n                        ngram_range=(2, 2))\r\n\r\nsps = tfidf.fit_transform(X)\r\nprint(tfidf.get_feature_names())\r\n# ['ab', 'bc', 'cd', 'de']\r\n```\r\n\r\nExample 2, when vocabulary is provided. This does *not* work as expected:\r\n\r\n```python\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\n\r\nX = ['abc',\r\n     'bcd',\r\n     'cde']\r\n\r\ntfidf = TfidfVectorizer(stop_words=None,\r\n                        analyzer='char',\r\n                        vocabulary={'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4},\r\n                        ngram_range=(2, 2))\r\n\r\nsps = tfidf.fit_transform(X)\r\nprint(tfidf.get_feature_names())\r\n# ['a', 'b', 'c', 'd', 'e']\r\n```\r\n\r\nNote that it works if the vocabulary I provide are the ngrams themselves:\r\n\r\n```python\r\nfrom sklearn.feature_extraction.text import TfidfVectorizer\r\n\r\nX = ['abc',\r\n     'bcd',\r\n     'cde']\r\n\r\ntfidf = TfidfVectorizer(stop_words=None,\r\n                        analyzer='char',\r\n                        vocabulary=['ab', 'bc', 'cd', 'de'],\r\n                        ngram_range=(2, 2))\r\n\r\nsps = tfidf.fit_transform(X)\r\nprint(tfidf.get_feature_names())\r\n# ['ab', 'bc', 'cd', 'de']\r\n```\r\n\r\nBut that seems kind of silly, since I can't possibly know all of the ngrams a priori for a large dataset.\r\n\r\n#### Expected Results\r\n\r\nExpected to still get ngrams when vocabulary is provided, but did not.\r\n\r\n#### Actual Results\r\n\r\nSee steps to reproduce above.\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nFor scikit-learn >= 0.20:\r\nimport sklearn; sklearn.show_versions()\r\nFor scikit-learn < 0.20:\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\n\r\n```\r\nSystem:\r\n    python: 3.7.5 (default, Oct 25 2019, 10:52:18)  [Clang 4.0.1 (tags\/RELEASE_401\/final)]\r\nexecutable: \/anaconda3\/envs\/myenv\/bin\/python\r\n   machine: Darwin-18.6.0-x86_64-i386-64bit\r\n\r\nPython dependencies:\r\n       pip: 19.3.1\r\nsetuptools: 42.0.2.post20191203\r\n   sklearn: 0.22\r\n     numpy: 1.17.4\r\n     scipy: 1.4.0\r\n    Cython: 0.29.14\r\n    pandas: 0.25.3\r\nmatplotlib: 3.1.2\r\n    joblib: 0.14.1\r\n\r\nBuilt with OpenMP: True\r\n```\r\n\r\n\r\n<!-- Thanks for contributing! -->","labels":["Bug","help wanted","module:feature_extraction"],"created_at":"2020-01-03T19:01:44Z","comments":9,"reactions":2,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/16017"},{"issue_number":209,"repository":"scikit-learn\/scikit-learn","title":"RANSACRegressor should use weighted loss functions","description":"The current loss functions disregard weights.\r\n\r\nSee #14325","labels":["Bug","module:linear_model"],"created_at":"2019-12-08T21:44:39Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15836"},{"issue_number":210,"repository":"scikit-learn\/scikit-learn","title":"test_uniform_grid[barnes_hut] can fail on aarch64","description":"```\r\n________________________ test_uniform_grid[barnes_hut] _________________________\r\n\r\nmethod = 'barnes_hut'\r\n\r\n    @pytest.mark.parametrize('method', ['barnes_hut', 'exact'])\r\n    def test_uniform_grid(method):\r\n        \"\"\"Make sure that TSNE can approximately recover a uniform 2D grid\r\n    \r\n        Due to ties in distances between point in X_2d_grid, this test is platform\r\n        dependent for ``method='barnes_hut'`` due to numerical imprecision.\r\n    \r\n        Also, t-SNE is not assured to converge to the right solution because bad\r\n        initialization can lead to convergence to bad local minimum (the\r\n        optimization problem is non-convex). To avoid breaking the test too often,\r\n        we re-run t-SNE from the final point when the convergence is not good\r\n        enough.\r\n        \"\"\"\r\n        seeds = [0, 1, 2]\r\n        n_iter = 500\r\n        for seed in seeds:\r\n            tsne = TSNE(n_components=2, init='random', random_state=seed,\r\n                        perplexity=20, n_iter=n_iter, method=method)\r\n            Y = tsne.fit_transform(X_2d_grid)\r\n    \r\n            try_name = \"{}_{}\".format(method, seed)\r\n            try:\r\n>               assert_uniform_grid(Y, try_name)\r\n\r\n..\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold\/lib\/python3.8\/site-packages\/sklearn\/manifold\/tests\/test_t_sne.py:784: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nY = array([[ 52.326397  , -15.92225   ],\r\n       [ 46.679527  , -20.175953  ],\r\n       [ 40.870537  , -24.181147  ],\r\n       ...[-35.291374  ,  22.122814  ],\r\n       [-42.2738    ,  18.793724  ],\r\n       [-48.922283  ,  15.606232  ]], dtype=float32)\r\ntry_name = 'barnes_hut_1'\r\n\r\n    def assert_uniform_grid(Y, try_name=None):\r\n        # Ensure that the resulting embedding leads to approximately\r\n        # uniformly spaced points: the distance to the closest neighbors\r\n        # should be non-zero and approximately constant.\r\n        nn = NearestNeighbors(n_neighbors=1).fit(Y)\r\n        dist_to_nn = nn.kneighbors(return_distance=True)[0].ravel()\r\n        assert dist_to_nn.min() > 0.1\r\n    \r\n        smallest_to_mean = dist_to_nn.min() \/ np.mean(dist_to_nn)\r\n        largest_to_mean = dist_to_nn.max() \/ np.mean(dist_to_nn)\r\n    \r\n        assert smallest_to_mean > .5, try_name\r\n>       assert largest_to_mean < 2, try_name\r\nE       AssertionError: barnes_hut_1\r\nE       assert 6.67359409617653 < 2\r\n\r\n..\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold\/lib\/python3.8\/site-packages\/sklearn\/manifold\/tests\/test_t_sne.py:807: AssertionError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nmethod = 'barnes_hut'\r\n\r\n    @pytest.mark.parametrize('method', ['barnes_hut', 'exact'])\r\n    def test_uniform_grid(method):\r\n        \"\"\"Make sure that TSNE can approximately recover a uniform 2D grid\r\n    \r\n        Due to ties in distances between point in X_2d_grid, this test is platform\r\n        dependent for ``method='barnes_hut'`` due to numerical imprecision.\r\n    \r\n        Also, t-SNE is not assured to converge to the right solution because bad\r\n        initialization can lead to convergence to bad local minimum (the\r\n        optimization problem is non-convex). To avoid breaking the test too often,\r\n        we re-run t-SNE from the final point when the convergence is not good\r\n        enough.\r\n        \"\"\"\r\n        seeds = [0, 1, 2]\r\n        n_iter = 500\r\n        for seed in seeds:\r\n            tsne = TSNE(n_components=2, init='random', random_state=seed,\r\n                        perplexity=20, n_iter=n_iter, method=method)\r\n            Y = tsne.fit_transform(X_2d_grid)\r\n    \r\n            try_name = \"{}_{}\".format(method, seed)\r\n            try:\r\n                assert_uniform_grid(Y, try_name)\r\n            except AssertionError:\r\n                # If the test fails a first time, re-run with init=Y to see if\r\n                # this was caused by a bad initialization. Note that this will\r\n                # also run an early_exaggeration step.\r\n                try_name += \":rerun\"\r\n                tsne.init = Y\r\n                Y = tsne.fit_transform(X_2d_grid)\r\n>               assert_uniform_grid(Y, try_name)\r\n\r\n..\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold\/lib\/python3.8\/site-packages\/sklearn\/manifold\/tests\/test_t_sne.py:792: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nY = array([[-18.169476  ,   6.0802336 ],\r\n       [-18.278513  ,   2.8822129 ],\r\n       [-18.671782  ,  -0.4646889 ],\r\n       ...[ 22.550077  ,  19.698557  ],\r\n       [ 21.399723  ,  22.933178  ],\r\n       [ 16.22136   ,  28.22955   ]], dtype=float32)\r\ntry_name = 'barnes_hut_1:rerun'\r\n\r\n    def assert_uniform_grid(Y, try_name=None):\r\n        # Ensure that the resulting embedding leads to approximately\r\n        # uniformly spaced points: the distance to the closest neighbors\r\n        # should be non-zero and approximately constant.\r\n        nn = NearestNeighbors(n_neighbors=1).fit(Y)\r\n        dist_to_nn = nn.kneighbors(return_distance=True)[0].ravel()\r\n        assert dist_to_nn.min() > 0.1\r\n    \r\n        smallest_to_mean = dist_to_nn.min() \/ np.mean(dist_to_nn)\r\n        largest_to_mean = dist_to_nn.max() \/ np.mean(dist_to_nn)\r\n    \r\n        assert smallest_to_mean > .5, try_name\r\n>       assert largest_to_mean < 2, try_name\r\nE       AssertionError: barnes_hut_1:rerun\r\nE       assert 2.145051767903112 < 2\r\n\r\n..\/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold\/lib\/python3.8\/site-packages\/sklearn\/manifold\/tests\/test_t_sne.py:807: AssertionError\r\n```\r\n\r\n\r\n```\r\n\r\nThe following NEW packages will be INSTALLED:\r\n\r\n    apipkg:                      1.5-py_0               conda-forge\r\n    attrs:                       19.3.0-py_0            conda-forge\r\n    binutils_impl_linux-aarch64: 2.29.1-hc862510_0      c4aarch64  \r\n    ca-certificates:             2019.11.28-hecc5488_0  conda-forge\r\n    certifi:                     2019.11.28-py38_0      conda-forge\r\n    cython:                      0.29.14-py38he1b5a44_0 conda-forge\r\n    execnet:                     1.7.1-py_0             conda-forge\r\n    importlib_metadata:          1.2.0-py38_0           conda-forge\r\n    joblib:                      0.14.0-py_0            conda-forge\r\n    libblas:                     3.8.0-14_openblas      conda-forge\r\n    libcblas:                    3.8.0-14_openblas      conda-forge\r\n    libffi:                      3.2.1-h4c5d2ac_1006    conda-forge\r\n    libgcc-ng:                   7.3.0-h5c90dd9_0       c4aarch64  \r\n    libgfortran-ng:              7.3.0-h6bc79d0_0       c4aarch64  \r\n    liblapack:                   3.8.0-14_openblas      conda-forge\r\n    libopenblas:                 0.3.7-h5ec1e0e_4       conda-forge\r\n    libstdcxx-ng:                7.3.0-h5c90dd9_0       c4aarch64  \r\n    more-itertools:              8.0.2-py_0             conda-forge\r\n    ncurses:                     6.1-hf484d3e_1002      conda-forge\r\n    numpy:                       1.17.3-py38h91f3968_0  conda-forge\r\n    openssl:                     1.1.1d-h516909a_0      conda-forge\r\n    packaging:                   19.2-py_0              conda-forge\r\n    pluggy:                      0.13.1-py38_0          conda-forge\r\n    py:                          1.8.0-py_0             conda-forge\r\n    pyparsing:                   2.4.5-py_0             conda-forge\r\n    pytest:                      5.3.1-py38_0           conda-forge\r\n    pytest-forked:               1.1.2-py_0             conda-forge\r\n    pytest-sugar:                0.9.2-py_0             conda-forge\r\n    pytest-timeout:              1.3.3-py_0             conda-forge\r\n    pytest-xdist:                1.30.0-py_0            conda-forge\r\n    python:                      3.8.0-heaf0f07_5       conda-forge\r\n    readline:                    8.0-h75b48e3_0         conda-forge\r\n    scikit-learn:                0.22-py38h1971d64_0    local      \r\n    scipy:                       1.3.2-py38hb5cb654_0   conda-forge\r\n    setuptools:                  42.0.2-py38_0          conda-forge\r\n    six:                         1.13.0-py38_0          conda-forge\r\n    sqlite:                      3.30.1-h283c62a_0      conda-forge\r\n    termcolor:                   1.1.0-py_2             conda-forge\r\n    tk:                          8.6.10-hed695b0_0      conda-forge\r\n    wcwidth:                     0.1.7-py_1             conda-forge\r\n    xz:                          5.2.4-hda93590_1001    conda-forge\r\n    zipp:                        0.6.0-py_0             conda-forge\r\n    zlib:                        1.2.11-h516909a_1006   conda-forge\r\n\r\n```\r\n\r\nhttps:\/\/cloud.drone.io\/conda-forge\/scikit-learn-feedstock\/32\/3\/2\r\n\r\n","labels":["Bug","help wanted","arch:arm","module:test-suite"],"created_at":"2019-12-07T18:20:53Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15821"},{"issue_number":211,"repository":"scikit-learn\/scikit-learn","title":"Silhouette score is not defined when the estimator return only 1 cluster","description":"https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/4f97facc3a992c6e2459c3da86c9d69b0688d5ab\/sklearn\/metrics\/cluster\/_unsupervised.py#L38\r\nI am wondering why the silhouette score is not defined when the estimator return only 1 cluster. In this case, if we want to do Hyperparameter tuning (GridSearch) based on the silhouette score, it will crash if the estimator return one cluster. In this case, I am wondering why we don't return the worst case (silhouette score = -1)? ","labels":["Bug","help wanted","module:metrics"],"created_at":"2019-11-25T21:49:31Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15717"},{"issue_number":212,"repository":"scikit-learn\/scikit-learn","title":"Segmentation fault with SVC with probability = True and degree 3","description":"I am using SVC with predict proba with below hyper-parameter on large data set. it is giving segmentation fault error. code works well for smaller data set.\r\nI don't think its memory issue we are using 96 GB RAM. I have also monitored system using htop during run & I didn't gave impression of memory issue.\r\n\r\n**hyper-parameter** :\r\nSVC(kernel='poly',gamma=100,C=10,degree=3,decision_function_shape='ovo',probability=True,random_state=42,verbose=True) \r\n**No of Features** : 768\r\n**No of rows** : 2.1 million","labels":["Bug","module:svm"],"created_at":"2019-09-18T08:42:38Z","comments":12,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/15008"},{"issue_number":213,"repository":"scikit-learn\/scikit-learn","title":"Use of X.dtype when it is not float","description":"A fairly common pattern in the scikit-learn code is to create intermediary arrays with `dtype=X.dtype`. That works well, as long as `X = check_arrays(X)` was run with a float dtype only, or in other words when `X.dtype` is not int.\r\n\r\nWhen `X.dtype` is int, `check_array(X)` with default parameters will pass it though, and then all intermediary objects will be of dtype int. \r\n\r\nFor instance,\r\n```py\r\nimport numpy as np\r\nfrom sklearn.cluster import MiniBatchKMeans\r\n\r\nX = np.array([[1, 2], [1, 4], [1, 0],\r\n              [4, 2], [4, 0], [4, 4],\r\n              [4, 5], [0, 1], [2, 2],\r\n              [3, 2], [5, 5], [1, -1]])\r\n# manually fit on batches\r\nkmeans = MiniBatchKMeans(n_clusters=2,\r\n                         random_state=0,\r\n                         batch_size=6)\r\nkmeans.partial_fit(X[0:6,:])\r\n```\r\n(taken from the [MiniBatchKMeans docstring](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.MiniBatchKMeans.html)) \r\nwill happily run in the integer space where both `sample_weight` and `cluster_centroid_` will be `int`. Discovered as part of https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/14307\r\n\r\nAnother point, that e.g. in linear models, `check_array(..., dtype=[np.float64, np.float32])` will only be run if `check_input=True` (default). Meaning that when `check_input=False` it might try to create a liner model where coefficients are an array of integers, and unless something fails due to a dtype mismatch the user will never know. I think we should always check that `X.dtype` is float, even when `check_input=False`. I believe the point of that flag was to avoid expensive checks and this doesn't cost anything.\r\n\r\nIn general any code that uses `X.dtype` to create intermediary arrays should be evaluated as to whether we are sure the dtype is float (or that ints would be acceptable).\r\n\r\nMight be a good sprint issue, not sure.","labels":["Bug"],"created_at":"2019-07-12T17:21:30Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/14314"},{"issue_number":214,"repository":"scikit-learn\/scikit-learn","title":"OneVsRestClassifier violates predict(X)==argmax(decision_function)","description":"#### Description\r\n\r\nWhen `myclassifier.decision_function` returns only `0`s for some sample, the wrapping OneVsRestClassifier will `predict` the last class instead of the first one.\r\n\r\nThe cause is in [multiclass.py](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/multiclass.py#L300..L303) where later classes rated 0 by the decision_function override earlier ones.\r\nThis violates the documented (and questioned, see #13631) requirement that `the row-wise arg-maximum is the predicted class`.\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.multiclass import OneVsRestClassifier\r\nfrom sklearn.base import BaseEstimator, ClassifierMixin\r\nfrom sklearn.utils.estimator_checks import check_classifiers_train\r\nfrom sklearn.utils.validation import check_X_y\r\n```\r\n\r\n\r\n```python\r\nclass Dummy(BaseEstimator, ClassifierMixin):\r\n    def fit(self, X, y):\r\n        X, y = check_X_y(X, y, multi_output=False)\r\n        self.classes_ = np.unique(y)\r\n        self.n_features_ = X.shape[1]\r\n        return self\r\n    def predict(self, X):\r\n        n_features = X.shape[1]\r\n        if self.n_features_ != n_features:\r\n            raise ValueError(\"Number of features of the model must \"\r\n                             \"match the input. Model n_features is %s and \"\r\n                             \"input n_features is %s \"\r\n                             % (self.n_features_, n_features))\r\n        return np.full(len(X), self.classes_[0])\r\n    def decision_function(self, X):\r\n        n_features = X.shape[1]\r\n        if self.n_features_ != n_features:\r\n            raise ValueError(\"Number of features of the model must \"\r\n                             \"match the input. Model n_features is %s and \"\r\n                             \"input n_features is %s \"\r\n                             % (self.n_features_, n_features))\r\n        return np.zeros(len(X))\r\n\r\nclass POVR(OneVsRestClassifier):\r\n    def _more_tags(self):\r\n        return {'poor_score': True}\r\n```\r\n\r\n\r\n```python\r\n>>> x = np.array([[0], [0], [0]])\r\n>>> y = np.arange(3)\r\n>>> d = Dummy().fit(x,y)\r\n>>> d.predict(x)\r\n    array([0, 0, 0])\r\n>>> d.decision_function(x)\r\n    array([0., 0., 0.])\r\n>>> od = POVR(Dummy()).fit(x,y)\r\n>>> od.classes_\r\n    array([0, 1, 2])\r\n>>> od.predict(x)\r\n    array([2, 2, 2])\r\n>>> od.decision_function(x)\r\n    array([[0., 0., 0.],\r\n           [0., 0., 0.],\r\n           [0., 0., 0.]])\r\n>>> np.argmax(od.decision_function(x), axis=1)\r\n    array([0, 0, 0])  # != od.predict(x)\r\n\r\n>>> check_classifiers_train('t', POVR(Dummy()))\r\n    ---------------------------------------------------------------------------\r\n\r\n    AssertionError                            Traceback (most recent call last)\r\n\r\n    <ipython-input-57-e45e548f419d> in <module>\r\n    ----> 1 check_classifiers_train('t', POVR(Dummy()))\r\n    \r\n\r\n    \/usr\/lib\/python3.7\/site-packages\/sklearn\/utils\/testing.py in wrapper(*args, **kwargs)\r\n        353             with warnings.catch_warnings():\r\n        354                 warnings.simplefilter(\"ignore\", self.category)\r\n    --> 355                 return fn(*args, **kwargs)\r\n        356 \r\n        357         return wrapper\r\n\r\n\r\n    \/usr\/lib\/python3.7\/site-packages\/sklearn\/utils\/estimator_checks.py in check_classifiers_train(name, classifier_orig, readonly_memmap)\r\n       1493                 else:\r\n       1494                     assert_equal(decision.shape, (n_samples, n_classes))\r\n    -> 1495                     assert_array_equal(np.argmax(decision, axis=1), y_pred)\r\n       1496 \r\n       1497                 # raises error on malformed input for decision_function\r\n\r\n\r\n    \/usr\/lib\/python3.7\/site-packages\/numpy\/testing\/_private\/utils.py in assert_array_equal(x, y, err_msg, verbose)\r\n        902     __tracebackhide__ = True  # Hide traceback for py.test\r\n        903     assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,\r\n    --> 904                          verbose=verbose, header='Arrays are not equal')\r\n        905 \r\n        906 \r\n\r\n\r\n    \/usr\/lib\/python3.7\/site-packages\/numpy\/testing\/_private\/utils.py in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\r\n        825                                 verbose=verbose, header=header,\r\n        826                                 names=('x', 'y'), precision=precision)\r\n    --> 827             raise AssertionError(msg)\r\n        828     except ValueError:\r\n        829         import traceback\r\n\r\n\r\n    AssertionError: \r\n    Arrays are not equal\r\n    \r\n    Mismatch: 100%\r\n    Max absolute difference: 2\r\n    Max relative difference: 1.\r\n     x: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\r\n     y: array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\r\n           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\r\n           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...\r\n```\r\n\r\n#### Versions\r\n~~~py\r\n\/usr\/lib\/python3.7\/site-packages\/numpy\/distutils\/system_info.py:639: UserWarning: \r\n    Atlas (http:\/\/math-atlas.sourceforge.net\/) libraries not found.\r\n    Directories to search for the libraries can be specified in the\r\n    numpy\/distutils\/site.cfg file (section [atlas]) or by setting\r\n    the ATLAS environment variable.\r\n  self.calc_info()\r\n\r\nSystem:\r\n    python: 3.7.3 (default, Mar 26 2019, 21:43:19)  [GCC 8.2.1 20181127]\r\nexecutable: \/usr\/bin\/python3\r\n   machine: Linux-5.1.11-arch1-1-ARCH-x86_64-with-arch\r\n\r\nBLAS:\r\n    macros: NO_ATLAS_INFO=1, HAVE_CBLAS=None\r\n  lib_dirs: \/usr\/lib64\r\ncblas_libs: cblas\r\n\r\nPython deps:\r\n       pip: 19.0.3\r\nsetuptools: 41.0.1\r\n   sklearn: 0.21.2\r\n     numpy: 1.16.4\r\n     scipy: 1.3.0\r\n    Cython: 0.29.10\r\n    pandas: 0.24.2\r\n~~~","labels":["Bug","help wanted","module:multiclass"],"created_at":"2019-06-19T10:49:30Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/14124"},{"issue_number":215,"repository":"scikit-learn\/scikit-learn","title":"data leak in GBDT due to warm start","description":"(This is about the non-histogram-based version of GBDTs)\r\n\r\nX is split into train and validation data with `train_test_split(random_state=self.random_state)`.\r\n\r\nAs @johannfaouzi noted, in a warm starting context, this will produce a leak if If `self.random_state` is a `RandomState` instance: some samples that were used for training in a previous `fit` might be used for validation now.\r\n\r\n~~I think the right fix would be to raise a `ValueError` if the provided random state isn't a number and early-stopping is activated~~","labels":["Bug","module:ensemble"],"created_at":"2019-06-06T16:09:45Z","comments":11,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/14034"},{"issue_number":216,"repository":"scikit-learn\/scikit-learn","title":"Strange behavior of label_binarize when there's only one class in y","description":"When there's only one class in y, label_binarize produces unexpected result\r\n```\r\nfrom sklearn.preprocessing import label_binarize\r\nlabel_binarize([1, 1, 1], classes=[1])\r\n# actual result [[0], [0], [0]]\r\n# expected result [[1], [1], [1]]\r\nlabel_binarize([1, 1, 1], classes=[0])\r\n# actual result [[0], [0], [0]]\r\n# expected result [[0], [0], [0]]\r\n```\r\nversion: 0.20.3 and 0.21 dev","labels":["Bug","module:preprocessing"],"created_at":"2019-04-19T03:27:53Z","comments":6,"reactions":2,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/13674"},{"issue_number":217,"repository":"scikit-learn\/scikit-learn","title":"MLPRegressor, warm_start not working as expected","description":"I think I found a bug when it comes to the MLPRegressor (in v.0.20.3), especially with \"warm_start\" set to true. The problem occured to me, as well as to other people I know. \r\n\r\nWe have a dataset with some training data, and try to train the NN, with MLPRegressor, of 5000 iterations, 50 hidden neurons in one layer and the fixed random seed 0. \r\n```\r\n\r\nnn = MLPRegressor(activation='logistic', solver='lbfgs', max_iter=5000, hidden_layer_sizes=(50,), alpha=0, random_state=0)\r\nnn.fit(x_train, y_train)\r\n```\r\n\r\nThis works as expected, however if we create an MLPRegressor with 1 iteration, but we are calling \"fit\" in a loop over a range of 5000, the function we have learned looks totally wrong, since it is totally inaccurate with respect to the trainingsset.\r\n\r\n```\r\nnn = MLPRegressor(activation='logistic', solver='lbfgs', max_iter=1,hidden_layer_sizes=(50,), alpha=0, random_state=0, warm_start=True)\r\nfor i in range(0,5000):\r\n  nn.fit(x_train,y_train) \r\n```\r\n\r\nWe tried calculating the mean squared error durring each iteration of the second code snippet, and we calculated the MSE after the first code snippet. None of the MSEs, calculated during the iterations, was even close to the MSE we calculated after the first code snippet. \r\n\r\nSince on the scikit-learn website it is stated, that warm start can be used for monitoring, I am pretty sure that this is a bug, since the end results are definitely not the same. ","labels":["Bug","module:neural_network"],"created_at":"2019-04-16T11:06:25Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/13656"},{"issue_number":218,"repository":"scikit-learn\/scikit-learn","title":"BaggingClassifier uses Class Label as Index to Array when Voting","description":"<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https:\/\/stackoverflow.com\/questions\/tagged\/scikit-learn\r\n- Mailing List: https:\/\/mail.python.org\/mailman\/listinfo\/scikit-learn\r\nFor more information, see User Questions: http:\/\/scikit-learn.org\/stable\/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\nBaggingClassifier uses Class Label as Index to Array when Voting\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\nProvide a base estimator to _BaggingClassifier_ that does not define the function _predict_proba_. This results in _BaggingClassifier_ resorting to voting. It appears the code for performing voting uses class labels as array indices instead of looking up the index of the class label in the _classes__ member.\r\n\r\nExample:\r\n```python\r\nimport numpy as np\r\nfrom sklearn.ensemble import BaggingClassifier\r\n\r\nclass Foo:\r\n    \r\n    def __init__(self):\r\n        pass\r\n    \r\n    def fit(self, X, Y, W=None):\r\n        return self\r\n    \r\n    def predict(self, X):\r\n        return np.full(X.shape[0], True, np.bool)\r\n    \r\n    def score(self, X, Y):\r\n        YH = self.predict(X)\r\n        return (Y == YH).mean()\r\n    \r\n    def get_params(self, deep=True):\r\n        return {}\r\n    \r\n    def set_params(self, **params):\r\n        for k, v in params:\r\n            setattr(self, k, v)\r\n        return self\r\n    \r\n# %%\r\nA = np.random.rand(10, 4)\r\nY = np.random.randint(2, size=10, dtype=np.bool)\r\nbc = BaggingClassifier(Foo())\r\nbc.fit(A, Y)\r\nYH = bc.predict(A)\r\nprint('BaggingClassifier Voting Result: ')\r\nprint(YH)\r\nprint('Ensemble Member Predictions: ')\r\nfor Ei in bc.estimators_:\r\n    print(Ei.predict(A))\r\n```\r\n\r\n#### Expected Results\r\n<!-- Example: No error is thrown. Please paste or describe the expected results.-->\r\nIn the above code snippet, _BaggingClassifier_ should return an array of _True_ since it is the majority prediction of all ensemble members.\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback. -->\r\n_BaggingClassifier_ returns an array of False. This issue only occurs when the base estimator does not define the function _predict_proba_.\r\n\r\nThe issue appears to be due to lines 137 and 140 in ensemble\/bagging.py.\r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/e14ac6d36d9dd069cc7fc2e51e4973514a003591\/sklearn\/ensemble\/bagging.py#L137\r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/e14ac6d36d9dd069cc7fc2e51e4973514a003591\/sklearn\/ensemble\/bagging.py#L140\r\n\r\nThe predictions of the ensemble members are directly used as indices into the original array. I'm guessing the prediction labels need to be converted into class labels using _estimator.classes__.\r\n\r\n#### Versions\r\nSystem:\r\n    python: 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]\r\nexecutable: C:\\Users\\XXXXXX\\Anaconda3\\pythonw.exe\r\n   machine: Windows 2012 ServerR2\r\n\r\nBLAS:\r\n    macros: \r\n  lib_dirs: \r\ncblas_libs: cblas\r\n\r\nPython deps:\r\n       pip: 18.1\r\nsetuptools: 40.6.3\r\n   sklearn: 0.20.1\r\n     numpy: 1.15.4\r\n     scipy: 1.1.0\r\n    Cython: 0.29.2\r\n    pandas: 0.23.4\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","module:ensemble"],"created_at":"2019-04-06T04:14:22Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/13587"},{"issue_number":219,"repository":"scikit-learn\/scikit-learn","title":"cross_validate hang randomly when training svc with polynomial kernel.","description":"#### Description\r\n`cross_validate` hang randomly when training svc with polynomial kernel.\r\n\r\n#### Steps\/Code to Reproduce\r\n```python\r\nfrom sklearn import datasets\r\nfrom sklearn.svm import SVC\r\nfrom sklearn.model_selection import train_test_split, cross_validate\r\n\r\niris = datasets.load_iris()\r\nx = iris.data\r\ny = iris.target\r\nX_choose, x_test, y_choose, y_test = train_test_split(x, y, test_size=0.3)\r\n\r\nparams = [[0.6652997139930452, 'poly', 7, 4178.386000737241],\r\n          [1.2346990434544882, 'poly', 7, 4317.581190465473],\r\n          [0.8156943235551155, 'poly', 8, 864.1583649816441]]\r\n\r\nfor c, kernel, degree, gamma in params:\r\n    clf = SVC(C=c, kernel=kernel, degree=degree, gamma=gamma)\r\n    cv_results = cross_validate(clf, X_choose, y_choose, cv=5,\r\n                                return_train_score=False)\r\n    print(cv_results['test_score'].mean())\r\n```\r\n\r\n#### Expected Results\r\nThree scores should be shown.\r\n\r\n#### Actual Results\r\nSometimes hang.\r\nI waited for few minutes.\r\nAlso I checked running code line doesn't change using gdb.\r\nBacktrace is [here](https:\/\/gist.github.com\/sam-yusuke\/7099829b1797d6867eb928264597979d)\r\n#### Versions\r\n```\r\n>>> import sklearn; sklearn.show_versions()\r\nSystem:\r\n    python: 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]\r\nexecutable: \/home\/yusuke\/miniconda3\/envs\/py37_automl_examples2\/bin\/python\r\n   machine: Linux-4.20.0-042000-generic-x86_64-with-debian-buster-sid\r\n\r\nBLAS:\r\n    macros: SCIPY_MKL_H=None, HAVE_CBLAS=None\r\n  lib_dirs: \/home\/yusuke\/miniconda3\/envs\/py37_automl_examples2\/lib\r\ncblas_libs: mkl_rt, pthread\r\n\r\nPython deps:\r\n       pip: 19.0.3\r\nsetuptools: 40.8.0\r\n   sklearn: 0.20.3\r\n     numpy: 1.16.2\r\n     scipy: 1.2.1\r\n    Cython: None\r\n    pandas: None\r\n```\r\n\r\nThank you!","labels":["Bug","help wanted","module:svm"],"created_at":"2019-04-02T02:25:23Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/13557"},{"issue_number":220,"repository":"scikit-learn\/scikit-learn","title":"API Inconsitency of predict and predict_proba in SVC","description":"When using `SVC(probability=True)` ~~or `SVR(probability=True)`~~ the output of `predict_proba` will not necessarily be consistent with `predict`, in the sense that,\r\n```py\r\nnp.argmax(self.predict_proba(X), axis=1) != self.predict(X)\r\n```\r\nthis is documented [in the user guide](https:\/\/scikit-learn.org\/stable\/modules\/svm.html#scores-and-probabilities),\r\n> In addition, the probability estimates may be inconsistent with the scores, in the sense that the \u201cargmax\u201d of the scores may not be the argmax of the probabilities. (E.g., in binary classification, a sample may be labeled by predict as belonging to a class that has probability <\u00bd according to predict_proba.) Platt\u2019s method is also known to have theoretical issues.\r\n\r\nIMO this is a violation of the API contract and should be fixed. \r\n\r\nThis is being continuously reported as a bug e.g.  https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/4800  https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12408 https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12982 and a few stack overflow issues e.g. https:\/\/stackoverflow.com\/a\/17019830 \r\n\r\nI encountered this in a project where detecting this discrepancy, evaluating the difference and deciding whether `predict` or `argmax(predict_proba` should be used in the end took some effort.\r\n\r\nOne pitfall is for instance to use `predict` to compute the accuracy, and then `predict_proba` for ROC AUC which can lead to somewhat problematic results if the predictions of these methods are not consistent.\r\n\r\nSeveral approaches could be used to fix it,\r\n\r\n1. Deprecate `probability=True` parameter in `SVC`, `NuSVC` estimator and suggest using `CalibratedClassifierCV(SVC(), cv=5)` instead. In my quick tests (on sparse data), the latter was actually faster and should yield comparable results that are also consistent between predict and `predict_proba`. Though more benchmarks may be needed.\r\n   There may also be some variation in the results, as libsvm uses a generalization of Platt scaling in the multiclass case by Wu et al 2014 (cf [docs](https:\/\/scikit-learn.org\/stable\/modules\/svm.html#scores-and-probabilities) that is not used in `CalibratedClassifierCV` as far as I understand?\r\n\r\nOne possibility could be to deprecate, but keep it to allow access to that functionality in libsvm. \r\n\r\n2. Compute `predict` as `argmax(predict_proba` when `probability=True`. This has the disadvantage of changing the results of predict depending on this input parameter.\r\n\r\n3. Dig into libsvm to understand how it could be fixed there. \r\n","labels":["Bug","Needs Decision","module:svm"],"created_at":"2019-02-21T12:29:29Z","comments":44,"reactions":5,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/13211"},{"issue_number":221,"repository":"scikit-learn\/scikit-learn","title":"Incorrect calculations of homogeneity, completeness and v-measure","description":"<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https:\/\/stackoverflow.com\/questions\/tagged\/scikit-learn\r\n- Mailing List: https:\/\/mail.python.org\/mailman\/listinfo\/scikit-learn\r\nFor more information, see User Questions: http:\/\/scikit-learn.org\/stable\/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\nCalculations of homogeneity, completeness and v-measure are now based on the original paper of Rosenberg & Hirschberg 2007. However, while I was doing research on fuzzy clustering evaluation techniques, I found the following paper of Utt et al. 2014 (http:\/\/www.lrec-conf.org\/proceedings\/lrec2014\/pdf\/829_Paper.pdf) which explained in a footnote that the original definitions of homogeneity and completeness contain typos. They claim it was confirmed by Rosenberg himself via personal communications.\r\n\r\nDefinitions used: \r\n\r\n- homogeneity = 1 - H(C|K) \/ H(C)\r\n- completeness = 1  - H(K|C) \/ H(K)\r\n\r\nCorrected definitions:\r\n\r\n- homogeneity = 1 - H(C|K) \/ H(C,K)\r\n- completeness = 1  - H(K|C) \/ H(K,C) \r\n\r\nFurthermore, since the calculations are now based on the mutual information score, this wouldn't be correct anymore. Also, the statement in the documentation about it being the same as normalized mutual information with the metric set to 'arithmetic' would be false.\r\n\r\n#### Steps\/Code to Reproduce\r\n<!--\r\nExample:\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation\r\n\r\ndocs = [\"Help I have a bug\" for i in range(1000)]\r\n\r\nvectorizer = CountVectorizer(input=docs, analyzer='word')\r\nlda_features = vectorizer.fit_transform(docs)\r\n\r\nlda_model = LatentDirichletAllocation(\r\n    n_topics=10,\r\n    learning_method='online',\r\n    evaluate_every=10,\r\n    n_jobs=4,\r\n)\r\nmodel = lda_model.fit(lda_features)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https:\/\/gist.github.com\r\n-->\r\nfrom sklearn.metrics import homogeneity_completeness_v_measure\r\n\r\n#### Expected Results\r\n<!-- Example: No error is thrown. Please paste or describe the expected results.-->\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback. -->\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nFor scikit-learn >= 0.20:\r\nimport sklearn; sklearn.show_versions()\r\nFor scikit-learn < 0.20:\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\nSystem:\r\n    python: 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)]\r\nexecutable: C:\\Users\\dtuser\\AppData\\Local\\Programs\\Python\\Python36\\python.exe\r\n   machine: Windows-7-6.1.7601-SP1\r\nBLAS:\r\n    macros: \r\n  lib_dirs: \r\ncblas_libs: cblas\r\nPython deps:\r\n       pip: 18.1\r\nsetuptools: 40.6.3\r\n   sklearn: 0.20.1\r\n     numpy: 1.15.4\r\n     scipy: 1.1.0\r\n    Cython: None\r\n    pandas: 0.23.4\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","help wanted","module:metrics"],"created_at":"2019-01-28T12:36:17Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/13058"},{"issue_number":222,"repository":"scikit-learn\/scikit-learn","title":"RadiusNeighborsRegression is inconsistent when extrapolation occurs","description":"#### Description\r\nThe behavior of `RadiusNeighborsRegression` is inconsistent when extrapolation occurs. The behavior depends on the chosen weight-function. \r\n\r\n-`weight=\"uniform\"` will return `[NaN]`.\r\n-`weight=\"distance\"`will raise an error\r\n-`weight=lambda d: d`will raise an error\r\n\r\n\r\n#### Steps\/Code to Reproduce\r\n```python\r\nfrom sklearn.neighbors import RadiusNeighborsRegressor\r\n\r\nX = [[1],[2],[3],[4]]\r\ny = [1,2,3,4]\r\n\r\nX_predict = [[-100]]\r\n\r\nmodel = RadiusNeighborsRegressor(radius=1.0, weights = \"distance\")\r\nfitm = model.fit(X,y)\r\n\r\n# raises ZeroDivisionError\r\nresult = fitm.predict(X_predict)  \r\n```\r\n\r\n#### Expected Results\r\nNo error is raised and `[[NaN]]` is returned. \r\n\r\n#### Actual Results\r\n```\r\n File \"c:\/test.py\", line 12, in <module>\r\n    result = fitm.predict(X_predict)\r\n  File \"C:\\...\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\regression.py\", line 296, in predict\r\n    for (i, ind) in enumerate(neigh_ind)])\r\n  File \"C:\\...\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\regression.py\", line 296, in <listcomp>\r\n    for (i, ind) in enumerate(neigh_ind)])\r\n  File \"C:\\...\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\", line 1158, in average\r\n    \"Weights sum to zero, can't be normalized\")\r\nZeroDivisionError: Weights sum to zero, can't be normalized\r\n```\r\n\r\n#### Versions\r\n```\r\nWindows-8.1-6.3.9600-SP0\r\nPython 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\r\nNumPy 1.14.3\r\nSciPy 1.1.0\r\nScikit-Learn 0.19.1\r\n```","labels":["Bug","help wanted","module:neighbors"],"created_at":"2019-01-11T20:10:27Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12960"},{"issue_number":223,"repository":"scikit-learn\/scikit-learn","title":"Error while using n_jobs=-1 with uwsgi","description":"Hi I am using sklearn n_jobs = -1 for multi process training of my model but i am getting below error.\r\n\r\nsklearn.externals.joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {EXIT(1), EXIT(1), EXIT(1)}","labels":["Bug"],"created_at":"2019-01-04T09:58:22Z","comments":2,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12922"},{"issue_number":224,"repository":"scikit-learn\/scikit-learn","title":"[MRG] Bugfix for not-yet-confirmed issue #12863: arpack returns singular values in ascending order, the opposite was supposed in sklearn","description":"Bugfix for not-yet-confirmed issue #12863: arpack returns singular values in ascending order, the opposite was supposed in sklearn\r\n\r\n<!--\r\nThanks for contributing a pull request! Please ensure you have taken a look at\r\nthe contribution guidelines: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/CONTRIBUTING.md#pull-request-checklist\r\n-->\r\n\r\n#### Reference Issues\/PRs\r\nFixes [MRG] not-yet-confirmed issue #12863 \r\n\r\n<!--\r\nExample: Fixes #1234. See also #3456.\r\nPlease use keywords (e.g., Fixes) to create link to the issues or pull requests\r\nyou resolved, so that they will automatically be closed when your pull request\r\nis merged. See https:\/\/github.com\/blog\/1506-closing-issues-via-pull-requests\r\n-->\r\n\r\n\r\n#### What does this implement\/fix? Explain your changes.\r\n\r\nIn the sklearn code it was assumed that the scipy wrapper for arpack singular value decomposition returns singular values (and corresponding vectors) in descending order. This is not the case. The ARPACK documentation (https:\/\/www.caam.rice.edu\/software\/ARPACK\/UG\/node136.html#SECTION001210000000000000000\r\n) clearly states that the singular values are returned in ascending order.\r\n\r\nThe bug manifests in differing results depending on the solver:\r\n\r\n```\r\nimport numpy as np\r\nfrom sklearn import cluster\r\n\r\nA= np.array([[-2, -4, 2], [-2, 1, 2], [4, 2, 5]])\r\n\r\nsc= cluster.bicluster.SpectralCoclustering(n_clusters= 2, \r\n                                           svd_method='randomized')\r\n\r\nsc.fit(A)\r\nprint(sc.column_labels_)\r\n```\r\ngives\r\n```\r\n[0 0 1]\r\n```\r\n, but\r\n```\r\nsc= cluster.bicluster.SpectralCoclustering(n_clusters= 2, \r\n                                           svd_method='arpack')\r\nsc.fit(A)\r\nprint(sc.column_labels_)\r\n```\r\ngives\r\n```\r\n[0 0 0]\r\n```\r\n\r\nThis bugfix makes the arpack call return the same result as the randomized sv based solution.\r\n\r\n#### Any other comments?\r\n\r\n\r\n<!--\r\nPlease be aware that we are a loose team of volunteers so patience is\r\nnecessary; assistance handling other issues is very welcome. We value\r\nall user contributions, no matter how minor they are. If we are slow to\r\nreview, either the pull request needs some benchmarking, tinkering,\r\nconvincing, etc. or more likely the reviewers are simply busy. In either\r\ncase, we ask for your understanding during the review process.\r\nFor more information, see our FAQ on this topic:\r\nhttp:\/\/scikit-learn.org\/dev\/faq.html#why-is-my-pull-request-not-getting-any-attention.\r\n\r\nThanks for contributing!\r\n-->\r\n","labels":["Bug","module:cluster","module:decomposition"],"created_at":"2019-01-01T13:40:30Z","comments":14,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/12898"},{"issue_number":225,"repository":"scikit-learn\/scikit-learn","title":"LogisticRegressionCV failure when not all classes appear in each fold","description":"#### Description\r\n\r\nWhen not all classes appear in each CV fold, LogisticRegressionCV fails when trying to collect the coefficient array.\r\n\r\n#### Steps\/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.linear_model import LogisticRegressionCV\r\n\r\nxtmp, ytmp = make_classification(n_samples=2500, n_informative=10, n_classes=6)\r\n\r\n# remove all but 2 instances of class 3, in order to make class 3 not-appear in at least one fold\r\nremove = np.where(ytmp == 3)[0][2:]\r\nkeep = np.setdiff1d(np.arange(len(ytmp), dtype='int'), remove)\r\n\r\nlr = LogisticRegressionCV(multi_class='multinomial', cv=KFold(5)).fit(xtmp[keep], ytmp[keep])\r\n```\r\n\r\n#### Expected Results\r\nNo errors; `lr` contains a fitted instance of `LogisticRegressionCV`.\r\n\r\n#### Actual Results\r\n```ValueError                                Traceback (most recent call last)\r\n<ipython-input-51-ac4dd6663338> in <module>\r\n      4 remove = np.where(ytmp == 3)[0][2:]  # remove all but 2 instances of class 3\r\n      5 keep = np.setdiff1d(np.arange(len(ytmp), dtype='int'), remove)\r\n----> 6 lr = LogisticRegressionCV(multi_class='multinomial', cv=KFold(5)).fit(xtmp[keep], ytmp[keep])\r\n\r\n<redacted>\/lib\/python3.6\/site-packages\/sklearn\/linear_model\/logistic.py in fit(self, X, y, sample_weight)\r\n   1796         if multi_class == 'multinomial':\r\n   1797             multi_coefs_paths, Cs, multi_scores, n_iter_ = zip(*fold_coefs_)\r\n-> 1798             multi_coefs_paths = np.asarray(multi_coefs_paths)\r\n   1799             multi_scores = np.asarray(multi_scores)\r\n   1800 \r\n\r\n<redacted>\/lib\/python3.6\/site-packages\/numpy\/core\/numeric.py in asarray(a, dtype, order)\r\n    499 \r\n    500     \"\"\"\r\n--> 501     return array(a, dtype, copy=False, order=order)\r\n    502 \r\n    503 \r\n\r\nValueError: could not broadcast input array from shape (10,5,21) into shape (10)\r\n```\r\n\r\nIf you go inspect the `multi_coefs_paths` object, you'll see that its elements have different shapes, due to the misaligned classes. Only workaround as far as I can tell is to do grid search manually.\r\n\r\n#### Versions\r\n\r\n```\r\nSystem:\r\n    python: 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44)  [GCC 7.3.0]\r\nexecutable: <redacted>\/bin\/python\r\n   machine: Linux-3.10.0-514.26.2.el7.x86_64-x86_64-with-redhat-7.3-Maipo\r\n\r\nBLAS:\r\n    macros: SCIPY_MKL_H=None, HAVE_CBLAS=None\r\n  lib_dirs: <redacted>\/lib\r\ncblas_libs: mkl_rt, pthread\r\n\r\nPython deps:\r\n       pip: 18.1\r\nsetuptools: 40.6.2\r\n   sklearn: 0.20.1\r\n     numpy: 1.15.4\r\n     scipy: 1.1.0\r\n    Cython: 0.29\r\n    pandas: 0.23.4\r\n```","labels":["Bug","help wanted","module:linear_model"],"created_at":"2018-12-20T21:41:39Z","comments":17,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12845"},{"issue_number":226,"repository":"scikit-learn\/scikit-learn","title":"LDA covariance shrinkage 'auto' bug","description":"<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https:\/\/stackoverflow.com\/questions\/tagged\/scikit-learn\r\n- Mailing List: https:\/\/mail.python.org\/mailman\/listinfo\/scikit-learn\r\nFor more information, see User Questions: http:\/\/scikit-learn.org\/stable\/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\nThe current implementation of shrinkage 'auto' is to calculate ledoit-wolf shrinkage factor separately for each matrix. \r\n\r\nThe within-class covariance matrices are calculated with different shrinkage factors, then summed together to be S_w. Because shrinkage factor depends largely on sample size, the shrinkage factor of every within-class covariance matrix will be much larger than that of the total covariance matrix, meaning diagonal entries are weighted more in S_w than in S_t. This will result in negative diagonal entries for the between-class covariance matrix $S_b=S_t-S_w$\r\n\r\nnegative variance then leads to weird behavior of eigen-value decomposition, including 1) wrong\/meaningless explained_variance 2) questionable eigenvectors\r\n\r\nthis problem can be resolved by calculating the ledoit-wolf shrinkage factor first and pass it as an argument to the LDA decoder.\r\n\r\nI'm not sure whether this is an intended feature or a bug.\r\n\r\nIf it's the former case, how should we interpret the eigenvectors?\r\n\r\n","labels":["Bug","module:discriminant_analysis"],"created_at":"2018-11-23T09:46:19Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12662"},{"issue_number":227,"repository":"scikit-learn\/scikit-learn","title":"Sklearn.svm.SVC,the \"predict\" method predict all samples to  same one label, which is wrong. ","description":"<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: https:\/\/stackoverflow.com\/questions\/tagged\/scikit-learn\r\n- Mailing List: https:\/\/mail.python.org\/mailman\/listinfo\/scikit-learn\r\nFor more information, see User Questions: http:\/\/scikit-learn.org\/stable\/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nI want to use sklearn.svm.SVC to predict probality of each label. However, when I use the method of \"predict\", the SVC estimator will predict all samples to the same label whether I set the probablity to True or False. \r\nWhen I replace SVC with LinearSVC, the result becomes normal.\r\nthe following \"Expected Results\" was run by replacing the SVC(probablity=True) with LinearSVC().\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\n```python\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn import datasets, linear_model\r\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\r\n\r\n# some code about to load X and y\r\n\r\nclf = SVC(probability=True)\r\nclf.fit(X, y)\r\ny_pred = clf.predict(X)\r\nprint(clf.classes_)\r\nprint('the true label:',y[:20])\r\nprint('the predict label:', clf.predict(X)[:20])\r\n```\r\n\r\n#### Expected Results\r\n['pop_and_registration' 'recall' 'repair_process' 'warranty_extension'\r\n 'warranty_lookup' 'warranty_policy']\r\nthe true label: ['warranty_lookup' 'warranty_lookup' 'warranty_lookup' 'warranty_lookup'\r\n 'warranty_policy' 'warranty_policy' 'warranty_policy' 'warranty_policy'\r\n 'warranty_policy' 'warranty_policy' 'pop_and_registration'\r\n 'pop_and_registration' 'warranty_policy' 'pop_and_registration'\r\n 'warranty_extension' 'warranty_extension' 'warranty_extension'\r\n 'warranty_policy' 'warranty_lookup' 'warranty_lookup']\r\nthe predict label: ['warranty_lookup' 'warranty_lookup' 'warranty_lookup' 'warranty_lookup'\r\n 'warranty_policy' 'warranty_policy' 'warranty_policy' 'warranty_policy'\r\n 'warranty_policy' 'warranty_policy' 'pop_and_registration'\r\n 'pop_and_registration' 'warranty_policy' 'pop_and_registration'\r\n 'warranty_extension' 'warranty_extension' 'warranty_extension'\r\n 'warranty_policy' 'warranty_lookup' 'warranty_lookup']\r\n\r\n\r\n#### Actual Results\r\n['pop_and_registration' 'recall' 'repair_process' 'warranty_extension'\r\n 'warranty_lookup' 'warranty_policy']\r\nthe true label: ['warranty_lookup' 'warranty_lookup' 'warranty_lookup' 'warranty_lookup'\r\n 'warranty_policy' 'warranty_policy' 'warranty_policy' 'warranty_policy'\r\n 'warranty_policy' 'warranty_policy' 'pop_and_registration'\r\n 'pop_and_registration' 'warranty_policy' 'pop_and_registration'\r\n 'warranty_extension' 'warranty_extension' 'warranty_extension'\r\n 'warranty_policy' 'warranty_lookup' 'warranty_lookup']\r\nthe predict label: ['pop_and_registration' 'pop_and_registration' 'pop_and_registration'\r\n 'pop_and_registration' 'pop_and_registration' 'pop_and_registration'\r\n 'pop_and_registration' 'pop_and_registration' 'pop_and_registration'\r\n 'pop_and_registration' 'pop_and_registration' 'pop_and_registration'\r\n 'pop_and_registration' 'pop_and_registration' 'pop_and_registration'\r\n 'pop_and_registration' 'pop_and_registration' 'pop_and_registration'\r\n 'pop_and_registration' 'pop_and_registration']\r\n\r\n\r\n\r\n#### Versions\r\nSystem\r\n------\r\n   machine: Windows-10-10.0.17134-SP0\r\n    python: 3.5.6 |Anaconda, Inc.| (default, Aug 26 2018, 16:05:27) [MSC v.1900 64 bit (AMD64)]\r\nexecutable: D:\\ProgramData\\Anaconda3\\envs\\py35\\python.exe\r\n\r\nBLAS\r\n----\r\ncblas_libs: cblas\r\n    macros: \r\n  lib_dirs: \r\n\r\nPython deps\r\n\r\n-----------\r\n\r\n     numpy: 1.15.2\r\n     scipy: 1.1.0\r\n   sklearn: 0.20.0\r\nsetuptools: 40.2.0\r\n    Cython: None\r\n    pandas: 0.23.4\r\n       pip: 10.0.1\r\n\r\n","labels":["Bug","module:svm"],"created_at":"2018-10-18T04:28:27Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12408"},{"issue_number":228,"repository":"scikit-learn\/scikit-learn","title":"[WIP] priority of features in decision to splitting","description":"\r\n#### Reference Issues\/PRs\r\nFixes #12259 . See also  issues #2386 , #8443 and  #12188 \r\n\r\n#### What does this implement\/fix? Explain your changes.\r\n\r\nBasically, Scikit-learn implements decision trees with 2 different types of splitters. A RandomSplitter and a BestSplitter. I focus on BestSplitter, which currently has two flaws:\r\n\r\n1. It is random (provide different outputs on the default config). This happens when there are two or more 'best' features with equal criterion value.\r\n2. It doesn't allow to give priors on the features, for example, when there is a Tie. It would be a nice-to-have if we could chose a features that we prefer. Either because it is easier to compute, or because it is less noisy or because it is easy to interpret. The ideal would be to encode it in the index of the column of the feature. Lower index means higher priority. \r\n\r\nWe have a supplementary constraint which is the Shuffle constrain. Because currently there is a max_features parameter which allows us to do not test all the features at each node if there are many. So this is a nice feature and I believe we should always shuffle. The problem is that shuffling means non deterministic...\r\n\r\n\r\n#### So we have:\r\n\r\n- Sorted \/ Unsorted       -> stands for the fact of giving priority to lower index features if and only if there is a Tie.\r\n- Deterministic \/ Non     -> stands for the fact of always obtaining the same result, at least for the default configuration.\r\n- Shuffled \/ Looped \/ No  -> stands for the fact of shuffling the features when we try them. We can shuffle them randomly, we can loop through them or can test them in order (from 0 to N). This is important only if max_featues < nb_features  or if we do not sort the features.\r\n\r\n\r\n\r\nSo for me the ideal is to have a a prior on the features (Sorted, saying lower index means I prefer the feature). \r\nWe need to do Shuffle or at least Loop, in order to support  max_features, which is often useful, and necessary for sparse cases.\r\n\r\nThen it should be deterministic in the default configuration, so if we want this we have the following possibilities:\r\n - we can do looping instead of shuffling: this would make random_state useless here... so is not viable\r\n - we can do shuffle and sort the features to the best of our knowledge: So we do shuffling and if 2 features from the tested features are 'best' we keep the lower index. This will make the default config where `max_features=n_features` stable regardless of the `random_state` because we will test all the features.\r\n \r\n\r\nSo I implemented this BestSplitter2 that shuffles, and is deterministic when `max_features=n_features` and gives priority to the lower index features when there is a tie.\r\n\r\n\r\nHere you can see an example of the previous and the new one:\r\nPrevious BestSplitter is on the left and New BestSplitter2 is on the right \r\n\r\n[![Screenshot-from-2018-10-12-11-31-55.png](https:\/\/i.postimg.cc\/m2wbstBY\/Screenshot-from-2018-10-12-11-31-55.png)](https:\/\/postimg.cc\/ppmNQV9r)\r\n\r\nWe observe that BestSplitter2 give priority to lower index features.\r\nAnd when we test stability \r\n\r\n\r\n```\r\nimport numpy as np\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.tree import DecisionTreeClassifier\r\n\r\niris = load_iris()\r\nx,y  = iris.data, iris.target\r\n\r\ndtc1 = DecisionTreeClassifier(random_state=1,splitter='best2')\r\ndtc2 = DecisionTreeClassifier(random_state=2,splitter='best2')\r\n\r\nrs = np.random.RandomState(1234)\r\nitr = rs.rand(x.shape[0]) < 0.75\r\n\r\ndtc1.fit(x[itr],y[itr])\r\ndtc2.fit(x[itr],y[itr])\r\n\r\nprint(  (dtc1.predict(x[~itr]) != dtc2.predict(x[~itr])).sum() )\r\n```\r\n\r\nWe obtain `0` as expected in #12259 \r\n\r\n\r\n#### Any other comments?\r\n\r\nI don't know what do you think, I think having many splitters is not a solution either.\r\nPersonally I would replace the current bestSplitter with the one described above, as they are essentially the same but with some improvements.\r\n\r\nThank you very much\r\n","labels":["Bug","Stalled","help wanted","module:tree","cython"],"created_at":"2018-10-12T09:37:35Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/12364"},{"issue_number":229,"repository":"scikit-learn\/scikit-learn","title":"Dask Joblib backend cancels required futures","description":"Here is a failing example that uses the Dask joblib backend to parallelize a nested RandomForestClassifier within a RandomSearchCV\r\n\r\n```python\r\nfrom scipy.stats import randint as sp_randint\r\n\r\nfrom sklearn.model_selection import GridSearchCV\r\nfrom sklearn.model_selection import RandomizedSearchCV\r\nfrom sklearn.datasets import load_digits\r\nfrom sklearn.ensemble import RandomForestClassifier\r\n\r\nfrom dask.distributed import Client\r\n\r\ndef test_random_forest_random_search():\r\n    with Client(processes=False) as client:\r\n        # Taken from http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_randomized_search.html\r\n\r\n        # get some data\r\n        digits = load_digits()\r\n        X, y = digits.data, digits.target\r\n\r\n        # build a classifier\r\n        clf = RandomForestClassifier(n_estimators=20)\r\n        # specify parameters and distributions to sample from\r\n        param_dist = {\"max_depth\": [3, None],\r\n                      \"max_features\": sp_randint(1, 11),\r\n                      \"min_samples_split\": sp_randint(2, 11),\r\n                      \"bootstrap\": [True, False],\r\n                      \"criterion\": [\"gini\", \"entropy\"]}\r\n\r\n        n_iter_search = 20\r\n        random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\r\n                                           n_iter=n_iter_search, cv=5)\r\n        from sklearn.externals import joblib\r\n        with joblib.parallel_backend('dask'):\r\n            random_search.fit(X, y)\r\n```\r\n\r\n### What happened\r\n\r\nBy placing a breakpoint here:\r\n\r\n```diff\r\ndiff --git a\/distributed\/client.py b\/distributed\/client.py\r\nindex f9cf7f94..e47d0d16 100644\r\n--- a\/distributed\/client.py\r\n+++ b\/distributed\/client.py\r\n@@ -314,6 +314,8 @@ class Future(WrappedKey):\r\n     def release(self, _in_destructor=False):\r\n         # NOTE: this method can be called from different threads\r\n         # (see e.g. Client.get() or Future.__del__())\r\n+        if not default_client().cluster.scheduler.story(self.key):\r\n+            import pdb; pdb.set_trace()\r\n         if not self._cleared and self.client.generation == self._generation:\r\n             self._cleared = True\r\n             try:\r\n```\r\n\r\nWe find that Joblib has stopped the computation\r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/3804ccd2770ac4c026a823d019091917e4a2c70e\/sklearn\/externals\/joblib\/parallel.py#L1002-L1004\r\n\r\nWhich then goes ahead and clears out live futures\r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/3804ccd2770ac4c026a823d019091917e4a2c70e\/sklearn\/externals\/joblib\/_dask.py#L156-L159\r\n\r\nThis then fails in retrieve when we get the job result\r\n\r\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/3804ccd2770ac4c026a823d019091917e4a2c70e\/sklearn\/externals\/joblib\/parallel.py#L901\r\n\r\nI'm curious why we're stopping things while we still have live futures.  Have we found a \"good enough\" result and are stopping early?  If so, should we be getting the results of the submitted-but-discarded futures?  Is there something that Dask can do to help here?","labels":["Bug"],"created_at":"2018-10-06T14:42:27Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12315"},{"issue_number":230,"repository":"scikit-learn\/scikit-learn","title":"DecisionTreeClassifier behaviour when there are 2 or more best splitter (a tie among splitters) ","description":"#### Description\r\n\r\nMy issue has been discussed in the past issues #2386  and #8443\r\n\r\nIt concerns `DecisionTreeClassifier` and the `splitter` parameter\r\nIn the [docs](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.decision_path) this parameter has two possible values  `splitter=\"best\"` and `splitter=\"random\"`\r\n\r\n\r\nThe current behaviour when  `splitter=\"best\"` is to shuffle the features at each step and take the best feature to split. In case there is a tie, we take a random one.\r\n\r\nIn some cases, there is a prior on the feature importance or in the ease of the interpretation. \r\n\r\nFor example, some features can be noisy (due to possible data errors or any other cause) and shouldn't be used for splitting unless there is no 'good quality' feature available that does the job. Similarly, some features can be easy to understand and some others can be obscure and shouldn't be used for splitting when possible.\r\n\r\n\r\nThe current implementation doesn't allow having this kind of prior. So when two features tie as the best splitter one get chosen at random. I believe this could be an important improvement. Specially when the trees do not have a `max_depth`, because they tend to over-fit on random features while there may be some \"better\" features (in terms of prior) that do the work.\r\n\r\nHaving an option such as `splitter=\"best_no_shuffle\"` would allow the user to provide the features in order of importance and then when two or more features tie as the best splitters, it would systemically chose the features with the lowest index. This was once proposed [here](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/2386#issuecomment-23148454) \r\n\r\n\r\nIf you believe that this can be a valuable improvement I can try to implement it.\r\nOr maybe you know a better workaround that solves my problem ! \r\n\r\n\r\n\r\n\r\n\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\nI have recovered the code from  #8443 because it is simple and explains the issue\r\n\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.datasets import load_iris\r\nfrom sklearn.tree import DecisionTreeClassifier\r\n\r\niris = load_iris()\r\nx,y  = iris.data, iris.target\r\n\r\ndtc1 = DecisionTreeClassifier(random_state=1)\r\ndtc2 = DecisionTreeClassifier(random_state=2)\r\n\r\nrs = np.random.RandomState(1234)\r\nitr = rs.rand(x.shape[0]) < 0.75\r\n\r\ndtc1.fit(x[itr],y[itr])\r\ndtc2.fit(x[itr],y[itr])\r\n\r\nprint(  (dtc1.predict(x[~itr]) != dtc2.predict(x[~itr])).sum() )\r\n```\r\n\r\n\r\n#### Expected Results\r\n`Should print 0`\r\n\r\n#### Actual Results\r\n`Prints 1`\r\n\r\n#### Versions\r\n\r\nSystem\r\n------\r\n   machine: Linux-4.4.0-135-generic-x86_64-with-Ubuntu-16.04-xenial\r\nexecutable: \/usr\/bin\/python3\r\n    python: 3.5.2 (default, Nov 23 2017, 16:37:01)  [GCC 5.4.0 20160609]\r\n\r\nBLAS\r\n----\r\ncblas_libs: openblas, openblas\r\n    macros: HAVE_CBLAS=None\r\n  lib_dirs: \/usr\/lib\r\n\r\nPython deps\r\n-----------\r\n     scipy: 0.19.0\r\n   sklearn: 0.21.dev0\r\n    pandas: 0.20.3\r\n     numpy: 1.13.3\r\n    Cython: 0.26\r\nsetuptools: 36.7.2\r\n       pip: 9.0.1\r\n\r\n\r\n\r\n","labels":["Bug","help wanted","module:tree"],"created_at":"2018-10-03T14:25:36Z","comments":7,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/12259"},{"issue_number":231,"repository":"scikit-learn\/scikit-learn","title":"Unavoidable \"y_true and y_pred contain different number of classes\" error inside a CV loop","description":"#### Description\r\nDuring cross-validation on a multi-class problem, it's technically possible to have classes present in the test data that don't appear in the training data.\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\n```python\r\nimport numpy as np\r\nfrom sklearn.metrics import make_scorer, log_loss\r\nfrom sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\r\nfrom sklearn.naive_bayes import BernoulliNB\r\n\r\nrs = np.random.RandomState(1389057)\r\n\r\ny = [\r\n    'cow',\r\n    'hedgehog',\r\n    'fox',\r\n    'fox',\r\n    'hedgehog',\r\n    'fox',\r\n    'hedgehog',\r\n    'cow',\r\n    'cow',\r\n    'fox'\r\n]\r\n\r\nx = rs.normal([0, 0], [1, 1], size=(len(y), 2))\r\n\r\nmodel = BernoulliNB()\r\n\r\ncv = StratifiedKFold(4, shuffle=True, random_state=rs)\r\n\r\nparam_dist = {\r\n    'alpha': np.logspace(np.log(0.1), np.log(1), 20)\r\n}\r\n\r\nsearch = RandomizedSearchCV(model, param_dist, 5,\r\n                            scoring=make_scorer(log_loss, needs_proba=True), cv=cv)\r\n\r\nsearch.fit(x, y)\r\n```\r\n\r\n#### Expected Results\r\n\r\nEither:\r\n1. Predicted classes from `predict_proba` are aligned with classes in the full training data, not just the in-fold subset.\r\n2. Classes not in the training data are ignored in the test data.\r\n\r\n#### Actual Results\r\n\r\nPredicted classes from `predict_proba` are aligned with classes in the in-fold subset only, but classes not in the training data are still used in the test data, causing the error.\r\n\r\nI understand that this is normatively \"correct\" behavior, but it makes it hard\/impossible to use in cross-validation with the existing APIs.\r\n\r\nFrom my perspective, the best solution would be to have `RandomizedSearchCV` pass a `labels=self.classes_` argument to its scorer. I'm not sure how well that generalizes.\r\n\r\n#### Versions\r\n\r\n```\r\nLinux-3.10.0-514.26.2.el7.x86_64-x86_64-with-redhat-7.3-Maipo\r\nPython 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) [GCC 7.2.0]\r\nNumPy 1.15.0\r\nSciPy 1.1.0\r\nScikit-Learn 0.19.1\r\n```\r\n","labels":["Bug","module:model_selection"],"created_at":"2018-08-07T20:48:34Z","comments":15,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/11777"},{"issue_number":232,"repository":"scikit-learn\/scikit-learn","title":"Graphical lasso Issue with graph_lasso() method","description":"#### Description\r\n Dear All,\r\n I am working on replicating a paper titled \u201cImproving Mean Variance Optimization through Sparse Hedging Restriction\u201d. The authors\u2019 idea is to use Graphical Lasso algorithm to infuse some bias in the estimation process of the inverse of the sample covariance matrix. The graphical lasso algorithm works perfectly fine in R, but when I use python on the same data with the same parameters I get two sorts of errors:\r\n \r\n1-  If I use coordinate descent (cd ) mode as a solver, I get a floating point error saying that: the matrix is not symmetric positive definite and that the system is too ill-conditioned  for this solver. \u201cFloatingPointError: Non SPD result: the system is too ill-conditioned for this solver. The system is too ill-conditioned for this solver\u201d  (The thing that bugs me is that I tried this solver on a simulated Positive definite matrix and It game me this error)\r\n \r\n2-  If I use the Least Angle Regression (LARS) mode (Which is less stable but recommended for ill-conditioned matrices) I get an  Overflow error stating that the integer is too large to be converted to a float \u201cOverflowError: int too large to convert to float\u201d\r\n \r\nTo my knowledge, unlike C++ and other languages, python is not restricted by an upper maximum for integer numbers (besides the capacity of the machine itself). Whereas the floats are restricted. I think this might be the source of the later problem. (I have also heard in the past that R is much more robust in terms of dealing ill-conditioned matrices). I would be glad to hear you experience with graph lasso in R or python. \r\nWith this email, I have attached a little python code that simulates this problem in a few lines. Any input will be of great appreciation.\r\nThank you all,\r\n \r\nSkander\r\n\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\nfrom sklearn.covariance import graph_lasso\r\nfrom sklearn.datasets import make_spd_matrix\r\n\r\nsymetric_PD_mx= make_spd_matrix(100)\r\n\r\nglout = graph_lasso(emp_cov=symetric_PD_mx, alpha=0.01,mode=\"lars\")\r\n\r\n#### Actual Results\r\n<!-- ---------------------------------------------------------------------------\r\nOverflowError                             Traceback (most recent call last)\r\n<ipython-input-10-9ac318eec6e7> in <module>()\r\n----> 1 glout = graph_lasso(emp_cov=symetric_PD_mx, alpha=0.01,mode=\"lars\")\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\my_env\\lib\\site-packages\\sklearn\\covariance\\graph_lasso_.py in graph_lasso(emp_cov, alpha, cov_init, mode, tol, enet_tol, max_iter, verbose, return_costs, eps, return_n_iter)\r\n    222                             sub_covariance, row, Xy=row, Gram=sub_covariance,\r\n    223                             alpha_min=alpha \/ (n_features - 1), copy_Gram=True,\r\n--> 224                             eps=eps, method='lars', return_path=False)\r\n    225                 # Update the precision matrix\r\n    226                 precision_[idx, idx] = (\r\n\r\n~\\AppData\\Local\\Continuum\\anaconda3\\envs\\my_env\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py in lars_path(X, y, Xy, Gram, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)\r\n    358                 L_ = L[:n_active, :n_active].copy()\r\n    359                 while not np.isfinite(AA):\r\n--> 360                     L_.flat[::n_active + 1] += (2 ** i) * eps\r\n    361                     least_squares, info = solve_cholesky(\r\n    362                         L_, sign_active[:n_active], lower=True)\r\n\r\nOverflowError: int too large to convert to float -->\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\nPython 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\r\nNumPy 1.14.3\r\nSciPy 1.1.0\r\nScikit-Learn 0.19.1\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","module:covariance"],"created_at":"2018-07-03T19:39:12Z","comments":1,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/11417"},{"issue_number":233,"repository":"scikit-learn\/scikit-learn","title":"f_regression takes square root of negative values when constant columns are present","description":"#### Description\r\n`sklearn.feature_selection.f_regression` raises `RuntimeWarning: invalid value encountered in sqrt` when an array with any constant column is passed in. \r\n\r\nIf this is the expected behavior, this issue should be closed. However, it seem suspicious that this only happens when `center` is `True`, and even so it may be worthwhile to check for constant columns and raise a more helpful warning.\r\n\r\nIf the call to `f_regression` below is replaced with\r\n`f_regression(X, y, center=False)` then all values are returned with no NaN's or warnings.\r\n\r\nI'm still not sure what conditions are needed for the error to occur. A constant column usually creates a division by zero warning, which appears to be the correct behavior. This specific array always results in square root of a negative, which is unexpected.\r\n\r\n#### Steps\/Code to Reproduce\r\n```python\r\nimport numpy as np\r\nfrom sklearn.feature_selection import f_regression\r\n\r\nX = np.array([[  0.92  ,   0.2674,  54.934 ,  26.    ],\r\n              [  0.92  ,   0.2674,  54.934 ,  28.    ],\r\n              [  0.92  ,   0.2674,  54.934 ,  23.    ],\r\n              [  0.92  ,   0.2674,  54.934 ,  21.    ],\r\n              [  0.92  ,   0.2674,  54.934 ,  29.    ],\r\n              [  0.92  ,   0.2674,  54.934 ,  25.    ],\r\n              [  0.92  ,   0.2674,  54.934 ,  23.    ],\r\n              [  0.92  ,   0.2674,  54.934 ,  28.    ],\r\n              [  0.92  ,   0.2674,  54.934 ,  27.    ],\r\n              [  0.92  ,   0.2674,  54.934 ,  27.    ]])\r\ny = np.array([ 0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92])\r\n\r\nf_regression(X, y)\r\n```\r\n\r\n#### Expected Results\r\nNo warning raised and no NaN's in output\r\n#### Actual Results\r\n\r\n```\r\n\/usr\/local\/miniconda3\/envs\/testml\/lib\/python3.6\/site-packages\/scikit_learn-0.19.1-py3.6-macosx-10.7-x86_64.egg\/sklearn\/feature_selection\/univariate_selection.py:292: RuntimeWarning: invalid value encountered in sqrt\r\n  n_samples * X_means ** 2)\r\n\/usr\/local\/miniconda3\/envs\/testml\/lib\/python3.6\/site-packages\/scipy\/stats\/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater\r\n  return (self.a < x) & (x < self.b)\r\n\/usr\/local\/miniconda3\/envs\/testml\/lib\/python3.6\/site-packages\/scipy\/stats\/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less\r\n  return (self.a < x) & (x < self.b)\r\n\/usr\/local\/miniconda3\/envs\/testml\/lib\/python3.6\/site-packages\/scipy\/stats\/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal\r\n  cond2 = cond0 & (x <= self.a)\r\n```\r\n```python\r\n(array([        nan, -8.        , -8.        , -8.07593079]),\r\n array([ nan,   1.,   1.,   1.]))\r\n```\r\n#### Another example without the error\r\n```python\r\n...\r\nf_regression(X, y, center=False)\r\n```\r\nreturns \r\n```\r\n(array([ -2.02661983e+16,  -2.02661983e+16,  -2.02661983e+16,\r\n          9.57231884e+02]),\r\n array([  1.00000000e+00,   1.00000000e+00,   1.00000000e+00,\r\n          1.88654093e-10]))\r\n```\r\n\r\n#### Versions\r\nDarwin-15.6.0-x86_64-i386-64bit\r\nPython 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \r\n[GCC 4.2.1 Compatible Clang 4.0.1 (tags\/RELEASE_401\/final)]\r\nNumPy 1.12.1\r\nSciPy 1.1.0\r\nScikit-Learn 0.19.1","labels":["Bug","help wanted","module:feature_selection"],"created_at":"2018-06-30T21:19:10Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/11395"},{"issue_number":234,"repository":"scikit-learn\/scikit-learn","title":"SGDRegressor gets poor fit with sparse matrix","description":"If a sparse matrix is passed into the function, it doesn't throw any error but gets a poor fit **sometimes**. I think it should either throw an error when the parameter is a sparse matrix or convert it into a dense matrix like https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/535\r\n\r\nBelow is some code to compare the differences. (It fits well on boston housing price even when the parameter is a sparse matrix but fails on diabetes.)\r\n\r\n```python\r\nimport numpy\r\nfrom scipy.sparse import csr_matrix\r\nfrom sklearn.datasets import load_diabetes\r\nfrom sklearn.linear_model import SGDRegressor, LinearRegression\r\nfrom sklearn.preprocessing import StandardScaler\r\nimport matplotlib.pyplot as plt\r\n\r\ndiabetes = load_diabetes()\r\nX = diabetes.data\r\n# X = csr_matrix(X)  # Uncomment this line to use a sparse matrix\r\ny = numpy.asarray(diabetes.target)\r\n\r\nscaler = StandardScaler(with_mean=False)\r\nscaler.fit(X)\r\nX = scaler.transform(X)\r\n\r\nestimator = SGDRegressor()\r\n# estimator = LinearRegression()  # LinearRegression works with sparse matrix\r\nestimator.fit(X, y)\r\npredicted = estimator.predict(X)\r\n\r\nfig, ax = plt.subplots()\r\nax.scatter(y, predicted, edgecolors=(0, 0, 0))\r\nax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\r\nax.set_xlabel('Measured')\r\nax.set_ylabel('Predicted')\r\nplt.show()\r\n```\r\nJust stating in the documentation that the input should be a dense matrix is not enough, it may cause surprises like this.\r\n\r\nNumPy 1.14.2\r\nSciPy 1.1.0\r\nScikit-Learn 0.19.1\r\n","labels":["Bug","help wanted","module:linear_model"],"created_at":"2018-05-31T15:53:38Z","comments":24,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/11178"},{"issue_number":235,"repository":"scikit-learn\/scikit-learn","title":"Bug function: reconstruct_from_patches_2d","description":"<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: http:\/\/stackoverflow.com\/questions\/tagged\/scikit-learn\r\n- Mailing List: https:\/\/mail.python.org\/mailman\/listinfo\/scikit-learn\r\nFor more information, see User Questions: http:\/\/scikit-learn.org\/stable\/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nFunction `reconstruct_from_patches_2d` is getting the right answer!!!\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\nCode:\r\n```python\r\nfrom time import time\r\nimport numpy as np\r\nfrom sklearn.feature_extraction.image import extract_patches_2d\r\nfrom sklearn.feature_extraction.image import reconstruct_from_patches_2d\r\n\r\nimg = np.ones((128, 256))\r\n\r\nheight, width = img.shape\r\n\r\n\r\n# Extract all reference patches from the left half of the image\r\nprint('Extracting reference patches...')\r\nt0 = time()\r\npatch_size = (128, 128)\r\ndata = extract_patches_2d(img, patch_size)\r\n#data = data.reshape(data.shape[0], -1)\r\n#intercept = np.mean(data, axis=0)\r\n#data -= intercept\r\nprint('done in %.2fs.' % (time() - t0))\r\n\r\nb = reconstruct_from_patches_2d(data, img.shape)\r\n```\r\n\r\n\r\n#### Actual Results\r\nb should be all ones!!!, The result is NOT!!!\r\n\r\n#### Versions\r\n\r\nWindows-10-10.0.16299-SP0\r\nPython 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]\r\nNumPy 1.14.2\r\nSciPy 1.0.1\r\nScikit-Learn 0.19.1\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","help wanted","module:feature_extraction"],"created_at":"2018-04-03T08:24:10Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/10910"},{"issue_number":236,"repository":"scikit-learn\/scikit-learn","title":"BUG Median not always being calculated correctly for DecisionTrees in the WeightedMedianCalculator","description":"It appears the existing \"_WeightedMedianCalculator.update_median_parameters_post_remove_(...)\" is not working correctly...  Here is what I am seeing with an illustrated example...\r\n\r\n```\r\ny = [0.14038694, 0.4173048, 0.55868983]\r\nsample_weights = [2, 1, 2]\r\n```\r\n\r\nin effect, the above is equivalent to:\r\n\r\n> 0.14038694, 0.14038694, 0.4173048, 0.55868983, 0.55868983\r\n\r\n\r\nThe process starts with these items being added to the _WeightedMedianCalculator_...  \r\nI have outputted key values after each movement (after _update_median_parameters_post_push_ and after _update_median_parameters_post_remove_ method calls):\r\n\r\n\r\n`> PUSH, data: 0.4173048;  orig_median: 0.0; new_median: 0.4173048;....`\r\nThis tells us that the sample line with y = 0.4173048 was added to a new group.  Since there is only one item this is the same as the median!\r\n\r\n\r\n`> PUSH;  data: 0.55868983;  orig_median: 0.4173048; new_median: 0.55868983;....`\r\nNext the 0.55868983 sample line is added to the group **(note that this has a weight of 2).**  We now have in effect:\r\n> 0.4174048, **0.55868983**, 0.55868983\r\n \r\nso the new median becomes 0.55868983 as expected.\r\n\r\n\r\n`> PUSH;  data: 0.14038694;  orig_median: 0.55868983; new_median: 0.4173048;....`\r\nNext the 0.14038694 sample line is added to the group.  We now have in effect:\r\n> 0.14038694, 0.14038694, **0.4173048**, 0.55868983, 0.55868983\r\n\r\nso the new median becomes 0.4173048 as expected.\r\n\r\n\r\n`> POP\/REMOVE;   data: 0.4173048;  orig_median: 0.4173048; new_median: 0.55868983;....`\r\n\r\nThe data equated to the sample line 0.4173048 is removed.  We now have:\r\n> 0.14038694, **0.14038694, 0.55868983**, 0.55868983\r\n\r\nThe new median looks incorrect!  Should it not be:\r\n= (0.14038694 + 0.55868983) \/ 2\r\n= 0.349538385\r\n?\r\n\r\nSteps to reproduce:\r\nUnfortunately it is difficult to provide a python script to demonstrate the above, as it requires debugging private values but can be achieved this way:\r\n\/tree\/utils.pyx -> WeightedMedianCalculator class:\r\n\r\n```\r\n    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil:\r\n        \"\"\"Remove a value from the MedianHeap, removing it\r\n        from consideration in the median calculation\r\n        \"\"\"\r\n        cdef int return_value\r\n        cdef DOUBLE_t original_median\r\n\r\n        if self.size() != 0:\r\n            original_median = self.get_median()\r\n\r\n        #FOR DEBUG:\r\n        cdef int original_k\r\n        cdef DOUBLE_t original_sum_w_0_k\r\n        cdef DOUBLE_t original_total_weight\r\n        original_k = self.k\r\n        original_sum_w_0_k = self.sum_w_0_k\r\n        original_total_weight = self.total_weight\r\n\r\n        return_value = self.samples.remove(data, weight)\r\n        self.update_median_parameters_post_remove(data, weight,\r\n                                                  original_median)\r\n\r\n        with gil:\r\n            print (\" \")\r\n            print (\"POP\/REMOVE, data: \" + str(data)\r\n                + \";  orig_median: \" + str(original_median) \r\n                + \"; new_median: \" + str(self.get_median()) \r\n                + \";  size: \" + str(self.size())\r\n                + \";  weight: \" + str(weight)\r\n                + \";  orig_total_weight: \" + str(original_total_weight)\r\n                + \";  self.total_weight: \" + str(self.total_weight)\r\n                + \";  orig_k: \" + str(original_k)               \r\n                + \";  self.k: \" + str(self.k)\r\n                + \";  orig_sum_w_0_k: \" + str(original_sum_w_0_k)\r\n                + \";  self.sum_w_0_k: \" + str(self.sum_w_0_k)\r\n                )\r\n\r\n        return return_value\r\n```\r\n```\r\n        cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1:\r\n        \"\"\"Push a value and its associated weight to the WeightedMedianCalculator\r\n\r\n        Return -1 in case of failure to allocate memory (and raise MemoryError)\r\n        or 0 otherwise.\r\n        \"\"\"\r\n        cdef int return_value\r\n        cdef DOUBLE_t original_median\r\n\r\n        #FOR DEBUG:\r\n        cdef int original_k\r\n        cdef DOUBLE_t original_sum_w_0_k\r\n        cdef DOUBLE_t original_total_weight\r\n        original_k = self.k\r\n        original_sum_w_0_k = self.sum_w_0_k\r\n        original_total_weight = self.total_weight\r\n\r\n        if self.size() != 0:\r\n            original_median = self.get_median()\r\n        # samples.push (WeightedPQueue.push) uses safe_realloc, hence except -1\r\n        return_value = self.samples.push(data, weight)\r\n        self.update_median_parameters_post_push(data, weight,\r\n                                                original_median)\r\n\r\n        with gil:\r\n            print (\" \")\r\n            print (\"PUSH, data: \" + str(data) \r\n                + \";  orig_median: \" + str(original_median) \r\n                + \"; new_median: \" + str(self.get_median()) \r\n                + \";  size: \" + str(self.size())\r\n                + \";  weight: \" + str(weight)\r\n                + \";  orig_total_weight: \" + str(original_total_weight)\r\n                + \";  self.total_weight: \" + str(self.total_weight)\r\n                + \";  orig_k: \" + str(original_k)               \r\n                + \";  self.k: \" + str(self.k)\r\n                + \";  orig_sum_w_0_k: \" + str(original_sum_w_0_k)\r\n                + \";  self.sum_w_0_k: \" + str(self.sum_w_0_k)\r\n                )\r\n\r\n        return return_value\r\n```\r\n\r\nAfter compilation, calling this script:\r\n\r\n```\r\nfrom sklearn.tree import DecisionTreeRegressor\r\n\r\ntrain_y = [0.4173048,0.55868983,0.14038694]\r\ntrain_X = [[ 15., 9.],\r\n           [ 19., 35.],\r\n           [ 40., 54.]]\r\nsample_weight = [1,2,2]\r\n\r\ndepth = 1\r\n\r\nwineTree = DecisionTreeRegressor(max_depth=depth, criterion='mae', random_state=1)\r\nwineTree.fit(train_X, train_y\r\n    , sample_weight=sample_weight\r\n    )\r\n```\r\n\r\nI think I know where the problem lies and how to fix this issue, but first wanted to ensure this behaviour isn't desired especially considering sample_weights could be represented as floats etc?\r\n\r\n\r\n","labels":["Bug","help wanted","module:tree"],"created_at":"2018-02-28T10:23:42Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/10725"},{"issue_number":237,"repository":"scikit-learn\/scikit-learn","title":"NMF n_components are not properly getting reflected in output when using Grid Search CV","description":"When I executed the example code from this [link ](http:\/\/scikit-learn.org\/0.18\/auto_examples\/plot_compare_reduction.html\r\n) \r\nand analyzed the grid search output (grid.cv_results_['params']), n_components are not properly getting reflected in output.\r\n\r\nPosting a small snippet of output of grid.cv_results_['params']:\r\n```\r\n{'classify__C': 1000,\r\n  'reduce_dim': NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\r\n    **n_components=None,** random_state=None, shuffle=False, solver='cd',\r\n    tol=0.0001, verbose=0),\r\n  'reduce_dim__n_components': 2},\r\n {'classify__C': 1000,\r\n  'reduce_dim': NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\r\n    **n_components=None**, random_state=None, shuffle=False, solver='cd',\r\n    tol=0.0001, verbose=0),\r\n  'reduce_dim__n_components': 4},\r\n {'classify__C': 1000,\r\n  'reduce_dim': NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\r\n    **n_components=None**, random_state=None, shuffle=False, solver='cd',\r\n    tol=0.0001, verbose=0),\r\n  'reduce_dim__n_components': 8},\r\n```\r\nwhere reduce_dim__n_components are updating for NMF but not the actual n_components in NMF\r\n\r\nThanks,\r\nPat\r\n\r\n","labels":["Bug","help wanted","module:decomposition","module:pipeline"],"created_at":"2017-12-15T13:21:52Z","comments":25,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/10329"},{"issue_number":238,"repository":"scikit-learn\/scikit-learn","title":"Problems with score_samples in BayesianGaussianMixture","description":"I have noticed that on many problems the `score_samples()` function in `sklearn.mixture.BayesianGaussianMixture` consistently gives lower test set likelihoods than just taking the posterior mean parameters from the mixture and plugging them into a regular mixture of Gaussians likelihood.\r\n\r\nI have coded up a demo on a toy problem to illustrate:\r\n\r\n```python\r\nimport numpy as np\r\nimport scipy.stats as ss\r\nfrom scipy.special import logsumexp\r\nfrom sklearn.mixture import BayesianGaussianMixture, GaussianMixture\r\n\r\n\r\ndef loglik_mixture(X, w, means, covs):\r\n    N = X.shape[0]\r\n\r\n    w = w \/ np.sum(w)  # Just to be sure normalized\r\n\r\n    loglik = np.zeros((N, len(w)))\r\n    for ii in range(len(w)):\r\n        mu = means[ii, :]\r\n        S = covs[ii, :, :]\r\n        gauss_part = ss.multivariate_normal.logpdf(X, mu, S)\r\n        loglik[:, ii] = np.log(w[ii]) + gauss_part\r\n    loglik = logsumexp(loglik, axis=1)\r\n    return loglik\r\n\r\n\r\ndef simple_data():\r\n    x = np.random.randn(1000, 2) + 1.0\r\n    idx = np.random.rand(1000) <= 0.5\r\n    x[idx, :] = -1 * x[idx, :]\r\n    return x\r\n\r\nnp.random.seed(1234)\r\n\r\ndelta = []\r\nfor _ in range(500):\r\n    x_train = simple_data()\r\n    x_test = simple_data()\r\n\r\n    gmm = GaussianMixture(n_components=2, covariance_type='full')\r\n    gmm.fit(x_train)\r\n    loglik0 = gmm.score_samples(x_test)\r\n    w, means, covs = gmm.weights_, gmm.means_, gmm.covariances_\r\n    loglik1 = loglik_mixture(x_test, w, means, covs)\r\n    # Demonstrate that loglik_mixture() is correct\r\n    np.testing.assert_allclose(loglik0, loglik1)\r\n    print('-' * 10)\r\n    print(f'MLE GMM loglik {np.mean(loglik0)}')\r\n\r\n    bgmm = BayesianGaussianMixture(n_components=2, covariance_type='full')\r\n    bgmm.fit(x_train)\r\n    loglik_bayes0 = bgmm.score_samples(x_test)\r\n    w, means, covs = bgmm.weights_, bgmm.means_, bgmm.covariances_\r\n    loglik_bayes1 = loglik_mixture(x_test, w, means, covs)\r\n    print(f'VB GMM loglik (built-in) {np.mean(loglik_bayes0)}')\r\n    print(f'improvement {np.mean(loglik_bayes0) - np.mean(loglik0)}')\r\n    print(f'VB GMM loglik {np.mean(loglik_bayes1)}')\r\n    print(f'improvement {np.mean(loglik_bayes1) - np.mean(loglik0)}')\r\n\r\n    delta.append(np.mean(loglik_bayes1) - np.mean(loglik_bayes0))\r\nprint(np.mean(delta))\r\nprint(np.mean(np.array(delta) > 0))\r\n```\r\nThis gives for instance:\r\n```\r\nMLE GMM loglik -3.362221\r\nVB GMM loglik (built-in) -3.367131\r\nimprovement -0.004911\r\nVB GMM loglik -3.361099\r\nimprovement 0.001121\r\n```\r\nThe built in score for VBGMM is consistently worse than MLE GMM but often better when the posterior mean is plugged in as a point estimate in loglik_mixture().  On this problem, the loglik_mixture() likelihood is always around 0.006 nats higher than the built in score function.\r\n\r\nThis makes me wonder if there is some normalization issue in the likelihood in _estimate_log_prob().  I am not sure where the derivation for the code in lines 690-698 is.\r\n\r\nThe documentation says it implements Blei and Jordan (2006), which has the posterior predictive in eqn (23).  But that is only an approximation and not necessarily even normalized.\r\n\r\nIf score_samples is used for evaluation purposes, it is not fair to compare VB-GMM to other models if its likelihood is not even normalized!  Some more investigation is needed here.","labels":["Bug","module:mixture"],"created_at":"2017-11-15T20:39:20Z","comments":2,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/10148"},{"issue_number":239,"repository":"scikit-learn\/scikit-learn","title":"Fitting ExpSineSquared kernel to large dimension data throws a ValueError on MacOS","description":"<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: http:\/\/stackoverflow.com\/questions\/tagged\/scikit-learn\r\n- Mailing List: https:\/\/mail.python.org\/mailman\/listinfo\/scikit-learn\r\nFor more information, see User Questions: http:\/\/scikit-learn.org\/stable\/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\n<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->\r\nValueError: \"array must not contain infs or NaNs\" thrown when fitting an ExpSineSquared kernel to large dimension data\r\n\r\n#### Steps\/Code to Reproduce\r\n<!--\r\nExample:\r\n```python\r\nfrom sklearn.feature_extraction.text import CountVectorizer\r\nfrom sklearn.decomposition import LatentDirichletAllocation\r\n\r\ndocs = [\"Help I have a bug\" for i in range(1000)]\r\n\r\nvectorizer = CountVectorizer(input=docs, analyzer='word')\r\nlda_features = vectorizer.fit_transform(docs)\r\n\r\nlda_model = LatentDirichletAllocation(\r\n    n_topics=10,\r\n    learning_method='online',\r\n    evaluate_every=10,\r\n    n_jobs=4,\r\n)\r\nmodel = lda_model.fit(lda_features)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https:\/\/gist.github.com\r\n-->\r\n```py\r\nimport sklearn.gaussian_process as gp\r\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\r\n\r\nwhile True:\r\n    xtr = np.random.rand(25,25)\r\n    ytr = np.random.rand(25)\r\n    model = gp.GaussianProcessRegressor(kernel=ExpSineSquared(),\r\n                                                alpha=1e-5,\r\n                                                n_restarts_optimizer=10,\r\n                                                normalize_y=True)\r\n    model.fit(xtr, ytr)\r\n```\r\n\r\n#### Expected Results\r\n<!-- Example: No error is thrown. Please paste or describe the expected results.-->\r\nNo error is thrown\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback. -->\r\n\r\n```\r\n  File \"<ipython-input-39-119d71c23c9f>\", line 1, in <module>\r\n    runfile('\/Users\/eric\/Google Drive\/Spatial conquest\/Neural Network\/GitHub Repository\/tensorlayer\/benchmarks\/bug.py', wdir='\/Users\/eric\/Google Drive\/Spatial conquest\/Neural Network\/GitHub Repository\/tensorlayer\/benchmarks')\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/spyder\/utils\/site\/sitecustomize.py\", line 710, in runfile\r\n    execfile(filename, namespace)\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/spyder\/utils\/site\/sitecustomize.py\", line 101, in execfile\r\n    exec(compile(f.read(), filename, 'exec'), namespace)\r\n\r\n  File \"\/Users\/eric\/Google Drive\/Spatial conquest\/Neural Network\/GitHub Repository\/tensorlayer\/benchmarks\/bug.py\", line 19, in <module>\r\n    model.fit(xtr, ytr)\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/sklearn\/gaussian_process\/gpr.py\", line 232, in fit\r\n    bounds))\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/sklearn\/gaussian_process\/gpr.py\", line 455, in _constrained_optimization\r\n    if convergence_dict[\"warnflag\"] != 0:\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/scipy\/optimize\/lbfgsb.py\", line 193, in fmin_l_bfgs_b\r\n    **opts)\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/scipy\/optimize\/lbfgsb.py\", line 328, in _minimize_lbfgsb\r\n    f, g = func_and_grad(x)\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/scipy\/optimize\/lbfgsb.py\", line 278, in func_and_grad\r\n    f = fun(x, *args)\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/scipy\/optimize\/optimize.py\", line 292, in function_wrapper\r\n    return function(*(wrapper_args + args))\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/scipy\/optimize\/optimize.py\", line 63, in __call__\r\n    fg = self.fun(x, *args)\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/sklearn\/gaussian_process\/gpr.py\", line 209, in obj_func\r\n    theta, eval_gradient=True)\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/sklearn\/gaussian_process\/gpr.py\", line 419, in log_marginal_likelihood\r\n    except np.linalg.LinAlgError:\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/scipy\/linalg\/decomp_cholesky.py\", line 81, in cholesky\r\n    check_finite=check_finite)\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/scipy\/linalg\/decomp_cholesky.py\", line 20, in _cholesky\r\n    a1 = asarray_chkfinite(a)\r\n\r\n  File \"\/Users\/eric\/Applications\/anaconda\/envs\/NN_on_GPU\/lib\/python3.6\/site-packages\/numpy\/lib\/function_base.py\", line 1215, in asarray_chkfinite\r\n    \"array must not contain infs or NaNs\")\r\n\r\nValueError: array must not contain infs or NaNs\r\n```\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\nDarwin-16.7.0-x86_64-i386-64bit\r\nPython 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:14:59) \r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\r\nNumPy 1.13.1\r\nSciPy 0.19.1\r\nScikit-Learn 0.19.0\r\n\r\n#### Additional comment\r\nThis problem seems MacOS specific as it does not occur on my linux system with exactly the same versions\r\nLinux-4.10.0-33-generic-x86_64-with-debian-stretch-sid\r\nPython 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:51:32) \r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\r\nNumPy 1.13.1\r\nSciPy 0.19.1\r\nScikit-Learn 0.19.0\r\n\r\nIn terms of investigation, the furthest i got was to see that `kernel(self.X_train_)` in sklearn\/gaussian_process\/gpr.py\", line 419, log_marginal_likelihood was diverging to infinity\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","module:gaussian_process"],"created_at":"2017-09-23T18:10:22Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/9824"},{"issue_number":240,"repository":"scikit-learn\/scikit-learn","title":"Bad results for LabelPropagation (and LabelSpreading) when using any kernel other than \"rbf\" or \"knn\"","description":"<!--\r\nIf your issue is a usage question, submit it here instead:\r\n- StackOverflow with the scikit-learn tag: http:\/\/stackoverflow.com\/questions\/tagged\/scikit-learn\r\n- Mailing List: https:\/\/mail.python.org\/mailman\/listinfo\/scikit-learn\r\nFor more information, see User Questions: http:\/\/scikit-learn.org\/stable\/support.html#user-questions\r\n-->\r\n\r\n<!-- Instructions For Filing a Bug: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nPoor results always obtained when using sklearn.semi_supervised.LabelPropagation (or LabelSpreading) with any (callable) kernel other than the predefined \"knn\", \"rbf\", or sklearn.metrics.pairwise.rbf_kernel\r\n\r\n#### Steps\/Code to Reproduce\r\nExample:\r\n```python\r\nfrom sklearn.semi_supervised import LabelPropagation, LabelSpreading\r\nfrom sklearn.metrics.pairwise import pairwise_kernels, rbf_kernel\r\n\r\ndef my_kernel(X, Z):\r\n        # This works good :\r\n\treturn rbf_kernel(X, Z)\r\n        # This seems to always work bad for any choice of metric (and parameters), except for \"rbf\" :\r\n\treturn pairwise_kernels(X, Z, metric=\"cosine\")\r\n\r\nh = LabelSpreading(kernel=my_kernel, n_neighbors=7, alpha=0.2).fit(X_labelled+X_unlabelled, Y_labelled+[-1 for _ in X_unlabelled])\r\nprint \"Result:\", 100*accuracy_score( Y_test, h.predict(X_test) )\r\n```\r\n\r\nHere is a complete other example using the optical digits dataset:\r\n```python\r\nfrom sklearn.metrics import accuracy_score\r\nfrom sklearn import datasets \r\nfrom sklearn.semi_supervised import LabelPropagation, LabelSpreading\r\nfrom sklearn.utils import shuffle\r\nfrom sklearn.metrics.pairwise import pairwise_kernels, rbf_kernel\r\n\r\ndef my_kernel(X, Z):\r\n\t# return rbf_kernel(X, Z, gamma=0.1) # works good\r\n\treturn pairwise_kernels(X, Z, metric=\"cosine\")  # works bad\r\n\t\r\nif __name__ == \"__main__\":\r\n\tdigits = datasets.load_digits()\r\n\tX, Y = shuffle( list(digits.data), list(digits.target) )\r\n\tLx = X[:50]; Ly = Y[:50]; Ux = X[50:]; Uy = Y[50:]\r\n\t\r\n\tprint Ly\r\n\th = LabelSpreading(kernel=my_kernel, n_neighbors=7, alpha=0.2).fit(Lx+Ux, Ly+[-1 for _ in Ux])\r\n\tprint \"Result:\", 100*accuracy_score( Uy, h.predict(Ux) )\r\n```\r\n\r\n#### Expected Results\r\nI expected kernels such as: sigmoid, polynomial, poly, linear, cosine (tested with various parameters) to perform reasonably good, but all of those give extremely poor results.\r\n\r\n#### Actual Results\r\nOnly kernel = \"knn\", or \"rbf\" (or a callable corresponding to an rbf kernel) gives good results.\r\n\r\n#### Versions\r\nLinux-3.10.0-514.6.1.el7.x86_64-x86_64-with-centos-7.3.1611-Core\r\n('Python', '2.7.5 (default, Nov  6 2016, 00:28:07) \\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)]')\r\n('NumPy', '1.11.2')\r\n('SciPy', '0.18.1')\r\n('Scikit-Learn', '0.19.0')\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","module:metrics","module:semi_supervised"],"created_at":"2017-09-09T21:05:59Z","comments":10,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/9722"},{"issue_number":241,"repository":"scikit-learn\/scikit-learn","title":"least_angle.py in lars_path - shapes not aligned on properly formatted data with specific alpha","description":"Encountered a similar error to https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/5873 - when running the following code:\r\n\r\n`a_selection = RandomizedLasso(alpha=0.025, normalize=False, n_jobs=1, random_state=42)\r\n a_selection.fit(X=x_sub, y=y_sub)`\r\n    \r\nI can't share the data as it's from clinical trials, but what I have noticed is that the error disappears (for this particular fit) when I remove the alpha parameter. The code takes 2 days to complete so I am worried a different alpha will break a different dataset input. The data frame is properly formatted, the row number fits the labels and there are no NaNs. The error I get is:\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/linear_model\/randomized_l1.py in fit(self, X, y)\r\n    110             n_jobs=self.n_jobs, verbose=self.verbose,\r\n    111             pre_dispatch=self.pre_dispatch, random_state=self.random_state,\r\n--> 112             sample_fraction=self.sample_fraction, **params)\r\n    113 \r\n    114         if scores_.ndim == 1:`\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/memory.py in __call__(self, *args, **kwargs)\r\n    281 \r\n    282     def __call__(self, *args, **kwargs):\r\n--> 283         return self.func(*args, **kwargs)\r\n    284 \r\n    285     def call_and_shelve(self, *args, **kwargs):`\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/linear_model\/randomized_l1.py in _resample_model(estimator_func, X, y, scaling, n_resampling, n_jobs, verbose, pre_dispatch, random_state, sample_fraction, **params)\r\n     52                 verbose=max(0, verbose - 1),\r\n     53                 **params)\r\n---> 54             for _ in range(n_resampling)):\r\n     55         scores_ += active_set\r\n     56 `\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/parallel.py in __call__(self, iterable)\r\n    756             # was dispatched. In particular this covers the edge\r\n    757             # case of Parallel used with an exhausted iterator.\r\n--> 758             while self.dispatch_one_batch(iterator):\r\n    759                 self._iterating = True\r\n    760             else:`\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/parallel.py in dispatch_one_batch(self, iterator)\r\n    606                 return False\r\n    607             else:\r\n--> 608                 self._dispatch(tasks)\r\n    609                 return True\r\n    610 `\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/parallel.py in _dispatch(self, batch)\r\n    569         dispatch_timestamp = time.time()\r\n    570         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)\r\n--> 571         job = self._backend.apply_async(batch, callback=cb)\r\n    572         self._jobs.append(job)\r\n    573 `\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/_parallel_backends.py in apply_async(self, func, callback)\r\n    107     def apply_async(self, func, callback=None):\r\n    108         \"\"\"Schedule a func to be run\"\"\"\r\n--> 109         result = ImmediateResult(func)\r\n    110         if callback:\r\n    111             callback(result)`\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/_parallel_backends.py in __init__(self, batch)\r\n    324         # Don't delay the application, to avoid keeping the input\r\n    325         # arguments in memory\r\n--> 326         self.results = batch()\r\n    327 \r\n    328     def get(self):`\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/parallel.py in __call__(self)\r\n    129 \r\n    130     def __call__(self):\r\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\r\n    132 \r\n    133     def __len__(self):`\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/externals\/joblib\/parallel.py in <listcomp>(.0)\r\n    129 \r\n    130     def __call__(self):\r\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\r\n    132 \r\n    133     def __len__(self):`\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/linear_model\/randomized_l1.py in _randomized_lasso(X, y, weights, mask, alpha, verbose, precompute, eps, max_iter)\r\n    171                                       copy_Gram=False, alpha_min=np.min(alpha),\r\n    172                                       method='lasso', verbose=verbose,\r\n--> 173                                       max_iter=max_iter, eps=eps)\r\n    174 \r\n    175     if len(alpha) > 1:`\r\n\r\n`~\/miniconda3\/lib\/python3.6\/site-packages\/sklearn\/linear_model\/least_angle.py in lars_path(X, y, Xy, Gram, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)\r\n    442 \r\n    443                 # TODO: this could be updated\r\n--> 444                 residual = y - np.dot(X[:, :n_active], coef[active])\r\n    445                 temp = np.dot(X.T[n_active], residual)\r\n    446 `\r\n\r\n`ValueError: shapes (49,17) and (16,) not aligned: 17 (dim 1) != 16 (dim 0)`\r\n\r\nVersions:\r\nLinux-4.10.0-32-generic-x86_64-with-debian-stretch-sid\r\nPython 3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:09:58)\r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\r\nNumPy 1.12.1\r\nSciPy 0.19.1\r\nScikit-Learn 0.18.2\r\n\r\nHappy to provide any additional information if needed.\r\n\r\nLastly, I get a message that RandomizedLasso with be deprecated - what will replace it's functionality? Setting n_jobs parameter to anything beyond 1 breaks the code, which was already reported - hope the replacement will fix that.\r\n\r\nMany thanks!","labels":["Bug","help wanted","module:linear_model"],"created_at":"2017-08-22T03:48:20Z","comments":30,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/9603"},{"issue_number":242,"repository":"scikit-learn\/scikit-learn","title":"Multioutput-multiclass estimators have broken score method.","description":"It looks to me like the decision trees use ``accuracy_score`` for their ``score`` but ``accuracy_score`` doesn't document that it's supporting ``multiclass-multioutput`` which the trees do.\r\n\r\n~~I guess the ``y_true == y_pred`` works in this case, but it should be documented.\r\nThere's no list of scores supporting multiclass-multioutput in the docs, and that should be fixed, too.~~\r\n\r\nUpdate:\r\nCalling ``score`` in this case errors :-\/","labels":["Bug","API","module:multioutput","module:multiclass"],"created_at":"2017-07-19T16:34:13Z","comments":10,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/9414"},{"issue_number":243,"repository":"scikit-learn\/scikit-learn","title":"PCA, LDA, unexpected explained_variance_ratio","description":"```python\r\nfrom sklearn import datasets\r\nfrom sklearn.decomposition import PCA\r\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\n\r\niris = datasets.load_iris()\r\n\r\nX = iris.data\r\ny = iris.target\r\ntarget_names = iris.target_names\r\n\r\n#### dimensionality reduction using PCA\r\npca = PCA(n_components=2)\r\nX_r = pca.fit(X).transform(X)\r\n\r\n#### Percentage of variance explained for each components\r\nprint('PCA: explained variance ratio (first two components): %s'\r\n      % str(pca.explained_variance_ratio_))\r\n\r\n#### dimensionality reduction using LDA\r\nlda = LinearDiscriminantAnalysis(n_components=2)\r\nX_r2 = lda.fit(X, y).transform(X)\r\n\r\nprint('LDA: explained variance ratio (first two components): %s'\r\n      % str(lda.explained_variance_ratio_))\r\n```\r\n#### Expected Results\r\nThe first componet of the PCA has a larger variance ratio than that from the first componet from LDA.\r\n\r\n#### Actual Results\r\nPCA: explained variance ratio (first two components): [ 0.925  0.053]\r\nLDA: explained variance ratio (first two components): [ 0.991  0.009]\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport numpy; print(\"NumPy\", numpy.__version__)\r\nimport scipy; print(\"SciPy\", scipy.__version__)\r\nimport sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n-->\r\n\r\nDarwin-14.5.0-x86_64-i386-64bit\r\nPython 3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) \r\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]\r\nNumPy 1.12.1\r\nSciPy 0.19.1\r\nScikit-Learn 0.18.2\r\n\r\n<!-- Thanks for contributing! -->","labels":["Bug","help wanted","module:decomposition"],"created_at":"2017-07-18T16:50:13Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/9400"},{"issue_number":244,"repository":"scikit-learn\/scikit-learn","title":"LatentDirichletAllocation  Perplexity too big on Wiki dump","description":"#### Description\r\nLDA Perplexity too big on Wiki dump\r\n\r\n#### Steps\/Code to Reproduce\r\nI used `gensim` `WikiCorpus` to obtain the Bag-of-Words for each document, then vectorised it using `scikit-learn` `CountVectorizer`. All is from the En Wiki dump dated 2017-04-10. I'm using the latest `scikit-learn`.\r\n\r\nThe LDA used non-informative prior,\r\n\r\n```python\r\nlda = LatentDirichletAllocation(\r\n    n_topics=20,\r\n    doc_topic_prior=1.0,\r\n    topic_word_prior=1.0,\r\n    max_iter=1000,\r\n    total_samples=n_docs,\r\n    n_jobs=-1\r\n)\r\n\r\nfor _ in range(runs):\r\n    lda.partial_fit(training_sample)\r\n    print(np.log(lda.perplexity(test_docs)))\r\n```\r\n\r\n#### Expected Results\r\n7.x - 8.x, which is what the Spark LDA would give and a personal code as well. As a baseline, the entropy (log-perplexity) of the mean of all the docs is 8.3 (I manually computed the entropy of M1 of the generated corpus for Python, it gives 8.3; for the corpus generated for Spark, it also gave 8.3). Normally after seeing 20% of data it should print out 8.3 or inferior.\r\n\r\n#### Actual Results\r\n13.8 all the way\r\n\r\n#### Versions\r\nAll Python packages are of the latest versions as of reporting.\r\n\r\n","labels":["Bug","help wanted","module:decomposition"],"created_at":"2017-05-27T21:22:58Z","comments":18,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8943"},{"issue_number":245,"repository":"scikit-learn\/scikit-learn","title":"train_test_split fails for too many values (32bit only)","description":"Consider the following code:\r\n\r\n```py\r\nimport numpy as np\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nn = 10000\r\ny = np.random.randint(0, 2, size=n)\r\n\r\ny_train, y_test = train_test_split(y, train_size=int(n\/2),\r\n                                   test_size=int(n\/2), stratify=y, random_state=123)\r\n\r\nprint('num train: {}'.format(len(y_train)))\r\nprint('train mean: {}'.format(y_train.mean()))\r\nprint('num test: {}'.format(len(y_test)))\r\nprint('test mean: {}'.format(y_test.mean()))\r\n```\r\n\r\nWhen n=10,000, I correctly obtain:\r\n```py\r\nnum train: 5000\r\ntrain mean: 0.4958\r\nnum test: 5000\r\ntest mean: 0.4958\r\n```\r\n\r\nBut for larger n, such as n=100,000, I get the following error:\r\n```pytb\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n\/home\/scott\/Development\/scratch\/sklearn\/stratify.py in <module>()\r\n      6 y = np.random.randint(0, 2, size=n)\r\n      7 \r\n----> 8 y_train, y_test = train_test_split(y, train_size=n\/2, test_size=n\/2, stratify=y, random_state=123)\r\n      9 \r\n     10 print 'num train: {}'.format(len(y_train))\r\n\r\n\/home\/scott\/anaconda\/lib\/python2.7\/site-packages\/sklearn\/model_selection\/_split.pyc in train_test_split(*arrays, **options)\r\n   1700     train, test = next(cv.split(X=arrays[0], y=stratify))\r\n   1701     return list(chain.from_iterable((safe_indexing(a, train),\r\n-> 1702                                      safe_indexing(a, test)) for a in arrays))\r\n   1703 \r\n   1704 \r\n\r\n\/home\/scott\/anaconda\/lib\/python2.7\/site-packages\/sklearn\/model_selection\/_split.pyc in <genexpr>((a,))\r\n   1700     train, test = next(cv.split(X=arrays[0], y=stratify))\r\n   1701     return list(chain.from_iterable((safe_indexing(a, train),\r\n-> 1702                                      safe_indexing(a, test)) for a in arrays))\r\n   1703 \r\n   1704 \r\n\r\n\/home\/scott\/anaconda\/lib\/python2.7\/site-packages\/sklearn\/utils\/__init__.pyc in safe_indexing(X, indices)\r\n    110             return X.take(indices, axis=0)\r\n    111         else:\r\n--> 112             return X[indices]\r\n    113     else:\r\n    114         return [X[idx] for idx in indices]\r\n\r\nIndexError: arrays used as indices must be of integer (or boolean) type\r\n```\r\n\r\nAnd for n=1,000,000, I don't get an exception, instead the strange results:\r\n```py\r\nnum train: 1785\r\ntrain mean: 0.414565826331\r\nnum test: 894\r\ntest mean: 0.414988814318\r\n```\r\n\r\nWhy is this? Is this a bug? Does train_test_split fail with too many values?","labels":["Bug","help wanted","module:model_selection"],"created_at":"2017-04-17T23:10:11Z","comments":10,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8755"},{"issue_number":246,"repository":"scikit-learn\/scikit-learn","title":"Surprising result in BayesianGaussianMixture","description":"I'm surprised by the results of BayesianGaussianMixture\r\n```python\r\nfrom sklearn.mixture import BayesianGaussianMixture\r\nfrom sklearn.datasets import make_blobs\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n%matplotlib inline\r\n\r\nrng = np.random.RandomState(3)\r\nX, y = make_blobs(n_samples=500, centers=10, random_state=rng, cluster_std=[rng.gamma(1.5) for i in range(10)])\r\n\r\nfig, axes = plt.subplots(2, 4)\r\ngammas = np.logspace(-5, 5, 4)\r\nfor gamma, ax in zip(gammas, axes.T):\r\n    bgmm = BayesianGaussianMixture(n_components=10, weight_concentration_prior=gamma).fit(X)\r\n    ax[0].scatter(X[:, 0], X[:, 1], s=5, alpha=.6, c=plt.cm.Vega10(bgmm.predict(X)))\r\n    ax[0].set_title(\"gamma={:.2f}\".format(gamma))\r\n    ax[1].bar(range(10), bgmm.weights_)\r\n```\r\n![image](https:\/\/cloud.githubusercontent.com\/assets\/449558\/24217565\/b050eca4-0f16-11e7-9806-ec85f9154ff3.png)\r\n\r\nI'm changing gamma pretty drastically but the distribution of weights doesn't seem to change. Is that expected?","labels":["Bug","help wanted","module:mixture"],"created_at":"2017-03-22T19:46:53Z","comments":12,"reactions":2,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8632"},{"issue_number":247,"repository":"scikit-learn\/scikit-learn","title":"Bug: BaggingClassifier for multiclass usage","description":"Hi, \r\n\r\nThe BaggingClassifier does not check if the number of classes of the random drawn samples for one of its estimators matches the number of classes in the dataset, resulting in an error message when its predict method is used: \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nSub-process traceback:\r\n---------------------------------------------------------------------------\r\nValueError                                         Thu Feb 16 11:17:13 2017\r\nPID: 25287                                    Python 2.7.5: \/usr\/bin\/python\r\n...........................................................................\r\n.local\/lib\/python2.7\/site-packages\/sklearn\/externals\/joblib\/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\r\n    126     def __init__(self, iterator_slice):\r\n    127         self.items = list(iterator_slice)\r\n    128         self._size = len(self.items)\r\n    129\r\n    130     def __call__(self):\r\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\r\n        func = <function _parallel_predict_proba>\r\n        args = ([SVC(C=100, cache_size=200, class_weight=None, co...5695085, shrinking=True,\r\n  tol=0.001, verbose=10), SVC(C=100, cache_size=200, class_weight=None, co...8919740, shrinking=True,\r\n  tol=0.001, verbose=10)], [array([   0,    1,    2, ..., 2045, 2046, 2047]), array([   0,    1,    2, ..., 2045, 2046, 2047])], memmap([[  1.21488877e-01,  -6.84937861e-01,  -5...022708e-05,   1.09372859e-06,  -8.55540498e-06]]), 1000)\r\n        kwargs = {}\r\n        self.items = [(<function _parallel_predict_proba>, ([SVC(C=100, cache_size=200, class_weight=None, co...5695085, shrinking=True,\r\n  tol=0.001, verbose=10), SVC(C=100, cache_size=200, class_weight=None, co...8919740, shrinking=True,\r\n  tol=0.001, verbose=10)], [array([   0,    1,    2, ..., 2045, 2046, 2047]), array([   0,    1,    2, ..., 2045, 2046, 2047])], memmap([[  1.21488877e-01,  -6.84937861e-01,  -5...022708e-05,   1.09372859e-06,  -8.55540498e-06]]), 1000), {})]\r\n    132\r\n    133     def __len__(self):\r\n    134         return self._size\r\n    135\r\n\r\n...........................................................................\r\n.local\/lib\/python2.7\/site-packages\/sklearn\/ensemble\/bagging.py in _parallel_predict_proba(estimators=[SVC(C=100, cache_size=200, class_weight=None, co...5695085, shrinking=True,\r\n  tol=0.001, verbose=10), SVC(C=100, cache_size=200, class_weight=None, co...8919740, shrinking=True,\r\n  tol=0.001, verbose=10)], estimators_features=[array([   0,    1,    2, ..., 2045, 2046, 2047]), array([   0,    1,    2, ..., 2045, 2046, 2047])], X=memmap([[  1.21488877e-01,  -6.84937861e-01,  -5...022708e-05,   1.09372859e-06,  -8.55540498e-06]]), n_classes=1000)\r\n    130     for estimator, features in zip(estimators, estimators_features):\r\n    131         if hasattr(estimator, \"predict_proba\"):\r\n    132             proba_estimator = estimator.predict_proba(X[:, features])\r\n    133\r\n    134             if n_classes == len(estimator.classes_):\r\n--> 135                 proba += proba_estimator\r\n        proba = array([[ 0.00130233,  0.00013968,  0.00144125, ....  0.00016293,\r\n         0.00010567,  0.00053245]])\r\n        proba_estimator = array([[  1.02577963e-03,   3.75469340e-04,   9....362413e-05,   1.45631109e-04,   3.04322015e-04]])\r\n    136\r\n    137             else:\r\n    138                 proba[:, estimator.classes_] += \\\r\n    139                     proba_estimator[:, range(len(estimator.classes_))]\r\n\r\nValueError: operands could not be broadcast together with shapes (8009,1000) (8009,999) (8009,1000)\r\n___________________________________________________________________________\r\n```\r\n\r\nIt would be nice to get a warning message, if the number of classes used to train an estimator in the BaggingClassifier would not match the overall number of classes in the trainingsset. \r\n\r\n\r\n","labels":["Bug","help wanted","module:ensemble"],"created_at":"2017-02-20T16:49:11Z","comments":3,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8409"},{"issue_number":248,"repository":"scikit-learn\/scikit-learn","title":"Error in using multi-label classification in partial_fit() in OvR","description":"- StackOverflow Question: https:\/\/stackoverflow.com\/questions\/42280439\/multi-label-out-of-core-learning-for-text-data-valueerror-on-partial-fit\r\n\r\n#### Description\r\nWhen using OneVsRestClassifier() with partial_fit() method, errors are thrown. When using fit(), no errors are thrown and everything works.\r\n\r\n#### Steps\/Code to Reproduce\r\n```\r\nfrom sklearn.naive_bayes import MultinomialNB\r\nfrom sklearn.feature_extraction.text import HashingVectorizer\r\nfrom sklearn.preprocessing import MultiLabelBinarizer\r\nfrom sklearn.multiclass import OneVsRestClassifier\r\nimport numpy as np\r\n\r\ncategories = ['a','b','c']\r\nX = [\"This is a test\", \"This is another attempt\", \"And this is a test too!\"]\r\nY = [['a', 'b'],['b', 'c'],['a', 'b']] \r\n\r\nmlb = MultiLabelBinarizer(classes=categories)\r\nvectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,         non_negative=True)\r\nclf = OneVsRestClassifier(MultinomialNB(alpha=0.01))\r\n\r\nX_train = vectorizer.fit_transform(X)\r\nY_train = mlb.fit_transform(Y)\r\n\r\n- Case1\r\nclf.partial_fit(X_train, Y_train, categories)\r\n- Case2\r\nclf.partial_fit(X_train, Y_train, mlb.transform(Y))\r\n```\r\n#### Description of code\r\n\r\n- Case1   Using classes=categories without transforming \r\n```partial_fit(X_train, Y_train, classes=categories)```\r\n\r\n        ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n\r\n- Case2   Using classes=mlb.transform(categories) i.e. after transforming from same multilabelbinarizer\r\n```partial_fit(X_train, Y_train, classes=mlb.transform(categories))```\r\n\r\n         ValueError: The object was not fitted with multilabel input.\r\n\r\n#### Expected Results\r\nNo error is thrown as when using fit().\r\n\r\n#### Actual Results\r\n- Case1\r\n> Traceback\r\n (most recent call last):\r\n  File \"\/path_to_module\/Check.py\", line 18, in <module>\r\n    clf.partial_fit(X_train, Y_train, categories)\r\n  File \"\/library\/python2.7\/dist-packages\/sklearn\/multiclass.py\", line 260, in partial_fit\r\n    if np.setdiff1d(y, self.classes_):\r\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\r\n\r\n- Case2\r\n> Traceback\r\n (most recent call last):\r\n  File \"\/path_to_module\/Check.py\", line 18, in <module>\r\n    clf.partial_fit(X_train, Y_train, mlb.transform(Y))\r\n  File \"\/library\/python2.7\/dist-packages\/sklearn\/multiclass.py\", line 265, in partial_fit\r\n    Y = self.label_binarizer_.transform(y)\r\n  File \"\/library\/python2.7\/dist-packages\/sklearn\/preprocessing\/label.py\", line 329, in transform\r\n    raise ValueError(\"The object was not fitted with multilabel\"\r\nValueError: The object was not fitted with multilabel input.\r\n\r\n#### Observation\r\n- In Case1, the error is because https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/multiclass.py#L260 is returning an array of booleans whereas it expects a single boolean value hence the error. But couldnt find what to do about it. How to pass Y or classes into it.\r\n\r\n- In Case2, the error occurs because partial_fit() calls the _check_partial_fit_first_call() function which sets the clf.classes_ in a different way using unique_labels() as seen here https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/utils\/multiclass.py#L308.\r\nThese unique labels are passed to clf.label_binarizer_ in this line https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/multiclass.py#L258, which leads to it assuming type of targets as 'multiclass' whereas actual type of target is multilabel, hence this error.\r\nThe fit() method handles classes_ in a different way (doesnt use unique_labels) and hence everything works correctly \r\n\r\n#### Versions\r\nLinux-3.16.0-77-generic-x86_64-with-Ubuntu-14.04-trusty\r\n('Python', '2.7.6 (default, Oct 26 2016, 20:30:19) \\n[GCC 4.8.4]')\r\n('NumPy', '1.12.0')\r\n('SciPy', '0.18.1')\r\n('Scikit-Learn', '0.18.1')\r\n\r\n","labels":["Bug","help wanted","module:multiclass"],"created_at":"2017-02-17T11:43:27Z","comments":18,"reactions":6,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8381"},{"issue_number":249,"repository":"scikit-learn\/scikit-learn","title":"Bug in bfgs gradient computation of MLPRegressor with multiple output neurons","description":"When implementing a special Neural Network based on MLPRegressor, I found the following problem when using bfgs training and multiple output neurons (I did not look into the other training methods):\r\n- The 'squared_loss' implementation uses np.mean to compute the overall loss. Thus, the method divides by the number of samples and the number of output neurons\/features included in the dimensions of y_true - y_pred.\r\n- The gradient computations do not include the number of output neurons. Gradients are only divided by the number of samples (_compute_loss_grad)\r\nOverall, this leads to the fact, that the gradient has a wrong scaling by the number of output neurons.\r\nAs the search direction is still alright, this does not cause too much pain. Still, it should be fixed.\r\n\r\nIn case this is not clear, I can see that I create a minimal example.\r\n\r\nCheers!\r\n\r\n#### Versions\r\n>>> import platform; print(platform.platform())\r\nLinux-3.16.0-4-amd64-x86_64-with-debian-8.5\r\n>>> import sys; print(\"Python\", sys.version)\r\n('Python', '2.7.9 (default, Mar  1 2015, 12:57:24) \\n[GCC 4.9.2]')\r\n>>> import numpy; print(\"NumPy\", numpy.__version__)\r\n('NumPy', '1.10.4')\r\n>>> import scipy; print(\"SciPy\", scipy.__version__)\r\n('SciPy', '0.14.0')\r\n>>> import sklearn; print(\"Scikit-Learn\", sklearn.__version__)\r\n('Scikit-Learn', '0.18.1')\r\n\r\n","labels":["Bug","help wanted","module:neural_network"],"created_at":"2017-02-13T14:25:25Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8349"},{"issue_number":250,"repository":"scikit-learn\/scikit-learn","title":"Bug in metrics.classification._check_targets?","description":"#### Description\r\nI came across this issue while checking a test in `test_matthews_corrcoef`. In this test two label vectors are defined. The groundtruth vector is filled with `a`'s and `b`'s. The prediction vector is filled with `-1`'s and `0`'s. The `_check_targets` in `sklearn.metrics.classification` reports these vectors are binary when it seems to me it should be multiclass instead.\r\n\r\nIf this is the correct behavior and these pair of vectors should be classified as binary, then there would need to be some association between the numbers in the prediction vector and letters in the groundtruth vector and this association should be captured in the `LabelEncoder` used latter in the `matthews_corrcoef` function, but it is not. So, there is either a bug in `_check_inputs` or the `LabelEncoder`. \r\n\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\nThe following code takes the bit of the `test_matthews_corrcoef` and `matthews_corrcoef` functions that produce the odd states. \r\n\r\n```\r\n    from sklearn.preprocessing import label_binarize, LabelEncoder\r\n    from sklearn.metrics.classification import _check_targets\r\n    import numpy as np\r\n    rng = np.random.RandomState(0)\r\n    y_true = [\"a\" if i == 0 else \"b\" for i in rng.randint(0, 2, size=20)]\r\n    y_pred = label_binarize(y_true, [\"a\", \"b\"]) * -1\r\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\r\n    lb = LabelEncoder()\r\n    lb.fit(np.hstack([y_true, y_pred]))\r\n    y_true = lb.transform(y_true)\r\n    y_pred = lb.transform(y_pred)\r\n    print('y_type = %r' % (y_type,))\r\n    print('lb.classes_ = %r' % (lb.classes_,))\r\n    print('y_true = %r' % (y_true,))\r\n    print('y_pred = %r' % (y_pred,))\r\n```\r\n\r\nThe initial values of y_true and y_pred are: \r\n```\r\ny_true = [u'a', u'b', u'b', u'a', u'b', u'b', u'b', u'b', u'b', u'b', u'b', u'a', u'a', u'b', u'a', u'a', u'a', u'a', u'a', u'b']\r\ny_pred = [[0], [-1], [-1], [0], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [0], [0], [-1], [0], [0], [0], [0], [0], [-1]]\r\n```\r\n\r\nAfter they leave _check_inputs they are\r\n```\r\ny_true = array([u'a', u'b', u'b', u'a', u'b', u'b', u'b', u'b', u'b', u'b', u'b',\r\n       u'a', u'a', u'b', u'a', u'a', u'a', u'a', u'a', u'b'], \r\n      dtype='<U1')\r\n\r\ny_pred =  array([ 0, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1,  0,  0,  0, 0,  0, -1])\r\n```\r\n\r\nAt the end the values are: \r\n\r\n#### Expected Results\r\n```\r\ny_type = 'multiclass'\r\nlb.classes_ = array([u'-1', u'0', u'a', u'b'], \r\n      dtype='<U21')\r\ny_true = array([2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3])\r\ny_pred = array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0])\r\n```\r\n\r\nOR \r\n\r\n```\r\ny_type = 'binary'\r\nlb.classes_ = array([u'a', u'b'], dtype='<U21')\r\ny_true = array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\r\ny_pred = array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0])\r\n```\r\n\r\n#### Actual Results\r\n\r\n```\r\ny_type = 'binary'\r\nlb.classes_ = array([u'-1', u'0', u'a', u'b'], \r\n      dtype='<U21')\r\ny_true = array([2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3])\r\ny_pred = array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0])\r\n```\r\n\r\nIt doesn't make sense to me that `_check_inputs` would output that these cases are binary when the LabelEncoder ends up encoding 4 classes and transforming them as such. \r\n\r\nFortunately, this behavior does not cause `matthews_corrcoef` to produce the wrong value because the vectors are centered in the next line of code which effectively undoes the bug. \r\n\r\nHowever, this is causing me issues because I'm attempting to extend `matthews_corrcoef` to the multiclass case (in PR #8094) and I need to be able to distinguish this case from the case where a predicted class exists that is not in the groundtruth or a groundtruth class exists that was not predicted. \r\n\r\n#### Thoughts on a fix\r\n\r\nMy idea for resolving this issue is to have `_check_inputs` check the type of a concatenated version of `y_true` and `y_pred`, however I'm not sure how to go about this with more complex types like `multilabel-indicator`. \r\n\r\n#### Versions\r\nLinux-3.13.0-105-generic-x86_64-with-Ubuntu-14.04-trusty\r\nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \r\n[GCC 4.8.2]\r\nNumPy 1.11.2\r\nSciPy 0.18.0\r\nScikit-Learn 0.19.dev0","labels":["Bug","module:metrics"],"created_at":"2016-12-21T21:48:35Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8098"},{"issue_number":251,"repository":"scikit-learn\/scikit-learn","title":"OneVsOneClassifier decision function shape non-standard","description":"For binary tasks, OvO has a shape of ``(n_samples, 2)`` which pretty much violates our standards.\r\nI'm not sure what the best solution is apart from just breaking it as a bug-fix.\r\nWe could add a parameter and deprecate it and then remove the parameter if we really want.\r\n\r\nThe OVR classifier also has a potential issue where the decision_function for binary is ``(n_samples, 1)`` instead of ``(n_samples,)``. That also seems non-standard. I'm not sure we have other multi-output classifiers with a decision function, so I'm not entirely certain what the standard should be. All the multi-label ones do ``(n_samples,)`` according to the tests.\r\n\r\nFound via #8022.","labels":["Bug","API","help wanted","module:multiclass"],"created_at":"2016-12-13T15:56:41Z","comments":15,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8049"},{"issue_number":252,"repository":"scikit-learn\/scikit-learn","title":"Randomness in LatentDirichletAllocation(learning_method=\"batch\")","description":"It looks to me as if there's randomness in the batch variant of the LDA in ``_em_step`` where ``random_init`` is set to ``True``. That seems incorrect to me.\r\n\r\nIn general I find the logic of the algorithm a bit hard to follow, I'm wondering whether we should either refactor it or implement batch and online in two entirely separate code paths.","labels":["Bug","module:decomposition"],"created_at":"2016-12-08T19:54:48Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/8020"},{"issue_number":253,"repository":"scikit-learn\/scikit-learn","title":"Is there an issue with TimeSeriesSplit and the learning_curve?","description":"#### Description\r\nWhen using the TimeSeriesSplit (TSS) CV generator with learning_curve(), the learning curve training sets appear to be computed as fractions of the first CV split, as opposed to the whole data set. \r\n\r\nThis means that the learning curve is being computed on the basis of a very small training set (just a small fraction of the total training set).\r\n\r\n#### Steps\/Code to Reproduce\r\n\r\n```\r\nfrom sklearn.datasets import make_classification\r\nfrom sklearn.model_selection import TimeSeriesSplit\r\nfrom sklearn.model_selection import learning_curve\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nimport numpy as np\r\n\r\ndata_set = make_classification(n_samples=2000, n_features=20, n_informative=2, \r\n                               n_redundant=2, n_repeated=0, n_classes=2, \r\n                               n_clusters_per_class=2, weights=None, flip_y=0.01, \r\n                               class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, \r\n                               shuffle=True, random_state=None)\r\n\r\nX, y = data_set\r\n\r\ntscv = TimeSeriesSplit(n_splits=10)\r\n\r\nestimator = DecisionTreeClassifier()\r\n\r\ntrain_sizes, train_scores, valid_scores = learning_curve(estimator, X, y, \r\n                                                         train_sizes=(np.linspace(0.1, 1.0, num=10)), \r\n                                                         cv=tscv, scoring='f1', exploit_incremental_learning=False, \r\n                                                         n_jobs=-1, pre_dispatch='2*n_jobs', verbose=1)\r\n\r\n```\r\n\r\n#### Expected Results\r\nlearning curve output looks like this: \r\n`[learning_curve] Training set sizes: [ 181  363  545  727  909 1091 1273 1455 1637 1819]`\r\n\r\n#### Actual Results\r\nTraining set sizes are calculated as a fraction of the smallest CV iteration - i.e. a fraction of 181, meaning that the learning curve ends up being very small. \r\n\r\n#### Possible fix\r\nIt is possible that line 750 in _validation.py needs to change from\r\n`n_max_training_samples = len(cv_iter[0][0])`\r\nTo\r\n`n_max_training_samples = len(cv_iter[-1][0])`\r\n\r\nThis would allow the learning curve to be computed on basis of the full size of the CV sets, rather than just the first (and smallest) fraction.\r\n\r\n#### Versions\r\nWindows-10-10.0.14393\r\n('Python', '2.7.12 |Continuum Analytics, Inc.| (default, Jun 29 2016, 11:07:13) [MSC v.1500 64 bit (AMD64)]')\r\n('NumPy', '1.11.2')\r\n('SciPy', '0.18.1')\r\n('Scikit-Learn', '0.18')\r\n","labels":["Bug","module:model_selection"],"created_at":"2016-11-07T13:38:53Z","comments":31,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/7834"},{"issue_number":254,"repository":"scikit-learn\/scikit-learn","title":"KernelDensity Score unstable with many samples and changing leaf_size","description":"Hi,\r\n\r\nI have the impression that KernelDensity.score_samples() behaves strangely when there are many samples to fit.  \r\n#### Description\r\n\r\nWhen I fit many samples adding a single additional sample has a very large impact on the score of an unchanged test set. I think the influence of a single additional training input should become smaller rather than larger, as the number of training samples goes up (as can be seen below this is initially the case, before strange behaviour occurs at >80 training samples). \r\n\r\nI get the expected behaviour when I implement the KDE by hand.\r\n#### Code\r\n\r\nHere's the code to reproduce the behaviour:\r\n\r\n``` py\r\nimport numpy as np\r\nfrom sklearn.neighbors import KernelDensity as KD\r\nfrom sklearn.datasets import load_digits\r\n\r\n###Load the digits dataset and split it into a training and a testing set                                                                                                                                            \r\n\r\ntrainSamp = load_digits()['images']\r\ntrainSampF = np.zeros((trainSamp.shape[0],trainSamp.shape[1]*trainSamp.shape[2]))\r\nfor i in range(trainSamp.shape[0]):\r\n     trainSampF[i] = trainSamp[i].flatten()\r\n\r\ntestSamp = trainSampF[-100:]\r\ntrainSamp = trainSampF[:-100]\r\ntrain = trainSamp[:]\r\ntest = testSamp[:20]\r\n\r\n###Fit a gaussian KDE with k*10 samples and score                                                                                                                                                                    \r\n###compare to a gaussian KDE with one additional sample and score                                                                                                                                                   \r\n\r\nfor k in range(10):\r\n     n = (k+1)*10\r\n     model = KD(bandwidth=0.175, kernel='gaussian')\r\n     model.fit(train[:n])\r\n     llsv = model.score_samples(test)\r\n     model.fit(train[:(n+1)])\r\n     llsv2 = model.score_samples(test)\r\n     print('evaluating score (noise free) for')\r\n     print(n,'training samples',llsv.mean(), ' and',n+1, 'training samples',llsv2.mean())\r\n     print('difference:',llsv.mean()-llsv2.mean())\r\n\r\n     print()\r\n```\r\n#### Expected Result\r\n\r\nDecreasing impact of adding a single training sample as training set gets larger.\r\n#### Actual Result\r\n\r\nWhen there are more than ~80 training samples adding a single training sample changes the score quite a lot.\r\n\r\nProgram output:\r\n\r\n```\r\nevaluating score (noise free) for\r\n10 training samples -16898.9523708  and 11 training samples -16779.047681\r\ndifference: -119.90468982\r\n\r\nevaluating score (noise free) for\r\n20 training samples -15013.1149057  and 21 training samples -15013.1636959\r\ndifference: 0.0487901641663\r\n\r\nevaluating score (noise free) for\r\n30 training samples -14134.3366974  and 31 training samples -14134.3694872\r\ndifference: 0.0327898228261\r\n\r\nevaluating score (noise free) for\r\n40 training samples -11150.9509101  and 41 training samples -11150.9756027\r\ndifference: 0.0246926125947\r\n\r\nevaluating score (noise free) for\r\n50 training samples -11148.725074  and 51 training samples -11148.7448766\r\ndifference: 0.0198026272956\r\n\r\nevaluating score (noise free) for\r\n60 training samples -10508.091069  and 61 training samples -10508.1075983\r\ndifference: 0.0165293019545\r\n\r\nevaluating score (noise free) for\r\n70 training samples -10508.2452197  and 71 training samples -10508.2594044\r\ndifference: 0.0141846349907\r\n\r\nevaluating score (noise free) for\r\n80 training samples -10024.2971185  and 81 training samples -10082.2179183\r\ndifference: 57.9207998393\r\n\r\nevaluating score (noise free) for\r\n90 training samples -9293.12908873  and 91 training samples -9565.75291888\r\ndifference: 272.623830155\r\n\r\nevaluating score (noise free) for\r\n100 training samples -9855.60678148  and 101 training samples -10123.3057225\r\ndifference: 267.698941066\r\n```\r\n#### Versions\r\n\r\nLinux-3.19.0-66-generic-x86_64-with-debian-jessie-sid\r\n('Python', '2.7.12 |Anaconda 4.0.0 (64-bit)| (default, Jul  2 2016, 17:42:40) \\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]')\r\n('NumPy', '1.11.1')\r\n('SciPy', '0.17.1')\r\n('Scikit-Learn', '0.17.1')\r\n#### What now?\r\n\r\nI would be glad if you could help me figure out what I did wrong or what's going wrong. Thanks!\r\n\r\n<!-- Thanks for contributing! -->\r\n","labels":["Bug","Moderate","help wanted","module:neighbors","Needs Investigation"],"created_at":"2016-09-26T12:49:54Z","comments":6,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/7492"},{"issue_number":255,"repository":"scikit-learn\/scikit-learn","title":"Perplexity not monotonically decreasing for batch Latent Dirichlet Allocation","description":"When using the batch method, the perplexity in LDA should be non-increasing in every iteration, right?\nI have cases where it does increase. If this is indeed a bug, I'll investigate.\n","labels":["Bug","help wanted","module:decomposition"],"created_at":"2016-05-12T16:27:24Z","comments":14,"reactions":5,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6777"},{"issue_number":256,"repository":"scikit-learn\/scikit-learn","title":"Normalization in SVD solver for LinearDiscriminantAnalysis can produce large scaling coefficients","description":"#### Description\n\nThe SVD solver contains the following code\n\n``` python\nstd = Xc.std(axis=0)\n# avoid division by zero in normalization\nstd[std == 0] = 1.\nfac = 1. \/ (n_samples - n_classes)\n#2) Within variance scaling\nX = np.sqrt(fac) * (Xc \/ std)\n```\n\nThis can result in very large scaling coefficients if std contains small values not exactly zero.\n#### Steps\/Code to Reproduce\n\nExample:\n\n``` python\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\ndata = [[ 1, 2, 0, 4],\n             [5, 6, 1e-7, 8],\n             [9, 10, 0, 12]]\nlabels = [0,0,1]\n\nlda.fit(data, labels)\nprint lda.scalings_\n```\n\n```\narray([[  8.83883476e-02],\n       [  8.83883476e-02],\n       [  3.53553391e+06],\n       [  8.83883476e-02]])\n```\n\nThe scaling value resulting from the normalization could ( and does ) result in poor classification performance. I haven't as yet checked this with other implementations to compare the results.\n#### Versions\n\nWindows-7-6.1.7601-SP1\n('Python', '2.7.11 |Anaconda 2.5.0 (32-bit)| (default, Mar  4 2016, 15:18:41) [MSC v.1500 32 bit (Intel)]')\n('NumPy', '1.10.4')\n('SciPy', '0.17.0')\n('Scikit-Learn', '0.17')\n","labels":["Bug","Enhancement","Needs Decision"],"created_at":"2016-04-27T17:35:58Z","comments":7,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6725"},{"issue_number":257,"repository":"scikit-learn\/scikit-learn","title":"Cloning CalibratedClassifierCV with prefit base_estimator","description":"I recently stumbled upon this issue, I needed to clone a `CalibratedClassifierCV` object when the latter is defined on top on a pre-fit (`cv=\"prefit\"`) base estimator. Unfortunately, because of the semantic of the clone interface, this cannot be achieved without \"unfitting\" the pre-fit base estimator. \n\nI think this is somehow related to @jnothman long standing frozen estimator discussion. \n","labels":["Bug","API","module:calibration"],"created_at":"2016-02-25T20:04:24Z","comments":5,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6451"},{"issue_number":258,"repository":"scikit-learn\/scikit-learn","title":"SGD classification unnecessarily slow","description":"Multiclass prediction using SGD on sparse data is unnecessarily slow. It seems to be copying large arrays _on every call to `predict`, `predict_proba` etc_, which kills its performance.\n\nI think the main culprit is `safe_sparse_dot`, which uses scipy's \"CSR \\* dense\" routine:\n\n``` python\ndef safe_sparse_dot(a, b, dense_output=False):\n    if issparse(a) or issparse(b):\n        ret = a * b  # <== this line here: `a` is CSR, `b` is dense clf.coef_.T\n        if dense_output and hasattr(ret, \"toarray\"):\n            ...\n```\n\nBecause `b` is transposed and scipy's multiplication invokes `b.ravel()`, this is very slow (copies `clf.coef_` internally).\n\nKeeping `clf.coef_.T` as a C-contiguous array [here](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/linear_model\/base.py#L252) improved the prediction performance of SGD classifier **7300x** for us (1s vs 137\u00b5s per call):\n\n``` python\ndef decision_function(self, X):\n    ...\n    # before:\n    # scores = safe_sparse_dot(X, self.coef_.T, dense_output=True) + self.intercept_\n\n    # after quick fix: self.coef_T = np.ascontiguousarray(self.coef_.T)\n    scores = safe_sparse_dot(X, self.coef_T, dense_output=True) + self.intercept_\n    ...\n```\n\nThe exact speedup numbers will vary, depending on `coef_` size (~the number of SGD classes and features).\n\nThis could be raised as an issue in scipy as well (I see no good reason for such inefficiency -- that `ravel()` is just too generous), but since the fix seems trivial, maybe it's worth addressing on sklearn side as well?\n\nThis is using scipy 0.16.1 and sklearn 0.17.\n","labels":["Bug","Moderate","help wanted","module:linear_model"],"created_at":"2016-01-19T08:31:58Z","comments":21,"reactions":2,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6186"},{"issue_number":259,"repository":"scikit-learn\/scikit-learn","title":"Problems with data scaling in sklearn.cross_decomposition.PLSRegression","description":"I am trying to fit some spectral data using PLS, and am having difficulties with the module.\n\nEssentially, when I use the default value of `scale=False`, I get a prediction, BUT all my predictions are scaled, and I just cannot figure out how to convert back to my original data space.  The same is true for the example code.  As you see, the scaled prediction is just off.  I assumed that I should be able to revert back to the \"unscaled\" data using `pls_scaled.y_mean_` and `pls_scaled.y_std_`, but that doesn't seem to work for me.  Any suggestions would be highly appreciated.\n\n```\nY = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])\nX = np.array([[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]])\npls_not_scaled=PLSRegression(n_components=2, scale=False)\npls_scaled=PLSRegression(n_components=2, scale=True)\npls_not_scaled.fit(X,Y)\nOut[]: PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\npls_scaled.fit(X,Y)\nOut[]: PLSRegression(copy=True, max_iter=500, n_components=2, scale=True, tol=1e-06)\nOut[]: PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)\nY_not_scaled_pred = pls_not_scaled.predict(X)\nY_scaled_pred = pls_scaled.predict(X)\n\nprint(Y)\n[[  0.1  -0.2]\n [  0.9   1.1]\n [  6.2   5.9]\n [ 11.9  12.3]]\n\nprint(Y_not_scaled_pred)\n[[  0.11029323  -0.09323388]\n [  0.86825958   0.77077371]\n [  6.24049125   6.31999396]\n [ 11.88095594  12.1024662 ]]\n\nprint(Y_scaled_pred)\n[[ 1.52568016  1.47577156]\n [ 2.43367318  2.3584298 ]\n [ 6.25638942  6.26638408]\n [ 8.88425724  8.99941456]]\n```\n\nFor those with a more visual bend:\n\n```\nplt.grid('on')\nplt.scatter(Y[:, 0], Y_not_scaled_pred[:, 0], color='blue')\nplt.scatter(Y[:, 0], Y_scaled_pred[:, 0], color='red')\nplt.xlabel('Predicted')\nplt.ylabel('Measured')\nplt.legend(['not_scaled','scaled'],'lower right')\n```\n\n![figure_1](https:\/\/cloud.githubusercontent.com\/assets\/13950307\/11730049\/a1fcfa66-9f60-11e5-960f-90fa260eef6a.png)\n","labels":["Bug","module:cross_decomposition"],"created_at":"2015-12-10T22:14:24Z","comments":19,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/6002"},{"issue_number":260,"repository":"scikit-learn\/scikit-learn","title":"BUG: StandardScaler partial_fit overflows","description":"The recent implementation of `partial_fit` for `StandardScaler` can overflow. A use case there is to transform indefinitely long stream of data, but that is problematic with the current implementation. The reason is that to compute the running mean, [we keep track](https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/master\/sklearn\/utils\/extmath.py#L788) of the sample sum.\r\n\r\nHere the code to reproduce the behavior. To simulate long stream of data would take long time; instead, I use samples with very large norm but the effect is the same. The same batch is presented to the transformer many times. The mean should be same.\r\n\r\n```python\r\nfrom sklearn.preprocessing import StandardScaler\r\nimport numpy as np\r\n\r\nrng = np.random.RandomState(0)\r\n\r\ndef gen_1d_uniform_batch(min_, max_, n):\r\n    return rng.uniform(min_, max_, size=(n, 1))\r\n\r\nmax_f = np.finfo(np.float64).max \/ 1e5\r\nmin_f = max_f \/ 1e2\r\nstream_dim = 100\r\nbatch_dim = 500000\r\nprint(\"mean overflow: batch vs online on %d repetitions\" % stream_dim)\r\n\r\nX = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)\r\n\r\nscaler = StandardScaler(with_std=False).fit(X)\r\nprint(scaler.mean_)\r\n[  1.79769313e+301]\r\n\r\niscaler = StandardScaler(with_std=False)\r\nbatch = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)\r\nfor _ in range(stream_dim):\r\n    iscaler = iscaler.partial_fit(batch)\r\nRuntimeWarning: overflow encountered in add\r\n  updated_mean = (last_sum + new_sum) \/ updated_sample_count\r\n\r\nprint(iscaler.mean_)\r\n[ inf]\r\n```\r\n","labels":["Bug","Moderate","help wanted","module:preprocessing"],"created_at":"2015-10-27T08:59:52Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/5602"},{"issue_number":261,"repository":"scikit-learn\/scikit-learn","title":"sklearn.cluster.AgglomerativeClustering: Can we do without completing the matrix? 'UserWarning:the number of connected components of the connectivity matrix is *>1. Completing it to avoid stopping the tree early.' ","description":"I have tried this both on the latest 0.16.1 version and on the latest bleeding edge version of sklearn '0.17.dev0' and this appears to be an issue in both.\n\nI use `sklearn.cluster.AgglomerativeClustering(affinity='precomputed',connectivity=Cmat,linkage='complete')`\nwhere Cmat is a connectivity matrix in which there are disconnected components. \nAs indicated by the source code, I get the error message UserWarning: the number of connected components of the connectivity matrix is *>1. Completing it to avoid stopping the tree early. \n\nHowever, reading the source code I see that when completing the connectivity matrix the developers are wondering whether the clustering can take place without completing the matrix:\n\"\"XXX: Can we do without completing the matrix?\"\"\n\nI am interested exactly in this development. Do you think sklearn is planning to fix this and make it possible to do the clustering without completing the matrix? I think it should not be too hard.\n\nBest,\nZhana\n","labels":["Bug","module:cluster"],"created_at":"2015-09-29T09:59:42Z","comments":15,"reactions":2,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/5327"},{"issue_number":262,"repository":"scikit-learn\/scikit-learn","title":"Dense svm and zeroed weight for samples of entire class","description":"This bug appears in current master, and for any dense svm class.\n\n```\nimport numpy as np\nfrom sklearn.svm import SVC\nX = np.array([[0, 0, 0],\n              [0, 0, 1],\n              [0, 1, 0],\n              [0, 1, 1],\n              [1, 0, 0],\n              [1, 0, 1],\n              [1, 1, 0],\n              [1, 1, 1]])\ny = np.array([0, 0, 0, 1, 1, 1, 2, 2])\nw = np.array([1, 1, 1, 1, 1, 1, 0, 0])\n\n\nf = SVC(kernel='linear', probability=True, random_state=1)\nf.fit(X,y, w)\nprint(f.classes_)\nprint(f.predict_proba(X))\n```\n\nOutput:\n\n```\n[0 1 2]\nwarning: class label 2 specified in weight is not found\n[[ 0.28963492  0.71036508]\n [ 0.39180833  0.60819167]\n [ 0.28963492  0.71036508]\n [ 0.39180833  0.60819167]\n [ 0.57544014  0.42455986]\n [ 0.68293573  0.31706427]\n [ 0.57544014  0.42455986]\n [ 0.68293573  0.31706427]]\n```\n\nHere we see that svmlib internally have lost 2nd class, at the same time sklean's wrapper class keeps all class labels inside, that's why predict_proba returns matrix of shape (n_samples, 2) instead of (n_sample, 3) (what is expected by bagging classifier implementation). I understand that it's insane usage of weights by itself, but together with bagging and dataset with many labels, bagging randomly zeroes complete classes, and this bug shows itself, because bagging expects that svm's return probability of classes which they hold (e.g. all classes).\n\nI investigated this a little bit, and can try to fix this, if someone will say that all this usage with bagging makes sense (Because i don't really sure about this).\n","labels":["Bug","module:svm"],"created_at":"2015-08-24T22:24:24Z","comments":19,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/5150"},{"issue_number":263,"repository":"scikit-learn\/scikit-learn","title":"Our R^2 makes odd assumptions, doesn't work with LeaveOneOut","description":"``` python\nfrom sklearn.datasets import make_regression\nfrom sklearn.cross_validation import cross_val_score, LeaveOneOut\nfrom sklearn.linear_model import Ridge\n\nX, y, coef_ = make_regression(random_state=42, noise=1, n_samples=200, coef=True)\ncross_val_score(Ridge(), X, y, cv=LeaveOneOut(len(X)))\n```\n\n> array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n>         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n>         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n>         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n>         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n\nMaybe the single sample in the LOO is not interpreted correctly? :-\/\n","labels":["Bug","API","module:model_selection"],"created_at":"2015-08-07T18:55:51Z","comments":20,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/5097"},{"issue_number":264,"repository":"scikit-learn\/scikit-learn","title":"has_fit_parameter will not work with **kwargs","description":"Bagging and boosting use `sklearn.utils.validation.has_fit_parameter` to produce a clear error when `sample_weight is not None` and the base estimator does not support `sample_weight`. However, the method of checking the argspec will not work if `sample_weight` is supported as a `**kwargs` in `fit`, thus raising an exception when none applies. (A similar concern exists for the proposed alternative to `sample_weight` as keyword arg, #4696.)\n","labels":["Bug","help wanted"],"created_at":"2015-06-18T04:02:37Z","comments":2,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/4871"},{"issue_number":265,"repository":"scikit-learn\/scikit-learn","title":"Dictionary learning is slower with n_jobs > 1","description":"Setting n_jobs > 1 in MiniBatchDictionaryLearning (and in function dictionary_learning_online) leads to worse performance.\n\nMulti processing is handled in sklearn.decompositions, function dict_learning, l 249 \n\n``` python\n    code_views = Parallel(n_jobs=n_jobs)(\n        delayed(_sparse_encode)(\n            X[this_slice], dictionary, gram, cov[:, this_slice], algorithm,\n            regularization=regularization, copy_cov=copy_cov,\n            init=init[this_slice] if init is not None else None,\n            max_iter=max_iter)\n        for this_slice in slices)\n```\n\nMinimal example :\nhttps:\/\/gist.github.com\/arthurmensch\/091d16c135f4a3ba5580\n\nOutput n_jobs = 1\n\n```\nDistorting image...\nExtracting reference patches...\ndone in 0.05s.\nLearning the dictionary...\ndone in 5.12s.\nExtracting noisy patches... \ndone in 0.02s.\nLasso LARS...\ndone in 10.24s.\n```\n\nOutput n_jobs == 2\n\n```\nDistorting image...\nExtracting reference patches...\ndone in 0.05s.\nLearning the dictionary...\ndone in 78.98s.\nExtracting noisy patches... \ndone in 0.02s.\nLasso LARS...\ndone in 6.15s.\n```\n\nOutput n_jobs == 4\n\n```\nDistorting image...\nExtracting reference patches...\ndone in 0.05s.\nLearning the dictionary...\ndone in 83.24s.\nExtracting noisy patches... \ndone in 0.02s.\nLasso LARS...\ndone in 3.82s.\n```\n\nWe can see that transform function of MiniBatchDictionaryLearning (relying on sparse_encode function) benefits from multi-processing as expected.\n\nDictionary learning relies on successive calls of sparse_encode function : slowness may come from this.\n","labels":["Bug","Performance","help wanted","module:decomposition"],"created_at":"2015-05-26T07:45:45Z","comments":10,"reactions":1,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/4769"},{"issue_number":266,"repository":"scikit-learn\/scikit-learn","title":"Ensure sparse `y` is supported in grid search etc.","description":"As reported at https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/1233#issuecomment-73467547, `GridSearchCV` currently breaks if it is fit with sparse `y` (at least for some supported `scipy` versions). This is due to a `len` call, when we should probably be using `check_consistent_length`. In any case, a test is needed, since we are meant to be preferring sparse matrices for multilabel data now...\n","labels":["Bug","help wanted","module:model_selection"],"created_at":"2015-02-09T09:35:14Z","comments":4,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/4225"},{"issue_number":267,"repository":"scikit-learn\/scikit-learn","title":"kneighbors_graph param check ineffective","description":"In `sklearn.neighbors.graph`, the `_check_params` function is called by both of the graph methods in the module. The idea seems to be to ensure that when the neighbors are precomputed that the parameters passed into say, `kneighbors_graph` match what was used in the computation of the `BallTree`. However, if the `BallTree` itself was from a prior fit, the check misses that fact. Demonstration:\n\n``` python\ndata = np.random.rand(100).reshape(10,10)\nn_neighbors = 2\nneigh_minkowski = neighbors.NearestNeighbors(n_neighbors, metric='minkowski')\nneigh_jaccard = neighbors.NearestNeighbors(n_neighbors, metric='jaccard')\nneigh_jaccard.fit(data)\n\n# \"Copies\" _fit_X, _tree, _fit_method\n# But it does not copy p, metric, effective_metric, effective_metric_params, etc.\nneigh_minkowski.fit(neigh_jaccard)\nassert(neigh_minkowski.metric == 'minkowski') # misleading, but explainable\n\n# Succeeds\nneighbors.kneighbors_graph(neigh_minkowski, n_neighbors, mode='distance', metric='minkowski')\n\n# Fails: ValueError: Got jaccard for metric, while the estimator has minkowski for the same parameter.\nneighbors.kneighbors_graph(neigh_minkowski, n_neighbors, mode='distance', metric='jaccard')\n```\n\nI'm not sure any of this matters since `kneighbors_graph` (and the things it calls) doesn't do anything with the name of the metric if the BallTree has been precomputed. But it's certainly the case the `_check_params()` is not having its intended effect. So does this matter? Should `NearestNeighbor._fit()` be modified so that it copies all relevant data over when X is \"precomputed\"?\n\nThis is a bit contrived, but I ran across it while trying to modify `Isomap` to handle non-Euclidean metrics. The issue is that Isomap initializes a `NearestNeighbors` instance with metric equal to 'minkowski' and `p=2`. So if you call `fit` with a precomputed `NearestNeighbors` using a different metric, then this \"fact\" is not transferred to the initialized instance. This can all be avoided if the API to Isomap was changed, but this particular point of `_check_params()` not always doing an effective check still holds.\n","labels":["Bug","module:neighbors"],"created_at":"2015-02-01T22:33:00Z","comments":8,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/4194"},{"issue_number":268,"repository":"scikit-learn\/scikit-learn","title":"Ensemble models (and maybe others?) don't check for negative sample_weight","description":"When sample weights are negative, the probabilities can come out negative as well:\n\n```\n>>> rng = np.random.RandomState(10)\n>>> X = rng.randn(10, 4)\n>>> y = rng.randint(0, 2, 10)\n>>> sample_weight = rng.randn(10)\n>>> clf = RandomForestClassifier().fit(X, y, sample_weight)\n>>> clf.predict_proba(X)\narray([[ 0.56133774,  0.43866226],\n       [ 1.03235924, -0.03235924],\n       [ 1.03235924, -0.03235924],\n       [ 1.03235924, -0.03235924],\n       [ 1.03235924, -0.03235924],\n       [ 1.03235924, -0.03235924],\n       [ 0.98071868,  0.01928132],\n       [ 0.56133774,  0.43866226],\n       [ 1.03235924, -0.03235924],\n       [ 1.03235924, -0.03235924]])\n```\n","labels":["Bug","module:ensemble"],"created_at":"2014-10-15T12:50:51Z","comments":40,"reactions":0,"url":"https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/3774"}]