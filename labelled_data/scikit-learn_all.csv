"issue_number","repository","title","description","labels","created_at","comments","reactions","url","classification"
"1","scikit-learn/scikit-learn","StandardScaler is `stateless`","### Describe the bug

The StandardScaler seems to be stateless in version 1.6.1. But fit changes the state of the StandardScaler if I got it correctly. 

### Steps/Code to Reproduce

```
StandardScaler()._get_tags()[""stateless""]
```

### Expected Results

False

### Actual Results

True

### Versions

```shell
System:
    python: 3.10.14 (main, Jul 18 2024, 22:40:44) [Clang 15.0.0 (clang-1500.1.0.2.5)]
executable: ****/python
   machine: macOS-15.2-arm64-arm-64bit

Python dependencies:
      sklearn: 1.6.1
          pip: 24.1.2
   setuptools: 71.0.3
        numpy: 1.26.4
        scipy: 1.13.1
       Cython: 3.0.11
       pandas: 2.2.3
   matplotlib: 3.9.2
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: ****.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: *****
        version: 0.3.27
threading_layer: pthreads
   architecture: neoversen1

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: *****
        version: None
```","['Bug']","2025-02-15T18:58:02Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/30840","Logical Bug"
"2","scikit-learn/scikit-learn","Bug: AttributeError in `str_escape` when handling `numpy.int64` in `sklearn.tree._export.py` in `/sklearn/tree/_export.py`","### Describe the bug


When exporting a decision tree using `sklearn.tree.export_text()` (or other related functions), an AttributeError occurs if a `numpy.int64` value is passed to `_export.py` instead of a string.

```
  File ""venv/lib/python3.10/site-packages/sklearn/tree/_export.py"", line 311, in node_to_str
    feature = self.str_escape(feature)
  File ""venv/lib/python3.10/site-packages/sklearn/tree/_export.py"", line 581, in str_escape
    return string.replace('""', r""\"""")
AttributeError: 'numpy.int64' object has no attribute 'replace'
```
Causes:
The function `str_escape(feature)` expects a string but receives a `numpy.int64` value.
`numpy.int64` does not have a .replace() method, causing an AttributeError. 

## Possible Fix:
Convert feature to a string before passing it to `str_escape()` in `_export.py`.
Modify line 581 in `_export.py`:

Before (causing error):
```
return string.replace('""', r""\"""")
```
## After (fixing error):

```
return str(string).replace('""', r""\"""")
```
This ensures that `feature` is always a string before calling `.replace()`.



### Steps/Code to Reproduce

This piece of code triggers the error:

```
import numpy as np
from sklearn.tree import DecisionTreeClassifier, export_graphviz

X = np.array([[0, 1], [1, 0], [1, 1], [0, 0]])
y = np.array([0, 1, 1, 0])

clf = DecisionTreeClassifier().fit(X, y)

feature_names = np.array([10, 20], dtype=np.int64)  # numpy.int64 feature names

# Debugging prints
print(""Feature Names:"", feature_names)
print(""Feature Name Types:"", [type(name) for name in feature_names])

# Attempt to trigger the error
export_graphviz(clf, out_file=None, feature_names=feature_names)
```

### Expected Results

A graph in PNG format. 

### Actual Results


```
AttributeError: 'numpy.int64' object has no attribute 'replace'
```

### Versions

```shell
System:
    python: 3.10.12 (main, Jan 17 2025, 14:35:34) [GCC 11.4.0]
executable: /usr/bin/python3
   machine: Linux-6.8.0-52-generic-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.6.1
          pip: 22.0.2
   setuptools: 59.6.0
        numpy: 2.2.3
        scipy: 1.15.1
       Cython: 3.0.11
       pandas: None
   matplotlib: 3.5.1
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libscipy_openblas
       filepath: lib/python3.10/site-packages/numpy.libs/libscipy_openblas64_-6bb31eeb.so
        version: 0.3.28
threading_layer: pthreads
   architecture: SkylakeX

       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libscipy_openblas
       filepath: lib/python3.10/site-packages/scipy.libs/libscipy_openblas-68440149.so
        version: 0.3.28
threading_layer: pthreads
   architecture: SkylakeX

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libgomp
       filepath: lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
```","['Bug', 'Needs Triage']","2025-02-14T13:54:26Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/30834","Syntax Error"
"3","scikit-learn/scikit-learn","Numpy Array Error when Training MultioutputClassifer with LogisticRegressionCV with classes underrepresented","### Describe the bug

When I train the MultioutputClassifer with LogisticRegressionCV with classes underrepresented, I get the following numpy error.
I think this is connected to the issue #28178 and #26401.

### Steps/Code to Reproduce

```python
import sklearn
print(sklearn.__version__)
from sklearn.linear_model import LogisticRegressionCV
from sklearn.multioutput import MultiOutputClassifier
import numpy as np


n, m = 20, 5
model = MultiOutputClassifier(LogisticRegressionCV())
X = np.random.randn(n, m)
y = np.concatenate([[np.random.randint(0, 2, n),
                     np.random.randint(0, 5, n)]], axis=0).T
y[-3:, 0] = [3, 4, 5]
model.fit(X, y)
```

### Expected Results

1.6.1

### Actual Results

1.6.1

```pytb
.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.
  warnings.warn(
Traceback (most recent call last):
  File ""error_skitlearn.py"", line 14, in <module>
    model.fit(X, y)
  File "".venv/lib/python3.12/site-packages/sklearn/multioutput.py"", line 543, in fit
    super().fit(X, Y, sample_weight=sample_weight, **fit_params)
  File "".venv/lib/python3.12/site-packages/sklearn/base.py"", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/sklearn/multioutput.py"", line 274, in fit
    self.estimators_ = Parallel(n_jobs=self.n_jobs)(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/sklearn/utils/parallel.py"", line 77, in __call__
    return super().__call__(iterable_with_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/joblib/parallel.py"", line 1918, in __call__
    return output if self.return_generator else list(output)
                                                ^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/joblib/parallel.py"", line 1847, in _get_sequential_output
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/sklearn/utils/parallel.py"", line 139, in __call__
    return self.function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/sklearn/multioutput.py"", line 63, in _fit_estimator
    estimator.fit(X, y, **fit_params)
  File "".venv/lib/python3.12/site-packages/sklearn/base.py"", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py"", line 2038, in fit
    coefs_paths = np.reshape(
                  ^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py"", line 324, in reshape
    return _wrapfunc(a, 'reshape', shape, order=order)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py"", line 54, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py"", line 42, in _wrapit
    conv = _array_converter(obj)
           ^^^^^^^^^^^^^^^^^^^^^
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.
```

### Versions

```shell
System:
    python: 3.12.3 (main, Jan 17 2025, 18:03:48) [GCC 13.3.0]
executable: .venv/bin/python
   machine: Linux-6.8.0-52-generic-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.6.1
          pip: 25.0.1
   setuptools: 75.8.0
        numpy: 2.2.3
        scipy: 1.15.1
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.10.0
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libscipy_openblas
       filepath: .venv/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-6bb31eeb.so
        version: 0.3.28
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libscipy_openblas
       filepath: .venv/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-68440149.so
        version: 0.3.28
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libgomp
       filepath: .venv/lib/python3.12/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
```","['Bug', 'Needs Triage']","2025-02-14T10:34:16Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/30832","Syntax Error"
"4","scikit-learn/scikit-learn","UnsetMetadataPassedError can point towards the wrong method","### Describe the bug

When `enable_metadata_routing=True`, for a missing `set_score_request`, `UnsetMetadataPassedError` message states that a `set_fit_request` is missing.

### Steps/Code to Reproduce

```python
from sklearn import set_config
from sklearn.exceptions import UnsetMetadataPassedError
from sklearn.model_selection import cross_validate
from sklearn.linear_model import LogisticRegression
import numpy as np

rng = np.random.RandomState(22)
n_samples, n_features = 10, 4
X = rng.rand(n_samples, n_features)
y = rng.randint(0, 2, size=n_samples)
sw = rng.randint(0, 5, size=n_samples)

set_config(enable_metadata_routing=True)
# missing set_score_request
logreg = LogisticRegression().set_fit_request(sample_weight=True)
try:
    cross_validate(
        logreg, X, y, 
        params={""sample_weight"":sw}, 
        error_score='raise'
    )
except UnsetMetadataPassedError as e:
    print(e)
```

### Expected Results

I would expect an error message pointing towards the missing `set_score_request`, and perhaps a less verbose message when only one metadata is passed. Something like:


'sample_weight' are passed to cross validation but are not explicitly set as requested or not requested for cross_validate's estimator: LogisticRegression. Call `.set_score_request(sample_weight=True)` on the estimator for using 'sample_weight' or `sample_weight=False` for not using it. See the Metadata Routing User guide...

### Actual Results

['sample_weight'] are passed to cross validation but are not explicitly set as requested or not requested for cross_validate's estimator: LogisticRegression. Call `.set_fit_request({{metadata}}=True)` on the estimator for each metadata in ['sample_weight'] that you want to use and `metadata=False` for not using it. See the Metadata Routing User guide...

### Versions

```shell
sklearn: 1.7.dev0
```","['Bug', 'Metadata Routing']","2025-02-12T16:16:09Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/30818","Syntax Error"
"5","scikit-learn/scikit-learn","sample_weight is silently ignored in LogisticRegressionCV.score when metadata routing is enabled","### Describe the bug

I'm not sure if it is a proper bug, or my lack of understanding of the metadata routing API ;)

When `enable_metadata_routing=True`, the `score` method of a `LogisticRegressionCV` estimator will ignore `sample_weight`.
```python
set_config(enable_metadata_routing=True)
logreg_cv = LogisticRegressionCV().fit(X, y)
logreg_cv.score(X, y, sample_weight=sw)==logreg_cv.score(X, y) #unweighted accuracy
```
I found it surprising, because the `score` method works fine when `enable_metadata_routing=False`, so the same piece of code behaves differently depending on the metadata routing config.
```python
set_config(enable_metadata_routing=False)
logreg_cv = LogisticRegressionCV().fit(X, y)
logreg_cv.score(X, y, sample_weight=sw) #weighted accuracy
```

If I understood the metadata routing API correctly, to make the `score` method `sample_weight` aware we need to explicitly pass a scorer that request it:
```python
set_config(enable_metadata_routing=True)
weighted_accuracy = make_scorer(accuracy_score).set_score_request(sample_weight=True)
logreg_cv = LogisticRegressionCV(scoring=weighted_accuracy).fit(X, y)
logreg_cv.score(X, y, sample_weight=sw) #weighted accuracy
```

If it's the intended behavior of the metadata routing API, maybe we should warn the user or raise an error in the first case, instead of silently ignoring `sample_weight` ?

### Steps/Code to Reproduce

```python
from sklearn import set_config
from sklearn.metrics import make_scorer, accuracy_score
from sklearn.linear_model import LogisticRegressionCV
import numpy as np

rng = np.random.RandomState(22)
n_samples, n_features = 10, 4
X = rng.rand(n_samples, n_features)
y = rng.randint(0, 2, size=n_samples)
sw = rng.randint(0, 5, size=n_samples)

set_config(enable_metadata_routing=True)
logreg_cv = LogisticRegressionCV()
logreg_cv.fit(X, y)
# sample_weight is silently ignored in logreg_cv.score
assert logreg_cv.score(X, y) == logreg_cv.score(X, y, sample_weight=sw) 
assert not logreg_cv.score(X, y, sample_weight=sw)==accuracy_score(logreg_cv.predict(X),y, sample_weight=sw)
```

### Expected Results

Either `logreg_cv.score(X, y, sample_weight=sw)` raises an error/warning or the assertions are false.

### Actual Results

The assertions are true.

### Versions

```shell
sklearn: 1.7.dev0
```","['Bug', 'Metadata Routing']","2025-02-12T15:49:01Z","3","1","https://github.com/scikit-learn/scikit-learn/issues/30817","Syntax Error"
"6","scikit-learn/scikit-learn","Windows free-threaded CPython 3.13 ValueError: concurrent send_bytes() calls are not supported","Noticed in [build log](https://github.com/scikit-learn/scikit-learn/actions/runs/13233133978/job/36933421850#step:5:2813). An automated issue was opened in https://github.com/scikit-learn/scikit-learn/issues/30801 and closed the next day.

This needs some investigation to figure out whether this can be reproduced locally and whether this is actually Windows-specific.

This may be a joblib issue as well.

```
================================== FAILURES ===================================
  _____________________________ test_absolute_error _____________________________
  
      def test_absolute_error():
          # For coverage only.
          X, y = make_regression(n_samples=500, random_state=0)
          gbdt = HistGradientBoostingRegressor(loss=""absolute_error"", random_state=0)
  >       gbdt.fit(X, y)
  
  ..\venv-test\Lib\site-packages\sklearn\ensemble\_hist_gradient_boosting\tests\test_gradient_boosting.py:225: 
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  ..\venv-test\Lib\site-packages\sklearn\base.py:1389: in wrapper
      return fit_method(estimator, *args, **kwargs)
  ..\venv-test\Lib\site-packages\sklearn\ensemble\_hist_gradient_boosting\gradient_boosting.py:663: in fit
      X_binned_train = self._bin_data(X_train, is_training_data=True)
  ..\venv-test\Lib\site-packages\sklearn\ensemble\_hist_gradient_boosting\gradient_boosting.py:1178: in _bin_data
      X_binned = self._bin_mapper.fit_transform(X)  # F-aligned array
  ..\venv-test\Lib\site-packages\sklearn\utils\_set_output.py:319: in wrapped
      data_to_wrap = f(self, X, *args, **kwargs)
  ..\venv-test\Lib\site-packages\sklearn\base.py:918: in fit_transform
      return self.fit(X, **fit_params).transform(X)
  ..\venv-test\Lib\site-packages\sklearn\ensemble\_hist_gradient_boosting\binning.py:234: in fit
      non_cat_thresholds = Parallel(n_jobs=self.n_threads, backend=""threading"")(
  ..\venv-test\Lib\site-packages\sklearn\utils\parallel.py:82: in __call__
      return super().__call__(iterable_with_config_and_warning_filters)
  ..\venv-test\Lib\site-packages\joblib\parallel.py:2007: in __call__
      return output if self.return_generator else list(output)
  ..\venv-test\Lib\site-packages\joblib\parallel.py:1711: in _get_outputs
      self._terminate_and_reset()
  ..\venv-test\Lib\site-packages\joblib\parallel.py:1386: in _terminate_and_reset
      self._backend.terminate()
  ..\venv-test\Lib\site-packages\joblib\_parallel_backends.py:262: in terminate
      self._pool.close()
  ..\..\..\..\pypa\cibuildwheel\Cache\nuget-cpython\python-freethreaded.3.13.0\tools\Lib\multiprocessing\pool.py:652: in close
      self._change_notifier.put(None)
  ..\..\..\..\pypa\cibuildwheel\Cache\nuget-cpython\python-freethreaded.3.13.0\tools\Lib\multiprocessing\queues.py:394: in put
      self._writer.send_bytes(obj)
  ..\..\..\..\pypa\cibuildwheel\Cache\nuget-cpython\python-freethreaded.3.13.0\tools\Lib\multiprocessing\connection.py:200: in send_bytes
      self._send_bytes(m[offset:offset + size])
  _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
  
  self = <multiprocessing.connection.PipeConnection object at 0x00000243F43267C0>
  buf = <memory at 0x00000243F4C65AC0>
  
      def _send_bytes(self, buf):
          if self._send_ov is not None:
              # A connection should only be used by a single thread
  >           raise ValueError(""concurrent send_bytes() calls ""
                               ""are not supported"")
  E           ValueError: concurrent send_bytes() calls are not supported
  
  ..\..\..\..\pypa\cibuildwheel\Cache\nuget-cpython\python-freethreaded.3.13.0\tools\Lib\multiprocessing\connection.py:287: ValueError
```
","['Bug', 'Needs Investigation', 'free-threading', 'OS:Windows']","2025-02-11T14:44:27Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/30810","Syntax Error"
"7","scikit-learn/scikit-learn","SequentialFeatureSelector fails on text features even though the estimator supports them","### Describe the bug

When a model can handle the data type (may it be text or NaN), `SequentialFeatureSelector` appears to be performing its own validation ignoring the capability of the model and apparently always insists that everything must be numbers. `cross_val_score` appears to be working so it's `SequentialFeatureSelector` that is rejecting the data.

### Steps/Code to Reproduce

```python
from sklearn.datasets import load_diabetes
from sklearn.feature_selection import SequentialFeatureSelector
from xgboost import XGBRegressor
from sklearn.model_selection import cross_val_score

import sklearn; print(F'{sklearn.__version__=}')
import xgboost; print(F'{xgboost.__version__=}')

X, y = load_diabetes(return_X_y=True, as_frame=True, scaled=False)
X['sex'] = X['sex'].apply(lambda x: 'M' if x==1.0 else 'F').astype('category')
model = XGBRegressor(enable_categorical=True, random_state=0)
print('Testing cross_val_score begins')
cross_val_score(model, X, y, error_score='raise') # no error
print('Testing cross_val_score ends')
print('Testing SequentialFeatureSelector begins')
SequentialFeatureSelector(model, tol=0).fit(X, y)
print('Testing SequentialFeatureSelector ends')
```

### Expected Results

```text
sklearn.__version__='1.6.1'
xgboost.__version__='2.1.4'
Testing cross_val_score begins
Testing cross_val_score ends
Testing SequentialFeatureSelector begins
Testing SequentialFeatureSelector ends
```
(No errors)

### Actual Results

```text
sklearn.__version__='1.6.1'
xgboost.__version__='2.1.4'
Testing cross_val_score begins
Testing cross_val_score ends
Testing SequentialFeatureSelector begins

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-29-fb1642c5f9e7> in <cell line: 16>()
     14 print('Testing cross_val_score ends')
     15 print('Testing SequentialFeatureSelector begins')
---> 16 SequentialFeatureSelector(model, tol=0).fit(X, y)
     17 print('Testing SequentialFeatureSelector ends')

/usr/local/lib/python3.10/dist-packages/sklearn/base.py in wrapper(estimator, *args, **kwargs)
   1387                 )
   1388             ):
-> 1389                 return fit_method(estimator, *args, **kwargs)
   1390 
   1391         return wrapper

/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_sequential.py in fit(self, X, y, **params)
    280             process_routing(self, ""fit"", **params)
    281         for _ in range(n_iterations):
--> 282             new_feature_idx, new_score = self._get_best_new_feature_score(
    283                 cloned_estimator, X, y, cv, current_mask, **params
    284             )

/usr/local/lib/python3.10/dist-packages/sklearn/feature_selection/_sequential.py in _get_best_new_feature_score(self, estimator, X, y, cv, current_mask, **params)
    311                 candidate_mask = ~candidate_mask
    312             X_new = X[:, candidate_mask]
--> 313             scores[feature_idx] = cross_val_score(
    314                 estimator,
    315                 X_new,

/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py in wrapper(*args, **kwargs)
    214                     )
    215                 ):
--> 216                     return func(*args, **kwargs)
    217             except InvalidParameterError as e:
    218                 # When the function is just a wrapper around an estimator, we allow

/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)
    682     scorer = check_scoring(estimator, scoring=scoring)
    683 
--> 684     cv_results = cross_validate(
    685         estimator=estimator,
    686         X=X,

/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py in wrapper(*args, **kwargs)
    214                     )
    215                 ):
--> 216                     return func(*args, **kwargs)
    217             except InvalidParameterError as e:
    218                 # When the function is just a wrapper around an estimator, we allow

/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)
    429     )
    430 
--> 431     _warn_or_raise_about_fit_failures(results, error_score)
    432 
    433     # For callable scoring, the return type is only know after calling. If the

/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py in _warn_or_raise_about_fit_failures(results, error_score)
    515                 f""Below are more details about the failures:\n{fit_errors_summary}""
    516             )
--> 517             raise ValueError(all_fits_failed_message)
    518 
    519         else:

ValueError: 
All the 5 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
1 fits failed with the following error:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py"", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 726, in inner_f
    return func(**kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py"", line 1143, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py"", line 603, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py"", line 1065, in _create_dmatrix
    return QuantileDMatrix(
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 726, in inner_f
    return func(**kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 1573, in __init__
    self._init(
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 1632, in _init
    it.reraise()
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 569, in reraise
    raise exc  # pylint: disable=raising-bad-type
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 550, in _handle_exception
    return fn()
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 637, in <lambda>
    return self._handle_exception(lambda: self.next(input_data), 0)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/data.py"", line 1402, in next
    input_data(**self.kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 726, in inner_f
    return func(**kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 617, in input_data
    new, cat_codes, feature_names, feature_types = _proxy_transform(
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/data.py"", line 1429, in _proxy_transform
    data, _ = _ensure_np_dtype(data, data.dtype)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/data.py"", line 224, in _ensure_np_dtype
    data = data.astype(dtype, copy=False)
ValueError: could not convert string to float: 'M'

--------------------------------------------------------------------------------
4 fits failed with the following error:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py"", line 866, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 726, in inner_f
    return func(**kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py"", line 1143, in fit
    train_dmatrix, evals = _wrap_evaluation_matrices(
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py"", line 603, in _wrap_evaluation_matrices
    train_dmatrix = create_dmatrix(
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py"", line 1065, in _create_dmatrix
    return QuantileDMatrix(
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 726, in inner_f
    return func(**kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 1573, in __init__
    self._init(
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 1632, in _init
    it.reraise()
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 569, in reraise
    raise exc  # pylint: disable=raising-bad-type
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 550, in _handle_exception
    return fn()
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 637, in <lambda>
    return self._handle_exception(lambda: self.next(input_data), 0)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/data.py"", line 1402, in next
    input_data(**self.kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 726, in inner_f
    return func(**kwargs)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/core.py"", line 617, in input_data
    new, cat_codes, feature_names, feature_types = _proxy_transform(
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/data.py"", line 1429, in _proxy_transform
    data, _ = _ensure_np_dtype(data, data.dtype)
  File ""/usr/local/lib/python3.10/dist-packages/xgboost/data.py"", line 224, in _ensure_np_dtype
    data = data.astype(dtype, copy=False)
ValueError: could not convert string to float: 'F'
```

### Versions

```shell
Python dependencies:
      sklearn: 1.6.1
          pip: 24.1.2
   setuptools: 75.1.0
        numpy: 1.26.4
        scipy: 1.13.1
       Cython: 3.0.11
       pandas: 2.2.2
   matplotlib: 3.7.5
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: mkl
    num_threads: 2
         prefix: libmkl_rt
       filepath: /usr/local/lib/libmkl_rt.so.2
        version: 2025.0.1-Product
threading_layer: gnu

       user_api: blas
   internal_api: mkl
    num_threads: 2
         prefix: libmkl_rt
       filepath: /usr/local/lib/python3.10/dist-packages/mkl_fft.libs/libmkl_rt-089e6a60.so.2
        version: 2025.0.1-Product
threading_layer: not specified

       user_api: openmp
   internal_api: openmp
    num_threads: 4
         prefix: libgomp
       filepath: /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 4
         prefix: libopenblas
       filepath: /usr/local/lib/python3.10/dist-packages/scipy.libs/libopenblasp-r0-01191904.3.27.so
        version: 0.3.27
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 4
         prefix: libgomp
       filepath: /usr/local/lib/python3.10/dist-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None

       user_api: openmp
   internal_api: openmp
    num_threads: 4
         prefix: libgomp
       filepath: /usr/local/lib/python3.10/dist-packages/xgboost.libs/libgomp-24e2ab19.so.1.0.0
        version: None
```","['Bug', 'Needs Info']","2025-02-08T00:48:33Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/30785","Syntax Error"
"8","scikit-learn/scikit-learn","_py_sort() returns ValueError on windows with numpy 1.26.4 but works correctly with numpy 2.x","### Describe the bug

_py_sort() returns ValueError with numpy 1.26.4 but works correctly with numpy 2.x. I have created 2 different conda envs with different numpy versions from conda-forge:
```
conda create -n numpy_1.26.4 numpy=1.26.4 scikit-learn=1.6.1 -c conda-forge --override-channels
```
and
```
conda create -n numpy_2 numpy=2 scikit-learn=1.6.1 -c conda-forge --override-channel
```
In each of the envs, I essentially reproduced https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/tests/test_tree.py#L2820 test_sort_log2_build test that shows different behavior. This works correctly with numpy 2, but with numpy 1.26.4 it returns: 
```
ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long'
```

### Steps/Code to Reproduce

In fact, this is just a copy of test_sort_log2_build test:
```
>>> import numpy as np
>>> print(np.__version__)
1.26.4
>>> import sklearn
>>> print(sklearn.__version__)
1.6.1
>>> from sklearn.tree._partitioner import _py_sort
>>> rng = np.random.default_rng(75)
>>> some = rng.normal(loc=0.0, scale=10.0, size=10).astype(np.float32)
>>> feature_values = np.concatenate([some] * 5)
>>> samples = np.arange(50)
>>> _py_sort(feature_values, samples, 50)
```

### Expected Results

```
>>> _py_sort(feature_values, samples, 50)
>>>
```
This is the normal behavior of the test in case numpy 2:
```
>>> import numpy as np
>>> print(np.__version__)
2.1.2
```

### Actual Results

```
>>> _py_sort(feature_values, samples, 50)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""_partitioner.pyx"", line 705, in sklearn.tree._partitioner._py_sort
ValueError: Buffer dtype mismatch, expected 'intp_t' but got 'long'
```
This behavior is reproduced if the test is run with numpy 1.26.4

### Versions

```shell
>>> import sklearn
>>> print(sklearn.__version__)
1.6.1
```","['Bug']","2025-02-07T14:47:55Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/30782","Syntax Error"
"9","scikit-learn/scikit-learn","Wrong Mutual Information Calculation","### Describe the bug

#### Issue
I encountered a bug unexpectedly while reviewing some metrics in a project.
When calculating mutual information using the `mutual_info_classif`, I noticed values higher than entropy, which is [impossible](https://en.wikipedia.org/wiki/Mutual_information#/media/File:Figchannel2017ab.svg). There is no such issue with `mutual_info_regression` (although, there, the self-mi is far from entropy, which may be another interesting case).

##### Implication
Any algorithm sorting features based on `mutual_info_classif` or any metric based on this function may be affected.

Thanks a lot for putting time on this.


P.S. In the minimal example, the feature is fixed (all one). However, I encountered the same issue in other scenarios as well. The example is just more simplified. The problem persists on both Linux and Mac. I attached personal computer session info.

### Steps/Code to Reproduce

```python
import numpy as np
import pandas as pd

from sklearn.feature_selection import mutual_info_classif

big_n = 1_000_000
bug_df = pd.DataFrame({
    'feature': np.ones(big_n),
    'target': (np.arange(big_n) < 100).astype(int),
})
bug_df

mi = mutual_info_classif(bug_df[['feature']], bug_df['target'])
entropy = mutual_info_classif(bug_df[['target']], bug_df['target'])

print(f""mi: {mi[0] :.6f}"")
print(f""self-mi (entropy): {entropy[0] :.6f}"")

from scipy import stats

scipy_entropy = stats.entropy([bug_df['target'].mean(), 1 - bug_df['target'].mean()])

print(f""scipy entropy: {scipy_entropy :.6f}"")
```

### Expected Results

```
mi: 0.000000
self-mi (entropy): 0.001023
scipy entropy: 0.001021
```

### Actual Results

```
mi: 0.215495
self-mi (entropy): 0.001023
scipy entropy: 0.001021
```

### Versions

```shell
System:
    python: 3.10.13 (main, Sep 11 2023, 08:16:02) [Clang 14.0.6 ]
executable: /Users/*/miniconda3/envs/*/bin/python
   machine: macOS-15.1.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.6.1
          pip: 23.3.1
   setuptools: 68.2.2
        numpy: 1.26.1
        scipy: 1.15.1
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.8.2
       joblib: 1.3.2
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /Users/*/miniconda3/envs/*/lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: /Users/*/miniconda3/envs/*/lib/python3.10/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```","['Bug', 'Needs Investigation']","2025-02-05T16:46:04Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/30772","Syntax Error"
"10","scikit-learn/scikit-learn","MiniBatchKMeans not handling sample weights as expected","### Describe the bug

Following up from PR #29907, we realised that when passing sample weights any resampling should be done with weights and replacement before passing through to other operations. 

MiniBatchKMeans has a similar bug where minibatch_indices are not resampled with weights but instead weights are passed on to the subsequent minibatch_step which returns resulting in sample weight equivalence not being respected (i.e., repeating and weighting a sample n times behave the same with similar outputs).

### Steps/Code to Reproduce

```python
from sklearn.cluster import MiniBatchKMeans, KMeans

import matplotlib.pyplot as plt
from scipy.stats import kstest,ttest_ind
from sklearn.datasets import make_blobs
import numpy as np

rng = np.random.RandomState(0)
    
centres = np.array([[0, 0, 0], [0, 5, 5], [3, 1, 1], [2, 4, 4], [100, 8, 800]])
X, y = make_blobs(
    n_samples=300,
    cluster_std=1,
    centers=centres,
    random_state=10,
)
# Create dataset with repetitions and corresponding sample weights
sample_weight = rng.randint(0, 10, size=X.shape[0])
X_resampled_by_weights = np.repeat(X, sample_weight, axis=0)
y_resampled_by_weights = np.repeat(y,sample_weight)

predictions_sw = []
predictions_dup = []
predictions_sw_mini = []
predictions_dup_mini = []

prediction_rank = np.argsort(y)[-1:]

for seed in range(100):

    ## Fit estimator
    est_sw = KMeans(random_state=seed,n_clusters=5).fit(X,y,sample_weight=sample_weight)
    est_dup = KMeans(random_state=seed,n_clusters=5).fit(X_resampled_by_weights,y_resampled_by_weights)
    est_sw_mini = MiniBatchKMeans(random_state=seed,n_clusters=5).fit(X,y,sample_weight=sample_weight)
    est_dup_mini = MiniBatchKMeans(random_state=seed,n_clusters=5).fit(X_resampled_by_weights,y_resampled_by_weights)
    
    ##Get predictions
    predictions_sw.append(est_sw.predict(X[prediction_rank]))
    predictions_dup.append(est_dup.predict(X[prediction_rank]))
    predictions_sw_mini.append(est_sw_mini.predict(X[prediction_rank]))
    predictions_dup_mini.append(est_dup_mini.predict(X[prediction_rank]))

fig = plt.figure(figsize=(10,5))
ax1=fig.add_subplot(1,2,1)
ax2=fig.add_subplot(1,2,2)

predictions_sw = np.asarray(predictions_sw).flatten()
predictions_dup = np.asarray(predictions_dup).flatten()
ax1.hist(predictions_sw)
ax1.hist(predictions_dup,alpha=0.5)
ax1.set_title(""KMeans: %.2f""%(kstest(predictions_sw,predictions_dup).pvalue))

predictions_sw_mini = np.asarray(predictions_sw_mini).flatten()
predictions_dup_mini = np.asarray(predictions_dup_mini).flatten()
ax2.hist(predictions_sw_mini,label=""weighted"")
ax2.hist(predictions_dup_mini,label=""repeated"",alpha=0.5)
ax2.set_title(""MiniBatchKMeans: %.2f""%(kstest(predictions_sw_mini,predictions_dup_mini).pvalue))
plt.legend()
```

### Expected Results

KMeans and Minibatch KMeans return similar histograms

### Actual Results

![Image](https://github.com/user-attachments/assets/141ecb6e-96a5-412b-a0fb-6197f7634ae9)

### Versions

```shell
System:
    python: 3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:13:44) [Clang 16.0.6 ]
executable: /Users/shrutinath/micromamba/envs/scikit-learn/bin/python
   machine: macOS-14.3-arm64-arm-64bit

Python dependencies:
      sklearn: 1.7.dev0
          pip: 24.0
   setuptools: 75.8.0
        numpy: 2.0.0
        scipy: 1.14.0
       Cython: 3.0.10
       pandas: 2.2.2
   matplotlib: 3.9.0
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /Users/shrutinath/micromamba/envs/scikit-learn/lib/libopenblas.0.dylib
        version: 0.3.27
threading_layer: openmp
   architecture: VORTEX

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: /Users/shrutinath/micromamba/envs/scikit-learn/lib/libomp.dylib
        version: None
```","['Bug']","2025-02-02T18:34:33Z","0","1","https://github.com/scikit-learn/scikit-learn/issues/30750","Logical Bug"
"11","scikit-learn/scikit-learn","Edge case bug in metadata routing","### Describe the bug

Hello, while using metadata routing I encountered what seems to be a bug. I do not have enough understanding of metadata routing to determine if it is actually a bug or an incorrect use.

Below is an example where I am using a meta estimator (`BaggingRegressor`) around a base estimator (`DecisionTreeRegressor`). In my use case, I need to dynamically wrap the base estimator in an `Adapter` to do some work before calling the fit method of the base estimator. This work is based on an extra parameter `extra_param`, which I request using the `set_fit_request` method. The parameter is passed sucessfully, but its type is altered from string to list on one edge case (when the string matches the number of samples of X).

### Steps/Code to Reproduce

```python
import numpy as np
import sklearn
from sklearn import base, ensemble, tree

sklearn.set_config(enable_metadata_routing=True)


class Adapter(base.BaseEstimator):
    def __init__(self, wrapped_estimator):
        self.wrapped_estimator = wrapped_estimator

    def fit(self, X, y, extra_param: str):
        # Do some work before delegating to the wrapped_estimator's fit method
        print(extra_param)
        assert isinstance(extra_param, str)
        return self.wrapped_estimator.fit(X, y)

    # Delegate other methods
    def __getattr__(self, name):
        return getattr(self.wrapped_estimator, name)


n, p = 10, 2
rng = np.random.default_rng(0)
x = rng.random((n, p))
y = rng.integers(0, 2, n)

estimator = tree.DecisionTreeRegressor()
adapter = Adapter(estimator)
adapter.set_fit_request(extra_param=True)
meta_estimator = ensemble.BaggingRegressor(adapter, n_estimators=1)

meta_estimator.fit(x, y, extra_param=""a"" * (n - 1))  # Pass
meta_estimator.fit(x, y, extra_param=""a"" * (n + 1))  # Pass
meta_estimator.fit(x, y, extra_param=""a"" * n)  # Fail
```

### Expected Results

No error is thrown. The `extra_param` string parameter passed to `Adapter.fit` should always be a string and thus the assertion should not fail.

### Actual Results

When the string length matches the number of samples, the string becomes a list, and the assertion fails.

```
aaaaaaaaa
aaaaaaaaaaa
['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']

Traceback (most recent call last):
  File ""minimal.py"", line 35, in <module>
    meta_estimator.fit(x, y, extra_param=""a"" * n)  # Fail
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".../miniforge3/envs/test/lib/python3.12/site-packages/sklearn/utils/validation.py"", line 63, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "".../miniforge3/envs/test/lib/python3.12/site-packages/sklearn/base.py"", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".../miniforge3/envs/test/lib/python3.12/site-packages/sklearn/ensemble/_bagging.py"", line 389, in fit
    return self._fit(X, y, max_samples=self.max_samples, **fit_params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".../miniforge3/envs/test/lib/python3.12/site-packages/sklearn/ensemble/_bagging.py"", line 532, in _fit
    all_results = Parallel(
                  ^^^^^^^^^
  File "".../miniforge3/envs/test/lib/python3.12/site-packages/sklearn/utils/parallel.py"", line 77, in __call__
    return super().__call__(iterable_with_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".../miniforge3/envs/test/lib/python3.12/site-packages/joblib/parallel.py"", line 1918, in __call__
    return output if self.return_generator else list(output)
                                                ^^^^^^^^^^^^
  File "".../miniforge3/envs/test/lib/python3.12/site-packages/joblib/parallel.py"", line 1847, in _get_sequential_output
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "".../miniforge3/envs/test/lib/python3.12/site-packages/sklearn/utils/parallel.py"", line 139, in __call__
    return self.function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".../miniforge3/envs/test/lib/python3.12/site-packages/sklearn/ensemble/_bagging.py"", line 197, in _parallel_build_estimators
    estimator_fit(X_, y_, **fit_params_)
  File ""minimal.py"", line 15, in fit
    assert isinstance(extra_param, str)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError
```

### Versions

```shell
System:
    python: 3.12.8 | packaged by conda-forge | (main, Dec  5 2024, 14:19:53) [Clang 18.1.8 ]
executable: /Users/alexandreperez/dev/lib/miniforge3/envs/fun-ltm1/bin/python
   machine: macOS-15.2-arm64-arm-64bit

Python dependencies:
      sklearn: 1.6.1
          pip: 24.3.1
   setuptools: 75.8.0
        numpy: 1.26.4
        scipy: 1.15.1
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.10.0
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 14
         prefix: libopenblas
       filepath: /Users/alexandreperez/dev/lib/miniforge3/envs/fun-ltm1/lib/python3.12/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: openmp
   internal_api: openmp
    num_threads: 14
         prefix: libomp
       filepath: /Users/alexandreperez/dev/lib/miniforge3/envs/fun-ltm1/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```","['Bug', 'Documentation', 'wontfix', 'Metadata Routing']","2025-01-30T12:27:35Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/30739","Syntax Error"
"12","scikit-learn/scikit-learn","`randomized_svd` incorrect for complex valued matrices","### Describe the bug

The `randomized_svd` utility function accepts complex valued inputs without error, but the result is inconsistent with `scipy.linalg.svd`.

### Steps/Code to Reproduce

```python
import numpy as np
from scipy import linalg
from sklearn.utils.extmath import randomized_svd

rng = np.random.RandomState(42)
X = rng.randn(100, 20) + 1j * rng.randn(100, 20)

_, s, _ = linalg.svd(X)
_, s2, _ = randomized_svd(X, n_components=5)

print(""s:"", s[:5])
print(""s2:"", s2[:5])
```

### Expected Results

I expected the singular values to be numerically close.

### Actual Results

```
s: [19.81481515 18.69019042 17.62107998 17.23689681 16.3148512 ]
s2: [11.25690754  9.97157079  9.01542947  8.06160863  7.54068744]
```

### Versions

```shell
System:
    python: 3.11.4 (main, Jul  5 2023, 08:40:20) [Clang 14.0.6 ]
executable: /Users/clane/miniconda3/bin/python
   machine: macOS-13.7-arm64-arm-64bit

Python dependencies:
      sklearn: 1.7.dev0
          pip: 25.0
   setuptools: 65.5.0
        numpy: 2.2.2
        scipy: 1.15.1
       Cython: 3.0.11
       pandas: 2.2.3
   matplotlib: 3.10.0
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libscipy_openblas
       filepath: /Users/clane/Projects/misc/scikit-learn/.venv/lib/python3.11/site-packages/numpy/.dylibs/libscipy_openblas64_.dylib
        version: 0.3.28
threading_layer: pthreads
   architecture: neoversen1

       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libscipy_openblas
       filepath: /Users/clane/Projects/misc/scikit-learn/.venv/lib/python3.11/site-packages/scipy/.dylibs/libscipy_openblas.dylib
        version: 0.3.28
threading_layer: pthreads
   architecture: neoversen1

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: /opt/homebrew/Cellar/libomp/19.1.3/lib/libomp.dylib
        version: None
```","['Bug']","2025-01-30T01:40:26Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/30736","Syntax Error"
"13","scikit-learn/scikit-learn","Error in `d2_log_loss_score` multiclass when one of the classes is missing in `y_true`.","### Describe the bug

Hello, I encountered an error with the `d2_log_loss_score` in the multiclass setting (i.e. when `y_pred` has shape (n, k) with k >= 3) when one of the classes is missing from the `y_true` labels, even when giving the labels through the `labels` argument. The error disappear when all the classes are present in `y_true`.

### Steps/Code to Reproduce

```python
from sklearn.metrics import d2_log_loss_score

y_true = [0, 1, 1]
y_pred = [[1, 0, 0], [1, 0, 0], [1, 0, 0]]
labels = [0, 1, 2]

d2_log_loss_score(y_true, y_pred, labels=labels)
```

### Expected Results

No error is thrown.

### Actual Results

```
Traceback (most recent call last):
  File ""minimal.py"", line 7, in <module>
    d2_log_loss_score(y_true, y_pred, labels=labels)
  File "".../python3.12/site-packages/sklearn/utils/_param_validation.py"", line 216, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "".../python3.12/site-packages/sklearn/metrics/_classification.py"", line 3407, in d2_log_loss_score
    denominator = log_loss(
                  ^^^^^^^^^
  File "".../python3.12/site-packages/sklearn/utils/_param_validation.py"", line 189, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "".../python3.12/site-packages/sklearn/metrics/_classification.py"", line 3023, in log_loss
    raise ValueError(
ValueError: The number of classes in labels is different from that in y_pred. Classes found in labels: [0 1 2]
```

### Versions

```shell
System:
    python: 3.12.8 | packaged by conda-forge | (main, Dec  5 2024, 14:19:53) [Clang 18.1.8 ]
executable: /Users/alexandreperez/dev/lib/miniforge3/envs/test/bin/python
   machine: macOS-15.2-arm64-arm-64bit

Python dependencies:
      sklearn: 1.6.1
          pip: 24.3.1
   setuptools: 75.8.0
        numpy: 2.2.2
        scipy: 1.15.1
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
    num_threads: 14
         prefix: libomp
       filepath: .../miniforge3/envs/test/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```","['Bug', 'Needs Investigation']","2025-01-24T11:01:39Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/30713","Syntax Error"
"14","scikit-learn/scikit-learn","Possible bug in sklearn 1.6.1 PartialDependenceDisplay.from_estimator when target and feature are both binary","### Describe the bug

PartialDependenceDisplay.from_estimator does not seem able to handle dummy variables when the response variable is binary. See example below. The example works fine in 1.5.2 but returns `ValueError: cannot reshape array of size 1 into shape (2)` in 1.6.1

### Steps/Code to Reproduce

```py
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import PartialDependenceDisplay

np.random.seed(42)
n_samples = 1000
age = np.random.normal(35, 10, n_samples)
smoker = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
prob_disease = 1 / (1 + np.exp(-(age - 35) / 10 - 2 * smoker))
heart_disease = (np.random.random(n_samples) < prob_disease).astype(int)
df = pd.DataFrame({""age"": age, ""smoker"": smoker, ""heart_disease"": heart_disease})
X = df[[""age"", ""smoker""]]
y = df[""heart_disease""]

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X, y)

pdp_age = PartialDependenceDisplay.from_estimator(rf_model, X, features=[0, 1])
```

### Expected Results

PDP plots for age and smoker.

### Actual Results

```tb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1], [line 19](vscode-notebook-cell:?execution_count=1&line=19)
     [16](vscode-notebook-cell:?execution_count=1&line=16) rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
     [17](vscode-notebook-cell:?execution_count=1&line=17) rf_model.fit(X, y)
---> [19](vscode-notebook-cell:?execution_count=1&line=19) pdp_age = PartialDependenceDisplay.from_estimator(rf_model, X, features=[0, 1])

File ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:707, in PartialDependenceDisplay.from_estimator(cls, estimator, X, features, sample_weight, categorical_features, feature_names, target, response_method, n_cols, grid_resolution, percentiles, method, n_jobs, verbose, line_kw, ice_lines_kw, pd_line_kw, contour_kw, ax, kind, centered, subsample, random_state)
    [701](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:701)         raise ValueError(
    [702](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:702)             f""When a floating-point, subsample={subsample} should be in ""
    [703](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:703)             ""the (0, 1) range.""
    [704](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:704)         )
    [706](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:706) # compute predictions and/or averaged predictions
--> [707](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:707) pd_results = Parallel(n_jobs=n_jobs, verbose=verbose)(
    [708](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:708)     delayed(partial_dependence)(
    [709](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:709)         estimator,
    [710](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:710)         X,
    [711](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:711)         fxs,
    [712](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:712)         sample_weight=sample_weight,
    [713](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:713)         feature_names=feature_names,
    [714](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:714)         categorical_features=categorical_features,
    [715](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:715)         response_method=response_method,
    [716](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:716)         method=method,
    [717](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:717)         grid_resolution=grid_resolution,
    [718](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:718)         percentiles=percentiles,
    [719](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:719)         kind=kind_plot,
    [720](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:720)     )
    [721](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:721)     for kind_plot, fxs in zip(kind_, features)
    [722](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:722) )
    [724](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:724) # For multioutput regression, we can only check the validity of target
    [725](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:725) # now that we have the predictions.
    [726](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:726) # Also note: as multiclass-multioutput classifiers are not supported,
    [727](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:727) # multiclass and multioutput scenario are mutually exclusive. So there is
    [728](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:728) # no risk of overwriting target_idx here.
    [729](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_plot/partial_dependence.py:729) pd_result = pd_results[0]  # checking the first result is enough

File ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:77, in Parallel.__call__(self, iterable)
     [72](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:72) config = get_config()
     [73](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:73) iterable_with_config = (
     [74](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:74)     (_with_config(delayed_func, config), args, kwargs)
     [75](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:75)     for delayed_func, args, kwargs in iterable
     [76](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:76) )
---> [77](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:77) return super().__call__(iterable_with_config)

File ~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1918, in Parallel.__call__(self, iterable)
   [1916](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1916)     output = self._get_sequential_output(iterable)
   [1917](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1917)     next(output)
-> [1918](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1918)     return output if self.return_generator else list(output)
   [1920](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1920) # Let's create an ID that uniquely identifies the current call. If the
   [1921](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1921) # call is interrupted early and that the same instance is immediately
   [1922](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1922) # re-used, this id will be used to prevent workers that were
   [1923](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1923) # concurrently finalizing a task from the previous call to run the
   [1924](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1924) # callback.
   [1925](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1925) with self._lock:

File ~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1847, in Parallel._get_sequential_output(self, iterable)
   [1845](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1845) self.n_dispatched_batches += 1
   [1846](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1846) self.n_dispatched_tasks += 1
-> [1847](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1847) res = func(*args, **kwargs)
   [1848](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1848) self.n_completed_tasks += 1
   [1849](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/joblib/parallel.py:1849) self.print_progress()

File ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:139, in _FuncWrapper.__call__(self, *args, **kwargs)
    [137](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:137)     config = {}
    [138](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:138) with config_context(**config):
--> [139](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/parallel.py:139)     return self.function(*args, **kwargs)

File ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216, in validate_params.<locals>.decorator.<locals>.wrapper(*args, **kwargs)
    [210](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:210) try:
    [211](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:211)     with config_context(
    [212](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:212)         skip_parameter_validation=(
    [213](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213)             prefer_skip_nested_validation or global_skip_validation
    [214](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:214)         )
    [215](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:215)     ):
--> [216](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216)         return func(*args, **kwargs)
    [217](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:217) except InvalidParameterError as e:
    [218](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218)     # When the function is just a wrapper around an estimator, we allow
    [219](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:219)     # the function to delegate validation to the estimator, but we replace
    [220](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:220)     # the name of the estimator by the name of the function in the error
    [221](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:221)     # message to avoid confusion.
    [222](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:222)     msg = re.sub(
    [223](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:223)         r""parameter of \w+ must be"",
    [224](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:224)         f""parameter of {func.__qualname__} must be"",
    [225](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:225)         str(e),
    [226](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:226)     )

File ~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:682, in partial_dependence(estimator, X, features, sample_weight, categorical_features, feature_names, response_method, percentiles, grid_resolution, method, kind)
    [676](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:676)     averaged_predictions = _partial_dependence_recursion(
    [677](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:677)         estimator, grid, features_indices
    [678](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:678)     )
    [680](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:680) # reshape averaged_predictions to
    [681](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:681) # (n_outputs, n_values_feature_0, n_values_feature_1, ...)
--> [682](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:682) averaged_predictions = averaged_predictions.reshape(
    [683](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:683)     -1, *[val.shape[0] for val in values]
    [684](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:684) )
    [685](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:685) pdp_results = Bunch(grid_values=values)
    [687](https://file+.vscode-resource.vscode-cdn.net/Users/vnijs/gh/pyrsm/examples/model/~/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/inspection/_partial_dependence.py:687) if kind == ""average"":

ValueError: cannot reshape array of size 1 into shape (2)
```
### Versions

```shell
System:
    python: 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 08:22:19) [Clang 14.0.6 ]
executable: /Users/vnijs/miniconda/envs/msba/bin/python
   machine: macOS-14.2.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.6.1
          pip: 24.3.1
   setuptools: 75.1.0
        numpy: 2.2.1
        scipy: 1.15.1
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.10.0
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
    num_threads: 16
         prefix: libomp
       filepath: /Users/vnijs/miniconda/envs/msba/lib/python3.12/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```","['Bug', 'Regression']","2025-01-20T00:00:08Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/30675","Syntax Error"
"15","scikit-learn/scikit-learn","average_precision_score produces unexpected output when scoring a single sample","### Describe the bug

When using `average_precision_score` and scoring a single sample, the metric ignores `y_score` and will always produce a score of 1.0 if `y_true = [1]` and otherwise will return a score of 0. I would have expected that it would instead raise an exception.

Potentially related to #30147, however I'm focusing on the minimal example with just a single sample.

### Steps/Code to Reproduce

```python
from sklearn.metrics import average_precision_score

y_score = [0]
y_true = [1]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 1.0

y_score = [1]
y_true = [1]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 1.0

y_score = [0.5]
y_true = [1]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 1.0

y_score = [0]
y_true = [0]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 0.0

y_score = [1]
y_true = [0]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 0.0

y_score = [0.5]
y_true = [0]
score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 0.0
```

Additionally, you can see that the average_precision_score returns a score opposite of what precision and recall return:

```python
from sklearn.metrics import average_precision_score, precision_score, recall_score

y_score = [0]
y_true = [1]

score = average_precision_score(y_true=y_true, y_score=y_score)
print(score)  # 1.0
score = precision_score(y_true=y_true, y_pred=y_score)
print(score)  # 0.0
score = recall_score(y_true=y_true, y_pred=y_score)
print(score)  # 0.0
```

### Expected Results

I would have expected the metric to raise an exception, similar to what happens when ROC_AUC is called with a single sample:

```python
score = roc_auc_score(y_true=y_true, y_score=y_score)
print(score)

```

```
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.
```

### Actual Results

Refer to code snippets above.

### Versions

```shell
System:
    python: 3.11.10 | packaged by conda-forge | (main, Sep 30 2024, 18:08:57) [GCC 13.3.0]
executable: /opt/conda/envs/ag-311/bin/python
   machine: Linux-5.15.0-1056-aws-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.5.1
          pip: 24.2
   setuptools: 60.2.0
        numpy: 1.26.4
        scipy: 1.12.0
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.9.2
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 128
         prefix: libopenblas
       filepath: /opt/conda/envs/ag-311/lib/libopenblasp-r0.3.28.so
        version: 0.3.28
threading_layer: pthreads
   architecture: SapphireRapids

       user_api: blas
   internal_api: openblas
    num_threads: 64
         prefix: libopenblas
       filepath: /opt/conda/envs/ag-311/lib/python3.11/site-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: Cooperlake

       user_api: openmp
   internal_api: openmp
    num_threads: 192
         prefix: libgomp
       filepath: /opt/conda/envs/ag-311/lib/python3.11/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
```
```
","['Bug', 'Needs Investigation']","2025-01-09T00:41:41Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/30615","Syntax Error"
"16","scikit-learn/scikit-learn","ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part.","Hello Scikit-learn team,

I am encountering an issue while running inference VotingClassifier model with `voting=""hard""` argument, I found that this issue may related to [NEP 34](https://numpy.org/neps/nep-0034-infer-dtype-is-object.html) restriction of `dtype=object` in numpy and the solution is downgrading to numpy `1.23.1`. However, it doesn't work in my case due to dependency conflicts with pandas and other packages. I'd appreciate if you could analyze this issue and provide an update when possible.

```
Traceback (most recent call last):
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 135, in <module>
    ensemble_model, trained_models, model_results, ensemble_results = main(sparse=False)
                                                                      ^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 127, in main
    trained_ensemble, ensemble_results = train_ensemble_model(
                                         ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 89, in train_ensemble_model
    ensemble_results, trained_ensemble = train_and_evaluate_ensemble(voting_clf, X_train, X_test, y_train, y_test)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training/ensemble_trainer.py"", line 33, in train_and_evaluate_ensemble
    y_pred_ensemble = voting_clf.predict(X_test)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py"", line 443, in predict
    predictions = self._predict(X)
                  ^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py"", line 80, in _predict
    return np.asarray([est.predict(X) for est in self.estimators_]).T
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part.
```

### Steps/Code to Reproduce

```
try:
  main_logger.info(""Training ensemble"")
  voting_clf.fit(X_train, y_train)
  
  main_logger.info(""Evaluating ensemble"")
  y_pred_ensemble = voting_clf.predict(X_test)
  results = classification_report(y_test, y_pred_ensemble, output_dict=True)
  main_logger.info(f""Ensemble Results:\n{classification_report(y_test, y_pred_ensemble)}"")
  
  return results, voting_clf

except Exception as e:
    main_logger.error(f""Error in ensemble training: {str(e)}"")
    raise
```

### Expected Results

```Finish training```

### Actual Results

```
Traceback (most recent call last):
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 135, in <module>
    ensemble_model, trained_models, model_results, ensemble_results = main(sparse=False)
                                                                      ^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 127, in main
    trained_ensemble, ensemble_results = train_ensemble_model(
                                         ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training.py"", line 89, in train_ensemble_model
    ensemble_results, trained_ensemble = train_and_evaluate_ensemble(voting_clf, X_train, X_test, y_train, y_test)
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/training/ensemble_trainer.py"", line 33, in train_and_evaluate_ensemble
    y_pred_ensemble = voting_clf.predict(X_test)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py"", line 443, in predict
    predictions = self._predict(X)
                  ^^^^^^^^^^^^^^^^
  File ""/home/mtoan65/Documents/Sentiment_Analysis/.venv/lib/python3.11/site-packages/sklearn/ensemble/_voting.py"", line 80, in _predict
    return np.asarray([est.predict(X) for est in self.estimators_]).T
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 33810) + inhomogeneous part.
```

### Versions

```shell
1.5.2
```
","['Bug', 'Needs Info']","2024-12-27T13:47:54Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/30546","Dependency Issue"
"17","scikit-learn/scikit-learn","AttributeError: 'super' object has no attribute '__sklearn_tags__'","### Describe the bug

```python
AttributeError                            Traceback (most recent call last)
[/usr/local/lib/python3.10/dist-packages/IPython/core/formatters.py](https://localhost:8080/#) in __call__(self, obj, include, exclude)
    968 
    969             if method is not None:
--> 970                 return method(include=include, exclude=exclude)
    971             return None
    972         else:

4 frames
[/usr/local/lib/python3.10/dist-packages/sklearn/base.py](https://localhost:8080/#) in __sklearn_tags__(self)
    538 
    539     def __sklearn_tags__(self):
--> 540         tags = super().__sklearn_tags__()
    541         tags.estimator_type = ""classifier""
    542         tags.classifier_tags = ClassifierTags()

AttributeError: 'super' object has no attribute '__sklearn_tags__'
```

### Steps/Code to Reproduce

.

### Expected Results

Working XGBClassifier model

### Actual Results

None

### Versions

```shell
1.6
```
","['Bug']","2024-12-26T20:46:53Z","13","0","https://github.com/scikit-learn/scikit-learn/issues/30542","Syntax Error"
"18","scikit-learn/scikit-learn","Feature Selectors fail to route metadata when inside a Pipeline","### Describe the bug

According to the [metadata routing docs](https://scikit-learn.org/1.6/metadata_routing.html#metadata-routing-support-status), Feature Selectors only have four classes that support metadata routing (as of v1.6):
- [sklearn.feature_selection.RFE](https://scikit-learn.org/1.6/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)
- [sklearn.feature_selection.RFECV](https://scikit-learn.org/1.6/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV)
- [sklearn.feature_selection.SelectFromModel](https://scikit-learn.org/1.6/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel)
- [sklearn.feature_selection.SequentialFeatureSelector](https://scikit-learn.org/1.6/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector)

Each of these classes fail to route metadata when used inside a Pipeline object. When `sample_weight` is provided in the Pipeline's `**fit_params`, the failure to pass `sample_weight` to the feature selector's estimator may result in incorrect feature selection (e.g., when the relationship between the features and the response are materially impacted by `sample_weight`).


### Steps/Code to Reproduce

```python
import numpy as np
import sklearn
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

sklearn.set_config(enable_metadata_routing=True)

X, y = load_iris(return_X_y=True, as_frame=True)
w = np.arange(len(X)) + 1

reg = LinearRegression().set_fit_request(sample_weight=True)
pipeline_reg = LinearRegression().set_fit_request(sample_weight=True)

pipeline_fs = SelectFromModel(
    reg,
    threshold=-np.inf,
    prefit=False,
    max_features=len(X.columns),
)

pipeline = Pipeline(
    [
        (""feature_selector"", pipeline_fs),
        (""regressor"", pipeline_reg),
    ]
)

pipeline.fit(X, y, sample_weight=w)
reg.fit(X, y, sample_weight=w)

test_passed = (
    pipeline[""feature_selector""].estimator_.coef_.tolist()
    == reg.coef_.tolist()
)
```

### Expected Results

The expected result is `test_passed = True`. 

i.e., the internal estimator of the pipeline's `feature_selector` should have `coef_` that exactly match the `coef_` from having a copied estimator fit on the same input (e.g., `(X, y, sample_weight)`). 

### Actual Results

The coefficients don't match between the pipeline's `feature_selector.estimator_` and the copied estimator trained on the same input `(X,y,sample_weight)`.
```
>>> pipeline[""feature_selector""].estimator_.coef_.tolist() == reg.coef_.tolist()
False
>>> pipeline[""feature_selector""].estimator_.coef_
array([-0.11190585, -0.04007949,  0.22864503,  0.60925205])
>>> reg.coef_
array([-0.14681895, -0.07652903,  0.28196639,  0.5732906 ])
```

Rather the coefficients of the pipeline's `feature_selector.estimator_` matches those of a copied estimator fit only on `(X,y)` without `sample_weight`.
```
>>> reg.fit(X,y).coef_
array([-0.11190585, -0.04007949,  0.22864503,  0.60925205])
```

### Versions

```shell
System:
    python: 3.11.9 (main, Apr  2 2024, 08:25:04) [Clang 15.0.0 (clang-1500.3.9.4)]
executable: /Users/kschluns/Library/Caches/pypoetry/virtualenvs/ds-sbraf-edgrceiw-py3.11/bin/python
   machine: macOS-15.1.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.6.0
          pip: 23.1.2
   setuptools: 75.6.0
        numpy: 1.26.4
        scipy: 1.14.1
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.10.0
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /Users/kschluns/Library/Caches/pypoetry/virtualenvs/ds-sbraf-edgrceiw-py3.11/lib/python3.11/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: /Users/kschluns/Library/Caches/pypoetry/virtualenvs/ds-sbraf-edgrceiw-py3.11/lib/python3.11/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```","['Bug']","2024-12-22T17:35:04Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/30527","Logical Bug"
"19","scikit-learn/scikit-learn","OPTICS.fit leaks memory when called under VS Code's built-in debugger","### Describe the bug

Running clustering algorithm with n_jobs parameter set to more than 1 thread causes memory leak each time algorithm is run.
This simple code causes additional memory leak at each loop cycle. The issue will not occur if i replace manifold reduction algorithm with precomputed features.

### Steps/Code to Reproduce

```python
import gc
import numpy as np
from sklearn.manifold import TSNE
from sklearn.cluster import OPTICS
import psutil
process = psutil.Process()


def main():
    data = np.random.random((100, 100))
    for _i in range(1, 50):
        points = TSNE().fit_transform(data)
        prediction = OPTICS(n_jobs=2).fit_predict(points)  # n_jobs!=1
        points = None
        prediction = None
        del prediction
        del points
        gc.collect()
        print(f""{process.memory_info().rss / 1e6:.1f} MB"")


main()
```

### Expected Results

Program's memory usage nearly constant between loop cycles

### Actual Results

Program's memory usage increases infinitely

### Versions

```shell
System:
    python: 3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]
executable: .venv\Scripts\python.exe
   machine: Windows-10-10.0.26100-SP0

Python dependencies:
      sklearn: 1.6.0
          pip: 24.3.1
   setuptools: 63.2.0
        numpy: 1.25.2
        scipy: 1.14.1
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.10.0
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
    num_threads: 16
         prefix: vcomp
       filepath: .venv\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libopenblas
       filepath: .venv\Lib\site-packages\numpy\.libs\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: Cooperlake

       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: .venv\Lib\site-packages\scipy.libs\libscipy_openblas-5b1ec8b915dfb81d11cebc0788069d2d.dll
        version: 0.3.27.dev
threading_layer: pthreads
   architecture: Cooperlake
```
","['Bug', 'Performance', 'Needs Investigation']","2024-12-21T15:50:53Z","18","0","https://github.com/scikit-learn/scikit-learn/issues/30525","Performance Issue"
"20","scikit-learn/scikit-learn","`remainder='passthrough'` block is missing from `ColumnTransformer` HTML repr since 1.5","In the following example, the `repr` of `ColumnTransformer` does not seem to work as I expect it:

https://scikit-learn.org/dev/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py

<img width=""809"" alt=""image"" src=""https://github.com/user-attachments/assets/f3f258e6-11f6-49ee-a9df-8c6a464fe792"" />

Instead I would expect a `passthrough` block and a `OneHotEncoder` block in the `ColumnTransformer`.
I think that we should check the reason and see if we can improve the rendering.","['Bug']","2024-12-17T16:28:45Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/30498","Logical Bug"
"21","scikit-learn/scikit-learn","Gaussian Mixture: Diagonal covariance vectors might contain unreasonably negative values when the input datatype is np.float32","### Describe the bug

The Gaussian Mixture implementation shows numerical instabilities on single-precision floating point input numbers, that even large values of the regularization parameter reg_covar (like 0.1) cannot mitigate.

More specifically, diagonal covariance elements must not be negative. However, due to the numerical instabilities intrinsic to floating point arithmetic, they might end up being tiny negative numbers that reg_covar must compensate.
It turns out that, for some input float32 , the covariance can reach the unreasonable value of -0.99999979.
This is because squaring float32 numbers significantly magnifies their precision errors.

The proposed solution consists in converting float32 values to float64 before squaring them.
Care must be taken to not increase memory consumption in the overall process.
Hence, as avgX_means is equal to avg_means2, the return value can be simplified.

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.mixture import GaussianMixture

model = GaussianMixture(n_components=2, covariance_type=""spherical"", reg_covar=0.1)
model.fit(np.array([[9999.0], [0.0]], dtype=np.float32))
model.covariances_
```

### Expected Results

```python
array([0.1, 0.1])
```

### Actual Results

```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [132], in <cell line: 49>()
     45 skgm._estimate_gaussian_covariances_diag = _optimized_estimate_gaussian_covariances_diag
     48 model = GaussianMixture(n_components=2,covariance_type=""spherical"", reg_covar=0.1)
---> 49 model.fit(np.array([[9999.0], [0.0]], dtype=np.float32))
     50 model.covariances_

File [~\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\mixture\_base.py:200](http://localhost:8888/lab/tree/~/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/mixture/_base.py#line=199), in BaseMixture.fit(self, X, y)
    174 def fit(self, X, y=None):
    175     """"""Estimate model parameters with the EM algorithm.
    176 
    177     The method fits the model ``n_init`` times and sets the parameters with
   (...)
    198         The fitted mixture.
    199     """"""
--> 200     self.fit_predict(X, y)
    201     return self

File [~\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\mixture\_base.py:253](http://localhost:8888/lab/tree/~/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/mixture/_base.py#line=252), in BaseMixture.fit_predict(self, X, y)
    250 self._print_verbose_msg_init_beg(init)
    252 if do_init:
--> 253     self._initialize_parameters(X, random_state)
    255 lower_bound = -np.inf if do_init else self.lower_bound_
    257 if self.max_iter == 0:

File [~\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\mixture\_base.py:160](http://localhost:8888/lab/tree/~/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/mixture/_base.py#line=159), in BaseMixture._initialize_parameters(self, X, random_state)
    155 else:
    156     raise ValueError(
    157         ""Unimplemented initialization method '%s'"" % self.init_params
    158     )
--> 160 self._initialize(X, resp)

File [~\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\mixture\_gaussian_mixture.py:723](http://localhost:8888/lab/tree/~/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/mixture/_gaussian_mixture.py#line=722), in GaussianMixture._initialize(self, X, resp)
    721 if self.precisions_init is None:
    722     self.covariances_ = covariances
--> 723     self.precisions_cholesky_ = _compute_precision_cholesky(
    724         covariances, self.covariance_type
    725     )
    726 elif self.covariance_type == ""full"":
    727     self.precisions_cholesky_ = np.array(
    728         [
    729             linalg.cholesky(prec_init, lower=True)
    730             for prec_init in self.precisions_init
    731         ]
    732     )

File [~\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\mixture\_gaussian_mixture.py:347](http://localhost:8888/lab/tree/~/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/mixture/_gaussian_mixture.py#line=346), in _compute_precision_cholesky(covariances, covariance_type)
    345 else:
    346     if np.any(np.less_equal(covariances, 0.0)):
--> 347         raise ValueError(estimate_precision_error_message)
    348     precisions_chol = 1.0 / np.sqrt(covariances)
    349 return precisions_chol

ValueError: Fitting the mixture model failed because some components have ill-defined empirical covariance (for instance caused by singleton or collapsed samples). Try to decrease the number of components, or increase reg_covar.
```

### Versions

```shell
System:
    python: 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]
executable: C:\Users\leonce.mekinda\AppData\Local\Programs\Python\Python39\python.exe
   machine: Windows-10-10.0.19043-SP0

Python dependencies:
      sklearn: 1.1.1
          pip: 22.0.4
   setuptools: 58.1.0
        numpy: 1.22.4
        scipy: 1.8.1
       Cython: 0.29.30
       pandas: 1.4.3
   matplotlib: 3.5.2
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\Users\leonce.mekinda\AppData\Local\Programs\Python\Python39\Lib\site-packages\numpy\.libs\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll
        version: 0.3.18
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12

       user_api: openmp
   internal_api: openmp
         prefix: vcomp
       filepath: C:\Users\leonce.mekinda\AppData\Local\Programs\Python\Python39\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None
    num_threads: 12

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\Users\leonce.mekinda\AppData\Local\Programs\Python\Python39\Lib\site-packages\scipy\.libs\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll
        version: 0.3.17
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12
```
","['Bug', 'Numerical Stability']","2024-12-02T01:02:22Z","20","0","https://github.com/scikit-learn/scikit-learn/issues/30382","Syntax Error"
"22","scikit-learn/scikit-learn","HTML display rendering poorly in vscode ""Dark High Contrast"" color theme","### Describe the bug

When I use vscode, I use the ""Dark High Contrast"" theme, as my eyes are tired. In this mode, some of the estimator names are not visible in the HTML display

### Steps/Code to Reproduce

Execute the following code in a vscode (for instance a cell)
```python
# %%
from sklearn.pipeline import make_pipeline
from sklearn.decomposition import PCA
from sklearn.ensemble import HistGradientBoostingRegressor

pipe = make_pipeline(PCA(), HistGradientBoostingRegressor())
pipe
```

### Expected Results

With the ""Dark (Visual Studio)"" theme, the result is:
![image](https://github.com/user-attachments/assets/1c8d52d4-ce8c-4e8a-a217-fc68be2f2f70)


### Actual Results

However, with the ""Dark High Contrast"", the result is
![image](https://github.com/user-attachments/assets/a229f0dd-c71f-4744-9733-00a82d5258c0)

Note that the title of the enclosing meta-estimator, here ""Pipeline"", is not visible

### Versions

```shell
git main of today (last commit: 426e6be923e34f68bc720ae625c8ca258f473265, merge of #30347)

System:
    python: 3.12.3 (main, Nov  6 2024, 18:32:19) [GCC 13.2.0]
executable: /bin/python3
   machine: Linux-6.8.0-49-generic-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.7.dev0
          pip: 24.0
   setuptools: 68.1.2
        numpy: 1.26.4
        scipy: 1.11.4
       Cython: 3.0.11
       pandas: 2.1.4+dfsg
   matplotlib: 3.6.3
       joblib: 1.3.2
threadpoolctl: 3.1.0
```
```
","['Bug', 'frontend']","2024-11-27T20:10:36Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/30357","Logical Bug"
"23","scikit-learn/scikit-learn","Hang when fitting `SVC` to a specific dataset","### Describe the bug

I am trying to fit an [`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) to a specific dataset. The training process gets stuck, never finishing.

scikit-learn uses a fork of LIBSVM [version 3.10.0](https://github.com/scikit-learn/scikit-learn/blame/caaa1f52a0632294bf951a9283d015f7b5dd5dd5/sklearn/svm/src/libsvm/svm.h#L4) from [2011](https://github.com/cjlin1/libsvm/releases/tag/v310). The equivalent code using a newer version of LIBSVM succeeds, suggesting that there is an upstream bug fix that scikit-learn could merge in.

### Steps/Code to Reproduce

[libsvm_problematic_dataset.csv](https://github.com/user-attachments/files/17927924/libsvm_problematic_dataset.csv)

```python
import logging

from polars import read_csv
from sklearn.svm import SVC

_logger = logging.getLogger(__name__)


def main():
    dataset = read_csv(
        source='libsvm_problematic_dataset.csv'
    )

    x = dataset.select('feature').to_numpy()
    y = dataset['label'].to_numpy()

    _logger.info(""Attempting to reproduce issue. If reproduced, the program will not exit."")

    SVC(
        C=100,
        kernel='poly',
        degree=4,
        gamma=0.9597420397825849,
        tol=0.01,
        cache_size=1000,
        class_weight={
            0: 1.04884106,
            1: 0.95550528
        },
        verbose=True
    ).fit(X=x, y=y)

    _logger.error(""The issue was not reproduced."")


if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)

    main()
```

### Expected Results

```
INFO:__main__:Attempting to reproduce issue. If reproduced, the program will not exit.
.................................................................................................
WARNING: using -h 0 may be faster
*..............................
WARNING: using -h 0 may be faster
*.............
WARNING: using -h 0 may be faster
*..................................................................
WARNING: using -h 0 may be faster
*..........................
WARNING: using -h 0 may be faster
*..........
WARNING: using -h 0 may be faster
*..............
WARNING: using -h 0 may be faster
*................
WARNING: using -h 0 may be faster
*................
WARNING: using -h 0 may be faster
*...........................
WARNING: using -h 0 may be faster
*..............
WARNING: using -h 0 may be faster
*.............
WARNING: using -h 0 may be faster
*.............
WARNING: using -h 0 may be faster
*.............................
WARNING: using -h 0 may be faster
*............................................
WARNING: using -h 0 may be faster
*.......
WARNING: using -h 0 may be faster
*......................
WARNING: using -h 0 may be faster
*.............
WARNING: using -h 0 may be faster
*.
WARNING: using -h 0 may be faster
*
optimization finished, #iter = 460766
obj = -245114.248664, rho = 1.000020
nSV = 2452, nBSV = 2450
Total nSV = 2452
ERROR:__main__:The issue was not reproduced.
```

This expected result was generated using LIBSVM [version 3.30.0](https://pypi.org/project/libsvm-official/3.30.0/) with the following code:
```python
import logging

from libsvm.svmutil import svm_train
from polars import read_csv

_logger = logging.getLogger(__name__)


def main():
    dataset = read_csv(
        source='libsvm_problematic_dataset.csv'
    )

    x = dataset.select('feature').to_numpy()
    y = dataset['label'].to_numpy()

    _logger.info(""Attempting to reproduce issue. If reproduced, the program will not exit."")

    svm_train(
        y, x,
        [
            '-s', 0,
            '-t', 1,
            '-d', 4,
            '-g', 0.9597420397825849,
            '-c', 100,
            '-m', 1000,
            '-e', 0.01,
            '-w0', 1.04884106,
            '-w1', 0.95550528
        ]
    )

    _logger.error(""The issue was not reproduced."")


if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)

    main()
```

### Actual Results

```
INFO:__main__:Attempting to reproduce issue. If reproduced, the program will not exit.
[LibSVM].....................................................
Warning: using -h 0 may be faster
*............
Warning: using -h 0 may be faster
*.....
Warning: using -h 0 may be faster
*.................
Warning: using -h 0 may be faster
*.....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
```

The program never exits.

### Versions

```shell
System:
    python: 3.11.10 | packaged by conda-forge | (main, Oct 16 2024, 01:26:25) [Clang 17.0.6 ]
executable: /opt/homebrew/Caskroom/miniforge/base/envs/libsvm-debugging/bin/python
   machine: macOS-14.7.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.5.2
          pip: 24.3.1
   setuptools: 75.6.0
        numpy: 2.1.3
        scipy: 1.14.1
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 10
         prefix: libopenblas
       filepath: /opt/homebrew/Caskroom/miniforge/base/envs/libsvm-debugging/lib/libopenblas.0.dylib
        version: 0.3.28
threading_layer: openmp
   architecture: VORTEX

       user_api: openmp
   internal_api: openmp
    num_threads: 10
         prefix: libomp
       filepath: /opt/homebrew/Caskroom/miniforge/base/envs/libsvm-debugging/lib/libomp.dylib
        version: None
```
","['Bug', 'Needs Investigation']","2024-11-27T02:41:11Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/30353","Syntax Error"
"24","scikit-learn/scikit-learn","NuSVC argument `class_weight` is not used","### Describe the bug

Like `SVC`, the class `NuSVC` takes argument `class_weight`. However, it looks like this argument is not used. After a quick look at the libsvm C code within sklearn as well as [libsvm's original documentation](https://www.csie.ntu.edu.tw/~cjlin/libsvm/), this seems to be expected: ""`wi` set the parameter C of class i to weight*C, for C-SVC"". I suggest that this argument should be removed from `NuSVC`'s constructor and from the documentation.

### Steps/Code to Reproduce

```python
from sklearn.svm import SVC, NuSVC

X = [[1., 2, 3], [0, 5, 2]]
y = [-1, 1]

NuSVC(verbose=True).fit(X, y).dual_coef_

optimization finished, #iter = 0
C = 2.587063
obj = 1.293532, rho = 0.000000
nSV = 2, nBSV = 0
Total nSV = 2
Out: [LibSVM]array([[-1.29353162,  1.29353162]])

SVC(C=2.587063, verbose=True).fit(X, y).dual_coef_

optimization finished, #iter = 1
obj = -1.293532, rho = 0.000000
nSV = 2, nBSV = 0
Total nSV = 2
Out: [LibSVM]array([[-1.29353162,  1.29353162]])

NuSVC(class_weight={-1:1.5, 1:.2}, verbose=True).fit(X, y).dual_coef_

optimization finished, #iter = 0
C = 2.587063
obj = 1.293532, rho = 0.000000
nSV = 2, nBSV = 0
Total nSV = 2
Out: [LibSVM]array([[-1.29353162,  1.29353162]])

SVC(C=2.587063, class_weight={-1:1.5, 1:.2}, verbose=True).fit(X, y).dual_coef_

optimization finished, #iter = 1
obj = -0.827860, rho = -0.600000
nSV = 2, nBSV = 1
Total nSV = 2
Out: [LibSVM]array([[-0.5174126,  0.5174126]])


NuSVC(class_weight={-1:0, 1:0}).fit(X, y).dual_coef_
Out: array([[-1.29353162,  1.29353162]])

SVC(class_weight={-1:0, 1:0}).fit(X, y).dual_coef_
Out: array([], shape=(1, 0), dtype=float64)
```


### Expected Results

As in the case of no `class_weight`, `NuSVM` should give the same `dual_coef_` as an `SVC` with the same `C`.
Also `class_weight={-1:0, 1:0}` should give the ""empty"" result.

### Actual Results

In all cases above `NuSVM` with class weight behaves exactly as when no weights are given.

### Versions

```shell
System:
    python: 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03)  [GCC 11.3.0]
executable: .../bin/python3.9
   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.5.2
          pip: 23.0.1
   setuptools: 67.6.0
        numpy: 2.0.2
        scipy: 1.13.1
       Cython: None
       pandas: None
   matplotlib: 3.9.2
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: blis
    num_threads: 1
         prefix: libblis
       filepath: .../lib/libblis.so.4.0.0
        version: 0.9.0
threading_layer: pthreads
   architecture: skx

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libgomp
       filepath: .../lib/libgomp.so.1.0.0
        version: None
```
","['Bug', 'Needs Investigation']","2024-11-22T13:37:27Z","15","1","https://github.com/scikit-learn/scikit-learn/issues/30332","Logical Bug"
"25","scikit-learn/scikit-learn","Error with set_output(transform='pandas') in ColumnTransformer when using OneHotEncoder with sparse output in intermediate steps","### Describe the bug

**Explanation**

Using the ColumnTransformer with set_output(transform='pandas') raises an error when there is a sparse intermediate output, even if the final output is dense. The error suggests setting sparse_output=False in OneHotEncoder, even though the intermediate sparse output should not impact the final dense output after transformations like TruncatedSVD.


The transformer raises this error even though the final output is dense due to the use of TruncatedSVD, which converts the intermediate sparse output to a dense matrix. The requirement to specify sparse_output=False for OneHotEncoder should not be enforced here, as the final output does not contain sparse data.

**Suggested Fix**

This check should be modified to allow cases where the final output is dense, regardless of intermediate sparse representations.




### Steps/Code to Reproduce

```python
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.decomposition import TruncatedSVD
from sklearn.compose import ColumnTransformer
from sklearn.datasets import load_diabetes
import pandas as pd

ds = load_diabetes()
df = pd.DataFrame(ds['data'], columns=ds['feature_names'])

ct = ColumnTransformer([
    ('ohe_tsvd', make_pipeline(OneHotEncoder(), TruncatedSVD()), ['sex']),
    ('mm', MinMaxScaler(), ['age', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']),
]).set_output(transform='pandas')

ct.fit_transform(df)
```


### Expected Results

pandas DataFrame as follow


ohe_mm__truncatedsvd0 | ohe_mm__truncatedsvd1 | mm__age | mm__bmi | mm__bp | mm__s1 | mm__s2 | mm__s3 | mm__s4 | mm__s5 | mm__s6
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
0.0 | 1.0 | 0.666667 | 0.582645 | 0.549296 | 0.294118 | 0.256972 | 0.207792 | 0.282087 | 0.562217 | 0.439394
1.0 | 0.0 | 0.483333 | 0.148760 | 0.352113 | 0.421569 | 0.306773 | 0.623377 | 0.141044 | 0.222437 | 0.166667
0.0 | 1.0 | 0.883333 | 0.516529 | 0.436620 | 0.289216 | 0.258964 | 0.246753 | 0.282087 | 0.496578 | 0.409091
1.0 | 0.0 | 0.083333 | 0.301653 | 0.309859 | 0.495098 | 0.447211 | 0.233766 | 0.423131 | 0.572923 | 0.469697
1.0 | 0.0 | 0.516667 | 0.206612 | 0.549296 | 0.465686 | 0.417331 | 0.389610 | 0.282087 | 0.362385 | 0.333333

### Actual Results

```pytb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[19], line 10
      5 df = pd.DataFrame(ds['data'], columns=ds['feature_names'])
      6 ct = ColumnTransformer([
      7     ('ohe_mm', make_pipeline(OneHotEncoder(), TruncatedSVD()), ['sex']),
      8     ('mm', MinMaxScaler(), ['age', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']),
      9 ]).set_output(transform='pandas')
---> 10 ct.fit_transform(df)

File [~/python312/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/utils/_set_output.py#line=315), in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)
    314 @wraps(f)
    315 def wrapped(self, X, *args, **kwargs):
--> 316     data_to_wrap = f(self, X, *args, **kwargs)
    317     if isinstance(data_to_wrap, tuple):
    318         # only wrap the first output for cross decomposition
    319         return_tuple = (
    320             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    321             *data_to_wrap[1:],
    322         )

File [~/python312/lib/python3.12/site-packages/sklearn/base.py:1473](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/base.py#line=1472), in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File [~/python312/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:976](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py#line=975), in ColumnTransformer.fit_transform(self, X, y, **params)
    973 else:
    974     routed_params = self._get_empty_routing()
--> 976 result = self._call_func_on_transformers(
    977     X,
    978     y,
    979     _fit_transform_one,
    980     column_as_labels=False,
    981     routed_params=routed_params,
    982 )
    984 if not result:
    985     self._update_fitted_transformers([])

File [~/python312/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:885](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py#line=884), in ColumnTransformer._call_func_on_transformers(self, X, y, func, column_as_labels, routed_params)
    873             extra_args = {}
    874         jobs.append(
    875             delayed(func)(
    876                 transformer=clone(trans) if not fitted else trans,
   (...)
    882             )
    883         )
--> 885     return Parallel(n_jobs=self.n_jobs)(jobs)
    887 except ValueError as e:
    888     if ""Expected 2D array, got 1D array instead"" in str(e):

File [~/python312/lib/python3.12/site-packages/sklearn/utils/parallel.py:74](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/utils/parallel.py#line=73), in Parallel.__call__(self, iterable)
     69 config = get_config()
     70 iterable_with_config = (
     71     (_with_config(delayed_func, config), args, kwargs)
     72     for delayed_func, args, kwargs in iterable
     73 )
---> 74 return super().__call__(iterable_with_config)

File [~/python312/lib/python3.12/site-packages/joblib/parallel.py:1918](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/joblib/parallel.py#line=1917), in Parallel.__call__(self, iterable)
   1916     output = self._get_sequential_output(iterable)
   1917     next(output)
-> 1918     return output if self.return_generator else list(output)
   1920 # Let's create an ID that uniquely identifies the current call. If the
   1921 # call is interrupted early and that the same instance is immediately
   1922 # re-used, this id will be used to prevent workers that were
   1923 # concurrently finalizing a task from the previous call to run the
   1924 # callback.
   1925 with self._lock:

File [~/python312/lib/python3.12/site-packages/joblib/parallel.py:1847](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/joblib/parallel.py#line=1846), in Parallel._get_sequential_output(self, iterable)
   1845 self.n_dispatched_batches += 1
   1846 self.n_dispatched_tasks += 1
-> 1847 res = func(*args, **kwargs)
   1848 self.n_completed_tasks += 1
   1849 self.print_progress()

File [~/python312/lib/python3.12/site-packages/sklearn/utils/parallel.py:136](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/utils/parallel.py#line=135), in _FuncWrapper.__call__(self, *args, **kwargs)
    134     config = {}
    135 with config_context(**config):
--> 136     return self.function(*args, **kwargs)

File [~/python312/lib/python3.12/site-packages/sklearn/pipeline.py:1310](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/pipeline.py#line=1309), in _fit_transform_one(transformer, X, y, weight, message_clsname, message, params)
   1308 with _print_elapsed_time(message_clsname, message):
   1309     if hasattr(transformer, ""fit_transform""):
-> 1310         res = transformer.fit_transform(X, y, **params.get(""fit_transform"", {}))
   1311     else:
   1312         res = transformer.fit(X, y, **params.get(""fit"", {})).transform(
   1313             X, **params.get(""transform"", {})
   1314         )

File [~/python312/lib/python3.12/site-packages/sklearn/base.py:1473](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/base.py#line=1472), in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File [~/python312/lib/python3.12/site-packages/sklearn/pipeline.py:533](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/pipeline.py#line=532), in Pipeline.fit_transform(self, X, y, **params)
    490 """"""Fit the model and transform with the final estimator.
    491 
    492 Fit all the transformers one after the other and sequentially transform
   (...)
    530     Transformed samples.
    531 """"""
    532 routed_params = self._check_method_params(method=""fit_transform"", props=params)
--> 533 Xt = self._fit(X, y, routed_params)
    535 last_step = self._final_estimator
    536 with _print_elapsed_time(""Pipeline"", self._log_message(len(self.steps) - 1)):

File [~/python312/lib/python3.12/site-packages/sklearn/pipeline.py:406](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/pipeline.py#line=405), in Pipeline._fit(self, X, y, routed_params)
    404     cloned_transformer = clone(transformer)
    405 # Fit or load from cache the current transformer
--> 406 X, fitted_transformer = fit_transform_one_cached(
    407     cloned_transformer,
    408     X,
    409     y,
    410     None,
    411     message_clsname=""Pipeline"",
    412     message=self._log_message(step_idx),
    413     params=routed_params[name],
    414 )
    415 # Replace the transformer of the step with the fitted
    416 # transformer. This is necessary when loading the transformer
    417 # from the cache.
    418 self.steps[step_idx] = (name, fitted_transformer)

File [~/python312/lib/python3.12/site-packages/joblib/memory.py:312](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/joblib/memory.py#line=311), in NotMemorizedFunc.__call__(self, *args, **kwargs)
    311 def __call__(self, *args, **kwargs):
--> 312     return self.func(*args, **kwargs)

File [~/python312/lib/python3.12/site-packages/sklearn/pipeline.py:1310](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/pipeline.py#line=1309), in _fit_transform_one(transformer, X, y, weight, message_clsname, message, params)
   1308 with _print_elapsed_time(message_clsname, message):
   1309     if hasattr(transformer, ""fit_transform""):
-> 1310         res = transformer.fit_transform(X, y, **params.get(""fit_transform"", {}))
   1311     else:
   1312         res = transformer.fit(X, y, **params.get(""fit"", {})).transform(
   1313             X, **params.get(""transform"", {})
   1314         )

File [~/python312/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/utils/_set_output.py#line=315), in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)
    314 @wraps(f)
    315 def wrapped(self, X, *args, **kwargs):
--> 316     data_to_wrap = f(self, X, *args, **kwargs)
    317     if isinstance(data_to_wrap, tuple):
    318         # only wrap the first output for cross decomposition
    319         return_tuple = (
    320             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    321             *data_to_wrap[1:],
    322         )

File [~/python312/lib/python3.12/site-packages/sklearn/base.py:1098](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/base.py#line=1097), in TransformerMixin.fit_transform(self, X, y, **fit_params)
   1083         warnings.warn(
   1084             (
   1085                 f""This object ({self.__class__.__name__}) has a `transform`""
   (...)
   1093             UserWarning,
   1094         )
   1096 if y is None:
   1097     # fit method of arity 1 (unsupervised transformation)
-> 1098     return self.fit(X, **fit_params).transform(X)
   1099 else:
   1100     # fit method of arity 2 (supervised transformation)
   1101     return self.fit(X, y, **fit_params).transform(X)

File [~/python312/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/utils/_set_output.py#line=315), in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)
    314 @wraps(f)
    315 def wrapped(self, X, *args, **kwargs):
--> 316     data_to_wrap = f(self, X, *args, **kwargs)
    317     if isinstance(data_to_wrap, tuple):
    318         # only wrap the first output for cross decomposition
    319         return_tuple = (
    320             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    321             *data_to_wrap[1:],
    322         )

File [~/python312/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py:1012](http://26.2.133.182:8888/lab/tree/jnote/sunkusun9/kaggle/PGS4_ep11/result/python312/lib/python3.12/site-packages/sklearn/preprocessing/_encoders.py#line=1011), in OneHotEncoder.transform(self, X)
   1010 if transform_output != ""default"" and self.sparse_output:
   1011     capitalize_transform_output = transform_output.capitalize()
-> 1012     raise ValueError(
   1013         f""{capitalize_transform_output} output does not support sparse data.""
   1014         f"" Set sparse_output=False to output {transform_output} dataframes or""
   1015         f"" disable {capitalize_transform_output} output via""
   1016         '` ohe.set_output(transform=""default"").'
   1017     )
   1019 # validation of X happens in _check_X called by _transform
   1020 warn_on_unknown = self.drop is not None and self.handle_unknown in {
   1021     ""ignore"",
   1022     ""infrequent_if_exist"",
   1023 }

ValueError: Pandas output does not support sparse data. Set sparse_output=False to output pandas dataframes or disable Pandas output via` ohe.set_output(transform=""default"").
```
### Versions

```shell
System:
    python: 3.12.6 (main, Sep 30 2024, 02:19:13) [GCC 9.4.0]
executable: /home/sun9sun9/python312/bin/python3.12
   machine: Linux-5.4.0-196-generic-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.5.2
          pip: 24.3.1
   setuptools: 75.2.0
        numpy: 1.26.4
        scipy: 1.12.0
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.8.4
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /home/sun9sun9/python312/lib/python3.12/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /home/sun9sun9/python312/lib/python3.12/site-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libgomp
       filepath: /home/sun9sun9/python312/lib/python3.12/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
```
","['Bug', 'Enhancement']","2024-11-20T05:43:16Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/30310","Syntax Error"
"26","scikit-learn/scikit-learn","ColumnTransformer does not validate sparse formats for X","### Describe the bug

If the underlying transformers all accept sparse input data, `ColumnTransformer` should also be able to accept sparse input data. That's indeed the case for the `csr`, `csc`, `lil` and `dok` formats but it raises errors for the `bsr`, `coo`, `dia` formats because those are not ""subscriptable"". 

As a possible fix, we could validate sparse input data by using `accept_sparse=(""csr"", ""csc"", ""lil"", ""dok"")` which will then convert to a ""subscriptable"" sparse format. Currently it is not done as `ColumnTransformer` relies on its own `_check_X` which often entirely bypasses the validation, maybe for performance reasons ?

### Steps/Code to Reproduce

```python
import numpy as np
from scipy.sparse import dia_array
from sklearn.compose import ColumnTransformer

rng = np.random.RandomState(1)
X = rng.uniform(size=(10, 3))
y = rng.randint(0, 3, size=10)
X = dia_array(X)

est = ColumnTransformer(transformers=[('trans1','passthrough',[0,1])])
est.fit(X, y)
```

### Expected Results

No error is thrown.

### Actual Results

```python-traceback
TypeError: 'dia_array' object is not subscriptable
```

### Versions

```shell
System:
    python: 3.12.5 | packaged by conda-forge | (main, Aug  8 2024, 18:32:50) [Clang 16.0.6 ]
executable: /Users/abaker/miniforge3/envs/sklearn-dev/bin/python
   machine: macOS-14.5-arm64-arm-64bit

Python dependencies:
      sklearn: 1.6.dev0
          pip: 24.2
   setuptools: 73.0.1
        numpy: 2.1.0
        scipy: 1.14.1
       Cython: 3.0.11
       pandas: 2.2.2
   matplotlib: 3.9.2
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /Users/abaker/miniforge3/envs/sklearn-dev/lib/libopenblas.0.dylib
        version: 0.3.27
threading_layer: openmp
   architecture: VORTEX

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: /Users/abaker/miniforge3/envs/sklearn-dev/lib/libomp.dylib
        version: None
```
","['Bug']","2024-11-14T16:40:29Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/30275","Runtime Error"
"27","scikit-learn/scikit-learn","Changelog check on towncrier false positive case","Observed on this PR: https://github.com/scikit-learn/scikit-learn/pull/30209
This run: https://github.com/scikit-learn/scikit-learn/actions/runs/11681055082/job/32525320042?pr=30209

The PR needs to add PR number to existing changelog, and changes another affected changelog, therefore there are 3 changelog files affected in the PR. However, the changelog checker complains with:

```
 Not all changelog file number(s) match this pull request number (30209):
doc/whats_new/upcoming_changes/sklearn.calibration/30171.api.rst
doc/whats_new/upcoming_changes/sklearn.frozen/29705.major-feature.rst
doc/whats_new/upcoming_changes/sklearn.frozen/30209.major-feature.rst
```

Which I'd say is a false positive.

cc @lesteve ","['Bug', 'Build / CI']","2024-11-05T09:28:21Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/30222","Logical Bug"
"28","scikit-learn/scikit-learn","OneVsRestClassifier cannot be used with TunedThresholdClassifierCV","https://github.com/scikit-learn/scikit-learn/blob/d5082d32de2797f9594c9477f2810c743560a1f1/sklearn/model_selection/_classification_threshold.py#L386

When predict is called on `OneVsRestClassifier`, it calls `predict_proba` on the underlying classifier.

If the underlying is a `TunedThresholdClassifierCV`, it redirects to the underlying estimator instead.

On the line referenced, I think that `OneVsRestClassifier` should check if the estimator is `TunedThresholdClassifierCV`, and if so use the `best_threshold_` instead of 0.5","['Bug', 'Needs Decision']","2024-10-09T07:31:21Z","10","0","https://github.com/scikit-learn/scikit-learn/issues/30036","Logical Bug"
"29","scikit-learn/scikit-learn","SGDOneClassSVM model does not converge with default stopping criteria(stops prematurely)","### Describe the bug

SGDOneClassSVM does not converge with default early stopping criteria, because the used loss is not actual loss, but only error, which can be easily 0.0 and then increase as the model converges to adequate solution. That is, the used for stopping and reported with verbose ""loss"" value doesn't accout for the full model formula/regularization. Also, pay attention to bias term to gauge convergence.
https://github.com/scikit-learn/scikit-learn/blob/c7839c48363d1531af9a00abfcb9d911ecfcb2b2/sklearn/linear_model/_sgd_fast.pyx.tp#L482
The optimization almost always stops after 6 epochs, the initial epoch, plus the 5 for stopping tolerance (can't change the number of epochs for the stopping tolerance btw).
The problem does not manifest with toy data(small dimensiaonal), becasue 6 epochs is likely enough for convergence to satisfactory solution.
In the reproduction code, mind the console output and comments. Possible workaround at the end of reproduction code, is to use tol=None with manual epoch limit(max_iter), but that slows the optimization down by a lot, since forbids the use of learning_rate=""adaptive"".

### Steps/Code to Reproduce
```python
import numpy as np
from matplotlib import pyplot as plt
import pandas as pd
#from sklearn.linear_model import Ridge
from sklearn.datasets import make_regression
from timeit import timeit
from sklearn.linear_model import SGDOneClassSVM
from sklearn.svm import OneClassSVM
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler

np.random.seed(123)

#no matter the feature count, optimization stops in 6 epochs
print(""fitting different feature counts:"")
featCnts = [10, 1000, 25000]
for featCnt in featCnts:
    print(""\n10k samples, {} features"".format(featCnt))
    x, y = make_regression(10000, featCnt, n_informative=featCnt // 10)
    x = MinMaxScaler().fit_transform(x) + 1.0 #make positive
    model = SGDOneClassSVM(nu=0.01, verbose=10)#, tol=1e-10)
    model.fit(x) #see console output

#model reports 0 ""loss"" even after just 1 sample
print(""\n\npartial fit test:"")
x, y = make_regression(10000, 2500//4, n_informative=250)
xPos = StandardScaler().fit_transform(x) + 100 #highly positive data
#Note: linear one class svm has to run with e.g. positive data for correct function(not centered about 0), otherwise data is not separable from origin
print(""xPos data mean"", xPos.mean())
model = SGDOneClassSVM(nu=0.01, verbose=10)#, tol=1e-10)
print(""fit 1 sample"")
model.partial_fit(xPos[:1], y) #gives avg loss 0.0
print(""fit 100 samples"")
model.partial_fit(xPos[:100], y)

#demonstrate that model output(decision function) is far from expected
# by comparing with a slow reference model
print(""\n\nreference comparison:"")
testNu = 0.5
modelSgd = make_pipeline( MinMaxScaler(), SGDOneClassSVM(nu=testNu, verbose=10) )
modelSgd.fit(x)
modelSgdDecFn = modelSgd.decision_function(x)
modelSgdClass = modelSgd.predict(x)

refMinMaxModel = make_pipeline( MinMaxScaler(), OneClassSVM(nu=testNu, verbose=10, tol=1e-10, kernel=""linear"", shrinking=False) ).fit(x)
refMinMax = refMinMaxModel.decision_function(x)
refMinMaxClass = refMinMaxModel.predict(x)

#slow sgd, manually tuned learning rate model
invScalingPower = np.emath.logn( len(x) * 9000, 100 ) #target lr reduction coefficient after N samples (len(x) * epoch count to get sample count)
#invScalingPower =0.2
print(""invScalingPower"", invScalingPower )
sgdMinMaxModel = make_pipeline( MinMaxScaler(), SGDOneClassSVM(nu=testNu, verbose=10, max_iter=10000//10, tol=None,
    learning_rate=""constant"", eta0=0.001, power_t=invScalingPower, average=True) ).fit(x)#, sgdoneclasssvm__coef_init=np.random.normal(-100.0, 1.0, x.shape[1])) # sgdoneclasssvm__offset_init=1000)
sgdMinMax = sgdMinMaxModel.decision_function(x)
sgdMinMaxClass = sgdMinMaxModel.predict(x)


df = pd.DataFrame({""sklearn default modelSgd"": modelSgdDecFn,  ""reference ocsvm refMinMax"": refMinMax, ""manual stopping sgdMinMax"": sgdMinMax, })
print(""decision function:\n"", df)
print(""correlation between decision_function:\n"", df.corr())

df = pd.DataFrame({""sklearn default modelSgd"": modelSgdClass,  ""reference ocsvm refMinMax"": refMinMaxClass, ""manual stopping sgdMinMax"": sgdMinMaxClass, })
print(""correlation between predict:\n"", df.corr())
```

### Expected Results

Model convergence criteria works adequately (especially important for the most efficient learning_rate=""adaptive""), and model reaches convergence.

### Actual Results

Optimization stops prematurely, usually after 6 epochs:
```
fitting different feature counts:

10k samples, 10 features
-- Epoch 1
Norm: 1.41, NNZs: 10, Bias: -5.067461, T: 10000, Avg. loss: 0.000088
Total training time: 0.00 seconds.
-- Epoch 2
Norm: 1.66, NNZs: 10, Bias: -6.148603, T: 20000, Avg. loss: 0.000187
Total training time: 0.00 seconds.
-- Epoch 3
Norm: 1.80, NNZs: 10, Bias: -6.766193, T: 30000, Avg. loss: 0.000267
Total training time: 0.00 seconds.
-- Epoch 4
Norm: 1.87, NNZs: 10, Bias: -7.203003, T: 40000, Avg. loss: 0.000191
Total training time: 0.00 seconds.
-- Epoch 5
Norm: 1.97, NNZs: 10, Bias: -7.528345, T: 50000, Avg. loss: 0.000193
Total training time: 0.00 seconds.
-- Epoch 6
Norm: 2.01, NNZs: 10, Bias: -7.798230, T: 60000, Avg. loss: 0.000285
Total training time: 0.00 seconds.
Convergence after 6 epochs took 0.01 seconds

10k samples, 1000 features
-- Epoch 1
Norm: 0.95, NNZs: 1000, Bias: -5.741972, T: 10000, Avg. loss: 0.000000
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 0.47, NNZs: 1000, Bias: -7.123019, T: 20000, Avg. loss: 0.000000
Total training time: 0.02 seconds.
-- Epoch 3
Norm: 0.32, NNZs: 1000, Bias: -7.932197, T: 30000, Avg. loss: 0.000000
Total training time: 0.03 seconds.
-- Epoch 4
Norm: 0.24, NNZs: 1000, Bias: -8.506685, T: 40000, Avg. loss: 0.000000
Total training time: 0.05 seconds.
-- Epoch 5
Norm: 0.38, NNZs: 1000, Bias: -8.948081, T: 50000, Avg. loss: 0.000001
Total training time: 0.06 seconds.
-- Epoch 6
Norm: 0.32, NNZs: 1000, Bias: -9.312374, T: 60000, Avg. loss: 0.000000
Total training time: 0.07 seconds.
Convergence after 6 epochs took 0.07 seconds

10k samples, 25000 features
-- Epoch 1
Norm: 4.73, NNZs: 25000, Bias: -5.741972, T: 10000, Avg. loss: 0.000000
Total training time: 0.24 seconds.
-- Epoch 2
Norm: 2.37, NNZs: 25000, Bias: -7.123019, T: 20000, Avg. loss: 0.000000
Total training time: 0.48 seconds.
-- Epoch 3
Norm: 1.58, NNZs: 25000, Bias: -7.932197, T: 30000, Avg. loss: 0.000000
Total training time: 0.71 seconds.
-- Epoch 4
Norm: 1.19, NNZs: 25000, Bias: -8.506685, T: 40000, Avg. loss: 0.000000
Total training time: 0.95 seconds.
-- Epoch 5
Norm: 0.95, NNZs: 25000, Bias: -8.952446, T: 50000, Avg. loss: 0.000000
Total training time: 1.19 seconds.
-- Epoch 6
Norm: 0.79, NNZs: 25000, Bias: -9.316738, T: 60000, Avg. loss: 0.000000
Total training time: 1.43 seconds.
Convergence after 6 epochs took 1.43 seconds


partial fit test:
xPos data mean 100.00000000000011
fit 1 sample
-- Epoch 1
Norm: 9397.76, NNZs: 625, Bias: 4.722997, T: 1, Avg. loss: 0.000000
Total training time: 0.00 seconds.
fit 100 samples
-- Epoch 1
Norm: 3262.77, NNZs: 625, Bias: 2.619430, T: 100, Avg. loss: 0.000000
Total training time: 0.00 seconds.


reference comparison:
-- Epoch 1
Norm: 1.21, NNZs: 625, Bias: -13.904809, T: 10000, Avg. loss: 0.001472
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 1.32, NNZs: 625, Bias: -15.210391, T: 20000, Avg. loss: 0.001823
Total training time: 0.02 seconds.
-- Epoch 3
Norm: 1.38, NNZs: 625, Bias: -15.971154, T: 30000, Avg. loss: 0.002099
Total training time: 0.03 seconds.
-- Epoch 4
Norm: 1.43, NNZs: 625, Bias: -16.509796, T: 40000, Avg. loss: 0.002299
Total training time: 0.04 seconds.
-- Epoch 5
Norm: 1.46, NNZs: 625, Bias: -16.926977, T: 50000, Avg. loss: 0.002480
Total training time: 0.04 seconds.
-- Epoch 6
Norm: 1.49, NNZs: 625, Bias: -17.267392, T: 60000, Avg. loss: 0.002436
Total training time: 0.05 seconds.
Convergence after 6 epochs took 0.05 seconds
..*
optimization finished, #iter = 2497
obj = 1924865835.187691, rho = 776455.657200
nSV = 5001, nBSV = 4999
[LibSVM]invScalingPower 0.25143814733164016
-- Epoch 1
Norm: 0.40, NNZs: 625, Bias: -3.925000, T: 10000, Avg. loss: 0.000117
Total training time: 0.01 seconds.
-- Epoch 2
Norm: 0.79, NNZs: 625, Bias: -8.770000, T: 20000, Avg. loss: 0.000492
Total training time: 0.02 seconds.
-- Epoch 3

...

-- Epoch 998
Norm: 24.84, NNZs: 625, Bias: -309.593000, T: 9980000, Avg. loss: 1.322528
Total training time: 14.50 seconds.
-- Epoch 999
Norm: 24.80, NNZs: 625, Bias: -309.597000, T: 9990000, Avg. loss: 1.315051
Total training time: 14.52 seconds.
-- Epoch 1000
Norm: 24.80, NNZs: 625, Bias: -309.597000, T: 10000000, Avg. loss: 1.310013
Total training time: 14.53 seconds.
decision function:
       sklearn default modelSgd  reference ocsvm refMinMax  manual stopping sgdMinMax
0                     0.152997               -8169.563012                  -2.922611
1                     0.208675               -5888.609526                  -2.068050
2                     0.460575                4671.620928                   1.890327
3                     0.324515                -993.308774                  -0.232038
4                     0.376079                1109.737641                   0.558291
...                        ...                        ...                        ...
9995                  0.203837               -5984.511457                  -2.103481
9996                  0.263156               -3678.767938                  -1.238047
9997                  0.408527                2547.241215                   1.097869
9998                  0.408359                2477.709831                   1.068493
9999                  0.534379                7887.142835                   3.098472

[10000 rows x 3 columns]
correlation between decision_function:
                            sklearn default modelSgd  reference ocsvm refMinMax  manual stopping sgdMinMax
sklearn default modelSgd                   1.000000                   0.999918                   0.999919
reference ocsvm refMinMax                  0.999918                   1.000000                   1.000000
manual stopping sgdMinMax                  0.999919                   1.000000                   1.000000
correlation between predict:
                            sklearn default modelSgd  reference ocsvm refMinMax  manual stopping sgdMinMax
sklearn default modelSgd                   1.000000                   0.195700                   0.203691
reference ocsvm refMinMax                  0.195700                   1.000000                   0.960769
manual stopping sgdMinMax                  0.203691                   0.960769                   1.000000
```

### Versions

```shell
System:
    python: 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)]
executable: E:\Program Files\Python\Python37\python.exe
   machine: Windows-10-10.0.17763-SP0

Python dependencies:
          pip: 24.0
   setuptools: 68.0.0
      sklearn: 1.0.2
        numpy: 1.21.6
        scipy: 1.7.3
       Cython: 3.0.11
       pandas: 1.3.5
   matplotlib: 3.5.3
       joblib: 1.0.1
threadpoolctl: 2.1.0

Built with OpenMP: True
```
","['Bug']","2024-10-07T18:11:39Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/30027","Syntax Error"
"30","scikit-learn/scikit-learn","ClassifierChain does not accept NaN values even when base estimator supports them","### Describe the bug

I am working on a multilabel classification problem using ClassifierChain with RandomForestClassifier as the base estimator.
I have encountered an issue where ClassifierChain raises a ValueError when the input data X contains np.nan values, even though RandomForestClassifier can handle np.nan values natively.
When I use RandomForestClassifier alone, it processes np.nan values without any problems, thanks to its internal tree splitting mechanism that supports missing values. Similarly, when I use MultiOutputClassifier with RandomForestClassifier, I do not encounter any errors with np.nan values.
However, when I use ClassifierChain, I receive an error during hyperparameter tuning.
Since the base estimator can handle np.nan values, I expected ClassifierChain to pass the data through without additional checks. It seems inconsistent that ClassifierChain does not support missing values when the base estimator does. 
I'm wondering if this is the intended behavior of ClassifierChain. If not, could it be updated to support missing values when the base estimator does? Alternatively, is there a recommended workaround that doesn't involve imputing or dropping missing values?

### Steps/Code to Reproduce

```py
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import ClassifierChain, MultiOutputClassifier


X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1) # Input data with NaN values
y = np.array([[0, 1], [0, 0], [1, 0], [1, 1]])

base_clf = RandomForestClassifier() # Base classifier

clf_br = MultiOutputClassifier(base_clf) # MultiOutputClassifier (Binary Relevance) - works fine with NaN
clf_br.fit(X, y)  # No error

clf_chain = ClassifierChain(base_clf) # ClassifierChain - raises error
clf_chain.fit(X, y)  # Raises ValueError about NaNs
```

### Expected Results

No error is thrown when using ClassifierChain with RandomForestClassifier as the base estimator, since RandomForestClassifier handles np.nan values natively without any issues. The model should fit and train successfully, just as it does with MultiOutputClassifier.

### Actual Results

Instead, when fitting the ClassifierChain, I receive the following error message:
```py-tb
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[2], line 5
      2 clf_chain = ClassifierChain(base_clf)
      4 # Fitting the model (this will likely raise an error due to NaN in X)
----> 5 clf_chain.fit(X, y)

File <env_path>/python3.12/site-packages/sklearn/base.py, line=1472, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File <env_path>/python3.12/site-packages/sklearn/multioutput.py, line=1029, in ClassifierChain.fit(self, X, Y, **fit_params)
   1005 """"""Fit the model to data matrix X and targets Y.
   1006 
   1007 Parameters
   (...)
   1026     Class instance.
   1027 """"""
   1028 _raise_for_params(fit_params, self, ""fit"")
-> 1030 super().fit(X, Y, **fit_params)
   1031 self.classes_ = [estimator.classes_ for estimator in self.estimators_]
   1032 return self

File <env_path>/python3.12/site-packages/sklearn/multioutput.py, line=720, in _BaseChain.fit(self, X, Y, **fit_params)
    699 @abstractmethod
    700 def fit(self, X, Y, **fit_params):
    701     """"""Fit the model to data matrix X and targets Y.
    702 
    703     Parameters
   (...)
    719         Returns a fitted instance.
    720     """"""
--> 721     X, Y = self._validate_data(X, Y, multi_output=True, accept_sparse=True)
    723     random_state = check_random_state(self.random_state)
    724     self.order_ = self.order

File <env_path>/python3.12/site-packages/sklearn/base.py, line=649, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)
    648         y = check_array(y, input_name=""y"", **check_y_params)
    649     else:
--> 650         X, y = check_X_y(X, y, **check_params)
    651     out = X, y
    653 if not no_val_X and check_params.get(""ensure_2d"", True):

File <env_path>/python3.12/site-packages/sklearn/utils/validation.py, line=1300, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
   1296         estimator_name = _check_estimator_name(estimator)
   1297     raise ValueError(
   1298         f""{estimator_name} requires y to be passed, but the target y is None""
   1299     )
-> 1301 X = check_array(
   1302     X,
   1303     accept_sparse=accept_sparse,
   1304     accept_large_sparse=accept_large_sparse,
   1305     dtype=dtype,
   1306     order=order,
   1307     copy=copy,
   1308     force_writeable=force_writeable,
   1309     force_all_finite=force_all_finite,
   1310     ensure_2d=ensure_2d,
   1311     allow_nd=allow_nd,
   1312     ensure_min_samples=ensure_min_samples,
   1313     ensure_min_features=ensure_min_features,
   1314     estimator=estimator,
   1315     input_name=""X"",
   1316 )
   1318 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
   1320 check_consistent_length(X, y)

File <env_path>/python3.12/site-packages/sklearn/utils/validation.py, line=1063, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)
   1058     raise ValueError(
   1059         ""Found array with dim %d. %s expected <= 2.""
   1060         % (array.ndim, estimator_name)
   1061     )
   1063 if force_all_finite:
-> 1064     _assert_all_finite(
   1065         array,
   1066         input_name=input_name,
   1067         estimator_name=estimator_name,
   1068         allow_nan=force_all_finite == ""allow-nan"",
   1069     )
   1071 if copy:
   1072     if _is_numpy_namespace(xp):
   1073         # only make a copy if `array` and `array_orig` may share memory`

File <env_path>/python3.12/site-packages/sklearn/utils/validation.py, line=122, in _assert_all_finite(X, allow_nan, msg_dtype, estimator_name, input_name)
    120 if first_pass_isfinite:
    121     return
--> 123 _assert_all_finite_element_wise(
    124     X,
    125     xp=xp,
    126     allow_nan=allow_nan,
    127     msg_dtype=msg_dtype,
    128     estimator_name=estimator_name,
    129     input_name=input_name,
    130 )

File <env_path>/python3.12/site-packages/sklearn/utils/validation.py, line=171, in _assert_all_finite_element_wise(X, xp, allow_nan, msg_dtype, estimator_name, input_name)
    155 if estimator_name and input_name == ""X"" and has_nan_error:
    156     # Improve the error message on how to handle missing values in
    157     # scikit-learn.
    158     msg_err += (
    159         f""\n{estimator_name} does not accept missing values""
    160         "" encoded as NaN natively. For supervised learning, you might want""
   (...)
    170         ""#estimators-that-handle-nan-values""
    171     )
--> 172 raise ValueError(msg_err)

ValueError: Input X contains NaN.
ClassifierChain does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values
```

### Versions

```shell
System and Versions:

Python version: 3.12.5 (packaged by conda-forge)
scikit-learn version: 1.5.1
```
","['Bug']","2024-09-16T07:54:41Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/29856","Logical Bug"
"31","scikit-learn/scikit-learn","Discrepancy between .fit_transform() and .transform() methods in the LLE module","### Describe the bug

A user would expect the same result from  
- `.fit(X)` and then `.transform(X)`
- `.fit_transformX()`

But this is not the case in the current code for `LocallyLinearEmbedding`. 

### Steps/Code to Reproduce

```python
from sklearn.manifold import LocallyLinearEmbedding
from sklearn.datasets import make_s_curve
import numpy as np

X,_ = make_s_curve(100)
methods = [""standard"", ""hessian"", ""ltsa"", ""modified""]
for method in methods:
    lle = LocallyLinearEmbedding(method=method, n_neighbors=12)
    fit_transform = lle.fit_transform(X)  
    fit_then_transform = lle.transform(X)
    equal = np.any(fit_transform == fit_then_transform)
    close_count = np.isclose(fit_transform ,fit_then_transform).sum()
    print(f""For {method} it is {equal} that f_t and f_then_t are equal."")
    print(f""Only {close_count} are close.\n"" )
```

### Expected Results

```text
For {method} it is True that `fit_transform(X) == transform(x)`.
```

### Actual Results

```text
For standard, it is False that `fit_transform(X) == transform(x)`.
Only 2 are close.

For hessian, it is False that `fit_transform(X) == transform(x)`.
Only 1 are close.

For ltsa, it is False that `fit_transform(X) == transform(x)`.
Only 1 are close.

For modified, it is False that `fit_transform(X) == transform(x)`.
Only 0 are close.
```

### Versions

```shell
System:
    python: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ]
executable: /Users/wonderman/miniforge3/envs/sklearn_dev_env/bin/python
   machine: macOS-14.6.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.6.dev0
          pip: 24.2
   setuptools: 72.2.0
        numpy: 2.1.0
        scipy: 1.14.1
       Cython: 3.0.11
       pandas: None
   matplotlib: None
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /Users/wonderman/miniforge3/envs/sklearn_dev_env/lib/libopenblas.0.dylib
        version: 0.3.27
threading_layer: openmp
   architecture: VORTEX

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: /Users/wonderman/miniforge3/envs/sklearn_dev_env/lib/libomp.dylib
        version: None
```
","['Bug']","2024-09-05T21:35:17Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/29792","Logical Bug"
"32","scikit-learn/scikit-learn","spin docs --no-plot runs the examples","Seen at the EuroScipy sprint

Commands run by spin:
```
$ export SPHINXOPTS=-W -D plot_gallery=0 -j auto
$ cd doc
$ make html
```

Looks like our Makefile does not use SPHINXOPTS the same way as expected:
Probably we have a slightly different way of building the doc

```
 make html-noplot -n
sphinx-build -D plot_gallery=0 -b html -d _build/doctrees  -T  . -jauto \
    _build/html/stable
echo
echo ""Build finished. The HTML pages are in _build/html/stable.""
```","['Bug', 'Sprint']","2024-08-30T08:31:28Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/29742","Logical Bug"
"33","scikit-learn/scikit-learn","Default argument pos_label=1 is not ignored in f1_score metric for multiclass classification","### Describe the bug

I get a `ValueError` for `pos_label=1` default argument value to `f1_score` metric with argument `average='micro'` for the iris flower classification problem:

```pytb
ValueError: pos_label=1 is not a valid label: It should be one of ['setosa' 'versicolor' 'virginica']
```

According to the documentation, the `pos_label` argument should be ignored for the multiclass problem:

https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#f1-score

_The class to report if `average='binary'` and the data is binary, otherwise this parameter is ignored._

Setting `pos_label` explicitly to None solves the problem and produces the expected output, see below.

### Steps/Code to Reproduce

```python
# Import necessary libraries
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import make_scorer, f1_score

# Load the Iris dataset
data = load_iris()
X = data.data  # Features
y = data.target  # Labels

# Convert labels to string type
y = np.array([data.target_names[label] for label in data.target])

# Initialize the Linear Discriminant Analysis classifier
classifier = LinearDiscriminantAnalysis()

# Define a custom scorer using F1 score with average='micro'
f1_scorer = make_scorer(f1_score, average='micro', pos_label=1)

# Perform cross-validation with cross_val_score
try:
    scores = cross_val_score(classifier, X, y, cv=5, scoring=f1_scorer)
    print(f""Cross-validated F1 Scores (micro average): {scores}"")
    print(f""Mean F1 Score: {np.mean(scores)}"")
except ValueError as e:
    print(f""Error: {e}"")
```

### Expected Results

```
Cross-validated F1 Scores (micro average): [1.         1.         0.96666667 0.93333333 1.        ]
Mean F1 Score: 0.9800000000000001
```

### Actual Results

```pytb
Cross-validated F1 Scores (micro average): [nan nan nan nan nan]
Mean F1 Score: nan
[C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\model_selection\_validation.py:1000](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/model_selection/_validation.py#line=999): UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File ""[C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py"", line 139](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/metrics/_scorer.py#line=138), in __call__
    score = scorer._score(
            ^^^^^^^^^^^^^^
  File ""[C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py"", line 371](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/metrics/_scorer.py#line=370), in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File ""[C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py"", line 89](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/metrics/_scorer.py#line=88), in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File ""[C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\utils\_response.py"", line 204](file:///C:/Users/rgt0227/AppData/Local/anaconda3/Lib/site-packages/sklearn/utils/_response.py#line=203), in _get_response_values
    raise ValueError(
ValueError: pos_label=1 is not a valid label: It should be one of ['setosa' 'versicolor' 'virginica']
```

### Versions

```shell
System:
    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\rgt0227\AppData\Local\anaconda3\python.exe
   machine: Windows-10-10.0.19045-SP0

Python dependencies:
      sklearn: 1.5.0
          pip: 23.2.1
   setuptools: 68.0.0
        numpy: 1.26.2
        scipy: 1.11.4
       Cython: None
       pandas: 2.1.1
   matplotlib: 3.8.0
       joblib: 1.2.0
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: mkl
    num_threads: 8
         prefix: mkl_rt
       filepath: C:\Users\rgt0227\AppData\Local\anaconda3\Library\bin\mkl_rt.2.dll
        version: 2023.1-Product
threading_layer: intel

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: vcomp
       filepath: C:\Users\rgt0227\AppData\Local\anaconda3\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None
```
","['Bug']","2024-08-29T07:45:11Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/29734","Syntax Error"
"34","scikit-learn/scikit-learn","LocallyLinearEmbedding : n_neighbors <= n_samples","### Describe the bug

Minor bug in `LocallyLinearEmbedding`'s parameter validation:

https://github.com/scikit-learn/scikit-learn/blob/70fdc843a4b8182d97a3508c1a426acc5e87e980/sklearn/manifold/_locally_linear.py#L226-L230

The `if` condition contradicts the error message in the case that `n_neighbors == N`. So you get a message like

```python-traceback
ValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3""
```

which doesn't make sense.

### Steps/Code to Reproduce

```python
import numpy as np
import sklearn.manifold

X = np.random.randn(3, 5)

embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])

embedder.fit_transform(X)
```

### Expected Results

n/a

### Actual Results

```python-traceback
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1119], line 8
      4 X = np.random.randn(3, 5)
      6 embedder = sklearn.manifold.LocallyLinearEmbedding(n_neighbors=X.shape[0])
----> 8 embedder.fit_transform(X)

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/utils/_set_output.py:313, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)
    311 @wraps(f)
    312 def wrapped(self, X, *args, **kwargs):
--> 313     data_to_wrap = f(self, X, *args, **kwargs)
    314     if isinstance(data_to_wrap, tuple):
    315         # only wrap the first output for cross decomposition
    316         return_tuple = (
    317             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    318             *data_to_wrap[1:],
    319         )

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:848, in LocallyLinearEmbedding.fit_transform(self, X, y)
    831 @_fit_context(prefer_skip_nested_validation=True)
    832 def fit_transform(self, X, y=None):
    833     \""\""\""Compute the embedding vectors for data X and transform X.
    834 
    835     Parameters
   (...)
    846         Returns the instance itself.
    847     \""\""\""
--> 848     self._fit_transform(X)
    849     return self.embedding_

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:795, in LocallyLinearEmbedding._fit_transform(self, X)
    793 X = self._validate_data(X, dtype=float)
    794 self.nbrs_.fit(X)
--> 795 self.embedding_, self.reconstruction_error_ = _locally_linear_embedding(
    796     X=self.nbrs_,
    797     n_neighbors=self.n_neighbors,
    798     n_components=self.n_components,
    799     eigen_solver=self.eigen_solver,
    800     tol=self.tol,
    801     max_iter=self.max_iter,
    802     method=self.method,
    803     hessian_tol=self.hessian_tol,
    804     modified_tol=self.modified_tol,
    805     random_state=random_state,
    806     reg=self.reg,
    807     n_jobs=self.n_jobs,
    808 )
    809 self._n_features_out = self.embedding_.shape[1]

File ~/Library/Python/3.12/lib/python/site-packages/sklearn/manifold/_locally_linear.py:227, in _locally_linear_embedding(X, n_neighbors, n_components, reg, eigen_solver, tol, max_iter, method, hessian_tol, modified_tol, random_state, n_jobs)
    223     raise ValueError(
    224         \""output dimension must be less than or equal to input dimension\""
    225     )
    226 if n_neighbors >= N:
--> 227     raise ValueError(
    228         \""Expected n_neighbors <= n_samples,  but n_samples = %d, n_neighbors = %d\""
    229         % (N, n_neighbors)
    230     )
    232 M_sparse = eigen_solver != \""dense\""
    234 if method == \""standard\"":

ValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 3""
```

### Versions

```shell
System:
    python: 3.12.3 (v3.12.3:f6650f9ad7, Apr  9 2024, 08:18:47) [Clang 13.0.0 (clang-1300.0.29.30)]
executable: /usr/local/bin/python3
   machine: macOS-14.5-arm64-arm-64bit

Python dependencies:
      sklearn: 1.5.0
          pip: 24.0
   setuptools: 70.0.0
        numpy: 1.26.4
        scipy: 1.13.0
       Cython: 3.0.10
       pandas: 2.2.2
   matplotlib: 3.8.4
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 11
         prefix: libopenblas
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: blas
   internal_api: openblas
    num_threads: 11
         prefix: libopenblas
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.26.dev
threading_layer: pthreads
   architecture: neoversen1

       user_api: openmp
   internal_api: openmp
    num_threads: 11
         prefix: libomp
       filepath: /Users/gabriel.kissin/Library/Python/3.12/lib/python/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```
```
","['Bug']","2024-08-25T21:22:56Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/29715","Syntax Error"
"35","scikit-learn/scikit-learn","GaussianProcessRegressor: wrong std and cov results when n_features>1 and no y normalization","### Describe the bug

When `n_features > 1` and `normalization_y` is `False`, the `GaussianProcessRegressor.predict` seems to return bad std and cov results, as it doesn't consider the scale of the different features (while it seems to be ok when `n_features > 1` and `normalization_y` is `True`).

By taking a look at the code, we can see that `GaussianProcessRegressor.predict` uses the `_y_train_std` attribute to compute the variance and covariance but this attribute is set to `ones(n_features)` when `normalize_y` is set to `False` (default value), giving equal scale to all features.

To fix this bug, one should always compute `_y_train_std` from the training data and use the boolean attribute `normalize_y` to undo the normalization of `y_mean` if necessary.

### Steps/Code to Reproduce

```python
import pytest
from numpy import array
from numpy import hstack
from sklearn.gaussian_process import GaussianProcessRegressor

x = array([[0.], [0.5], [1.]])
y = hstack((x**2, 10*x**2))
# Note that the second output is equal to 10 times the first one.

# With output normalization
gpr = GaussianProcessRegressor(normalize_y=True)
gpr.fit(x, y)
std = gpr.predict(array([[0.25]]), return_std=True)[1][0]
assert std[0] != std[1]
assert std[0] == pytest.approx(std[1]/10, rel=1e-9)
# As expected, the variance of the second output is 10 times larger than the first output.

# Without output normalization
gpr = GaussianProcessRegressor(normalize_y=False)
gpr.fit(x, y)
std = gpr.predict(array([[0.25]]), return_std=True)[1][0]
assert std[0] == std[1]
# The variance of the second output is equal to the variance of the first output.
```

### Expected Results

Without output normalization, the variance of the second output should be 10 times larger than the first output.

### Actual Results

Without output normalization, the variance of the second output is equal to the variance of the first output.

### Versions

```shell
System:
    python: 3.9.12 (tags/v3.9.12:b28265d, Mar 23 2022, 23:52:46) [MSC v.1929 64 bit (AMD64)]
executable: C:\Users\matthias.delozzo\workspace\GEMSEO\gemseo\.tox\py39\.venv\Scripts\python.exe
   machine: Windows-10-10.0.19045-SP0

Python dependencies:
      sklearn: 1.5.1
          pip: None
   setuptools: None
        numpy: 1.26.4
        scipy: 1.13.1
       Cython: None
       pandas: 2.2.2
   matplotlib: 3.9.2
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: C:\Users\matthias.delozzo\workspace\GEMSEO\gemseo\.tox\py39\.venv\Lib\site-packages\numpy.libs\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: vcomp
       filepath: C:\Users\matthias.delozzo\workspace\GEMSEO\gemseo\.tox\py39\.venv\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: C:\Users\matthias.delozzo\workspace\GEMSEO\gemseo\.tox\py39\.venv\Lib\site-packages\scipy.libs\libopenblas_v0.3.27--3aa239bc726cfb0bd8e5330d8d4c15c6.dll
        version: 0.3.27
threading_layer: pthreads
   architecture: Haswell
```
","['Bug']","2024-08-21T10:39:55Z","8","0","https://github.com/scikit-learn/scikit-learn/issues/29697","Logical Bug"
"36","scikit-learn/scikit-learn","test_svm fails on i386 with scipy 1.13","### Describe the bug

scipy 1.13 is triggering test failure in test_svc_ovr_tie_breaking[NuSVC] on i386 architecture.

The error can be seeing in debian CI tests, https://ci.debian.net/packages/s/scikit-learn/unstable/i386/
Full test log at https://ci.debian.net/packages/s/scikit-learn/unstable/i386/50043538/
or https://ci.debian.net/packages/s/scikit-learn/testing/i386/50043537/


### Steps/Code to Reproduce

On an i386 system with scipy 1.13 installed
```
$ pytest-3 /usr/lib/python3/dist-packages/sklearn/svm/tests/test_svm.py -k test_svc_ovr_tie_breaking
```

### Expected Results

test should pass 

### Actual Results

```
1333s _______________________ test_svc_ovr_tie_breaking[NuSVC] _______________________
1333s 
1333s SVCClass = <class 'sklearn.svm._classes.NuSVC'>
1333s 
1333s     @pytest.mark.parametrize(""SVCClass"", [svm.SVC, svm.NuSVC])
1333s     def test_svc_ovr_tie_breaking(SVCClass):
1333s         """"""Test if predict breaks ties in OVR mode.
1333s         Related issue: https://github.com/scikit-learn/scikit-learn/issues/8277
1333s         """"""
1333s         X, y = make_blobs(random_state=0, n_samples=20, n_features=2)
1333s     
1333s         xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)
1333s         ys = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
1333s         xx, yy = np.meshgrid(xs, ys)
1333s     
1333s         common_params = dict(
1333s             kernel=""rbf"", gamma=1e6, random_state=42, decision_function_shape=""ovr""
1333s         )
1333s         svm = SVCClass(
1333s             break_ties=False,
1333s             **common_params,
1333s         ).fit(X, y)
1333s         pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])
1333s         dv = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])
1333s >       assert not np.all(pred == np.argmax(dv, axis=1))
1333s E       assert not True
1333s E        +  where True = <function all at 0xf689d5e0>(array([1, 1, 1, ..., 1, 1, 1]) == array([1, 1, ..., dtype=int32)
1333s E        +    where <function all at 0xf689d5e0> = np.all
1333s E           
1333s E           Full diff:
1333s E           - array([1, 1, 1, ..., 1, 1, 1], dtype=int32)
1333s E           ?                              -------------
1333s E           + array([1, 1, 1, ..., 1, 1, 1]))
1333s 
1333s /usr/lib/python3/dist-packages/sklearn/svm/tests/test_svm.py:1225: AssertionError
```

### Versions

```shell
$ python3 -c ""import sklearn; sklearn.show_versions()""
# (on amd64, edited manually for i386)

System:
    python: 3.12.4 (main, Jul 15 2024, 12:17:32) [GCC 13.3.0]
executable: /usr/bin/python3
   machine: Linux-6.9.11-i386-i686-with-glibc2.39

Python dependencies:
      sklearn: 1.4.2
          pip: 24.1.1
   setuptools: 70.3.0
        numpy: 1.26.4
        scipy: 1.13.1
       Cython: 3.0.10
       pandas: 2.2.2+dfsg
   matplotlib: 3.8.3
       joblib: 1.3.2
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /usr/lib/i386-linux-gnu/openblas-pthread/libopenblasp-r0.3.27.so
        version: 0.3.27
threading_layer: pthreads
   architecture: Haswell
    num_threads: 8

       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /usr/lib/i386-linux-gnu/libgomp.so.1.0.0
        version: None
```
","['Bug']","2024-08-06T21:45:19Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/29633","Syntax Error"
"37","scikit-learn/scikit-learn","mirrors-prettier pre-commit has been archived so maybe should be replaced","### Describe the bug

Noticed your [mirrors-prettier pre-commit](https://github.com/pre-commit/mirrors-prettier) has been archived. I was going to suggest you remove and/or look for alternative linters for the scss / js files.

### Steps/Code to Reproduce

Noticed this in the .pre-commit-config.yaml

```yaml
-   repo: https://github.com/pre-commit/mirrors-prettier
    rev: v2.7.1
    hooks:
    -   id: prettier
        files: ^doc/scss/|^doc/js/scripts/
        exclude: ^doc/js/scripts/vendor/
        types_or: [""scss"", ""javascript""]
```

### Expected Results

N/A

### Actual Results

N/A

### Versions

```shell
1.5.1
```
","['Bug']","2024-08-04T07:29:35Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/29621","Logical Bug"
"38","scikit-learn/scikit-learn","PowerTransformer's standardize sensitive to small differences in data, with MinMaxScaler unable to scale","### Describe the bug

Edit - when I wrote ""sensitive to small differences in data"",  I meant the different outputs when a value is changed from 4.61 to 4.62 as shown below.   
Edit2 - I'm not sure this should be even flagged as a bug, rather it's an unexpected result that arises when you really shouldn't use PowerTransformer on certain data. However, users might apply the PowerTransformer on hundreds of features at once, and perhaps the situation described below could raise a warning?

Hello,  
I'm trying to test how a Yeo-Johnson transformation might affect a model I'm working on (however, this issue is related to standardization rather than to the Yeo-Johnson transformation itself).  
For certain features, which aren't well suited for a Yeo-Johnson or Box-Cox transformation, the algorithm returns extremely similar values (when standardize=False), with differences only in the last few significant digits. This is consistent with scipy's `yeojohnson` and behaving as intended. However, then the standardization (standardize=True) can either yield unexpected results, or not, depending on small differences in the original data. 

In one of such cases, the standardization (standardize=True) returns values that maintain the original trend, but are very small, in the order of 10^-17. This behavior of the standardization algorithm is very sensitive to minimal differences in the original data, as if I change just a 4.62 value to 4.61, it succeeds and creates a standardized array with a reasonable value range, I would say masking these very small differences in the original transformed array; while sometimes it doesn't succeed and yields a standardized array with values of ~10^-17, as mentioned above.  
Furthermore, when the standardized data is in the 10^-17 range, even MinMaxScaler can't scale properly these values, which remain in the order of 10^-17.   




I have included a simple array of 10 observations that can reproduce the issue.  

In the example, the initial value of data[0, 0] is 4.61, which superficially doesn't show issues when using `standardize=True`. Here, PowerTransformer returns a range of values from -1.04 to 2.58, which follow the original trend and can be further transformed by the MinMaxScaler to the range (0, 1).    

With minimal differences to the original data, i.e. if data[0, 0] is 4.62, PowerTransformer's standardization returns values in the range of -10^-18 to 0, so extremely close to 0; and MinMaxScaler retains the proportionalities, but doesn't scale them properly.  

With `standardize=False`, the method returns the transformed values, which as mentioned above are very close, down to almost the last significant digit, and MinMaxScaler is unable to scale them.  


### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.preprocessing import PowerTransformer, MinMaxScaler

data = np.array([[4.61],
                 [4.56],
                 [4.59],
                 [4.56],
                 [4.71],
                 [5.63],
                 [4.66],
                 [4.67],
                 [4.70],
                 [4.61]])

for value in range(2):
    # Tests with 4.61 and 4.62
    data[0, 0] += value * 0.01
    print(f""\nWhen data[0, 0] is: {data[0, 0]}"")
    pt = PowerTransformer(standardize=True) # ALSO TRY WITH: `standardize=False`
    data_pt = pt.fit_transform(data)
    print(""\nAfter Powertransformer:"")
    print(data_pt)
    mms = MinMaxScaler(feature_range=(0,1))
    data_mms = mms.fit_transform(data_pt)
    print(""\nAfter MinMaxScaler:"")
    print(data_mms)
```

### Expected Results

Power-transformed data, and scaled data in the range (0, 1)

### Actual Results

```
When data[0, 0] is: 4.61

After Powertransformer:
[[-0.45266018]
 [-1.04111841]
 [-0.67899027]
 [-1.04111841]
 [ 0.40739416]
 [ 2.58016302]
 [ 0.04526602]
 [ 0.09053204]
 [ 0.36212814]
 [-0.45266018]]

After MinMaxScaler:
[[0.1625]
 [0.    ]
 [0.1   ]
 [0.    ]
 [0.4   ]
 [1.    ]
 [0.3   ]
 [0.3125]
 [0.3875]
 [0.1625]]

When data[0, 0] is: 4.62

After Powertransformer:
[[-4.16333634e-17]
 [-1.17961196e-16]
 [-7.63278329e-17]
 [-1.17961196e-16]
 [ 4.16333634e-17]
 [ 2.77555756e-16]
 [ 0.00000000e+00]
 [ 6.93889390e-18]
 [ 4.16333634e-17]
 [-5.55111512e-17]]

After MinMaxScaler:
[[7.63278329e-17]
 [0.00000000e+00]
 [4.16333634e-17]
 [0.00000000e+00]
 [1.59594560e-16]
 [3.95516953e-16]
 [1.17961196e-16]
 [1.24900090e-16]
 [1.59594560e-16]
 [6.24500451e-17]]
```

### Versions

```shell
System:
    python: 3.12.4 | packaged by conda-forge | (main, Jun 17 2024, 10:04:44) [MSC v.1940 64 bit (AMD64)]
executable: C:\Users\Roberto\miniconda3\envs\ml2\python.exe
   machine: Windows-10-10.0.19045-SP0

Python dependencies:
      sklearn: 1.5.1
          pip: 24.1.2
   setuptools: 72.1.0
        numpy: 2.0.1
        scipy: 1.14.0
       Cython: None
       pandas: 2.2.2
   matplotlib: 3.9.1
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libscipy_openblas
       filepath: C:\Users\Roberto\miniconda3\envs\ml2\Lib\site-packages\numpy.libs\libscipy_openblas64_-fb1711452d4d8cee9f276fd1449ee5c7.dll
        version: 0.3.27
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: vcomp
       filepath: C:\Users\Roberto\miniconda3\envs\ml2\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libscipy_openblas
       filepath: C:\Users\Roberto\miniconda3\envs\ml2\Lib\site-packages\scipy.libs\libscipy_openblas-5b1ec8b915dfb81d11cebc0788069d2d.dll
        version: 0.3.27.dev
threading_layer: pthreads
   architecture: Haswell
```
","['Bug']","2024-07-31T10:35:55Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/29595","Logical Bug"
"39","scikit-learn/scikit-learn","BUG Problem when `CalibratedClassifierCV` train contains 2 classes but data contains more","### Describe the bug

In `CalibratedClassifierCV` when a train split contains 2 classes (binary) but the data contains more (>=3) classes, we assume the data is binary:

https://github.com/scikit-learn/scikit-learn/blob/d20e0b9abc4a4798d1fd839db50b19c01723094e/sklearn/calibration.py#L605-L607

and we only end up fitting one calibrator:

https://github.com/scikit-learn/scikit-learn/blob/d20e0b9abc4a4798d1fd839db50b19c01723094e/sklearn/calibration.py#L620-L621

Context: noticed when looking #29545 and trying to update [`test_calibration_less_classes`](https://github.com/scikit-learn/scikit-learn/blob/d20e0b9abc4a4798d1fd839db50b19c01723094e/sklearn/tests/test_calibration.py#L441)

### Steps/Code to Reproduce

```python
import numpy as np

from sklearn.model_selection import KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.calibration import CalibratedClassifierCV

X = np.random.randn(12, 5)
y = [0, 0, 0, 0] + [1, 1, 1, 1] + [2, 2, 2, 2]
clf = DecisionTreeClassifier(random_state=7)
cal_clf = CalibratedClassifierCV(
    clf, method=""sigmoid"", cv=KFold(3), ensemble=True
)
cal_clf.fit(X, y)
for i in range(3):
    print(f'Fold: {i}')
    proba = cal_clf.calibrated_classifiers_[i].predict_proba(X)
    print(proba)
```

### Expected Results

Expect proba to be 0 ONLY for the class not present in the train subset.

### Actual Results

```
Fold: 0  # train contains class 1 and 2, we take the first `pos_class_indices` (1) to be the positive class
[[0. 1. 0.]
 [0. 1. 0.]
 [0. 1. 0.]
 [0. 1. 0.]
 [0. 1. 0.]
 [0. 1. 0.]
 [0. 1. 0.]
 [0. 1. 0.]
 [0. 1. 0.]
 [0. 1. 0.]
 [0. 1. 0.]
 [0. 1. 0.]]
Fold: 1  # train contains class 0 and 2, 0 is the first `pos_class_indices`
[[1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]]
Fold: 2  # train contains class 0 and 1, `0` is the first `pos_class_indices`
[[1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]]
```

A reasonable fix is to check when `CalibratedClassifierCV.classes_` is greater than `estimator.classes_` and output both proba and 1 - proba (assuming we can know which class the estimator deemed to be the positive class).

It does raise the question of whether we should warn when this happens..?


### Versions

```shell
Used main
```
","['Bug']","2024-07-24T06:40:05Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/29551","Logical Bug"
"40","scikit-learn/scikit-learn","decomposition.PCA(svd_solver='covariance_eigh') is less stable with numpy==2.0","### Describe the bug

`decomposition.PCA(svd_solver='covariance_eigh')` is less stable with numpy==2.0

I noticed this issue as some tests started failing at the downstream [dask-ml/#997](https://github.com/dask/dask-ml/pull/997)

For a certain data input, `pca.transform` gives incredibly large values, which are not seen with numpy==1.24.3

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn import datasets, decomposition
X = datasets.make_low_rank_matrix(1000, 10, effective_rank=2, random_state=0, tail_strength=0)
pca = decomposition.PCA(n_components=None, whiten=True, svd_solver='auto').fit(X)
np.max(np.abs(pca.transform(X)))
```


### Expected Results

```python
>>> 3.9065841446726326  # with numpy==1.24.3
```


### Actual Results

```python
>>> np.float64(874957.7078303652)  # with numpy==2.0
```

Both uses sklearn==1.5.1, so probably an upstream issue.

Indeed, the singular values from numpy==2.0 contains zero
```python
pca.singular_values_
# with numpy==1.24.3
>>> array([9.99821683e-01, 7.78610978e-01, 3.67623087e-01, 1.05238902e-01,
       1.83047062e-02, 1.92838400e-03, 1.23400336e-04, 4.77881459e-06,
       1.12514225e-07, 2.93106569e-09])
# with numpy==2.0
>>> array([9.99821683e-01, 7.78610978e-01, 3.67623087e-01, 1.05238902e-01,
       1.83047062e-02, 1.92838400e-03, 1.23400336e-04, 4.77880939e-06,
       1.12582366e-07, 0.00000000e+00])
```
Probably this zero makes something wrong?

### Versions

```shell
### numpy==2.0 configuration

In [10]: sklearn.show_versions()

System:
    python: 3.12.4 | packaged by Anaconda, Inc. | (main, Jun 18 2024, 15:03:56) [MSC v.1929 64 bit (AMD64)]
executable: C:\Users\oqf\AppData\Local\anaconda3\envs\numpy2\python.exe
   machine: Windows-10-10.0.19045-SP0

Python dependencies:
      sklearn: 1.5.1
          pip: 24.0
   setuptools: 69.5.1
        numpy: 2.0.0
        scipy: 1.14.0
       Cython: None
       pandas: 2.2.2
   matplotlib: None
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
    num_threads: 12
         prefix: vcomp
       filepath: C:\Users\oqf\AppData\Local\anaconda3\envs\numpy2\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 12
         prefix: libscipy_openblas
       filepath: C:\Users\oqf\AppData\Local\anaconda3\envs\numpy2\Lib\site-packages\numpy.libs\libscipy_openblas64_-fb1711452d4d8cee9f276fd1449ee5c7.dll    
        version: 0.3.27
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 12
         prefix: libscipy_openblas
       filepath: C:\Users\oqf\AppData\Local\anaconda3\envs\numpy2\Lib\site-packages\scipy.libs\libscipy_openblas-5b1ec8b915dfb81d11cebc0788069d2d.dll       
        version: 0.3.27.dev
threading_layer: pthreads
   architecture: Haswell


### numpy==1.24.3 configuration
```python
System:
    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\oqf\AppData\Local\anaconda3\python.exe
   machine: Windows-10-10.0.19045-SP0

Python dependencies:
      sklearn: 1.5.1
          pip: 23.2.1
   setuptools: 68.0.0
        numpy: 1.24.3
        scipy: 1.11.1
       Cython: 3.0.10
       pandas: 2.0.3
   matplotlib: 3.7.2
       joblib: 1.2.0
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: mkl
    num_threads: 10
         prefix: mkl_rt
       filepath: C:\Users\oqf\AppData\Local\anaconda3\Library\bin\mkl_rt.2.dll
        version: 2023.1-Product
threading_layer: intel

       user_api: openmp
   internal_api: openmp
    num_threads: 12
         prefix: vcomp
       filepath: C:\Users\oqf\AppData\Local\anaconda3\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None

       user_api: openmp
   internal_api: openmp
    num_threads: 12
         prefix: libiomp
       filepath: C:\Users\oqf\AppData\Local\anaconda3\Library\bin\libiomp5md.dll
        version: None
```
```
","['Bug']","2024-07-22T00:40:46Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/29534","Logical Bug"
"41","scikit-learn/scikit-learn","RFE results are inconsistent between machines with ties in feature importances at threshold","### Describe the bug

RFE uses np.argsort on the feature_importances from the estimator, this is not repeatable across machines. This only matters when there are ties in the feature importances that overlap with the threshold. For example if the importances are [0, 2, 0, 1] and it needs to eliminate 1 feature, it may not choose the same feature to eliminate on all machines.

### Steps/Code to Reproduce

It is difficult to write code that will show this as it requires multiple machines to test; however, the following test could be added to the _rfe unit tests to check if the sort is stable, though the sort could be consistent but not stable, so this isn't enough to prove an issue.
```
def test_rfe_ties_around_threshold():
    X, y = make_classification(n_features=47, random_state=0)
    clf = MockClassifier() # mock classifier returns constant feature_importances
    rfe = RFE(estimator=clf, n_features_to_select=4, step=2)
    rfe.fit(X, y)

    assert_array_equal(rfe.support_ ,np.array([*[False]*43, *[True]*4]))
```

### Expected Results

Expected to get the same results on all machines, being stable is not necessary though it is an easy way to enforce testable consistency.

### Actual Results

Current sort does not result in a stable sort and fails the previously provided unit test.

### Versions

```shell
main branch
```


This issue is most likely to occur with zero importance features, especially at the initial steps of RFE.","['Bug']","2024-07-21T20:29:03Z","14","0","https://github.com/scikit-learn/scikit-learn/issues/29531","Runtime Error"
"42","scikit-learn/scikit-learn","NDCG in case of abscence of relevant items","### Describe the bug

In `sklearn.metrics._ndcg_sample_scores`, there is a counterintuitive handling of the case where all true relevances are equal to zero for some samples. In this case, DCG = 0, IDCG = 0, and the whole NDCG is not defined. In `sklearn` implementation it is defined as 0 and included in the averaged NDCG calculation.  The least leads to strange effects, like `ndcg_score(y,y) != 1`; moreover, it affects the metric value in non-trivial cases too.

In the original 2002 paper where NDCG is proposed, it is not stated how to handle such situations, but it is clearly mentioned that 
```
The (D)CG vectors for each IR technique can be normalized by dividing them
by the corresponding ideal (D)CG vectors, component by component. In this way,
for any vector position, the normalized value 1 represents ideal performance,
and values in the range [0, 1) the share of ideal performance cumulated by each
technique.
```
meaning that NDCG(y,y) must always be 1. 


I suggest excluding observations without relevant items and/or throwing a warning.

### Steps/Code to Reproduce

```
>>> from sklearn.metrics import ndcg_score
>>> y = np.array([[1.0, 0.0, 1.0], [0.0, 0.0, 0.0]])
>>> ndcg_score(y, y)
```


### Expected Results

1.

### Actual Results

0.5

### Versions

```shell
This code was not changed in 1.5, so I guess for newer versions the issue also is actual.



System:
    python: 3.11.8 (main, Feb 12 2024, 14:50:05) [GCC 13.2.1 20230801]
executable: /usr/bin/python3
   machine: Linux-6.6.19-1-MANJARO-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.3.1
          pip: 24.0
   setuptools: 69.0.3
        numpy: 1.26.4
        scipy: 1.10.1
       Cython: 3.0.9
       pandas: 1.5.3
   matplotlib: 3.7.1
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/arabella/.local/lib/python3.11/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/arabella/.local/lib/python3.11/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: Haswell
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/arabella/.local/lib/python3.11/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so
        version: 0.3.18
threading_layer: pthreads
   architecture: Haswell
    num_threads: 8
```
","['Bug', 'help wanted']","2024-07-19T01:23:38Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/29521","Performance Issue"
"43","scikit-learn/scikit-learn","GroupKFold inconsistent under ties in group sizes.","### Describe the bug

Due to the use of argsort (a non stable sort withthout the stable parameter introduced in numpy 2.0), GroupKFold is not always reproducible when there are ties in group sizes.

### Steps/Code to Reproduce

You may need to run this on different machines, but you can reproduce this issue with even less code than GroupKFold by simply testing `np.argsort`.
```
import numpy as np

x = np.array([0.37454012, 0.95071431, 0.73199394, 0.        , 0.15601864,
       0.        , 0.05808361, 0.86617615, 0.60111501, 0.70807258,
       0.02058449, 0.96990985, 0.83244264, 0.21233911, 0.18182497,
       0.18340451, 0.30424224, 0.52475643, 0.43194502, 0.29122914,
       0.61185289, 0.13949386, 0.29214465, 0.36636184, 0.45606998,
       0.78517596, 0.19967378, 0.51423444, 0.59241457, 0.        ,
       0.60754485, 0.17052412, 0.06505159, 0.        , 0.96563203,
       0.80839735, 0.        , 0.        , 0.68423303, 0.44015249,
       0.12203823])

print(np.argsort(x))
```
Both times with numpy version: 1.26.4
machine a: `array(3,5,29,33,36,37,10,6,32,40...`
machine b: `array(37,36,3,29,5,33,10,6,32,40...`

### Expected Results

Technically a stable sort isn't required, no need to maintain the original ordering. However a consistent sort result is required for reproducibility.  Stable sort is an easy way to achieve this that is already built into numpy.

### Actual Results

Both times with numpy version: 1.26.4
machine a: `array(3,5,29,33,36,37,10,6,32,40...`
machine b: `array(37,36,3,29,5,33,10,6,32,40...`

### Versions

```shell
sklearn versions are mostly irrelevant here as the issue is with calling a numpy method that requires additional changes.
```
","['Bug']","2024-07-15T19:59:57Z","7","1","https://github.com/scikit-learn/scikit-learn/issues/29495","Logical Bug"
"44","scikit-learn/scikit-learn","KernelDensity(bandwidth='silverman') doesn't throw proper error for 1d X","Essentially the bandwidth estimation codepath is not covered in the common tests, but it should be :)","['Bug']","2024-07-10T01:32:02Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/29443","Syntax Error"
"45","scikit-learn/scikit-learn","kernel_approximation.Nystroem with precomputed kernel","### Describe the bug

I am trying to get a Nystroem approximation of a pre computed kernel but it throws an error if I use n_components anything less than the number of datapoints. Unless my understanding is wrong, does this not defeat the point of the approximation? Please advise, code below:



I have come from this resolved issue https://github.com/scikit-learn/scikit-learn/issues/14641 

### Steps/Code to Reproduce

```python
from sklearn.svm import SVC
from sklearn.kernel_approximation import Nystroem
 
# data shape (3000,50)
# kernel matrix shape (3000,3000)
clf = SVC()
feature_map_nystroem = Nystroem(
    kernel = 'precomputed',
    random_state=1,
    n_components=300
)
kernel_transformed = feature_map_nystroem.fit_transform(kernel)
clf.fit(kernel_transformed, y)
```



### Expected Results

I expect this to work. 

### Actual Results

Instead it gives error:

```python
in check_pairwise_arrays(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)
    153 if precomputed:
    154     if X.shape[1] != Y.shape[0]:
--> 155 raise ValueError(""Precomputed metric requires shape ""

    156                 ""(n_queries, n_indexed). Got (%d, %d) ""
    157                          ""for %d indexed."" %
    158                          (X.shape[0], X.shape[1], Y.shape[0]))
    159 elif X.shape[1] != Y.shape[1]:
    160     raise ValueError(""Incompatible dimension for X and Y matrices: ""
    161                      ""X.shape[1] == %d while Y.shape[1] == %d"" % (
    162                          X.shape[1], Y.shape[1]))
 
ValueError: Precomputed metric requires shape (n_queries, n_indexed). Got (300, 3000) for 300 indexed.
```

### Versions

```shell
System:
    python: 3.8.19 (default, Mar 20 2024, 19:58:24)  [GCC 11.2.0]
executable: /opt/conda/miniconda3/envs/python3.8/bin/python
   machine: Linux-6.1.0-21-cloud-amd64-x86_64-with-glibc2.17
 
Python dependencies:
          pip: 24.0
   setuptools: 63.1.0
      sklearn: 0.24.2
        numpy: 1.21.6
        scipy: 1.10.1
       Cython: 3.0.10
       pandas: 1.2.5
   matplotlib: 3.4.3
       joblib: 1.4.2
threadpoolctl: 3.5.0
```
","['Bug']","2024-06-26T15:50:45Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/29353","Syntax Error"
"46","scikit-learn/scikit-learn","`SimpleImputer` Fails with pyarrow String Types in sklearn","### Describe the bug

When using SimpleImputer from sklearn with pyarrow string types, the imputer fails with an error. This issue occurs when attempting to impute missing values in a DataFrame containing pyarrow string columns.

### Steps/Code to Reproduce

```python
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer

# Create a DataFrame with pyarrow string types
data = {
    'mpg': [18, 25, 120, 120],
    'make': ['Ford', 'Chevy', np.nan, 'Tesla']
}
df = (pd.DataFrame(data)
       .astype({'make':'string[pyarrow]'})
)

# Initialize SimpleImputer
imputer = SimpleImputer(strategy='most_frequent')

# Attempt to fit and transform the DataFrame
imputer.fit_transform(df[[""make""]])
```


### Expected Results

The SimpleImputer should handle pyarrow string types and impute the missing values without raising an error.

### Actual Results

```
AttributeError                            Traceback (most recent call last)
Cell In[158], line 18
     15 imputer = SimpleImputer(strategy='most_frequent')
     17 # Attempt to fit and transform the DataFrame
---> 18 imputer.fit_transform(df[[\""make\""]])

File ~/.envs/menv/lib/python3.10/site-packages/sklearn/utils/_set_output.py:295, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)
    293 @wraps(f)
    294 def wrapped(self, X, *args, **kwargs):
--> 295     data_to_wrap = f(self, X, *args, **kwargs)
    296     if isinstance(data_to_wrap, tuple):
    297         # only wrap the first output for cross decomposition
    298         return_tuple = (
    299             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    300             *data_to_wrap[1:],
    301         )

File ~/.envs/menv/lib/python3.10/site-packages/sklearn/base.py:1098, in TransformerMixin.fit_transform(self, X, y, **fit_params)
   1083         warnings.warn(
   1084             (
   1085                 f\""This object ({self.__class__.__name__}) has a `transform`\""
   (...)
   1093             UserWarning,
   1094         )
   1096 if y is None:
   1097     # fit method of arity 1 (unsupervised transformation)
-> 1098     return self.fit(X, **fit_params).transform(X)
   1099 else:
   1100     # fit method of arity 2 (supervised transformation)
   1101     return self.fit(X, y, **fit_params).transform(X)

File ~/.envs/menv/lib/python3.10/site-packages/sklearn/base.py:1474, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1467     estimator._validate_params()
   1469 with config_context(
   1470     skip_parameter_validation=(
   1471         prefer_skip_nested_validation or global_skip_validation
   1472     )
   1473 ):
-> 1474     return fit_method(estimator, *args, **kwargs)

File ~/.envs/menv/lib/python3.10/site-packages/sklearn/impute/_base.py:427, in SimpleImputer.fit(self, X, y)
    423     self.statistics_ = self._sparse_fit(
    424         X, self.strategy, self.missing_values, fill_value
    425     )
    426 else:
--> 427     self.statistics_ = self._dense_fit(
    428         X, self.strategy, self.missing_values, fill_value
    429     )
    431 return self

File ~/.envs/menv/lib/python3.10/site-packages/sklearn/impute/_base.py:510, in SimpleImputer._dense_fit(self, X, strategy, missing_values, fill_value)
    503 elif strategy == \""most_frequent\"":
    504     # Avoid use of scipy.stats.mstats.mode due to the required
    505     # additional overhead and slow benchmarking performance.
    506     # See Issue 14325 and PR 14399 for full discussion.
    507 
    508     # To be able access the elements by columns
    509     X = X.transpose()
--> 510     mask = missing_mask.transpose()
    512     if X.dtype.kind == \""O\"":
    513         most_frequent = np.empty(X.shape[0], dtype=object)

AttributeError: 'bool' object has no attribute 'transpose'""
```

### Versions

```shell
System:
    python: 3.10.14 (main, Mar 19 2024, 21:46:16) [Clang 15.0.0 (clang-1500.3.9.4)]
executable: /Users/matt/.envs/menv/bin/python
   machine: macOS-14.2.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.4.1.post1
          pip: 24.0
   setuptools: 67.6.1
        numpy: 1.23.5
        scipy: 1.10.0
       Cython: 0.29.37
       pandas: 2.2.2
   matplotlib: 3.6.2
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/matt/.envs/menv/lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.20
threading_layer: pthreads
   architecture: armv8
    num_threads: 8

       user_api: openmp
   internal_api: openmp
         prefix: libomp
       filepath: /Users/matt/.envs/menv/lib/python3.10/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/matt/.envs/menv/lib/python3.10/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.18
threading_layer: pthreads
   architecture: armv8
    num_threads: 8
```
","['Bug', 'New Feature', 'Pandas compatibility']","2024-06-19T00:03:26Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/29291","Syntax Error"
"47","scikit-learn/scikit-learn","`_BaseEncoder` with boolean `categories_` that include `nan` fails on `transform` when `X` is boolean","### Describe the bug

An `Encoder` that was fitted on a `DataFrame` with boolean columns that include `NaN` will fail when transforming a boolean `X` due to a mismatch in the `dtype`s when calling `_check_unknown`. Since `X` has no `object` `dtype`, there is an attempt to call `np.isnan(known_values)`, which fails because `known_values` _does_ have an `object` `dtype`.

As far as I can tell, this can be fixed by casting the `dtype` of `values` in [`_check_unknown`](https://github.com/scikit-learn/scikit-learn/blob/7e8ad632ff/sklearn/utils/_encode.py#L243) to the `dtype` of `known_values`:
```python
if values.dtype != known_values.dtype:
     values = values.astype(known_values.dtype)
```

### Steps/Code to Reproduce

```python
import pandas as pd

from sklearn.preprocessing import OrdinalEncoder

x = pd.DataFrame({'a': [True, False, np.nan]})
o = OrdinalEncoder()
o.fit_transform(x)

y = pd.DataFrame({'a': [True, True, False]})
o.transform(y)
```

### Expected Results

I expect the array to be transformed according to the known classes:
```python
array([[1.],
       [1.],
       [0.]])
```

### Actual Results

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[1], line 10
      7 o.fit_transform(x)
      9 y = pd.DataFrame({'a': [True, True, False]})
---> 10 o.transform(y)

File ~/miniconda3/envs/analytics-models-v2/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295, in _wrap_method_output.<locals>.wrapped(self, X, *args, **kwargs)
    293 @wraps(f)
    294 def wrapped(self, X, *args, **kwargs):
--> 295     data_to_wrap = f(self, X, *args, **kwargs)
    296     if isinstance(data_to_wrap, tuple):
    297         # only wrap the first output for cross decomposition
    298         return_tuple = (
    299             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    300             *data_to_wrap[1:],
    301         )

File ~/miniconda3/envs/analytics-models-v2/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:1578, in OrdinalEncoder.transform(self, X)
   1564 """"""
   1565 Transform X to ordinal codes.
   1566 
   (...)
   1575     Transformed input.
   1576 """"""
   1577 check_is_fitted(self, ""categories_"")
-> 1578 X_int, X_mask = self._transform(
   1579     X,
   1580     handle_unknown=self.handle_unknown,
   1581     force_all_finite=""allow-nan"",
   1582     ignore_category_indices=self._missing_indices,
   1583 )
   1584 X_trans = X_int.astype(self.dtype, copy=False)
   1586 for cat_idx, missing_idx in self._missing_indices.items():

File ~/miniconda3/envs/analytics-models-v2/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:206, in _BaseEncoder._transform(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)
    204 Xi = X_list[i]
    205 breakpoint()
--> 206 diff, valid_mask = _check_unknown(Xi, self.categories_[i], return_mask=True)
    208 if not np.all(valid_mask):
    209     if handle_unknown == ""error"":

File ~/miniconda3/envs/analytics-models-v2/lib/python3.11/site-packages/sklearn/utils/_encode.py:307, in _check_unknown(values, known_values, return_mask)
    304         valid_mask = np.ones(len(values), dtype=bool)
    306 # check for nans in the known_values
--> 307 if np.isnan(known_values).any():
    308     diff_is_nan = np.isnan(diff)
    309     if diff_is_nan.any():
    310         # removes nan from valid_mask

TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```

### Versions

```shell
System:
    python: 3.11.8 (main, Feb 26 2024, 15:43:17) [Clang 14.0.6 ]
executable: ~/miniconda3/envs/analytics-models-v2/bin/python
   machine: macOS-10.16-x86_64-i386-64bit

Python dependencies:
      sklearn: 1.4.1.post1
          pip: 23.3.1
   setuptools: 68.2.2
        numpy: 1.26.4
        scipy: 1.13.0
       Cython: None
       pandas: 2.1.4
   matplotlib: 3.8.4
       joblib: 1.4.0
threadpoolctl: 3.4.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 6
         prefix: libopenblas
       filepath: ~/miniconda3/envs/analytics-models-v2/lib/libopenblasp-r0.3.21.dylib
        version: 0.3.21
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 12
         prefix: libomp
       filepath: ~/miniconda3/envs/analytics-models-v2/lib/python3.11/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```
","['Bug']","2024-06-11T21:02:29Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/29241","Runtime Error"
"48","scikit-learn/scikit-learn","classification_report with output_dict=True leads to brittle output","### Describe the workflow you want to enable

When `output_dict` is `True`, the returned `dict` structure is brittle and breaks if one of the class name is the same as one of the average metrics.  
Here for example one of the class is named ""accuracy"" so that it doesn't appear in the returned `dict` .

``` python
from sklearn.metrics import classification_report
from pprint import pprint

classification_report(
    [""chat"", ""accuracy""],
    [""chat"", ""accuracy""],
    output_dict=True,
)
```

``` python
{'accuracy': 1.0,
 'chat': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},
 'macro avg': {'f1-score': 1.0,
               'precision': 1.0,
               'recall': 1.0,
               'support': 2.0},
 'weighted avg': {'f1-score': 1.0,
                  'precision': 1.0,
                  'recall': 1.0,
                  'support': 2.0}}
```

### Describe your proposed solution

Any unambiguous output structure, such as separating between class-wise and average metrics:
``` python
{
    'class': {
        'accuracy': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},
        'chat': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},
    },
    'average': {
        'accuracy': 1.0,
        'macro avg': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},
        'weighted avg': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},
    },
}
```

Or:
``` python
{
    'class': {
        'accuracy': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},
        'chat': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},
    },
    'accuracy': 1.0,
    'macro avg': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},
    'weighted avg': {'f1-score': 1.0, 'precision': 1.0, 'recall': 1.0, 'support': 1.0},
}
```

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_","['Bug']","2024-06-06T12:11:50Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/29205","Logical Bug"
"49","scikit-learn/scikit-learn","FIX TunedThresholdClassifierCV error or warn with informative message on invalid metrics","This PR fixes two usability problems with the new `TunedThresholdClassifierCV` when using it with invalid values for the `scoring` parameter:

- the first case, is passing a scoring name or scorer object that expects metrics defined for unthresholded predictions (e.g. ROC-AUC). This is clearly invalid and we can raise a `ValueError` with a meaning error message.
- the second case is passing an under-specified scoring function that would return a constant prediction on a given dataset. In this case I chose to warn the user but leave the dummy threshold value that results from this case.

For the second point we could instead warn the user and keep on using 0.5 as the threshold which is probably less pathological/arbitrary.

Alternatively we could also raise a `ValueError` but I am worried that this error could be triggered for bad reasons when doing a grid search or an other resampling procedure around the `TunedThresholdClassifierCV` instance, hence I thought that a warning would be less disruptive.

/cc @glemaitre @lorentzenchr ","['Bug', 'module:model_selection', 'To backport']","2024-05-22T16:57:58Z","20","0","https://github.com/scikit-learn/scikit-learn/pull/29082","Syntax Error"
"50","scikit-learn/scikit-learn","MultiOutputClassifier does not rely on estimator to provide pairwise tag","### Describe the bug

I use the `MultiOutputClassifier` function to make `SVC` multilabel. 

Then, if I use the linear or rbf kernel the cross_validation function works perfectly fine.

However, when I use `SVC` with precomputed kernel is having an `ValueError: Precomputed matrix must be a square matrix`. 

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import pairwise_distances
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import cross_val_score, cross_validate

svm = SVC(kernel='precomputed', C=100, random_state=42)
multilabel_classifier = MultiOutputClassifier(svm, n_jobs=-1)

X = np.random.rand(1000, 1000)
y = np.random.randint(0, 2, size=(1000, 6))

kernel_eucl = pairwise_distances(X, metric='euclidean')

cross_validate(
    multilabel_classifier, kernel_eucl, y, cv=10, scoring='f1_weighted', n_jobs=-1
)
```

### Expected Results

An weighted f1-score.

### Actual Results

```pytb
ValueError: 
All the 10 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
10 fits failed with the following error:

 File ""C:\Users\bscuser\anaconda3\Lib\site-packages\sklearn\svm\_base.py"", line 217, in fit
    raise ValueError(
ValueError: Precomputed matrix must be a square matrix. Input is a 900x1000 matrix.
```

### Versions

```shell
System:
    python: 3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\bscuser\anaconda3\python.exe
   machine: Windows-10-10.0.19045-SP0

Python dependencies:
      sklearn: 1.2.2
          pip: 23.3.1
   setuptools: 68.2.2
        numpy: 1.26.4
        scipy: 1.11.4
       Cython: None
       pandas: 2.1.4
   matplotlib: 3.8.0
       joblib: 1.2.0
threadpoolctl: 2.2.0

Built with OpenMP: True

threadpoolctl info:
       filepath: C:\Users\bscuser\anaconda3\Library\bin\mkl_rt.2.dll
         prefix: mkl_rt
       user_api: blas
   internal_api: mkl
        version: 2023.1-Product
    num_threads: 4
threading_layer: intel

       filepath: C:\Users\bscuser\anaconda3\vcomp140.dll
         prefix: vcomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 4

       filepath: C:\Users\bscuser\anaconda3\Library\bin\libiomp5md.dll
         prefix: libiomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 4
```
","['Bug']","2024-05-14T10:55:28Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/29016","Syntax Error"
"51","scikit-learn/scikit-learn","Yeo-Johnson inverse_transform fails silently on extreme skew data","### Describe the bug

The Yeo-Johnson is not a surjective transformation for negative lambdas. Therefore, the inverse transformation returns `np.nan` when inverse transforming values outside the range of the transform. This failure is silent, so it took me quite a while of debugging to understand this behavior.

The problematic lines are

https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/preprocessing/_data.py#L3390

and 

https://github.com/scikit-learn/scikit-learn/blob/8721245511de2f225ff5f9aa5f5fadce663cd4a3/sklearn/preprocessing/_data.py#L3386

in which we might compute `np.power(something_negative, not_integral_value)`, which of course returns `np.nan` as per https://numpy.org/doc/stable/reference/generated/numpy.power.html

### Steps/Code to Reproduce

To reproduce for positive values (there is a similar problem for negative values):

```python
import numpy as np
import sklearn.preprocessing
trans = sklearn.preprocessing.PowerTransformer(method='yeo-johnson')
x = np.array([1,1,1e10]).reshape(-1, 1) # extreme skew
trans.fit(x)
lmbda = trans.lambdas_[0] 
print(lmbda)
assert lmbda < 0 # == -0.096 negative value

# any value `psi` for which lambda*psi+1 <= 0 will result in nan due to lacking support, since the forwards transformation 
# is not surjective on negative lambdas. In this specific case, 10*-0.096 < 1
psi = np.array([10]).reshape(-1, 1)
x = trans.inverse_transform(psi).item()
print(x)
assert np.isnan(x)
```

### Expected Results

 The code should either:

1) validate its inputs and raise an exception
2) validate its inputs and raise a warning
3) fail silently, but have it documented behavior

### Actual Results

It just prints

```
-0.0962322261004418
nan
```

### Versions

```shell
System:
    python: 3.11.3 (main, Jan 18 2024, 19:07:12) [Clang 18.0.0 (https://github.com/llvm/llvm-project 75501f53624de92aafce2f1da698
executable: /home/pyodide/this.program
   machine: Emscripten-3.1.46-wasm32-32bit

Python dependencies:
      sklearn: 1.3.1
          pip: None
   setuptools: None
        numpy: 1.26.1
        scipy: 1.11.2
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.3.2
threadpoolctl: 3.2.0

Built with OpenMP: False
```
","['Bug']","2024-05-04T08:23:07Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/28946","Syntax Error"
"52","scikit-learn/scikit-learn","BUG internal indexing tools trigger error with pandas < 2.0.0","[#28375](https://github.com/scikit-learn/scikit-learn/pull/28375#issuecomment-2088926826) triggers errors for pandas < 2.0.0, despite just using scikit-learn internal functionalities.

As documented in https://scikit-learn.org/dev/install.html, we have pandas >= 1.1.3.","['Bug', 'Pandas compatibility']","2024-05-02T09:58:49Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/28931","Syntax Error"
"53","scikit-learn/scikit-learn","`FunctionTransformer` need `feature_names_out` even if `func` returns DataFrame","### Describe the bug

Trying to call `transform` for `FunctionTransformer` for which `feature_names_out` is configured raises error that advises to use `set_output(transform='pandas')`. But this doesn't change anything.

### Steps/Code to Reproduce

```py
import numpy as np
import pandas as pd
from sklearn.preprocessing import FunctionTransformer

my_transformer = FunctionTransformer(
    lambda X : pd.concat(
        [
            X[col].rename(f""{col} {str(power)}"")**power
            for col in X
            for power in range(2,4)
        ],
        axis=1
    ),
    feature_names_out = (
        lambda transformer, input_features: [
            f""{feature} {power_str}""
            for feature in input_features
            for power_str in [""square"", ""cubic""]
        ]
    )
)
# I specified transform=pandas
my_transformer.set_output(transform='pandas')
sample_size = 10
X = pd.DataFrame({
    ""feature 1"" : [1,2,3,4,5],
    ""feature 2"" : [3,4,5,6,7]
})
my_transformer.fit(X)
my_transformer.transform(X)
```

### Expected Results

`pandas.DataFrame` like following

|    |   feature 1 square |   feature 1 cubic |   feature 2 square |   feature 2 cubic |
|---:|-------------------:|------------------:|-------------------:|------------------:|
|  0 |                  1 |                 1 |                  9 |                27 |
|  1 |                  4 |                 8 |                 16 |                64 |
|  2 |                  9 |                27 |                 25 |               125 |
|  3 |                 16 |                84 |                 36 |               216 |
|  4 |                 25 |               125 |                 49 |               343 |

### Actual Results

```
ValueError: The output generated by `func` have different column names than the ones provided by `get_feature_names_out`. Got output with columns names: ['feature 1 2', 'feature 1 3', 'feature 2 2', 'feature 2 3'] and `get_feature_names_out` returned: ['feature 1 square', 'feature 1 cubic', 'feature 2 square', 'feature 2 cubic']. The column names can be overridden by setting `set_output(transform='pandas')` or `set_output(transform='polars')` such that the column names are set to the names provided by `get_feature_names_out`.
```

### Versions

```shell
System:
    python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]
executable: /usr/bin/python3
   machine: Linux-6.5.0-14-generic-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.4.1.post1
          pip: 24.0
   setuptools: 68.2.2
        numpy: 1.24.2
        scipy: 1.11.1
       Cython: None
       pandas: 2.2.1
   matplotlib: 3.7.1
       joblib: 1.3.1
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/fedor/.local/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
        version: 0.3.21
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12

       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/fedor/.local/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
    num_threads: 12

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/fedor/.local/lib/python3.10/site-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12
```
```
","['Bug']","2024-04-06T10:17:51Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/28780","Syntax Error"
"54","scikit-learn/scikit-learn","Missing _ZdlPv symbol in _argkmin_classmode for manylinux wheels produced by meson","The current work-around is to use `-fno-sized-deallocation` see https://github.com/scikit-learn/scikit-learn/pull/28506#discussion_r1512897297 for more details.

This can be reproduced locally with cibuildwheel.
```
python -m cibuildwheel --only cp312-manylinux_x86_64
```
will produced a manylinux wheel is in the wheelhouse folder which you can install through something like this:
```
pip install wheelhouse/scikit_learn-1.5.dev0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
```

Traceback from build log:
```
 python -c 'import sklearn.metrics'
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/__init__.py"", line 7, in <module>
    from . import cluster
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/cluster/__init__.py"", line 25, in <module>
    from ._unsupervised import (
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/cluster/_unsupervised.py"", line 23, in <module>
    from ..pairwise import _VALID_METRICS, pairwise_distances, pairwise_distances_chunked
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/pairwise.py"", line 43, in <module>
    from ._pairwise_distances_reduction import ArgKmin
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/__init__.py"", line 94, in <module>
    from ._dispatcher import (
  File ""/home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py"", line 17, in <module>
    from ._argkmin_classmode import (
ImportError: /home/lesteve/micromamba/envs/del/lib/python3.12/site-packages/sklearn/metrics/_pairwise_distances_reduction/_argkmin_classmode.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZdlPv
```
","['Bug', 'Build / CI']","2024-03-08T09:24:56Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/28596","Dependency Issue"
"55","scikit-learn/scikit-learn","Inaccurate Attribute Listing with dir(obj) for Classes Using available_if Conditional Method Decorator","### Describe the bug

When utilizing the `available_if` decorator from SciKit Learn to conditionally expose methods based on specific object state or conditions, we observe that the `dir(obj)` function may return inaccurate results. Specifically, `dir(obj)` continues to list methods that should be conditionally hidden based on the `available_if` decorator's logic. This discrepancy arises because the `__dir__` method on the affected classes does not dynamically account for this conditional availability. As a result, users and consuming code may be misled about the actual methods available for use on instances of the class at runtime, potentially leading to unexpected `AttributeErrors` when accessing supposedly available methods.

### Steps/Code to Reproduce

I will test with the SVC, but it can apply to other classes.

```python
from sklearn.svm import SVC
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)

model = SVC(probability=False)
model.fit(X[:100], y[:100])

# Check if 'predict_proba' is listed by dir()
print(""'predict_proba' in dir(model):"", ""predict_proba"" in dir(model))

# Attempt to call 'predict_proba'
try:
    prob_predictions = model.predict_proba(X[:2])
    print(""Predict_proba called successfully."")
except AttributeError as e:
    print(""Attempting to call 'predict_proba' raised an AttributeError:"", e)

```

### Expected Results

It should print out the following:
```
'predict_proba' in dir(model): False
Attempting to call 'predict_proba' raised an AttributeError: predict_proba is not available when probability=False
```

Methods decorated with `available_if` and whose conditions raise an `AttributeError` should not appear in the list returned by `dir(obj)`.

### Actual Results

It should print out the following:
```
'predict_proba' in dir(model): True
Attempting to call 'predict_proba' raised an AttributeError: predict_proba is not available when probability=False
```

Methods decorated with `available_if` and whose conditions raise an `AttributeError` appears in the list returned by `dir(obj)`.

Traceback when you show the error itself (comes from the check in `available_if`.
```
Traceback (most recent call last):
  File ""/Users/kmcgrady/Projects/test-scripts/xg_example.py"", line 48, in <module>
    prob_predictions = model.predict_proba(X[:2])
  File ""/Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/lib/python3.10/site-packages/sklearn/utils/_available_if.py"", line 31, in __get__
    if not self.check(obj):
  File ""/Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/lib/python3.10/site-packages/sklearn/svm/_base.py"", line 827, in _check_proba
    raise AttributeError(
AttributeError: predict_proba is not available when probability=False
```

### Versions

```shell
System:
    python: 3.10.10 (main, May 13 2023, 16:09:51) [Clang 14.0.3 (clang-1403.0.22.14.1)]
executable: /Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/bin/python
   machine: macOS-14.3.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.3.2
          pip: 23.3.1
   setuptools: 68.2.2
        numpy: 1.26.4
        scipy: 1.11.4
       Cython: None
       pandas: 2.2.1
   matplotlib: 3.8.2
       joblib: 1.3.2
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
    num_threads: 10
         prefix: libomp
       filepath: /Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/lib/python3.10/site-packages/sklearn/.dylibs/libomp.dylib
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 10
         prefix: libopenblas
       filepath: /Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: blas
   internal_api: openblas
    num_threads: 10
         prefix: libopenblas
       filepath: /Users/kmcgrady/.local/share/virtualenvs/test-scripts-fLg6VU9n/lib/python3.10/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: armv8
```
","['Bug']","2024-03-01T07:06:00Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/28558","Syntax Error"
"56","scikit-learn/scikit-learn","Two different versions for weighted lorenz curve calculation in the examples","### Describe the issue linked to the documentation

There are 2 definitions of (weighted) `lorenz_curve()` functions [here](https://scikit-learn.org/stable/auto_examples/linear_model/plot_tweedie_regression_insurance_claims.html) and [here](https://scikit-learn.org/stable/auto_examples/linear_model/plot_poisson_regression_non_normal_loss.html)

The difference is in the X coordinates that these functions returns. Both return X coordinates between 0 and 1, but the first example returns **equally spaced** X coordinates:
```python
cumulated_samples = np.linspace(0, 1, len(cumulated_claim_amount))
```
and the second example return **un-equally spaced** X coordinates (spaced using the samples weights):
```python
cumulated_exposure = np.cumsum(ranked_exposure)
cumulated_exposure /= cumulated_exposure[-1]
```

### Suggest a potential alternative/fix

_No response_","['Bug', 'Documentation']","2024-02-26T06:29:54Z","8","0","https://github.com/scikit-learn/scikit-learn/issues/28534","Logical Bug"
"57","scikit-learn/scikit-learn","Improve ""polars"" integration (error, warning & linting examples)","### Describe the workflow you want to enable

using polars data (DataFrame, Series) is already supported in many places which is awesome, thank you!!

But in many places there are still
- errors / crashes -> required conversion to numpy/pandas
- warnings -> requires conversion to numpy/pandas
- linting/type problems -> requires updates to typing signalture?

# Examples

## Code

```python
import pandas as pd
import polars as pl
from sklearn.datasets import make_classification
from sklearn.inspection import permutation_importance
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, cross_validate

X, y = make_classification()

# X = pd.DataFrame(X)
# y = pd.Series(y)

# X = pl.DataFrame(X)
# y = pl.Series(y)

clf = LogisticRegression()

clf.fit(
    X=X,
    y=y,
)

clf.score(
    X=X,  # Lint/Type Problem
    y=y,
)

cross_val_score(
    estimator=clf,
    X=X,  # Lint/Type Problem
    y=y,  # ERROR with polars
)

cross_validate(
    estimator=clf,
    X=X,  # Lint/Type Problem
    y=y,  # ERROR with polars
)

permutation_importance(
    estimator=clf,
    X=X,  # WARNING with polars + Lint/Type Problem
    y=y,
)

clf.predict(X)
```

## Errors / crashes using polars

Both `cross_val_score` and `cross_validate` crash using polars Series with message:
- `TypeError: cannot use `__getitem__` on Series of dtype Int32 with argument (array([18, 21, 22, 23, 24, 25,...`

### Temporary solution

use
- `to_numpy()` -> works with numpy array
- `to_pandas()` -> works with pandas Series

## Warnings

`permutation_importance` creates ""UserWarnings"" using polars DataFrame with message:
- `UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names`

### Temporary Solution

use
- `to_pandas()` -> works with pandas DataFrame

## Linting / Type problems

numpy arrays and pandas DataFrame have ""full support"" while polars looks a little sad  

![image](https://github.com/scikit-learn/scikit-learn/assets/25177421/b0aa54c2-52e0-4d4c-8c7c-448a9fbb978e)











### Describe your proposed solution

would love to see further support for polars DataFrame / Series types.

Not sure why some things error, because I thought polars provides all the required apis.

Any way to assist / help?

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_","['Bug']","2024-02-20T16:22:18Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/28488","Runtime Error"
"58","scikit-learn/scikit-learn","Crash in T-SNE","### Describe the bug

I got a crash using the (external) [hdbscan package](https://github.com/scikit-learn-contrib/hdbscan) in some special situation. I [debugged it](https://github.com/scikit-learn-contrib/hdbscan/issues/623) and found out that it happens in the scikit-learn package, specifically its T-SNE implementation. The hdbscan maintainer ([Leland McInnes](https://github.com/lmcinnes)!) suggested to report it here.

Is this something that should be guarded for in scikit-learn?

### Steps/Code to Reproduce

The simplest way to reproduce it (using the hdbscan package) is:
```py
import numpy as np
import hdbscan
model = hdbscan.HDBSCAN(gen_min_span_tree=True)
data = np.zeros((91, 3))
clustering = model.fit(data)
clustering.minimum_spanning_tree_.plot()
```
Note that it also happens when only a relative small proportion of points are equal (but only sometimes?), this is just the easiest way to show it. By default some warnings are displayed:
> ...\sklearn\decomposition\_pca.py:685: RuntimeWarning: invalid value encountered in divide
>   self.explained_variance_ratio_ = self.explained_variance_ / total_var
> ...\sklearn\manifold\_t_sne.py:1002: RuntimeWarning: invalid value encountered in divide
>   X_embedded = X_embedded / np.std(X_embedded[:, 0]) * 1e-4 

In the end it appears to be a problem in `sklearn.manifold._t_sne._barnes_hut_tsne.gradient()`, not (always?) being able to handle `nan` values. For example, this reproduces the crash:
```py
import numpy as np
from sklearn.manifold._t_sne import _barnes_hut_tsne
neighbors = np.array([1, 2, 0, 2, 0, 1], dtype='int64')
val_P = np.full_like(neighbors, 2 / 45, dtype='float32')
pos_output = np.full((3, 2), np.nan, dtype='float32')
forces = np.zeros_like(pos_output)
indptr = np.arange(7, step=2, dtype='int64')
_barnes_hut_tsne.gradient(val_P, pos_output, neighbors, indptr, forces, 0.5, 2, 11)
```
One layer deeper, the crash occurs inside `sklearn.neighbors._quad_tree._QuadTree.build_tree()`, as follows:
```py
import numpy as np
from sklearn.neighbors._quad_tree import _QuadTree
qt = _QuadTree(2, 11)
X = np.full((3, 2), np.nan, dtype='float32')
qt.build_tree(X)
```
The output of this (due to `verbose=11`) up to the crash is:
> [QuadTree] bounding box axis 0 : [nan, nan]
> [QuadTree] bounding box axis 1 : [nan, nan]
> [QuadTree] Inserting depth 0
> [QuadTree] inserted point 0 in cell 0
> [QuadTree] Inserting depth 0
> [QuadTree] inserted point 0 in new child 1
> [QuadTree] Inserting depth 0
> [QuadTree] Inserting depth 1
> ...
> [QuadTree] Inserting depth 6271
> [QuadTree] inserted point 0 in new child 6272
> [QuadTree] Inserting depth 6271
> [QuadTree] Inserting depth 6272

I didn't dig into the QuadTree code.

### Expected Results

I would expect anything but Python crashing.

### Actual Results

Python crashed: its console window just went away silently (on Windows).

### Versions

```shell
System:
  python: 3.12.1 (tags/v3.12.1:2305ca5, Dec 7 2023, 22:03:25) [MSC v.1937 64 bit (AMD64)]
executable: C:\Users\Public\Software\Python\python.exe
 machine: Windows-11-10.0.22631-SP0

Python dependencies:
   sklearn: 1.4.0
     pip: 23.3.2
 setuptools: 69.0.3
    numpy: 1.26.3
    scipy: 1.12.0
   Cython: 3.0.8
   pandas: 2.2.0
 matplotlib: 3.8.2
   joblib: 1.3.2
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
   user_api: blas
 internal_api: openblas
  num_threads: 16
    prefix: libopenblas
   filepath: C:\Users\Public\Software\Python\Lib\site-packages\numpy.libs\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll
    version: 0.3.23.dev
threading_layer: pthreads
 architecture: Cooperlake

   user_api: openmp
 internal_api: openmp
  num_threads: 16
    prefix: vcomp
   filepath: C:\Users\Public\Software\Python\Lib\site-packages\sklearn\.libs\vcomp140.dll
    version: None

   user_api: blas
 internal_api: openblas
  num_threads: 16
    prefix: libopenblas
   filepath: C:\Users\Public\Software\Python\Lib\site-packages\scipy.libs\libopenblas_v0.3.20-571-g3dec11c6-gcc_10_3_0-c2315440d6b6cef5037bad648efc8c59.dll
    version: 0.3.21.dev
threading_layer: pthreads
 architecture: Cooperlake
```
","['Bug', 'help wanted', 'module:manifold', 'cython']","2024-02-05T23:03:32Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/28368","Syntax Error"
"59","scikit-learn/scikit-learn","Special characters (e.g. &) are not escaped by sklearn.tree.export_graphviz","### Describe the bug

Exporting a decision tree where the `feature_names` or `class_names` contain special characters (particularly `&<>`) results in invalid graphviz output, as those characters have specific meanings to graphviz. Escaping to `&amp;`, `&lt;` and `&gt;` results in correct output. This can of course be done by the user but it's something I think scikit-learn should handle internally.

### Steps/Code to Reproduce

```python
from sklearn.datasets import load_iris
from sklearn import tree
iris = load_iris()
clf = tree.DecisionTreeClassifier()
clf = clf.fit(iris.data, iris.target)

target_names = [""setosa & 123"", ""versicolor"", ""virginca""]
# target_names = [""setosa &amp; 123"", ""versicolor"", ""virginca""]  # This one works

tree.export_graphviz(
		clf,
		out_file=""tree.dot"",
		feature_names=iris.feature_names,
		class_names=target_names,
		filled=True,
		special_characters=True,
		)

```

Then run graphviz

```bash
dot tree.dot -Tsvg -o tree.svg 
```

### Expected Results

Graphviz successfully converts to SVG without error.

### Actual Results

```
Error: not well-formed (invalid token) in line 1 
... <br/>class = setosa & 123 ...
in label of node 0
Error: not well-formed (invalid token) in line 1 
... <br/>class = setosa & 123 ...
in label of node 1
```

Although SVG output is written to disk it is not correct.
![image](https://github.com/scikit-learn/scikit-learn/assets/8050853/8aa517f5-9764-4d9a-93f9-d20864f6085c)


### Versions

```shell
System:
    python: 3.8.10 (default, Nov 22 2023, 10:22:35)  [GCC 9.4.0]
executable: /home/domdf/Python/01 GitHub Repos/13 GunShotMatch/gunshotmatch-cli/venv/bin/python3
   machine: Linux-5.15.0-92-generic-x86_64-with-glibc2.29

Python dependencies:
      sklearn: 1.3.2
          pip: 23.3.2
   setuptools: 69.0.3
        numpy: 1.24.4
        scipy: 1.10.1
       Cython: None
       pandas: 2.0.3
   matplotlib: 3.7.4
       joblib: 1.3.2
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
    num_threads: 16
         prefix: libgomp
       filepath: /home/domdf/Python/01 GitHub Repos/13 GunShotMatch/gunshotmatch-cli/venv/lib/python3.8/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libopenblas
       filepath: /home/domdf/Python/01 GitHub Repos/13 GunShotMatch/gunshotmatch-cli/venv/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
        version: 0.3.21
threading_layer: pthreads
   architecture: Zen

       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libopenblas
       filepath: /home/domdf/Python/01 GitHub Repos/13 GunShotMatch/gunshotmatch-cli/venv/lib/python3.8/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so
        version: 0.3.18
threading_layer: pthreads
   architecture: Zen
```
","['Bug']","2024-02-01T10:21:36Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/28339","Syntax Error"
"60","scikit-learn/scikit-learn","Tests failing when cuda installed but no GPU is present","after doing `conda install pytorch cupy`, my tests fail with:

```
FAILED sklearn/metrics/tests/test_common.py::test_array_api_compliance[
accuracy_score-check_array_api_binary_classification_metric-cupy-None-None] 
- cupy_backends.cuda.api.runtime.CUDARuntimeError: cudaErrorNoDevice: 
- no CUDA-capable device is detected
```

I don't think tests should ever fail for this, should they?

cc @ogrisel @betatim ","['Bug']","2024-01-26T15:37:06Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/28280","Runtime Error"
"61","scikit-learn/scikit-learn","Exception in LogisticRegressionCV","### Describe the bug

The code provided below raises ValueError. I guess that the problem is that minor classes may not be included in **train** or **val** sets for some folds during internal cross-validation, even with stratified split. This produces errors with some metrics other than default (accuracy).

One solution may be setting log-proba to -inf for classes not present in the train set, as well as providing label argument. How can I fix this in the most simple way?

### Steps/Code to Reproduce

```
import numpy as np
from sklearn.linear_model import LogisticRegressionCV
X = np.zeros((10, 1))
y = [1, 1, 1, 1, 1, 2, 2, 2, 2, 3]
logreg = LogisticRegressionCV(cv=5, scoring='neg_log_loss')
logreg.fit(X, y)
```

### Expected Results

No exception thrown

### Actual Results

ValueError: y_true and y_pred contain different number of classes 2, 3. Please provide the true labels explicitly through the labels argument. Classes found in y_true: [0 1]

### Versions

```shell
System:
    python: 3.10.8 (main, Nov  2 2023, 15:57:09) [GCC 9.4.0]
executable: /data/osedukhin/tabular-models/venv/bin/python
   machine: Linux-5.4.0-123-generic-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.4.0
          pip: 22.2.2
   setuptools: 63.2.0
        numpy: 1.26.2
        scipy: 1.11.3
       Cython: None
       pandas: 2.1.3
   matplotlib: 3.8.1
       joblib: 1.3.2
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 32
         prefix: libopenblas
       filepath: /data/osedukhin/tabular-models/venv/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: SkylakeX

       user_api: openmp
   internal_api: openmp
    num_threads: 32
         prefix: libgomp
       filepath: /data/osedukhin/tabular-models/venv/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 32
         prefix: libopenblas
       filepath: /data/osedukhin/tabular-models/venv/lib/python3.10/site-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: SkylakeX
```
","['Bug']","2024-01-18T18:36:29Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/28178","Syntax Error"
"62","scikit-learn/scikit-learn","Inconsistency in `DecisionTreeClassifier` Threshold Behavior","### Describe the bug

I've encountered an unexpected behavior in `DecisionTreeClassifier` when using a decision stump (a tree with one root node and two leaf children). My assumption is based on the standard decision tree logic where a feature value `x` is classified to the left child if `x <= threshold`. Therefore, I expect the following assertion to always be true:
```python
assert clf.apply([[clf.tree_.threshold[0]]]) == 1
```

This assertion is meant to test that a feature value equal to the root node's threshold is classified to the left child node, in accordance with the `x <= threshold` rule. However, I have observed that in approximately 25% of the cases, the data point is unexpectedly classified to the right child node instead of the left.

The issue persists regardless of whether the `threshold[0]` is explicitly converted to `float32` or not. Given scikit-learn's documentation stating that
> All decision trees use `np.float32` arrays internally. If training data is not in this format, a copy of the dataset will be made

this behavior is puzzling. Precise thresholding is crucial for my application. Thank you for your time and effort in maintaining this important library and for any insights you can provide regarding this issue.

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.tree import DecisionTreeClassifier

correct, wrong = [], []
n_trials = 500
for seed in range(n_trials):
    rand = np.random.RandomState(seed)
    X = rand.normal(size=(100, 1))
    y = rand.randint(0, 2, size=X.shape[0])
    clf = DecisionTreeClassifier(max_depth=1, random_state=0).fit(X, y)
    
    thres = clf.tree_.threshold[0]
    # thres = np.float32(thres) # gives the same correct-wrong distribution
    if clf.apply([[thres]]) == 1:
        correct.append(thres)
    else:
        wrong.append(thres)

    assert clf.apply([[np.nextafter(np.float32(thres), np.float32(-np.inf))]]) == 1 # this is true though
print(f'{len(correct) / n_trials:%}') # 75%
print(f'{len(wrong) / n_trials:%}') # 25%
```

### Expected Results

```
100.0000%
0.0000%
```

### Actual Results

```
75.0000%
25.0000%
```

### Versions

```shell
System:
    python: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]
executable: /usr/bin/python3
   machine: Linux-6.1.58+-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.2.2
          pip: 23.1.2
   setuptools: 67.7.2
        numpy: 1.23.5
        scipy: 1.11.4
       Cython: 3.0.8
       pandas: 1.5.3
   matplotlib: 3.7.1
       joblib: 1.3.2
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 2
         prefix: libopenblas
       filepath: /usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so
        version: 0.3.20
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 2
         prefix: libgomp
       filepath: /usr/local/lib/python3.10/dist-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 2
         prefix: libopenblas
       filepath: /usr/local/lib/python3.10/dist-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: Haswell
```
","['Bug']","2024-01-18T18:17:14Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/28175","Syntax Error"
"63","scikit-learn/scikit-learn","Bug in utils/multiclass.py/_ovr_decision_function","### Describe the workflow you want to enable

Dear scikit learn developpers,

I think the implementation of `_ovr_decision_function` in utils
/multiclass.py doesn't work properly when the parameter `confidences` is probability. While as the documentation suggests, it can be a probability .

```
confidences : array-like of shape (n_samples, n_classifiers)
        Decision functions or predicted probabilities for positive class
        for each binary classifier.
```

The problem is the following two lines of codes

```
sum_of_confidences[:, i] -= confidences[:, k]
sum_of_confidences[:, j] += confidences[:, k]
```

In this context, there is a binary classifier for class `i` vs `j`. And `j` is the positive class. 

If `confidences` is ""decision_function"", then it works. Because if ""decision funtion"" is negative, it means the classifier thinks the negatve class `i` is more possible. And the `-=` will increase the `sum_of_confidences` of `i`, and decrease the `sum_of_confidences` of `j`.

However, if  `confidences` is ""probability"", it doesn't work. Because probability is always greater than zero. So the `sum_of_confidences` of `i` will always decrease, even when `i` is more likely to happend (prob of `j` < 0.5).




### Describe your proposed solution

""decision_function"" is centered at 0, while ""probability"" is centerd at 0.5. These two cases should be handled seperately.

### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_","['Bug']","2023-12-17T13:50:59Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/27973","Logical Bug"
"64","scikit-learn/scikit-learn","Pandas Copy-on-Write mode should be enabled in all tests","### Describe the bug

Pandas COW will be enabled by default in version 3.0.
For example, today I just found that `TargetEncoder` doesn't work properly with it enabled.
There are probably many other examples that could be uncovered by testing.

### Steps/Code to Reproduce

```python
import pandas as pd
from sklearn.preprocessing import TargetEncoder
pd.options.mode.copy_on_write = True

df = pd.DataFrame({
    ""x"": [""a"", ""b"", ""c"", ""c""],
    ""y"": [4., 5., 6., 7.]
})
t = TargetEncoder(target_type=""continuous"")
t.fit(df[[""x""]], df[""y""])
```

### Expected Results

No error.

### Actual Results
```
ValueError                                Traceback (most recent call last)
Cell In[2], line 10
      5 df = pd.DataFrame({
      6     ""x"": [""a"", ""b"", ""c"", ""c""],
      7     ""y"": [4., 5., 6., 7.]
      8 })
      9 t = TargetEncoder(target_type=""continuous"")
---> 10 t.fit(df[[""x""]], df[""y""])

File ~/.conda/envs/jhop311/lib/python3.11/site-packages/sklearn/base.py:1152, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1145     estimator._validate_params()
   1147 with config_context(
   1148     skip_parameter_validation=(
   1149         prefer_skip_nested_validation or global_skip_validation
   1150     )
   1151 ):
-> 1152     return fit_method(estimator, *args, **kwargs)

File ~/.conda/envs/jhop311/lib/python3.11/site-packages/sklearn/preprocessing/_target_encoder.py:203, in TargetEncoder.fit(self, X, y)
    186 @_fit_context(prefer_skip_nested_validation=True)
    187 def fit(self, X, y):
    188     """"""Fit the :class:`TargetEncoder` to X and y.
    189 
    190     Parameters
   (...)
    201         Fitted encoder.
    202     """"""
--> 203     self._fit_encodings_all(X, y)
    204     return self

File ~/.conda/envs/jhop311/lib/python3.11/site-packages/sklearn/preprocessing/_target_encoder.py:332, in TargetEncoder._fit_encodings_all(self, X, y)
    330 if self.smooth == ""auto"":
    331     y_variance = np.var(y)
--> 332     self.encodings_ = _fit_encoding_fast_auto_smooth(
    333         X_ordinal, y, n_categories, self.target_mean_, y_variance
    334     )
    335 else:
    336     self.encodings_ = _fit_encoding_fast(
    337         X_ordinal, y, n_categories, self.smooth, self.target_mean_
    338     )

File sklearn/preprocessing/_target_encoder_fast.pyx:82, in sklearn.preprocessing._target_encoder_fast._fit_encoding_fast_auto_smooth()

File stringsource:660, in View.MemoryView.memoryview_cwrapper()

File stringsource:350, in View.MemoryView.memoryview.__cinit__()

ValueError: buffer source array is read-only
```

### Versions

```shell
System:
    python: 3.11.3 | packaged by conda-forge | (main, Apr  6 2023, 08:57:19) [GCC 11.3.0]
executable: /home/jhopfens/.conda/envs/jhop311/bin/python
   machine: Linux-3.10.0-1160.99.1.el7.x86_64-x86_64-with-glibc2.17

Python dependencies:
      sklearn: 1.3.2
          pip: 23.0.1
   setuptools: 67.6.1
        numpy: 1.25.2
        scipy: 1.11.2
       Cython: 3.0.0
       pandas: 2.1.0
   matplotlib: 3.7.2
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/jhopfens/.conda/envs/jhop311/lib/python3.11/site-packages/numpy.libs/libopenblas64_p-r0-5007b62f.3.23.dev.so
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: SkylakeX
    num_threads: 64

       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/jhopfens/.conda/envs/jhop311/lib/python3.11/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
    num_threads: 128

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/jhopfens/.conda/envs/jhop311/lib/python3.11/site-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: SkylakeX
    num_threads: 64
```
","['Bug']","2023-11-30T16:54:28Z","9","0","https://github.com/scikit-learn/scikit-learn/issues/27879","Syntax Error"
"65","scikit-learn/scikit-learn","HalvingGridSearchCV should not care about parameter-grid layout but apparently does","### Describe the bug

I have two parameter-grid layouts, both specifying the exact same set of configurations. The difference is that the choices are listed in a different order, e.g., [False, True] versus [True, False]. My understanding is that this should not matter given that HalvingGridSearchCV tries all configurations on the first step. However, I get different results when using first grid versus the second.

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.svm import SVC
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

#three grids with identical configuration space

svc_grid_1 = dict(C=[1,2,3],
                gamma=['auto', 'scale'],
                shrinking=(True, False),
                kernel=['sigmoid', 'linear', 'poly', 'rbf'],
                max_iter = [5000, 10000, -1]
                )

svc_grid_2 = dict(C=[1,2,3],
                gamma=['auto', 'scale'],
                shrinking=(True, False),
                kernel=['rbf', 'poly', 'linear',  'sigmoid'],  #reordered choices
                max_iter = [5000, 10000, -1]
                )

svc_grid_3 = dict(kernel=['sigmoid', 'linear', 'poly', 'rbf'],  #reordered keys
                  C=[1,2,3],
                  shrinking=(True, False),
                  max_iter = [5000, 10000, -1],
                  gamma=['auto', 'scale'],
                )

###How many different combinations?

from sklearn.model_selection import ParameterGrid
param_grid_1 = ParameterGrid(svc_grid_1)  #a list of dictionaries, one for each combo
print(f'{len(param_grid_1)=}')  #144
param_grid_2 = ParameterGrid(svc_grid_2)  #a list of dictionaries, one for each combo
print(f'{len(param_grid_1)=}')  #144
param_grid_3 = ParameterGrid(svc_grid_3)  #a list of dictionaries, one for each combo
print(f'{len(param_grid_1)=}')  #144

all([d in param_grid_2 and d in param_grid_3 for d in param_grid_1])  #True

#random data

np.random.seed(0)
n_samples, n_features = 1313, 6
x_train = np.random.rand(n_samples, n_features)
y_train = np.random.randint(0, 2, n_samples)  # binary target with values in {0, 1}

#Try Grid 1

svc_model = SVC(probability=True, random_state=1)  #base model
min_res = 30

#do the grid search
halving_cv = HalvingGridSearchCV(
    svc_model, svc_grid_1,  
    scoring=""roc_auc"",  
    n_jobs=1,  
    min_resources=min_res,
    factor=2,  
    cv=5, random_state=1234,
    refit=True,  
)

grid_result_1 = halving_cv.fit(x_train, y_train)
grid_result_1.best_params_ 
#{'C': 1,
# 'gamma': 'scale',
# 'kernel': 'sigmoid',
# 'max_iter': -1,
# 'shrinking': True}

#Try grid 2

svc_model = SVC(probability=True, random_state=1)  #base model

min_res = 30

#do the grid search
halving_cv = HalvingGridSearchCV(
    svc_model, svc_grid_2,  
    scoring=""roc_auc"", 
    n_jobs=1,  
    min_resources=min_res,
    factor=2, 
    cv=5, random_state=1234,
    refit=True, 
)

grid_result_2 = halving_cv.fit(x_train, y_train)
grid_result_2.best_params_ 
#{'C': 1,
 #'gamma': 'scale',
 #'kernel': 'sigmoid',
 #'max_iter': 5000,
 #'shrinking': True}

set(grid_result_1.best_params_.values()) == set(grid_result_2.best_params_.values())  #False
```
### Expected Results

best_params_  would be same for both grids

### Actual Results

They are different.

### Versions

```shell
System:
    python: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]
executable: /usr/bin/python3
   machine: Linux-5.15.120+-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.2.2
          pip: 23.1.2
   setuptools: 67.7.2
        numpy: 1.23.5
        scipy: 1.11.3
       Cython: 3.0.4
       pandas: 1.5.3
   matplotlib: 3.7.1
       joblib: 1.3.2
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /usr/local/lib/python3.10/dist-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so
        version: 0.3.20
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libgomp
       filepath: /usr/local/lib/python3.10/dist-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /usr/local/lib/python3.10/dist-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: Haswell
```
","['Bug', 'Needs Investigation']","2023-11-07T17:36:38Z","9","0","https://github.com/scikit-learn/scikit-learn/issues/27740","Logical Bug"
"66","scikit-learn/scikit-learn","sklearn PCA rotates a single vector","### Describe the bug

The issue we recently discovered is that sklearn PCA rotates the input when only a single variable is fed into the model.  

I am aware there are infinite rotations when there is a single vector fed into the model, however, the output from PCA should intuitively make sense. So I suggest hard coding this scenario (which I guess already done in R)


### Steps/Code to Reproduce
```python
import numpy as np
from sklearn.decomposition import PCA

x = np.array([1,2,3,4,5,6,7,8,9,10.]).reshape(-1,1).astype('float64')

p = PCA().fit_transform(x)
p
```
### output:
```
array([[ 4.5],
       [ 3.5],
       [ 2.5],
       [ 1.5],
       [ 0.5],
       [-0.5],
       [-1.5],
       [-2.5],
       [-3.5],
       [-4.5]])
```

as we see here the value 4.5 is corresponding to the smallest value in the set (1) and the value -4.5 is corresponding to the highest value in the input set (10).

### Expected Results:
```
array([[-4.5],
       [-3.5],
       [-2.5],
       [-1.5],
       [-0.5],
       [ 0.5],
       [ 1.5],
       [ 2.5],
       [ 3.5],
       [ 4.5]])
```



### Steps/Code to Reproduce

```
from sklearn.decomposition import PCA

x= np.array([1,2,3,4,5,6,7,8,9,10.]).reshape(-1,1).astype('float64')

p = PCA().fit_transform(x)
p
```

### Expected Results

```
array([[-4.5],
       [-3.5],
       [-2.5],
       [-1.5],
       [-0.5],
       [ 0.5],
       [ 1.5],
       [ 2.5],
       [ 3.5],
       [ 4.5]])
```

### Actual Results

```
array([[ 4.5],
       [ 3.5],
       [ 2.5],
       [ 1.5],
       [ 0.5],
       [-0.5],
       [-1.5],
       [-2.5],
       [-3.5],
       [-4.5]])
```

### Versions

```shell
System:
    python: 3.11.4 | packaged by Anaconda, Inc. | (main, Jul  5 2023, 13:47:18) [MSC v.1916 64 bit (AMD64)]
executable: e:\Users\***\miniconda3\python.exe
   machine: Windows-10-10.0.22621-SP0

Python dependencies:
      sklearn: 1.3.0
          pip: 23.2.1
   setuptools: 68.0.0
        numpy: 1.24.3
        scipy: 1.11.1
       Cython: None
       pandas: 2.0.3
   matplotlib: 3.7.1
       joblib: 1.2.0
threadpoolctl: 2.2.0

Built with OpenMP: True

threadpoolctl info:
       filepath: E:\Users\***\miniconda3\Library\bin\mkl_rt.2.dll
         prefix: mkl_rt
       user_api: blas
   internal_api: mkl
        version: 2023.1-Product
    num_threads: 8
threading_layer: intel

       filepath: E:\Users\***\miniconda3\vcomp140.dll
         prefix: vcomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 16

       filepath: E:\Users\***\miniconda3\Library\bin\libiomp5md.dll
         prefix: libiomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 16
```
","['Bug']","2023-10-19T09:09:38Z","9","1","https://github.com/scikit-learn/scikit-learn/issues/27620","Logical Bug"
"67","scikit-learn/scikit-learn","Non-metric MDS gives nonsense results on the Digits dataset","### Describe the bug

I am running metric and nonmetric MDS on the Digits dataset, and obtain nonsense results with `metric=False`. At the same time, my understanding is that non-metric MDS is more flexible and should not yield worse results compared to metric MDS. Is my understanding wrong, or does it suggest some problems with the non-metric MDS implementation?

### Steps/Code to Reproduce

```Python
%matplotlib notebook

import pylab as plt
import numpy as np
from sklearn.datasets import load_digits
from sklearn.manifold import MDS

digits = load_digits()
X, y = digits.data, digits.target

Z1 = MDS(n_components=2, n_init=1, random_state=42).fit_transform(X)
Z2 = MDS(n_components=2, n_init=1, random_state=42, metric=False).fit_transform(X)

titles = ['Metric MDS', 'Non-metric MDS']

fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(6, 3), layout='constrained')

for i,Z in enumerate([Z1, Z2]):
    ax = axs.ravel()[i]
    
    ax.set_aspect('equal', 'datalim')
    
    ind = np.random.permutation(X.shape[0])
    ax.scatter(*Z[ind].T, s=3, color=plt.cm.Dark2(y)[ind])

    ax.set_xticks([])
    ax.set_yticks([])
    for sp in ax.spines:
        ax.spines[sp].set_visible(False)
        
    ax.set_title(titles[i])
    
fig.savefig('digits-embed-mds-nonmetric.png', dpi=300)
```

### Expected Results

Non-metric MDS giving something sensible.

### Actual Results

![digits-embed-mds-nonmetric](https://github.com/scikit-learn/scikit-learn/assets/8970231/aa6af88a-75d0-4ef2-ab6e-6482c4df5ae8)


### Versions

```shell
System:
    python: 3.7.9 (default, Aug 31 2020, 12:42:55)  [GCC 7.3.0]
executable: /home/dmitry/anaconda3/bin/python
   machine: Linux-5.15.0-76-generic-x86_64-with-debian-bullseye-sid

Python dependencies:
          pip: 22.3.1
   setuptools: 65.6.3
      sklearn: 0.24.1
        numpy: 1.21.5
        scipy: 1.7.3
       Cython: 0.29.32
       pandas: 1.3.5
   matplotlib: 3.5.2
       joblib: 1.1.1
threadpoolctl: 2.2.0

Built with OpenMP: True
```
","['Bug', 'Needs Investigation']","2023-08-07T15:03:55Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/27028","Logical Bug"
"68","scikit-learn/scikit-learn","PoissonRegressor lbfgs solver giving coefficients of 0 and Runtime Warning","### Describe the bug

See the following [stack exchange](https://stats.stackexchange.com/questions/622085/sklearn-poissonregressor-giving-all-coefficients-zero) post (the solution to my original issue was to use newton-cholesky solver)

When fitting a Poisson Regression (without regularization) to some dummy data I encounter:

- with lbfgs solver, a Runtime Warning, a non-zero intercept and all coefficients as zero
- with newton-cholesky solver, coefficients as expected

Some people on StackExchange have mentioned it is worth submitting an issue (there was a similar [one](https://github.com/scikit-learn/scikit-learn/issues/24752) faced with Logistic Regression).

### Steps/Code to Reproduce

```python
import statsmodels.api as sm
import statsmodels.formula.api as smf
import pandas as pd
from sklearn.linear_model import PoissonRegressor
from sklearn.preprocessing import (
    OneHotEncoder,
)
data = sm.datasets.get_rdataset('Insurance', package='MASS').data
# Fit Poisson regression using formula interface
formula = ""Claims ~ C(District, Treatment(1)) + C(Group, Treatment('<1l')) + C(Age, Treatment('<25')) + Holders""
model_smf = smf.poisson(formula=formula, data=data).fit()
print(type(model_smf))
print(model_smf.summary())
# with sklearn OneHotEncoder

X_train_ohe = OneHotEncoder(sparse_output=False, drop=[1, ""<1l"", ""<25""]).fit(data[[""District"", ""Group"", ""Age""]])
X_train_ohe = pd.DataFrame(X_train_ohe.transform(data[[""District"", ""Group"", ""Age""]]), columns=X_train_ohe.get_feature_names_out())

X_train = pd.concat([X_train_ohe, data[[""Holders""]]], axis=1)
y_train = data[""Claims""]

# one-hot encode the categorical columns, and drop the baseline column
# with lbfgs solver

model_sklearn_lbfgs = PoissonRegressor(alpha=0).fit(X_train, y_train)

print(model_sklearn_lbfgs.intercept_)
print(model_sklearn_lbfgs.coef_)
# with newton-cholesky solver

model_sklearn_nc = PoissonRegressor(alpha=0, solver='newton-cholesky').fit(X_train, y_train)

print(model_sklearn_nc.intercept_)
print(model_sklearn_nc.coef_)
```

### Expected Results

Expected intercept/coefficients (from statsmodels and sklearn with newton-cholesky solver):
```
2.8456859090224444
[-4.47436243e-01 -9.30629212e-01 -1.46547942e+00  1.00603354e+00
  4.71328611e-01 -5.98213625e-01  5.69685530e-01  6.85301286e-01
  2.22504699e+00 -1.58161172e-05]
```

### Actual Results

Result with lbfgs:
```
3.896592058397603
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
/home/akaash/Downloads/statsmodels-play/venv/lib/python3.10/site-packages/sklearn/linear_model/_linear_loss.py:290: RuntimeWarning: invalid value encountered in matmul
  grad[:n_features] = X.T @ grad_pointwise + l2_reg_strength * weights
```

### Versions

```shell
System:
    python: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0]
executable: /home/akaash/Downloads/statsmodels-play/venv/bin/python
   machine: Linux-5.19.0-46-generic-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.3.0
          pip: 23.2.1
   setuptools: 68.0.0
        numpy: 1.25.1
        scipy: 1.11.1
       Cython: None
       pandas: 2.0.3
   matplotlib: 3.7.2
       joblib: 1.3.1
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 6
         prefix: libopenblas
       filepath: /home/akaash/Downloads/statsmodels-play/venv/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-7a851222.3.23.so
        version: 0.3.23
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 6
         prefix: libopenblas
       filepath: /home/akaash/Downloads/statsmodels-play/venv/lib/python3.10/site-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 6
         prefix: libgomp
       filepath: /home/akaash/Downloads/statsmodels-play/venv/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
```
","['Bug', 'help wanted']","2023-08-04T22:18:48Z","16","0","https://github.com/scikit-learn/scikit-learn/issues/27016","Syntax Error"
"69","scikit-learn/scikit-learn","inverse_transform of SimpleImputer with empty features changes order of columns","### Describe the bug

When one uses a SimpleImputer with `keep_empty_features=False` and `add_indicator=True`, then if a column has only missing values during fit, but has values during transform, then the inverse transform will shift all columns to the right of that column one to the left. 

(Note that I found this while experimenting and is not something that we rely on being fixed)

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.impute import SimpleImputer

X1 = np.array(
    [
        [np.nan, 2.0, 3.0],
        [np.nan, 2.0, 3.0],
    ]
)

X2 = np.array(
    [
        [1.0, 2.0, 3.0],
        [1.0, 2.0, 3.0],
    ]
)

imputer = SimpleImputer(add_indicator=True)
imputer.fit(X1)
print(imputer.inverse_transform(imputer.transform(X2)))
```

### Expected Results

I think the best way to go is to fill the missing columns with the value from `missing_values`.

```python
[[nan 2. 3.]
 [nan 2. 3.]]
```

### Actual Results

```python
[[2. 3. 0.]
 [2. 3. 0.]]
```

### Versions

```shell
System:
    python: 3.11.4
executable: [redacted]
   machine: macOS-13.4.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.3.0
          pip: 23.2
   setuptools: 68.0.0
        numpy: 1.25.1
        scipy: 1.11.1
       Cython: None
       pandas: 2.0.3
   matplotlib: 3.7.2
       joblib: 1.3.0
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 12
         prefix: libopenblas
       filepath: [redacted]
        version: 0.3.23
threading_layer: openmp
   architecture: VORTEX

       user_api: openmp
   internal_api: openmp
    num_threads: 12
         prefix: libomp
       filepath: [redacted]
        version: None
```
","['Bug', 'help wanted']","2023-08-04T09:38:19Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/27012","Logical Bug"
"70","scikit-learn/scikit-learn","`ValueError: Input contains NaN.` in `sklearn.manifold.smacof`","### Describe the bug

I accidentally stumbled onto a `ValueError` when executing `smacof`. I hacked into `_mds.py` to save both the offending `dissimilarities` as well as the randomly generated `X`, then cut them down to minimal shape that still exhibits the error. This data is attached below in the MCVE.

### Steps/Code to Reproduce

```
import numpy as np                          
import sklearn.manifold                     
                                            
dis = np.array([
    [0.0, 1.732050807568877, 1.7320508075688772], 
    [1.732050807568877, 0.0, 6.661338147750939e-16],
    [1.7320508075688772, 6.661338147750939e-16, 0.0]
])  
init = np.array([
    [0.08665881585055124, 0.7939114643387546],
    [0.9959834154297658, 0.7555546025640025],
    [0.8766008278401566, 0.4227358815811242]
])  
sklearn.manifold.smacof(dis, init=init, normalized_stress=""auto"", metric=False, n_init=1)
```

### Expected Results

No errors

### Actual Results

```
Traceback (most recent call last):
  File "".../rep_error.py"", line 14, in <module>
    sklearn.manifold.smacof(dis, init=init, normalized_stress=""auto"", metric=False, n_init=1)
  File "".../.direnv/python-3.9.5/lib/python3.9/site-packages/sklearn/manifold/_mds.py"", line 329, in smacof
    pos, stress, n_iter_ = _smacof_single(
  File "".../.direnv/python-3.9.5/lib/python3.9/site-packages/sklearn/manifold/_mds.py"", line 128, in _smacof_single
    dis = euclidean_distances(X)
  File "".../.direnv/python-3.9.5/lib/python3.9/site-packages/sklearn/metrics/pairwise.py"", line 310, in euclidean_distances
    X, Y = check_pairwise_arrays(X, Y)
  File "".../.direnv/python-3.9.5/lib/python3.9/site-packages/sklearn/metrics/pairwise.py"", line 156, in check_pairwise_arrays
    X = Y = check_array(
  File "".../.direnv/python-3.9.5/lib/python3.9/site-packages/sklearn/utils/validation.py"", line 959, in check_array
    _assert_all_finite(
  File "".../.direnv/python-3.9.5/lib/python3.9/site-packages/sklearn/utils/validation.py"", line 124, in _assert_all_finite
    _assert_all_finite_element_wise(
  File "".../.direnv/python-3.9.5/lib/python3.9/site-packages/sklearn/utils/validation.py"", line 173, in _assert_all_finite_element_wise
    raise ValueError(msg_err)
ValueError: Input contains NaN.
```

### Versions

```shell
System:
    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]
executable: .../.direnv/python-3.9.5/bin/python
   machine: Linux-5.4.0-148-generic-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.3.0
          pip: 23.2.1
   setuptools: 44.0.0
        numpy: 1.25.1
        scipy: 1.11.1
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.3.1
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
    num_threads: 32
         prefix: libgomp
       filepath: .../.direnv/python-3.9.5/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 32
         prefix: libopenblas
       filepath: .../.direnv/python-3.9.5/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-7a851222.3.23.so
        version: 0.3.23
threading_layer: pthreads
   architecture: Zen

       user_api: blas
   internal_api: openblas
    num_threads: 32
         prefix: libopenblas
       filepath: .../.direnv/python-3.9.5/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-23e5df77.3.21.dev.so
        version: 0.3.21.dev
threading_layer: pthreads
   architecture: Zen
```
","['Bug', 'Needs Investigation']","2023-08-03T04:24:30Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/26999","Syntax Error"
"71","scikit-learn/scikit-learn","Agglomerative clustering training error for seuclidean/mahalanobis affinity and single linkage","### Describe the bug

When trying Agglomerative clustering model training with the affinity as 'seuclidean' or 'mahalanobis' and the linkage as 'single' the training fails. The same affinity values along with other linkage such as 'average' options executes for model training. There's no specification given for this issue. Also, in the code I can see the handling for the single linkage is different and there is some cython code which is not accessible.

### Steps/Code to Reproduce

```

from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import load_iris
model = AgglomerativeClustering(affinity='mahalanobis', linkage='average')
data = load_iris(as_frame=True)['data']
model.fit(data)

```

### Expected Results

No error should be thrown sig

### Actual Results

![image](https://github.com/scikit-learn/scikit-learn/assets/69189799/c3da5d93-b215-4fd8-8610-a4c8ce632f8d)


### Versions

```shell
System:
    python: 3.8.17 (default, Jul  5 2023, 21:04:15)  [GCC 11.2.0]
executable: /home/albint/miniconda3/envs/myenv/bin/python
   machine: Linux-5.15.0-78-generic-x86_64-with-glibc2.17

Python dependencies:
      sklearn: 1.3.0
          pip: 23.1.2
   setuptools: 67.8.0
        numpy: 1.24.4
        scipy: 1.10.1
       Cython: 3.0.0
       pandas: 2.0.3
   matplotlib: 3.7.2
       joblib: 1.3.1
threadpoolctl: 3.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /home/albint/miniconda3/envs/myenv/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
        version: 0.3.21
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /home/albint/miniconda3/envs/myenv/lib/python3.8/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so
        version: 0.3.18
threading_layer: pthreads
   architecture: Haswell

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libgomp
       filepath: /home/albint/miniconda3/envs/myenv/lib/python3.8/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
```
","['Bug', 'help wanted']","2023-08-01T11:50:21Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/26961","Syntax Error"
"72","scikit-learn/scikit-learn","Balanced Accuracy Score is NOT equal to Recall Score ","### Describe the bug

By definition balanced accuracy should be equal to recall averaged over all the classes. Current implementation gives different answers. Please see the example below. 

```
import scikit.metrics as skm

y_true = [1,1]
y_pred = [1,2]

skm.recall_score(y_true, y_pred, average='macro')  # 0.25
skm.balanced_accuracy_score(y_true, y_pred)  # 0.5

```

### Steps/Code to Reproduce

```
import scikit.metrics as skm

y_true = [1,1]
y_pred = [1,2]

recall = skm.recall_score(y_true, y_pred, average='macro')  
balanced_acc = skm.balanced_accuracy_score(y_true, y_pred) 
```

### Expected Results

```
recall = balaced_acc = 0.25
```

### Actual Results

```
recall = 0.25
balanced_acc = 0.5
```

### Versions

```shell
import sklearn; sklearn.show_versions()
```
","['Bug', 'New Feature']","2023-07-24T21:52:02Z","5","1","https://github.com/scikit-learn/scikit-learn/issues/26892","Logical Bug"
"73","scikit-learn/scikit-learn","Handling `pd.NA` in encoders","It seems that we don't handle it properly `pd.NA` in the encoder and thus differently than `np.nan`.
`pd.NA` will raise an error as in the following reproducible:

```python
df = pd.DataFrame({""col_1"": [""A"", ""B"", pd.NA]})
OneHotEncoder(sparse_output=False).fit_transform(df)
```

or 

```python
df = pd.DataFrame({""col_1"": [""A"", ""B"", pd.NA]})
OrdinalEncoder().fit_transform(df)
```

leading to:

<details>

```pytb
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
File [~/Documents/packages/scikit-learn/sklearn/utils/_encode.py:174](https://file+.vscode-resource.vscode-cdn.net/home/glemaitre/Documents/packages/scikit-learn/~/Documents/packages/scikit-learn/sklearn/utils/_encode.py:174), in _unique_python(values, return_inverse, return_counts)
    [172](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=171) uniques_set, missing_values = _extract_missing(uniques_set)
--> [174](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=173) uniques = sorted(uniques_set)
    [175](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=174) uniques.extend(missing_values.to_list())

File [~/miniconda3/envs/dev/lib/python3.10/site-packages/pandas/_libs/missing.pyx:388](https://file+.vscode-resource.vscode-cdn.net/home/glemaitre/Documents/packages/scikit-learn/~/miniconda3/envs/dev/lib/python3.10/site-packages/pandas/_libs/missing.pyx:388), in pandas._libs.missing.NAType.__bool__()

TypeError: boolean value of NA is ambiguous

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
[/home/glemaitre/Documents/packages/scikit-learn/examples/model_selection/plot_tuned_threshold_classifier_with_metadata_routing.py](https://file+.vscode-resource.vscode-cdn.net/home/glemaitre/Documents/packages/scikit-learn/examples/model_selection/plot_tuned_threshold_classifier_with_metadata_routing.py) in line 3
      [188](file:///home/glemaitre/Documents/packages/scikit-learn/examples/model_selection/plot_tuned_threshold_classifier_with_metadata_routing.py?line=187) # %%
      [189](file:///home/glemaitre/Documents/packages/scikit-learn/examples/model_selection/plot_tuned_threshold_classifier_with_metadata_routing.py?line=188) df = pd.DataFrame({""col_1"": [""A"", ""B"", pd.NA]})
----> [190](file:///home/glemaitre/Documents/packages/scikit-learn/examples/model_selection/plot_tuned_threshold_classifier_with_metadata_routing.py?line=189) OneHotEncoder(sparse_output=False).fit_transform(df)

File [~/Documents/packages/scikit-learn/sklearn/utils/_set_output.py:140](https://file+.vscode-resource.vscode-cdn.net/home/glemaitre/Documents/packages/scikit-learn/~/Documents/packages/scikit-learn/sklearn/utils/_set_output.py:140), in _wrap_method_output..wrapped(self, X, *args, **kwargs)
    [138](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_set_output.py?line=137) @wraps(f)
    [139](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_set_output.py?line=138) def wrapped(self, X, *args, **kwargs):
--> [140](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_set_output.py?line=139)     data_to_wrap = f(self, X, *args, **kwargs)
    [141](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_set_output.py?line=140)     if isinstance(data_to_wrap, tuple):
    [142](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_set_output.py?line=141)         # only wrap the first output for cross decomposition
    [143](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_set_output.py?line=142)         return_tuple = (
    [144](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_set_output.py?line=143)             _wrap_data_with_container(method, data_to_wrap[0], X, self),
    [145](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_set_output.py?line=144)             *data_to_wrap[1:],
    [146](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_set_output.py?line=145)         )

File [~/Documents/packages/scikit-learn/sklearn/base.py:948](https://file+.vscode-resource.vscode-cdn.net/home/glemaitre/Documents/packages/scikit-learn/~/Documents/packages/scikit-learn/sklearn/base.py:948), in TransformerMixin.fit_transform(self, X, y, **fit_params)
    [933](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=932)         warnings.warn(
    [934](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=933)             (
    [935](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=934)                 f""This object ({self.__class__.__name__}) has a `transform`""
   (...)
    [943](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=942)             UserWarning,
    [944](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=943)         )
    [946](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=945) if y is None:
    [947](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=946)     # fit method of arity 1 (unsupervised transformation)
--> [948](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=947)     return self.fit(X, **fit_params).transform(X)
    [949](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=948) else:
    [950](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=949)     # fit method of arity 2 (supervised transformation)
    [951](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=950)     return self.fit(X, y, **fit_params).transform(X)

File [~/Documents/packages/scikit-learn/sklearn/base.py:1215](https://file+.vscode-resource.vscode-cdn.net/home/glemaitre/Documents/packages/scikit-learn/~/Documents/packages/scikit-learn/sklearn/base.py:1215), in _fit_context..decorator..wrapper(estimator, *args, **kwargs)
   [1208](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=1207)     estimator._validate_params()
   [1210](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=1209) with config_context(
   [1211](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=1210)     skip_parameter_validation=(
   [1212](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=1211)         prefer_skip_nested_validation or global_skip_validation
   [1213](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=1212)     )
   [1214](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=1213) ):
-> [1215](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/base.py?line=1214)     return fit_method(estimator, *args, **kwargs)

File [~/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py:982](https://file+.vscode-resource.vscode-cdn.net/home/glemaitre/Documents/packages/scikit-learn/~/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py:982), in OneHotEncoder.fit(self, X, y)
    [972](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=971)     warnings.warn(
    [973](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=972)         (
    [974](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=973)             ""`sparse` was renamed to `sparse_output` in version 1.2 and ""
   (...)
    [978](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=977)         FutureWarning,
    [979](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=978)     )
    [980](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=979)     self.sparse_output = self.sparse
--> [982](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=981) self._fit(
    [983](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=982)     X,
    [984](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=983)     handle_unknown=self.handle_unknown,
    [985](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=984)     force_all_finite=""allow-nan"",
    [986](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=985) )
    [987](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=986) self._set_drop_idx()
    [988](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=987) self._n_features_outs = self._compute_n_features_outs()

File [~/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py:97](https://file+.vscode-resource.vscode-cdn.net/home/glemaitre/Documents/packages/scikit-learn/~/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py:97), in _BaseEncoder._fit(self, X, handle_unknown, force_all_finite, return_counts, return_and_ignore_missing_for_infrequent)
     [94](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=93) Xi = X_list[i]
     [96](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=95) if self.categories == ""auto"":
---> [97](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=96)     result = _unique(Xi, return_counts=compute_counts)
     [98](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=97)     if compute_counts:
     [99](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/preprocessing/_encoders.py?line=98)         cats, counts = result

File [~/Documents/packages/scikit-learn/sklearn/utils/_encode.py:42](https://file+.vscode-resource.vscode-cdn.net/home/glemaitre/Documents/packages/scikit-learn/~/Documents/packages/scikit-learn/sklearn/utils/_encode.py:42), in _unique(values, return_inverse, return_counts)
     [11](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=10) """"""Helper function to find unique values with support for python objects.
     [12](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=11) 
     [13](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=12) Uses pure python method for object dtype, and numpy method for
   (...)
     [39](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=38)     array. Only provided if `return_counts` is True.
     [40](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=39) """"""
     [41](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=40) if values.dtype == object:
---> [42](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=41)     return _unique_python(
     [43](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=42)         values, return_inverse=return_inverse, return_counts=return_counts
     [44](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=43)     )
     [45](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=44) # numerical
     [46](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=45) return _unique_np(
     [47](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=46)     values, return_inverse=return_inverse, return_counts=return_counts
     [48](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=47) )

File [~/Documents/packages/scikit-learn/sklearn/utils/_encode.py:179](https://file+.vscode-resource.vscode-cdn.net/home/glemaitre/Documents/packages/scikit-learn/~/Documents/packages/scikit-learn/sklearn/utils/_encode.py:179), in _unique_python(values, return_inverse, return_counts)
    [177](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=176) except TypeError:
    [178](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=177)     types = sorted(t.__qualname__ for t in set(type(v) for v in values))
--> [179](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=178)     raise TypeError(
    [180](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=179)         ""Encoders require their input to be uniformly ""
    [181](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=180)         f""strings or numbers. Got {types}""
    [182](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=181)     )
    [183](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=182) ret = (uniques,)
    [185](file:///home/glemaitre/Documents/packages/scikit-learn/sklearn/utils/_encode.py?line=184) if return_inverse:

TypeError: Encoders require their input to be uniformly strings or numbers. Got ['NAType', 'str']
```

</details>

If `np.nan` is used, then we are fine:

```python
df = pd.DataFrame({""col_1"": [""A"", ""B"", np.nan]})
OneHotEncoder(sparse_output=False).fit_transform(df)
```

```
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]])
```

```python
df = pd.DataFrame({""col_1"": [""A"", ""B"", np.nan]})
OrdinalEncoder().fit_transform(df)
```

```
array([[ 0.],
       [ 1.],
       [nan]])
```","['Bug', 'module:preprocessing']","2023-07-24T18:33:03Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/26890","Runtime Error"
"74","scikit-learn/scikit-learn","Ridge and RidgeCV has different result even if alpha is the same","### Describe the bug

(From discussion in https://github.com/scikit-learn/scikit-learn/discussions/26733)

In one of my dataset, I try to specify only one value in alphas in RidgeCV, and expects RidgeCV should give me the same result as using Ridge with same alpha value. But it would not (see the code and output below). And the RidgeCV result seems abnormal.

I found that the problem is related to some large values we used in sample_weight. I can reproduce it with make_regression.

I guess the large value in sample_weight may caused some overflow in RidgeCV intermediate calculation?
Given that RidgeCV and Ridge are using different solvers under the hood, I think it is not surprise that they may behave differently in this case? I just scaled down the weight and the problem is gone.



### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.linear_model import Ridge, RidgeCV
from sklearn.datasets import make_regression

X, y = make_regression(n_features=2)
wgt = np.random.randint(2e5, 2e11, size=100)

print('=== RidgeCV ===')
clf = RidgeCV(alphas=[0.001],fit_intercept=True).fit(
    X=X,
    y=y,
    sample_weight=wgt,
)
print(f'{clf.coef_ = }')
print(f'{clf.intercept_ = }')
print(f'{clf.alpha_ = }')

print('=== Ridge ===')
clf = Ridge(alpha=0.001,fit_intercept=True).fit(
    X=X,
    y=y,
    sample_weight=wgt
)
print(f'{clf.coef_ = }')
print(f'{clf.intercept_ = }')
```

### Expected Results

The output of RidgeCV and Ridge is the same.

### Actual Results

```
=== RidgeCV ===
'clf.coef_ = array([-95.63781897,  31.72140137])'
'clf.intercept_ = 2.6375015138569937'
'clf.alpha_ = 0.001'
=== Ridge ===
'clf.coef_ = array([41.8887401, 33.131992 ])'
'clf.intercept_ = -4.440892098500626e-16'
```

### Versions

```shell
I am using scikit-learn 1.2.1 on python 3.8 on Linux.
```
","['Bug', 'module:linear_model', 'Needs Investigation', 'Numerical Stability']","2023-07-17T18:59:24Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/26848","Logical Bug"
"75","scikit-learn/scikit-learn","AttributeError: This 'LabelEncoder' has no attribute 'set_output'","### Describe the bug

I tried to call **'set_output'** from LabelEncoder object and got the AttributeError.

[The document](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) says sklearn.preprocessing.LabelEncoder has **'set_output'** method, but it was not working.

Soon I found most of other **'set_output'** available estimators inherits both of sklearn.base.OneToOneFeatureMixin and sklearn.base.TransformerMinxin

Howerver, [LabelEncoder](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/preprocessing/_label.py) only inherits the TransformerMinxin.

```python
class LabelEncoder(TransformerMixin, BaseEstimator):
```

</br>

Function **'set_output'** seems available when **'_auto_wrap_is_configured'** is True. [(utils._set_output.py)](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/_set_output.py)

```python
    @available_if(_auto_wrap_is_configured)
    def set_output(self, *, transform=None):
        """"""Set output container.

        See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py`
        for an example on how to use the API.

        Parameters
        ----------
        transform : {""default"", ""pandas""}, default=None
            Configure output of `transform` and `fit_transform`.

            - `""default""`: Default output format of a transformer
            - `""pandas""`: DataFrame output
            - `None`: Transform configuration is unchanged

        Returns
        -------
        self : estimator instance
            Estimator instance.
        """"""
        if transform is None:
            return self

        if not hasattr(self, ""_sklearn_output_config""):
            self._sklearn_output_config = {}

        self._sklearn_output_config[""transform""] = transform
        return self
```

</br>

Then estimator should have **'get_feature_names_out'** to make **'_auto_wrap_is_configured'** returns True. [(utils._set_output.py)](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/_set_output.py)

```python
def _auto_wrap_is_configured(estimator):
    """"""Return True if estimator is configured for auto-wrapping the transform method.

    `_SetOutputMixin` sets `_sklearn_auto_wrap_output_keys` to `set()` if auto wrapping
    is manually disabled.
    """"""
    auto_wrap_output_keys = getattr(estimator, ""_sklearn_auto_wrap_output_keys"", set())
    return (
        hasattr(estimator, ""get_feature_names_out"")
        and ""transform"" in auto_wrap_output_keys
    )
```

</br>

 To have **'get_feature_names_out'** attr, estimator should inherit OneToOneFeatureMixin as I think. [(base.py)](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/base.py)

```python
class OneToOneFeatureMixin:
    """"""Provides `get_feature_names_out` for simple transformers.

    This mixin assumes there's a 1-to-1 correspondence between input features
    and output features, such as :class:`~preprocessing.StandardScaler`.
    """"""

    def get_feature_names_out(self, input_features=None):
        """"""Get output feature names for transformation.

        Parameters
        ----------
        input_features : array-like of str or None, default=None
            Input features.

            - If `input_features` is `None`, then `feature_names_in_` is
              used as feature names in. If `feature_names_in_` is not defined,
              then the following input feature names are generated:
              `[""x0"", ""x1"", ..., ""x(n_features_in_ - 1)""]`.
            - If `input_features` is an array-like, then `input_features` must
              match `feature_names_in_` if `feature_names_in_` is defined.

        Returns
        -------
        feature_names_out : ndarray of str objects
            Same as input features.
        """"""
        check_is_fitted(self, ""n_features_in_"")
        return _check_feature_names_in(self, input_features)
```

</br>

I want to know that it is kind of a bug, or the document says wrong information.

### Steps/Code to Reproduce

```python
from sklearn.preprocessing import LabelEncoder

LabelEncoder().set_output()
```

### Expected Results

No error is thrown

### Actual Results

```python
--------------------------------------------------------------------------------
AttributeError                                 Traceback (most recent call last)                        
/tmp/ipykernel_16765/3091610596.py in <module>
---> 1 LabelEncoder().set_output()

~/.local/lib/python3.8/site-packages/sklearn/utils/_available_if.py in __get__(self, obj, owner)
      31        # this is to allow access to the docstrings.
      32        if not self.check(obj):
--->  33            raise attr_err
      34        out = MethodType(self.fn, obj)
      35
AttributeError: This 'LabelEncoder' has no attribute 'set_output'
```
            
             
            

### Versions

```shell
1.2.2
```
","['Bug', 'Documentation']","2023-06-27T07:08:20Z","10","6","https://github.com/scikit-learn/scikit-learn/issues/26711","Syntax Error"
"76","scikit-learn/scikit-learn","Automatic bandwidth calculation valid only for normalized data","### Describe the bug

`sklearn.neighbors.KernelDensity` supports automatic (optimal) bandwidth calculation via `bandwidth = 'silverman'` and `bandwidth = 'scott'`. The algorithm computes the appropriate observation-weighted bandwidth factors (proportional to nobs^0.2) but does not adjust for the standard deviation or interquartile range of the dataset. Roughly, the algorithm should scale the dataset's standard error by the algorithmic bandwidth factors.

See, e.g., [Wikipedia](https://en.wikipedia.org/wiki/Kernel_density_estimation#A_rule-of-thumb_bandwidth_estimator). The implementation in `scipy.stats._kde` is correct.

### Steps/Code to Reproduce
```python
import matplotlib.pyplot as plot
import numpy as np
from sklearn.neighbors import KernelDensity
from scipy.stats import gaussian_kde

data = np.random.normal( scale = 0.01, size = 100 )

#
# 1. sklearn (auto)
#
kd_sklearn_auto = KernelDensity( kernel = 'gaussian', bandwidth = 'silverman' )
kd_sklearn_auto.fit( np.reshape( data, ( -1, 1 ) ) )

#
# 2. sklearn (manual)
#
kd_sklearn_manual = KernelDensity( kernel = 'gaussian', bandwidth = 0.9 * np.std( data ) / len( data ) ** ( 1 / 5 ) )
kd_sklearn_manual.fit( np.reshape( data, ( -1, 1 ) ) )

#
# 3. scipy
#
kd_scipy = gaussian_kde( data, bw_method = 'silverman' )

#
# 4. show the difference
#
xs = np.arange( start = -0.05, stop = 0.05, step = 1e-4 )
plot.plot( xs, np.exp( kd_sklearn_auto.score_samples( np.reshape( xs, ( -1, 1 ) ) ) ), label = 'KDE SKLearn (auto)' )
plot.plot( xs, np.exp( kd_sklearn_manual.score_samples( np.reshape( xs, ( -1, 1 ) ) ) ), label = 'KDE SKLearn (manual)' )
plot.plot( xs, kd_scipy.pdf( xs ), label = 'KDE SciPy' )
plot.hist( data, label = 'Data' )
plot.legend()
plot.show()
```

### Expected Results

Automatic SKLearn bandwidth curve should approximately match SciPy bandwidth curve, roughly the shape of the underlying data histogram.

### Actual Results

Automatic SKLearn bandwidth curve generates a flat PDF.

### Versions

```shell
System:
    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]
executable: /local_disk0/.ephemeral_nfs/envs/pythonEnv-67c47d19-3f15-49a2-ab8f-bcf25a2bc29f/bin/python
   machine: Linux-5.15.0-1038-azure-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.2.2
          pip: 21.2.4
   setuptools: 58.0.4
        numpy: 1.20.3
        scipy: 1.7.1
       Cython: 0.29.24
       pandas: 1.3.4
   matplotlib: 3.4.3
       joblib: 1.2.0
threadpoolctl: 2.2.0

Built with OpenMP: True

threadpoolctl info:
       filepath: /databricks/python3/lib/python3.9/site-packages/numpy.libs/libopenblasp-r0-5bebc122.3.13.dev.so
         prefix: libopenblas
       user_api: blas
   internal_api: openblas
        version: 0.3.13.dev
    num_threads: 6
threading_layer: pthreads
   architecture: Haswell

       filepath: /local_disk0/.ephemeral_nfs/envs/pythonEnv-67c47d19-3f15-49a2-ab8f-bcf25a2bc29f/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
         prefix: libgomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 6

       filepath: /databricks/python3/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-085ca80a.3.9.so
         prefix: libopenblas
       user_api: blas
   internal_api: openblas
        version: 0.3.9
    num_threads: 6
threading_layer: pthreads
   architecture: Haswell
```
","['Bug']","2023-06-21T21:41:09Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/26658","Syntax Error"
"77","scikit-learn/scikit-learn","TransformedTargetRegressor forces 1d y shape to regressor","### Describe the bug

I experience the following error when using TransformedTargetRegressor with my skorch model:
ValueError: The target data shouldn't be 1-dimensional but instead have 2 dimensions, with the second dimension having the same size as the number of regression targets (usually 1). Please reshape your target data to be 2-dimensional (e.g. y = y.reshape(-1, 1).

#### After checking the Source Code this lead me the the following unexpected behaivor which makes little sense:

If TransformedTargetRegressor is fitted with with a 2d dimensional y, it will still be transformed to a 1d dimensional output

y should have the same input and output shapes with a TransformedTargetRegressor or there should be an init argument to disable the change of the input shape
(Yes, internally it gets casted to 2d, but Im talking about the In and Outputs) 

https://github.com/scikit-learn/scikit-learn/blob/364c77e04/sklearn/compose/_target.py#L20
TransformedTargetRegressor-->fit

```python
        if y.ndim == 1:
            y_2d = y.reshape(-1, 1)
        else:
            y_2d = y
        self._fit_transformer(y_2d)

[...]

        if y_trans.ndim == 2 and y_trans.shape[1] == 1:
            y_trans = y_trans.squeeze(axis=1)
```
But in the end we squeeze it back into a 1d which causes issues for models which expect a 2d input of y
y was 2d in the beginning for a reason

### **The following code would solve this:**
```
        if y_trans.ndim == 2 and y_trans.shape[1] == 1 and y.ndim==1:  #only squeeze back to 1d if y is 1d
            y_trans = y_trans.squeeze(axis=1)
```

This could only create an issue where the y input was for some reason 2d but should be 1d for the regressor. 
In this case an attribute would be nice
```
        if y_trans.ndim == 2 and y_trans.shape[1] == 1 and self.output_dim == 1:
            y_trans = y_trans.squeeze(axis=1)
```

Also in TransformedTargetRegressor-->predict the results dont get squeezed after the prediction of the estimator - only if the original input shape was 1, in that case it is squeezed

So the result looks as expected, but only if the regressor takes a 1d y
If the estimator expects a 2d y the code fails

### Steps/Code to Reproduce

```python
regressor = TransformedTargetRegressor(
    transformer=MinMaxScaler()
)
X, y = np.random.rand(10, 10), np.expand_dims(np.random.rand(10), 1)
regressor.fit(X, y)
```

### Expected Results

The shape of y stays the same as the input OR there is a attribute which allows the choice of  (1d or original)  or (1d or 2d)

input | internal | output
2d > 2d > 2d
1d > 2d > 1d

### Actual Results

the regressor gets just a 1d array even through y was specifically set to 2d
(I don't know how to extract these results without an debugger)

It works for this example because the default regressor is used, but when using it with other models they might need the 2nd dimention of y, because it was specifically reshaped  (-1,1)

input | internal | output
2d > 2d > 1d    THIS creates issues for the regressive which is passed to the Transformer if it expects a 2d array because a 2d y was given 
1d > 2d > 1d

### Versions

```shell
System:
    python: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]
executable: /anaconda/envs/azureml_py310_sdkv2/bin/python
   machine: Linux-5.15.0-1017-azure-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.1.3
          pip: 22.1.2
   setuptools: 61.2.0
        numpy: 1.23.2
        scipy: 1.9.0
       Cython: None
       pandas: 1.4.3
   matplotlib: 3.6.2
       joblib: 1.2.0
threadpoolctl: 3.1.0
```
","['Bug']","2023-06-07T13:58:14Z","9","0","https://github.com/scikit-learn/scikit-learn/issues/26530","Syntax Error"
"78","scikit-learn/scikit-learn","Inconsitency between C-contiguous and F-contiguous arrays","### No consistency between C-contiguous and F-contiguous arrays for LinearRegression()

At least for LinearRegression() : In some edge case (when X is almost singular), there is huge difference between C-contiguous and F-contiguous arrays predictions.

- This is due to the fact that array product gives different results between contiguous and F-contiguous arrays ([cf this Stack Overflow questions that I posted](https://stackoverflow.com/questions/76388886/python-rounding-errors-between-c-contiguous-and-f-contiguous-arrays-for-matrix))
- These ""edge cases"" can actually be quite common in time-series predictions, where a lot of auto-regressive features can easily be correlated
- I would strongly advise parsing all arrays to C-contiguous before doing the predictions/fitting.
- Please also note that **fitting** with F-contiguous or C-contiguous can also give different results.
- The worst is not that this is happening, it is that no warning are being raised whatsoever.
- Also, F-contiguous arrays are extremely common in pandas DataFrames, which is what a lot of developers are using in this context...

### Steps/Code to Reproduce

```python
import numpy as np; print(np.__version__) # 1.23.5
import scipy; print(scipy.__version__) # 1.10.0
import sklearn as sk; print(sk.__version__) # 1.2.1

from sklearn.linear_model import LinearRegression
import pandas as pd

# Parameters 
seed, N_obs, N_feat, mu_x, sigma_x, mu_y, sigma_y = 0, 100, 1000, 100, 0.1, 100, 1

# 1) Creating a weird edge-case X, y :
np.random.seed(seed)
s = pd.Series(np.random.normal(mu_x, sigma_x, N_obs))
X = np.stack([s.ewm(com=com).mean() for com in np.arange(N_feat)]).T
y = np.random.normal(mu_y, sigma_y, N_obs)

# 2) Showing that there is different results for C-cont vs F-cont arrays :
model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X)
y_pred_c = model.predict(np.ascontiguousarray(X))

# Either just plot it and see :
import matplotlib.pyplot as plt
plt.scatter(y_pred, y_pred_c)

# Or look at the data :
np.var(y_pred)
np.var(y_pred - y_pred_c)
np.corrcoef(y_pred, y_pred_c)[0,1] # == 0.40295584536349216
# --> y_pred EXTREMELY different than y_pred_c
```

### Expected Results

We expect y_pred to be fully equal to y_pred_c.
Or at least `np.corrcoef(y_pred, y_pred_c)[0,1] > .99`


### Actual Results

`np.corrcoef(y_pred, y_pred_c)[0,1] # == 0.40295584536349216`
- ypred and y_pred_c are totally different.

### Versions

```shell
System:
    python: 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0]
executable: /opt/anaconda3/bin/python
   machine: Linux-5.10.0-23-cloud-amd64-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.2.1
          pip: 22.3.1
   setuptools: 65.6.3
        numpy: 1.23.5
        scipy: 1.10.0
       Cython: None
       pandas: 1.5.3
   matplotlib: 3.7.0
       joblib: 1.1.1
threadpoolctl: 2.2.0

Built with OpenMP: True

threadpoolctl info:
       filepath: /opt/anaconda3/lib/libmkl_rt.so.1
         prefix: libmkl_rt
       user_api: blas
   internal_api: mkl
        version: 2021.4-Product
    num_threads: 64
threading_layer: intel

       filepath: /opt/anaconda3/lib/libiomp5.so
         prefix: libiomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 128

       filepath: /opt/anaconda3/lib/libgomp.so.1.0.0
         prefix: libgomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 128
```
","['Bug']","2023-06-02T15:05:37Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/26493","Syntax Error"
"79","scikit-learn/scikit-learn","Numpy Array Error when Training LogisticRegressionCV","### Describe the bug

When I attempt to train LogisticRegressionCV, I get the error: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.

The inputs to LogisticRegressionCV are:
- X = output of StandardScaler.fit_transform, with a shape of (24, 12) and dtype of float64
- y = array with shape (24, ) and dtype of int64

I checked that there are not null or infinite values in either array.

### Steps/Code to Reproduce

```
data_for_color_training = training_data.loc[training_data[""IdentCode""] == color]
x_train = data_for_color_training.loc[:, (data_for_color_training.columns.str.startswith(""dx"") | data_for_color_training.columns.str.startswith(""dy"") | data_for_color_training.columns.str.startswith(""dz""))]
y_train = data_for_color_training[""Grade""]
			
#--------------Edit specific pre-processing and model here-----------------------------
scaler = StandardScaler()
X_train = scaler.fit_transform(x_train)
modelTrained = LogisticRegressionCV(max_iter=10000).fit(X_train, y_train)
```

### Expected Results

The variable modelTrained will output a LogisticRegressionCV model.

### Actual Results
```traceback
Traceback (most recent call last):
  File ""C:\Users\...\PythonScripts\UserDefinedModel.py"", line 22, in train_predict_by_colorcode
    modelTrained = LogisticRegressionCV(max_iter=10000).fit(X_train, y_train)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_logistic.py"", line 1912, in fit
    coefs_paths = np.reshape(
                  ^^^^^^^^^^^
  File ""<__array_function__ internals>"", line 200, in reshape
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 298, in reshape
    return _wrapfunc(a, 'reshape', newshape, order=order)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 54, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 43, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
                     ^^^^^^^^^^^^
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 298, in reshape
    return _wrapfunc(a, 'reshape', newshape, order=order)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 54, in _wrapfunc
    return _wrapit(obj, method, *args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\core\fromnumeric.py"", line 43, in _wrapit
    result = getattr(asarray(obj), method)(*args, **kwds)
                     ^^^^^^^^^^^^
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (5, 10) + inhomogeneous part.
```
### Versions

```shell
System:
    python: 3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]
executable: C:\Users\...\AppData\Local\Programs\Python\Python311\python.exe
   machine: Windows-10-10.0.19044-SP0

Python dependencies:
      sklearn: 1.2.2
          pip: 22.3.1
   setuptools: 65.5.0
        numpy: 1.24.1
        scipy: 1.10.0
       Cython: None
       pandas: 1.5.3
   matplotlib: 3.6.3
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\numpy\.libs\libopenblas64__v0.3.21-gcc_10_3_0.dll
        version: 0.3.21
threading_layer: pthreads
   architecture: Haswell
    num_threads: 16

       user_api: openmp
internal_api: openmp
         prefix: vcomp
       filepath: C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None
    num_threads: 16

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\Users\...\AppData\Local\Programs\Python\Python311\Lib\site-packages\scipy.libs\libopenblas-802f9ed1179cb9c9b03d67ff79f48187.dll
        version: 0.3.18
threading_layer: pthreads
   architecture: Haswell
    num_threads: 16
```
","['Bug', 'module:linear_model']","2023-05-19T02:51:59Z","11","0","https://github.com/scikit-learn/scikit-learn/issues/26401","Syntax Error"
"80","scikit-learn/scikit-learn","SplineTransformer(extrapolate=""periodic"") outputs nan values for constant features","While reviewing #24145 I discovered the following bug:

```python
In [1]: import numpy as np

In [2]: from sklearn.preprocessing import SplineTransformer

In [3]: SplineTransformer(extrapolation=""periodic"").fit_transform(np.ones(shape=(5, 1)))
Out[3]: 
array([[nan, nan, nan, nan],
       [nan, nan, nan, nan],
       [nan, nan, nan, nan],
       [nan, nan, nan, nan],
       [nan, nan, nan, nan]])
```

Batman!

This is caused by:

- https://github.com/scikit-learn/scikit-learn/blob/ea046f024694b9a558c882b8c2610c52dad95e29/sklearn/preprocessing/_polynomial.py#L962-L971

`spl.t[n] - spl.t[spl.k]` is typically zero for constant features.

While fixing it for constant features is probably easy (just use `x = X[:, i]`), I wonder if other related numerical stability problems can be triggered for nearly constant features.

But maybe we can solve this problem in two stages: first the nan problem caused by modulus by exact zero and then investigate behavior on nearly constant data.
","['Bug']","2023-05-17T13:56:20Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/26390","Logical Bug"
"81","scikit-learn/scikit-learn","SequentialFeatureSelector in backward auto mode will always remove one feature","https://github.com/scikit-learn/scikit-learn/blob/364c77e047ca08a95862becf40a04fe9d4cd2c98/sklearn/feature_selection/_sequential.py#L273

The initial value of `old_score` is incorrect if `direction == 'backward'`. With the current initial value of `-np.inf, ((new_score - old_score) < self.tol)` would always be `False` no matter what `new_score` is returned by `self._get_best_new_feature_score()`, so that at least one feature would be removed in the first iteration.

`old_score` needs to be set to an initial value that is the `cross_val_score` with all the features in use if `direction == 'backward'`.","['Bug', 'module:feature_selection']","2023-05-14T19:30:13Z","2","1","https://github.com/scikit-learn/scikit-learn/issues/26369","Logical Bug"
"82","scikit-learn/scikit-learn","Duality gap computation in covariance.GraphicalLasso yields negative values.","### Describe the bug

The computation of the duality gap in `_dual_gap(emp_cov, precision_, alpha)` of `GraphicalLasso` uses the definition from `Duchi et al., 2012`. 
However, their duality gap is expressed given a _feasible_ dual variable. In the implementation, it is applied to a primal variable that doesn't necessarily satisfy the dual variable's feasibility constraints.
This results in potentially negative values for the duality gap.

The primal problem reads

$$P(\Theta) = \min_\Theta \varphi(\Theta) + \nu(\Theta)$$
with
$$\varphi(\Theta) =  -\log \det (\Theta)  + \langle S, \Theta \rangle \text{ and } \nu(\Theta) =  \Vert \Lambda \odot \Theta \Vert_1$$
The Fenchel-Rockafellar dual problem is then given by

$$D(W) = \min_{W}  \varphi^{\star} (W) + \nu^{\star} (-W)$$
$$= \min_{W} -\log \det(S - W) - p \quad \text{s.t. } \quad \vert W_{ij} \vert < \lambda_{ij}$$
We can then compute the duality gap as follows:
$$G(\Theta, W) = P(\Theta) + D(W)$$
Only for a *feasible* $W$ can we actually take $\theta = (S - W)^{-1}$ and thus simplify the duality gap expression and fall back on the implemented formula:
$$G(\Theta) = \langle S, \Theta \rangle + \Vert \Lambda \odot \Theta \Vert_1 - p$$ 
@mathurinm 

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.covariance import GraphicalLasso

np.random.seed(0)
X = np.random.randn(100, 50)
emp_cov = X.T @ X / len(X)

clf = GraphicalLasso(verbose=True, tol=1e-20).fit(emp_cov)
```

### Expected Results

The expected results would be positive values for the duality gap at all iterations.

### Actual Results

Here are the duality gaps for the first 4 iterations : 
```
[graphical_lasso] Iteration   0, cost  5.24e+01, dual gap 6.745e-01
[graphical_lasso] Iteration   1, cost  5.24e+01, dual gap -3.808e-05
[graphical_lasso] Iteration   2, cost  5.24e+01, dual gap -4.379e-09
[graphical_lasso] Iteration   3, cost  5.24e+01, dual gap 5.049e-09

```

### Versions

```shell
System:
    python: 3.10.6 (main, Oct 24 2022, 16:07:47) [GCC 11.2.0]
executable: /home/cpouliquen/anaconda3/envs/these-can/bin/python3.10
   machine: Linux-5.4.0-148-generic-x86_64-with-glibc2.27

Python dependencies:
          pip: 22.2.2
   setuptools: 64.0.2
      sklearn: 1.0.2
        numpy: 1.22.3
        scipy: 1.8.1
       Cython: 0.29.33
       pandas: 1.5.1
   matplotlib: 3.5.3
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True
```
```
","['Bug', 'module:covariance']","2023-05-03T14:53:33Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/26321","Logical Bug"
"83","scikit-learn/scikit-learn","Cloned estimators have identical randomness but different RNG instances","### Describe the bug

Cloned estimators have identical randomness but different RNG instances. According to documentation, it should be the other way around: different randomness but identical RNG instances.

Related #25395 

The User Guide [says](https://scikit-learn.org/stable/common_pitfalls.html#controlling-randomness):
> For an optimal robustness of cross-validation (CV) results, pass RandomState instances when creating estimators

> ```python
> rf_inst = RandomForestClassifier(random_state=np.random.RandomState(0))
> cross_val_score(rf_inst, X, y)
> ```
> ...
> Since `rf_inst` was passed a `RandomState` instance, each call to `fit` starts from a different RNG. As a result, the random subset of features will be different for each folds

In regards to cloning, the same reference says:
> ```python
> rng = np.random.RandomState(0)
> a = RandomForestClassifier(random_state=rng)
> b = clone(a)
> ```
> Moreover, `a` and `b` will influence each-other since they share the same internal RNG: calling `a.fit` will consume `b`s RNG, and calling `b.fit` will consume `a`s RNG, since they are the same. 

The actual behaviour does not follow this description. 

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn import clone
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_validate
from sklearn.ensemble import RandomForestClassifier

rng = np.random.RandomState(0)
X, y = make_classification(random_state=rng)
rf = RandomForestClassifier(random_state=rng)

d = cross_validate(rf, X, y, return_estimator=True, cv=2)
rngs = [e.random_state for e in d['estimator']]
# estimators corresponding to different CV runs have different but identical RNGs:
print(rngs[0] is rngs[1]) # False
print(all(rngs[0].randint(10, size=10) == rngs[1].randint(10, size=10))) # True

rf_clone = clone(rf)
rngs = [rf.random_state, rf_clone.random_state]
print(rngs[0] is rngs[1]) # False
print(all(rngs[0].randint(10, size=10) == rngs[1].randint(10, size=10))) # True
```

### Expected Results

True False True False

### Actual Results

False True False True

### Versions

```shell
Tested on a two-week-old dev build and also on the following version (Kaggle)

System:
    python: 3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:53)  [GCC 9.4.0]
executable: /opt/conda/bin/python3.7
   machine: Linux-5.15.90+-x86_64-with-debian-bullseye-sid

Python dependencies:
          pip: 22.3.1
   setuptools: 59.8.0
      sklearn: 1.0.2
        numpy: 1.21.6
        scipy: 1.7.3
       Cython: 0.29.34
       pandas: 1.3.5
   matplotlib: 3.5.3
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True
```
","['Bug', 'Documentation', 'Needs Decision']","2023-04-11T16:36:07Z","25","0","https://github.com/scikit-learn/scikit-learn/issues/26148","Logical Bug"
"84","scikit-learn/scikit-learn","RandomForest not passing feature names to trees and creating warnings.","### Describe the bug

I fit a decision forest with training data that includes feature names. When I call predict_proba on the forest everything is fine. When I call rf.estimators_[0].predict_proba it will warn that it was not trained with feature names.


### Steps/Code to Reproduce
```python
import numpy as np
from sklearn.ensemble import RandomForestClassifier

print(sklearn.__version__)

data = np.random.normal([1,2,3,4,5,6,7,8,9,10], size=(1000, 10))
feature_names = [f'F{i}' for i in range(10)]
df = pd.DataFrame(data=data, columns=feature_names)
y = np.ones(1000)
y[500:] = 0

rf = RandomForestClassifier()
rf.fit(df, y)

print(f""Feature names in Forest: {rf.feature_names_in_} "")

print('Classing pred proba on forest')
rf.predict_proba(df)
print('Done calling pred proba on forest')

# THIS GIVES A WARNING
rf.estimators_[0].predict_proba(df)
```

### Expected Results

No warnings

### Actual Results
```
Feature names in Forest: ['F0' 'F1' 'F2' 'F3' 'F4' 'F5' 'F6' 'F7' 'F8' 'F9'] 
Classing pred proba on forest
Done calling pred proba on forest
.....lib/python3.8/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names
  warnings.warn(
```
### Versions

```shell
sk>>> sklearn.show_versions()

System:
    python: 3.8.16 (default, Mar  2 2023, 03:21:46)  [GCC 11.2.0]
executable: ....../bin/python
   machine: Linux-5.15.0-60-generic-x86_64-with-glibc2.17

Python dependencies:
      sklearn: 1.2.2
          pip: 23.0.1
   setuptools: 65.6.3
        numpy: 1.23.1
        scipy: 1.9.0
       Cython: None
       pandas: 1.5.3
   matplotlib: None
       joblib: 1.1.1
threadpoolctl: 2.2.0
```
","['Bug', 'Moderate', 'module:ensemble']","2023-04-10T22:29:00Z","16","0","https://github.com/scikit-learn/scikit-learn/issues/26140","Logical Bug"
"85","scikit-learn/scikit-learn","1.2.1: cannot build documentation without installing module","### Describe the bug

Looks like something is wrong and I cannot build docuemtation without installing module.


### Steps/Code to Reproduce

N/A

### Expected Results

It should be possible to build documentation out of only what is in source tree a d withoit have installed module.

### Actual Results

<details>

```console
++ ls -1d lib.linux-x86_64-cpython-38
+ PYTHONPATH=/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/build/lib.linux-x86_64-cpython-38
+ PBR_VERSION=1.2.1
+ PDM_PEP517_SCM_VERSION=1.2.1
+ SETUPTOOLS_SCM_PRETEND_VERSION=1.2.1
+ /usr/bin/sphinx-build -n -T -b man doc build/sphinx/man
Running Sphinx v6.1.3

Traceback (most recent call last):
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build/__init__.py"", line 48, in <module>
    from ._check_build import check_build  # noqa
ModuleNotFoundError: No module named 'sklearn.__check_build._check_build'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/config.py"", line 351, in eval_config_file
    exec(code, namespace)  # NoQA: S102
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/doc/conf.py"", line 20, in <module>
    from sklearn.externals._packaging.version import parse
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__init__.py"", line 81, in <module>
    from . import __check_build  # noqa: F401
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build/__init__.py"", line 50, in <module>
    raise_build_error(e)
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build/__init__.py"", line 31, in raise_build_error
    raise ImportError(
ImportError: No module named 'sklearn.__check_build._check_build'
___________________________________________________________________________
Contents of /home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build:
__init__.py               _check_build.pyx          _check_build.c
__pycache__
___________________________________________________________________________
It seems that scikit-learn has not been built correctly.

If you have installed scikit-learn from source, please do not forget
to build the package before using it: run `python setup.py install` or
`make` in the source directory.

If you have used an installer, please check that it is suited for your
Python version, your operating system and your platform.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/cmd/build.py"", line 279, in build_main
    app = Sphinx(args.sourcedir, args.confdir, args.outputdir,
  File ""/usr/lib/python3.8/site-packages/sphinx/application.py"", line 202, in __init__
    self.config = Config.read(self.confdir, confoverrides or {}, self.tags)
  File ""/usr/lib/python3.8/site-packages/sphinx/config.py"", line 173, in read
    namespace = eval_config_file(filename, tags)
  File ""/usr/lib/python3.8/site-packages/sphinx/config.py"", line 364, in eval_config_file
    raise ConfigError(msg % traceback.format_exc()) from exc
sphinx.errors.ConfigError: There is a programmable error in your configuration file:

Traceback (most recent call last):
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build/__init__.py"", line 48, in <module>
    from ._check_build import check_build  # noqa
ModuleNotFoundError: No module named 'sklearn.__check_build._check_build'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/config.py"", line 351, in eval_config_file
    exec(code, namespace)  # NoQA: S102
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/doc/conf.py"", line 20, in <module>
    from sklearn.externals._packaging.version import parse
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__init__.py"", line 81, in <module>
    from . import __check_build  # noqa: F401
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build/__init__.py"", line 50, in <module>
    raise_build_error(e)
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build/__init__.py"", line 31, in raise_build_error
    raise ImportError(
ImportError: No module named 'sklearn.__check_build._check_build'
___________________________________________________________________________
Contents of /home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build:
__init__.py               _check_build.pyx          _check_build.c
__pycache__
___________________________________________________________________________
It seems that scikit-learn has not been built correctly.

If you have installed scikit-learn from source, please do not forget
to build the package before using it: run `python setup.py install` or
`make` in the source directory.

If you have used an installer, please check that it is suited for your
Python version, your operating system and your platform.


Configuration error:
There is a programmable error in your configuration file:

Traceback (most recent call last):
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build/__init__.py"", line 48, in <module>
    from ._check_build import check_build  # noqa
ModuleNotFoundError: No module named 'sklearn.__check_build._check_build'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/config.py"", line 351, in eval_config_file
    exec(code, namespace)  # NoQA: S102
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/doc/conf.py"", line 20, in <module>
    from sklearn.externals._packaging.version import parse
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__init__.py"", line 81, in <module>
    from . import __check_build  # noqa: F401
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build/__init__.py"", line 50, in <module>
    raise_build_error(e)
  File ""/home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build/__init__.py"", line 31, in raise_build_error
    raise ImportError(
ImportError: No module named 'sklearn.__check_build._check_build'
___________________________________________________________________________
Contents of /home/tkloczko/rpmbuild/BUILD/scikit-learn-1.2.1/sklearn/__check_build:
__init__.py               _check_build.pyx          _check_build.c
__pycache__
___________________________________________________________________________
It seems that scikit-learn has not been built correctly.

If you have installed scikit-learn from source, please do not forget
to build the package before using it: run `python setup.py install` or
`make` in the source directory.

If you have used an installer, please check that it is suited for your
Python version, your operating system and your platform.
```
</details>

### Versions

```shell
1.2.1
```
","['Bug', 'Documentation', 'Needs Decision - Include Feature']","2023-03-06T13:10:05Z","22","0","https://github.com/scikit-learn/scikit-learn/issues/25766","Dependency Issue"
"86","scikit-learn/scikit-learn","Differences in scalar vs vectorized predictions with `GaussianProcessRegressor`","### Describe the bug

I would expect that calling `GaussianProcessRegressor.predict(X)` with a single X matrix, or with repeated scalar evaluations rows of X should would give nearly the same result, but they don't.

An example is given below.

### Steps/Code to Reproduce

```python
import numpy as np
import pandas as pd
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import DotProduct, RBF
from sklearn.model_selection import train_test_split


data_url = ""http://lib.stat.cmu.edu/datasets/boston""
raw_df = pd.read_csv(data_url, sep=""\s+"", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]


def evaluate_gpr_vec_and_scalar(gpr, X):
    res_vector = gpr.predict(X).squeeze()
    res_scalar = np.array([gpr.predict(xi.reshape(1, -1)) for xi in X]).squeeze()
    return res_scalar, res_vector

# load/split data
X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=0)

# train
gpr = GaussianProcessRegressor(DotProduct() + RBF(), alpha=1.)
gpr.fit(X_train, y_train)

# predict scalar and vector
pred_scalar, pred_vector = evaluate_gpr_vec_and_scalar(gpr, X_test)
err = pred_scalar - pred_vector
print(err.max())
```

### Expected Results

I would expect the error to be 0 or small - perhaps around machine `eps`, if anything.

### Actual Results

`2.1609594114124775e-08`


### Versions

```shell
System:
    python: 3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:41:54) [Clang 13.0.1 ]
executable: /Users/phil/miniconda3/envs/skl/bin/python
   machine: macOS-12.5.1-x86_64-i386-64bit
Python dependencies:
      sklearn: 1.1.1
          pip: 22.3
   setuptools: 65.5.0
        numpy: 1.23.4
        scipy: 1.9.3
       Cython: None
       pandas: 1.5.1
   matplotlib: 3.6.1
       joblib: 1.2.0
threadpoolctl: 3.1.0
Built with OpenMP: True
threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/phil/miniconda3/envs/skl/lib/libopenblasp-r0.3.21.dylib
        version: 0.3.21
threading_layer: openmp
   architecture: Nehalem
    num_threads: 8
       user_api: openmp
   internal_api: openmp
         prefix: libomp
       filepath: /Users/phil/miniconda3/envs/skl/lib/libomp.dylib
        version: None
    num_threads: 8
```
","['Bug', 'Needs Investigation', 'Numerical Stability']","2023-03-03T01:20:51Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/25750","Syntax Error"
"87","scikit-learn/scikit-learn","Is the check of strict convergence in KMeans too expensive for the benefits ?","### Describe the bug

In `KMeans` scikit-learn defines [`strict_convergence`](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/cluster/_kmeans.py#L701) as the event of producing the same label assignments at two successive iterations.

When this happens, it means convergence for both labels and centroids (set aside possible oscillations due to numerical instability, were the iterations to continue).

But checking for strict convergence seems to be somewhat expensive (one loop over the last two label assignments of each sample per iteration), and if the user properly set `tol` it doesn't seem necessary at all ?

Checking for strict convergence seems to really help when `tol==0`. With `tol==0` I've seen cases of endless oscillations around 0 because of numerical instability, but never reaching 0, and finally terminating at `max_iter` iterations.

For the general case, isn't it detrimental to performance though ? one can expect the performance cost to be significant for small dimensions of data, for which an additional pass on a column is marginally more expensive.

So I would maybe suggest the following improvements:

- [ ] enable automatically the strict convergence checks only if `tol==0` (or if `tol` is ""very small"")
- [ ] maybe expose to the user the choice of enabling strict convergence at each iteration ?


### Versions

```shell
1.3
```
","['Bug', 'module:cluster', 'Needs Investigation']","2023-02-27T15:53:14Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/25716","Performance Issue"
"88","scikit-learn/scikit-learn","SequentialFeatureSelector is not working with ColumnTransformer","### Describe the bug

Please see the code.

### Steps/Code to Reproduce

```python
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

import pandas as pd
import numpy as np


# dummy data
N = 100
dummy_x = pd.DataFrame(
    np.random.randn(N,3),
    columns = list('abc'),
)

dummy_y = pd.DataFrame(
    np.random.choice([0,1], size= (N,1)),
    columns = ['label'],
)


num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy=""median"")),
        ('std_scaler', StandardScaler()),
    ])

ct_parts = [
                ('num', num_pipeline, [0,1,2]),
]

data_preparation_pipe = ColumnTransformer(ct_parts, remainder='passthrough')


from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.ensemble import GradientBoostingClassifier

model = Pipeline(
    [
        # ('data_prep', num_pipeline),
        ('data_prep', data_preparation_pipe),
        ('ML', GradientBoostingClassifier()),
    ]
)

sfs = SequentialFeatureSelector(
    model,
)

sfs.fit(dummy_x, dummy_y)
```




### Expected Results

No error

### Actual Results

```python-traceback
Traceback (most recent call last):
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3460, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""/tmp/ipykernel_3433582/3663298101.py"", line 1, in <module>
    sfs.fit(dummy_x, dummy_y)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/feature_selection/_sequential.py"", line 268, in fit
    new_feature_idx, new_score = self._get_best_new_feature_score(
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/feature_selection/_sequential.py"", line 299, in _get_best_new_feature_score
    scores[feature_idx] = cross_val_score(
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/model_selection/_validation.py"", line 515, in cross_val_score
    cv_results = cross_validate(
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/model_selection/_validation.py"", line 285, in cross_validate
    _warn_or_raise_about_fit_failures(results, error_score)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/model_selection/_validation.py"", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError: 
All the 5 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
5 fits failed with the following error:
Traceback (most recent call last):
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/utils/__init__.py"", line 416, in _get_column_indices
    idx = _safe_indexing(np.arange(n_columns), key)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/utils/__init__.py"", line 356, in _safe_indexing
    return _array_indexing(X, indices, indices_dtype, axis=axis)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/utils/__init__.py"", line 185, in _array_indexing
    return array[key] if axis == 0 else array[:, key]
IndexError: index 1 is out of bounds for axis 0 with size 1

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/model_selection/_validation.py"", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/pipeline.py"", line 401, in fit
    Xt = self._fit(X, y, **fit_params_steps)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/pipeline.py"", line 359, in _fit
    X, fitted_transformer = fit_transform_one_cached(
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/joblib/memory.py"", line 349, in __call__
    return self.func(*args, **kwargs)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/pipeline.py"", line 893, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/utils/_set_output.py"", line 142, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py"", line 724, in fit_transform
    self._validate_column_callables(X)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py"", line 426, in _validate_column_callables
    transformer_to_input_indices[name] = _get_column_indices(X, columns)
  File ""/software/anaconda3/envs/TOSC_ML/lib/python3.10/site-packages/sklearn/utils/__init__.py"", line 418, in _get_column_indices
    raise ValueError(
ValueError: all features must be in [0, 0] or [-1, 0]
```

### Versions

```shell
System:
    python: 3.10.9 | packaged by conda-forge | (main, Feb  2 2023, 20:20:04) [GCC 11.3.0]
executable: /software/anaconda3/envs/TOSC_ML/bin/python
   machine: Linux-4.18.0-305.3.1.el8.x86_64-x86_64-with-glibc2.28

Python dependencies:
      sklearn: 1.2.1
          pip: 23.0
   setuptools: 67.3.2
        numpy: 1.23.5
        scipy: 1.10.0
       Cython: None
       pandas: 1.5.3
   matplotlib: 3.7.0
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /software/anaconda3/envs/TOSC_ML/lib/libopenblasp-r0.3.21.so
        version: 0.3.21
threading_layer: pthreads
   architecture: Zen
    num_threads: 128

       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /software/anaconda3/envs/TOSC_ML/lib/libgomp.so.1.0.0
        version: None
    num_threads: 128
```
","['Bug', 'Moderate', 'help wanted']","2023-02-27T04:35:40Z","5","4","https://github.com/scikit-learn/scikit-learn/issues/25711","Syntax Error"
"89","scikit-learn/scikit-learn","Multioutput regressors raise ValueError when scoring with `multioutput=""raw_values""`","### Describe the bug

The goal of the `multioutput=""raw_values""` parameter in the regression metrics is to be able to inspect the individual scores of a multioutput metaestimator, but the `_score` function in `_validation.py` expects a number, not the array it actually outputs.

We should add an exception to take into account this scenario.

### Steps/Code to Reproduce

```python
from sklearn.datasets import make_regression
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.metrics import make_scorer, mean_absolute_error
from sklearn.model_selection import cross_validate
from sklearn.multioutput import MultiOutputRegressor

X, Y = make_regression(n_features=10, n_targets=2, random_state=0)
hist_gbdt = HistGradientBoostingRegressor(random_state=0)
model = MultiOutputRegressor(hist_gbdt) # RegressorChain fails as well
scoring = {""MO_MAE"": make_scorer(mean_absolute_error, multioutput=""raw_values"")}

cv_results = cross_validate(model, X, Y, scoring=scoring)
cv_results[""test_MO_MAE""]
```

### Expected Results

Array of shape (`n_outputs`, `n_splits`), in this case shape (2, 5) as using the default KFold cross-validation.

### Actual Results

```python-traceback
File ~/miniforge3/envs/joblib-benchmark/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:708, in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)
    705 result[""fit_error""] = None
    707 fit_time = time.time() - start_time
--> 708 test_scores = _score(estimator, X_test, y_test, scorer, error_score)
    709 score_time = time.time() - start_time - fit_time
    710 if return_train_score:

File ~/miniforge3/envs/joblib-benchmark/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:809, in _score(estimator, X_test, y_test, scorer, error_score)
    807                 score = score.item()
    808         if not isinstance(score, numbers.Number):
--> 809             raise ValueError(error_msg % (score, type(score), name))
    810         scores[name] = score
    811 else:  # scalar

ValueError: scoring must return a number, got [110.4080722   83.27384784] (<class 'numpy.ndarray'>) instead. (scorer=MO_MAE)
```

### Versions

```shell
System:
    python: 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03)  [GCC 11.3.0]
executable: /home/arturoamor/miniforge3/envs/joblib-benchmark/bin/python3.9
   machine: Linux-5.14.0-1057-oem-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.2.1
          pip: 23.0.1
   setuptools: 67.3.2
        numpy: 1.24.2
        scipy: 1.10.1
       Cython: None
       pandas: 1.5.3
   matplotlib: 3.7.0
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/arturoamor/miniforge3/envs/joblib-benchmark/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/arturoamor/miniforge3/envs/joblib-benchmark/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-15028c96.3.21.so
        version: 0.3.21
threading_layer: pthreads
   architecture: SkylakeX
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/arturoamor/miniforge3/envs/joblib-benchmark/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so
        version: 0.3.18
threading_layer: pthreads
   architecture: SkylakeX
    num_threads: 8
```
","['Bug', 'module:multioutput']","2023-02-23T11:09:26Z","2","1","https://github.com/scikit-learn/scikit-learn/issues/25666","Logical Bug"
"90","scikit-learn/scikit-learn","Inflated results on random-data with SVM","### Describe the bug

When trying to train/evaluate a support vector machine in scikit-learn, I am experiencing some unexpected behaviour and I am wondering whether I am doing something wrong or that this is a possible bug.

In a very specific subset of circumstances, namely:

- `LeaveOneOut() `is used as cross-validation procedure
- The SVM is used, with `probability = True` and a small `C`  such as` 0.01`
- The y labels are balanced (i.e. the mean of y is 0.5)

The results of the trained SVM are very good on randomly generated data - while they should be near chance. If the y labels are a bit different, or the SVM is swapped out for a ``LogisticRegression``, it gives expected results (Brier of 0.25, AUC near 0.5). 
But for the named circumstances, the Brier is roughly 0.10 - 0.15 and AUC > 0.9 if the y labels are balanced.

### Steps/Code to Reproduce
```python

from sklearn import svm
from sklearn.linear_model import LogisticRegression
import numpy as np
from sklearn.model_selection import GridSearchCV, StratifiedKFold, LeaveOneOut, KFold
from sklearn.metrics import roc_auc_score, brier_score_loss
from tqdm import tqdm
import pandas as pd


N = 20
N_FEATURES = 50


scores = []
for z in tqdm(range(500)):
    X = np.random.normal(0, 1, size=(N, N_FEATURES))
    y = np.random.binomial(1, 0.5, size=N)
    
    if z < 10:
        y = np.array([0, 1] * int(N/2))
        y = np.random.permutation(y)

    y_real, y_pred = [], []
    skf_outer = LeaveOneOut()
    for train_index, test_index in skf_outer.split(X, y):
        X_train, X_test = X[train_index], X[test_index, :]
        y_train, y_test = y[train_index], y[test_index]

        clf = svm.SVC(probability=True, C=0.01)

        clf.fit(X_train, y_train)
        predictions = clf.predict_proba(X_test)[:, 1]

        y_pred.extend(predictions)
        y_real.extend(y_test)

    scores.append([np.mean(y), 
                   brier_score_loss(np.array(y_real), np.array(y_pred)), 
                   roc_auc_score(np.array(y_real), np.array(y_pred))])

df_scores = pd.DataFrame(scores)
df_scores.columns = ['y_label', 'brier', 'auc']
df_scores['y_0.5'] = df_scores['y_label'] == 0.5
df_scores = df_scores.groupby(['y_0.5']).mean()
print(df_scores)
```

### Expected Results

I would expect that all results would be somewhat similar, with a Brier ~0.25 and AUC ~0.5.

### Actual Results

```
        y_label     brier       auc
y_0.5                              
False  0.514649  0.298204  0.216884
True   0.500000  0.159728  0.999080
```

Here, you can see that if the ``np.mean`` of the ``y_labels`` is 0.5, the results are actually really really good.
While the data is randomly generated for 500 times

### Versions

```shell
System:
    python: 3.8.15 (default, Nov 24 2022, 14:38:14) [MSC v.1916 64 bit (AMD64)]
executable: C:\ProgramData\Anaconda3\envs\test\python.exe
   machine: Windows-10-10.0.19044-SP0
Python dependencies:
      sklearn: 1.2.0
          pip: 22.2.2
   setuptools: 61.2.0
        numpy: 1.19.5
        scipy: 1.10.0
       Cython: 0.29.14
       pandas: 1.4.4
   matplotlib: 3.6.3
       joblib: 1.2.0
threadpoolctl: 2.2.0
Built with OpenMP: True
threadpoolctl info:
       filepath: C:\ProgramData\Anaconda3\envs\test\Library\bin\mkl_rt.1.dll
         prefix: mkl_rt
       user_api: blas
   internal_api: mkl
        version: 2021.4-Product
    num_threads: 8
threading_layer: intel
       filepath: C:\Users\manuser\AppData\Roaming\Python\Python38\site-packages\scipy.libs\libopenblas-802f9ed1179cb9c9b03d67ff79f48187.dll
         prefix: libopenblas
       user_api: blas
   internal_api: openblas
        version: 0.3.18
    num_threads: 16
threading_layer: pthreads
   architecture: Prescott
       filepath: C:\ProgramData\Anaconda3\envs\test\Lib\site-packages\sklearn\.libs\vcomp140.dll
         prefix: vcomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 8
       filepath: C:\ProgramData\Anaconda3\envs\test\Library\bin\libiomp5md.dll
         prefix: libiomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 8
       filepath: C:\Users\manuser\AppData\Roaming\Python\Python38\site-packages\mxnet\libopenblas.dll
         prefix: libopenblas
       user_api: blas
   internal_api: openblas
        version: None
    num_threads: 16
threading_layer: pthreads
   architecture: Prescott
       filepath: C:\ProgramData\Anaconda3\envs\test\Lib\site-packages\torch\lib\libiomp5md.dll
         prefix: libiomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 16
       filepath: C:\ProgramData\Anaconda3\envs\test\Lib\site-packages\torch\lib\libiompstubs5md.dll
         prefix: libiomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 1
```
","['Bug', 'module:svm', 'Needs Investigation']","2023-02-17T14:45:57Z","8","0","https://github.com/scikit-learn/scikit-learn/issues/25631","Logical Bug"
"91","scikit-learn/scikit-learn","KernelDensity incorrect handling of bandwidth","### Describe the bug

I was using kernel density estimator
https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html
using 'silverman' or 'scott' as the bandwidth argument. Then I found that the bandwidth automatically adjusted by the algorithm is independent of the actual scale of the dataset. In fact, I was shocked to find that the calculation of a bandwidth in https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/neighbors/_kde.py for 'silverman' and 'scott' does not check the scales of data at all. 

Suppose I fit the model `kde` to some 2D data `X` and get the bandwidth as `kde.bandwidth_`.
Next, I fit the model `kde` to the same 2D data `X` but with all elements multiplied by, say, 20 and get the bandwidth as `kde.bandwidth_`. 
I found that these two values of `kde.bandwidth_` are equal (it is calculated from the shape of `X`, see the source code). But obviously they should differ by a factor of 20 if the bandwidth is really computed in a truly adaptive manner.

For your reference, I want to mention that scipy's KDE https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html calculates the covariance of data to extract the scale of data. I think this is the right thing to do.

Note that if the bandwidth is incorrect, everything else is incorrect too, including probablities of samples, etc.

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.neighbors import KernelDensity
X = np.random.randn(1000, 2)
kde = KernelDensity(bandwidth='scott')
kde.fit(X)
print(kde.bandwidth_)

kde.fit(X * 20)
print(kde.bandwidth_)
```

### Expected Results

Different bandwidths for data sets with different scales.

### Actual Results

0.31622776601683794
0.31622776601683794

### Versions

```shell
1.2.1
```
","['Bug', 'module:neighbors']","2023-02-16T06:16:04Z","6","1","https://github.com/scikit-learn/scikit-learn/issues/25623","Logical Bug"
"92","scikit-learn/scikit-learn","KBinsDiscretizer creates wrong bins.","### Describe the bug

`KBinsDiscretizer` gives wrong bins and wrong transformed data, when the inputs contains only 2 distinct values, and `n_bins=3`. It only produces 1 bin, which is not expected.  The warning shows some bins are too small so they are merged.

Note that when setting `n_bins=2`, `KBinsDiscretizer` gives more reasonable bins and correct results. However, from the `bin_edges_`, it seems `n_bins=2` is handled separately.

### My guess about the cause
I think the root cause is inconsistent handling of `-inf` and `+inf` in fitting and transforming. 

In fitting, `-inf` and `+inf` are not considered when removing duplicated bin edges. But in transforming data, the first and last bin edges are replaces with `-inf` and `+inf`. Such differences obviously removes some necessary bins.

Taking the case I mentioned as a example, current implementation deduplicates bin edges from `[-1.0, -1.0, 0.0, 0.0]` to `[-1.0, 0.0]`.  Then in transforming, all real values are actually in one bin, since the bin edges are replaced with `-inf` and `+inf`.

A more reasonable conversion is replacing first and last bin edges with `-inf` and `+inf` in fitting, i.e., `[-inf, -1.0, 0.0, inf]`, then removing duplicated edges, which gives `[-inf, -1.0, 0.0, inf]`. This will give expected results.




### Steps/Code to Reproduce

```
from sklearn.preprocessing import KBinsDiscretizer
X = [[-1], [-1], [ 0], [ 0]]
discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')
discretizer.fit(X)

print(discretizer.bin_edges_)
Xt = discretizer.transform(X)
print(Xt)
```

### Expected Results

The last `print` should give 
```
[[0.]
 [0.]
 [1.]
 [1.]]
```
or 

```
[[1.]
 [1.]
 [2.]
 [2.]]
```
. It depends on how to implement it correctly.

### Actual Results

The last `print` gives
```
[[0.]
 [0.]
 [0.]
 [0.]]
```

### Versions

```shell
System:
    python: 3.8.16 (default, Jan 17 2023, 16:42:09)  [Clang 14.0.6 ]
executable: /opt/anaconda3/envs/sklearn/bin/python3
   machine: macOS-10.16-x86_64-i386-64bit

Python dependencies:
      sklearn: 1.2.1
          pip: 22.3.1
   setuptools: 65.6.3
        numpy: 1.24.2
        scipy: 1.10.0
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
         prefix: libomp
       filepath: /opt/anaconda3/envs/sklearn/lib/python3.8/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
    num_threads: 10

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /opt/anaconda3/envs/sklearn/lib/python3.8/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.21
threading_layer: pthreads
   architecture: Nehalem
    num_threads: 10

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /opt/anaconda3/envs/sklearn/lib/python3.8/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.18
threading_layer: pthreads
   architecture: Nehalem
    num_threads: 10
```
```
","['Bug', 'Needs Decision', 'module:preprocessing']","2023-02-13T02:51:44Z","11","0","https://github.com/scikit-learn/scikit-learn/issues/25594","Logical Bug"
"93","scikit-learn/scikit-learn","Importing BaseEstimator leads to unnecessary memory usage","### Describe the bug

Importing `BaseEstimator` from `sklearn.base` causes a cascade of imports that leads to unnecessary memory usage (500MiB of stuff at peak, see screenshot below).

![Screenshot from 2023-02-10 19-23-53](https://user-images.githubusercontent.com/1296726/218169206-d8e940e9-40ba-458d-8047-deacf2f30521.png)


### Steps/Code to Reproduce

```python
from sklearn.base import BaseEstimator
```

### Expected Results

I would expect such a basic class to import some utilities around, but nothing major.

### Actual Results

The code ends up importing a large amount of unnecessary and large modules, the full chain to some point is:
 - code -> `from sklearn.base import BaseEstimator`
 - sklearn/\_\_init\_\_.py:82 -> `from .base import clone`
 - sklearn/base.py:17 -> `from .utils import _IS_32BIT`
 - sklearn/utils/\_\_init\_\_.py:21 -> `from scipy.sparse import issparse` (This is the most useless import, as the issparse function is a oneliner that calls isinstance)
 - scipy/sparse/\_\_init\_\_.py:283 -> `from . import csgraph`
 - scipy/sparse/csgraph/\_\_init\_\_.py:185 -> `from ._laplacian import laplacian`
 - scipy/sparse/csgraph/_laplacian.py:7 -> `from scipy.sparse.linalg import LinearOperator`
 - scipy/sparse/linalg/\_\_init\_\_.py:120 -> `from ._isolve import *`
 - scipy/sparse/linalg/_isolve/\_\_init\_\_.py:4 -> `from .iterative import *`
 - scipy/sparse/linalg/_isolve/iterative.py:9 -> `from . import _iterative`

I guess the issue I am reporting is that importing a simple estimator base class should not cause so many unrelated imports and spike memory usage by doing so.

### Versions

```shell
System:
    python: 3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]
executable: /home/jjan/dev/sec-certs-page/virt/bin/python
   machine: Linux-6.0.12-arch1-1-x86_64-with-glibc2.36

Python dependencies:
      sklearn: 1.2.1
          pip: 23.0
   setuptools: 61.2.0
        numpy: 1.22.3
        scipy: 1.10.0
       Cython: None
       pandas: 1.4.2
   matplotlib: 3.5.1
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/jjan/dev/sec-certs-page/virt/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
    num_threads: 16

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/jjan/dev/sec-certs-page/virt/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-2f7c42d4.3.18.so
        version: 0.3.18
threading_layer: pthreads
   architecture: Zen
    num_threads: 16

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/jjan/dev/sec-certs-page/virt/lib/python3.10/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so
        version: 0.3.18
threading_layer: pthreads
   architecture: Zen
    num_threads: 16
```
","['Bug', 'Needs Investigation']","2023-02-10T18:42:11Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/25590","Performance Issue"
"94","scikit-learn/scikit-learn","set_output API do not preserve original dtypes for pandas","### Describe the bug

Following issue #24182,

When using the set_output with expected output to be a pandas' data frame, while converting tougher columns with different dtypes the output does not preserve the original dtype but the ""common type"" by numpy.

Possible workaround is to create transformer for each column, but it doesn't feel like the right approach.

In collaboration with @BenEfrati

### Steps/Code to Reproduce

```python
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer



X = pd.DataFrame({
                  ""age"": [1, 2, 4],
                  ""float"": [1.5, 2.5, None],
                  })
X[""age""] = X[""age""].astype(np.uint8)
X.dtypes

# age        uint8
# float    float64
# dtype: object

simple_imputer = SimpleImputer(strategy='constant', fill_value=1).set_output(transform=""pandas"")
X_trans_partial = simple_imputer.fit_transform(X)
X_trans_partial.dtypes

# age      float64
# float    float64
# dtype: object
```

### Expected Results

We'd expect the age column to keep being an unsigned 8-bit integer.

### Actual Results

the age column is now a 64-bit floating point.

### Versions

```shell
System:
    python: 3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]
executable: C:\New folder\venv\Scripts\python.exe
   machine: Windows-10-10.0.19044-SP0

Python dependencies:
      sklearn: 1.2.1
          pip: 23.0
   setuptools: 58.1.0
        numpy: 1.24.2
        scipy: 1.10.0
       Cython: None
       pandas: 1.5.3
   matplotlib: None
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\New folder\venv\Lib\site-packages\numpy\.libs\libopenblas64__v0.3.21-gcc_10_3_0.dll
        version: 0.3.21
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12

       user_api: openmp
   internal_api: openmp
         prefix: vcomp
       filepath: C:\New folder\venv\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None
    num_threads: 12

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\New folder\venv\Lib\site-packages\scipy.libs\libopenblas-802f9ed1179cb9c9b03d67ff79f48187.dll
        version: 0.3.18
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12
```
","['Bug', 'module:impute', 'Needs Decision - Include Feature']","2023-02-07T08:52:18Z","6","1","https://github.com/scikit-learn/scikit-learn/issues/25560","Logical Bug"
"95","scikit-learn/scikit-learn","Enable feature selectors to pass pandas DataFrame to estimator","### Describe the workflow you want to enable

When running SequentialFeatureSelector (or, presumably, other feature selection methods) with a pandas DataFrame input, the reduced-feature input is passed to the estimator as a numpy array. This seems inconsistent with the other transformers that successfully maintain the pandas indices.

My actual use case is the following: Select the best k features that complement a set of fixed features. I was going to implement it like this: (happy to take suggestions for alternatives).

```python
import pandas as pd
import numpy as np
import sklearn
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

X=pd.DataFrame(np.random.rand(10,3))
y=pd.Series(np.random.rand(10))

fixed_features=pd.DataFrame(np.random.rand(X.shape[0],2))
appender= FunctionTransformer(lambda X:pd.concat([X,fixed_features.reindex(X.index)],axis=1))
X.shape,appender.fit_transform(X).shape
```

    ((10, 3), (10, 5))


```python
LR_with_fixed=LR=Pipeline(steps=[
    ('append',appender),
    ('LR',LinearRegression())
])
SFS_with_fixed = SequentialFeatureSelector(
    LR_with_fixed,n_features_to_select=1, direction=""forward""
)
_=SFS_with_fixed.fit(X,y)
```

    ValueError: 
    All the 5 fits failed.
    .....
      File ""/tmp/ipykernel_275/897617596.py"", line 2, in <lambda>
        appender= FunctionTransformer(lambda X:pd.concat([X,fixed_features.reindex(X.index)],axis=1))
    AttributeError: 'numpy.ndarray' object has no attribute 'index'



### Describe your proposed solution

I believe that it should be possible to pass the reduced matrix to the estimator as a pandas DataFrame, and this would be consistent with the way other Transformers work currently.

### Describe alternatives you've considered, if relevant

An alternative that would suffice for my particular use case is to directly pass a set of fixed features as a parameter to the feature selector.

### Additional context

_No response_","['Bug']","2023-01-26T17:11:58Z","10","0","https://github.com/scikit-learn/scikit-learn/issues/25492","Logical Bug"
"96","scikit-learn/scikit-learn","SVC and OneClassSVM fails to fit or have wrong fitted attributes with null sample weights","### Describe the bug

*SVC().fit(X, y, w)* fails when the targets *y* are multiclass and the sample_weights *w* zero out one of the classes.
* Dense *X* produces incorrect arrays for *support_*, *n_support_*, and *dual_coef_* attributes, some with wrong dimension.
* Sparse *X* errors out when trying to construct sparse *dual_coef_* from incompatible arguments.

A warning is emitted (*e.g.*, ""class label 0 specified in weight is not found""), but it does not indicate that the arrays on the trained SVC object are incorrect.

Seems to be a case that was not tested by PR #14286.

### Workaround

Replace the zero weights (or negative weights) with very small values like 1e-16. 

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.svm import SVC

X = np.array([[0., 0.], [1., 0.], [0., 1.]])   # or sp.csr_matrix(...)
y = [0, 1, 2]
w = [0., 1., 1.]                               # class 0 has zero weight
clf = SVC().fit(X, y, w)
```

### Expected Results

The fitted attributes should be
```
classes_: [0 1 2]
support_: [1 2]
support_vectors_: [[1. 0.]
                   [0. 1.]]
n_support_: [0 1 1]
dual_coef_: [[0. 0. 0.]
             [0. 1. -1.]]
```
assuming the 'arbitrary' values in *dual_coef_* are set to zero.

### Actual Results

For dense *X*, the fitted attributes are actually
```
classes_: [0 1 2]
support_: [0 1]              <-- should be [1 2]
support_vectors_: [[1. 0.]
                   [0. 1.]]
n_support_: [1 1]            <-- should be [0 1 1]
dual_coef_: [[ 1. -1.]]      <-- should be [[0. 0. 0.] [0. 1. -1.]]
```
For sparse *X* it raises
```
ValueError: indices and data should have the same size
```
with traceback
```
sklearn/svm/_base.py:252, in fit(self, X, y, sample_weight)
--> 252 fit(X, y, sample_weight, solver_type, kernel, random_seed=seed)

sklearn/svm/_base.py:413, in _sparse_fit(self, X, y, sample_weight, solver_type, kernel, random_seed)
--> 413     self.dual_coef_ = sp.csr_matrix(
    414         (dual_coef_data, dual_coef_indices, dual_coef_indptr), (n_class, n_SV)
    415     )

scipy/sparse/_compressed.py:106, in _cs_matrix.__init__(self, arg1, shape, dtype, copy)
--> 106 self.check_format(full_check=False)

scipy/sparse/_compressed.py:176, in _cs_matrix.check_format(self, full_check)
    174 # check index and data arrays
    175 if (len(self.indices) != len(self.data)):
--> 176     raise ValueError(""indices and data should have the same size"")
```


### Versions

```shell
System:
    python: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:55:37)  [Clang 14.0.6 ]
executable: miniconda3/bin/python
   machine: macOS-13.1-x86_64-i386-64bit

Python dependencies:
      sklearn: 1.2.0
          pip: 22.3.1
   setuptools: 65.6.3
        numpy: 1.24.1
        scipy: 1.10.0
       Cython: 0.29.33
       pandas: 1.5.2
   matplotlib: 3.6.2
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: mkl
         prefix: libmkl_rt
       filepath: miniconda3/lib/libmkl_rt.dylib
        version: 2020.0.4
threading_layer: intel
    num_threads: 8

       user_api: openmp
   internal_api: openmp
         prefix: libomp
       filepath: miniconda3/lib/libomp.dylib
        version: None
    num_threads: 16
```
","['Bug', 'help wanted', 'Hard']","2023-01-12T19:40:35Z","12","0","https://github.com/scikit-learn/scikit-learn/issues/25380","Logical Bug"
"97","scikit-learn/scikit-learn","Segmentation fault occurs when KernelPCA is loaded after torchvision","### Describe the bug

If KernelPCA is loaded after torchvision, then I get a segmentation fault. If I load them in the opposite order then I get no segmentaton fault.

### Steps/Code to Reproduce

```
import torchvision
from sklearn.decomposition import KernelPCA

import numpy as np

transformer = KernelPCA(n_components=7, kernel='rbf')
X_transformed = transformer.fit_transform(np.zeros([100,100]))
```

### Expected Results

No segmentation fault

### Actual Results

Segmentation fault (core dumped)

### Versions

```shell
System:
    python: 3.8.10 (default, Mar 15 2022, 12:22:08)  [GCC 9.4.0]
executable: /usr/bin/python3
   machine: Linux-5.4.0-77-generic-x86_64-with-glibc2.29

Python dependencies:
          pip: 22.1
   setuptools: 45.2.0
      sklearn: 0.24.2
        numpy: 1.22.4
        scipy: 1.4.1
       Cython: 0.29.23
       pandas: 1.4.3
   matplotlib: 3.5.2
       joblib: 1.0.1
threadpoolctl: 2.1.0

Built with OpenMP: True


torchvision version: 0.10.0+cu102
```
","['Bug', 'module:decomposition']","2022-12-19T17:04:31Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/25213","Runtime Error"
"98","scikit-learn/scikit-learn","Sparse data representations results in worse models than dense data for some classifiers","### Describe the bug

Using scipy sparse matrices with sklearn LogisticRegression greatly improves speed and therefore is desirable in many scenarios.

However, it appears that sparse versus dense data representations yield different (worse) results for some sklearn classifiers.

My perhaps naive assumption is that sparse versus dense is just a method of representing the data and operations performed on the sparse or dense data (including model training) should yield identical or nearly identical results.

A notebook gist looking at sparse versus dense results for nine solvers can be found here: https://gist.github.com/mmulthaup/db619d8b5ea4baf4a00153b055a7e9a8

### Steps/Code to Reproduce

```py
#Minimal example
import sklearn
import scipy
import numpy as np
 
#Artificial data
y = np.repeat(1,100).tolist()+np.repeat(0,100).tolist()
X = np.concatenate([scipy.stats.poisson.rvs(0.2,size=[100,1000]),scipy.stats.poisson.rvs(0.1,size=[100,1000])])
 
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    X, y, test_size=0.5, random_state=42)
X_train_sparse = scipy.sparse.bsr_array(X_train)
X_test_sparse = scipy.sparse.bsr_array(X_test)
 
#Modeling
model = sklearn.linear_model.LogisticRegression(solver=""saga"",random_state=42,max_iter=4000)
dense_scores = model.fit(X_train,y_train).predict_proba(X_test)[:,1]
sparse_scores = model.fit(X_train_sparse,y_train).predict_proba(X_test_sparse)[:,1]

print(f""Dense AUC: {round(sklearn.metrics.roc_auc_score(y_test,dense_scores),3)}"") #Dense AUC: 1.0
print(f""Sparse AUC: {round(sklearn.metrics.roc_auc_score(y_test,sparse_scores),3)}"") #Sparse AUC: 0.584
```

### Expected Results

Dense AUC: 1.0
Sparse AUC: 1.0

### Actual Results

Dense AUC: 1.0
Sparse AUC: 0.584


### Versions

```shell
Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7fc557a3f310>
Traceback (most recent call last):
  File ""/databricks/python/lib/python3.9/site-packages/threadpoolctl.py"", line 400, in match_module_callback
    self._make_module_from_path(filepath)
  File ""/databricks/python/lib/python3.9/site-packages/threadpoolctl.py"", line 515, in _make_module_from_path
    module = module_class(filepath, prefix, user_api, internal_api)
  File ""/databricks/python/lib/python3.9/site-packages/threadpoolctl.py"", line 606, in __init__
    self.version = self.get_version()
  File ""/databricks/python/lib/python3.9/site-packages/threadpoolctl.py"", line 646, in get_version
    config = get_config().split()
AttributeError: 'NoneType' object has no attribute 'split'

System:
    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]
executable: /local_disk0/.ephemeral_nfs/envs/pythonEnv-0c18486f-171e-4bd6-9fb6-691f3a2da533/bin/python
   machine: Linux-5.4.0-1088-aws-x86_64-with-glibc2.31

Python dependencies:
      sklearn: 1.2.0
          pip: 21.2.4
   setuptools: 61.2.0
        numpy: 1.23.5
        scipy: 1.9.3
       Cython: 0.29.28
       pandas: 1.5.2
   matplotlib: 3.5.1
       joblib: 1.2.0
threadpoolctl: 2.2.0

Built with OpenMP: True

threadpoolctl info:
       filepath: /local_disk0/.ephemeral_nfs/envs/pythonEnv-0c18486f-171e-4bd6-9fb6-691f3a2da533/lib/python3.9/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
         prefix: libgomp
       user_api: openmp
   internal_api: openmp
        version: None
    num_threads: 16

       filepath: /local_disk0/.ephemeral_nfs/envs/pythonEnv-0c18486f-171e-4bd6-9fb6-691f3a2da533/lib/python3.9/site-packages/scipy.libs/libopenblasp-r0-41284840.3.18.so
         prefix: libopenblas
       user_api: blas
   internal_api: openblas
        version: 0.3.18
    num_threads: 16
threading_layer: pthreads
   architecture: Haswell
```
","['Bug', 'module:linear_model']","2022-12-16T14:53:20Z","5","1","https://github.com/scikit-learn/scikit-learn/issues/25198","Runtime Error"
"99","scikit-learn/scikit-learn","Instability in fastica for float32","Originally seen in https://github.com/scikit-learn/scikit-learn/issues/24131#issuecomment-1208091119 for more details on a single random seed and only for Atlas but I have managed to reproduce with OpenBLAS see below.

For now we have a work-around for the CI in #24198 but it is still an issue and in an ideal world, someone would investigate and try to see whether it is fixable.

Other possibly related issues and PRs are mentioned in https://github.com/scikit-learn/scikit-learn/pull/24198#issuecomment-1252616127: https://github.com/scikit-learn/scikit-learn/issues/2735 and https://github.com/scikit-learn/scikit-learn/pull/2738.

Here is a snippet that reproduces the same problem on OpenBLAS:

```py
import numpy as np
from scipy import stats
from sklearn.decomposition import fastica
import sys

global_random_seed = 20
global_dtype = np.float32


def center_and_norm(x, axis=-1):
    x = np.rollaxis(x, axis)
    x -= x.mean(axis=0)
    x /= x.std(axis=0)


rng = np.random.RandomState(global_random_seed)
n_samples = 1000
# Generate two sources:
s1 = (2 * np.sin(np.linspace(0, 100, n_samples)) > 0) - 1
s2 = stats.t.rvs(1, size=n_samples, random_state=global_random_seed)
s = np.c_[s1, s2].T
center_and_norm(s)
s = s.astype(global_dtype)
s1, s2 = s

# Mixing angle
phi = 0.6
mixing = np.array([[np.cos(phi), np.sin(phi)], [np.sin(phi), -np.cos(phi)]])
mixing = mixing.astype(global_dtype)
m = np.dot(mixing, s)
center_and_norm(m)

algo = 'deflation'
nl = 'logcosh'
whiten = 'arbitrary-variance'

problematic_random_state = 13441

k_, mixing_, s_ = fastica(
    m.T, fun=nl, whiten=whiten, algorithm=algo, random_state=problematic_random_state
)
```

You get a warning with a division by zero:
```
/home/lesteve/dev/scikit-learn/sklearn/decomposition/_fastica.py:89: RuntimeWarning: invalid value encountered in divide
  w1 /= np.sqrt((w1**2).sum())
```

And then the following traceback because NaNs are passed into a BLAS routine eventually:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In [1], line 39
     35 whiten = 'arbitrary-variance'
     37 problematic_random_state = 13441 # both for openblas and atlas (also numpy < and >= 1.23)
---> 39 k_, mixing_, s_ = fastica(
     40     m.T, fun=nl, whiten=whiten, algorithm=algo, random_state=problematic_random_state
     41 )

File ~/dev/scikit-learn/sklearn/decomposition/_fastica.py:322, in fastica(X, n_components, algorithm, whiten, fun, fun_args, max_iter, tol, w_init, whiten_solver, random_state, return_X_mean, compute_sources, return_n_iter)
    174 """"""Perform Fast Independent Component Analysis.
    175 
    176 The implementation is based on [1]_.
   (...)
    308        pp. 411-430.
    309 """"""
    310 est = FastICA(
    311     n_components=n_components,
    312     algorithm=algorithm,
   (...)
    320     random_state=random_state,
    321 )
--> 322 S = est._fit_transform(X, compute_sources=compute_sources)
    324 if est._whiten in [""unit-variance"", ""arbitrary-variance""]:
    325     K = est.whitening_

File ~/dev/scikit-learn/sklearn/decomposition/_fastica.py:683, in FastICA._fit_transform(self, X, compute_sources)
    680 else:
    681     self.components_ = W
--> 683 self.mixing_ = linalg.pinv(self.components_, check_finite=False)
    684 self._unmixing = W
    686 return S

File ~/miniconda3/lib/python3.9/site-packages/scipy/linalg/_basic.py:1304, in pinv(a, atol, rtol, return_rank, check_finite, cond, rcond)
   1231 """"""
   1232 Compute the (Moore-Penrose) pseudo-inverse of a matrix.
   1233 
   (...)
   1301 
   1302 """"""
   1303 a = _asarray_validated(a, check_finite=check_finite)
-> 1304 u, s, vh = _decomp_svd.svd(a, full_matrices=False, check_finite=False)
   1305 t = u.dtype.char.lower()
   1306 maxS = np.max(s)

File ~/miniconda3/lib/python3.9/site-packages/scipy/linalg/_decomp_svd.py:133, in svd(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)
    131     raise LinAlgError(""SVD did not converge"")
    132 if info < 0:
--> 133     raise ValueError('illegal value in %dth argument of internal gesdd'
    134                      % -info)
    135 if compute_uv:
    136     return u, s, v

ValueError: illegal value in 4th argument of internal gesdd
```","['Bug', 'module:decomposition']","2022-11-25T10:00:12Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/25038","Logical Bug"
"100","scikit-learn/scikit-learn","forced threading joblib backend in pairwise_distances","### Describe the bug

The function pairwise_distances in pairwise.py comes with a forced ""threading"" joblib backend.
This slows down the system if one uses a python callable as distance function, due to the global interpreter lock - in practice there is no parallelism.

Is there a reason for this choice?

In my opition the backend should be different if the metric is a custom callable, or at least, the user should be able to force a different one.

### Steps/Code to Reproduce

Invoke pairwise_distance with a callable as metric and n_jobs > 1

```
def myCustomDistanceWrittenInPython(p0, p1):
    return myhexoticmetric

with parallel_backend('loky', n_jobs=n_jobs):
    pairwise_distances(X, metric=myCustomDistanceWrittenInPython)
```

### Expected Results

joblib creates n_jobs processes and runs parallel

### Actual Results

the global interpreter lock of python prevents a correct parllelization

### Versions

```shell
sklearn main branch after commit e947074f63c6602ae15cd57b1fa4f6658040cff7 of Fri Nov 11 05:10:31 2022 -0500
```
","['Bug', 'module:metrics']","2022-11-11T13:39:48Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/24896","Performance Issue"
"101","scikit-learn/scikit-learn","Gaussian process `log_marginal_likelihood()` `theta` parameter: pass as `log(theta)`?","### Discussed in https://github.com/scikit-learn/scikit-learn/discussions/24765

I opened this as a discussion, but it is probably more suited for an issue, as it may lead to a documentation fix.

System Info

```
System:
    python: 3.10.7 (main, Sep  8 2022, 14:34:29) [GCC 12.2.0]
executable: /usr/bin/python3
   machine: Linux-5.19.0-1-amd64-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.1.2
          pip: 22.2
   setuptools: 59.6.0
        numpy: 1.21.5
        scipy: 1.8.1
       Cython: 0.29.32
       pandas: 1.3.5
   matplotlib: 3.5.2
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
        version: None
    num_threads: 16
```

<div type='discussions-op-text'>

<sup>Originally posted by **elcorto** October 26, 2022</sup>
## Preliminaries

I'm a bit confused by `GaussianProcess{Regressor,Classifier}`'s internal hyperparameter representation and the nature of the `theta` argument of the `log_marginal_likelihood()` method.

The doc strings of these methods say

```
theta : array-like of shape (n_kernel_params,) default=None
    Kernel hyperparameters for which the log-marginal likelihood is
    evaluated. If None, the precomputed log_marginal_likelihood
    of ``self.kernel_.theta`` is returned.
```

However, the GP's internal hyper optimizer code path seems to work with `log(theta)`.

```py
>>> from sklearn.gaussian_process import GaussianProcessRegressor
>>> from sklearn.gaussian_process.kernels import RBF, WhiteKernel

>>> gp=GaussianProcessRegressor(kernel=RBF()+WhiteKernel())
>>> gp.fit(rand(100,3), rand(100))

>>> gp.kernel_
RBF(length_scale=59.8) + WhiteKernel(noise_level=0.0812)

>>> log_theta=gp.kernel_.theta
>>> log_theta
array([ 4.09017207, -2.51094817])

>>> exp(log_theta)
array([59.75017175,  0.08119122])
```


The value of the `kernel_.theta` attribute is actually `log(theta)`. And indeed, the [`theta` getter][getter] returns `log(theta)`, so they are not *stored* in log format but only returned that way.

The getter's docs say

```
Returns the (flattened, log-transformed) non-fixed hyperparameters.

Note that theta are typically the log-transformed values of the
kernel's hyperparameters as this representation of the search space
is more amenable for hyperparameter search, as hyperparameters like
length-scales naturally live on a log-scale.

Returns
-------
theta : ndarray of shape (n_dims,)
    The non-fixed, log-transformed hyperparameters of the kernel
```

OK.

## Question

When calling `log_marginal_likelihood(theta)`, [the code sets `kernel.theta = theta`][call_setter], which calls [the `theta` setter][setter] which does essentially `exp(theta)`. So from that I'd assume that we need to call `log_marginal_likelihood(log(theta))`. Indeed, continuing with above's code

```py
>>> gp.log_marginal_likelihood_value_
-19.58130329439676

>>> gp.log_marginal_likelihood(log_theta)
-19.58130329439676
```

shows that we need to pass `log(theta)` to `log_marginal_likelihood()`. Also when trying to plot `log_marginal_likelihood` on a grid of hyperparameters, I only get what looks like correct results if I pass in `log(theta)`. If that is true, should the documentation of the `log_marginal_likelihood()` methods be adapted accordingly?

Thanks.


[setter]: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/gaussian_process/kernels.py#L290
[getter]: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/gaussian_process/kernels.py#L266
[call_setter]: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/gaussian_process/_gpr.py#L543
</div>","['Bug', 'module:gaussian_process']","2022-10-29T11:43:20Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/24786","Logical Bug"
"102","scikit-learn/scikit-learn","PCA crashes when fit to large array","### Describe the bug

When trying to fit a `PCA` model to a large dataset with `shape=(30000, 28000)`, the model fits for a bit more than 30 minutes using nearly all cores and then crashes out of python. I've confirmed that a dataset with `shape=(20000, 28000)` succeeds without issue. I watched both in `htop` and found memory usage to never exceed 30% available memory.

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn import decomposition

data = np.random.randn(30000, 28000)
pca_model = decomposition.PCA(0.95, whiten=True)
pca_model.fit(data)
```

### Expected Results

The model should be fit without crashing out of python.

### Actual Results

```
free(): invalid size
Aborted (core dumped)
```

### Versions

```shell
System:
    python: 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0]
executable: /usr/local/bin/python
   machine: Linux-4.15.0-136-generic-x86_64-with-glibc2.29

Python dependencies:
      sklearn: 1.1.2
          pip: 20.2.4
   setuptools: 63.2.0
        numpy: 1.22.3
        scipy: 1.9.0
       Cython: None
       pandas: 1.4.2
   matplotlib: None
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/container-user/.local/lib/python3.8/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
    num_threads: 72

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/container-user/.local/lib/python3.8/site-packages/numpy.libs/libopenblas64_p-r0-2f7c42d4.3.18.so
        version: 0.3.18
threading_layer: pthreads
   architecture: SkylakeX
    num_threads: 64

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/container-user/.local/lib/python3.8/site-packages/scipy.libs/libopenblasp-r0-9f9f5dbc.3.18.so
        version: 0.3.18
threading_layer: pthreads
   architecture: SkylakeX
    num_threads: 64
```
","['Bug', 'module:decomposition']","2022-10-25T21:36:20Z","5","3","https://github.com/scikit-learn/scikit-learn/issues/24757","Runtime Error"
"103","scikit-learn/scikit-learn","Improvement for Gaussian NB by rethinking the variance smoothing","### Describe the workflow you want to enable

## Problem background

In [sklearn.naive_bayes.GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussian+nb), a genarative probability model is given by 

P(c| **x** ) = P(c) P( **x** |c)/P( **x** ) , where x<sub>i</sub> ~ N(u<sub>i</sub>, v<sub>i</sub>)

However, sometimes the variance for P( x<sub>i</sub> |c) is zero. In other words, with label c, x<sub>i</sub> is a constant value in the dataset.  

## Current solution in `sklearn.naive_bayes.GaussianNB`

It 's seen from the source that 

```python
self.epsilon_ = self.var_smoothing * np.var(X, axis=0).max()
```

and that

```python
self.var_ = np.zeros((n_classes, n_features))
self.var_[:, :] -= self.epsilon_ 
self.var_[i, :] = new_sigma
self.var_[:, :] += self.epsilon_
```


### Describe your proposed solution

## The problem of the current implementation
You can see, the current implementation would add a  portion of the max variance of all the features(without considering the class), which makes the updated variance never being zero (unless all features are constants).  
This is questionable in two cases:

1.  When there is no feature that has a zero variance. 
 Well, the current algs still adds a small variance to each feature. This seems to be fair(all features are considered), but it is actually dangerous. Not all features are preprossed using something like `StdScaler`, which means their variances can differ dramatically. Adding a big value(coming from the feature with largest variance) blindly would damage the probability computation of the feature with small variance greatly. For example, the variance of the biggest feature is 100, while other features have a variance of 1, let var_smoothing be 1, then the updated variance by current implementation would be 200 and 101-s, making the significant wrong probability calculation of other features. 

2. When there is indeed a feature x1 that has a zero variance under some class c1.
 In this case, adding the max variance exaggerated the instability of this feature. You see, the ground-truth variance of x1|c1 may be something like 0.0001, but due to the max-variance feature x2, it is added by 100. which is obviously unreasonable.  

## My implementation 
1. If there is no variable that is zero-variance, do not apply var-smoothing. 
2. If a feature x1 under some class c1 has a zero variance, use the variance of x1 without knowing the class to be the smoothing variance, instead of using the max variance of all features. 
3. If the variance of x1, even without knowing the class, is zero. throw a warning and ignore this feature for decision. 


## Experiment

I tested my implementation on the famous [ling-spam dataset](https://www.kaggle.com/datasets/mandygu/lingspam-dataset)

- model: I use the bag of words model to first do a feature extraction, then `sklearn.naive_bayes.GaussianNB` is applied for both the current implementation and my new implementation
- hyperparameter tuning
  -  to make the competition fair, I applied a parameter tuning to both implementations on validation set
  - I used GridSearchCV to search 'var_smoothing' in [1e-13, 1e-11, 1e-9, 1e-7, 1e-5, 1e-3]
  - I also used `geatpy`(a genetic algorithm lib) to optimize the cross validation loss by tuning var_smoothing

- experiment result on test set
  - current implementation
     ```python
     accuracy=0.9615384615384616
     recall=0.9307692307692308
      f1=0.9603174603174605
     ```
    ![image](https://user-images.githubusercontent.com/41530341/197404136-3b6cb6b1-93c0-4b86-ae47-8af897d83af3.png)
  - my implementation
     ```python
     accuracy=0.9807692307692307
     recall=0.9615384615384616
     f1=0.9803921568627451
     ```
     ![image](https://user-images.githubusercontent.com/41530341/197404183-7e687a01-ecd3-4986-9e38-8507c5aa9cb2.png)
     ![image](https://user-images.githubusercontent.com/41530341/197404176-19e82c70-b97a-4122-b5f9-08eef5b89f94.png)




### Describe alternatives you've considered, if relevant

I know that this implentation includes a partial fit algorithm that can further add more samples to train the model furtherly. I've roughly read the report Stanford CS tech report STAN-CS-79-773, but I am still not sure whether my solution would break the formulation of the partial fit problem or whether the current implementation is necessary for doing an accurate partial fit. Currently I think it doesn't matters, but if you find something wrong please give me some insight. 

I will make a PR if my solution is indeed theoretically and experimentally better. 

### Additional context

_No response_","['Bug', 'New Feature', 'module:naive_bayes']","2022-10-23T16:36:23Z","12","0","https://github.com/scikit-learn/scikit-learn/issues/24732","Performance Issue"
"104","scikit-learn/scikit-learn","AttributeError: 'MLPRegressor' object has no attribute '_best_coefs'","The following parameters were working fine with another dataset. When I switched to a new dataset, for some reason an `AttributeError` is occurring. Any ideas?

```python
model = MLPRegressor(hidden_layer_sizes=(10), activation='tanh', batch_size=1000,
  learning_rate_init=0.1, max_iter=5000, momentum=.9, solver='adam', early_stopping=True,
  random_state=1, verbose=1)
X = [[0.,0.,0.,0.2173913,0.,0.5,
  0.,0.46666667,0.76351351,0.41140777,0.55555556,0.03361345,
  0.09022556,0.78107607,0.13250518,1.,0.,0.,
  0.,0.,1.],
 [0.80769231,1.,0.,0.69565217,0.20942029,0.5,
  0.,0.13333333,0.,0.,0.88888889,0.95424837,
  0.96491228,0.83766234,0.97584541,0.,0.,0.,
  0.,0.,0.]] 
y = [1.0, 1.0]
model.fit(X, y)
```

Output:
```
...
Iteration 4997, loss = 0.00001391
Validation score: nan
Iteration 4998, loss = 0.00002568
Validation score: nan
Iteration 4999, loss = 0.00001822
Validation score: nan
Iteration 5000, loss = 0.00001470
Validation score: nan

AttributeError: 'MLPRegressor' object has no attribute '_best_coefs'
```

The error occurs here:
https://github.com/scikit-learn/scikit-learn/blob/7c2a58d51f4528827e9bfe9c43d06c5c1716bfb8/sklearn/neural_network/_multilayer_perceptron.py#L687

The error also seems to completely disappear when there are more than around 10-25 training examples.","['Bug', 'module:neural_network']","2022-10-21T03:10:32Z","10","0","https://github.com/scikit-learn/scikit-learn/issues/24713","Logical Bug"
"105","scikit-learn/scikit-learn","Exit Code -1073741819 when doing K-means++ clustering","### Describe the bug

Unfortunately I am getting an exit code in Pycharm when doing clustering with k-means++.
I tried nearly everything. Setup new Pycharm project try using different versions of numpy or sklearn.

### Steps/Code to Reproduce

```python
def __cluster(num_clusters: int, data: list[list[float]]):
    km = KMeans(n_clusters=num_clusters)
    return km.fit_predict(data)


distributions_filename: str = ""distributions.json""
with open(distributions_filename) as f:
    distributions: list[list[float]] = json.load(f)
    buckets = __cluster(num_clusters=200, data=distributions)
```
### Expected Results

clustered buckets

### Actual Results

before the clustering is finished I get an error: ""Process finished with exit code -1073741819 (0xC0000005)""

### Versions

```shell
System:
    python: 3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]
executable: C:\Users\maron\Desktop\test\venv\Scripts\python.exe
   machine: Windows-10-10.0.19044-SP0

Python dependencies:
      sklearn: 1.1.2
          pip: 21.3.1
   setuptools: 60.2.0
        numpy: 1.23.3
        scipy: 1.9.1
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.2.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
         prefix: vcomp
       filepath: C:\Users\maron\Desktop\test\venv\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None
    num_threads: 32

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\Users\maron\Desktop\test\venv\Lib\site-packages\numpy\.libs\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll
        version: 0.3.20
threading_layer: pthreads
   architecture: Zen
    num_threads: 24

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\Users\maron\Desktop\test\venv\Lib\site-packages\scipy\.libs\libopenblas.PZA5WNOTOH6FZLB2KBVKAURAKVTFSNNU.gfortran-win_amd64.dll
        version: 0.3.18
threading_layer: pthreads
   architecture: Zen
    num_threads: 24
```
","['Bug', 'module:cluster', 'Needs Investigation']","2022-09-29T10:53:39Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/24540","Logical Bug"
"106","scikit-learn/scikit-learn","GroupShuffleSplit chokes on pd.Int16Dtype() with a cryptic error","### Describe the bug

`GroupShuffleSplit` chokes on `pd.Int16Dtype()` with a cryptic error.
It looks like internally the data series gets converted to a list, and list comparison returns a scalar, while an iterable is expected

### Steps/Code to Reproduce

```python
data = pd.DataFrame({""clusters"": [1, 2, 3, pd.NA, pd.NA],
                    ""x"" : [0,1,2,3,4]} )

splitter = GroupShuffleSplit(test_size=.2, n_splits=2, random_state = 7)
split = splitter.split(data, groups=data['clusters'])
train_inds, test_inds = next(split)
```

### Expected Results

e.g. `[0,1,3]` and `[2,4]`

### Actual Results

```
AttributeError                            Traceback (most recent call last)
Input In [23], in <cell line: 3>()
      1 splitter = GroupShuffleSplit(test_size=.2, n_splits=2, random_state = 7)
      2 split = splitter.split(data, groups=data['clusters'])
----> 3 train_inds, test_inds = next(split)

File /opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_split.py:1600, in BaseShuffleSplit.split(self, X, y, groups)
   1570 """"""Generate indices to split data into training and test set.
   1571 
   1572 Parameters
   (...)
   1597 to an integer.
   1598 """"""
   1599 X, y, groups = indexable(X, y, groups)
-> 1600 for train, test in self._iter_indices(X, y, groups):
   1601     yield train, test

File /opt/conda/lib/python3.8/site-packages/sklearn/model_selection/_split.py:1805, in GroupShuffleSplit._iter_indices(self, X, y, groups)
   1803 if groups is None:
   1804     raise ValueError(""The 'groups' parameter should not be None."")
-> 1805 groups = check_array(groups, ensure_2d=False, dtype=None)
   1806 classes, group_indices = np.unique(groups, return_inverse=True)
   1807 for group_train, group_test in super()._iter_indices(X=classes):
   1808     # these are the indices of classes in the partition
   1809     # invert them into data indices

File /opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:805, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
    799         raise ValueError(
    800             ""Found array with dim %d. %s expected <= 2.""
    801             % (array.ndim, estimator_name)
    802         )
    804     if force_all_finite:
--> 805         _assert_all_finite(array, allow_nan=force_all_finite == ""allow-nan"")
    807 if ensure_min_samples > 0:
    808     n_samples = _num_samples(array)

File /opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py:126, in _assert_all_finite(X, allow_nan, msg_dtype)
    122     raise ValueError(""Input contains NaN"")

AttributeError: 'bool' object has no attribute 'any'
```

### Versions

```shell
System:
    python: 3.8.8 (default, Apr 13 2021, 19:58:26)  [GCC 7.3.0]
executable: /opt/conda/bin/python
   machine: Linux-5.4.162-86.275.amzn2.x86_64-x86_64-with-glibc2.10

Python dependencies:
          pip: 22.1.2
   setuptools: 59.5.0
      sklearn: 1.0.2
        numpy: 1.22.3
        scipy: 1.6.3
       Cython: 0.29.23
       pandas: 1.2.4
   matplotlib: 3.4.2
       joblib: 1.0.1
threadpoolctl: 2.1.0

Built with OpenMP: True
```
","['Bug', 'module:model_selection']","2022-09-21T03:28:21Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/24486","Logical Bug"
"107","scikit-learn/scikit-learn","MLPRegressor - Validation score wrongly defined","### Describe the bug

In MLPRegressor, if the option early_stopping is set as True, the model will monitor the loss calculated on the validation set in stead of the training set, using the same loss formulation which is the mean squared error. However, as implemented in the line 719 in the source code:
```
self.validation_scores_.append(self.score(X_val, y_val))
```
The function ""score"", which returns (to confirm) the coefficient of determination, is used.  This is not correct. It should be something like:
```
self.validation_scores_.append(mean_squared_error(self.predict(X_val), y_val))
```


### Steps/Code to Reproduce

Sorry, I don't have time to write a simple code. But the error is quite clear.

### Expected Results

The validation score must be mean squared error.

### Actual Results

Coefficient of determination

### Versions

```shell
1.1.1
```
","['Bug', 'Enhancement', 'Needs Decision', 'module:neural_network']","2022-09-09T18:12:16Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/24411","Logical Bug"
"108","scikit-learn/scikit-learn","GridSearchCV does not seem to recognize whether estimators from StackingClassifier are fitted or not","### Describe the bug

There seems to be a bug with the combination of `GridSearchCV` and `StackingClassifier` when the parameter `cv` of  `StackingClassifier` is set to 'prefit'. With this option, the estimators of the `StackingClassifier` should be fitted before fitting the stacked model, and only the final_estimator would then be fitted. When including the `StackingClassifier` within `GridSearchCV` however, the fact that estimators have already been fitted does not seem to be recognized.



### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier
from sklearn.model_selection import GridSearchCV

# Creating toy data set
n_features = 3
n_instances = 40
train = np.random.rand(n_instances, n_features)
label = np.random.randint(0,2,n_instances)

# Declaring estimators
log_clf = LogisticRegression()
gau_clf = GaussianNB()

# Fitting estimators
log_clf.fit(train, label)
gau_clf.fit(train, label)

# Creating stacked model
estimators = [
    (""log"", log_clf),
    (""gau"", gau_clf)
]

boost = GradientBoostingClassifier()
stack = StackingClassifier(estimators=estimators,
                          final_estimator=boost,
                          cv = 'prefit')

# Creating the Grid CV
param_search = {
    'final_estimator__max_depth': [1]
    }

gridcv = GridSearchCV(stack, 
                      param_grid=param_search)

# Fitting the stack and gridcv models
stack.fit(train, label) # works fine 
gridcv.fit(train, label) # sklearn.exceptions.NotFittedError: This LogisticRegression instance is not fitted yet. 
                         # Call 'fit' with appropriate arguments before using this estimator

```

### Expected Results

In the above code, the `stack` model works fine and no error related to the estimators' previous fitting is thrown.

However, when included in the GridSearchCV, the fact that estimator models have been fitted already does not seem to be recognized and a `sklearn.exceptions.NotFittedError` error is prompted. 



### Actual Results

```
Traceback (most recent call last):
  File ""c:\Users\levesque\Documents\Python\AMEX\errorExample.py"", line 39, in <module>
    gridcv.fit(train, label) # sklearn.exceptions.NotFittedError: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate
arguments before using this estimator
  File ""C:\Users\levesque\Documents\Python\AMEX\env\lib\site-packages\sklearn\model_selection\_search.py"", line 875, in fit
    self._run_search(evaluate_candidates)
  File ""C:\Users\levesque\Documents\Python\AMEX\env\lib\site-packages\sklearn\model_selection\_search.py"", line 1379, in _run_search
    evaluate_candidates(ParameterGrid(self.param_grid))
  File ""C:\Users\levesque\Documents\Python\AMEX\env\lib\site-packages\sklearn\model_selection\_search.py"", line 852, in evaluate_candidates
    _warn_or_raise_about_fit_failures(out, self.error_score)
  File ""C:\Users\levesque\Documents\Python\AMEX\env\lib\site-packages\sklearn\model_selection\_validation.py"", line 367, in _warn_or_raise_about_fit_failures
    raise ValueError(all_fits_failed_message)
ValueError:
All the 5 fits failed.
It is very likely that your model is misconfigured.
You can try to debug the error by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
5 fits failed with the following error:
Traceback (most recent call last):
  File ""C:\Users\levesque\Documents\Python\AMEX\env\lib\site-packages\sklearn\model_selection\_validation.py"", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File ""C:\Users\levesque\Documents\Python\AMEX\env\lib\site-packages\sklearn\ensemble\_stacking.py"", line 584, in fit
    return super().fit(X, self._le.transform(y), sample_weight)
  File ""C:\Users\levesque\Documents\Python\AMEX\env\lib\site-packages\sklearn\ensemble\_stacking.py"", line 183, in fit
    check_is_fitted(estimator)
  File ""C:\Users\levesque\Documents\Python\AMEX\env\lib\site-packages\sklearn\utils\validation.py"", line 1345, in check_is_fitted
    raise NotFittedError(msg % {""name"": type(estimator).__name__})
sklearn.exceptions.NotFittedError: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
```

### Versions

```shell
System:
    python: 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]
executable: C:\Users\levesque\Documents\Python\AMEX\env\Scripts\python.exe
   machine: Windows-10-10.0.17763-SP0

Python dependencies:
      sklearn: 1.1.2
          pip: 22.2.2
   setuptools: 57.4.0
        numpy: 1.23.1
        scipy: 1.9.0
       Cython: None
       pandas: 1.4.3
   matplotlib: 3.5.2
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
         prefix: vcomp
       filepath: C:\Users\levesque\Documents\Python\AMEX\env\Lib\site-packages\sklearn\.libs\vcomp140.dll
        version: None
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\Users\levesque\Documents\Python\AMEX\env\Lib\site-packages\numpy\.libs\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll
        version: 0.3.20
threading_layer: pthreads
   architecture: Haswell
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: C:\Users\levesque\Documents\Python\AMEX\env\Lib\site-packages\scipy\.libs\libopenblas.PZA5WNOTOH6FZLB2KBVKAURAKVTFSNNU.gfortran-win_amd64.dll
        version: 0.3.18
threading_layer: pthreads
   architecture: Haswell
    num_threads: 8
```
","['Bug', 'module:model_selection', 'module:base']","2022-09-09T13:50:18Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/24409","Logical Bug"
"109","scikit-learn/scikit-learn","BayesianRidge prediction standard deviation affected by uniform sample weights","### Describe the bug

The standard deviation of predictions obtained by setting return_std=True on predict(), is clearly affected by uniform sample_weight vectors on fit(). A uniform sample_weight vector will act as a constant on the likelihood function, and I am therefore questioning the effect it has on the standard deviation of predictions. As shown below by the example, the scale (1 vs. 20) of the uniform sample_weight vector directly affects the scale of the standard deviations. The example is based on the Curve Fitting with Bayesian Ridge Regression [example](https://scikit-learn.org/stable/auto_examples/linear_model/plot_bayesian_ridge_curvefit.html#sphx-glr-auto-examples-linear-model-plot-bayesian-ridge-curvefit-py)).

### Steps/Code to Reproduce

```
import numpy as np
from sklearn.linear_model import BayesianRidge
import matplotlib.pyplot as plt

def func(x):
    return np.sin(2 * np.pi * x)

size = 25
rng = np.random.RandomState(1234)
x_train = rng.uniform(0.0, 1.0, size)
y_train = func(x_train) + rng.normal(scale=0.1, size=size)
x_test = np.linspace(0.0, 1.0, 100)

n_order = 3
X_train = np.vander(x_train, n_order + 1, increasing=True)
X_test = np.vander(x_test, n_order + 1, increasing=True)
reg = BayesianRidge(tol=1e-6, fit_intercept=False)

init = [1.0, 1e-3]
reg.set_params(alpha_init=init[0], lambda_init=init[1])

fig, axes = plt.subplots(1, 2, figsize=(8, 4))
for i, ax in enumerate(axes):
    if i == 0:
        reg.fit(X_train, y_train, sample_weight=np.ones(size))
        ymean, ystd = reg.predict(X_test, return_std=True)

        ax.plot(x_test, func(x_test), color=""blue"", label=""sin($2\\pi x$)"")
        ax.scatter(x_train, y_train, s=50, alpha=0.5, label=""observation"")
        ax.plot(x_test, ymean, color=""red"", label=""predict mean"")
        ax.fill_between(x_test, ymean - ystd, ymean + ystd, color=""pink"", alpha=0.5, label=""predict std"")
        ax.set_title('sample_weight=np.ones(size)')
        ax.set_ylim(-1.6, 1.6)
        ax.legend()
    elif i == 1:
        reg.fit(X_train, y_train, sample_weight=20*np.ones(size))
        ymean, ystd = reg.predict(X_test, return_std=True)

        ax.plot(x_test, func(x_test), color=""blue"", label=""sin($2\\pi x$)"")
        ax.scatter(x_train, y_train, s=50, alpha=0.5, label=""observation"")
        ax.plot(x_test, ymean, color=""red"", label=""predict mean"")
        ax.fill_between(x_test, ymean - ystd, ymean + ystd, color=""pink"", alpha=0.5, label=""predict std"")
        ax.set_title('sample_weight=20 * np.ones(size)')
        ax.set_ylim(-1.6, 1.6)
        ax.legend()

plt.tight_layout()
plt.show()
```



### Expected Results


![expected](https://user-images.githubusercontent.com/42994401/187893778-bd076956-dd22-4b53-90ea-faec7c27c98f.png)



### Actual Results


![actual](https://user-images.githubusercontent.com/42994401/187893819-3654f021-67db-4703-a2b2-cad2ae87071d.png)



### Versions

```shell
System:
    python: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\AAA\anaconda3\python.exe
   machine: Windows-10-10.0.19041-SP0

Python dependencies:
          pip: 20.2.4
   setuptools: 50.3.1.post20201107
      sklearn: 0.24.0
        numpy: 1.22.4
        scipy: 1.5.2
       Cython: 0.29.21
       pandas: 1.1.3
   matplotlib: 3.3.2
       joblib: 0.17.0
threadpoolctl: 2.1.0

Built with OpenMP: True
```
","['Bug', 'module:linear_model', 'Needs Investigation']","2022-09-01T10:33:32Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/24313","Logical Bug"
"110","scikit-learn/scikit-learn","sklearn.svm.SVR use more RAM memory on newer versions","### Describe the bug

Hi, I've previously reported a bug that [RandomForestClassifier](https://github.com/scikit-learn/scikit-learn/issues/24124) consumes more memory on newer versions, and I've also noticed that the svm API also consumes more memory on newer versions. The detailed test information is as follows.
|Memory| Version|
|--|--|
|274MB|1.0.1|
|276MB|0.20.3|
|180MB|0.19.2|


### Steps/Code to Reproduce

[Download dataset](https://drive.google.com/drive/folders/1tI0h0ZoL8bnvlZyVsa0-QPdKTcrZ_kVv?usp=sharing)

```python
import math
import pandas as pd
import numpy as np
import random
import torch
from sklearn.svm import SVR
from sklearn.ensemble import BaggingRegressor

def random_init(**kwargs):
    random.seed(kwargs['seed'])
    np.random.seed(kwargs['seed'])
    torch.manual_seed(kwargs['seed'])
    torch.cuda.manual_seed(kwargs['seed'])
    torch.backends.cudnn.deterministic = True
def load_data(df,cv=False,target=False,**kwargs):
    num_samples = len(df)
    sample_size = sum([len(args['categories'][c]) for c in args['categories']]) + len(args['num_feats'])
    dataset = torch.zeros((num_samples,sample_size),dtype=torch.float)
    idx = 0
    for c in args['cat_feats']:
        for i in range(len(args['categories'][c])):
            dataset[np.array(df[c])==args['categories'][c][i],idx] = 1.0
            idx += 1
    for n in args['num_feats']:
        dataset[:,idx] = torch.from_numpy(np.array(df[n]))
        idx += 1
    if target:
        targets = torch.from_numpy(np.array(df['target']))
    else:
        targets = None
    
    if cv == False:
        return dataset, targets

    idx = [i for i in range(num_samples)]
    random.shuffle(idx)
    trainset = dataset[idx[0:int(num_samples*(1-kwargs['cv_percentage']))]]
    traintargets = targets[idx[0:int(num_samples*(1-kwargs['cv_percentage']))]]
    validset = dataset[idx[int(num_samples*(1-kwargs['cv_percentage'])):]]
    validtargets = targets[idx[int(num_samples*(1-kwargs['cv_percentage'])):]]
    return trainset, validset, traintargets, validtargets  
def get_stats(trainset):
    mean = torch.mean(trainset,dim=0)
    std = torch.std(trainset,dim=0)
    for i in range(trainset.shape[1]):
        if ((trainset[:,i]==0) | (trainset[:,i]==1)).all():
            mean[i] = 0.5
            std[i] = 1.0
    return mean, std
#Global arguments
args = {
    'cv_percentage': 0.1,
    'seed': 0,
    }
# Load data
random_init(**args)
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')
args['cat_feats'] = [c for c in np.sort(train_data.columns) if 'cat' in c]
args['num_feats'] = [c for c in np.sort(train_data.columns) if 'cont' in c]
args['categories'] = {c: np.unique(train_data[c]) for c in args['cat_feats']}
testset, _ = load_data(test_data,cv=False,target=False,**args)
trainset, validset, traintargets, validtargets = load_data(train_data,cv=True,target=True,**args)
args['mean'],args['std'] = get_stats(trainset)
val_pred = {}
test_pred = {}
# Bagged SVRs
import tracemalloc
tracemalloc.start()
print('Bagged SVRs...')
# 0.19.2 180MB
# 0.20.3 276
# SVR arguments
args = {**args,**{
    'kernel': 'linear',
    'n_estimators': 3,
    'max_samples': 20000,
    'max_features':50
    }}
random_init(**args)
svrs = BaggingRegressor(SVR(kernel=args['kernel'],C=1.0, epsilon=0.2),
                        n_estimators=args['n_estimators'],max_samples=args['max_samples'],max_features=args['max_features'])
svrs.fit(((trainset-args['mean'])/args['std']).numpy(),traintargets.numpy())
val_pred['svrs'] = svrs.predict(((validset-args['mean'])/args['std']).numpy())
test_pred['svrs'] = svrs.predict(((testset-args['mean'])/args['std']).numpy())
currentSVR, peakSVR = tracemalloc.get_traced_memory()
print(""SVR current memory usage is {"",currentSVR /1024/1024,""}MB; SVR Peak memory was :{"",peakSVR / 1024/1024,""}MB"")
```

### Expected Results

same memory usage

### Actual Results

new version use more memory.

### Versions

```shell
1.0.1, 0.20.3, 0.19.2
```
","['Bug', 'module:svm', 'Needs Investigation']","2022-08-06T16:44:58Z","3","1","https://github.com/scikit-learn/scikit-learn/issues/24138","Performance Issue"
"111","scikit-learn/scikit-learn","Gaussian Process regression get_params set_params does not yield same predictor? ","### Describe the bug

This is on version 1.1.1

I *think* the example below is the correct way of setting GPR params saved from a previous .fit() tuning via the optimizer. Reading through the code I don't yet understand why the parameters `self.L_` and `self.alpha_` are not being updated but maybe am missing something in my reading.

See below for the example. Everything passes except the last line. 

### Steps/Code to Reproduce

```python
import sys

import numpy as np
from scipy.linalg import cholesky
from sklearn.gaussian_process import GaussianProcessRegressor, kernels
from sklearn.gaussian_process.kernels import *

np.random.seed(0)

X = np.random.randn(100, 3)
y = np.random.randn(100, 1)

KERNEL = Matern()
# make it once, fit and get the params
gp_ = GaussianProcessRegressor(kernel=KERNEL, normalize_y=True, alpha=1e-5)
gp_.fit(X, y)
params = gp_.kernel_.get_params(deep=False)  # tried deep=True but doesn't matter

# make it again, don't fit, but set the params
gp = GaussianProcessRegressor(kernel=KERNEL, normalize_y=True, alpha=1e-5)
gp.optimizer = None  # this should make fit do very little
gp.fit(X, y)  # this creates kernele

# do it again for good measure? Maybe we need to do this several times to trigger it?
gp.kernel_.set_params(**params)
gp.fit(X, y)  # do this again to now recreate L_
gp.kernel_.set_params(**params)


def get_K_L_(self):
    K = self.kernel_(self.X_train_)
    K[np.diag_indices_from(K)] += self.alpha
    L_ = cholesky(K, lower=True)
    return K, L_


Ka, La = get_K_L_(gp_)
Kb, Lb = get_K_L_(gp)

a = gp_.__getstate__()
b = gp.__getstate__()

# here you will see alpha_ and L_ are not the same. Something is not updating them correctly?
# see https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/gaussian_process/_gpr.py#L322
for k in a:
    if isinstance(a[k], np.ndarray):
        print(k, type(a[k]), np.all(a[k] == b[k]))


assert np.all(gp_.X_train_ == gp.X_train_)  # passes
assert gp_.alpha == gp.alpha  # passes
assert np.allclose(Ka, Kb), 'Ka Kb do not match'  # passes
assert np.allclose(La, Lb), 'La Lb do not match'  # passes
# it is bizarre that the output of the kernels matchs but not L_????
assert np.allclose(gp.L_, gp_.L_), 'L_ do not match'  # FAILS
```

### Expected Results

No exceptions. The gp and gp_ should return the same results, have same state (except for the optimizer).

### Actual Results

```
_y_train_mean <class 'numpy.ndarray'> True
_y_train_std <class 'numpy.ndarray'> True
X_train_ <class 'numpy.ndarray'> True
y_train_ <class 'numpy.ndarray'> True
L_ <class 'numpy.ndarray'> False
alpha_ <class 'numpy.ndarray'> False
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-5-71be2b0a00fc> in <module>
     53 assert np.allclose(La, Lb), 'La Lb do not match'  # passes
     54 # it is bizarre that the output of the kernels matchs but not L_????
---> 55 assert np.allclose(gp.L_, gp_.L_), 'L_ do not match'  # FAILS

AssertionError: L_ do not match
```

### Versions

```shell
System:
    python: 3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]
executable: /home/cottrell/anaconda3/envs/3.10.4/bin/python
   machine: Linux-5.15.0-43-generic-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.1.1
          pip: 22.1.2
   setuptools: 61.2.0
        numpy: 1.23.1
        scipy: 1.8.1
       Cython: 0.29.30
       pandas: 1.4.3
   matplotlib: 3.5.2
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/cottrell/anaconda3/envs/3.10.4/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-742d56dc.3.20.so
        version: 0.3.20
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12

       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/cottrell/anaconda3/envs/3.10.4/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
    num_threads: 12

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/cottrell/anaconda3/envs/3.10.4/lib/python3.10/site-packages/scipy.libs/libopenblasp-r0-8b9e111f.3.17.so
        version: 0.3.17
threading_layer: pthreads
   architecture: Haswell
    num_threads: 12

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/cottrell/anaconda3/envs/3.10.4/lib/python3.10/site-packages/opencv_python_headless.libs/libopenblas-r0-f650aae0.3.3.so
        version: None
threading_layer: disabled
   architecture: Haswell
    num_threads: 1
```
","['Bug', 'module:gaussian_process', 'Needs Investigation']","2022-08-04T08:59:03Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/24107","Logical Bug"
"112","scikit-learn/scikit-learn","Weights are being normalized using number of samples as opposed to sum in GaussianMixture","### Describe the bug

Weights are being normalized at Line https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/mixture/_gaussian_mixture.py#L718 using `n_samples`. It should be done using `weights.sum()` as
done in `_m_step()` here: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/mixture/_gaussian_mixture.py#L756.

### Steps/Code to Reproduce

Weights are being normalized at Line https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/mixture/_gaussian_mixture.py#L718 using `n_samples`. It should be done using `weights.sum()` as
done in `_m_step()` here: https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/mixture/_gaussian_mixture.py#L756.

### Expected Results

Correct weights.

### Actual Results

Incorrect weights.

### Versions

```shell
System:
    python: 3.9.13 (main, May 24 2022, 21:28:31)  [Clang 13.1.6 (clang-1316.0.21.2)]
executable: /Users/kshitijgoel/Documents/main/code.nosync/self_organizing_gmm/.venv/bin/python
   machine: macOS-12.4-x86_64-i386-64bit

Python dependencies:
      sklearn: 1.1.1
          pip: 22.2.1
   setuptools: 62.3.2
        numpy: 1.23.1
        scipy: 1.8.1
       Cython: 0.29.30
       pandas: 1.4.3
   matplotlib: 3.5.2
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: False

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/kshitijgoel/Documents/main/code.nosync/self_organizing_gmm/.venv/lib/python3.9/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.20
threading_layer: pthreads
   architecture: Haswell
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/kshitijgoel/Documents/main/code.nosync/self_organizing_gmm/.venv/lib/python3.9/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.17
threading_layer: pthreads
   architecture: Haswell
    num_threads: 8
```
","['Bug', 'Moderate', 'module:mixture']","2022-08-02T16:18:04Z","7","1","https://github.com/scikit-learn/scikit-learn/issues/24085","Logical Bug"
"113","scikit-learn/scikit-learn","Build time test issue on arm64 and ppc64el","### Describe the bug

The Debian autobuilders catched a build time test error for the architectures [arm64](https://buildd.debian.org/status/fetch.php?pkg=scikit-learn&arch=arm64&ver=1.1.1-1&stamp=1653341247&raw=0) and [ppc64el](https://buildd.debian.org/status/fetch.php?pkg=scikit-learn&arch=ppc64el&ver=1.1.1-1&stamp=1653347532&raw=0).

This relates to issue #22503 but that was reported against version 1.0.2.  I created a new issue since it now links to the recent build logs of your latest version. 

### Steps/Code to Reproduce

If you have access to arm64 or ppc64el architectures try to build the code and run the test suite.

### Expected Results

See linked logs.

### Actual Results

If you browse the end of the logs I've linked above both architectures are ending up with the following test suite error:

    =================================== FAILURES ===================================
    ________ [doctest] sklearn.ensemble._weight_boosting.AdaBoostRegressor _________
    1026     Examples
    1027     --------
    1028     >>> from sklearn.ensemble import AdaBoostRegressor
    1029     >>> from sklearn.datasets import make_regression
    1030     >>> X, y = make_regression(n_features=4, n_informative=2,
    1031     ...                        random_state=0, shuffle=False)
    1032     >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)
    1033     >>> regr.fit(X, y)
    1034     AdaBoostRegressor(n_estimators=100, random_state=0)
    1035     >>> regr.predict([[0, 0, 0, 0]])
    Expected:
        array([4.7972...])
    Got:
        array([5.74049295])

The fact that the error is absolutely identical for am64 and ppc64el lets me assume that there is really an issue on non-amd64 64 bit architectures. 

### Versions

```shell
The packaged version ofscikit-learn which belongs to the linked autobuilder logs is 1.1.1
```
","['Bug', 'arch:arm', 'module:test-suite']","2022-07-15T07:42:21Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/23901","Dependency Issue"
"114","scikit-learn/scikit-learn","Issue with affinity propagation and dtype np.float32","While working on https://github.com/scikit-learn/scikit-learn/pull/23838, it seems that the issue observed in https://github.com/scikit-learn/scikit-learn/issues/10832 was not solved since it breaks for different random seeds.

Therefore, we should revisit the original issue.","['Bug', 'module:cluster']","2022-07-14T14:43:37Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/23897","Dependency Issue"
"115","scikit-learn/scikit-learn","check_estimator() not respecting custom estimator constraints via check_estimators_fit_returns_self()","### Describe the bug

# check_estimators_fit_returns_self() does not pass with n_feature constraints
Building a custom estimator extending BaseEstimator with certain constraints such as:

## Example of constraints in the test.
```
X = check_array(
    X, 
    accept_sparse = False,
    accept_large_sparse = False,
    ensure_min_samples = 2,
    ensure_min_features = 3,
    force_all_finite = 'allow-nan')
```
The input and output of the estimator results in error thrown within `check_estimator`.

It appears that the tests run by `check_estimator` still check with input data with n_features = 2 for instance. Thus, the check_array with the above constraints will throw an error. This is not limited to the number of features but also whether force_all_finite allows nan or not. Some tests check with this value as True.

The test `check_estimators_fit_returns_self()` generates blobs with n_features = 2. Thus, this test will fail for any estimator that has feature_constraints != 2. 

See: https://github.com/scikit-learn/scikit-learn/blob/baf0ea25d/sklearn/utils/estimator_checks.py#L2581-L2595

## TL:DR
It appears that despite setting certain constraints of the input X,y. Such as `force_all_finite` and `ensure_min_features`. Some tests still use the default values for the `check_array` function and fail the check_estimator test due to this.

## The full error
<details>
<summary> Full error </summary>

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [5], in <cell line: 1>()
----> 1 check_estimator(model)

File ~/miniforge3/lib/python3.9/site-packages/sklearn/utils/estimator_checks.py:623, in check_estimator(estimator, generate_only, Estimator)
    621 for estimator, check in checks_generator():
    622     try:
--> 623         check(estimator)
    624     except SkipTest as exception:
    625         # SkipTest is thrown when pandas can't be imported, or by checks
    626         # that are in the xfail_checks tag
    627         warnings.warn(str(exception), SkipTestWarning)

File ~/miniforge3/lib/python3.9/site-packages/sklearn/utils/_testing.py:318, in _IgnoreWarnings.__call__.<locals>.wrapper(*args, **kwargs)
    316 with warnings.catch_warnings():
    317     warnings.simplefilter(""ignore"", self.category)
--> 318     return fn(*args, **kwargs)

File ~/miniforge3/lib/python3.9/site-packages/sklearn/utils/estimator_checks.py:2595, in check_estimators_fit_returns_self(name, estimator_orig, readonly_memmap)
   2592     X, y = create_memmap_backed_data([X, y])
   2594 set_random_state(estimator)
-> 2595 assert estimator.fit(X, y) is estimator

File ~/Documents/VOIDEV/tandem-riding-detection-service/service/source/models/physics/_base.py:42, in PhysicsRegressor.fit(self, X, y)
     20 """"""A fitting function which only checks the format of X and y.
     21 
     22 About the expected input data
   (...)
     38     Returns self.
     39 """"""
     40 self.n_features_in_ = 3
---> 42 X, y = check_X_y(
     43     X,
     44     y,
     45     accept_sparse = False,
     46     accept_large_sparse = False,
     47     ensure_min_samples = 2,
     48     ensure_min_features = 3,
     49     force_all_finite = True)
     51 #assert X.shape[-1] == 3, f""Expected 3 features. Received {X.shape[-1]}""
     53 return self

File ~/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:1074, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
   1069         estimator_name = _check_estimator_name(estimator)
   1070     raise ValueError(
   1071         f""{estimator_name} requires y to be passed, but the target y is None""
   1072     )
-> 1074 X = check_array(
   1075     X,
   1076     accept_sparse=accept_sparse,
   1077     accept_large_sparse=accept_large_sparse,
   1078     dtype=dtype,
   1079     order=order,
   1080     copy=copy,
   1081     force_all_finite=force_all_finite,
   1082     ensure_2d=ensure_2d,
   1083     allow_nd=allow_nd,
   1084     ensure_min_samples=ensure_min_samples,
   1085     ensure_min_features=ensure_min_features,
   1086     estimator=estimator,
   1087     input_name=""X"",
   1088 )
   1090 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
   1092 check_consistent_length(X, y)

File ~/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:918, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)
    916     n_features = array.shape[1]
    917     if n_features < ensure_min_features:
--> 918         raise ValueError(
    919             ""Found array with %d feature(s) (shape=%s) while""
    920             "" a minimum of %d is required%s.""
    921             % (n_features, array.shape, ensure_min_features, context)
    922         )
    924 if copy and np.may_share_memory(array, array_orig):
    925     array = np.array(array, dtype=dtype, order=order)

ValueError: Found array with 2 feature(s) (shape=(21, 2)) while a minimum of 3 is required.
```
</details>

## Reproduction
Let's create a very basic estimator which only takes 3 features. Then run check_estimator on this.

```
class DummyEstimator(BaseEstimator):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        X, y = check_X_y(
            X,
            y,
            accept_sparse = False,
            accept_large_sparse = False,
            ensure_min_features = 3,
            force_all_finite = 'allow-nan')
        return self

    def predict(self, X):
        X  = check_array(
            X,
            accept_sparse = False,
            accept_large_sparse = False,
            ensure_min_features = 3,
            force_all_finite = 'allow-nan')
        
        # very dumb example but it does not matter
        y_pred = X[:,0] + X[:,1] + X[:,3]
        
        y_pred  = check_array(
            y_pred,
            accept_sparse = False,
            accept_large_sparse = False,
            ensure_min_features = 3,
            ensure_2d = False,
            force_all_finite = 'allow-nan')
        
        return y_pred
```

Error produced:
```ValueError: Found array with 2 feature(s) (shape=(21, 2)) while a minimum of 3 is required.```

### Steps/Code to Reproduce

Run a check_estimator on an instance of this dummy estimator

```
class DummyEstimator(BaseEstimator):
    def __init__(self):
        pass

    def fit(self, X, y=None):
        X, y = check_X_y(
            X,
            y,
            accept_sparse = False,
            accept_large_sparse = False,
            ensure_min_features = 3,
            force_all_finite = 'allow-nan')
        return self

    def predict(self, X):
        X  = check_array(
            X,
            accept_sparse = False,
            accept_large_sparse = False,
            ensure_min_features = 3,
            force_all_finite = 'allow-nan')
        
        # very dumb example but it does not matter
        y_pred = X[:,0] + X[:,1] + X[:,3]
        
        y_pred  = check_array(
            y_pred,
            accept_sparse = False,
            accept_large_sparse = False,
            ensure_min_features = 3,
            ensure_2d = False,
            force_all_finite = 'allow-nan')
        
        return y_pred
```

### Expected Results

Passing all tests.

### Actual Results

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Input In [5], in <cell line: 1>()
----> 1 check_estimator(DummyEstimator())

File ~/miniforge3/lib/python3.9/site-packages/sklearn/utils/estimator_checks.py:623, in check_estimator(estimator, generate_only, Estimator)
    621 for estimator, check in checks_generator():
    622     try:
--> 623         check(estimator)
    624     except SkipTest as exception:
    625         # SkipTest is thrown when pandas can't be imported, or by checks
    626         # that are in the xfail_checks tag
    627         warnings.warn(str(exception), SkipTestWarning)

File ~/miniforge3/lib/python3.9/site-packages/sklearn/utils/_testing.py:318, in _IgnoreWarnings.__call__.<locals>.wrapper(*args, **kwargs)
    316 with warnings.catch_warnings():
    317     warnings.simplefilter(""ignore"", self.category)
--> 318     return fn(*args, **kwargs)

File ~/miniforge3/lib/python3.9/site-packages/sklearn/utils/estimator_checks.py:2595, in check_estimators_fit_returns_self(name, estimator_orig, readonly_memmap)
   2592     X, y = create_memmap_backed_data([X, y])
   2594 set_random_state(estimator)
-> 2595 assert estimator.fit(X, y) is estimator

File ~/Documents/VOIDEV/tandem-riding-detection-service/service/source/models/physics/_base.py:15, in DummyEstimator.fit(self, X, y)
     14 def fit(self, X, y=None):
---> 15     X, y = check_X_y(
     16         X,
     17         y,
     18         accept_sparse = False,
     19         accept_large_sparse = False,
     20         ensure_min_features = 3,
     21         force_all_finite = 'allow-nan')
     22     return self

File ~/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:1074, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
   1069         estimator_name = _check_estimator_name(estimator)
   1070     raise ValueError(
   1071         f""{estimator_name} requires y to be passed, but the target y is None""
   1072     )
-> 1074 X = check_array(
   1075     X,
   1076     accept_sparse=accept_sparse,
   1077     accept_large_sparse=accept_large_sparse,
   1078     dtype=dtype,
   1079     order=order,
   1080     copy=copy,
   1081     force_all_finite=force_all_finite,
   1082     ensure_2d=ensure_2d,
   1083     allow_nd=allow_nd,
   1084     ensure_min_samples=ensure_min_samples,
   1085     ensure_min_features=ensure_min_features,
   1086     estimator=estimator,
   1087     input_name=""X"",
   1088 )
   1090 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
   1092 check_consistent_length(X, y)

File ~/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:918, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)
    916     n_features = array.shape[1]
    917     if n_features < ensure_min_features:
--> 918         raise ValueError(
    919             ""Found array with %d feature(s) (shape=%s) while""
    920             "" a minimum of %d is required%s.""
    921             % (n_features, array.shape, ensure_min_features, context)
    922         )
    924 if copy and np.may_share_memory(array, array_orig):
    925     array = np.array(array, dtype=dtype, order=order)

ValueError: Found array with 2 feature(s) (shape=(21, 2)) while a minimum of 3 is required.
```

### Versions

```shell
System:
    python: 3.9.7 (default, Sep 16 2021, 23:53:23)  [Clang 12.0.0 ]
executable: /Users/alexandervialabellander/miniforge3/bin/python
   machine: macOS-12.2.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.1.1
          pip: 21.2.4
   setuptools: 58.0.4
        numpy: 1.22.3
        scipy: 1.7.3
       Cython: None
       pandas: 1.3.5
   matplotlib: 3.5.0
       joblib: 1.1.0
threadpoolctl: 2.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
         prefix: libomp
       filepath: /Users/alexandervialabellander/miniforge3/lib/python3.9/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/alexandervialabellander/miniforge3/lib/python3.9/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.18
threading_layer: pthreads
   architecture: armv8
    num_threads: 8

       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/alexandervialabellander/miniforge3/lib/libopenblasp-r0.3.17.dylib
        version: 0.3.17
threading_layer: pthreads
   architecture: armv8
    num_threads: 8
```
","['Bug', 'module:test-suite', 'Developer API']","2022-07-12T09:22:08Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/23885","Logical Bug"
"116","scikit-learn/scikit-learn","`OneVsOneClassifier` does not accept custom input types","### Describe the bug

Due to the additional validation in #6626, `OneVsOneClassifier` cannot be used with custom types that work like arrays but cannot be converted to arrays. In my case I am the maintainer of [scikit-fda](https://fda.readthedocs.io/en/latest/), a functional data library that attempts to be compatible with scikit-learn. The metaestimators `OneVsRestClassifier` and `OneVsOneClassifier` should be trivially compatible with our data. Currently `OneVsRestClassifier` works fine, while `OneVsOneClassifier` doesn't.

I think that scikit-learn has sometimes very aggressive validation that makes it difficult to extend it for custom objects, as in this case.

### Steps/Code to Reproduce

I have no example using only scikit-learn, as you need custom types and classifiers / transformers to trigger it
The following code is adapted from https://fda.readthedocs.io/en/latest/auto_tutorial/plot_skfda_sklearn.html#multiclass-and-multioutput-classification-utilities
Note that the code works if you replace OneVsOneClassifier with OneVsRestClassifier, as the later does not have that aggressive validation.

```python
import skfda
import skfda.preprocessing.dim_reduction.variable_selection as vs
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.multiclass import OneVsOneClassifier

X, y = skfda.datasets.fetch_phoneme(return_X_y=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

pipeline = Pipeline([
    (""dim_reduction"", vs.RKHSVariableSelection(n_features_to_select=3)),
    (""classifier"", SVC()),
])

multiclass = OneVsOneClassifier(pipeline)

multiclass.fit(X_train, y_train)
multiclass.score(X_test, y_test)
```

### Expected Results

The result of the classification.

### Actual Results

```
TypeError                                 Traceback (most recent call last)
TypeError: float() argument must be a string or a number, not 'FDataGrid'

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
<ipython-input-2-66e12b97fe3e> in <module>
     17 multiclass = OneVsOneClassifier(pipeline)
     18 
---> 19 multiclass.fit(X_train, y_train)
     20 multiclass.score(X_test, y_test)

~/Programas/Utilidades/Lenguajes/miniconda3/envs/fda38/lib/python3.8/site-packages/sklearn/multiclass.py in fit(self, X, y)
    726         """"""
    727         # We need to validate the data because we do a safe_indexing later.
--> 728         X, y = self._validate_data(
    729             X, y, accept_sparse=[""csr"", ""csc""], force_all_finite=False
    730         )

~/Programas/Utilidades/Lenguajes/miniconda3/envs/fda38/lib/python3.8/site-packages/sklearn/base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)
    579                 y = check_array(y, **check_y_params)
    580             else:
--> 581                 X, y = check_X_y(X, y, **check_params)
    582             out = X, y
    583 

~/Programas/Utilidades/Lenguajes/miniconda3/envs/fda38/lib/python3.8/site-packages/sklearn/utils/validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
    962         raise ValueError(""y cannot be None"")
    963 
--> 964     X = check_array(
    965         X,
    966         accept_sparse=accept_sparse,

~/Programas/Utilidades/Lenguajes/miniconda3/envs/fda38/lib/python3.8/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
    744                     array = array.astype(dtype, casting=""unsafe"", copy=False)
    745                 else:
--> 746                     array = np.asarray(array, order=order, dtype=dtype)
    747             except ComplexWarning as complex_warning:
    748                 raise ValueError(

ValueError: setting an array element with a sequence.

```

### Versions

```shell
System:
    python: 3.8.0 | packaged by conda-forge | (default, Nov 22 2019, 19:11:38)  [GCC 7.3.0]
executable: /home/carlos/Programas/Utilidades/Lenguajes/miniconda3/envs/fda38/bin/python3.8
   machine: Linux-5.4.0-92-generic-x86_64-with-glibc2.10

Python dependencies:
          pip: 21.3.1
   setuptools: 60.5.0
      sklearn: 1.0.2
        numpy: 1.22.0
        scipy: 1.7.3
       Cython: 0.29.26
       pandas: 1.4.0
   matplotlib: 3.5.1
       joblib: 1.1.0
threadpoolctl: 3.0.0

Built with OpenMP: True
```
","['Bug', 'module:multiclass']","2022-06-28T12:39:29Z","6","1","https://github.com/scikit-learn/scikit-learn/issues/23779","Logical Bug"
"117","scikit-learn/scikit-learn","fetch_lfw_pairs issue","### Describe the bug

On two different machines for exactly same code, I am getting different result.



### Steps/Code to Reproduce

```py
from sklearn.datasets import fetch_lfw_pairs
from sklearn.utils import resample

X = fetch_lfw_pairs(
    subset=""test"",
    # funneled=False,
    # slice_=(slice(0, 250), slice(0, 250)),
    resize=1,
    color=True
  )

n_samples = 1
imgs, y = resample(X.pairs, X.target, n_samples=n_samples, random_state=101)

print(imgs[0][0])
```

### Expected Results

Both on both machines should be the same.

### Actual Results

Result on Machine 1:

```
array([[[ 77.,  77.,  77.],
        [ 94.,  88.,  76.],
        [107., 101.,  77.],
        ...,
        [159., 124.,  97.],
        [141., 110.,  84.],
        [136., 109.,  82.]],

       [[ 78.,  78.,  80.],
        [108., 101.,  91.],
        [128., 121.,  99.],
        ...,
        [159., 124.,  97.],
        [151., 120.,  94.],
        [149., 124.,  98.]],

       [[ 82.,  80.,  85.],
        [111., 104.,  96.],
        [130., 122., 103.],
        ...,
        [163., 131., 105.],
        [157., 129., 104.],
        [159., 138., 110.]],

       ...,

       [[ 27.,  27.,  27.],
        [ 27.,  27.,  27.],
        [ 24.,  24.,  24.],
        ...,
        [151., 172., 235.],
        [142., 163., 228.],
        [128., 150., 210.]],

       [[ 26.,  24.,  25.],
        [ 33.,  33.,  33.],
        [ 30.,  29.,  27.],
        ...,
        [150., 171., 236.],
        [136., 157., 222.],
        [112., 133., 194.]],

       [[ 31.,  30.,  26.],
        [ 36.,  35.,  33.],
        [ 37.,  36.,  34.],
        ...,
        [146., 167., 230.],
        [124., 145., 208.],
        [ 93., 112., 171.]]], dtype=float32)
```

Result on Machine 2:
```
array([[[0.        , 0.00392157, 0.        ],
        [0.00392157, 0.01176471, 0.04705882],
        [0.01960784, 0.07450981, 0.16470589],
        ...,
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.00392157]],

       [[0.        , 0.        , 0.        ],
        [0.00392157, 0.01176471, 0.05098039],
        [0.01960784, 0.07843138, 0.16862746],
        ...,
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.00392157]],

       [[0.        , 0.00392157, 0.        ],
        [0.00392157, 0.01568628, 0.05490196],
        [0.01568628, 0.07450981, 0.16470589],
        ...,
        [0.        , 0.        , 0.00392157],
        [0.        , 0.00392157, 0.        ],
        [0.        , 0.00392157, 0.        ]],

       ...,

       [[0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ],
        ...,
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ]],

       [[0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ],
        ...,
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ]],

       [[0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ],
        ...,
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ],
        [0.        , 0.        , 0.        ]]], dtype=float32)

```

### Versions

```shell
Machine 1:

System:
    python: 3.8.5 (default, Sep  4 2020, 07:30:14)  [GCC 7.3.0]
executable: /home/anks/miniconda3/envs/stealth/bin/python
   machine: Linux-5.17.9-1-MANJARO-x86_64-with-glibc2.10

Python dependencies:
          pip: 21.2.4
   setuptools: 58.0.4
      sklearn: 1.0.2
        numpy: 1.21.2
        scipy: 1.7.3
       Cython: 0.29.30
       pandas: 1.3.0
   matplotlib: 3.5.1
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True

Machine 2:

System:
    python: 3.8.5 | packaged by conda-forge | (default, Sep 24 2020, 16:55:52)  [GCC 7.5.0]
executable: /home/aimluser/miniconda3/envs/application_env/bin/python3.8
   machine: Linux-4.4.0-042stab145.3-x86_64-with-glibc2.10

Python dependencies:
          pip: 22.1.2
   setuptools: 62.1.0
      sklearn: 1.0.2
        numpy: 1.22.3
        scipy: 1.8.0
       Cython: None
       pandas: 1.3.0
   matplotlib: 3.5.2
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True
```
","['Bug', 'module:datasets', 'Needs Investigation']","2022-06-24T13:15:12Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/23750","Logical Bug"
"118","scikit-learn/scikit-learn","BUG unpenalized GLM with LBFGS solver does not yield minimum norm solution with fit_intercept=True","### Summary
While unpenalized GLMs like `PoissonRegressor(alpha=0, fit_intercept=False)` yield a minimum norm solution on wide data, `PoissonRegressor(alpha=0, fit_intercept=True)` does not.

Detected in #23314.

#### Minimal Reproducible Example
```python
import numpy as np
from numpy.testing import assert_allclose
import pytest
from sklearn.datasets import make_low_rank_matrix
from sklearn.linear_model import PoissonRegressor
from sklearn.linear_model._linear_loss import LinearModelLoss


alpha = 0  # unpenalized, even tiny values like 1e-10 does not help.
fit_intercept = True  # if set to False, all tests pass.
model = PoissonRegressor(
    alpha = alpha,
    fit_intercept=fit_intercept,
    tol=1e-12,
    max_iter=1000,
)

# same as glm_dataset in test_glm.py for ""wide""
n_samples, n_features = 4, 12
k = n_samples
rng = np.random.RandomState(42)
X = make_low_rank_matrix(
    n_samples=n_samples,
    n_features=n_features,
    effective_rank=k,
    tail_strength=0.1,
    random_state=rng,
)
X[:, -1] = 1  # last columns acts as intercept
U, s, Vt = np.linalg.svd(X, full_matrices=False)

raw_prediction = rng.uniform(low=-3, high=3, size=n_samples)
# minimum norm solution min ||w||_2 such that raw_prediction = X w:
# w = X'(XX')^-1 raw_prediction = V s^-1 U' raw_prediction
coef_unpenalized = Vt.T @ np.diag(1 / s) @ U.T @ raw_prediction

linear_loss = LinearModelLoss(base_loss=model._get_loss(), fit_intercept=True)
sw = np.full(shape=n_samples, fill_value=1 / n_samples)
y = linear_loss.base_loss.link.inverse(raw_prediction)

if fit_intercept:
    X = X[:, :-1]  # remove intercept
    intercept = coef_unpenalized[-1]
    coef = coef_unpenalized[:-1]
else:
    intercept = 0
    coef = coef_unpenalized

model.fit(X, y)

# This is true. So we have a solution.
assert_allclose(model.predict(X), y, rtol=1e-6)

# But it is NOT the minimum norm solution
norm_solution = np.linalg.norm(np.r_[intercept, coef])
norm_model = np.linalg.norm(np.r_[model.intercept_, model.coef_])
print(f""norm_solution = {norm_solution}"")
print(f""norm_model    = {norm_model}"")
assert norm_model == pytest.approx(norm_solution, rel=1e-4)
```
Output
```
norm_solution = 2.494710738642327
norm_model    = 2.5295744931923707
AssertionError
```

The strange thing is that setting `fit_intercept=False`, norms and coefficients are equal, i.e. the minimum norm solution is returned by the LBFGS solver.
On top, even setting a tiny positive penalty like `alpha=1e-10` does not give the minimum norm solution.","['Bug', 'module:linear_model']","2022-06-17T09:41:36Z","4","2","https://github.com/scikit-learn/scikit-learn/issues/23670","Logical Bug"
"119","scikit-learn/scikit-learn","`distance_threshold` not respected in `AgglomerativeClustering` with sparse `connectivity`","### Describe the bug

When passing a sparse `connectivity` to `AgglomerativeClustering` constrained by a given `distance_threshold`, the current implementation may return a clustering not respecting this constraint.

This is at least true for the 'complete' and 'average' linkage criteria. I found the origin of the problem in [`linkage_tree`](https://github.com/scikit-learn/scikit-learn/blob/80598905e517759b4696c74ecc35c6e2eb508cff/sklearn/cluster/_agglomerative.py#L384), because I saw the only moment the inter-point distances were calculated was here:

https://github.com/scikit-learn/scikit-learn/blob/80598905e517759b4696c74ecc35c6e2eb508cff/sklearn/cluster/_agglomerative.py#L565-L568

So the only distance info we have is for connected points. Then, for `linkage='complete'`, in the situation where you have clustered together two neighbours A and B, and in the next step you consider aggregating C, which is B's neighbour but not A's, if `d(A, C) > distance_threshold` but at the same time `d(B, C)` is the minimum remaining distance (at the top of the heap), then C will be aggregated with these two.

Now I'm not sure at all on how to fix this properly. One option is to compute all pairwise distances in the connection matrix `A` when `A` and the heap `inertia` are initialised:

 https://github.com/scikit-learn/scikit-learn/blob/80598905e517759b4696c74ecc35c6e2eb508cff/sklearn/cluster/_agglomerative.py#L596-L604

or to somehow compute the ones needed on the fly when two nodes with different neighbourhoods are merged. Both solutions seem quite expensive though.

To give you some context, I encountered this while trying to do a spatial aggregation of administrative boundaries (England's LSOAs), in such a way that an indicator (let's say average income) does not vary too much within the aggregated regions. In this kind of geographic context, this is a rather common task, the [Ward variant](https://pysal.org/spopt/generated/spopt.region.WardSpatial.html#spopt.region.WardSpatial) is even implemented in the `pysal` package.

### Steps/Code to Reproduce

```python
import numpy as np
import pandas as pd
from sklearn.feature_extraction.image import grid_to_graph
from sklearn.cluster import AgglomerativeClustering

rng = np.random.RandomState(0)
n_samples = 100
conn = np.ones([n_samples, n_samples], dtype=bool)
# Add some random sparsity, 70% at most
masked = (rng.random((n_samples, 70)) * n_samples).astype(int)
for i, nbh in enumerate(masked):
    conn[i, nbh] = False
    conn[nbh, i] = False
X = rng.randn(n_samples, 1)
clustering = AgglomerativeClustering(
    n_clusters=None,
    distance_threshold=1,
    connectivity=conn,
    linkage='complete',
    affinity='l1',
)
clustering.fit(X)
df = pd.DataFrame({'labels': clustering.labels_, 'x': X.flatten()})
clustered_df = df.groupby(""labels"")[""x""].agg([""min"", ""max""])

# This should be < distance_threshold
print((clustered_df['max'] - clustered_df['min']).max())
# 1.20248995732655
```

### Expected Results

A float lower than 1, meaning the linkage criterion was respected.

### Actual Results

1.20248995732655

### Versions

```shell
System:
    python: 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:56:21)  [GCC 10.3.0]
executable: /home/thomaslouf/.conda/envs/ses-ling/bin/python
   machine: Linux-4.15.0-144-generic-x86_64-with-glibc2.27

Python dependencies:
      sklearn: 1.1.1
          pip: 22.1.2
   setuptools: 62.3.2
        numpy: 1.22.4
        scipy: 1.8.1
       Cython: None
       pandas: 1.4.2
   matplotlib: 3.5.2
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/thomaslouf/.conda/envs/ses-ling/lib/libopenblasp-r0.3.20.so
        version: 0.3.20
threading_layer: pthreads
   architecture: Bulldozer
    num_threads: 32

       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/thomaslouf/.conda/envs/ses-ling/lib/libgomp.so.1.0.0
        version: None
    num_threads: 1
```
","['Bug', 'module:cluster', 'Needs Investigation']","2022-06-06T15:48:21Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/23550","Logical Bug"
"120","scikit-learn/scikit-learn","RFECV race condition on estimator","In RFECV, at
https://github.com/scikit-learn/scikit-learn/blob/fb3ed90fb501a755ce2938fb566bd0f6e2235054/sklearn/feature_selection/_rfe.py#L723-L726
the estimator is passed as-is to the fit function. Since `fit` modifies the object without copying, this is prone to race conditions (see example below).

Contrast this to BaseSearchCV, where the estimator is properly cloned:
https://github.com/scikit-learn/scikit-learn/blob/fb3ed90fb501a755ce2938fb566bd0f6e2235054/sklearn/model_selection/_search.py#L823-L833

---

<details>
<summary>On my system, with parameter `n_jobs=-1`, I got the following error:</summary>

```
5 fits failed with the following error:
Traceback (most recent call last):
  File "".../site-packages/sklearn/model_selection/_validation.py"", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "".../site-packages/sklearn/feature_selection/_rfe.py"", line 723, in fit
    scores = parallel(
  File "".../site-packages/joblib/parallel.py"", line 1056, in __call__
    self.retrieve()
  File "".../site-packages/joblib/parallel.py"", line 935, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File ""/usr/lib/python3.10/multiprocessing/pool.py"", line 771, in get
    raise self._value
  File ""/usr/lib/python3.10/multiprocessing/pool.py"", line 125, in worker
    result = (True, func(*args, **kwds))
  File "".../site-packages/joblib/_parallel_backends.py"", line 595, in __call__
    return self.func(*args, **kwargs)
  File "".../site-packages/joblib/parallel.py"", line 262, in __call__
    return [func(*args, **kwargs)
  File "".../site-packages/joblib/parallel.py"", line 262, in <listcomp>
    return [func(*args, **kwargs)
  File "".../site-packages/sklearn/utils/fixes.py"", line 117, in __call__
    return self.function(*args, **kwargs)
  File "".../site-packages/sklearn/feature_selection/_rfe.py"", line 37, in _rfe_single_fit
    return rfe._fit(
  File "".../site-packages/sklearn/feature_selection/_rfe.py"", line 327, in _fit
    self.scores_.append(step_score(self.estimator_, features))
  File "".../site-packages/sklearn/feature_selection/_rfe.py"", line 40, in <lambda>
    lambda estimator, features: _score(
  File "".../site-packages/sklearn/model_selection/_validation.py"", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File "".../site-packages/sklearn/metrics/_scorer.py"", line 219, in __call__
    return self._score(
  File "".../site-packages/sklearn/metrics/_scorer.py"", line 261, in _score
    y_pred = method_caller(estimator, ""predict"", X)
  File "".../site-packages/sklearn/metrics/_scorer.py"", line 71, in _cached_call
    return getattr(estimator, method)(*args, **kwargs)
  File "".../site-packages/sklearn/ensemble/_forest.py"", line 835, in predict
    return self.classes_.take(np.argmax(proba, axis=1), axis=0)
AttributeError: 'list' object has no attribute 'take'
```
</details>

It is generated from the following snippet:
```py
  rf = RandomForestClassifier()
  rfecv = RFECV(rf, scoring='accuracy', n_jobs=-1)
  rfecv.fit(X_train, y_train)
```

The error appears to happen because `n_outputs_` is not constant between runs. The error does not happen without parallelism.","['Bug', 'module:feature_selection']","2022-06-03T09:07:46Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/23533","Performance Issue"
"121","scikit-learn/scikit-learn","Poor performance of KNeighborsClassifier for sklearn version >=0.24.2","### Describe the bug

Hi. I used **sklearn.neighbors.KNeighborsClassifier** in my program but I noticed that its runtime increased when adopting higher versions of  sklearn.

As shown in the following experiment results, this model performs best if we use sklearn <0.24.2.

**My question is that why does such a library version perform better? Are there any issues in newer versions?**

The detailed information is as follows:
Runtime | Memory | Version
-- | -- | --
**451.2495478** | **2337.8830499649** | **1.0.1**
**441.4340003** | **2338.7937555313** | **0.24.2**
26.25025550 | 414.1812286377 | 0.23.2
26.42619310 | 408.6705312729 | 0.22.1
26.42619310 | 408.6052923203 | 0.22
31.0837617 | 409.0471181870 | 0.21.3
29.0283316 | 408.5676021576 | 0.20.3
22.6652508 | 409.7873554230 | 0.19.2

My program only used the apis of  KNeighborsClassifier provided by sklearn.
```Python
knn = KNeighborsClassifier(n_neighbors = 3)
```


### Steps/Code to Reproduce

I create a minor program that used KNeighborsClassifier and you can test it on [colab](https://colab.research.google.com/drive/1GU9Hw8jKObcO5a54-7cNXAA-t9BJuKFi?usp=sharing). The runtime is still higher when adopting the newer sklearn version on this example.

[train.csv](https://drive.google.com/file/d/1f77gSB472ouhZ5bK0ipocBVa3SweGKLc/view?usp=sharing)

Using the above `train.csv` and the following snippet:

```python
# %%
# %%
import pandas as pd

train_data = pd.read_csv(""train.csv"")
total = train_data.isnull().sum().sort_values(ascending=False)
percent_1 = train_data.isnull().sum() / train_data.isnull().count() * 100
percent_2 = (round(percent_1, 1)).sort_values(ascending=False)
missing_data = pd.concat([total, percent_2], axis=1, keys=[""Total"", ""%""])
cor = train_data.corr()
cor_target = abs(cor[""target""])
relevant_features = cor_target[cor_target > 0.5]
s = train_data.dtypes == ""object""
train_data_cat_var = list(s[s].index)
train_data.drop(
    [""nom_5"", ""nom_6"", ""nom_7"", ""nom_8"", ""nom_9"", ""ord_3"", ""ord_4"", ""ord_5""],
    axis=1,
    inplace=True,
)
train_data_cat_var = [
    ele
    for ele in train_data_cat_var
    if ele
    not in [""nom_5"", ""nom_6"", ""nom_7"", ""nom_8"", ""nom_9"", ""ord_3"", ""ord_4"", ""ord_5""]
]
final_train_data = pd.get_dummies(
    train_data, columns=train_data_cat_var, drop_first=True
)
features = final_train_data.drop([""target""], axis=1).columns
target = final_train_data[""target""]

# %%
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    final_train_data[features], target, test_size=0.2
)

# %%
from sklearn.neighbors import KNeighborsClassifier

# %%
%%time
knn = KNeighborsClassifier(n_neighbors=3, algorithm=""brute"")
knn.fit(X_train, y_train)
Y_pred_knn = knn.predict(X_test)
```


![image-20220520110041914](https://user-images.githubusercontent.com/26241319/169444004-c50c2e22-489c-4ce9-8771-99bec82693bc.png)


### Expected Results

Similar runtime for the different sklearn versions or better performance for higher sklearn versions.

### Actual Results

According to the above experiment results, when sklearn version is 0.24.2 or 1.0.1, the performance of the model is worse.

### Versions


I test my program with sklearn 1.0.1, 0.24.2, 0.23.2, 0.22.1, 0.22, 0.21.3, 0.20.3 and 0.19.2.

","['Bug', 'module:neighbors', 'Needs Investigation']","2022-05-20T03:30:52Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/23427","Performance Issue"
"122","scikit-learn/scikit-learn","The data type of input data for LinearRegression class will affect the results","### Describe the bug

Our team just used the class sklearn.linear_model.LinearRegression to do multi-linear regression. And, we found out that the same data, which means the values are identical for each element, with different data formats will return different coefficients. The array we use are all integer values but saved as float32 and float64, separately. However, the array in float64 as input get us the expected result, but float32 doesn't.

The code below is a simple reproducible example.

### Steps/Code to Reproduce

```python
In [1]: from sklearn.linear_model import LinearRegression
    ...: import numpy as np
    ...: rng = np.random.default_rng(0)
    ...: x_1_array = rng.integers(low=1000, high=10000, size=10_000_000)
    ...: x_2_array = rng.integers(low=1000, high=10000, size=10_000_000)
    ...: x_3_array = rng.integers(low=1000, high=10000, size=10_000_000)
    ...: y_array = 3 * x_1_array + x_2_array - 5 * x_3_array
    ...: intercept_data = rng.integers(low=-3, high=3)
    ...: print(f'True intercept {intercept_data}')
    ...: y_array += intercept_data
    ...: X_train = np.stack([x_1_array,x_2_array,x_3_array],axis=-1)
    ...: y_train = y_array
    ...: linreg = LinearRegression()
    ...: model = linreg.fit(X_train.astype(np.float64), y_train)
    ...: print(f'Float 64 Intercept: {linreg.intercept_}')
    ...: print(f'Float 64 Coefficient: {linreg.coef_}')
    ...: model = linreg.fit(X_train.astype(np.float32), y_train)
    ...: print(f'Float 32 Intercept: {linreg.intercept_}')
    ...: print(f'Float 32 Coefficient: {linreg.coef_}')
```

### Expected Results

![image](https://user-images.githubusercontent.com/51532833/168492124-d98f9663-4899-47ee-83b2-4952462c37a6.png)

### Actual Results

![image](https://user-images.githubusercontent.com/51532833/168492136-4e0f4100-9e48-4f58-aa2a-97e826f06c61.png)


### Versions

```shell
System:
    python: 3.7.9 (v3.7.9:13c94747c7, Aug 15 2020, 01:31:08)  [Clang 6.0 (clang-600.0.57)]
executable: /Users/lazy/PycharmProjects/projects/bin/python
   machine: Darwin-21.4.0-x86_64-i386-64bit

Python dependencies:
          pip: 21.3
   setuptools: 58.2.0
      sklearn: 1.0.2
        numpy: 1.21.4
        scipy: 1.7.2
       Cython: None
       pandas: 1.3.4
   matplotlib: 3.4.3
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True
```
","['Bug', 'module:linear_model', 'float32']","2022-05-15T20:17:09Z","8","1","https://github.com/scikit-learn/scikit-learn/issues/23376","Logical Bug"
"123","scikit-learn/scikit-learn","SelfTrainingClassifier on a Pipeline","### Describe the bug

SelfTrainingClassifier cannot be fit on text data even if the base_estimator parameter is an estimator that can accept text data (e.g. a pipeline with text preprocessing). In particular, it seems that SelfTrainingClassifier validates the data on the classifier within the pipeline (hence being unable to accept text) instead of validating on the entire pipeline (which can accept text).

### Steps/Code to Reproduce

```python
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.semi_supervised import SelfTrainingClassifier

#pulling text data
categories = ['alt.atheism', 'soc.religion.christian']
twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)

X = pd.DataFrame({'text': twenty_train.data})
y = twenty_train.target
#changing some labels to -1 for semi supervised
y[len(y) - 100:len(y)] = -1

#building pipeline
preprocessor = ColumnTransformer(
    transformers =  [
        ('vectorizer', CountVectorizer(), 'text')
    ])

classifier = MultinomialNB()

pipe = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', classifier)])

#fitting SelfTrainingClassifier with the pipeline as the base estimator
stc = SelfTrainingClassifier(pipe)
stc.fit(X,y)

#note that fitting pipe does work
#pipe.fit(X,y)
```

### Expected Results

No error is thrown.

### Actual Results

```python-traceback
ValueError                                Traceback (most recent call last)
Input In [27], in <cell line: 30>()
     28 #fitting SelfTrainingClassifier with the pipeline as the base estimator
     29 stc = SelfTrainingClassifier(pipe)
---> 30 stc.fit(X,y)

File ~/code/scikit-learn/sklearn/semi_supervised/_self_training.py:181, in SelfTrainingClassifier.fit(self, X, y)
    162 """"""
    163 Fit self-training classifier using `X`, `y` as training data.
    164 
   (...)
    177     Fitted estimator.
    178 """"""
    179 # we need row slicing support for sparce matrices, but costly finiteness check
    180 # can be delegated to the base estimator.
--> 181 X, y = self._validate_data(
    182     X, y, accept_sparse=[""csr"", ""csc"", ""lil"", ""dok""], force_all_finite=False
    183 )
    185 if self.base_estimator is None:
    186     raise ValueError(""base_estimator cannot be None!"")

File ~/code/scikit-learn/sklearn/base.py:596, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, **check_params)
    594         y = check_array(y, input_name=""y"", **check_y_params)
    595     else:
--> 596         X, y = check_X_y(X, y, **check_params)
    597     out = X, y
    599 if not no_val_X and check_params.get(""ensure_2d"", True):

File ~/code/scikit-learn/sklearn/utils/validation.py:1074, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
   1069         estimator_name = _check_estimator_name(estimator)
   1070     raise ValueError(
   1071         f""{estimator_name} requires y to be passed, but the target y is None""
   1072     )
-> 1074 X = check_array(
   1075     X,
   1076     accept_sparse=accept_sparse,
   1077     accept_large_sparse=accept_large_sparse,
   1078     dtype=dtype,
   1079     order=order,
   1080     copy=copy,
   1081     force_all_finite=force_all_finite,
   1082     ensure_2d=ensure_2d,
   1083     allow_nd=allow_nd,
   1084     ensure_min_samples=ensure_min_samples,
   1085     ensure_min_features=ensure_min_features,
   1086     estimator=estimator,
   1087     input_name=""X"",
   1088 )
   1090 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
   1092 check_consistent_length(X, y)

File ~/code/scikit-learn/sklearn/utils/validation.py:856, in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)
    854         array = array.astype(dtype, casting=""unsafe"", copy=False)
    855     else:
--> 856         array = np.asarray(array, order=order, dtype=dtype)
    857 except ComplexWarning as complex_warning:
    858     raise ValueError(
    859         ""Complex data not supported\n{}\n"".format(array)
    860     ) from complex_warning

File ~/mambaforge/envs/dev/lib/python3.10/site-packages/pandas/core/generic.py:2064, in NDFrame.__array__(self, dtype)
   2063 def __array__(self, dtype: npt.DTypeLike | None = None) -> np.ndarray:
-> 2064     return np.asarray(self._values, dtype=dtype)

ValueError: could not convert string to float: 'From: [...]'
```

### Versions

```shell
System:
    python: 3.10.4 

Python dependencies:
          pip: 22.0.4
   setuptools: 62.1.0
      sklearn: 1.0.2
        numpy: 1.21.5
        scipy: 1.7.3
       Cython: None
       pandas: 1.4.2
   matplotlib: None
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True
```
","['Bug', 'module:semi_supervised']","2022-05-10T16:04:11Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/23323","Logical Bug"
"124","scikit-learn/scikit-learn","Yeo-Johnson Power Transformer gives Numpy warning (and raises scipy.optimize._optimize.BracketError in some cases)","### Describe the bug

When I use a power transformer with yeo-johnson method I get this warning in numpy:

`../lib/python3.10/site-packages/numpy/core/_methods.py:235: RuntimeWarning: overflow encountered in multiply`

Strangely I can't surpress this warning with filter.warnings()

**Tl;dr: The eroor seems to appear when multiple instances of the transformer are called, in this example because n_jobs=2 is set in the preprocessor. But the bug also appears with n_jobs=1 when you call this preprocessor in a grid search for example.**

### Steps/Code to Reproduce

The following code snippet will raise a warning:

```python
import numpy as np
from sklearn.preprocessing import PowerTransformer

rng = np.random.default_rng(0)
X = rng.exponential(size=(50,))
X -= X.max()
X = -X
X += 100
X = X.reshape(-1, 1)

method = ""yeo-johnson""
transformer = PowerTransformer(method=method, standardize=False)
transformer.fit_transform(X)
```

### Expected Results

No Warnings

### Actual Results
```pytb
/Users/glemaitre/mambaforge/envs/dev/lib/python3.8/site-packages/numpy/core/_methods.py:233: RuntimeWarning: overflow encountered in multiply
  x = um.multiply(x, x, out=x)

```

### Versions

```shell
System:
    python: 3.8.12 | packaged by conda-forge | (default, Sep 16 2021, 01:38:21)  [Clang 11.1.0 ]
executable: /Users/glemaitre/mambaforge/envs/dev/bin/python
   machine: macOS-12.3.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.2.dev0
          pip: 21.3
   setuptools: 58.2.0
        numpy: 1.21.6
        scipy: 1.8.0
       Cython: 0.29.24
       pandas: 1.4.2
   matplotlib: 3.4.3
       joblib: 1.0.1
threadpoolctl: 2.2.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/glemaitre/mambaforge/envs/dev/lib/libopenblas_vortexp-r0.3.18.dylib
        version: 0.3.18
threading_layer: openmp
   architecture: VORTEX
    num_threads: 8

       user_api: openmp
   internal_api: openmp
         prefix: libomp
       filepath: /Users/glemaitre/mambaforge/envs/dev/lib/libomp.dylib
        version: None
    num_threads: 8
```
","['Bug', 'module:preprocessing']","2022-05-10T09:40:28Z","28","0","https://github.com/scikit-learn/scikit-learn/issues/23319","Runtime Error"
"125","scikit-learn/scikit-learn","Investigate SAG/SAGA solver","#### Description
The newly introduced tight tests for `Ridge` in #22910 together with the random seed fixture in #22749 revealed some shortcomings of the sag and saga solver:

1. ~~It shows some random behavior even with fixed random seed.~~
2. The `tol` needs to be set much smaller to receive comparable results with the other solvers of `Ridge`.

Ideally, the cause for both issues can be identified and fixed.

#### Some links for context
https://github.com/scikit-learn/scikit-learn/issues/23014
https://github.com/scikit-learn/scikit-learn/pull/23017
https://github.com/scikit-learn/scikit-learn/pull/23026
https://github.com/scikit-learn/scikit-learn/pull/23152
#23177

","['Bug', 'Moderate', 'help wanted', 'module:linear_model']","2022-04-21T15:42:38Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/23180","Logical Bug"
"126","scikit-learn/scikit-learn","Wrong normalization for pseudolikelihood in Restricted Boltzmann Machine.","### Describe the bug

Pseudolikelihood of a sample under Restricted Boltzmann machine includes  the number of features  as a multiplier. 
Thus pseudolikelihood reflects the dimensionality of visible units, apart from the sample probability. 
I believe this multiplier should be removed. 

### Steps/Code to Reproduce

from sklearn.neural_network import BernoulliRBM
import numpy as np
 
RBM_= BernoulliRBM(verbose=1) 
X_10= np.ones([10,10])
X_1000= np.ones([10,1000])
                  

print('Learning RBM with 10 visible units:')
RBM_.fit(X_small)
print(100*'_')
print('Learning RBM with 1000 visible units')
RBM_.fit(X_large)
                  

### Expected Results

The pseudolikelihoods for 10 and 1000 visible units should be of the same order at corresponding learning stages.

### Actual Results

Learning RBM with 10 visible units:
[BernoulliRBM] Iteration 1, pseudo-likelihood = -0.19, time = 0.00s
[BernoulliRBM] Iteration 2, pseudo-likelihood = -0.28, time = 0.00s
[BernoulliRBM] Iteration 3, pseudo-likelihood = -0.22, time = 0.00s
[BernoulliRBM] Iteration 4, pseudo-likelihood = -0.12, time = 0.00s
[BernoulliRBM] Iteration 5, pseudo-likelihood = -0.10, time = 0.00s
[BernoulliRBM] Iteration 6, pseudo-likelihood = -0.07, time = 0.00s
[BernoulliRBM] Iteration 7, pseudo-likelihood = -0.07, time = 0.00s
[BernoulliRBM] Iteration 8, pseudo-likelihood = -0.08, time = 0.00s
[BernoulliRBM] Iteration 9, pseudo-likelihood = -0.10, time = 0.01s
[BernoulliRBM] Iteration 10, pseudo-likelihood = -0.06, time = 0.00s
____________________________________________________________________________________________________
Learning RBM with 1000 visible units
[BernoulliRBM] Iteration 1, pseudo-likelihood = -3.80, time = 0.01s
[BernoulliRBM] Iteration 2, pseudo-likelihood = -0.76, time = 0.02s
[BernoulliRBM] Iteration 3, pseudo-likelihood = -0.94, time = 0.01s
[BernoulliRBM] Iteration 4, pseudo-likelihood = -1.97, time = 0.01s
[BernoulliRBM] Iteration 5, pseudo-likelihood = -2.01, time = 0.01s
[BernoulliRBM] Iteration 6, pseudo-likelihood = -0.61, time = 0.01s
[BernoulliRBM] Iteration 7, pseudo-likelihood = -1.06, time = 0.01s
[BernoulliRBM] Iteration 8, pseudo-likelihood = -1.09, time = 0.01s
[BernoulliRBM] Iteration 9, pseudo-likelihood = -0.28, time = 0.01s
[BernoulliRBM] Iteration 10, pseudo-likelihood = -1.01, time = 0.01s


### Versions

```shell
System:
    python: 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]
executable: /home/x/PycharmProjects/RBM/env/bin/python
   machine: Linux-4.15.0-142-generic-x86_64-with-debian-stretch-sid

Python dependencies:
          pip: 22.0.4
   setuptools: 40.8.0
      sklearn: 1.0.2
        numpy: 1.21.5
        scipy: 1.7.3
       Cython: None
       pandas: None
   matplotlib: 3.5.1
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True
```
","['Bug', 'module:neural_network', 'Needs Investigation']","2022-04-21T15:30:40Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/23179","Logical Bug"
"127","scikit-learn/scikit-learn","Cache final transformer in pipeline with memory setting","### Describe the bug

When setting the `memory` parameter of a transformer `Pipeline` (i.e., one whose last step is a transformer), the final transformer is not cached.

Discovered at https://stackoverflow.com/q/71812869/10495893.

### Steps/Code to Reproduce

```
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
import time

class Test(BaseEstimator, TransformerMixin):
    def __init__(self, col):
        self.col = col

    def fit(self, X, y=None):
        print(self.col)
        return self

    def transform(self, X, y=None):
        for t in range(5):
            # just to slow it down / check caching.
            print(""."")
            time.sleep(1)
        #print(self.col)
        return X

pipline = Pipeline(
    [
        (""test"", Test(col=""this_column"")),
        (""test2"", Test(col=""that_column""))
    ],
    memory=""tmp/cache"",
)

pipline.fit(None)
pipline.fit(None)
pipline.fit(None)
```

### Expected Results

```
this_column
.
.
.
.
.
that_column
```

### Actual Results

```
this_column
.
.
.
.
.
that_column
that_column
that_column
```

### Versions

```shell
System:
    python: 3.7.13 (default, Mar 16 2022, 17:37:17)  [GCC 7.5.0]
executable: /usr/bin/python3
   machine: Linux-5.4.144+-x86_64-with-Ubuntu-18.04-bionic

Python dependencies:
          pip: 21.1.3
   setuptools: 57.4.0
      sklearn: 1.0.2
        numpy: 1.21.5
        scipy: 1.4.1
       Cython: 0.29.28
       pandas: 1.3.5
   matplotlib: 3.2.2
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True
```
","['Bug', 'module:pipeline']","2022-04-11T14:55:57Z","10","0","https://github.com/scikit-learn/scikit-learn/issues/23112","Logical Bug"
"128","scikit-learn/scikit-learn","`SequentialFeatureSelector` is not passing pandas df to estimator/pipeline","### Describe the bug

`SequentialFeatureSelector` cannot be used with a pipeline that expects a pandas dataframe (e.g. a one containing a `ColumnTransformer`) as an input.


At the same time the same pipeline can be used in `cross_val_score`.

### Steps/Code to Reproduce

```
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.compose import make_column_selector
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score

import pandas as pd
import numpy as np

df = pd.DataFrame(data=np.random.rand(10,4), columns=[""1"",""2"", ""3"", ""4""])
y = np.random.randint(0,2, size=(10,))

ct = ColumnTransformer(
    transformers=[
        (""numerical"", StandardScaler(), make_column_selector(pattern=""1"")),
    ],
    remainder=""passthrough"",
)

pipeline = Pipeline([(""ct"", ct), (""lr"", LogisticRegression())])

# this works
print(""cross_val_score"", cross_val_score(pipeline, df, y, cv=2))

sfs = SequentialFeatureSelector(pipeline, cv=2)
sfs.fit(df, y=y)
```

### Expected Results

No error is thrown and the features are selected.

### Actual Results

```
....sklearn/model_selection/_validation.py:372: FitFailedWarning: 
2 fits failed out of a total of 2.
...
ValueError: make_column_selector can only be applied to pandas dataframes
```

### Versions

```shell
System:
    python: 3.9.5 (default, Nov 23 2021, 15:27:38)  [GCC 9.3.0]
executable: /root/.cache/pypoetry/virtualenvs/science-yd9zGFjU-py3.9/bin/python
   machine: Linux-5.13.0-35-generic-x86_64-with-glibc2.31

Python dependencies:
          pip: 22.0.3
   setuptools: 60.9.3
      sklearn: 1.0.2
        numpy: 1.19.5
        scipy: 1.8.0
       Cython: None
       pandas: 1.4.1
   matplotlib: 3.5.1
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True
```
```
","['Bug', 'module:feature_selection']","2022-04-11T11:15:18Z","2","1","https://github.com/scikit-learn/scikit-learn/issues/23107","Logical Bug"
"129","scikit-learn/scikit-learn","BUG unpenalized Ridge does not give minimum norm solution","#### Describe the bug
As noted in #22910, `Ridge(alpha=0, fit_intercept=True)` does not give the minimal norm solution for wide data, i.e. `n_features > n_samples`.

Note that we nowhere guarantee that we provide the **minimum norm solution**.

Edit: Same seems to hold for `LinearRegression`, see #26164.

#### Probable Cause
For wide X, the least squares problem reads a bit different: $\mathrm{min} ||w||_2$ subject to $Xw = y$ with solution $w = X'(XX')^{-1} y$, see e.g. http://ee263.stanford.edu/lectures/min-norm.pdf.
With explicit intercept $w_0$, this reads $w = X'(XX' + 1 1')^{-1} y$, where 1 is a column vector of ones. $w_0 = 1'(XX' + 1 1')^{-1} y$.
**This is incompatible with our mean centering approach.**

#### Example

<details>

```python
import numpy as np
from numpy.testing import assert_allclose
from scipy import linalg
from sklearn.datasets import make_low_rank_matrix
from sklearn.linear_model import Ridge

n_samples, n_features = 4, 12  # wide data
k = min(n_samples, n_features)
rng = np.random.RandomState(42)
X = make_low_rank_matrix(
    n_samples=n_samples, n_features=n_features, effective_rank=k
)
X[:, -1] = 1  # last columns acts as intercept
U, s, Vt = linalg.svd(X)
assert np.all(s) > 1e-3  # to be sure X is not singular
U1, U2 = U[:, :k], U[:, k:]
Vt1, _ = Vt[:k, :], Vt[k:, :]
y = rng.uniform(low=-10, high=10, size=n_samples)
# w = X'(XX')^-1 y = V s^-1 U' y
coef_ols = Vt1.T @ np.diag(1 / s) @ U1.T @ y

model = Ridge(alpha=0, fit_intercept=True)
X = X[:, :-1]  # remove intercept
intercept = coef_ols[-1]
coef = coef_ols[:-1]
model.fit(X, y)

# Check that we have found a solution => residuals = 0
assert_allclose(model.predict(X), y)
# Check that `coef`, `intercept` also provide a valid solution
assert_allclose(X @ coef + intercept, y)

# Ridge does not give the minimum norm solution. (This should be equal.)
np.linalg.norm(np.r_[model.intercept_, model.coef_]) > np.linalg.norm(
    np.r_[intercept, coef]
)
```
This last statement should be be `False`. It proves that Ridge does not give the miminum norm solution. 

</details>","['Bug', 'module:linear_model']","2022-03-25T13:55:07Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/22947","Logical Bug"
"130","scikit-learn/scikit-learn","Birch is not finding clusters correctly","### Describe the bug

Birch do not correctly find clusters if the number of samples of one cluster is much larger than another cluster even if the distance between clusters is much larger than threshold.

### Steps/Code to Reproduce

```python
from sklearn.datasets import make_blobs
from sklearn.cluster import Birch
import numpy as np

X1, y1 = make_blobs(n_samples=10000, centers=[[0.6, 0.6]], cluster_std=0.01, random_state=0)
X2, y2 = make_blobs(n_samples=100, centers=[[0.1, 0.1]], cluster_std=0.01, random_state=0)

X = np.append(X1, X2, axis = 0)

brc = Birch(n_clusters=None, threshold = 0.1)
brc.fit(X)

brc.subcluster_centers_
```

### Expected Results

Expected two subclusters  since the distance between clusters is much larger than used threshold.

### Actual Results

Only one subcluster is detected

`brc.subcluster_centers_`:

```
array([[0.59494558, 0.59509375]])
```


### Versions

```shell
System:
    python: 3.8.6 | packaged by conda-forge | (default, Jan 25 2021, 23:21:18)  [GCC 9.3.0]
executable: /home/envs/env1_custom/bin/python
   machine: Linux-4.14.252

Python dependencies:
          pip: 22.0.3
   setuptools: 60.9.3
      sklearn: 1.0.2
        numpy: 1.20.2
        scipy: 1.7.0
       Cython: None
       pandas: 1.2.4
   matplotlib: 3.5.1
       joblib: 1.0.1
threadpoolctl: 2.1.0

Built with OpenMP: True
```
","['Bug', 'module:cluster']","2022-03-15T17:12:24Z","6","1","https://github.com/scikit-learn/scikit-learn/issues/22854","Logical Bug"
"131","scikit-learn/scikit-learn","Verbosity option is not working in GridSearchCV (Jupyter notebook)","### Describe the bug

So this issue has been addressed before [here ](https://github.com/scikit-learn/scikit-learn/issues/22291) by [darrencl](https://github.com/darrencl), but the user didn't follow up with [lesteve](https://github.com/lesteve) response.

The problem is that GridSearchCV doesn't show the elapsed time periodically, or any log, I am set`n_jobs = -1`, and `verbose = 1`. I tried setting `n_jobs` to other values, the same with `verbose`, but nothing happened.
Note that this didn't happen until I updated scikit-learn from version 0.22.1 to 1.0.2.

[lesteve](https://github.com/lesteve) in his response assumed that this problem is due to `ipykernel <6`, which is not the case with me.

### Steps/Code to Reproduce

```
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn import datasets

iris = datasets.load_iris()
params = {'n_estimators':[10,20,30,40,50,60],
          'max_depth':[20,50,60,70,80]}
grid_obj = GridSearchCV(estimator=RandomForestClassifier(), param_grid=params, n_jobs=-1, verbose=1, cv=5)
grid_obj.fit(iris.data, iris.target)
```

### Expected Results

This is the output when using version 0.22.1
![image](https://user-images.githubusercontent.com/17684093/158401635-31755363-2108-41e7-8f7f-08985abd03b8.png)


### Actual Results

```Fitting 5 folds for each of 30 candidates, totalling 150 fits```

### Versions

```shell
System:
    python: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]
executable: D:\Programs\ApplicationsSetup\anaconda3\python.exe
   machine: Windows-10-10.0.19041-SP0

Python dependencies:
          pip: 21.2.2
   setuptools: 58.0.4
      sklearn: 1.0.2
        numpy: 1.19.2
        scipy: 1.6.2
       Cython: 0.29.25
       pandas: 1.4.1
   matplotlib: 3.5.1
       joblib: 1.1.0
threadpoolctl: 2.2.0


!jupyter --version
Selected Jupyter core packages...
IPython          : 7.31.1
ipykernel        : 6.4.1
ipywidgets       : 7.6.5
jupyter_client   : 6.1.12
jupyter_core     : 4.9.1
jupyter_server   : 1.13.5
jupyterlab       : 3.2.9
nbclient         : 0.5.11
nbconvert        : 6.1.0
nbformat         : 5.1.3
notebook         : 6.4.9
qtconsole        : 5.2.2
traitlets        : 5.1.1
```
","['Bug', 'module:model_selection', 'Needs Investigation']","2022-03-15T14:48:31Z","22","6","https://github.com/scikit-learn/scikit-learn/issues/22849","Logical Bug"
"132","scikit-learn/scikit-learn","Formula for dual gap of elastic net in coordinate descent solver","### Describe the bug
The computation of the dual gap for the elastic net in the coordinate descent solver (`enet_coordinate_descent`) might be wrong.

The elastic net minimizes
```
Primal(w) = (1/2) * ||y - X w||_2^2 + alpha * ||w||_1 + beta/2 * ||w||_2^2
```

### Lasso
For the pure Lasso, i.e. `beta=0`, the dual becomes [1]
```
Dual(nu) = -1/2 * ||nu||_2^2 - nu'y   if ||X'nu||_ = max(abs(X'nu)) <= alpha   else -
```
with `nu = Xw - y` (=minus the residual `R`) as possible dual point. This yields the Lasso dual gap
```
Primal(w) - Dual(nu)
```
In case of `||X'nu||_ > alpha`, one uses a down-scaled variable `nu` in the dual.

### Elastic Net
For the elastic net, the dual, see section 5.2.3 in [2], becomes
```
Dual(nu) = -1/2 * ||nu||_2^2 - nu'y - 1/(2*beta) * sum_j (|X_j'nu| - alpha)_+^2
```
with `X_j` the j-th column of `X` and `x_+ = max(0, x)` the positive part.

### References
[1] Kim, Seung-Jean et al. An Interior-Point Method for Large-Scale $\ell_1$-Regularized Least Squares. IEEE Journal of Selected Topics in Signal Processing 1 (2007): 606-617. [pdf link](http://stanford.edu/~boyd/papers/pdf/l1_ls.pdf)
[2] Mendler-Dnner, Celestine et al. Primal-Dual Rates and Certificates. ICML (2016). [arxiv link](https://arxiv.org/pdf/1602.05205.pdf)

### Expected Results

```python
if beta > =0:
    R = y - X @ w  # note: R = -nu
    R_norm2 = R @ R
    gap = R_norm2 - R @ y + alpha * l1_norm + beta/2 * l2_norm
    gap += 1/(2*beta) * np.sum(np.max(0, np.abs(X.T @ R) - alpha)**2)
```
At least a test for it, similar to the pure Lasso case in https://github.com/scikit-learn/scikit-learn/blob/2c2f31d68c21b7647557b2f776e86c05954d80bf/sklearn/linear_model/tests/test_coordinate_descent.py#L292

### Actual Results

I start to be rattled by the `-beta * w` term in line 205.
https://github.com/scikit-learn/scikit-learn/blob/2c2f31d68c21b7647557b2f776e86c05954d80bf/sklearn/linear_model/_cd_fast.pyx#L205-L236


","['Bug', 'module:linear_model']","2022-03-14T18:48:33Z","13","0","https://github.com/scikit-learn/scikit-learn/issues/22836","Logical Bug"
"133","scikit-learn/scikit-learn","Second build failure for s390x and ppc64","### Describe the bug

Hi,
besides issue #22503 I've observed an _additional_ error in the test suite for the architectures s390x and ppc64.

### Steps/Code to Reproduce

```
pytest -m ""not network"" -v --color=no -k ""not test_load_boston_alternative and not test_dump""
```
You can find a [full build log for arm64](https://buildd.debian.org/status/fetch.php?pkg=scikit-learn&arch=s390x&ver=1.0.2-1&stamp=1644952396&raw=0) where you can seek for the string above to see the start of the test procedure.

### Expected Results

Passing the test suite without any error.

### Actual Results

    ___________ [doctest] sklearn.feature_extraction._hash.FeatureHasher ___________
    081     DictVectorizer : Vectorizes string-valued features using a hash table.
    082     sklearn.preprocessing.OneHotEncoder : Handles nominal/categorical features.
    083
    084     Examples
    085     --------
    086     >>> from sklearn.feature_extraction import FeatureHasher
    087     >>> h = FeatureHasher(n_features=10)
    088     >>> D = [{'dog': 1, 'cat':2, 'elephant':4},{'dog': 2, 'run': 5}]
    089     >>> f = h.transform(D)
    090     >>> f.toarray()
    Expected:
        array([[ 0.,  0., -4., -1.,  0.,  0.,  0.,  0.,  0.,  2.],
               [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])
    Got:
        array([[ 0.,  0.,  0., -1.,  0.,  0.,  0.,  4.,  0.,  2.],
               [ 0.,  0.,  0., -2., -5.,  0.,  0.,  0.,  0.,  0.]])

    /<<PKGBUILDDIR>>/.pybuild/cpython3_3.9/build/sklearn/feature_extraction/_hash.py:90: DocTestFailure
    = 2 failed, 22017 passed, 404 skipped, 3 deselected, 59 xfailed, 38 xpassed, 2351 warnings in 686.07s (0:11:26) =


### Versions

```shell
`1.0.2`

Also see the [full build log](https://buildd.debian.org/status/fetch.php?pkg=scikit-learn&arch=s390x&ver=1.0.2-1&stamp=1644952396&raw=0) what other components are used.
```
","['Bug', 'module:feature_extraction', 'Needs Investigation']","2022-02-16T07:22:50Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/22504","Runtime Error"
"134","scikit-learn/scikit-learn","`test_y_multioutput`in Gaussian Process is failing on Debian 32bit","### Describe the bug

On 32bit systems on debian the test `test_y_multioutput` is failing.

The test will probably be just skipped during the build, this is not urgent, but maybe something underlying multioutput and Gaussian Process is hidden here (see #19995).

### Steps/Code to Reproduce

```
 pytest sklearn/gaussian_process/tests/test_gpr.py -k test_y_multioutput
```

### Expected Results

The test passes.

### Actual Results

```
=================================== FAILURES ===================================
______________________________ test_y_multioutput ______________________________
    def test_y_multioutput():
        # Test that GPR can deal with multi-dimensional target values
        y_2d = np.vstack((y, y * 2)).T
    
        # Test for fixed kernel that first dimension of 2d GP equals the output
        # of 1d GP and that second dimension is twice as large
        kernel = RBF(length_scale=1.0)
    
        gpr = GaussianProcessRegressor(kernel=kernel, optimizer=None, normalize_y=False)
        gpr.fit(X, y)
    
        gpr_2d = GaussianProcessRegressor(kernel=kernel, optimizer=None, normalize_y=False)
        gpr_2d.fit(X, y_2d)
    
        y_pred_1d, y_std_1d = gpr.predict(X2, return_std=True)
        y_pred_2d, y_std_2d = gpr_2d.predict(X2, return_std=True)
        _, y_cov_1d = gpr.predict(X2, return_cov=True)
        _, y_cov_2d = gpr_2d.predict(X2, return_cov=True)
    
        assert_almost_equal(y_pred_1d, y_pred_2d[:, 0])
        assert_almost_equal(y_pred_1d, y_pred_2d[:, 1] / 2)
    
        # Standard deviation and covariance do not depend on output
        assert_almost_equal(y_std_1d, y_std_2d)
        assert_almost_equal(y_cov_1d, y_cov_2d)
    
        y_sample_1d = gpr.sample_y(X2, n_samples=10)
        y_sample_2d = gpr_2d.sample_y(X2, n_samples=10)
        assert_almost_equal(y_sample_1d, y_sample_2d[:, 0])
    
        # Test hyperparameter optimization
        for kernel in kernels:
            gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True)
            gpr.fit(X, y)
    
            gpr_2d = GaussianProcessRegressor(kernel=kernel, normalize_y=True)
            gpr_2d.fit(X, np.vstack((y, y)).T)
    
>           assert_almost_equal(gpr.kernel_.theta, gpr_2d.kernel_.theta, 4)
E           AssertionError: 
E           Arrays are not almost equal to 4 decimals
E           
E           Mismatched elements: 2 / 3 (66.7%)
E           Max absolute difference: 4.77758733
E           Max relative difference: 27.70946804
E            x: array([ -4.6052,  -0.3981, -11.5129])
E            y: array([  0.1724,   0.4926, -11.5129])
sklearn/gaussian_process/tests/test_gpr.py:379: AssertionError
```

### Versions

```shell
scikit-learn 1.0.2

Python dependencies:
          pip: None
   setuptools: 59.6.0
      sklearn: 1.0.2
        numpy: 1.21.5
        scipy: 1.7.3
       Cython: 0.29.24
       pandas: 1.3.5
   matplotlib: 3.5.1
       joblib: 0.17.0
threadpoolctl: 3.1.0

Built with OpenMP: True
```
```
","['Bug', 'module:gaussian_process']","2022-02-11T02:41:23Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/22446","Dependency Issue"
"135","scikit-learn/scikit-learn","StandardScaler and PolynomialFeatures fail on zero-feature inputs during fit (should become passthrough)","### Describe the bug

If you use StandardScaler or PolynomialFeatures (or other transformers, these are the two that hit me first) as elements in  a complex pipeline, an issue comes up if you ever fit these transformers on data that has no features (empty data). This happens a lot when you build a complex pipeline and you don't know what the features will be in advance. For example, in reinforcement learning problems, some problems will have context features while others won't, and we need pipelines for both the action and context features which are then concatenated (using FeatureUnion). My current workaround is that when I try to fit my pipeline, if it fails, it eliminates StandardScalers and PolynomialFeatures because they are a part of the context pipeline and we don't have context features for this problem, then fit again. 

### Steps/Code to Reproduce


from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('polynomial', PolynomialFeatures()),
    ('fully_transformed_action_and_context', StandardScaler()),
])

pipeline.fit([[]])

### Expected Results

No error is thrown. Pipeline becomes equivalent to passthrough.

### Actual Results

  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/pipeline.py"", line 390, in fit
    Xt = self._fit(X, y, **fit_params_steps)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/pipeline.py"", line 348, in _fit
    X, fitted_transformer = fit_transform_one_cached(
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/joblib/memory.py"", line 355, in __call__
    return self.func(*args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/pipeline.py"", line 893, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py"", line 847, in fit_transform
    return self.fit(X, **fit_params).transform(X)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/preprocessing/_polynomial.py"", line 287, in fit
    _, n_features = self._validate_data(X, accept_sparse=True).shape
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/base.py"", line 561, in _validate_data
    X = check_array(X, **check_params)
  File ""/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/utils/validation.py"", line 806, in check_array
    raise ValueError(
ValueError: Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required.


### Versions

```shell
System:
    python: 3.8.2 (v3.8.2:7b3ab5921f, Feb 24 2020, 17:52:18)  [Clang 6.0 (clang-600.0.57)]
executable: /usr/local/bin/python3.8
   machine: macOS-10.15.7-x86_64-i386-64bit
Python dependencies:
          pip: 20.0.2
   setuptools: 41.2.0
      sklearn: 1.0.1
        numpy: 1.19.5
        scipy: 1.4.1
       Cython: 0.29.16
       pandas: 1.4.0
   matplotlib: 3.2.1
       joblib: 0.14.1
threadpoolctl: 3.0.0
Built with OpenMP: True
```
","['Bug', 'module:preprocessing']","2022-02-10T19:26:04Z","9","0","https://github.com/scikit-learn/scikit-learn/issues/22442","Logical Bug"
"136","scikit-learn/scikit-learn","PCA hangs occasionally when applied to data that fits in memory","### Describe the bug

When trying to `fit` PCA with a large enough array that fits in memory (approximately `300 000 x 750`) with an aim to make about 3 times reduction, unstable results are obtained: in most cases the *fit succeeds*, but occasionaly it *hangs ""forever""*.

Random seed is fixed, but unfortunalely it does not help in this case.
The `svd_solver` option is left at the default. Looks like explicitly specifying `svd_solver='arpack'` does not cause this issue.

### Steps/Code to Reproduce

```python
import numpy as np
import random

np.random.seed(42)
random.seed(42)

from sklearn.decomposition import PCA


if __name__ == '__main__':
    embeddings = np.random.RandomState(42).normal(size=(328039, 768))
    embeddings = embeddings.astype(np.float32)

    for i in range(20):
        print('Iteration', i)
        pca = PCA(n_components=260, random_state=42)
        pca = pca.fit(embeddings)

```

### Expected Results

Stable results: consistently successful fits over a finite time.

### Actual Results

Sometimes the process hangs, in most cases in the first 10 iterations. I haven't found any consistency pattern at what time it fails.

### Versions

```shell
System:
   python: 3.9.7 | packaged by conda-forge 
   machine: Windows-10-10.0.19043-SP0
   RAM: 32Gb

Python dependencies:
          pip: 21.3.1
   setuptools: 60.8.1
      sklearn: 1.0.1
        numpy: 1.20.1
        scipy: 1.7.1
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.1.0
threadpoolctl: 3.1.0

Built with OpenMP: True
```
","['Bug', 'module:decomposition', 'Needs Investigation']","2022-02-10T16:53:43Z","10","0","https://github.com/scikit-learn/scikit-learn/issues/22434","Performance Issue"
"137","scikit-learn/scikit-learn","Y_trans is not equal to y_scores_ in PLSRegression","### Describe the bug

The documentation that `Y_trans` obtained from `fit_transform` should be equal to `y_scores_` fitted attribute but this is not the case.

We should investigate if this is normal or not

### Steps/Code to Reproduce

```python
In [1]: from sklearn.cross_decomposition import PLSRegression
   ...: X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]]
   ...: Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
   ...: pls2 = PLSRegression(n_components=2)
   ...: X_trans, Y_trans = pls2.fit_transform(X, Y)
```

### Expected Results

`Y_trans` should be equal to `y_scores_`.

### Actual Results

```python

In [2]: Y_trans
Out[2]: 
array([[-1.10089007,  2.64706486],
       [-1.73668766, -5.50753532],
       [ 1.05463999,  5.91255735],
       [ 1.78293774, -3.05208689]])

In [3]: pls2.y_scores_
Out[3]: 
array([[-1.40620988,  0.09323305],
       [-1.10143324, -0.85118366],
       [ 0.37266921,  1.64933181],
       [ 2.13497391, -0.8913812 ]])
```

### Versions

```shell
In [8]: sklearn.show_versions()

System:
    python: 3.8.12 | packaged by conda-forge | (default, Sep 16 2021, 01:38:21)  [Clang 11.1.0 ]
executable: /Users/glemaitre/mambaforge/envs/dev/bin/python
   machine: macOS-12.1-arm64-arm-64bit

Python dependencies:
          pip: 21.3
   setuptools: 58.2.0
      sklearn: 1.1.dev0
        numpy: 1.21.2
        scipy: 1.8.0.dev0+1902.b795164
       Cython: 0.29.24
       pandas: 1.3.3
   matplotlib: 3.4.3
       joblib: 1.2.0.dev0
threadpoolctl: 3.0.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /Users/glemaitre/mambaforge/envs/dev/lib/libopenblas_vortexp-r0.3.18.dylib
        version: 0.3.18
threading_layer: openmp
   architecture: VORTEX
    num_threads: 8

       user_api: openmp
   internal_api: openmp
         prefix: libomp
       filepath: /Users/glemaitre/mambaforge/envs/dev/lib/libomp.dylib
        version: None
    num_threads: 8
```
```
","['Bug', 'module:cross_decomposition']","2022-02-08T17:36:42Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/22420","Logical Bug"
"138","scikit-learn/scikit-learn","BUG? LinearSVC with hinge loss and L2 penalty, solver='liblinear' seems to get stuck compared to other solvers","### Describe the bug

Compared to a vanilla coordinate descent on the dual, liblinear plateaus at a higher objective value.

### Steps/Code to Reproduce
```python
import warnings
import numpy as np
import matplotlib.pyplot as plt
from numba import njit
from numpy.linalg import norm
from sklearn.svm import LinearSVC

from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings('ignore', category=ConvergenceWarning)

C = 0.1
clf = LinearSVC(C=C, penalty='l2', dual=True, fit_intercept=False, tol=1e-12,
                loss='hinge', random_state=0)


@njit
def loss(w, yX, C):
    return np.maximum(0, 1 - yX @ w).sum() + norm(w) ** 2 / (2 * C)


@njit
def cd_dual(yX, C, n_iter):
    """"""Solve problem with coordinate descent in the dual, ie minimize:
    norm(yX @ mu) ** 2 / 2 + mu.sum()    subject to 0 <= mu <= C

    with link equation w = yX @ mu  (=sum_i mu_i * y_i * X[i]""""""
    n_samples, n_features = yX.shape
    lipschitz = np.zeros(n_samples)
    for j in range(n_samples):
        lipschitz[j] = norm(yX[j]) ** 2

    mu = np.zeros(n_samples)
    w = np.zeros(n_features)
    losses = np.zeros(n_iter)
    for it in range(n_iter):
        for j in range(n_samples):
            old_mu_j = mu[j]
            grad_f_j = yX[j] @ w - 1
            new_mu_j = max(0, min(old_mu_j - grad_f_j / lipschitz[j], C))

            if new_mu_j != old_mu_j:
                w += yX[j] * (new_mu_j - old_mu_j)
                mu[j] = new_mu_j
        losses[it] = loss(w, yX, C)

    return w, losses


fig, axarr = plt.subplots(1, 2, figsize=(10, 4), constrained_layout=True)
for idx, (n_samples, n_features) in enumerate([[100, 500], [500, 100]]):
    np.random.seed(0)
    X = np.random.randn(n_samples, n_features)
    y = X @ np.random.randn(n_features) + 0.1 * np.random.randn(n_samples)
    y = 2 * (y > 0) - 1
    yX = X * y[:, None]

    max_iter = 3000 if n_samples > n_features else 300
    losses_sk = []
    iter_sk = np.geomspace(1, max_iter, num=50).astype(int)
    for n_iter in iter_sk:
        clf.max_iter = n_iter
        clf.fit(X, y)
        w = clf.coef_[0]
        losses_sk.append(loss(w, yX, C))
    losses_sk = np.array(losses_sk)

    w, losses_cd = cd_dual(yX, C, max_iter)
    loss_min = min(losses_cd.min(), losses_sk.min())

    axarr[idx].semilogy(iter_sk, losses_sk - loss_min, label='sklearn')
    axarr[idx].semilogy(losses_cd - loss_min, label='dual coordinate descent')
    axarr[idx].set_xlabel(""iteration"")
    axarr[idx].set_ylabel(""loss - min loss"")
    axarr[idx].set_title(f'n_samples, n_features=({n_samples}, {n_features})')
    axarr[idx].legend()
plt.show(block=False)


# no intercept is fitted
np.testing.assert_allclose(clf.decision_function(X), X @ clf.coef_[0])
```

### Expected Results

I expect both orange and blue suboptimality curve to go to zero. The blue one (sklearn) does not. This is particularly visible when n_samples > features (right plot)

### Actual Results

![image](https://user-images.githubusercontent.com/8993218/150823358-ae25d607-1a0c-4685-95dc-6acb134e43b0.png)


### Versions

```shell
System:
    python: 3.9.7 (default, Sep 16 2021, 13:09:58)  [GCC 7.5.0]
executable: /home/mathurin/anaconda3/bin/python
   machine: Linux-5.13.0-27-generic-x86_64-with-glibc2.31

Python dependencies:
          pip: 21.2.4
   setuptools: 58.0.4
      sklearn: 1.0.1
        numpy: 1.20.3
        scipy: 1.7.1
       Cython: 0.29.24
       pandas: 1.3.4
   matplotlib: 3.4.3
       joblib: 1.1.0
threadpoolctl: 2.2.0

Built with OpenMP: True
```
","['Bug', 'module:svm']","2022-01-24T16:28:27Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/22283","Logical Bug"
"139","scikit-learn/scikit-learn","Failing test test_quantile_estimates_calibration with q=.9","### Describe the bug

Hello,

I built the source of the main branch and ran the tests. The quantile estimates calibration test is failing after 2 mins 33 sec and a lot of processor usage. 

Thanks!

### Steps/Code to Reproduce

I launched the test in my dev folder with:

```
pytest -v -k test_quantile_estimates_calibration
```

### Expected Results

Test passed

### Actual Results

```python-traceback
=========================================================================================== test session starts ===========================================================================================
platform linux -- Python 3.10.2, pytest-6.2.5, py-1.11.0, pluggy-1.0.0 -- /home/francois/mambaforge/envs/sklearn-env/bin/python
cachedir: .pytest_cache
rootdir: /home/francois/Documents/scikit-learn, configfile: setup.cfg, testpaths: sklearn
plugins: xdist-2.5.0, forked-1.4.0
collected 26675 items / 26672 deselected / 3 selected                                                                                                                                                     

sklearn/linear_model/tests/test_quantile.py::test_quantile_estimates_calibration[0.5] PASSED                                                                                                        [ 33%]
sklearn/linear_model/tests/test_quantile.py::test_quantile_estimates_calibration[0.9] FAILED                                                                                                        [ 66%]
sklearn/linear_model/tests/test_quantile.py::test_quantile_estimates_calibration[0.05] PASSED                                                                                                       [100%]

================================================================================================ FAILURES =================================================================================================
________________________________________________________________________________ test_quantile_estimates_calibration[0.9] _________________________________________________________________________________

q = 0.9

    @pytest.mark.parametrize(""q"", [0.5, 0.9, 0.05])
    def test_quantile_estimates_calibration(q):
        # Test that model estimates percentage of points below the prediction
        X, y = make_regression(n_samples=1000, n_features=20, random_state=0, noise=1.0)
        quant = QuantileRegressor(
            quantile=q,
            alpha=0,
            solver_options={""lstsq"": False},
        ).fit(X, y)
>       assert np.mean(y < quant.predict(X)) == approx(q, abs=1e-2)
E       assert 0.721 == 0.9  1.0e-02
E         +0.721
E         -0.9  1.0e-02

sklearn/linear_model/tests/test_quantile.py:129: AssertionError
================================================================ 1 failed, 2 passed, 26672 deselected, 1032 warnings in 153.11s (0:02:33) =================================================================

```

### Versions

```
System:
    python: 3.10.2 | packaged by conda-forge | (main, Jan 14 2022, 08:02:09) [GCC 9.4.0]
executable: /home/francois/mambaforge/envs/sklearn-env/bin/python
   machine: Linux-5.13.0-27-generic-x86_64-with-glibc2.31

Python dependencies:
          pip: 21.3.1
   setuptools: 59.8.0
      sklearn: 1.1.dev0
        numpy: 1.22.0
        scipy: 1.7.3
       Cython: 0.29.26
       pandas: 1.3.5
   matplotlib: 3.5.1
       joblib: 1.1.0
threadpoolctl: 3.0.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
         prefix: libopenblas
       filepath: /home/francois/mambaforge/envs/sklearn-env/lib/libopenblasp-r0.3.18.so
        version: 0.3.18
threading_layer: pthreads
   architecture: SkylakeX
    num_threads: 8

       user_api: openmp
   internal_api: openmp
         prefix: libgomp
       filepath: /home/francois/mambaforge/envs/sklearn-env/lib/libgomp.so.1.0.0
        version: None
    num_threads: 8
```","['Bug', 'module:linear_model']","2022-01-21T15:13:45Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/22262","Logical Bug"
"140","scikit-learn/scikit-learn","Sphinx docs don't build correctly on Windows 10","### Describe the bug

I'm on Windows 10. The scikit-learn contributor docs prescribe the following to build the docs (after necessary installs): 
![image](https://user-images.githubusercontent.com/34613774/149794271-b07390d7-5c60-4c21-b7c5-3f6a69565222.png)

However, in practice, running `make` only results in: 
![image](https://user-images.githubusercontent.com/34613774/149794451-91c7aa44-2020-46eb-8dca-fbc87b50ea93.png)

Then, I attempt to run `make html-noplot`

### Steps/Code to Reproduce

```
cd doc
make
make html-noplot
```

### Expected Results

No error is thrown and docs are properly built with no plots.

### Actual Results

![image](https://user-images.githubusercontent.com/34613774/149795038-dc2a2ee9-be86-452a-89fb-9b8ee321a922.png)
![image](https://user-images.githubusercontent.com/34613774/149795120-447955de-2b2b-4daa-a7cc-543b7e8d357a.png)
this continues for a while, until
![image](https://user-images.githubusercontent.com/34613774/149795516-9b51a639-af59-4e65-b5c1-20f71379f7de.png)
which continues for a while, until it crashes at the end
![image](https://user-images.githubusercontent.com/34613774/149795707-b0e696e1-4a73-4ec1-a26e-5828427ed08a.png)

Finally, checking the actual directory I find only a `module/generated` directory with several images inside.

### Versions

![image](https://user-images.githubusercontent.com/34613774/149796023-401770e6-3485-43fd-b44f-df4e0ff2c0fe.png)
","['Bug', 'Build / CI']","2022-01-17T15:19:09Z","3","1","https://github.com/scikit-learn/scikit-learn/issues/22232","Dependency Issue"
"141","scikit-learn/scikit-learn","power_t: does it make sense for this parameter to have negative values","sklearn/linear_model/_stochastic_gradient.py

Ref #22115

>I do not anticipate negative `power_t` to be mathematically meaningful but apparently our code accepts it without crashing... So ok with documenting it.

_Originally posted by @ogrisel in https://github.com/scikit-learn/scikit-learn/pull/22115#r780109928_

@thomasjpfan 
>I feel like this a case where documenting `-inf` will lead to more people trying out. If this is not mathematically meaningful, then we could be promoting a bad practice?

cc:  @glemaitre","['Bug', 'help wanted', 'module:linear_model']","2022-01-10T19:07:21Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/22178","Logical Bug"
"142","scikit-learn/scikit-learn","Cross endianness and bitness pickle issues with KNeighborsClassifier / KDTree","Reported in https://github.com/scikit-learn/scikit-learn/issues/21237 (cross endianness). There is a similar issue for the cross bitness, to reproduce:

Generate a pickle on a 64bit machine:
```py
from sklearn.datasets import make_classification
X, y = make_classification(random_state=0)

from sklearn.neighbors import KNeighborsClassifier
clf = KNeighborsClassifier(algorithm='kd_tree')
clf.fit(X, y)
import pickle
pickle.dump(clf, open('/tmp/kneighbors.pkl', 'wb'))
```

Open it on a 32bit machine:
```
docker run -it -v /tmp:/io lesteve/i386-scikit-learn python3 -c 'import pickle; pickle.load(open(""/io/kneighbors.pkl"", ""rb""))'
```
Output:
```
WARNING: The requested image's platform (linux/386) does not match the detected host platform (linux/amd64) and no specific platform was requested
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""sklearn/neighbors/_binary_tree.pxi"", line 1062, in sklearn.neighbors._kd_tree.BinaryTree.__setstate__
    self._update_memviews()
  File ""sklearn/neighbors/_binary_tree.pxi"", line 1004, in sklearn.neighbors._kd_tree.BinaryTree._update_memviews
    self.idx_array = self.idx_array_arr
ValueError: Buffer dtype mismatch, expected 'ITYPE_t' but got 'long long'
```

Quite likely solving the issue is rather similar to https://github.com/scikit-learn/scikit-learn/pull/21552 and https://github.com/scikit-learn/scikit-learn/pull/21539.","['Bug', 'module:neighbors']","2021-11-04T11:17:12Z","2","1","https://github.com/scikit-learn/scikit-learn/issues/21553","Dependency Issue"
"143","scikit-learn/scikit-learn","Ridge with cholesky solver returns NaN","### Describe the bug

When using Ridge with a big number of features and the Cholesky solver, the model gets full of NaNs.
Playing with the number of features we can see that the imprecision grows until it becomes NaN.

The bug is architecture-dependent.
The provided snippet works perfectly in an `Intel(R) Xeon(R) CPU E3-1585L v5 @ 3.00GHz`.
It fails when running in an `Intel(R) Xeon(R) Gold 6140 CPU @ 2.30GHz`.

A workaround was found by blindly exporting OPENBLAS_CORETYPE=""Broadwell"".

### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.linear_model import Ridge

# we need something big to reproduce:
n_inputs = 1000
n_targets = 1
n_samples = 20000

rng = np.random.default_rng(seed=42)

b = rng.random((n_targets,))
w = rng.random((n_targets, n_inputs)) / n_inputs

X = rng.random((n_samples, n_inputs))
y = rng.random((n_samples, n_targets))

for i in range(n_samples):
    y[i] = (w @ X[i]) + b

# ""lsqr"" does not have the same problem as ""cholesky""
model = Ridge(alpha=0.0, tol=1e-6, solver=""cholesky"")
model.fit(X, y)

np.testing.assert_almost_equal(model.coef_, w)
np.testing.assert_almost_equal(model.intercept_, b)

error = np.abs(model.predict(X) - y).sum()
np.testing.assert_almost_equal(error, 0.0, decimal=4)

assert np.isfinite(model.coef_).all()
assert np.isfinite(model.intercept_).all()

print(""done!"")
```

### Expected Results

done!

### Actual Results

Numpy assertion errors as `model.coef_` is full of `NaN`.

### Versions

The software is the same on both machines:
```
System:
    python: 3.8.9 (default, Apr  2 2021, 11:20:07)  [GCC 10.3.0]
executable: ***/.venv/bin/python
   machine: Linux-3.10.0-1062.9.1.el7.x86_64-x86_64-with-glibc2.2.5

Python dependencies:
          pip: 20.2.3
   setuptools: 54.2.0.post0
      sklearn: 0.24.1
        numpy: 1.20.2
        scipy: 1.6.1
       Cython: 0.29.22
       pandas: 1.2.3
   matplotlib: 3.4.1
       joblib: 1.0.1
threadpoolctl: 2.1.0

Built with OpenMP: True
```","['Bug', 'module:linear_model']","2021-10-24T19:49:12Z","8","0","https://github.com/scikit-learn/scikit-learn/issues/21447","Logical Bug"
"144","scikit-learn/scikit-learn","Using a custom rbf kernel function for sklearn's SVC is way faster than built-in method","### Describe the bug

I've noticed a rather strange behavior when using **Scikit-Learn's SVC** implementation. Using the **built-in rbf kernel** with SVC is **slower** by magnitudes than passing a **custom rbf function to SVC()**. 

From what I could see and understand so far, the only difference between the two versions is that **in the built-in rbf case, not sklearn but libsvm will compute the kernel**. Passing a dedicated kernel function as hyperparameter to SVC() leads to the computation of the kernel inside sklearn, not in libsvm. The results are identical, but **the latter case takes only a fraction of the computation time**. 

I've created a toy dataset that mimics the data I am currently working on. This probably only becomes relevant with larger datasets. There, the discrepancy in computation time likely increases.

Does anyone know why this is happening? Is this the expected behavior?


### Steps/Code to Reproduce
```py
import numpy as np
from time import time
from sklearn.svm import SVC
from sklearn.datasets import make_classification
from sklearn.metrics.pairwise import rbf_kernel
from sklearn.metrics import accuracy_score

# create toy data
n_features = 1000
n_samples = 10000
n_informative = 10
X, y = make_classification(n_samples, n_features, n_informative=n_informative)
gamma = 1 / n_features

# fit SVC with built-in rbf kernel
svc_built_in = SVC(kernel='rbf', gamma=gamma)
np.random.seed(13)
t1 = time()
svc_built_in.fit(X, y)
acc = accuracy_score(y, svc_built_in.predict(X))
print(""Fitting SVC with built-in kernel took {:.1f} seconds"".format(time()-t1))
print(""Accuracy: {}"".format(acc))

# fit SVC with custom rbf kernel
svc_custom = SVC(kernel=rbf_kernel, gamma=gamma)
np.random.seed(13)
t1 = time()
svc_custom.fit(X, y)
acc = accuracy_score(y, svc_custom.predict(X))
print(""Fitting SVC with a custom kernel took {:.1f} seconds"".format(time()-t1))
print(""Accuracy: {}"".format(acc))
```
### Expected Results

I would have assumed that both versions should run in the same time.

### Actual Results

Fitting SVC with built-in kernel took 58.6 seconds
Accuracy: 0.9846
Fitting SVC with a custom kernel took 3.2 seconds
Accuracy: 0.9846

### Versions

System:
    python: 3.8.12 (default, Oct 12 2021, 13:49:34)  [GCC 7.5.0]
executable: /home/nwinter/anaconda3/envs/mmll_gists/bin/python
   machine: Linux-5.11.0-37-generic-x86_64-with-glibc2.17

Python dependencies:
          pip: 21.2.4
   setuptools: 58.0.4
      sklearn: 1.0
        numpy: 1.21.3
        scipy: 1.7.1
       Cython: None
       pandas: 1.3.4
   matplotlib: 3.4.3
       joblib: 1.1.0
threadpoolctl: 3.0.0

Built with OpenMP: True","['Bug', 'Performance', 'module:svm']","2021-10-22T18:43:34Z","4","2","https://github.com/scikit-learn/scikit-learn/issues/21410","Performance Issue"
"145","scikit-learn/scikit-learn","LogisticRegression with SAGA using sample_weight does not converge","### Describe the bug

I am fittinga logistic regression model on a sparse matrix and a binary response. Many of the rows of the matrix are repeated, so to speed things up I switched to a smaller sparse matrix with non-repeated rows and use the repetitions of the rows to calculate the`sample_weight`argument to`fit`.

The issue is that when I work with the weighted model, `fit` produces a warning that it dit not converge because it took too many steps. 

I looked a bit under the hood of `fit` and`sag_solver`does some scaling of `alpha` and `beta` using `n_samples`. The resulting `alpha_scaled` and `beta_scaled` are different between the weighted and unweighted cases and they should not be (the loss function is the same). Perhaps the equivalent scaling for the weighted case should be the sum of the weights (if the intended 'unit' of the weights is 'count') and not just `n_samples`. Not sure if this is the issue, but it just it made me worry that the`sample_weight`argument is used in a bit naive way just as a multiplier for the loss function, while there might be scaling implications that are not accounted for when deciding when to stop. 

UPDATE: It seems that the issue is with the SAGA solver. I tried with liblinear and it seems to work. This will solve my immediate problem, because for now I only want the L1-reg. It is still good to look into this, because at the moment only SAGA offers elasticnet.

### Steps/Code to Reproduce

[vttrifonov/logistic_sample_weights.ipynb](https://gist.github.com/vttrifonov/55ded32d11f93d26e64fdb2d4c7434d2)

### Expected Results

In the above code I expect the second fit to run much faster than the first and to produce the same coefficients. 

### Actual Results

It fails in both.

### Versions

System:
    python: 3.7.0 (default, Jun 28 2018, 07:39:16)  [Clang 4.0.1 (tags/RELEASE_401/final)]
executable: /Users/vtrifonov/projects/tiny-proteins/env/bin/python
   machine: Darwin-19.6.0-x86_64-i386-64bit

Python dependencies:
          pip: 21.2.2
   setuptools: 58.0.4
      sklearn: 0.24.2
        numpy: 1.20.3
        scipy: 1.7.1
       Cython: None
       pandas: 1.3.2
   matplotlib: 3.4.2
       joblib: 1.0.1
threadpoolctl: 2.2.0

Built with OpenMP: True","['Bug', 'Moderate', 'help wanted', 'module:linear_model']","2021-10-12T00:51:24Z","11","0","https://github.com/scikit-learn/scikit-learn/issues/21305","Dependency Issue"
"146","scikit-learn/scikit-learn","SpectralClustering breaks normalized Laplacian when the row sums are negative","### Describe the bug

_laplacian_dense from scipy\sparse\csgraph\_laplacian.py breaks, crushing SpectralClustering


### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.cluster import SpectralClustering
x = np.random.randn(100, 10)
x = x @ x.T
spectral = SpectralClustering(n_clusters=4, affinity='precomputed',
    random_state=0).fit(x)
result = list(spectral.labels_)
print(result)
```

### Expected Results

No error

### Actual Results

```python-traceback
$ C:/Users/Name/AppData/Local/Programs/Python/Python37/python.exe c:/Users/Name/Downloads/tmp1.py
C:\Users\Name\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse\csgraph\_laplacian.py:118: RuntimeWarning: invalid value encountered in sqrt
  w = np.where(isolated_node_mask, 1, np.sqrt(w))
Traceback (most recent call last):
  File ""c:/Users/Name/Downloads/tmp1.py"", line 6, in <module>
    random_state=0).fit(x)
  File ""C:\Users\Name\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\cluster\_spectral.py"", line 590, in fit
    verbose=self.verbose,
  File ""C:\Users\Name\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\cluster\_spectral.py"", line 308, in spectral_clustering
    drop_first=False,
  File ""C:\Users\Name\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\manifold\_spectral_embedding.py"", line 300, in spectral_embedding
    laplacian, k=n_components, sigma=1.0, which=""LM"", tol=eigen_tol, v0=v0
  File ""C:\Users\Name\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse\linalg\eigen\arpack\arpack.py"", line 1645, in eigsh
    hermitian=True, tol=tol)
  File ""C:\Users\Name\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse\linalg\eigen\arpack\arpack.py"", line 1069, in get_OPinv_matvec
    return LuInv(A).matvec
  File ""C:\Users\Name\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\sparse\linalg\eigen\arpack\arpack.py"", line 936, in __init__
    self.M_lu = lu_factor(M)
  File ""C:\Users\Name\AppData\Local\Programs\Python\Python37\lib\site-packages\scipy\linalg\decomp_lu.py"", line 
70, in lu_factor
    a1 = asarray_chkfinite(a)
  File ""C:\Users\Name\AppData\Local\Programs\Python\Python37\lib\site-packages\numpy\lib\function_base.py"", line 489, in asarray_chkfinite
    ""array must not contain infs or NaNs"")
ValueError: array must not contain infs or NaNs
```

### Versions

System:
    python: 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\Name\AppData\Local\Programs\Python\Python37\python.exe
   machine: Windows-10-10.0.19041-SP0

Python dependencies:
          pip: 21.2.4
   setuptools: 58.1.0
      sklearn: 1.0
        numpy: 1.21.2
        scipy: 1.7.1
       Cython: 0.29.24
       pandas: 1.3.3
   matplotlib: 3.4.3
       joblib: 1.0.1
threadpoolctl: 2.2.0","['Bug', 'module:cluster']","2021-10-07T20:36:58Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/21274","Logical Bug"
"147","scikit-learn/scikit-learn","CountVectorizer(lowercase=True,strip_accents='unicode') may produce vocab that contains uppercase chars","### Describe the bug

Some characters are transformed to uppercase chars (eg  -> TM), despite lowercase=True.
This then gives warning messages ""UserWarning: Upper case characters found in vocabulary while 'lowercase' is True""
(OK, there are worse things happening in the world ;-). Thanks for scikit-learn)

### Steps/Code to Reproduce

```python
from sklearn.feature_extraction.text import CountVectorizer
x = ['This is Problematic.','THIS IS NOT']

cv = CountVectorizer(
    lowercase=True,
    strip_accents='unicode',
    ngram_range=(1,1)
    )

x_v = cv.fit_transform(x)
print(cv.get_feature_names_out()) # Contains ""problematicTM""

# then you can get a warning
# ""UserWarning: Upper case characters found in vocabulary while 'lowercase' is True""
# if you both create a classifier AND run cv.fit_transform
# (no message if you only call one)

from sklearn.svm import LinearSVC
y = [1,0]
xtest = ['This is not']
ytest = [0]

# comment either of the 2 following lines, no warning message displayed
clf = LinearSVC(random_state=42).fit(x_v, y)
xtest_v = cv.transform(xtest)
```

### Expected Results

print: ['is' 'not' 'problematictm' 'this']

### Actual Results

print: ['is' 'not' 'problematicTM' 'this']

warning message:
/Users/fps/_fps/DeveloperTools/virtualenvs/fps_env/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents
  warnings.warn(

### Versions

/Users/fps/_fps/DeveloperTools/virtualenvs/fps_env/lib/python3.9/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.
  warnings.warn(

System:
    python: 3.9.2 (v3.9.2:1a79785e3e, Feb 19 2021, 09:06:10)  [Clang 6.0 (clang-600.0.57)]
executable: /Users/fps/_fps/DeveloperTools/virtualenvs/fps_env/bin/python3
   machine: macOS-10.16-x86_64-i386-64bit

Python dependencies:
          pip: 21.2.4
   setuptools: 49.2.1
      sklearn: 1.0
        numpy: 1.21.2
        scipy: 1.7.1
       Cython: None
       pandas: 1.3.3
   matplotlib: None
       joblib: 1.0.1
threadpoolctl: 2.2.0

Built with OpenMP: True","['Bug', 'module:feature_extraction']","2021-09-30T21:51:08Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/21207","Logical Bug"
"148","scikit-learn/scikit-learn","Is _generate_sample_indices working correctly ?","### Describe the bug

I fit two random forests (with just one tree each) with identical parameters, but for the second one I disable the row subsampling by setting `bootstrap=False` and instead pass the inbag data generated with _generate_sample_indices.

I believe that the only stochastic elements of a random forest are the row and column subsampling. I disable the former by setting `bootstrap=False` and the latter as well by choosing `max_features = p`.


### Steps/Code to Reproduce

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble._forest import _generate_sample_indices
from sklearn.datasets import load_boston
import matplotlib.pyplot as plt

boston = load_boston()
xtrain, ytrain = boston.data, boston.target
n_samples, p = xtrain.shape

rf = RandomForestRegressor(n_estimators=1, 
    min_samples_leaf =10,max_features = p)
rf.fit(xtrain, ytrain)

tree=rf.estimators_[0]
sampled_indices = _generate_sample_indices(tree.random_state, n_samples, n_samples)

rf0 = RandomForestRegressor(n_estimators=1, 
    min_samples_leaf =10,max_features = p, bootstrap=False)
rf0.fit(xtrain[sampled_indices,:], ytrain[sampled_indices])

p  =  rf.predict(xtrain)
p0 = rf0.predict(xtrain)

plt.scatter(p,p0)
```

### Expected Results

I would expect the two forests (consisting of a single tree) to be identical ?


### Actual Results

The predictions are quite different as you can see in the output

### Versions

System:
    python: 3.8.3 (default, Jul  2 2020, 11:26:31)  [Clang 10.0.0 ]
executable: /opt/anaconda3/bin/python
   machine: macOS-10.16-x86_64-i386-64bit

Python dependencies:
          pip: 20.1.1
   setuptools: 49.2.0.post20200714
      sklearn: 0.23.1
        numpy: 1.19.5
        scipy: 1.5.0
       Cython: 0.29.21
       pandas: 1.0.5
   matplotlib: 3.2.2
       joblib: 0.16.0
threadpoolctl: 2.1.0

Built with OpenMP: True
","['Bug', 'module:ensemble', 'module:tree']","2021-09-09T20:16:26Z","8","0","https://github.com/scikit-learn/scikit-learn/issues/21002","Logical Bug"
"149","scikit-learn/scikit-learn","HalvingGridSearchCV number of classes","#### Describe the bug

Errors:
You may have to run the code several times to see these errors. I think most of them is to do with the way the initial number of samples is selected; If the initial number of samples contains the same target then the fit function fails. ~I also seem to be running into problems with no samples being passed?~

ValueError: The number of classes has to be greater than one; got 1 class
~ValueError: Found array with 0 sample(s) (shape=(0, 22)) while a minimum of 1 is required.~
ValueError: zero-size array to reduction operation maximum which has no identity


oblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.
ERROR: The process ""8404"" not found.
ERROR: The process ""9492"" not found.
ERROR: The process ""28656"" not found.
ERROR: The process ""29236"" not found.
ERROR: The process ""29412"" not found.
ERROR: The process ""9004"" not found.
ERROR: The process ""28564"" not found.
ERROR: The process ""12072"" not found.
ERROR: The process with PID 3748 (child process of PID 11860) could not be terminated.
Reason: Access is denied.
ERROR: The process ""4372"" not found.
ERROR: The process ""2436"" not found.
ERROR: The process with PID 28212 (child process of PID 13280) could not be terminated.
Reason: Access is denied.
ERROR: The process ""25468"" not found.
ERROR: The process with PID 27876 (child process of PID 9884) could not be terminated.
Reason: Access is denied.
ERROR: The process with PID 27912 (child process of PID 19872) could not be terminated.
Reason: Access is denied.

#### Steps/Code to Reproduce

Example:
```python
import numpy as np
from sklearn.decomposition import KernelPCA
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV
from sklearn.pipeline import Pipeline

from sklearn.svm import SVC
from sklearn.model_selection import ShuffleSplit
from sklearn.metrics import make_scorer, f1_score

data_input = np.random.randn(4000,22)
data_output = np.round(np.random.random((4000,))).astype((int))

search_points = 10
gamma = np.logspace(-9, 1, search_points)
coef = np.linspace(-1, 1, search_points)
points = np.round(np.linspace(1, len(data_input[0, :]), search_points))
FE_algorithm = KernelPCA(kernel='rbf')
FE_grid = {'FE__n_components': points.astype(int), 'FE__gamma': gamma,'FE__coef0': coef,}

regularisation = np.logspace(0,1,search_points)
Class_algorithm = SVC(kernel='linear')
CA_grid = {'CA__C': regularisation,}

pipe = Pipeline(steps=[('FE', FE_algorithm), ('CA', Class_algorithm)])
param_grid = {**FE_grid, **CA_grid} # concat dicts

scorer = make_scorer(f1_score, average='weighted')
search = HalvingGridSearchCV(pipe, param_grid, cv=ShuffleSplit(test_size=0.20, n_splits=1, random_state=0),
                                      n_jobs=-1, verbose=3, scoring=scorer)

search.fit(data_input, data_output)
```

#### Versions

Windows-10-10.0.19041-SP0
Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)]
NumPy 1.19.5
SciPy 1.6.3
Scikit-Learn 0.24.1



","['Bug', 'module:model_selection']","2021-08-19T13:43:05Z","10","0","https://github.com/scikit-learn/scikit-learn/issues/20775","Logical Bug"
"150","scikit-learn/scikit-learn","RegressorChain support for Pipelines including ColumnTransformer","#### Describe the bug

I can't seem to get the `RegressorChain` working with pipelines that include a `ColumnTransformer`. I posted an issue on StackOverflow with more: https://stackoverflow.com/questions/68430993/sklearn-using-regressorchain-with-columntransformer-in-pipelines .

Somewhere in `__init__.py / _get_column_indices(X, key)` this call fails: `all_columns = X.columns`  saying `'numpy.ndarray' object has no attribute 'columns'`. Because this is a known issue with `ColumnTransformer`, I suspect the `RegressorChain` can't be used with it.

I'm not sure if this is a supported scenario, but the documentation for RegressorChain (https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.RegressorChain.html), for `set_params`, includes this:

""The method works on simple estimators **as well as on nested objects (such as Pipeline)**. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.""

So I was led to assume it would also work with Pipelines including the column transformer.

#### Steps/Code to Reproduce

Any example with a Pipeline containing a ColumnTransformer and a Regressor. The StackOverflow link I included above has my code.

#### Expected Results

Fitted pipeline.

#### Actual Results

```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
C:\ProgramData\Anaconda3\envs\py38aml\lib\site-packages\sklearn\utils\__init__.py in _get_column_indices(X, key)
    373         try:
--> 374             all_columns = X.columns
    375         except AttributeError:

AttributeError: 'numpy.ndarray' object has no attribute 'columns'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-181-24da1e03388c> in <module>
      3 
      4 chain_regressor = RegressorChain(base_estimator=chain_pipeline) #, order=[1,0,2])
----> 5 chain_regressor.fit(X, y)
      6 
      7 

C:\ProgramData\Anaconda3\envs\py38aml\lib\site-packages\sklearn\multioutput.py in fit(self, X, Y, **fit_params)
    840         self : object
    841         """"""
--> 842         super().fit(X, Y, **fit_params)
    843         return self
    844 

C:\ProgramData\Anaconda3\envs\py38aml\lib\site-packages\sklearn\multioutput.py in fit(self, X, Y, **fit_params)
    507         for chain_idx, estimator in enumerate(self.estimators_):
    508             y = Y[:, self.order_[chain_idx]]
--> 509             estimator.fit(X_aug[:, :(X.shape[1] + chain_idx)], y,
    510                           **fit_params)
    511             if self.cv is not None and chain_idx < len(self.estimators_) - 1:

C:\ProgramData\Anaconda3\envs\py38aml\lib\site-packages\sklearn\compose\_target.py in fit(self, X, y, **fit_params)
    205             self.regressor_ = clone(self.regressor)
    206 
--> 207         self.regressor_.fit(X, y_trans, **fit_params)
    208 
    209         return self

C:\ProgramData\Anaconda3\envs\py38aml\lib\site-packages\sklearn\pipeline.py in fit(self, X, y, **fit_params)
    339         """"""
    340         fit_params_steps = self._check_fit_params(**fit_params)
--> 341         Xt = self._fit(X, y, **fit_params_steps)
    342         with _print_elapsed_time('Pipeline',
    343                                  self._log_message(len(self.steps) - 1)):

C:\ProgramData\Anaconda3\envs\py38aml\lib\site-packages\sklearn\pipeline.py in _fit(self, X, y, **fit_params_steps)
    301                 cloned_transformer = clone(transformer)
    302             # Fit or load from cache the current transformer
--> 303             X, fitted_transformer = fit_transform_one_cached(
    304                 cloned_transformer, X, y, None,
    305                 message_clsname='Pipeline',

C:\ProgramData\Anaconda3\envs\py38aml\lib\site-packages\joblib\memory.py in __call__(self, *args, **kwargs)
    350 
    351     def __call__(self, *args, **kwargs):
--> 352         return self.func(*args, **kwargs)
    353 
    354     def call_and_shelve(self, *args, **kwargs):

C:\ProgramData\Anaconda3\envs\py38aml\lib\site-packages\sklearn\pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    752     with _print_elapsed_time(message_clsname, message):
    753         if hasattr(transformer, 'fit_transform'):
--> 754             res = transformer.fit_transform(X, y, **fit_params)
    755         else:
    756             res = transformer.fit(X, y, **fit_params).transform(X)

C:\ProgramData\Anaconda3\envs\py38aml\lib\site-packages\sklearn\compose\_column_transformer.py in fit_transform(self, X, y)
    503         self._validate_transformers()
    504         self._validate_column_callables(X)
--> 505         self._validate_remainder(X)
    506 
    507         result = self._fit_transform(X, y, _fit_transform_one)

C:\ProgramData\Anaconda3\envs\py38aml\lib\site-packages\sklearn\compose\_column_transformer.py in _validate_remainder(self, X)
    330         cols = []
    331         for columns in self._columns:
--> 332             cols.extend(_get_column_indices(X, columns))
    333 
    334         remaining_idx = sorted(set(range(self._n_features)) - set(cols))

C:\ProgramData\Anaconda3\envs\py38aml\lib\site-packages\sklearn\utils\__init__.py in _get_column_indices(X, key)
    374             all_columns = X.columns
    375         except AttributeError:
--> 376             raise ValueError(""Specifying the columns using strings is only ""
    377                              ""supported for pandas DataFrames"")
    378         if isinstance(key, str):

ValueError: Specifying the columns using strings is only supported for pandas DataFrames
```

#### Versions

System:
    python: 3.8.10 (default, May 19 2021, 13:12:57) [MSC v.1916 64 bit (AMD64)]
executable: C:\ProgramData\Anaconda3\envs\py38aml\python.exe
   machine: Windows-10-10.0.22000-SP0

Python dependencies:
          pip: 21.1.3
   setuptools: 52.0.0.post20210125
      sklearn: 0.24.2
        numpy: 1.20.2
        scipy: 1.6.2
       Cython: None
       pandas: 1.2.5
   matplotlib: 3.3.4
       joblib: 1.0.1
threadpoolctl: 2.2.0

Built with OpenMP: True","['Bug', 'help wanted', 'Hard', 'module:preprocessing']","2021-07-18T16:46:16Z","4","1","https://github.com/scikit-learn/scikit-learn/issues/20557","Logical Bug"
"151","scikit-learn/scikit-learn","AdaBoost's training error can increase with a larger number of trees","We encountered the following behavior of AdaBoost when working on the [scikit-learn mooc](https://mooc-forums.inria.fr/moocsl/t/quiz-m6-03-question-3/3128/15). Here is a script that reproduces the problem:

We compute the validation curves for 3 models with different numbers of leaf nodes in the weak learners and each time study the impact of the number of trees on the train and validation errors.


```python
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import validation_curve
from sklearn.datasets import fetch_california_housing
import matplotlib.pyplot as plt


data, target = fetch_california_housing(return_X_y=True)

for max_leaf_nodes in [50, 100, 500]:
    adaboost = AdaBoostRegressor(
        learning_rate=1.,
        base_estimator=DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes),
    )
    param_range = [1, 2, 5, 10, 20, 50, 100, 500]
    train_scores, test_scores = validation_curve(
        adaboost,
        data,
        target,
        param_name=""n_estimators"",
        param_range=param_range,
        scoring=""neg_mean_absolute_error"",
        n_jobs=4,
    )
    train_errors, test_errors = -train_scores, -test_scores
    plt.figure()
    plt.errorbar(param_range, train_errors.mean(axis=1),
                 yerr=train_errors.std(axis=1), label=""Training score"",
                 alpha=0.7)
    plt.errorbar(param_range, test_errors.mean(axis=1),
                 yerr=test_errors.std(axis=1), label=""Cross-validation score"",
                 alpha=0.7)
    plt.legend()
    plt.xscale(""log"")
    plt.xticks(param_range)
    plt.ylabel(""Mean absolute error in k$\n(smaller is better)"")
    plt.xlabel(""# estimators"")
    _ = plt.title(
        f""AdaBoost regressor with {max_leaf_nodes=}\n""
        f""best CV score: {test_errors.mean(axis=1).min():.4f}""
    )
```

Here are the results:

![adaboost_50_leaves](https://user-images.githubusercontent.com/89061/124276287-e0752f00-db43-11eb-9a45-ae83a6507218.png)
![adaboost_100_leaves](https://user-images.githubusercontent.com/89061/124276290-e10dc580-db43-11eb-8848-32b8212019e6.png)
![adaboost_500_leaves](https://user-images.githubusercontent.com/89061/124276292-e1a65c00-db43-11eb-8525-8f8da999477e.png)

For 500 leaves we get the expected behavior: both the training and validation errors decrease when increasing the number of trees. At some point we reach a plateau of diminishing returns (expected, at least for validation error).

However for lower number of leaves (e.g. the second plot with `max_leaf_nodes=50`), the training set is growing with more trees! This is really something that I would not have expected. I wonder if this is not revealing a bug in our implementation.

For reference, for lower values of `max_leaf_nodes` both the training and validation errors stay do not decrease (and even increase again) and they stay close to one another (severe under fitting) with large error bars.","['Bug', 'module:ensemble']","2021-07-02T12:49:24Z","13","0","https://github.com/scikit-learn/scikit-learn/issues/20443","Logical Bug"
"152","scikit-learn/scikit-learn","Numerical stability problems in GaussianProcessRegressor on 32 bit Python","See the failing tests observed on 32 bit Linux with Python 3.7 or 3.8 in #20334 for details.

Those tests are skipped for now to avoid blocking the nightly build and release processes.","['Bug', 'Build / CI']","2021-06-23T13:00:50Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/20337","Logical Bug"
"153","scikit-learn/scikit-learn","Test failure: test_loadings_converges in test_pls.py","Observed on travis last night, possibly caused by the upgrade of numpy / scipy:

```python
___________________________ test_loadings_converges ____________________________
[gw1] linux -- Python 3.7.10 /tmp/tmp.J9NDQpQnRJ/venv/bin/python

    def test_loadings_converges():
        """"""Test that CCA converges. Non-regression test for #19549.""""""
        X, y = make_regression(n_samples=200, n_features=20, n_targets=20, random_state=20)
        cca = CCA(n_components=10, max_iter=500)
        with pytest.warns(None) as record:
            cca.fit(X, y)
        # ConvergenceWarning is not raised
>       assert not record
E       assert not WarningsChecker(record=True)
/tmp/tmp.J9NDQpQnRJ/venv/lib/python3.7/site-packages/sklearn/cross_decomposition/tests/test_pls.py:583: AssertionError
```

I haven't investigated the cause yet. I can probably give it a look interactively in a linux arm64 docker container on my M1 laptop.","['Bug', 'Build / CI']","2021-06-23T10:20:45Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/20335","Logical Bug"
"154","scikit-learn/scikit-learn","ElasticNet (and Lasso) for multiple outputs is not described correctly","#### Describe the issue linked to the documentation

There are 2 ways to run ElasticNet with multiple outputs. The first is to run it for each output (method A) and the second is to use a different objective, using `||W||_21` as norm (method B). From what I see, ElasticNet is supposed to apply method A and MultiTaskElasticNet uses method B. enet_path and lasso_path apply method B.

However, ElasticNet and Lasso's path method is exactly enet_path (`path = staticmethod(enet_path)`). This causes confusion, as

1. fit and path don't apply the same algorithm
2. method B is documented in by enet_path and hence auto-included on the pages of ElasticNet and Lasso

#### Suggest a potential alternative/fix

Changing the API for `path` to match method A would cause compatibility issues. I propose to add a note somewhere to explain that `path` doesn't apply the same algorithm as `fit` in Lasso and ElasticNet, and add a link to MultiTaskLasso and MultiTaskElasticNet.
","['Bug', 'Documentation', 'module:linear_model']","2021-04-12T21:35:35Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/19868","UI/UX Bug"
"155","scikit-learn/scikit-learn","ClassifierChain should only accept multilabel-indicator","By trying to solve https://github.com/scikit-learn/scikit-learn/issues/19357 and writing some test, it seems that `ClassifierChain` is expected to be fitted with `multilabel-indicator` target (each column should only contain 0/1 classes).

However, there is no check and one can fit a `multiclass-multioutput` target. The classifier will later fail if calling `decision_function` that would return an array of shape `n_samples, 3` while an array of `n_samples,` is expected. I assume a similar behaviour for `predict_proba`.

I think that we should check the type of target to raise a proper error at `fit`.","['Bug', 'module:multioutput']","2021-04-09T17:34:43Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/19853","Logical Bug"
"156","scikit-learn/scikit-learn","Inconsistent behavior with bagging & base estimator class_weight","#### Describe the bug

Bagging Classifier transforms y to indices between 0..N but does not impact the possible `base_estimator.class_weight` attribute making the fit method fails when using class_weight: the class could not be found.

#### Steps/Code to Reproduce

This is working fine: 

```python
from sklearn.datasets import make_classification
from sklearn.linear_model import PassiveAggressiveClassifier

X, y = make_classification(n_samples=10000, n_features=5, n_redundant=0, n_clusters_per_class=1, weights=[0.5])
# let's say y is not 0 or 1 but something else like 1 and 2 for instance:
y += 1
# now i fit a classifier with class_weight, works fine
PassiveAggressiveClassifier(class_weight={1: 1, 2: 1}).fit(X, y)
```

Now i'm just wrapping it around BaggingClassifier:

```python
from sklearn.ensemble import BaggingClassifier

BaggingClassifier(PassiveAggressiveClassifier(class_weight={1: 1, 2: 1})).fit(X, y)
```

I'm getting: 

```pytb
ValueError: Class label 2 not present.
```

It seems it comes from here : https://github.com/scikit-learn/scikit-learn/blob/95119c13af77c76e150b753485c662b7c52a41a2/sklearn/ensemble/_bagging.py#L654 

Because bagging transform y but then class_weight attribute of the base estimator is not aligned with the new labels.

Is it expected or this is a bug ?

#### Versions

```
 System:
    python: 3.7.9 (default, Feb 13 2021, 00:48:23)  [GCC 10.2.1 20210110]
executable: /home/.../.pyenv/versions/3.7.9/bin/python3.7
   machine: Linux-5.10.0-3-amd64-x86_64-with-debian-bullseye-sid

Python dependencies:
          pip: 21.0.1
   setuptools: 53.0.0
      sklearn: 0.24.1
        numpy: 1.20.1
        scipy: 1.6.1
       Cython: None
       pandas: None
   matplotlib: None
       joblib: 1.0.1
threadpoolctl: 2.1.0

Built with OpenMP: True
```","['Bug', 'module:ensemble']","2021-03-11T19:53:38Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/19665","Logical Bug"
"157","scikit-learn/scikit-learn","sklearn.manifold.locally_linear_embedding() enforces use of euclidean distance when passing a NearestNeighbors instance with a precomputed distance matrix as parameter","<!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Describe the bug
<!--
A clear and concise description of what the bug is.
-->
When calling the `locally_linear_embedding()` method with a _NearestNeighbors_ instance as its `X` parameter, the euclidean distance is always used in the standard version of the algorithm (`method = 'standard'`) defeating the purpose of passing down a _NearestNeighbors_ instance with a precomputed distance matrix (`metric = 'precomputed'`).

#### Steps/Code to Reproduce
<!--
Please add a minimal example that we can reproduce the error by running the
code. Be as succinct as possible, do not depend on external data. In short, we
are going to copy-paste your code and we expect to get the same
result as you.

Example:
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
docs = [""Help I have a bug"" for i in range(1000)]
vectorizer = CountVectorizer(input=docs, analyzer='word')
lda_features = vectorizer.fit_transform(docs)
lda_model = LatentDirichletAllocation(
    n_topics=10,
    learning_method='online',
    evaluate_every=10,
    n_jobs=4,
)
model = lda_model.fit(lda_features)
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->

```python
from sklearn.neighbors import NearestNeighbors
from sklearn.manifold import locally_linear_embedding

D = distance_matrix(data) # Precomputation of the distance matrix of some data with a custom distance measure

nbrs = NearestNeighbors(n_neighbors = 5,
                        metric = 'precomputed')
nbrs.fit(D) # NearestNeighbors instance with a precomputed distance matrix

Z,_ =  locally_linear_embedding(nbrs,
                                n_neighbors = 5,
                                n_components = 2) # Passing down the NearestNeighbors instance with a precomputed distance matrix
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
Embeddings computed using the distance matrix passed down to `locally_linear_embedding()` through the _NearestNeighbors_ instance.

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->
Embeddings are instead computed using the distance matrix passed down to `locally_linear_embedding()` through the _NearestNeighbors_ instance treating it as a dataset with shape _(n_samples, n_features)_. This happens when calling `barycenter_kneighbors_graph()` on line 320 which in turn calls the `kneighbors()` with a new instance of _NearestNeighbors_. This new instance has attribute `self.effective_metric_='euclidian'`. The problem happens when `kneighbors()` does a internal check on `self.effective_metric_` and decides to use euclidian metric.
#### Versions
<!--
Please run the following snippet and paste the output below.
For scikit-learn >= 0.20:
import sklearn; sklearn.show_versions()
For scikit-learn < 0.20:
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import sklearn; print(""Scikit-Learn"", sklearn.__version__)
import imblearn; print(""Imbalanced-Learn"", imblearn.__version__)
-->

System:
    python: 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]
executable: C:\ProgramData\Anaconda3\python.exe
   machine: Windows-10-10.0.19041-SP0

Python dependencies:
          pip: 20.1.1
   setuptools: 49.2.0.post20200714
      sklearn: 0.23.1
        numpy: 1.18.5
        scipy: 1.5.0
       Cython: 0.29.21
       pandas: 1.0.5
   matplotlib: 3.2.2
       joblib: 0.16.0
threadpoolctl: 2.1.0

Built with OpenMP: True

<!-- Thanks for contributing! -->
","['Bug']","2021-02-19T18:26:28Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/19505","Logical Bug"
"158","scikit-learn/scikit-learn","SVC's predict_proba(...) predicts the exact same probability for wildly different inputs","I'm using `scikit-learn` version 0.24.1.

I suspect some form of memoization or extreme low sensitivity happening in the SVC `predict_proba(...)` implementation whereby once a SVC model is built from a Pipeline with inputs scaling preprocessing and calibration and when setting the `random_state` argument; I get the exact same predicted probability for wildly different inputs.

I also checked the `decision_function(..)` result and it returns the exact same value for two wildly different `x_test` inputs.

This issue only happens when setting the argument `random_state` e.g. to zero. It is hard to create a MRE here also because of NDA and confidentiality of the dataset and work but I propose to export the pipeline like this:

```
from joblib import dump, load
dump(model, '/some/where/model.joblib')
```
and provide the two wildly different inputs leading to the same exact `predict_proba(...)` and `decision_function(...)` results. Would that be a viable solution to reproduce and fix the possible bug?

A simplified relevant example of my pipeline is the following:
```
pipeline = Pipeline(steps=[('preprocess', MaxAbsScaler()),
                           ('svm', SVC(kernel='rbf', probability=True, 
                                       random_state=0, decision_function_shape='ovr', 
                                       break_ties=False, ...))]
params = [{...}]
model = GridSearchCV(pipeline, params, ...).fit(x_train, y_train).best_estimator_

# and now I get
prob1 = model.predict_proba(x_test1)
prob2 = model.predict_proba(x_test2_wildly_diff_from_test1)
assert np.abs(prob1 - prob2) < 1e-10
```

For example, using `random_seed=0`:
```
>>> import numpy as np
>>> from scipy.spatial import distance
# how far are the two x_test vectors from each other?
>>> distance.cosine(x_test1, x_test2)  # EDIT: very far apart angle-wise e.g. 90
1.0280449512858494
>>> distance.euclidean(x_test1, x_test2) # very far in euclidean distance
30675.221284568033
>>> model.predict_proba(x_test1)
array([[0.86879653, 0.13120347]])
>>> model.predict_proba(x_test2)
array([[0.86879653, 0.13120347]])
>>> model.decision_function(x_test1)
array([-0.03474242])
>>> model.decision_function(x_test2)
array([-0.03474242])
```

UPDATE: what bothers me is not that they are close, what spooks me is that the two completely different vectors end up getting the exact same distance (to the 1e-10 decimal place) from the decision boundary `decision_function(...)` and therefore the exact probability too `predict_proba(...)`. I'm still thinking how to further validate and scrutinize this case ...
","['Bug', 'module:svm']","2021-02-12T10:03:43Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/19447","Logical Bug"
"159","scikit-learn/scikit-learn","Interactive Imputer cannot accept PLSRegression() as an estimator due to ""shape mismatch""","<!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Describe the bug
When setting the estimator as PLSRegression(), a ValueError is triggered by module '_iteractive.py' in line 348, caused by ""shape mismatch""

#### Steps/Code to Reproduce

Example:
```python
import numpy as np

from sklearn.datasets import fetch_california_housing
from sklearn.cross_decomposition import PLSRegression
from sklearn.experimental import enable_iterative_imputer  # noqa
from sklearn.impute import IterativeImputer

rng = np.random.RandomState(42)

X_california, y_california = fetch_california_housing(return_X_y=True)
X_california = X_california[:400]
y_california = y_california[:400]

def add_missing_values(X_full, y_full):
    n_samples, n_features = X_full.shape

    # Add missing values in 75% of the lines
    missing_rate = 0.75
    n_missing_samples = int(n_samples * missing_rate)

    missing_samples = np.zeros(n_samples, dtype=bool)
    missing_samples[: n_missing_samples] = True

    rng.shuffle(missing_samples)
    missing_features = rng.randint(0, n_features, n_missing_samples)
    X_missing = X_full.copy()
    X_missing[missing_samples, missing_features] = np.nan
    y_missing = y_full.copy()

    return X_missing, y_missing

X_miss_california, y_miss_california = add_missing_values(
    X_california, y_california)

imputer = IterativeImputer(estimator=PLSRegression(n_components=2))

X_imputed = imputer.fit_transform(X_miss_california)
print(X_imputed)
```


#### Expected Results: after applying the workaround below:
```python
[[   8.3252       41.            6.98412698 ...    2.55555556
    37.88       -122.25930206]
 [   8.3014       21.            6.23813708 ...    2.10984183
    37.86       -122.22      ]
 [   7.2574       52.            8.28813559 ...    2.80225989
    37.85       -122.24      ]
 ...
 [   3.60438721   50.            5.33480176 ...    2.30396476
    37.88       -122.29      ]
 [   5.1675       52.            6.39869281 ...    2.44444444
    37.89       -122.29      ]
 [   5.1696       52.            6.11590296 ...    2.70619946
    37.8709526  -122.29      ]]
```

#### Actual Results
```python
File ""/home/hushsh/py3/lib/python3.6/site-packages/sklearn/impute/_iterative.py"", line 348, in _impute_one_feature
    X_filled[missing_row_mask, feat_idx] = imputed_values
ValueError: shape mismatch: value array of shape (27,1) could not be broadcast to indexing result of shape (27,) 
```

#### Versions
System:
    python: 3.6.9 (default, Oct  8 2020, 12:12:24)  [GCC 8.4.0]
executable: /home/hushsh/raid_data/py3/bin/python
   machine: Linux-5.4.0-60-generic-x86_64-with-LinuxMint-19.3-tricia

Python dependencies:
          pip: 21.0.1
   setuptools: 47.3.1
      sklearn: 0.24.1
        numpy: 1.18.1
        scipy: 1.4.1
       Cython: 0.29.15
       pandas: 1.1.3
   matplotlib: 3.1.2
       joblib: 0.14.1
threadpoolctl: 2.1.0

Built with OpenMP: True

#### My Workaround that fixed the bug: Insert the following three lines before line 348
```python
shape_imputed_values = imputed_values.shape
if len(shape_imputed_values)>1:
    # convert 2D array to 1D array fixes the bug:
    imputed_values = imputed_values.reshape(shape_imputed_values[0])
```

<!-- Thanks for contributing! -->
","['Bug', 'Hard', 'module:cross_decomposition']","2021-02-05T06:33:02Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/19352","Logical Bug"
"160","scikit-learn/scikit-learn","ExpSineSquared GP Kernel is not PD","Here's a bit of test code:

```python
import numpy as np
import scipy
from sklearn.gaussian_process.kernels import ExpSineSquared

L = 1.0

# create some train/test data on a grid
train_len = 4
r = np.linspace(0, L, train_len)
train_x, train_y = np.meshgrid(r, r)
train_in = np.stack((train_x.flatten(), train_y.flatten()), axis=-1)

# get the kernel
kernel = ExpSineSquared()

K = kernel(train_in) + 1e-4 * np.eye(train_len**2)

print(np.sort(np.linalg.eigvals(K)))
```

This rather popular kernel for modeling periodic functions doesn't seem to be a truly positive definite kernel, at least in the sklearn implementation, as some of the eigenvalues printed are negative. Is this an issue with the sklearn implementation, or is this a weird issue with the kernel itself?

I think folks found something similar in #9824 ","['Bug', 'module:gaussian_process']","2021-02-03T23:40:11Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/19343","Dependency Issue"
"161","scikit-learn/scikit-learn","sklearn Birch memory usage is over 3x higher than original BIRCH (+ Python overheads)","The original BIRCH algorithm defined clustering features as CF=(N,LS,SS) where N is an integer, LS is a vector (""linear sum"") and SS is a scalar value (sum of squared values). Assuming double precision, a clustering feature for d dimensional data should hence use `4+8*d+8` bytes.
Memory efficiency is the key motivation of BIRCH.

The sklearn implementation will require WAY more memory, and its not just to boxing overhead:
https://github.com/scikit-learn/scikit-learn/blob/8c6a045e46abe94e43a971d4f8042728addfd6a7/sklearn/cluster/_birch.py#L286-L290
https://github.com/scikit-learn/scikit-learn/blob/8c6a045e46abe94e43a971d4f8042728addfd6a7/sklearn/cluster/_birch.py#L292-L297
as you can see, it stores (N,LS,LS/N,SS,||LS/N||) instead, about `4+8*d*2+8*2` which is about twice the original memory requirements not even taking boxing overhead into account.

The CFTree in BIRCH is meant to be low level - largely a list of CFs and child pointers for inner nodes. Here, the current sklearn implementation uses substantially more memory, too:
https://github.com/scikit-learn/scikit-learn/blob/8c6a045e46abe94e43a971d4f8042728addfd6a7/sklearn/cluster/_birch.py#L139-L152
We first have several constants replicated: threshold, branching factor, is_leaf and n_features are redundantly stored in every node of the tree.
`subclusters_` is okay, that is what you would expect: a list of `_CFSubcluster`; except that this one is not preallocated in size.
`init_centroids_` is a *copy* of the centroids of the CFs, `init_sq_norm_` of their norms. Then there is a double-chaining of leaves in every node (leaf or not) via `prev_leaf_` and `next_leaf_`, but they are only navigated in forward order ever, and _get_leaves will likely be called only once and it would be possible to collect them by tree traversal more efficiently rather than maintaining this list throughout the tree construction. There even is an additional `dummy_leaf_` to access the double-linked list because of that, which is completely initialized with an empty matrix.
The additional copies of LS/N and ||LS/N|| increase the Birch memory consumption to be more than three times the original memory; not taking any additional Python overheads into account.","['Bug', 'module:cluster']","2021-01-23T15:04:54Z","0","1","https://github.com/scikit-learn/scikit-learn/issues/19258","Performance Issue"
"162","scikit-learn/scikit-learn","train_test_split stratify with a pandas Int32 dtype causes error","<!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Describe the bug
<!--
A clear and concise description of what the bug is.
-->

It appears since 0.23.0 using multiple columns in train_test_split's stratify option results in an error if one column is the pandas nullable int `Int32Dtype()` type. Error does not occur in 0.22.2.post1

#### Steps/Code to Reproduce
<!--
Please add a minimal example that we can reproduce the error by running the
code. Be as succinct as possible, do not depend on external data. In short, we
are going to copy-paste your code and we expect to get the same
result as you.

Example:

```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
docs = [""Help I have a bug"" for i in range(1000)]
vectorizer = CountVectorizer(input=docs, analyzer='word')
lda_features = vectorizer.fit_transform(docs)
lda_model = LatentDirichletAllocation(
    n_topics=10,
    learning_method='online',
    evaluate_every=10,
    n_jobs=4,
)
model = lda_model.fit(lda_features)
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->

The following code will work in 0.22.2.post1 but will give an error in 0.23.0 or later (including 0.24.1). If only column `a` or `c` is chosen for `stratify` it will work. It will also work if column `c` is a numpy `int32` type rather than the nullable pandas `Int32`.

```
from sklearn.model_selection import train_test_split
import pandas as pd

df = pd.DataFrame({'a':['a','b','b']*10,'b':list(range(30)),'c':[1,2,2]*10})
df['c']=df['c'].astype('Int32')

x_train, x_test, y_train, y_test=train_test_split(df.drop(columns='a'),df['a'],test_size=.33, stratify=df[['a','c']])

```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
No error is thrown and results are:
```
>>> x_train
   b  c
4  4  2
1  1  2
5  5  2
3  3  1
>>> x_test
   b  c
0  0  1
2  2  2
>>> y_train
4    b
1    b
5    b
3    a
Name: y_train, dtype: object
>>> y_test
0    a
2    b
Name: y_test, dtype: object
```
#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py"", line 2197, in train_test_split
    train, test = next(cv.split(X=arrays[0], y=stratify))
  File ""/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py"", line 1793, in split
    y = check_array(y, ensure_2d=False, dtype=None)
  File ""/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py"", line 63, in inner_f
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py"", line 560, in check_array
    array = array.astype(dtype)
  File ""/usr/local/lib/python3.7/site-packages/pandas/core/generic.py"", line 5548, in astype
    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors,)
  File ""/usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py"", line 604, in astype
    return self.apply(""astype"", dtype=dtype, copy=copy, errors=errors)
  File ""/usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py"", line 409, in apply
    applied = getattr(b, f)(**kwargs)
  File ""/usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py"", line 595, in astype
    values = astype_nansafe(vals1d, dtype, copy=True)
  File ""/usr/local/lib/python3.7/site-packages/pandas/core/dtypes/cast.py"", line 997, in astype_nansafe
    return arr.astype(dtype, copy=True)
ValueError: could not convert string to float: 'a'
```

#### Versions
<!--
Please run the following snippet and paste the output below.
For scikit-learn >= 0.20:
import sklearn; sklearn.show_versions()
For scikit-learn < 0.20:
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import sklearn; print(""Scikit-Learn"", sklearn.__version__)
import imblearn; print(""Imbalanced-Learn"", imblearn.__version__)
-->
System:
    python: 3.7.8 (default, Jul  4 2020, 08:48:22)  [Clang 11.0.0 (clang-1100.0.33.17)]
executable: /usr/local/opt/python/bin/python3.7
   machine: Darwin-18.7.0-x86_64-i386-64bit

Python dependencies:
          pip: 20.0.2
   setuptools: 46.0.0
      sklearn: 0.24.1
        numpy: 1.19.4
        scipy: 1.4.1
       Cython: None
       pandas: 1.1.5
   matplotlib: 3.2.1
       joblib: 0.14.1
threadpoolctl: 2.1.0

Built with OpenMP: True

<!-- Thanks for contributing! -->
","['Bug', 'Regression', 'module:model_selection']","2021-01-22T22:40:46Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/19250","Logical Bug"
"163","scikit-learn/scikit-learn","check_decision_proba_consistency fails with LinearDiscriminantAnalysis","The following common check recently started to fail on a Windows CI job:

```python-traceback
name = 'LinearDiscriminantAnalysis'
estimator_orig = LinearDiscriminantAnalysis()

    @ignore_warnings(category=FutureWarning)
    def check_decision_proba_consistency(name, estimator_orig):
        # Check whether an estimator having both decision_function and
        # predict_proba methods has outputs with perfect rank correlation.
    
        centers = [(2, 2), (4, 4)]
        X, y = make_blobs(n_samples=100, random_state=0, n_features=4,
                          centers=centers, cluster_std=1.0, shuffle=True)
        X_test = np.random.randn(20, 2) + 4
        estimator = clone(estimator_orig)
    
        if (hasattr(estimator, ""decision_function"") and
                hasattr(estimator, ""predict_proba"")):
    
            estimator.fit(X, y)
            # Since the link function from decision_function() to predict_proba()
            # is sometimes not precise enough (typically expit), we round to the
            # 10th decimal to avoid numerical issues.
            a = estimator.predict_proba(X_test)[:, 1].round(decimals=10)
            b = estimator.decision_function(X_test).round(decimals=10)
>           assert_array_equal(rankdata(a), rankdata(b))
E           AssertionError: 
E           Arrays are not equal
E           
E           Mismatched elements: 2 / 20 (10%)
E           Max absolute difference: 0.5
E           Max relative difference: 0.02631579
E            x: array([ 7. ,  8. , 11. ,  9. , 17. , 10. ,  5. , 14. ,  6. ,  1. , 19.5,
E                   4. ,  2. , 16. , 12. , 13. ,  3. , 15. , 19.5, 18. ])
E            y: array([ 7.,  8., 11.,  9., 17., 10.,  5., 14.,  6.,  1., 20.,  4.,  2.,
E                  16., 12., 13.,  3., 15., 19., 18.])
```

This happened on this PR which should not have any impact on the behavior `LinearDiscriminantAnalysis.predict_proba`: #17743.

https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=25474&view=logs&j=d32b16b6-cb9d-571b-e765-de83708fb8dd&t=b93f76c1-c2c9-579e-c2ec-c4f438af1261


I suspect the test to be too brittle. Maybe using a test set more related to the original distribution (blobs) would avoid ties or caused by arbitrary rounding?","['Bug', 'module:test-suite']","2021-01-21T08:39:53Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/19224","Logical Bug"
"164","scikit-learn/scikit-learn","[MRG] Correct handling of missing_values and NaN in SimpleImputer for object arrays (closes #19071)","#### Reference Issues/PRs

Fixes #19071.
In `SimpleImputer` strategies mean/median you can use input arrays with dtype is `object`. Imagine numeric data in the input array, except for missing values. In object arrays, you have a lot of flexibility on how to encode missing values. You can use `np.nan`, `None`, a string constant or anything else. There are two scenarios which are described in the issue #19071:

1.  There is more than one type of missing value in the input object array, one of them is specified as the `missing_values` parameter of the `SimpleImputer`. In this case the mean/median imputation should fail, because there are still non-numeric values in the input array other than `missing_values` which is masked. See #19071 for an example of not failing.
2. There is just one type of missing value in the input object array and that is specified as the `missing_values` parameter of the `SimpleImputer`. In this case the mean/median imputation must not fail and should calculate mean/median from the remaining values of the input array. See #19071 for an example of this failing with `missing_values=None`.

#### What does this implement/fix? Explain your changes.

In the `_validate_input` method, I keep the dtype as object.
In the `_dense_fit` method I try to convert the object array into float64 to prepare for mean/median calculation. If the not-masked values still contain some non-numeric values (scenario 1), ValueError is raised with the following informative error message:
```
Non-numeric values other than missing_values={missing_values}, showing {num_show}/{num_notmasked_nan}: {examples}
```
where the number of examples is limited to three.

#### Any other comments?

A comprehensive set of unit tests is added, which covers both scenarios.","['Bug', 'Stalled', 'help wanted', 'module:impute']","2020-12-29T19:58:11Z","1","1","https://github.com/scikit-learn/scikit-learn/pull/19079","Logical Bug"
"165","scikit-learn/scikit-learn","SimpleImputer, missing_values and None","There are issues with `SimpleImputer` and None:

1. None values are treated as NaNs even when the dtype is `object`. Unlike the `float` dtype, `object` can support both nans **and** None so there's no reason to treat them the same.

```py
import numpy as np
from sklearn.impute import SimpleImputer

a = np.arange(5).reshape(-1, 1).astype(object)
a[3] = None

# This will not fail and impute None as if it were nan
SimpleImputer(missing_values=np.nan).fit_transform(a)
```

2. Unlike what the doc says, `missing_values=None` is not supported. In fact, `missing_values` is expected to be of numerical dtype, not `NoneType`. The error message isn't super descriptive:

```py
import numpy as np
from sklearn.impute import SimpleImputer

a = np.arange(5).reshape(-1, 1).astype(object)
a[3] = None

SimpleImputer(missing_values=None).fit_transform(a)
```
fails with:

`ValueError: Input contains NaN, infinity or a value too large for dtype('float64').`

(I haven't checked other imputers which may or may not have similar issues)","['Bug', 'help wanted', 'module:impute']","2020-12-26T14:52:03Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/19071","Logical Bug"
"166","scikit-learn/scikit-learn","BallTree query match time is O(n) not O(log(n))","I've run performance analysis on matching NN with BallTree (same with KDTree), and the matching time is linear to number of elements, and should be O(log(n)). 

Here are the result of benchmark:
num_elements, match_time
10000 0.09097146987915039
20000 0.18194293975830078
40000 0.3668830394744873
80000 0.7527577877044678

Here is my code:
```

from sklearn.neighbors import BallTree
import numpy as np 
import time 


def tree_perf(tree_size):
    X = np.random.rand(tree_size, 512)
    Y1 = np.random.rand(1, 512)
    Y2 = np.random.rand(10, 512)

    ts = time.time()
    kdt = BallTree(X, leaf_size=30, metric='euclidean')
    load_tree = time.time() - ts
    num_nn = 1
    ts = time.time()
    vs = kdt.query(Y1, k=num_nn, return_distance=True)
    match1 = time.time() - ts
    ts = time.time()
    vs = kdt.query(Y2, k=num_nn, return_distance=True)
    match10 = time.time() - ts
    print(tree_size, load_tree, match1, match10)

print(""num_elements"", ""load_tree"", ""match_1"", ""match_10"")

for i in range(100):
    tree_size = 10000 + i * 10000
    tree_perf(tree_size)
```","['Bug', 'Performance', 'help wanted', 'module:neighbors']","2020-12-23T22:29:08Z","18","0","https://github.com/scikit-learn/scikit-learn/issues/19066","Performance Issue"
"167","scikit-learn/scikit-learn","t-SNE runs out of memory just before returning results and after finishing computation","#### Describe the bug

Based on the verbose output of TSNE it seems that the whole computation actually finishes fine and just when it is about to return the results it writes a large amount of data to RAM and then gets killed by the system.

#### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.manifold import TSNE

large_N = 100_000  # for my system this is large enough, of course smaller N like 10_000 work
pca_result = np.random.normal(size=(large_N, 50))
tsne = TSNE(n_components=2, verbose=10, perplexity=40, n_iter=300)
tsne_pca_results = tsne.fit_transform(pca_result)
```

#### Expected Results
Running this should behave the same way as for smaller N like 10.000, or, run out of memory during execution, because the TSNE needs more memory to run for given N than provided by the system.

#### Actual Results
Following the verbose output and the runtime, even for the given large_N of 100.000 TSNE (which I believe uses BarnesHut implementation as default) is actually able to index the data and compute conditional probabilities. The verbose output also shows the running of 300 iterations with reduced error each time. The last line of output shows the achieved the KL divergence. At this point I believe TSNE has actually run successfully and should just return embedding matrix of shape (N,2) which is much smaller than (N,50) and should therefore have no problem to fit into memory. But when opening `htop` in a parallel terminal one can actually observe how the program writes data into the RAM within seconds, fills it completly up and then gets killed by the system. Below the output as observed on my terminal:

```
t-SNE] Computing 121 nearest neighbors...                                                                                                                                                  | 0/202 [00:00<?, ?it/s]
[t-SNE] Indexed 535327 samples in 5.554s...                                                                                                                                                                        
[t-SNE] Computed neighbors for 535327 samples in 57.435s...                                                                                                                                                        
[t-SNE] Computed conditional probabilities for sample 1000 / 535327                                                                                                                                                
[t-SNE] Computed conditional probabilities for sample 2000 / 535327                                                                                                                                                
[t-SNE] Computed conditional probabilities for sample 3000 / 535327                                                                                                                                                
[t-SNE] Computed conditional probabilities for sample 4000 / 535327                                                                                                                                                
[t-SNE] Computed conditional probabilities for sample 5000 / 535327                                                                                                                                                
[t-SNE] Computed conditional probabilities for sample 6000 / 535327                                                                                                                                                
[t-SNE] Computed conditional probabilities for sample 7000 / 535327                                                                                                                                                
[t-SNE] Computed conditional probabilities for sample 8000 / 535327
...
[t-SNE] Computed conditional probabilities for sample 534000 / 535327
[t-SNE] Computed conditional probabilities for sample 535000 / 535327
[t-SNE] Computed conditional probabilities for sample 535327 / 535327
[t-SNE] Mean sigma: 0.000000
[t-SNE] Computed conditional probabilities in 70.161s
[t-SNE] Iteration 50: error = 123.6066971, gradient norm = 0.0000000 (50 iterations in 89.115s)
[t-SNE] Iteration 50: gradient norm 0.000000. Finished.
[t-SNE] KL divergence after 50 iterations with early exaggeration: 123.606697
[t-SNE] Iteration 100: error = 7.7342658, gradient norm = 0.0000001 (50 iterations in 101.858s)
[t-SNE] Iteration 150: error = 7.7342658, gradient norm = 0.0000067 (50 iterations in 110.148s)
[t-SNE] Iteration 200: error = 7.7170248, gradient norm = 0.0012155 (50 iterations in 119.877s)
[t-SNE] Iteration 250: error = 6.7275314, gradient norm = 0.0006462 (50 iterations in 91.746s)
[t-SNE] Iteration 300: error = 4.7438197, gradient norm = 0.0004072 (50 iterations in 57.282s)
[t-SNE] KL divergence after 300 iterations: 4.743820
Killed
```

#### Versions

Output of `import sklearn; sklearn.show_versions()`:
```
System:
    python: 3.6.9 (default, Apr 18 2020, 01:56:04)  [GCC 8.4.0]
executable: /lhome/davidj2/code/sync/pointset_attention_net/.venv/bin/python3.6
   machine: Linux-4.15.0-108-generic-x86_64-with-Ubuntu-18.04-bionic

Python dependencies:
          pip: 20.2.4
   setuptools: 50.3.2
      sklearn: 0.23.2
        numpy: 1.19.4
        scipy: 1.5.3
       Cython: 0.29.21
       pandas: 1.1.4
   matplotlib: 3.3.3
       joblib: 0.17.0
threadpoolctl: 2.1.0

Built with OpenMP: True
```","['Bug', 'module:manifold']","2020-12-04T10:50:12Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/18966","Performance Issue"
"168","scikit-learn/scikit-learn",".fit.transform != .fit_transform inconsistency in PCA results","PCA's fit_transform returns different results than the application of fit and transform methods individually. A piece of code that shows the inconsistency is given below.

```python
import numpy as np
from sklearn.decomposition import PCA

nn = np.array([[0,1,2],[3,4,5],[6,7,8]])
pca = PCA(n_components=2, random_state=42)
print(pca.fit_transform(nn))

nn = np.array([[0,1,2],[3,4,5],[6,7,8]])
pca = PCA(n_components=2, random_state=42)
pca.fit(nn)
print(pca.transform(nn))
```

Please run the code with sklearn 0.23.2

","['Bug', 'module:decomposition']","2020-11-29T21:50:16Z","12","2","https://github.com/scikit-learn/scikit-learn/issues/18941","Logical Bug"
"169","scikit-learn/scikit-learn","MDS implementation has error in smacof algorithm for non-metric case","Dear Scikit team,
I really enjoy using the scikit learn library and I am very greatful for your very impactful work. I hope I can contribute with this bug report (if it is true) to this library.
Keep on your good work!

I checked existing issues that concern MDS it is not the same as #16846 but to solve it I would also address #11381

#### Describe the bug
Everything concerns the MDS implementation in `scikit-learn/sklearn/manifold/_mds.py`. I have copied the relevant snipped of code below and added the line numbers I am referring to. All equations I am referring to are from [1]

__Short version:__
- The initialization of the disparities in [line 103](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/manifold/_mds.py#L103) leads to an error when calculating the stress and the update of the configuration.
- The standardization in [line 106](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/manifold/_mds.py#L106) is wrong.
- The library can not reproduce results from the [r library mass](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/isoMDS.html)

__Long version:__
I found the following error in the implementation:
- In `scikit-learn.sklearn.manifold._mds._smacof_single` in [line 103](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/manifold/_mds.py#L103) the disparities get initialized with the distances. In [line 104](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/manifold/_mds.py#L104) only the values of the upper triangle matrix get overwritten by the calculated disparities (`disparities_flat`). This results in a matrix that has disparities (similar to rank distances) in the upper triangle and euclidean distances in the lower triangle. Therefore the normalization factor in [line 106](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/manifold/_mds.py#L106) is calculated based on a matrix that is half distances and half disparities
- Line 106 standardized the disparities to 
![grafik](https://user-images.githubusercontent.com/31069329/100464473-f6160800-30cd-11eb-9118-94e754c2cc24.png)
which corresponds to step 3 and 9 of the `Smacof algorithm with Admissibly Transformed Proximities`[2]. However, from equation 9.1 it seems like this corresponds only to the upper triangle matrix (which explains the `n(n-1)/2`) in the implementation of line [106](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/manifold/_mds.py#L106), all elements from the matrix are used which results in a wrong standardization factor.
- Also [line 106](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/manifold/_mds.py#L106) the term ```(disparities ** 2).sum()``` should not be in the square-root. This results from a transformation when we are looking for a normalization factor `a` that is applied to the disparities such that:  
![grafik](https://user-images.githubusercontent.com/31069329/100475051-64fd5c00-30e2-11eb-87b5-45bdeb3e723f.png)
We then should calculate `a` as 
![grafik](https://user-images.githubusercontent.com/31069329/100475075-7b0b1c80-30e2-11eb-9e4a-b9e8b97b3f50.png)
(equations 9.1 + 9.2) from [1]

- When the stress is computed in [line 110](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/manifold/_mds.py#L110), a difference between the dissimilarities and the disparities is computed. As stated above, the disparities had the dissimilarities in the lower triangle matrix (which I think is confusing) which would normally mean that the all differences in the lower triangle matrix are zero. However, as the disparities got scaled in [line 106](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/manifold/_mds.py#L106), the differences in the lower triangle matrix are no longer zero but some (more or less) random values. 


There are also some smaller issues that I noticed:
- After the last update of the configuration, the stress is not updated. This means the old stress is returned (I think this is the same as #16846)
- I think it would be good to return the normalized stress 

__References__
[1]:  Modern Multidimensional Scaling - Theory and Applications Borg, I.; Groenen P. Springer Series in Statistics (1997), Version from 2005
[2]: Modern Multidimensional Scaling - Theory and Applications Borg, I.; Groenen P. Springer Series in Statistics (1997), Version from 2005, Page 204 in Chapter _9. Metric and non-metric MDS_

Code from [scikit MDS implementation](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/manifold/_mds.py#L89) (for reference)
```python
ir = IsotonicRegression()
    for it in range(max_iter):
        # Compute distance and monotonic regression
        dis = euclidean_distances(X)

        if metric:
            disparities = dissimilarities
        else:
            dis_flat = dis.ravel()
            # dissimilarities with 0 are considered as missing values
            dis_flat_w = dis_flat[sim_flat != 0]

            # Compute the disparities using a monotonic regression
            disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)
[103]    disparities = dis_flat.copy()
[104]    disparities[sim_flat != 0] = disparities_flat
[105]    disparities = disparities.reshape((n_samples, n_samples))
[106]    disparities *= np.sqrt((n_samples * (n_samples - 1) / 2) /
                                   (disparities ** 2).sum())

        # Compute stress
[110]    stress = ((dis.ravel() - disparities.ravel()) ** 2).sum() / 2

```

#### Steps/Code to Reproduce
The code and data to calculate the results is in this gist: (https://gist.github.com/henrymartin1/05a2aa5210e52216abd7ce61e6918e3a). The relevant file is`scikit_mds_bugreport.py`

I honestly had a hard time to come up with code that proofs that there is a problem. This is what I did: 
- I created an array and its distance matrix in python and stored it as .csv
- Using this data, I calculated the optimal configuration and the stress using the non-metric MDS implementation of the [r-library mass](https://www.rdocumentation.org/packages/MASS/versions/7.3-53/topics/isoMDS)
- I calculate the optimal configuration and the stress using scikit-learns mds
- I calculate the non-metric stress using my own implementation of non-metric stress
- I plot the optimal scikit configuration and the optimal r configuration

#### Expected Results
- The two optimal configurations would be the same or very similar
- The stress reported by r and the stress reported by scikit are the same
- The configurations found by scikit and by r have the same stress when I calculate it using my own implementation of the stress function. (+ they match with what is reported by scikit and the r implementation)

#### Actual Results
- The optimal scikit configuration and the optimal r configuration are significantly different (see plots below)
- My calculated stress for the r configuration and the scikit configuration are different
- The scikit stress is not normalized (not necessarily a problem)
##### Result mds scikit 
![grafik](https://user-images.githubusercontent.com/31069329/100474874-f8825d00-30e1-11eb-920d-c568ffe6e93c.png)
##### Result mds r
![grafik](https://user-images.githubusercontent.com/31069329/100474890-020bc500-30e2-11eb-8914-3ae4e61a486e.png)


#### Versions
System:
    python: 3.8.6 | packaged by conda-forge | (default, Oct  7 2020, 18:22:52) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\henry\.conda\envs\evhomepv\python.exe
   machine: Windows-10-10.0.19041-SP0

Python dependencies:
          pip: 20.2.4
   setuptools: 49.6.0.post20201009
      sklearn: 0.23.2
        numpy: 1.19.4
        scipy: 1.5.3
       Cython: None
       pandas: 1.1.4
   matplotlib: 3.3.2
       joblib: 0.17.0
threadpoolctl: 2.1.0

Built with OpenMP: True

<!-- Thanks for contributing! -->
","['Bug', 'module:manifold']","2020-11-27T18:11:35Z","7","3","https://github.com/scikit-learn/scikit-learn/issues/18933","Logical Bug"
"170","scikit-learn/scikit-learn","Duplicate check_finite when calling scipy.linalg functions","Most functions in `scipy.linalg` functions (e.g. `svd`, `qr`, `eig`, `eigh`, `pinv`, `pinv2` ...) have a default kwarg `check_finite=True` that we  typically leave to the default value in scikit-learn.

As we already validate the input data for most estimators in scikit-learn, this check is redundant and can cause significant overhead, especially at predict / transform time. We should probably always call those method with an explicit `check_finite=False` in scikit-learn.

This issue shall probably be addressed in many PRs, probably one per module that imports  `scipy.linalg`.

We should still make sure that the estimators raise a `ValueError` with the expected error message when fed with numpy arrays with infinite some values (`-np.inf`, `np.inf` or `np.nan`). This can be done manually by calling `sklearn.utils.estimator_checks.check_estimators_nan_inf` on the estimator, which should be automatically be called by `sklearn.tests.test_common` but we need to check that it's actually the case when reviewing such PRs.","['Bug', 'Moderate', 'Performance']","2020-11-13T17:35:09Z","34","2","https://github.com/scikit-learn/scikit-learn/issues/18837","Logical Bug"
"171","scikit-learn/scikit-learn","Inconsistency between Gaussian Process Regressor vs Classifier causing LinAlgError","<!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Describe the bug
As my current dataset was encountering with LinAlgErrors ('n-th leading minor of the array is not positive definite') while being fitted with GaussianProcessClassifier, I delved into the code a little bit and saw that the classifier, contrary to its regressor counterpart, does not contain an alpha parameter to ensure numerical stability. What is missing is basically one line, namely [this one](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/gaussian_process/_gpr.py#L263) possibly just under [this one](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/gaussian_process/_gpc.py#L414). 

For a better understanding of the requirement here, see [here](https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) and [here](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/gaussian_process/_gpr.py#L268).

#### Steps/Code to Reproduce
This is unfortunately not so easy, but one can try to reproduce this by using a dataset with feature count ~ sample count.

#### Versions
scikit-learn: 0.23.2

","['Bug', 'module:gaussian_process']","2020-10-25T17:28:16Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/18683","Logical Bug"
"172","scikit-learn/scikit-learn","LinearDiscriminantAnalysis does not handle degenerate problems well","As investigated when trying to solve unstable common tests in #18667, it seems that `LinearDiscriminantAnalysis` is bad a handling classification problems that are too trivial such as illustrated in the following (using MKL):

```python
>>> from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
>>> LinearDiscriminantAnalysis(solver=""svd"").fit([[0], [1], [1]], [0, 1, 1])
Traceback (most recent call last):
  File ""<ipython-input-6-034ea174ac0e>"", line 1, in <module>
    LinearDiscriminantAnalysis(solver=""svd"").fit([[0], [1], [1]], [0, 1, 1])
  File ""/home/ogrisel/code/scikit-learn/sklearn/discriminant_analysis.py"", line 553, in fit
    self._solve_svd(X, y)
  File ""/home/ogrisel/code/scikit-learn/sklearn/discriminant_analysis.py"", line 478, in _solve_svd
    _, S, Vt = linalg.svd(X, full_matrices=0)
  File ""/home/ogrisel/miniforge3/envs/dev/lib/python3.8/site-packages/scipy/linalg/decomp_svd.py"", line 121, in svd
    lwork = _compute_lwork(gesXd_lwork, a1.shape[0], a1.shape[1],
  File ""/home/ogrisel/miniforge3/envs/dev/lib/python3.8/site-packages/scipy/linalg/lapack.py"", line 950, in _compute_lwork
    ret = routine(*args, **kwargs)
ValueError: On entry to DGESDD parameter number 10 had an illegal value

>>> LinearDiscriminantAnalysis(solver=""eigen"").fit([[0], [1], [1]], [0, 1, 1])
/home/ogrisel/code/scikit-learn/sklearn/covariance/_empirical_covariance.py:88: UserWarning: Only one sample available. You may want to reshape your data array
  warnings.warn(""Only one sample available. ""
Traceback (most recent call last):
  File ""<ipython-input-7-57ea859bdde2>"", line 1, in <module>
    LinearDiscriminantAnalysis(solver=""eigen"").fit([[0], [1], [1]], [0, 1, 1])
  File ""/home/ogrisel/code/scikit-learn/sklearn/discriminant_analysis.py"", line 558, in fit
    self._solve_eigen(X, y,
  File ""/home/ogrisel/code/scikit-learn/sklearn/discriminant_analysis.py"", line 419, in _solve_eigen
    evals, evecs = linalg.eigh(Sb, Sw)
  File ""/home/ogrisel/miniforge3/envs/dev/lib/python3.8/site-packages/scipy/linalg/decomp.py"", line 578, in eigh
    raise LinAlgError('The leading minor of order {} of B is not '
LinAlgError: The leading minor of order 1 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.

>>> lda = LinearDiscriminantAnalysis(solver=""lsqr"").fit([[0], [1], [1]], [0, 1, 1])
/home/ogrisel/code/scikit-learn/sklearn/covariance/_empirical_covariance.py:88: UserWarning: Only one sample available. You may want to reshape your data array
  warnings.warn(""Only one sample available. ""
LinearDiscriminantAnalysis(solver='lsqr')
```

The least squares solver does not fail (but still raises a warning) and the resulting model is unable to classifiy the training set correctly:

```python
>>> lda.predict([[0], [1]])
array([1, 1])
```

The above was done with scipy 1.5.2 with OpenBLAS from conda-forge under Linux.

I am not sure what we could do about it. Maybe we could at least try to raise the same exception for all the solvers when the problem is degenerate. The original exception of the eigen cause could still be raised as the cause of the user friendly exception (`raise e1 from e2` in Python 3). For the SVD case this happens when the estimated rank is 0.","['Bug']","2020-10-22T09:21:05Z","3","1","https://github.com/scikit-learn/scikit-learn/issues/18671","Logical Bug"
"173","scikit-learn/scikit-learn","fit.transform not equal to fit_transform in NMF with solver 'mu' and init 'nndsvda'","#### Describe the bug
The test verifying that `fit.transform == fit_transform` failed for the `NMF` estimator with `solver='mu'` and init 'nndsvda'

Discovered in #16948, see also #18505.

cc @jeremiedbb @TomDLT , maybe @vene . Thanks!

#### Steps/Code to Reproduce
```python
from sklearn.utils.estimator_checks import check_transformer_general
from sklearn.decomposition import NMF

check_transformer_general('NMF', NMF(init='nndsvda', max_iter=1000, solver='mu'))
```

#### Expected Results
The test passes.

#### Actual Results
```pytr
Traceback (most recent call last):
  File ""debug_nmf.py"", line 4, in <module>
    check_transformer_general(
  File ""/home/cmarmo/software/scikit-learn/sklearn/utils/_testing.py"", line 302, in wrapper
    return fn(*args, **kwargs)
  File ""/home/cmarmo/software/scikit-learn/sklearn/utils/estimator_checks.py"", line 1305, in check_transformer_general
    _check_transformer(name, transformer, X, y)
  File ""/home/cmarmo/software/scikit-learn/sklearn/utils/estimator_checks.py"", line 1390, in _check_transformer
    assert_allclose_dense_sparse(
  File ""/home/cmarmo/software/scikit-learn/sklearn/utils/_testing.py"", line 409, in assert_allclose_dense_sparse
    assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)
  File ""/home/cmarmo/.skldevenv/lib64/python3.8/site-packages/numpy/testing/_private/utils.py"", line 1532, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/home/cmarmo/.skldevenv/lib64/python3.8/site-packages/numpy/testing/_private/utils.py"", line 846, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=0.01
fit_transform and transform outcomes not consistent in NMF(init='nndsvda', max_iter=1000, random_state=0, solver='mu')
Mismatched elements: 18 / 90 (20%)
Max absolute difference: 0.04219191
Max relative difference: 1.
 x: array([[5.584139e-01, 5.114790e-01, 4.818158e-01],
       [7.176271e-02, 9.486441e-02, 2.635424e-01],
       [1.848232e-01, 1.241254e-01, 9.756835e-22],...
 y: array([[5.566442e-01, 5.129683e-01, 4.835650e-01],
       [7.235885e-02, 9.501380e-02, 2.622291e-01],
       [1.846617e-01, 1.244062e-01, 1.105227e-07],...
```

#### Versions
```
System:
    python: 3.8.5 (default, Aug 12 2020, 00:00:00)  [GCC 10.2.1 20200723 (Red Hat 10.2.1-1)]
executable: /home/cmarmo/.skldevenv/bin/python
   machine: Linux-5.8.13-200.fc32.x86_64-x86_64-with-glibc2.2.5

Python dependencies:
          pip: 20.2.3
   setuptools: 41.6.0
      sklearn: 0.24.dev0
        numpy: 1.18.5
        scipy: 1.5.2
       Cython: 0.29.21
       pandas: 1.1.0
   matplotlib: 3.3.0
       joblib: 0.16.0
threadpoolctl: 2.1.0

Built with OpenMP: True
```","['Bug', 'module:decomposition']","2020-10-21T09:41:07Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/18663","Logical Bug"
"174","scikit-learn/scikit-learn","Bug: VotingClassifier does not support estimators using class_weight","When using `VotingClassifier`, the estimators used inside cannot use the  `class_weight` parameter. The reason for that, I believe, is that `VotingClassifier` is encoding the labels before passing them to the single estimators, but not encoding the labels in `class_weight`. This throws a `ValueError: Class label <> not present`.

As a matter of fact, I wonder why this encoding would even happen in the first place, as it is handled by each model individually.

Here is a reproducible example:

```
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
import numpy as np

X = np.array([[1,2], [1,3], [2,1], [2,3], [2,3]])
y = np.array([""a"", ""b"", ""a"", ""b"", ""a""])

LR = LogisticRegression(class_weight={""a"":1, ""b"": 2})
VC = VotingClassifier([(""LR"", LR)])

VC.fit(X, y)
```

Traceback:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-3ae1f62ef011> in <module>
      9 VC = VotingClassifier([(""LR"", LR)])
     10 
---> 11 VC.fit(X, y)

/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_voting.py in fit(self, X, y, sample_weight)
    220         transformed_y = self.le_.transform(y)
    221 
--> 222         return super().fit(X, transformed_y, sample_weight)
    223 
    224     def predict(self, X):

/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_voting.py in fit(self, X, y, sample_weight)
     66                 delayed(_parallel_fit_estimator)(clone(clf), X, y,
     67                                                  sample_weight=sample_weight)
---> 68                 for clf in clfs if clf not in (None, 'drop')
     69             )
     70 

/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py in __call__(self, iterable)
   1002             # remaining jobs.
   1003             self._iterating = False
-> 1004             if self.dispatch_one_batch(iterator):
   1005                 self._iterating = self._original_iterator is not None
   1006 

/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
    833                 return False
    834             else:
--> 835                 self._dispatch(tasks)
    836                 return True
    837 

/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py in _dispatch(self, batch)
    752         with self._lock:
    753             job_idx = len(self._jobs)
--> 754             job = self._backend.apply_async(batch, callback=cb)
    755             # A job can complete so quickly than its callback is
    756             # called before we get here, causing self._jobs to

/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py in apply_async(self, func, callback)
    207     def apply_async(self, func, callback=None):
    208         """"""Schedule a func to be run""""""
--> 209         result = ImmediateResult(func)
    210         if callback:
    211             callback(result)

/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py in __init__(self, batch)
    588         # Don't delay the application, to avoid keeping the input
    589         # arguments in memory
--> 590         self.results = batch()
    591 
    592     def get(self):

/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py in __call__(self)
    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    255             return [func(*args, **kwargs)
--> 256                     for func, args, kwargs in self.items]
    257 
    258     def __len__(self):

/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py in <listcomp>(.0)
    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    255             return [func(*args, **kwargs)
--> 256                     for func, args, kwargs in self.items]
    257 
    258     def __len__(self):

/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/_base.py in _parallel_fit_estimator(estimator, X, y, sample_weight)
     34             raise
     35     else:
---> 36         estimator.fit(X, y)
     37     return estimator
     38 

/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py in fit(self, X, y, sample_weight)
   1599                       penalty=penalty, max_squared_sum=max_squared_sum,
   1600                       sample_weight=sample_weight)
-> 1601             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))
   1602 
   1603         fold_coefs_, _, n_iter_ = zip(*fold_coefs_)

/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py in __call__(self, iterable)
   1002             # remaining jobs.
   1003             self._iterating = False
-> 1004             if self.dispatch_one_batch(iterator):
   1005                 self._iterating = self._original_iterator is not None
   1006 

/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
    833                 return False
    834             else:
--> 835                 self._dispatch(tasks)
    836                 return True
    837 

/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py in _dispatch(self, batch)
    752         with self._lock:
    753             job_idx = len(self._jobs)
--> 754             job = self._backend.apply_async(batch, callback=cb)
    755             # A job can complete so quickly than its callback is
    756             # called before we get here, causing self._jobs to

/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py in apply_async(self, func, callback)
    207     def apply_async(self, func, callback=None):
    208         """"""Schedule a func to be run""""""
--> 209         result = ImmediateResult(func)
    210         if callback:
    211             callback(result)

/opt/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py in __init__(self, batch)
    588         # Don't delay the application, to avoid keeping the input
    589         # arguments in memory
--> 590         self.results = batch()
    591 
    592     def get(self):

/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py in __call__(self)
    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    255             return [func(*args, **kwargs)
--> 256                     for func, args, kwargs in self.items]
    257 
    258     def __len__(self):

/opt/anaconda3/lib/python3.7/site-packages/joblib/parallel.py in <listcomp>(.0)
    254         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    255             return [func(*args, **kwargs)
--> 256                     for func, args, kwargs in self.items]
    257 
    258     def __len__(self):

/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py in _logistic_regression_path(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)
    841     le = LabelEncoder()
    842     if isinstance(class_weight, dict) or multi_class == 'multinomial':
--> 843         class_weight_ = compute_class_weight(class_weight, classes, y)
    844         sample_weight *= class_weight_[le.fit_transform(y)]
    845 

/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/class_weight.py in compute_class_weight(class_weight, classes, y)
     63             i = np.searchsorted(classes, c)
     64             if i >= len(classes) or classes[i] != c:
---> 65                 raise ValueError(""Class label {} not present."".format(c))
     66             else:
     67                 weight[i] = class_weight[c]

ValueError: Class label a not present.
```

This issue was noticed [here](https://stackoverflow.com/questions/60992466/using-class-weights-with-sklearn-votingclassifier#comment108421729_60992466).

System:
    python: 3.7.6 
   machine: macOS

   sklearn: 0.22.1
     numpy: 1.18.1
     scipy: 1.4.1","['Bug', 'module:ensemble']","2020-10-06T20:20:24Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/18550","Logical Bug"
"175","scikit-learn/scikit-learn","Error on using None for missing values in SimpleImputer with boolean arrays","```python
from sklearn.impute import SimpleImputer
import pandas as pd
X2 = pd.DataFrame({'a': [True, False, True, False, None]})
SimpleImputer(strategy='most_frequent').fit_transform(X2)
```

> TypeError: '<' not supported between instances of 'NoneType' and 'bool'

Interestingly this works when the remaining types are float, or when using ``np.NaN`` for the missing value.

Potentially related to #17625, where I suggested treating ``None`` as ``np.NaN``. 

FYI ``X2.a.unique()`` gives the expected result, ``np.unique(X2.a)`` errors. I'm not sure if using the  ``__array_function__`` protocol might help us here, we might be casting to numpy arrays before hitting the unique.","['Bug', 'module:impute']","2020-09-25T16:30:47Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/18461","Logical Bug"
"176","scikit-learn/scikit-learn","feature_importance causes a BSOD on Windows 10","
#### Describe the bug
Running permutation_importance on a medium-sized data set results in a BSOD on Windows 10. The dataset is 470605 x 332, code is running in a Jupyter notebook, Python version 3.7.6, scikit version 0.22.1.
The BSOD is a KERNEL_SECURITY_CHECK_FAILURE, with ERROR_CODE: `(NTSTATUS) 0xc0000409 - The system detected an overrun of a stack-based buffer in this application. This overrun could potentially allow a malicious user to gain control of this application.`
The machine has a Ryzen 5 3600 with 16GB of RAM.

#### Steps/Code to Reproduce

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
rf = RandomForestClassifier(n_estimators = 250,
                           n_jobs = -1,
                           oob_score = True,
                           bootstrap = True,
                           random_state = 42)
rf.fit(X_train, y_train)
permImp = permutation_importance(rf,
                                 X_val,
                                 y_val,
                                 scoring='f1',
                                 n_repeats=5,
                                 n_jobs=-1,
                                 random_state=42)
```

#### Expected Results
No BSOD, permutation importance computed.

#### Actual Results
BSOD after ~1-2 minutes

#### Versions
> sklearn.show_versions()

System:
    python: 3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\lucag\anaconda3\python.exe
   machine: Windows-10-10.0.18362-SP0

Python dependencies:
       pip: 20.0.2
setuptools: 45.2.0.post20200210
   sklearn: 0.22.1
     numpy: 1.18.1
     scipy: 1.4.1
    Cython: 0.29.15
    pandas: 1.0.1
matplotlib: 3.1.3
    joblib: 0.14.1

Built with OpenMP: True
<!-- Thanks for contributing! -->
","['Bug', 'Large Scale', 'OS:Windows']","2020-08-18T16:15:05Z","23","0","https://github.com/scikit-learn/scikit-learn/issues/18187","Dependency Issue"
"177","scikit-learn/scikit-learn","SpectralClustering with assign_labels='discretize' returns less clusters than specified in n_clusters","I'm not sure if this is a bug or an undocumented feature (or at least I couldn't find any info about it).

Using sklearn version 0.22.1.

The following code is producing the behavior mentioned in the title:

```python
clustering = SpectralClustering(n_clusters=4, affinity='rbf', assign_labels='discretize', random_state=0).fit(X)
np.unique(clustering.labels_) shows just 3 labels. 
```

X is a np.array of shape (258, 257).
This happens only with some random seeds. I was not able to reproduce it with assign_labels='kmeans' (running multiple times).
Also the labels of the clusters are not consecutive as if less than n_clusters where picked out of the n_components.","['Bug', 'module:cluster']","2020-08-04T10:04:28Z","8","0","https://github.com/scikit-learn/scikit-learn/issues/18083","Logical Bug"
"178","scikit-learn/scikit-learn","Unexpected behavior when passing multiple parameter sets to RandomizedSearchCV","#### Describe the bug

Here is part of the documentation for the `param_distributions` parameter of [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html):

> If a list of dicts is given, first a dict is sampled uniformly, and then a parameter is sampled using that dict as above.

My interpretation is that if I pass a list of two dictionaries, then at each iteration:

- First, one of the two dictionaries will be chosen at random
- Then, a set of parameters within that dictionary will be chosen at random

I have found that that is **not** the case. Instead, I have found that if one of the two dictionaries has many more possible parameter combinations, then the larger dictionary will usually be chosen at each iteration.

#### Steps/Code to Reproduce

```python
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV

X, y = load_iris(return_X_y=True)
rf = RandomForestClassifier()

# 30 possible combinations
params1 = {}
params1['n_estimators'] = [10, 20, 30, 40, 50]
params1['min_samples_leaf'] = [1, 2, 3, 4, 5, 6]

# 120 possible combinations
params2 = {}
params2['n_estimators'] = [60, 70, 80]
params2['min_samples_leaf'] = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
params2['max_features'] = ['auto', None]
params2['bootstrap'] = [True, False]

params_both = [params1, params2]

rand = RandomizedSearchCV(rf, params_both, cv=5, scoring='accuracy', n_iter=50, random_state=1)
rand.fit(X, y)
print(sorted(rand.cv_results_['param_n_estimators']))
```

#### Expected Results

Since `n_iter=50`, I would expect that `params1` and `params2` would each be chosen about 25 times.

#### Actual Results

Here is the actual output of the last line:

```
[10, 20, 30, 40, 40, 50, 50, 50, 50, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 70, 70, 70, 70, 70, 70, 70, 70, 70, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]
```

As you can see, `params1` (which had values 10 through 50) was chosen only 9 times, and whereas `params2` (which had values 60 through 80) was chosen 41 times.

#### Comments

There seem to be two possibilities:

1. The current behavior of `RandomizedSearchCV` **is** the desired behavior. In that case, I would propose tweaking the documentation to make this behavior more clear.
2. The current behavior of `RandomizedSearchCV` **is not** the desired behavior. In that case, I would propose fixing the behavior so that it matches the documentation.

I don't have a strong feeling about which behavior is the ""optimal"" behavior.

This was implemented in #14549 by @amueller, so he may have some insight on this!

#### Versions

```
System:
    python: 3.8.2 | packaged by conda-forge | (default, Apr 24 2020, 07:56:27)  [Clang 9.0.1 ]
executable: /Users/kevin/miniconda3/envs/sk23/bin/python
   machine: macOS-10.14.6-x86_64-i386-64bit

Python dependencies:
          pip: 20.1.1
   setuptools: 46.4.0.post20200518
      sklearn: 0.23.1
        numpy: 1.18.4
        scipy: 1.4.1
       Cython: None
       pandas: 1.0.3
   matplotlib: 3.2.1
       joblib: 0.15.1
threadpoolctl: 2.0.0

Built with OpenMP: True
```
","['Bug', 'Moderate', 'help wanted', 'module:model_selection']","2020-08-01T19:23:24Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/18057","Logical Bug"
"179","scikit-learn/scikit-learn","predict_proba() for linear models with log loss can return NaNs with large model coefficients","<!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Describe the bug
The predict_proba() function can return NaNs for linear models with log loss if
the training has resulted in a model with large coefficients. The linear equation
can result in decision_function() returning large negative values for every class
for a single input. This means that the normalization in predict_proba_lr()
(line 327 in https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/_base.py )
divides by zero, resulting in NaNs being returned.

I stumbled across this problem when using SGDClassifier with log loss and default
alpha. I wanted to train the model using minibatches and monitor the loss after
each batch (using metrics.log_loss and the outputs of predict_proba() ). I happened
to hit values of the model coefficients that created this issue. Increasing alpha to
increase regularization helps prevent the issue, but it would be good if it could be
avoided somehow, or if a more pertinent warning were given to the user.

#### Steps/Code to Reproduce
See gist: https://gist.github.com/richardtomsett/8b814f30e1d665fae2b4085d3e4156f5

#### Expected Results
predict_proba() returns a valid categorical probability distribution over the classes

#### Actual Results
predict_proba() returns NaN for some inputs

#### Versions
System:
    python: 3.8.3 (default, May 19 2020, 13:54:14)  [Clang 10.0.0 ]
executable: [redacted]
   machine: macOS-10.15.3-x86_64-i386-64bit

Python dependencies:
          pip: 20.0.2
   setuptools: 47.1.1.post20200604
      sklearn: 0.23.1
        numpy: 1.18.5
        scipy: 1.4.1
       Cython: None
       pandas: None
   matplotlib: 3.3.0
       joblib: 0.15.1
threadpoolctl: 2.1.0

Built with OpenMP: True
","['Bug', 'module:linear_model']","2020-07-23T16:37:57Z","10","1","https://github.com/scikit-learn/scikit-learn/issues/17978","UI/UX Bug"
"180","scikit-learn/scikit-learn","Single linkage option in Agglomerative causes MemoryError for very large numbers","#### Describe the bug
Using the single linkage option in Agglomerative clustering results in a MemoryError when the input data contains very large values, likely due to numeric overflows. 

#### Steps/Code to Reproduce
Example: 
Running the code below with half of the sample data does not produce a MemoryError (although the same numpy RuntimeWarning is being thrown). However, if the data contains as many very large numbers as in my sample below my machine with 16Gb RAM runs out of memory and I get a MemoryError. To me this seems a bit disproportionate as the sample data is not very large.

```python
from sklearn.cluster import AgglomerativeClustering

X = [[1.30830774e+307, 6.02217328e+307],
     [1.54166067e+308, 1.75812744e+308],
     [5.57938866e+307, 4.13840113e+307],
     [1.36302835e+308, 1.07968131e+308],
     [1.58772669e+308, 1.19380571e+307],
     [2.20362426e+307, 1.58814671e+308],
     [1.06216028e+308, 1.14258583e+308],
     [7.18031911e+307, 1.69661213e+308],
     [7.91182553e+307, 5.12892426e+307],
     [5.58470885e+307, 9.13566765e+306],
     [1.22366243e+308, 8.29427922e+307],
     [4.39205961e+306, 1.26048413e+308],
     [4.61599953e+306, 7.24075646e+307],
     [1.66596896e+308, 1.65498552e+308],
     [4.73958815e+307, 7.66412710e+307],
     [1.57013390e+308, 1.03527051e+308],
     [1.28464631e+308, 1.68216358e+308],
     [1.07121506e+308, 3.11489418e+307],
     [2.97524276e+307, 1.59238260e+306],
     [1.24529964e+308, 5.18478922e+306],
     [7.55088957e+307, 7.08726240e+307],
     [8.38161061e+307, 1.50363727e+307],
     [1.11502738e+308, 3.77564517e+307],
     [1.33977566e+308, 4.21606136e+307],
     [3.18410347e+306, 1.41182675e+308],
     [1.37949806e+307, 1.33091975e+308],
     [8.84125355e+307, 8.83334687e+307],
     [4.16683700e+307, 4.38042780e+307],
     [6.33989592e+307, 1.13127438e+308],
     [3.30270525e+307, 5.82271903e+307],
     [1.73464450e+308, 1.57922601e+308],
     [8.50840567e+307, 8.34750980e+307],
     [1.06389805e+308, 7.01361194e+307],
     [1.20971776e+308, 1.42173002e+308],
     [1.91271271e+307, 7.63302091e+307],
     [1.46375293e+308, 3.27352625e+307],
     [5.64255485e+307, 1.62562194e+308],
     [5.96180877e+307, 4.31806469e+307],
     [1.18773200e+308, 4.91146568e+307],
     [1.19298612e+307, 1.54110558e+308],
     [3.51302340e+307, 5.60375139e+307],
     [5.14281492e+307, 9.76302343e+307],
     [1.77408023e+308, 1.65760854e+308],
     [1.46544741e+308, 1.47445640e+308],
     [1.76109403e+308, 1.30923042e+308],
     [1.64984146e+308, 2.58303609e+307],
     [1.61558758e+307, 1.03985868e+308],
     [1.37977676e+308, 4.90921157e+307],
     [1.01105745e+308, 1.57678709e+307],
     [1.24672794e+308, 5.96657664e+307]]

clusterer = AgglomerativeClustering(linkage='single')
clusterer.fit(X)
```

#### Expected Results
No error is thrown or ValueError that specifies the range of allowed values. GaussianMixture clustering is handling the same data like this:
```
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
```

#### Actual Results

```python-traceback
C:\Program Files\Python37\lib\site-packages\numpy\core\fromnumeric.py:90: RuntimeWarning: overflow encountered in reduce
  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

Traceback (most recent call last):
  File ""C:/Users/thaar/PycharmProjects/sklearn-dev/agglomerative_test.py"", line 58, in <module>
    clusterer.fit(X)
  File ""C:\Program Files\Python37\lib\site-packages\sklearn\cluster\_agglomerative.py"", line 898, in fit
    self.n_leaves_)
  File ""C:\Program Files\Python37\lib\site-packages\sklearn\cluster\_agglomerative.py"", line 674, in _hc_cut
    label[_hierarchical._hc_get_descendent(-node, children, n_leaves)] = i
  File ""sklearn\cluster\_hierarchical_fast.pyx"", line 96, in sklearn.cluster._hierarchical_fast._hc_get_descendent
MemoryError
```

#### Versions
System:
    python: 3.7.3 (v3.7.3:ef4ec6ed12, Mar 25 2019, 22:22:05) [MSC v.1916 64 bit (AMD64)]
executable: C:\Users\thaar\PycharmProjects\sklearn-dev\venv\Scripts\python.exe
   machine: Windows-10-10.0.18362-SP0

Python dependencies:
          pip: 20.1.1
   setuptools: 49.2.0
      sklearn: 0.23.1
        numpy: 1.18.4
        scipy: 1.4.1
       Cython: None
       pandas: 1.0.3
   matplotlib: 3.2.1
       joblib: 0.14.1
threadpoolctl: 2.0.0

Built with OpenMP: True

<!-- Thanks for contributing! -->
","['Bug', 'help wanted', 'module:cluster']","2020-07-20T20:40:21Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/17960","Performance Issue"
"181","scikit-learn/scikit-learn","[0.23.1] AdaBoostRegressor failes on arm(rhel) processors","#### Describe the bug
On arm processors (AWS:gravition2, rhel), I get the following failure in version 0.23.1
```
937     Examples
938     --------
939     >>> from sklearn.ensemble import AdaBoostRegressor
940     >>> from sklearn.datasets import make_regression
941     >>> X, y = make_regression(n_features=4, n_informative=2,
942     ...                        random_state=0, shuffle=False)
943     >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)
944     >>> regr.fit(X, y)
945     AdaBoostRegressor(n_estimators=100, random_state=0)
946     >>> regr.predict([[0, 0, 0, 0]])
Expected:
    array([4.7972...])
Got:
    array([5.57874246])

/home/ec2-user/github/sklearn_master/sklearn/ensemble/_weight_boosting.py:946: DocTestFailure
```

#### Steps/Code to Reproduce
`pytest -v sklearn/ensemble/_weight_boosting.py::sklearn.ensemble._weight_boosting.AdaBoostRegressor`

#### Expected Results
PASSED is thrown. 

#### Actual Results
FAILED is thrown.
```
946     >>> regr.predict([[0, 0, 0, 0]])
Expected:
    array([4.7972...])
Got:
    array([5.57874246])
```

#### Versions
```
System:
    python: 3.6.8 (default, Dec  5 2019, 16:02:25)  [GCC 8.3.1 20191121 (Red Hat 8.3.1-5)]
executable: /usr/bin/python3
   machine: Linux-4.18.0-193.1.2.el8_2.aarch64-aarch64-with-redhat-8.2-Ootpa

Python dependencies:
          pip: 20.1.1
   setuptools: 39.2.0
      sklearn: 0.23.1
        numpy: 1.14.3
        scipy: 1.0.0
       Cython: 0.29
       pandas: 1.0.5
   matplotlib: 3.2.1
       joblib: 0.14.0
threadpoolctl: 2.1.0

Built with OpenMP: True
Linux-4.18.0-193.1.2.el8_2.aarch64-aarch64-with-redhat-8.2-Ootpa
Traceback (most recent call last):
  File ""<string>"", line 3, in <module>
NameError: name 'Python' is not defined
```","['Bug', 'help wanted', 'module:ensemble', 'arch:arm']","2020-07-01T00:55:28Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/17798","Runtime Error"
"182","scikit-learn/scikit-learn","pandas dtypes ""boolean"" not supported in classification target","<!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Describe the bug
Pandas has extended NumPy's dtypes and these dtypes extensions are not all supported as targets in a sklearn classifier. In particular, if the target y is a Pandas ""boolean"" dtype, a classifier such as LogisticRegression fails whereas if the target is a numpy ""bool"" dtype, the classifier will not fail.

#### Steps/Code to Reproduce
```python
import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.DataFrame({""boolean_col"": pd.Series([False, True, False, True], dtype=""boolean""),
                   ""bool_col"": pd.Series([False, True, False, True], dtype=""bool""),
                   ""num_col"": pd.Series([1, 2, 3, 4])})

clf = LogisticRegression()
```




#### Expected Results
```python
clf.fit(df[[""num_col""]], df.bool_col)
--------------------------------------------------------------------------------------------------------------
LogisticRegression()
```
#### Actual Results
```python
clf.fit(df[[""num_col""]], df.boolean_col)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-9a0c5abf260e> in <module>
----> 1 clf.fit(df[[""num_col""]], df.boolean_col)

~\AppData\Local\Continuum\anaconda3\envs\lognode\lib\site-packages\sklearn\linear_model\_logistic.py in fit(self, X, y, sample_weight)
   1343                                    order=""C"",
   1344                                    accept_large_sparse=solver != 'liblinear')
-> 1345         check_classification_targets(y)
   1346         self.classes_ = np.unique(y)
   1347 

~\AppData\Local\Continuum\anaconda3\envs\lognode\lib\site-packages\sklearn\utils\multiclass.py in check_classification_targets(y)
    170     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',
    171                       'multilabel-indicator', 'multilabel-sequences']:
--> 172         raise ValueError(""Unknown label type: %r"" % y_type)
    173 
    174 

ValueError: Unknown label type: 'unknown'
```
#### Versions
System:
    python: 3.8.2 (default, Apr 14 2020, 19:01:40) [MSC v.1916 64 bit (AMD64)]
executable: ~\AppData\Local\Continuum\anaconda3\envs\lognode\python.exe
   machine: Windows-10-10.0.18362-SP0

Python dependencies:
          pip: 20.0.2
   setuptools: 46.1.3.post20200330
      sklearn: 0.23.1
        numpy: 1.18.1
        scipy: 1.4.1
       Cython: None
       pandas: 1.0.3
   matplotlib: 3.2.1
       joblib: 0.14.1
threadpoolctl: 2.1.0

Built with OpenMP: True","['Bug']","2020-06-23T15:25:30Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/17675","Dependency Issue"
"183","scikit-learn/scikit-learn","inconsistent treatment of None and np.NaN in SimpleImputer","Doing constant imputation treats only the ""missing_value"" as missing, so a ``None`` by default stays there:
```python
from sklearn.impute import SimpleImputer
import numpy as np
X = np.array([1, 2, np.NaN, None]).reshape(-1, 1)
SimpleImputer(strategy='constant', fill_value=""asdf"").fit_transform()
```
```
array([[1],
       [2],
       ['asdf'],
       [None]], dtype=object)
```
However, using strategy='mean' coerces the None to NaN and so both are replaced:
```python
SimpleImputer(strategy='mean').fit_transform(X)
```
```
array([[1. ],
       [2. ],
       [1.5],
       [1.5]])
```
I don't think the definition of what's missing should depend on the strategy. @thomasjpfan argues that the current constant behavior is inconvenient because it means you have to impute both values separately if you want to one-hot-encode.

It seems more safe to treat them differently but I'm not sure there's a use-case for that. This came up in #17317.
I think this only matters in these two, as other imputers don't allow dtype object arrays.","['Bug']","2020-06-17T20:23:13Z","6","1","https://github.com/scikit-learn/scikit-learn/issues/17625","Logical Bug"
"184","scikit-learn/scikit-learn","Linear SVM does wrong prediction","#### Describe the bug
The prediction for specific train data is wrong

#### Steps/Code to Reproduce
You can run the following code. While the first decision limit is correct, for the 2nd pair of points the decision limit is totally wrong placed

Example:
```python
from sklearn import svm
from matplotlib import pyplot as plt
import numpy as np

lclf = svm.LinearSVC()

p1 = 0.25
p2 = -0.25

x_min = -2
x_max = 2
x_h = 0.01
y_min = -2
y_max = 2
y_h = 0.01
xx, yy = np.meshgrid(np.arange(x_min, x_max, x_h),
                    np.arange(y_min, y_max, y_h))

lclf.fit([[p1,0],[p2,0]],[-1,1])

plt.plot(p1, 0, ""ro"")
plt.plot(p2, 0, ""go"")

Z = lclf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.gca().contourf(xx, yy, Z, colors = [""red"", ""green""], alpha = 0.1)
plt.show()


#The following does not work

p3 = 0.025545042466768184
p4 = 0.0293294646367189

x_min = 0
x_max = 0.035
x_h = 0.00001
y_min = -0.01
y_max = 0.01
y_h = 0.001
xx, yy = np.meshgrid(np.arange(x_min, x_max, x_h),
                    np.arange(y_min, y_max, y_h))

lclf.fit([[p3, 0], [p4,0]], [1,-1])

plt.plot(p3, 0, ""go"")
plt.plot(p4, 0, ""ro"")

Z = lclf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.gca().contourf(xx, yy, Z, colors = [""red"", ""green""], alpha = 0.1)
plt.show()
```


#### Expected Results
The decision boarder should be centered between the two train points

#### Actual Results
the boarder is on the left of both points

#### Versions
System:
    python: 3.7.5 (tags/v3.7.5:5c02a39a0b, Oct 15 2019, 00:11:34) [MSC v.1916 64 bit (AMD64)]
executable: C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python37_64\python.exe
   machine: Windows-10-10.0.18362-SP0

Python dependencies:
       pip: 20.0.2
setuptools: 46.1.3
   sklearn: 0.22.2.post1
     numpy: 1.18.3
     scipy: 1.4.1
    Cython: None
    pandas: 1.0.3
matplotlib: 3.2.1
    joblib: 0.14.1

Built with OpenMP: True
","['Bug', 'module:linear_model', 'module:svm']","2020-06-10T13:37:27Z","9","0","https://github.com/scikit-learn/scikit-learn/issues/17557","Logical Bug"
"185","scikit-learn/scikit-learn","LinearDiscriminantAnalysis cannot predict_proba if labels are constant","I know that most of the times labels will not be constant, however I am using `check_estimator` to check a custom estimator which has a `base_estimator` attribute and scikit-learn chooses LinearDiscriminantAnalysis as base estimator.

#### Describe the bug

```python
import numpy as np
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
rnd = np.random.RandomState(0)
X_train = rnd.uniform(size=(10, 3))
y = np.ones(10)
LinearDiscriminantAnalysis().fit(X_train, y).predict_proba(X_train)
```

The code above will throw the following error:

`AxisError: axis 1 is out of bounds for array of dimension 1`

It does not happen if I use the `predict` method. I also tried with a pair more classifiers (ensembles) and it actually works. I also tried the other discriminant analysis, the quadratic, and it does not let me fit because y is constant. This error makes sense to me, but I can't understand what is going on with the one in the linear discriminant analysis,.

#### Versions
Linux-4.4.0-179-generic-x86_64-with-glibc2.10
Python 3.8.3 (default, May 19 2020, 18:47:26) 
[GCC 7.3.0]
NumPy 1.18.1
SciPy 1.4.1
Scikit-Learn 0.23.1

","['Bug']","2020-06-01T08:23:10Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/17401","Runtime Error"
"186","scikit-learn/scikit-learn","_weighted_percentile does not lead to the same result than np.median","While reviewing a test in https://github.com/scikit-learn/scikit-learn/pull/16937, it appears that our implementation of `_weighted_percentile` with unit `sample_weight` will lead to a different result than `np.median` which is a bit problematic for consistency.

In the gradient-boosting, it brakes the loss equivalence because the initial predictions are different. We could bypass this issue by always computing the median using `_weighted_percentile` there.

```python
import pytest
import numpy as np
from sklearn.utils.stats import _weighted_percentile

rng = np.random.RandomState(42)
X = rng.randn(10)
X.sort()
sample_weight = np.ones(X.shape)

median_numpy = np.median(X)
median_numpy_percentile = np.percentile(X, 50)
median_sklearn = _weighted_percentile(X, sample_weight, percentile=50.0)

assert median_numpy == pytest.approx(np.mean(X[[4, 5]]))
assert median_sklearn == pytest.approx(X[4])
assert median_numpy == median_numpy_percentile
assert median_sklearn == pytest.approx(median_numpy)
```","['Bug']","2020-05-28T10:03:16Z","16","0","https://github.com/scikit-learn/scikit-learn/issues/17370","Logical Bug"
"187","scikit-learn/scikit-learn","KMeans(init='k-means++') performance issue with OpenBLAS","I open this issue to investigate a performance problem that might be related to #17230.

I adapted the reproducer of #17230 to display more info and make it work on a medium-size ranom dataset.

```python
from sklearn import cluster
from time import time
from pprint import pprint
from threadpoolctl import threadpool_info
import numpy as np


pprint(threadpool_info())
rng = np.random.RandomState(0)
data = rng.randn(5000, 50)
t0_global = time()
for k in range(1, 15):
    t0 = time()
    # print(f""Running k-means with k={k}: "", end="""", flush=True)
    cluster.KMeans(
        n_clusters=k,
        random_state=42,
        n_init=10,
        max_iter=2000,
        algorithm='full',
        init='k-means++').fit(data)
    # print(f""{time() - t0:.3f} s"")

print(f""Total duration: {time() - t0_global:.3f} s"")
```

I tried to run this on Linux with scikit-learn master (therefore including the #16499 fix)  with 2 different builds of scipy (with openblas from pypi and MKL from anaconda) and various values for `OMP_NUM_THREADS` (unset, `OMP_NUM_THREADS=1`, `OMP_NUM_THREADS=2`, `OMP_NUM_THREADS=4`) on a laptop with 2 physical cpu cores (4 logical cpus).

In both cases, I use the same scikit-learn binaries (built with GCC in editable mode). I just change the env.

The summary is:

- with MKL there is not problem: large or unset values of `OMP_NUM_THREADS` are faster than `OMP_NUM_THREADS=1`;
- with OpenBLAS without explicit setting of `OMP_NUM_THREADS` or setting a large value for it is significanlty slower forced sequential run with `OMP_NUM_THREADS=1`.

I will include my runs in the first comment. 

/cc @jeremiedbb ","['Bug', 'Performance', 'module:cluster']","2020-05-25T09:51:23Z","11","0","https://github.com/scikit-learn/scikit-learn/issues/17334","Performance Issue"
"188","scikit-learn/scikit-learn","sklearn.utils.resample stratify parameter is not working","I am using 

```
Python 3.6.8
scikit-learn==0.23.1
```

I am using ```sklearn.utils,resample``` for stratified sampling. Here is my code:-

```python
from sklearn.utils import resample
y=[1,1,2,3,2,2,1,3,1,1,2,3,2,2,1,3,4,4]
sample = resample(y, n_samples=5, replace=False, stratify=y,
         random_state=0)
print(sample)
```

gives me output:-
```
[3, 2, 2, 1, 1]
```

Expected output:-
output should contain all the distinct values in y. At least one ""4"" should be present in the output. For eg.
```
[3,2,2,1,4]
```

Am I missing something? Thank you for your help in advance.","['Bug', 'module:utils']","2020-05-24T06:15:29Z","10","0","https://github.com/scikit-learn/scikit-learn/issues/17321","Logical Bug"
"189","scikit-learn/scikit-learn","GridsearchCV.score with multimetric scoring and callable refit","#### Describe the bug
When using GridsearchCV with multimetric scoring and a callable as refit, the `GridsearchCV.score` function doesn't works since `score = self.scorer_[self.refit] if self.multimetric_ else self.scorer_` seems to wait only for a string in case of multimetric scoring
#### Steps/Code to Reproduce
```
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold
from sklearn.linear_model import LogisticRegression
import numpy as np

def get_best_index(cv_results):
    best_rank_mask = cv_results['rank_test_roc_auc'] == cv_results['rank_test_roc_auc'].min()
    params_best_score = np.array(cv_results['params'])[best_rank_mask]
    params_name = params_best_score[0].keys()
    classifier_params_names = [_ for _ in params_name if 'classifier' in _]
    if 'classifier__C' in classifier_params_names or 'classifier__base_estimator__C' in classifier_params_names:
        classifier_params = [_['classifier__C' if 'classifier__C' in classifier_params_names else 'classifier__base_estimator__C'] for _ in params_best_score]
        params_best_score = params_best_score[classifier_params == min(classifier_params)]
    best_params = params_best_score[0]
    best_index = int(np.where(np.array(cv_results['params']) == best_params)[0])
    return best_index

breast = load_breast_cancer()
X = breast.data
y = breast.target
cv = RepeatedStratifiedKFold(5, 2, random_state=111)
params_dic = {'C': np.arange(0.1, 1.1, 0.1)}
clf = GridSearchCV(LogisticRegression(penalty='l2', max_iter=1e5, solver='saga'), params_dic, scoring=['roc_auc', 'accuracy'], cv=cv, refit=get_best_index, n_jobs=4)
clf.fit(X,y)
clf.score(X,y)
```

#### Actual Results
File ""C:\Users\Tim\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py"", line 447, in score
    score = self.scorer_[self.refit] if self.multimetric_ else self.scorer_
KeyError: <function get_best_index at 0x0000028DD66BDBF8>

#### Expected Results
Since refit is a callable I don't know how he could know which metric to choose for scoring., However, if I give a string with the metric I chose, i.e. 'roc_auc', to refit argument the best index won't be chosen in the way I want. Maybe in case of multimetric scoring and callable refit, ask for dictionnary instead like {score: callable} and the score will be used in GridsearchCV.score ?

#### Versions
System:
    python: 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]
executable: C:\Users\Tim\Anaconda3\python.exe
   machine: Windows-10-10.0.18362-SP0
Python dependencies:
       pip: 20.0.2
setuptools: 39.1.0
   sklearn: 0.22.1
     numpy: 1.18.1
     scipy: 1.4.1
    Cython: 0.28.2
    pandas: 1.0.0
matplotlib: 2.2.2
    joblib: 0.14.1
Built with OpenMP: True
","['Bug', 'module:model_selection']","2020-04-27T10:54:52Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/17058","Logical Bug"
"190","scikit-learn/scikit-learn","BUG: Test failure on ppc64le","<!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Describe the bug

While working on https://github.com/conda-forge/scikit-learn-feedstock/pull/123, I found that `manifold/tests/test_t_sne.py::test_uniform_grid[barnes_hut]` fails on ppc64le; @rth [asked](https://github.com/conda-forge/scikit-learn-feedstock/pull/123#issuecomment-614625410) me to open a bug here.

```
=========================== short test summary info ============================
FAILED manifold/tests/test_t_sne.py::test_uniform_grid[barnes_hut] - Assertio...
= 1 failed, 13718 passed, 398 skipped, 6 xfailed, 1674 warnings in 382.15s (0:06:22) =
```

Here are the logs, the failure is the same across cpython 3.6-3.8:
* [python 3.6](https://travis-ci.com/github/conda-forge/scikit-learn-feedstock/jobs/319979948)
* [python 3.7](https://travis-ci.com/github/conda-forge/scikit-learn-feedstock/jobs/319979949)
* [python 3.8](https://travis-ci.com/github/conda-forge/scikit-learn-feedstock/jobs/319979950)

Interestingly, [PyPy](https://travis-ci.com/github/conda-forge/scikit-learn-feedstock/jobs/319979951) doesn't have that failure (but has some segfaults instead..., see #16956).

A more detailed trace is below the fold. From what I can see from the log, the failure seems to be a question of ""only"" the quality of an embedding, and not a hard failure per se.

<details>

```
=================================== FAILURES ===================================
________________________ test_uniform_grid[barnes_hut] _________________________
[gw2] linux -- Python 3.6.10 $PREFIX/bin/python

method = 'barnes_hut'

    @pytest.mark.parametrize('method', ['barnes_hut', 'exact'])
    def test_uniform_grid(method):
        """"""Make sure that TSNE can approximately recover a uniform 2D grid
    
        Due to ties in distances between point in X_2d_grid, this test is platform
        dependent for ``method='barnes_hut'`` due to numerical imprecision.
    
        Also, t-SNE is not assured to converge to the right solution because bad
        initialization can lead to convergence to bad local minimum (the
        optimization problem is non-convex). To avoid breaking the test too often,
        we re-run t-SNE from the final point when the convergence is not good
        enough.
        """"""
        seeds = [0, 1, 2]
        n_iter = 500
        for seed in seeds:
            tsne = TSNE(n_components=2, init='random', random_state=seed,
                        perplexity=20, n_iter=n_iter, method=method)
            Y = tsne.fit_transform(X_2d_grid)
    
            try_name = ""{}_{}"".format(method, seed)
            try:
>               assert_uniform_grid(Y, try_name)

../_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.6/site-packages/sklearn/manifold/tests/test_t_sne.py:784: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

Y = array([[ 52.326397  , -15.92225   ],
       [ 46.679527  , -20.175953  ],
       [ 40.870537  , -24.181147  ],
       ...[-35.291374  ,  22.122814  ],
       [-42.2738    ,  18.793724  ],
       [-48.922283  ,  15.606232  ]], dtype=float32)
try_name = 'barnes_hut_1'

    def assert_uniform_grid(Y, try_name=None):
        # Ensure that the resulting embedding leads to approximately
        # uniformly spaced points: the distance to the closest neighbors
        # should be non-zero and approximately constant.
        nn = NearestNeighbors(n_neighbors=1).fit(Y)
        dist_to_nn = nn.kneighbors(return_distance=True)[0].ravel()
        assert dist_to_nn.min() > 0.1
    
        smallest_to_mean = dist_to_nn.min() / np.mean(dist_to_nn)
        largest_to_mean = dist_to_nn.max() / np.mean(dist_to_nn)
    
        assert smallest_to_mean > .5, try_name
>       assert largest_to_mean < 2, try_name
E       AssertionError: barnes_hut_1
E       assert 6.67359409617653 < 2

../_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.6/site-packages/sklearn/manifold/tests/test_t_sne.py:807: AssertionError

During handling of the above exception, another exception occurred:

method = 'barnes_hut'

    @pytest.mark.parametrize('method', ['barnes_hut', 'exact'])
    def test_uniform_grid(method):
        """"""Make sure that TSNE can approximately recover a uniform 2D grid
    
        Due to ties in distances between point in X_2d_grid, this test is platform
        dependent for ``method='barnes_hut'`` due to numerical imprecision.
    
        Also, t-SNE is not assured to converge to the right solution because bad
        initialization can lead to convergence to bad local minimum (the
        optimization problem is non-convex). To avoid breaking the test too often,
        we re-run t-SNE from the final point when the convergence is not good
        enough.
        """"""
        seeds = [0, 1, 2]
        n_iter = 500
        for seed in seeds:
            tsne = TSNE(n_components=2, init='random', random_state=seed,
                        perplexity=20, n_iter=n_iter, method=method)
            Y = tsne.fit_transform(X_2d_grid)
    
            try_name = ""{}_{}"".format(method, seed)
            try:
                assert_uniform_grid(Y, try_name)
            except AssertionError:
                # If the test fails a first time, re-run with init=Y to see if
                # this was caused by a bad initialization. Note that this will
                # also run an early_exaggeration step.
                try_name += "":rerun""
                tsne.init = Y
                Y = tsne.fit_transform(X_2d_grid)
>               assert_uniform_grid(Y, try_name)

../_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.6/site-packages/sklearn/manifold/tests/test_t_sne.py:792: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

Y = array([[-18.169476  ,   6.0802336 ],
       [-18.278513  ,   2.8822129 ],
       [-18.671782  ,  -0.4646889 ],
       ...[ 22.550077  ,  19.698557  ],
       [ 21.399723  ,  22.933178  ],
       [ 16.22136   ,  28.22955   ]], dtype=float32)
try_name = 'barnes_hut_1:rerun'

    def assert_uniform_grid(Y, try_name=None):
        # Ensure that the resulting embedding leads to approximately
        # uniformly spaced points: the distance to the closest neighbors
        # should be non-zero and approximately constant.
        nn = NearestNeighbors(n_neighbors=1).fit(Y)
        dist_to_nn = nn.kneighbors(return_distance=True)[0].ravel()
        assert dist_to_nn.min() > 0.1
    
        smallest_to_mean = dist_to_nn.min() / np.mean(dist_to_nn)
        largest_to_mean = dist_to_nn.max() / np.mean(dist_to_nn)
    
        assert smallest_to_mean > .5, try_name
>       assert largest_to_mean < 2, try_name
E       AssertionError: barnes_hut_1:rerun
E       assert 2.145051767903112 < 2

../_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_pla/lib/python3.6/site-packages/sklearn/manifold/tests/test_t_sne.py:807: AssertionError
=============================== warnings summary ===============================
[...]
```

</details>
","['Bug', 'help wanted', 'module:test-suite']","2020-04-18T09:53:47Z","1","1","https://github.com/scikit-learn/scikit-learn/issues/16955","Logical Bug"
"191","scikit-learn/scikit-learn","In multifold.MDS  stress value doesnt correspond to returned coordinates","<p><strong>Description</strong></p>
<p>&nbsp;</p>
<p>In multifold.MDS&nbsp; stress value doesn&rsquo;t correspond to returned coordinates.</p>
<p>&nbsp;</p>
<p><strong>How to reproduce:</strong></p>
<p>&nbsp;</p>
<p>Apply MDS to four points in 2d-plane:</p>
<p>&nbsp;</p>
<p>X &nbsp;&nbsp;Y</p>
<p>1 &nbsp;&nbsp;5</p>
<p>1 &nbsp;&nbsp;4</p>
<p>1&nbsp; &nbsp;1</p>
<p>3&nbsp; &nbsp;3</p>
<p>&nbsp;</p>
<p>If distances are Euclidean, then disparities matrix will be:</p>
<p>&nbsp;</p>
<p>disparities =</p>
<p>&nbsp;[ 0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;4.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2.82842712&nbsp; ]</p>
<p>&nbsp;[ 1.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2.23606798&nbsp; ]</p>
<p>&nbsp;[ 4.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2.82842712&nbsp; ]</p>
<p>&nbsp;[ 2.82842712&nbsp; &nbsp; &nbsp; 2.23606798&nbsp; &nbsp; &nbsp;2.82842712&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ]</p>
<p>&nbsp;</p>
<p>Perform multidimensional scaling:</p>
<p>&nbsp;</p>
<p>mds = manifold.MDS(n_components=2, dissimilarity=""precomputed"", random_state=42,  metric = True)</p>
<p>results = mds.fit(disparities)</p>
<p>coords = results.embedding_</p>
<p>&nbsp;</p>
<p>cords =</p>
<p>&nbsp;[ -0.32503572&nbsp; &nbsp; &nbsp; 1.78399044 ]</p>
<p>&nbsp;[&nbsp; 0.09843686&nbsp; &nbsp; &nbsp; 0.87890749 ]</p>
<p>&nbsp;[&nbsp; 1.47842546&nbsp; &nbsp; &nbsp;-1.76761757 ]</p>
<p>&nbsp;[ -1.2518266&nbsp; &nbsp; &nbsp; -0.89528037 ]</p>
<p>&nbsp;</p>
<p>stress = results.stress_</p>
<p>0.0045113518633979315</p>
<p>&nbsp;</p>
<p>Now calculate stress by hand.</p>
<p>&nbsp;</p>
<p>First calculate Euclidian distances which correspond coordinates returned:</p>
<p>&nbsp;</p>
<p>dis = euclidean_distances(coords)</p>
<p>&nbsp;[ 0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.9992518&nbsp; &nbsp; &nbsp; &nbsp;3.98326395&nbsp; &nbsp; &nbsp; 2.83503675&nbsp; ]</p>
<p>&nbsp;[ 0.9992518&nbsp; &nbsp;&nbsp;&nbsp;0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2.98470492&nbsp; &nbsp; &nbsp; 2.22956362&nbsp; ]</p>
<p>&nbsp;[ 3.98326395&nbsp; &nbsp;2.98470492&nbsp; &nbsp; &nbsp;0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2.86622548&nbsp; ]</p>
<p>&nbsp;[ 2.83503675&nbsp; &nbsp;2.22956362&nbsp; &nbsp; &nbsp;2.86622548&nbsp; &nbsp; &nbsp; 0.&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ]</p>
<p>&nbsp;</p>
<p>And now we can calculate stress value:</p>
<p>&nbsp;</p>
<p>stress =</p>
<p>((dis.ravel() - disparities.ravel()) ** 2).sum() / 2</p>
<p>0.0020293041020245147</p>
<p>&nbsp;</p>
<p>So, the real stress calculated using returned coordinates doesn&rsquo;t correspond to the stress value</p>
<p>results.stress_ = 0.0045113518633979315</p>
<p>&nbsp;</p>
<p>After some debugging it is clear that MDS returns coordinates for the current iteration and stress for the previous iteration.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><strong>Here is the script to reproduce:</strong></p>
<p>&nbsp;</p>
<p>from sklearn import manifold</p>
<p>from sklearn.metrics.pairwise import euclidean_distances</p>
<p>import pandas as pd</p>
<p>&nbsp;</p>
<p>data = [['A',1,5],['B',1,4],['C',1,1],['D',3,3]]</p>
<p>df = pd.DataFrame(data, columns = ['Name', 'X','Y'])</p>
<p>&nbsp;</p>
<p>#Distance matrix</p>
<p>disparities = euclidean_distances(df[['X','Y']])</p>
<p>mds = manifold.MDS(n_components=2, dissimilarity=""precomputed"", random_state=42,&nbsp; metric = True)</p>
<p>results = mds.fit(disparities)</p>
<p>coords = results.embedding_</p>
<p>stress = results.stress_</p>
<p>print ('returned stress=',stress)</p>
<p>&nbsp;</p>
<p># Calculate Stress by hand</p>
<p>dis = euclidean_distances(coords)</p>
<p>real_stress = ((dis.ravel() - disparities.ravel()) ** 2).sum() / 2</p>
<p>print('real stress =',real_stress)</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><strong>Versions:</strong></p>
<p>System:</p>
<p>&nbsp;&nbsp;&nbsp; python: 3.7.6 (default, Dec 30 2019, 19:38:36)&nbsp; [Clang 10.0.0 (clang-1000.11.45.5)]</p>
<p>executable: /usr/local/opt/python/bin/python3.7</p>
<p>&nbsp;&nbsp; machine: Darwin-17.7.0-x86_64-i386-64bit</p>
<p>&nbsp;</p>
<p>Python deps:</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; pip: 19.3.1</p>
<p>setuptools: 42.0.2</p>
<p>&nbsp;&nbsp; sklearn: 0.21.3</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; numpy: 1.17.4</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; scipy: 1.3.2</p>
<p>&nbsp;&nbsp;&nbsp; Cython: 0.29.15</p>
<p>&nbsp;&nbsp;&nbsp; pandas: 1.0.1</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>","['Bug', 'help wanted', 'module:manifold']","2020-04-05T11:07:53Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/16846","Logical Bug"
"192","scikit-learn/scikit-learn","MultiOutputClassifier missleading score","https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/multioutput.py#L421

Well, just to let you know that when using a MultiOutputClassifier I was finding the score results to be surprisingly low. After digging a bit, I found that the score of a MultiOutputClassifier was being calculated as the mean of y being equal to y_pred for all targets: 
`np.mean(np.all(y == y_pred, axis=1))` 

For me, that was not, at all, the expected behaviour when you state that score ""_Returns the mean accuracy on the given test data and labels_""! Indeed, I was expecting the returned accuracy to consider all hits. Something like: 
`np.mean(np.mean(y == y_pred, axis=0))`

A simple example:
```
y = [['low', 'medium', 'medium'], ['medium', 'medium', 'medium'], ['medium', 'high', 'high']]
y_pred = [['medium', 'medium', 'medium'], ['medium', 'high', 'medium'], ['low', 'high', 'high']]
```
Would return an accuracy of 0 instead of 0.667!
Why not give the user the chance to choose?

Anyway, if you find the current behaviour to be the expected one, feel free to close this issue.","['Bug']","2020-04-03T00:40:49Z","3","1","https://github.com/scikit-learn/scikit-learn/issues/16832","Logical Bug"
"193","scikit-learn/scikit-learn","Pipeline requires both fit and transform method to be available instead of only fit_transform","
#### Describe the bug
Calling a pipeline with a nonparametric function causes an error since the function `transform()` is missing.  The pipeline itself calls the function `fit_transform()` if it's present.  For nonparametric functions (the most prominent being t-SNE) a regular `transform()` method does not exist since there is no projection or mapping that is learned.  It could still be used for dimensionality reduction.

#### Steps/Code to Reproduce
Example:
```python
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.pipeline import make_pipeline

make_pipeline(TSNE(), PCA())
```


#### Expected Results
A pipeline is created.

#### Actual Results
Output:
```
TypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'TSNE(angle=0.5,...
```
#### Possible Solution
Editing this https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/pipeline.py#L179
to the following in order to reflect the change:
```python
if (not hasattr(t, ""fit_transform"")) or not (hasattr(t, ""fit"") and hasattr(t, ""transform"")):
```

#### Versions
```
import sklearn; sklearn.show_versions()
System:
    python: 3.8.2 (default, Feb 26 2020, 22:21:03)  [GCC 9.2.1 20200130]
executable: /usr/bin/python3
   machine: Linux-5.5.9-arch1-2-x86_64-with-glibc2.2.5

Python dependencies:
       pip: 20.0.2
setuptools: 46.0.0
   sklearn: 0.22.2.post1
     numpy: 1.18.1
     scipy: 1.4.1
    Cython: 0.29.15
    pandas: 1.0.1
matplotlib: 3.2.0
    joblib: 0.14.1

Built with OpenMP: True
```

<!-- Thanks for contributing! -->
","['Bug', 'Low Priority', 'module:pipeline']","2020-03-16T22:40:24Z","11","0","https://github.com/scikit-learn/scikit-learn/issues/16710","Logical Bug"
"194","scikit-learn/scikit-learn","SVC with ADABoosting","

I am trying to use SVC with ADAboosting. WIth SVC, but not other base estimators, the initial estimator does not seemed to be trained.

Comparing the initial estimator of the ensemble to a single estimator with the same hyper-parameters 

create an ADA ensemble

<img width=""986"" alt=""Screen Shot 2020-03-04 at 9 15 10 PM"" src=""https://user-images.githubusercontent.com/19198429/75944794-39ed2100-5e5e-11ea-9125-b06cfaf729d5.png"">

Create a single SVC

<img width=""553"" alt=""Screen Shot 2020-03-04 at 9 15 20 PM"" src=""https://user-images.githubusercontent.com/19198429/75944859-58ebb300-5e5e-11ea-9f3c-65c098b4a9f4.png"">

Compare the accuracies

<img width=""415"" alt=""Screen Shot 2020-03-04 at 9 15 49 PM"" src=""https://user-images.githubusercontent.com/19198429/75944878-630db180-5e5e-11ea-9760-a38601e0d873.png"">


I have tried different hyper-parameters and had the same issue. This issue does happen for RandomForests and DecisionTree Classifiers


","['Bug', 'module:ensemble']","2020-03-05T03:24:27Z","8","0","https://github.com/scikit-learn/scikit-learn/issues/16642","Logical Bug"
"195","scikit-learn/scikit-learn","OneHotEncoder.get_feature_names doesn't work with integer column names","```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
X = pd.DataFrame({1: np.random.randint(0, 10, size=400)})
OneHotEncoder().fit(X).get_feature_names([1])
```
```pythontb
TypeError: unsupported operand type(s) for +: 'int' and 'str'
```","['Bug', 'Needs Decision', 'module:preprocessing']","2020-02-28T21:04:42Z","29","0","https://github.com/scikit-learn/scikit-learn/issues/16593","Runtime Error"
"196","scikit-learn/scikit-learn","categoricalNB fails with sparse matrix","According to the docs CategoricalNB requires X to be ""array-like, sparse matrix}, shape = [n_samples, n_features]"". However if I try to fit with a sparse matrix I get an error saying it has to be dense. It works fine passing a sparse matrix to fit a MultinomialNB model:

TypeError                                 Traceback (most recent call last)
<ipython-input-19-afb2039f4bc4> in <module>
      1 from sklearn.naive_bayes import CategoricalNB
      2 m = CategoricalNB()
----> 3 m.fit(x, y)
      4 preds = m.predict(x)
      5 m.score(x, y)

~\Anaconda3\lib\site-packages\sklearn\naive_bayes.py in fit(self, X, y, sample_weight)
   1101         self : object
   1102         """"""
-> 1103         return super().fit(X, y, sample_weight=sample_weight)
   1104 
   1105     def partial_fit(self, X, y, classes=None, sample_weight=None):

~\Anaconda3\lib\site-packages\sklearn\naive_bayes.py in fit(self, X, y, sample_weight)
    607         self : object
    608         """"""
--> 609         X, y = self._check_X_y(X, y)
    610         _, n_features = X.shape
    611         self.n_features_ = n_features

~\Anaconda3\lib\site-packages\sklearn\naive_bayes.py in _check_X_y(self, X, y)
   1161         # X, y = check_array(X, y, dtype='int', accept_sparse=False,
   1162         #                    force_all_finite=True)
-> 1163         X, y = check_X_y(X, y, accept_sparse=False, force_all_finite=True)
   1164         X, y = check_X_y(X, y, dtype='int')
   1165         if np.any(X < 0):

~\Anaconda3\lib\site-packages\sklearn\utils\validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
    753                     ensure_min_features=ensure_min_features,
    754                     warn_on_dtype=warn_on_dtype,
--> 755                     estimator=estimator)
    756     if multi_output:
    757         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,

~\Anaconda3\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
    509                                       dtype=dtype, copy=copy,
    510                                       force_all_finite=force_all_finite,
--> 511                                       accept_large_sparse=accept_large_sparse)
    512     else:
    513         # If np.array(..) gives ComplexWarning, then we convert the warning

~\Anaconda3\lib\site-packages\sklearn\utils\validation.py in _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)
    304 
    305     if accept_sparse is False:
--> 306         raise TypeError('A sparse matrix was passed, but dense '
    307                         'data is required. Use X.toarray() to '
    308                         'convert to a dense numpy array.')

TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.","['Bug', 'module:naive_bayes']","2020-02-27T11:38:27Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/16561","Runtime Error"
"197","scikit-learn/scikit-learn","LinearSVC multiclass is suboptimal compared to voting","<!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Description
LinearSVC with multiclass data seems to do worse than creating multiple binary classifiers and voting using `decision_function()`. Since the `multiclass` default is `ovr`, this is probably not expected behavior.


#### Steps/Code to Reproduce
An [MWE](https://gist.github.com/abhishek-ghose/ff52ac298c82b70c6562bf7a29f3ef76) that demonstrates custom fitting of multiple `LinearSVC`and voting. This uses a toy dataset, where the data for a class is a vertical stripe. The code snippet generates the dataset.


The code tries the `LinearSVC()` as-is and compares it to a *custom* implementation - shown by functions `custom_ovr_fit()` and `custom_ovr_predict()`. 

#### Expected Results
There should be minor differences between the results of the two approaches.

#### Actual Results

I see something like the following on my machine, but results might vary across machines (I've tried, this is possibly because of different versions) - however the difference in the two approaches is always substantial (its `76.31%` below, but on other machines the gap closes to `~50%`.):

```
========Comparison summary========
	For c=0.25000, scikit F1=0.47, custom F1=0.83, pct. improve. over scikit = 76.94%
	For c=0.50000, scikit F1=0.48, custom F1=0.84, pct. improve. over scikit = 76.07%
	For c=1.00000, scikit F1=0.49, custom F1=0.87, pct. improve. over scikit = 76.31%
	For c=2.00000, scikit F1=0.49, custom F1=0.72, pct. improve. over scikit = 46.08%
	For c=4.00000, scikit F1=0.41, custom F1=0.53, pct. improve. over scikit = 29.29%

	**scikit: best param=1.00000, score=0.49
	**custom: best param=1.00000, score=0.87
	**pct. improve. over scikit's best score = 76.31%
```

I am using the `F1-macro` score. Not only are the values for a given `C` often not close, but the best score, across the parameter search space - represented by `pct. improve. over scikit's best score` - is drastically different.

As one might expect, this is dataset dependent. For ex on the `digits` dataset I did not see much of difference, but on the `wine` dataset, I see a gap of `~11%`. 


#### Versions
I don't have `imblearn` installed (please let me know if this is necessary) - here's the rest of the info.
```
Windows-10-10.0.17763-SP0
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]
NumPy 1.17.1
SciPy 1.1.0
Scikit-Learn 0.19.1
```","['Bug', 'module:svm']","2020-02-24T22:46:20Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/16538","Logical Bug"
"198","scikit-learn/scikit-learn","Decision Tree probabilities with balanced class weight","<!--
Before submitting a bug, please make sure the issue hasn't been already
addressed by searching through the past issues.
-->

#### Describe the bug
I'm not sure if this is a bug or expected behaviour, but it's something that tripped me up and I thought I'd ask about it. The issue comes from fitting a decision tree model with `class_weight = 'balanced'`.

#### Steps/Code to Reproduce

```
import pandas as pd
import numpy as np
from sklearn import tree
from sklearn.datasets import load_breast_cancer

# Load data
data = load_breast_cancer()
X = data.data
y = data.target

# Build Decision Tree
dt = tree.DecisionTreeClassifier(
    max_depth = 1,
    class_weight = 'balanced',
    random_state = 0
).fit(
    X = X,
    y = y
)

# Output decision tree nodes and probabilities on data
node = dt.apply(X)
output_prob = dt.predict_proba(X)[:, 1]
results = pd.DataFrame({'y': y, 'node': node, 'output_prob': output_prob})

# Compare output node probabilities with calculated probabilities
def f(x):
    d = {}
    d['output_prob'] = x['output_prob'].max()
    d['calc_prob'] = sum(x['y']) / x['y'].count()
    return pd.Series(d)

comparison = results.groupby('node').apply(f)
```

#### Expected Results
I would expect that the probabilities output from `predict_proba` to match the calculated probabilities. 

#### Actual Results
The output from `predict_proba` does not match the calculated probabilities. I suspect what's happening is that the probabilities being output are calculated from the input data _after_ it's been balanced, and not from the input data itself. 

Even though the classes have been balanced during the building of the model, I wouldn't expect these balanced weightings to be used in the probabilities being generated, although maybe that's a problem with my expectations . This showed up when I used the probabilities to calculate the expected versus actual target rate.

If this _is_ expected behaviour, then maybe it would be useful to have a parameter in the `predict_proba` function to allow someone to choose to output the probabilities either pre or post balancing?

#### Versions
System:
    python: 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]
executable: /home/ec2-user/anaconda3/envs/test/bin/python
   machine: Linux-4.14.165-102.185.amzn1.x86_64-x86_64-with-glibc2.10

Python deps:
       pip: 19.3.1
setuptools: 42.0.2.post20191201
   sklearn: 0.21.3
     numpy: 1.17.3
     scipy: 1.3.2
    Cython: 0.29.14
    pandas: 0.25.3","['Bug', 'module:tree']","2020-02-19T10:38:15Z","4","1","https://github.com/scikit-learn/scikit-learn/issues/16479","Logical Bug"
"199","scikit-learn/scikit-learn","Support of passthrough=True in Stacking when X a dataframe of mixed type","In `StackingClassifier` and `StackingRegressor`, we added an option `passthrough=True/False` which will concatenate the internal predictions of the first layer model with the original dataset. While everything is going if `X` is only numerical, things start to be complicated when we are dealing with mixed types and dataframe.

Let's illustrate the issue by tacking the Titanic dataset:

## Workflow

### Some imports

```python
import numpy as np
from pandas.api.types import CategoricalDtype

from sklearn.base import clone
from sklearn.compose import make_column_selector as selector
from sklearn.compose import make_column_transformer
from sklearn.datasets import fetch_openml
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import RidgeClassifierCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import OrdinalEncoder

np.random.seed(0)
```

### Load the dataset

```python
X, y = fetch_openml(""titanic"", version=1, as_frame=True, return_X_y=True)
subset_feature = ['embarked', 'sex', 'pclass', 'age', 'fare']
X = X[subset_feature]

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)
```

### Define first-layer learners

Since we are dealing with mixed-types, the first-layer learners need to define some preprocessor.
Here, we will stack a linear model and a tree-based model and then we defined 2 types of preprocessors.

```python
gradient_based_processor = make_column_transformer(
    (make_pipeline(StandardScaler(), SimpleImputer(strategy=""median"")),
     selector(dtype_exclude=CategoricalDtype)),
    (make_pipeline(SimpleImputer(strategy='constant', fill_value='missing'),
                   OneHotEncoder(handle_unknown='ignore')),
     selector(dtype_include=CategoricalDtype))
)

categories = [
    X[col].dtype.categories.tolist() + [""missing""] if X[col].isnull().any()
    else X[col].dtype.categories.tolist()
    for col in selector(dtype_include=CategoricalDtype)(X)
]

tree_based_processor = make_column_transformer(
    (make_pipeline(SimpleImputer(strategy='constant', fill_value='missing'),
                   OrdinalEncoder(categories=categories)),
     selector(dtype_include=CategoricalDtype))
)

rf = make_pipeline(clone(tree_based_processor), RandomForestClassifier())
lr = make_pipeline(clone(gradient_based_processor), LogisticRegression())

# sanity check
print(lr.fit(X_train, y_train).score(X_test, y_test))
print(rf.fit(X_train, y_train).score(X_test, y_test))
```
```
0.8048780487804879
0.8048780487804879
```

Up to now, we have a standard learner. Let's try to stack them

### Only stack the predictions of the first-layer models

Let's use some stacking. By default, we will stack the predictions of the first-layer models

```python
numerical_ridge = RidgeClassifierCV()
model = StackingClassifier(
    estimators=[(""lr"", lr), (""rf"", rf)],
    final_estimator=numerical_ridge,
    passthrough=False
)
print(model.fit(X_train, y_train).score(X_test, y_test))
```
```
0.8048780487804879
```

We don't have any issues since the predictions are only numerical. The issue starts if we want to concatenate the original `X`

### Concatenate `X` with the predictions of the first-layer models

If we try directly to pass through `X` we get an issue since the ridge classifier did not encode the data in `X`:

```python
model = StackingClassifier(
    estimators=[(""lr"", lr), (""rf"", rf)],
    final_estimator=numerical_ridge,
    passthrough=True
)
try:
    print(model.fit(X_train, y_train).score(X_test, y_test))
except Exception as e:
    print(e)
```
```
could not convert string to float: 'S'
```

Thus, one has to create a pipeline to deal with the data:

```python
ridge = make_pipeline(clone(gradient_based_processor), RidgeClassifierCV())
ridge.named_steps[""columntransformer""].set_params(remainder=""passthrough"")

model = StackingClassifier(
    estimators=[(""lr"", lr), (""rf"", rf)],
    final_estimator=ridge,
    passthrough=True
)
try:
    print(model.fit(X_train, y_train).score(X_test, y_test))
except Exception as e:
    print(e)
```
```
make_column_selector can only be applied to pandas dataframes
```

So here we actually have the following issue:

* the concatenation does not preserve dataframe. This could be solved by stacking all the data to `X` (which should manage without importing `pandas`). However, it means that we might want to generate some **feature names** for the predictions generated by the first-layers. So this is my first question, what shall we do?
* if the above issue can be fixed, then we start to have another issue. When creating the pipeline, the user would need to specifically state to let pass through the remainder in the final estimator. Otherwise, the prediction columns will be dropped. So, we might want to interfere with the column transformer of the last steps to be sure to pass through the predictions. But API I don't know what is the best way to do so?

ping @jnothman @qinhanmin2014 @thomasjpfan 

Sorry for the long narration.","['Bug', 'New Feature', 'API']","2020-02-18T18:46:38Z","2","7","https://github.com/scikit-learn/scikit-learn/issues/16473","Dependency Issue"
"200","scikit-learn/scikit-learn","mutual_info_regression is missing a factor?","```
from pylab import *

from sklearn.feature_selection import mutual_info_regression

mi = dict()
for n in np.logspace(2, 5, 10):
    n = int(n)
    x = np.random.randn(n)[:, None]
    mi[n] = mutual_info_regression(x, x.squeeze(), n_neighbors=5).item()
mi = pd.Series(mi)
mi.name = 'mi'
mi.index.names = ['n']
mi = mi.reset_index()

figure()
plot(mi.n, mi.mi, 'o')
xscale('log')
grid()
figure()
plot(mi.n, mi.mi - np.log(mi.n), 'o')
xscale('log')
ylim(0, ylim()[1] * 2)
grid()
```

![image](https://user-images.githubusercontent.com/223276/74650392-873c7380-5179-11ea-95d4-34d8c004bd08.png)
","['Bug', 'module:feature_selection']","2020-02-17T11:35:02Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/16457","UI/UX Bug"
"201","scikit-learn/scikit-learn","HistGradientBoostingClassifier slow in prediction mode","While HistGradientBoostingClassifier is 100 faster than GradientBoostingClassifier when fitting the model, I found it to be very slow in case of predicting the class probabilities, in my case about 100 times slower :-(

For example: 
GradientBoostingClassifier: 3.2 min for training for 1 million examples. 32 ms for 1000 predictions.
HistGradientBoostingClassifier: 7s for training. 1s for 1000 predictions","['Bug', 'Performance', 'module:ensemble']","2020-02-11T15:02:02Z","13","0","https://github.com/scikit-learn/scikit-learn/issues/16429","Performance Issue"
"202","scikit-learn/scikit-learn","Unable to Take Cartesian Products of Many Arrays","
#### Describe the bug
When trying to run cartesian from sklearn.utils.extmath on more 32 arrays, the module crashes due to np.indices functioning rejecting the input shape in the source code.

#### Steps/Code to Reproduce
cartesian(([(1, 2), (3, 4)], [(5, 6), (7, 8)], [(9, 10), (7, 11)], [(3, 12), (13, 14)], [(9, 15), (3, 16)], [(17, 18), (3, 19)], [(1, 20), (21, 22)], [(21, 23), (3, 24)], [(25, 26), (27, 28)], [(21, 29), (3, 30)], [(9, 31), (27, 32)], [(9, 33), (1, 34)], [(21, 35), (17, 36)], [(13, 37), (7, 38)], [(25, 39), (7, 40)], [(1, 41), (27, 42)], [(21, 43), (17, 44)], [(17, 45), (3, 46)], [(25, 47), (17, 48)], [(21, 49), (17, 50)], [(5, 51), (13, 52)], [(1, 53), (7, 54)], [(25, 55), (13, 56)], [(5, 57), (7, 58)], [(9, 59), (1, 60)], [(25, 61), (5, 62)], [(3, 63), (27, 64)], [(25, 65), (7, 66)], [(1, 67), (27, 68)], [(27, 69), (7, 70)], [(21, 71), (17, 72)], [(17, 73), (5, 74)], [(9, 75), (21, 76)], [(21, 77), (13, 78)], [(25, 79), (3, 80)], [(9, 81), (25, 82)], [(9, 83), (7, 84)], [(17, 85), (27, 86)], [(5, 87), (27, 88)], [(25, 89), (13, 90)]))

Example:
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
docs = [""Help I have a bug"" for i in range(1000)]
vectorizer = CountVectorizer(input=docs, analyzer='word')
lda_features = vectorizer.fit_transform(docs)
lda_model = LatentDirichletAllocation(
    n_topics=10,
    learning_method='online',
    evaluate_every=10,
    n_jobs=4,
)
model = lda_model.fit(lda_features)
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->

```
Sample code to reproduce the problem
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
The cartesian product of the lists.
#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->

Traceback (most recent call last):
  File ""/home/hlc5v/.conda/envs/graph_env/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/hlc5v/.conda/envs/graph_env/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/hlc5v/.conda/envs/graph_env/lib/python3.6/cProfile.py"", line 160, in <module>
    main()
  File ""/home/hlc5v/.conda/envs/graph_env/lib/python3.6/cProfile.py"", line 153, in main
    runctx(code, globs, None, options.outfile, options.sort)
  File ""/home/hlc5v/.conda/envs/graph_env/lib/python3.6/cProfile.py"", line 20, in runctx
    filename, sort)
  File ""/home/hlc5v/.conda/envs/graph_env/lib/python3.6/profile.py"", line 64, in runctx
    prof.runctx(statement, globals, locals)
  File ""/home/hlc5v/.conda/envs/graph_env/lib/python3.6/cProfile.py"", line 100, in runctx
    exec(cmd, globals, locals)
  File ""../../DNF_Approx/GraphConvert/graphtodnf.py"", line 191, in <module>
    main()
  File ""../../DNF_Approx/GraphConvert/graphtodnf.py"", line 176, in main
    pi, vars, weights = createPathSet(curr, infected, args.susceptible[j], t=int(args.time_steps))
  File ""../../DNF_Approx/GraphConvert/graphtodnf.py"", line 103, in createPathSet
    PI = cartesian_product(*pathSet)
  File ""../../DNF_Approx/GraphConvert/graphtodnf.py"", line 113, in cartesian_product
    arr = np.empty([len(a) for a in arrays] + [la], dtype=tuple)
ValueError: sequence too large; cannot be greater than 32

#### Versions
<!--
Please run the following snippet and paste the output below.
For scikit-learn >= 0.20:
import sklearn; sklearn.show_versions()
For scikit-learn < 0.20:
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import sklearn; print(""Scikit-Learn"", sklearn.__version__)
import imblearn; print(""Imbalanced-Learn"", imblearn.__version__)
-->
System:
    python: 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018, 14:57:15) [MSC v.1915 64 bit (AMD64)]
executable: C:\Users\hcars\PycharmProjects\DNF_git\GraphForecastApprox\venv\Scripts\python.exe
   machine: Windows-10-10.0.17763-SP0
Python dependencies:
       pip: 19.0.3
setuptools: 40.8.0
   sklearn: 0.22.1
     numpy: 1.17.2
     scipy: 1.4.1
    Cython: None
    pandas: 0.25.1
matplotlib: 3.1.1
    joblib: 0.14.1
Built with OpenMP: True


<!-- Thanks for contributing! -->
","['Bug', 'module:utils']","2020-01-31T16:15:39Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/16350","Runtime Error"
"203","scikit-learn/scikit-learn","In PassiveAggressiveClassifier, inconsistency between the parameter ""loss"" and the attribute ""loss_function_""","#### Describe the issue linked to the documentation
The two possible loss values for the PassiveAggressiveClassifier are ""hinge"" and ""squared_hinge"", but in both cases, the classifier attribute ""loss_function_"" is the hinge loss, and only the learning rate is adapted. 
``` python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

from sklearn.linear_model import PassiveAggressiveClassifier

heldout = [0.95, 0.90, 0.75, 0.50, 0.01]
rounds = 20
X, y = datasets.load_digits(return_X_y=True)

classifiers = [
    (""Passive-Aggressive I"", PassiveAggressiveClassifier(loss='hinge',
                                                         C=1.0, tol=1e-4)),
    (""Passive-Aggressive II"", PassiveAggressiveClassifier(loss='squared_hinge',
                                                          C=1.0, tol=1e-4))
]

xx = 1. - np.array(heldout)

for name, clf in classifiers:
    print(""training %s"" % name)
    rng = np.random.RandomState(42)
    yy = []
    for i in heldout:
        yy_ = []
        for r in range(rounds):
            X_train, X_test, y_train, y_test = \
                train_test_split(X, y, test_size=i, random_state=rng)
            clf.fit(X_train, y_train)
            y_pred = clf.predict(X_test)
            yy_.append(1 - np.mean(y_pred == y_test))
       print(clf.loss_function_)
```
returns
```
training Passive-Aggressive I
<sklearn.linear_model._sgd_fast.Hinge object at 0x1172efb30>
training Passive-Aggressive II
<sklearn.linear_model._sgd_fast.Hinge object at 0x1172efcd0>
```



#### Suggest a potential alternative/fix
shouldn't the signature of the classifier reflect the implemented algorithm, and allow the user to set the learning_rate parameter instead of the loss?
Indeed, the user set loss parameter is not used, and loss=""hinge"" is hardcoded instead of loss=self.loss ([here](https://github.com/scikit-learn/scikit-learn/blob/40f668c03891caaeda231279fe12cbb95b198cbe/sklearn/linear_model/_passive_aggressive.py#L251) and [here](https://github.com/scikit-learn/scikit-learn/blob/40f668c03891caaeda231279fe12cbb95b198cbe/sklearn/linear_model/_passive_aggressive.py#L223))","['Bug', 'Documentation', 'module:linear_model']","2020-01-29T10:16:48Z","1","0","https://github.com/scikit-learn/scikit-learn/issues/16270","Logical Bug"
"204","scikit-learn/scikit-learn","TSNE implementation finds variation where there is none","#### Describe the bug
Manifold.TSNE gives back a lower-dimensional representation of the input data, which contains variation, even if the input data only contains zeros (it seems to do this in all cases where all input samples are the same).

#### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.manifold import TSNE

f = TSNE(n_components=2)

zero_array = np.zeros((993, 20))
tsne_array = f.fit_transform(zero_array)

```

#### Expected Results
A warning, or all data mapped to the same point.

#### Actual Results

A lower-dimensional array (here (993, 2) shape) with different vectors!
```python
[[-0.21821421 -0.16358966]
 [-0.21803701 -0.16340736]
 [-0.21847947 -0.16356741]
 ...
 [-0.07215934  0.15475279]
 [-0.4158346   0.2995374 ]
 [-0.21808246 -0.16424587]]
```

#### Versions

Python 3.7.5
NumPy 1.17.4
SciPy 1.3.2
Scikit-Learn 0.21.3

<!-- Thanks for contributing! -->
","['Bug', 'module:manifold']","2020-01-26T12:31:23Z","6","3","https://github.com/scikit-learn/scikit-learn/issues/16238","UI/UX Bug"
"205","scikit-learn/scikit-learn","n_jobs greater than 1 causes segmentation fault","#### Describe the bug
Setting n_jobs for `sklearn.manifold.TSNE` to anything greater than 1 (including -1) causes a segmentation fault

#### Steps/Code to Reproduce
<!--
Please add a minimal example that we can reproduce the error by running the
code. Be as succinct as possible, do not depend on external data. In short, we
are going to copy-paste your code and we expect to get the same
result as you. -->

Example:
```python
import numpy as np
from sklearn.manifold import TSNE
TSNE(n_components=2,n_jobs=1).fit_transform(np.zeros((100,512)) # Runs fine
TSNE(n_components=2,n_jobs=2).fit_transform(np.zeros((100,512)) # Segmentation fault
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
Would expect it to work with any amount of `n_jobs`

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->
Segmentation fault

#### Versions
<!--
Please run the following snippet and paste the output below.
For scikit-learn >= 0.22.1:
import sklearn; sklearn.show_versions()
For scikit-learn < 0.20:
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import sklearn; print(""Scikit-Learn"", sklearn.__version__)
import imblearn; print(""Imbalanced-Learn"", imblearn.__version__)
-->
System:
    python: 3.7.3 (default, Mar 26 2019, 01:59:45)  [GCC 5.4.0 20160609]
executable: /home/<ME>/.virtualenvs/reid/bin/python3.7
   machine: Linux-4.15.0-39-generic-x86_64-with-Ubuntu-16.04-xenial
```
Python dependencies:
       pip: 20.0.1
setuptools: 41.0.1
   sklearn: 0.22.1
     numpy: 1.16.3
     scipy: 1.2.1
    Cython: 0.29.12
    pandas: 0.25.1
matplotlib: 3.0.3
    joblib: 0.13.2

Built with OpenMP: True
```
N.B. Could very well be related to #12922 but I'm not sure.

<!-- Thanks for contributing! -->
","['Bug', 'module:manifold', 'segfault']","2020-01-23T14:03:05Z","10","0","https://github.com/scikit-learn/scikit-learn/issues/16184","Runtime Error"
"206","scikit-learn/scikit-learn","PERF predict_proba is slow when n_jobs > 1 for random forests","The use of `n_jobs > 1` for small batch can slow down the prediction for forest models. This is probably due to the overhead incurred by using `joblib` (create thread, check system info, ...) which is dominate the runtime compared to the computations. This was reported originally in joblib/joblib#982.

A couple of ideas to solve this:
- Set `n_jobs=1` when the size of the batch is small.
- Introduce a `n_jobs_predict` parameters that would default to `1/n_jobs` for forests but that can be set separately.

","['Bug', 'module:ensemble']","2020-01-17T10:37:50Z","0","0","https://github.com/scikit-learn/scikit-learn/issues/16143","Performance Issue"
"207","scikit-learn/scikit-learn","Wrongly implemented test in RidgeCV","The following test does not actually test anything:

https://github.com/scikit-learn/scikit-learn/blob/bdf1ae9e4a739abf06076e67dc453774ffc886a7/sklearn/linear_model/tests/test_ridge.py#L478-L502

`gcv_ridge` will compute the error by aggregating all predictions and compute the `explained_variance` on all predictions.

However, `loo_ridge` will make a `GridSearchCV` which will compute the score for each individual sample (because of a LOO CV) and then report the mean. However, the explained variance will always be 1.0 since we are computing the variance on a single sample.

So the test is passing by chance.","['Bug', 'Moderate', 'help wanted', 'module:linear_model']","2020-01-07T17:24:00Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/16041","Logical Bug"
"208","scikit-learn/scikit-learn","TfidfVectorizer ngrams does not work when vocabulary provided","<!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description

The `TfidfVectorizer` does not honor the `ngram_range` argument when the `vocabulary` is provided.

#### Steps/Code to Reproduce

Example 1, vocabulary is *not* provided, this works as expected:

```python
from sklearn.feature_extraction.text import TfidfVectorizer

X = ['abc',
     'bcd',
     'cde']

tfidf = TfidfVectorizer(stop_words=None,
                        analyzer='char',
                        ngram_range=(2, 2))

sps = tfidf.fit_transform(X)
print(tfidf.get_feature_names())
# ['ab', 'bc', 'cd', 'de']
```

Example 2, when vocabulary is provided. This does *not* work as expected:

```python
from sklearn.feature_extraction.text import TfidfVectorizer

X = ['abc',
     'bcd',
     'cde']

tfidf = TfidfVectorizer(stop_words=None,
                        analyzer='char',
                        vocabulary={'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4},
                        ngram_range=(2, 2))

sps = tfidf.fit_transform(X)
print(tfidf.get_feature_names())
# ['a', 'b', 'c', 'd', 'e']
```

Note that it works if the vocabulary I provide are the ngrams themselves:

```python
from sklearn.feature_extraction.text import TfidfVectorizer

X = ['abc',
     'bcd',
     'cde']

tfidf = TfidfVectorizer(stop_words=None,
                        analyzer='char',
                        vocabulary=['ab', 'bc', 'cd', 'de'],
                        ngram_range=(2, 2))

sps = tfidf.fit_transform(X)
print(tfidf.get_feature_names())
# ['ab', 'bc', 'cd', 'de']
```

But that seems kind of silly, since I can't possibly know all of the ngrams a priori for a large dataset.

#### Expected Results

Expected to still get ngrams when vocabulary is provided, but did not.

#### Actual Results

See steps to reproduce above.

#### Versions
<!--
Please run the following snippet and paste the output below.
For scikit-learn >= 0.20:
import sklearn; sklearn.show_versions()
For scikit-learn < 0.20:
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import sklearn; print(""Scikit-Learn"", sklearn.__version__)
-->

```
System:
    python: 3.7.5 (default, Oct 25 2019, 10:52:18)  [Clang 4.0.1 (tags/RELEASE_401/final)]
executable: /anaconda3/envs/myenv/bin/python
   machine: Darwin-18.6.0-x86_64-i386-64bit

Python dependencies:
       pip: 19.3.1
setuptools: 42.0.2.post20191203
   sklearn: 0.22
     numpy: 1.17.4
     scipy: 1.4.0
    Cython: 0.29.14
    pandas: 0.25.3
matplotlib: 3.1.2
    joblib: 0.14.1

Built with OpenMP: True
```


<!-- Thanks for contributing! -->","['Bug', 'help wanted', 'module:feature_extraction']","2020-01-03T19:01:44Z","9","2","https://github.com/scikit-learn/scikit-learn/issues/16017","Logical Bug"
"209","scikit-learn/scikit-learn","RANSACRegressor should use weighted loss functions","The current loss functions disregard weights.

See #14325","['Bug', 'module:linear_model']","2019-12-08T21:44:39Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/15836","Logical Bug"
"210","scikit-learn/scikit-learn","test_uniform_grid[barnes_hut] can fail on aarch64","```
________________________ test_uniform_grid[barnes_hut] _________________________

method = 'barnes_hut'

    @pytest.mark.parametrize('method', ['barnes_hut', 'exact'])
    def test_uniform_grid(method):
        """"""Make sure that TSNE can approximately recover a uniform 2D grid
    
        Due to ties in distances between point in X_2d_grid, this test is platform
        dependent for ``method='barnes_hut'`` due to numerical imprecision.
    
        Also, t-SNE is not assured to converge to the right solution because bad
        initialization can lead to convergence to bad local minimum (the
        optimization problem is non-convex). To avoid breaking the test too often,
        we re-run t-SNE from the final point when the convergence is not good
        enough.
        """"""
        seeds = [0, 1, 2]
        n_iter = 500
        for seed in seeds:
            tsne = TSNE(n_components=2, init='random', random_state=seed,
                        perplexity=20, n_iter=n_iter, method=method)
            Y = tsne.fit_transform(X_2d_grid)
    
            try_name = ""{}_{}"".format(method, seed)
            try:
>               assert_uniform_grid(Y, try_name)

../_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold/lib/python3.8/site-packages/sklearn/manifold/tests/test_t_sne.py:784: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

Y = array([[ 52.326397  , -15.92225   ],
       [ 46.679527  , -20.175953  ],
       [ 40.870537  , -24.181147  ],
       ...[-35.291374  ,  22.122814  ],
       [-42.2738    ,  18.793724  ],
       [-48.922283  ,  15.606232  ]], dtype=float32)
try_name = 'barnes_hut_1'

    def assert_uniform_grid(Y, try_name=None):
        # Ensure that the resulting embedding leads to approximately
        # uniformly spaced points: the distance to the closest neighbors
        # should be non-zero and approximately constant.
        nn = NearestNeighbors(n_neighbors=1).fit(Y)
        dist_to_nn = nn.kneighbors(return_distance=True)[0].ravel()
        assert dist_to_nn.min() > 0.1
    
        smallest_to_mean = dist_to_nn.min() / np.mean(dist_to_nn)
        largest_to_mean = dist_to_nn.max() / np.mean(dist_to_nn)
    
        assert smallest_to_mean > .5, try_name
>       assert largest_to_mean < 2, try_name
E       AssertionError: barnes_hut_1
E       assert 6.67359409617653 < 2

../_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold/lib/python3.8/site-packages/sklearn/manifold/tests/test_t_sne.py:807: AssertionError

During handling of the above exception, another exception occurred:

method = 'barnes_hut'

    @pytest.mark.parametrize('method', ['barnes_hut', 'exact'])
    def test_uniform_grid(method):
        """"""Make sure that TSNE can approximately recover a uniform 2D grid
    
        Due to ties in distances between point in X_2d_grid, this test is platform
        dependent for ``method='barnes_hut'`` due to numerical imprecision.
    
        Also, t-SNE is not assured to converge to the right solution because bad
        initialization can lead to convergence to bad local minimum (the
        optimization problem is non-convex). To avoid breaking the test too often,
        we re-run t-SNE from the final point when the convergence is not good
        enough.
        """"""
        seeds = [0, 1, 2]
        n_iter = 500
        for seed in seeds:
            tsne = TSNE(n_components=2, init='random', random_state=seed,
                        perplexity=20, n_iter=n_iter, method=method)
            Y = tsne.fit_transform(X_2d_grid)
    
            try_name = ""{}_{}"".format(method, seed)
            try:
                assert_uniform_grid(Y, try_name)
            except AssertionError:
                # If the test fails a first time, re-run with init=Y to see if
                # this was caused by a bad initialization. Note that this will
                # also run an early_exaggeration step.
                try_name += "":rerun""
                tsne.init = Y
                Y = tsne.fit_transform(X_2d_grid)
>               assert_uniform_grid(Y, try_name)

../_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold/lib/python3.8/site-packages/sklearn/manifold/tests/test_t_sne.py:792: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

Y = array([[-18.169476  ,   6.0802336 ],
       [-18.278513  ,   2.8822129 ],
       [-18.671782  ,  -0.4646889 ],
       ...[ 22.550077  ,  19.698557  ],
       [ 21.399723  ,  22.933178  ],
       [ 16.22136   ,  28.22955   ]], dtype=float32)
try_name = 'barnes_hut_1:rerun'

    def assert_uniform_grid(Y, try_name=None):
        # Ensure that the resulting embedding leads to approximately
        # uniformly spaced points: the distance to the closest neighbors
        # should be non-zero and approximately constant.
        nn = NearestNeighbors(n_neighbors=1).fit(Y)
        dist_to_nn = nn.kneighbors(return_distance=True)[0].ravel()
        assert dist_to_nn.min() > 0.1
    
        smallest_to_mean = dist_to_nn.min() / np.mean(dist_to_nn)
        largest_to_mean = dist_to_nn.max() / np.mean(dist_to_nn)
    
        assert smallest_to_mean > .5, try_name
>       assert largest_to_mean < 2, try_name
E       AssertionError: barnes_hut_1:rerun
E       assert 2.145051767903112 < 2

../_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold/lib/python3.8/site-packages/sklearn/manifold/tests/test_t_sne.py:807: AssertionError
```


```

The following NEW packages will be INSTALLED:

    apipkg:                      1.5-py_0               conda-forge
    attrs:                       19.3.0-py_0            conda-forge
    binutils_impl_linux-aarch64: 2.29.1-hc862510_0      c4aarch64  
    ca-certificates:             2019.11.28-hecc5488_0  conda-forge
    certifi:                     2019.11.28-py38_0      conda-forge
    cython:                      0.29.14-py38he1b5a44_0 conda-forge
    execnet:                     1.7.1-py_0             conda-forge
    importlib_metadata:          1.2.0-py38_0           conda-forge
    joblib:                      0.14.0-py_0            conda-forge
    libblas:                     3.8.0-14_openblas      conda-forge
    libcblas:                    3.8.0-14_openblas      conda-forge
    libffi:                      3.2.1-h4c5d2ac_1006    conda-forge
    libgcc-ng:                   7.3.0-h5c90dd9_0       c4aarch64  
    libgfortran-ng:              7.3.0-h6bc79d0_0       c4aarch64  
    liblapack:                   3.8.0-14_openblas      conda-forge
    libopenblas:                 0.3.7-h5ec1e0e_4       conda-forge
    libstdcxx-ng:                7.3.0-h5c90dd9_0       c4aarch64  
    more-itertools:              8.0.2-py_0             conda-forge
    ncurses:                     6.1-hf484d3e_1002      conda-forge
    numpy:                       1.17.3-py38h91f3968_0  conda-forge
    openssl:                     1.1.1d-h516909a_0      conda-forge
    packaging:                   19.2-py_0              conda-forge
    pluggy:                      0.13.1-py38_0          conda-forge
    py:                          1.8.0-py_0             conda-forge
    pyparsing:                   2.4.5-py_0             conda-forge
    pytest:                      5.3.1-py38_0           conda-forge
    pytest-forked:               1.1.2-py_0             conda-forge
    pytest-sugar:                0.9.2-py_0             conda-forge
    pytest-timeout:              1.3.3-py_0             conda-forge
    pytest-xdist:                1.30.0-py_0            conda-forge
    python:                      3.8.0-heaf0f07_5       conda-forge
    readline:                    8.0-h75b48e3_0         conda-forge
    scikit-learn:                0.22-py38h1971d64_0    local      
    scipy:                       1.3.2-py38hb5cb654_0   conda-forge
    setuptools:                  42.0.2-py38_0          conda-forge
    six:                         1.13.0-py38_0          conda-forge
    sqlite:                      3.30.1-h283c62a_0      conda-forge
    termcolor:                   1.1.0-py_2             conda-forge
    tk:                          8.6.10-hed695b0_0      conda-forge
    wcwidth:                     0.1.7-py_1             conda-forge
    xz:                          5.2.4-hda93590_1001    conda-forge
    zipp:                        0.6.0-py_0             conda-forge
    zlib:                        1.2.11-h516909a_1006   conda-forge

```

https://cloud.drone.io/conda-forge/scikit-learn-feedstock/32/3/2

","['Bug', 'help wanted', 'arch:arm', 'module:test-suite']","2019-12-07T18:20:53Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/15821","Runtime Error"
"211","scikit-learn/scikit-learn","Silhouette score is not defined when the estimator return only 1 cluster","https://github.com/scikit-learn/scikit-learn/blob/4f97facc3a992c6e2459c3da86c9d69b0688d5ab/sklearn/metrics/cluster/_unsupervised.py#L38
I am wondering why the silhouette score is not defined when the estimator return only 1 cluster. In this case, if we want to do Hyperparameter tuning (GridSearch) based on the silhouette score, it will crash if the estimator return one cluster. In this case, I am wondering why we don't return the worst case (silhouette score = -1)? ","['Bug', 'help wanted', 'module:metrics']","2019-11-25T21:49:31Z","10","0","https://github.com/scikit-learn/scikit-learn/issues/15717","Logical Bug"
"212","scikit-learn/scikit-learn","Segmentation fault with SVC with probability = True and degree 3","I am using SVC with predict proba with below hyper-parameter on large data set. it is giving segmentation fault error. code works well for smaller data set.
I don't think its memory issue we are using 96 GB RAM. I have also monitored system using htop during run & I didn't gave impression of memory issue.

**hyper-parameter** :
SVC(kernel='poly',gamma=100,C=10,degree=3,decision_function_shape='ovo',probability=True,random_state=42,verbose=True) 
**No of Features** : 768
**No of rows** : 2.1 million","['Bug', 'module:svm']","2019-09-18T08:42:38Z","12","0","https://github.com/scikit-learn/scikit-learn/issues/15008","Performance Issue"
"213","scikit-learn/scikit-learn","Use of X.dtype when it is not float","A fairly common pattern in the scikit-learn code is to create intermediary arrays with `dtype=X.dtype`. That works well, as long as `X = check_arrays(X)` was run with a float dtype only, or in other words when `X.dtype` is not int.

When `X.dtype` is int, `check_array(X)` with default parameters will pass it though, and then all intermediary objects will be of dtype int. 

For instance,
```py
import numpy as np
from sklearn.cluster import MiniBatchKMeans

X = np.array([[1, 2], [1, 4], [1, 0],
              [4, 2], [4, 0], [4, 4],
              [4, 5], [0, 1], [2, 2],
              [3, 2], [5, 5], [1, -1]])
# manually fit on batches
kmeans = MiniBatchKMeans(n_clusters=2,
                         random_state=0,
                         batch_size=6)
kmeans.partial_fit(X[0:6,:])
```
(taken from the [MiniBatchKMeans docstring](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html)) 
will happily run in the integer space where both `sample_weight` and `cluster_centroid_` will be `int`. Discovered as part of https://github.com/scikit-learn/scikit-learn/pull/14307

Another point, that e.g. in linear models, `check_array(..., dtype=[np.float64, np.float32])` will only be run if `check_input=True` (default). Meaning that when `check_input=False` it might try to create a liner model where coefficients are an array of integers, and unless something fails due to a dtype mismatch the user will never know. I think we should always check that `X.dtype` is float, even when `check_input=False`. I believe the point of that flag was to avoid expensive checks and this doesn't cost anything.

In general any code that uses `X.dtype` to create intermediary arrays should be evaluated as to whether we are sure the dtype is float (or that ints would be acceptable).

Might be a good sprint issue, not sure.","['Bug']","2019-07-12T17:21:30Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/14314","Logical Bug"
"214","scikit-learn/scikit-learn","OneVsRestClassifier violates predict(X)==argmax(decision_function)","#### Description

When `myclassifier.decision_function` returns only `0`s for some sample, the wrapping OneVsRestClassifier will `predict` the last class instead of the first one.

The cause is in [multiclass.py](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/multiclass.py#L300..L303) where later classes rated 0 by the decision_function override earlier ones.
This violates the documented (and questioned, see #13631) requirement that `the row-wise arg-maximum is the predicted class`.

#### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.multiclass import OneVsRestClassifier
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.estimator_checks import check_classifiers_train
from sklearn.utils.validation import check_X_y
```


```python
class Dummy(BaseEstimator, ClassifierMixin):
    def fit(self, X, y):
        X, y = check_X_y(X, y, multi_output=False)
        self.classes_ = np.unique(y)
        self.n_features_ = X.shape[1]
        return self
    def predict(self, X):
        n_features = X.shape[1]
        if self.n_features_ != n_features:
            raise ValueError(""Number of features of the model must ""
                             ""match the input. Model n_features is %s and ""
                             ""input n_features is %s ""
                             % (self.n_features_, n_features))
        return np.full(len(X), self.classes_[0])
    def decision_function(self, X):
        n_features = X.shape[1]
        if self.n_features_ != n_features:
            raise ValueError(""Number of features of the model must ""
                             ""match the input. Model n_features is %s and ""
                             ""input n_features is %s ""
                             % (self.n_features_, n_features))
        return np.zeros(len(X))

class POVR(OneVsRestClassifier):
    def _more_tags(self):
        return {'poor_score': True}
```


```python
>>> x = np.array([[0], [0], [0]])
>>> y = np.arange(3)
>>> d = Dummy().fit(x,y)
>>> d.predict(x)
    array([0, 0, 0])
>>> d.decision_function(x)
    array([0., 0., 0.])
>>> od = POVR(Dummy()).fit(x,y)
>>> od.classes_
    array([0, 1, 2])
>>> od.predict(x)
    array([2, 2, 2])
>>> od.decision_function(x)
    array([[0., 0., 0.],
           [0., 0., 0.],
           [0., 0., 0.]])
>>> np.argmax(od.decision_function(x), axis=1)
    array([0, 0, 0])  # != od.predict(x)

>>> check_classifiers_train('t', POVR(Dummy()))
    ---------------------------------------------------------------------------

    AssertionError                            Traceback (most recent call last)

    <ipython-input-57-e45e548f419d> in <module>
    ----> 1 check_classifiers_train('t', POVR(Dummy()))
    

    /usr/lib/python3.7/site-packages/sklearn/utils/testing.py in wrapper(*args, **kwargs)
        353             with warnings.catch_warnings():
        354                 warnings.simplefilter(""ignore"", self.category)
    --> 355                 return fn(*args, **kwargs)
        356 
        357         return wrapper


    /usr/lib/python3.7/site-packages/sklearn/utils/estimator_checks.py in check_classifiers_train(name, classifier_orig, readonly_memmap)
       1493                 else:
       1494                     assert_equal(decision.shape, (n_samples, n_classes))
    -> 1495                     assert_array_equal(np.argmax(decision, axis=1), y_pred)
       1496 
       1497                 # raises error on malformed input for decision_function


    /usr/lib/python3.7/site-packages/numpy/testing/_private/utils.py in assert_array_equal(x, y, err_msg, verbose)
        902     __tracebackhide__ = True  # Hide traceback for py.test
        903     assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
    --> 904                          verbose=verbose, header='Arrays are not equal')
        905 
        906 


    /usr/lib/python3.7/site-packages/numpy/testing/_private/utils.py in assert_array_compare(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)
        825                                 verbose=verbose, header=header,
        826                                 names=('x', 'y'), precision=precision)
    --> 827             raise AssertionError(msg)
        828     except ValueError:
        829         import traceback


    AssertionError: 
    Arrays are not equal
    
    Mismatch: 100%
    Max absolute difference: 2
    Max relative difference: 1.
     x: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
     y: array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...
```

#### Versions
~~~py
/usr/lib/python3.7/site-packages/numpy/distutils/system_info.py:639: UserWarning: 
    Atlas (http://math-atlas.sourceforge.net/) libraries not found.
    Directories to search for the libraries can be specified in the
    numpy/distutils/site.cfg file (section [atlas]) or by setting
    the ATLAS environment variable.
  self.calc_info()

System:
    python: 3.7.3 (default, Mar 26 2019, 21:43:19)  [GCC 8.2.1 20181127]
executable: /usr/bin/python3
   machine: Linux-5.1.11-arch1-1-ARCH-x86_64-with-arch

BLAS:
    macros: NO_ATLAS_INFO=1, HAVE_CBLAS=None
  lib_dirs: /usr/lib64
cblas_libs: cblas

Python deps:
       pip: 19.0.3
setuptools: 41.0.1
   sklearn: 0.21.2
     numpy: 1.16.4
     scipy: 1.3.0
    Cython: 0.29.10
    pandas: 0.24.2
~~~","['Bug', 'help wanted', 'module:multiclass']","2019-06-19T10:49:30Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/14124","Logical Bug"
"215","scikit-learn/scikit-learn","data leak in GBDT due to warm start","(This is about the non-histogram-based version of GBDTs)

X is split into train and validation data with `train_test_split(random_state=self.random_state)`.

As @johannfaouzi noted, in a warm starting context, this will produce a leak if If `self.random_state` is a `RandomState` instance: some samples that were used for training in a previous `fit` might be used for validation now.

~~I think the right fix would be to raise a `ValueError` if the provided random state isn't a number and early-stopping is activated~~","['Bug', 'module:ensemble']","2019-06-06T16:09:45Z","11","0","https://github.com/scikit-learn/scikit-learn/issues/14034","Security Vulnerability"
"216","scikit-learn/scikit-learn","Strange behavior of label_binarize when there's only one class in y","When there's only one class in y, label_binarize produces unexpected result
```
from sklearn.preprocessing import label_binarize
label_binarize([1, 1, 1], classes=[1])
# actual result [[0], [0], [0]]
# expected result [[1], [1], [1]]
label_binarize([1, 1, 1], classes=[0])
# actual result [[0], [0], [0]]
# expected result [[0], [0], [0]]
```
version: 0.20.3 and 0.21 dev","['Bug', 'module:preprocessing']","2019-04-19T03:27:53Z","6","2","https://github.com/scikit-learn/scikit-learn/issues/13674","UI/UX Bug"
"217","scikit-learn/scikit-learn","MLPRegressor, warm_start not working as expected","I think I found a bug when it comes to the MLPRegressor (in v.0.20.3), especially with ""warm_start"" set to true. The problem occured to me, as well as to other people I know. 

We have a dataset with some training data, and try to train the NN, with MLPRegressor, of 5000 iterations, 50 hidden neurons in one layer and the fixed random seed 0. 
```

nn = MLPRegressor(activation='logistic', solver='lbfgs', max_iter=5000, hidden_layer_sizes=(50,), alpha=0, random_state=0)
nn.fit(x_train, y_train)
```

This works as expected, however if we create an MLPRegressor with 1 iteration, but we are calling ""fit"" in a loop over a range of 5000, the function we have learned looks totally wrong, since it is totally inaccurate with respect to the trainingsset.

```
nn = MLPRegressor(activation='logistic', solver='lbfgs', max_iter=1,hidden_layer_sizes=(50,), alpha=0, random_state=0, warm_start=True)
for i in range(0,5000):
  nn.fit(x_train,y_train) 
```

We tried calculating the mean squared error durring each iteration of the second code snippet, and we calculated the MSE after the first code snippet. None of the MSEs, calculated during the iterations, was even close to the MSE we calculated after the first code snippet. 

Since on the scikit-learn website it is stated, that warm start can be used for monitoring, I am pretty sure that this is a bug, since the end results are definitely not the same. ","['Bug', 'module:neural_network']","2019-04-16T11:06:25Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/13656","UI/UX Bug"
"218","scikit-learn/scikit-learn","BaggingClassifier uses Class Label as Index to Array when Voting","<!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->
BaggingClassifier uses Class Label as Index to Array when Voting

#### Steps/Code to Reproduce

Provide a base estimator to _BaggingClassifier_ that does not define the function _predict_proba_. This results in _BaggingClassifier_ resorting to voting. It appears the code for performing voting uses class labels as array indices instead of looking up the index of the class label in the _classes__ member.

Example:
```python
import numpy as np
from sklearn.ensemble import BaggingClassifier

class Foo:
    
    def __init__(self):
        pass
    
    def fit(self, X, Y, W=None):
        return self
    
    def predict(self, X):
        return np.full(X.shape[0], True, np.bool)
    
    def score(self, X, Y):
        YH = self.predict(X)
        return (Y == YH).mean()
    
    def get_params(self, deep=True):
        return {}
    
    def set_params(self, **params):
        for k, v in params:
            setattr(self, k, v)
        return self
    
# %%
A = np.random.rand(10, 4)
Y = np.random.randint(2, size=10, dtype=np.bool)
bc = BaggingClassifier(Foo())
bc.fit(A, Y)
YH = bc.predict(A)
print('BaggingClassifier Voting Result: ')
print(YH)
print('Ensemble Member Predictions: ')
for Ei in bc.estimators_:
    print(Ei.predict(A))
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
In the above code snippet, _BaggingClassifier_ should return an array of _True_ since it is the majority prediction of all ensemble members.

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->
_BaggingClassifier_ returns an array of False. This issue only occurs when the base estimator does not define the function _predict_proba_.

The issue appears to be due to lines 137 and 140 in ensemble/bagging.py.

https://github.com/scikit-learn/scikit-learn/blob/e14ac6d36d9dd069cc7fc2e51e4973514a003591/sklearn/ensemble/bagging.py#L137

https://github.com/scikit-learn/scikit-learn/blob/e14ac6d36d9dd069cc7fc2e51e4973514a003591/sklearn/ensemble/bagging.py#L140

The predictions of the ensemble members are directly used as indices into the original array. I'm guessing the prediction labels need to be converted into class labels using _estimator.classes__.

#### Versions
System:
    python: 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]
executable: C:\Users\XXXXXX\Anaconda3\pythonw.exe
   machine: Windows 2012 ServerR2

BLAS:
    macros: 
  lib_dirs: 
cblas_libs: cblas

Python deps:
       pip: 18.1
setuptools: 40.6.3
   sklearn: 0.20.1
     numpy: 1.15.4
     scipy: 1.1.0
    Cython: 0.29.2
    pandas: 0.23.4

<!-- Thanks for contributing! -->
","['Bug', 'module:ensemble']","2019-04-06T04:14:22Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/13587","Logical Bug"
"219","scikit-learn/scikit-learn","cross_validate hang randomly when training svc with polynomial kernel.","#### Description
`cross_validate` hang randomly when training svc with polynomial kernel.

#### Steps/Code to Reproduce
```python
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_validate

iris = datasets.load_iris()
x = iris.data
y = iris.target
X_choose, x_test, y_choose, y_test = train_test_split(x, y, test_size=0.3)

params = [[0.6652997139930452, 'poly', 7, 4178.386000737241],
          [1.2346990434544882, 'poly', 7, 4317.581190465473],
          [0.8156943235551155, 'poly', 8, 864.1583649816441]]

for c, kernel, degree, gamma in params:
    clf = SVC(C=c, kernel=kernel, degree=degree, gamma=gamma)
    cv_results = cross_validate(clf, X_choose, y_choose, cv=5,
                                return_train_score=False)
    print(cv_results['test_score'].mean())
```

#### Expected Results
Three scores should be shown.

#### Actual Results
Sometimes hang.
I waited for few minutes.
Also I checked running code line doesn't change using gdb.
Backtrace is [here](https://gist.github.com/sam-yusuke/7099829b1797d6867eb928264597979d)
#### Versions
```
>>> import sklearn; sklearn.show_versions()
System:
    python: 3.7.3 (default, Mar 27 2019, 22:11:17)  [GCC 7.3.0]
executable: /home/yusuke/miniconda3/envs/py37_automl_examples2/bin/python
   machine: Linux-4.20.0-042000-generic-x86_64-with-debian-buster-sid

BLAS:
    macros: SCIPY_MKL_H=None, HAVE_CBLAS=None
  lib_dirs: /home/yusuke/miniconda3/envs/py37_automl_examples2/lib
cblas_libs: mkl_rt, pthread

Python deps:
       pip: 19.0.3
setuptools: 40.8.0
   sklearn: 0.20.3
     numpy: 1.16.2
     scipy: 1.2.1
    Cython: None
    pandas: None
```

Thank you!","['Bug', 'help wanted', 'module:svm']","2019-04-02T02:25:23Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/13557","Performance Issue"
"220","scikit-learn/scikit-learn","API Inconsitency of predict and predict_proba in SVC","When using `SVC(probability=True)` ~~or `SVR(probability=True)`~~ the output of `predict_proba` will not necessarily be consistent with `predict`, in the sense that,
```py
np.argmax(self.predict_proba(X), axis=1) != self.predict(X)
```
this is documented [in the user guide](https://scikit-learn.org/stable/modules/svm.html#scores-and-probabilities),
> In addition, the probability estimates may be inconsistent with the scores, in the sense that the argmax of the scores may not be the argmax of the probabilities. (E.g., in binary classification, a sample may be labeled by predict as belonging to a class that has probability < according to predict_proba.) Platts method is also known to have theoretical issues.

IMO this is a violation of the API contract and should be fixed. 

This is being continuously reported as a bug e.g.  https://github.com/scikit-learn/scikit-learn/issues/4800  https://github.com/scikit-learn/scikit-learn/issues/12408 https://github.com/scikit-learn/scikit-learn/issues/12982 and a few stack overflow issues e.g. https://stackoverflow.com/a/17019830 

I encountered this in a project where detecting this discrepancy, evaluating the difference and deciding whether `predict` or `argmax(predict_proba` should be used in the end took some effort.

One pitfall is for instance to use `predict` to compute the accuracy, and then `predict_proba` for ROC AUC which can lead to somewhat problematic results if the predictions of these methods are not consistent.

Several approaches could be used to fix it,

1. Deprecate `probability=True` parameter in `SVC`, `NuSVC` estimator and suggest using `CalibratedClassifierCV(SVC(), cv=5)` instead. In my quick tests (on sparse data), the latter was actually faster and should yield comparable results that are also consistent between predict and `predict_proba`. Though more benchmarks may be needed.
   There may also be some variation in the results, as libsvm uses a generalization of Platt scaling in the multiclass case by Wu et al 2014 (cf [docs](https://scikit-learn.org/stable/modules/svm.html#scores-and-probabilities) that is not used in `CalibratedClassifierCV` as far as I understand?

One possibility could be to deprecate, but keep it to allow access to that functionality in libsvm. 

2. Compute `predict` as `argmax(predict_proba` when `probability=True`. This has the disadvantage of changing the results of predict depending on this input parameter.

3. Dig into libsvm to understand how it could be fixed there. 
","['Bug', 'Needs Decision', 'module:svm']","2019-02-21T12:29:29Z","44","5","https://github.com/scikit-learn/scikit-learn/issues/13211","UI/UX Bug"
"221","scikit-learn/scikit-learn","Incorrect calculations of homogeneity, completeness and v-measure","<!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->
Calculations of homogeneity, completeness and v-measure are now based on the original paper of Rosenberg & Hirschberg 2007. However, while I was doing research on fuzzy clustering evaluation techniques, I found the following paper of Utt et al. 2014 (http://www.lrec-conf.org/proceedings/lrec2014/pdf/829_Paper.pdf) which explained in a footnote that the original definitions of homogeneity and completeness contain typos. They claim it was confirmed by Rosenberg himself via personal communications.

Definitions used: 

- homogeneity = 1 - H(C|K) / H(C)
- completeness = 1  - H(K|C) / H(K)

Corrected definitions:

- homogeneity = 1 - H(C|K) / H(C,K)
- completeness = 1  - H(K|C) / H(K,C) 

Furthermore, since the calculations are now based on the mutual information score, this wouldn't be correct anymore. Also, the statement in the documentation about it being the same as normalized mutual information with the metric set to 'arithmetic' would be false.

#### Steps/Code to Reproduce
<!--
Example:
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

docs = [""Help I have a bug"" for i in range(1000)]

vectorizer = CountVectorizer(input=docs, analyzer='word')
lda_features = vectorizer.fit_transform(docs)

lda_model = LatentDirichletAllocation(
    n_topics=10,
    learning_method='online',
    evaluate_every=10,
    n_jobs=4,
)
model = lda_model.fit(lda_features)
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->
from sklearn.metrics import homogeneity_completeness_v_measure

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->

#### Versions
<!--
Please run the following snippet and paste the output below.
For scikit-learn >= 0.20:
import sklearn; sklearn.show_versions()
For scikit-learn < 0.20:
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import sklearn; print(""Scikit-Learn"", sklearn.__version__)
-->
System:
    python: 3.6.7 (v3.6.7:6ec5cf24b7, Oct 20 2018, 13:35:33) [MSC v.1900 64 bit (AMD64)]
executable: C:\Users\dtuser\AppData\Local\Programs\Python\Python36\python.exe
   machine: Windows-7-6.1.7601-SP1
BLAS:
    macros: 
  lib_dirs: 
cblas_libs: cblas
Python deps:
       pip: 18.1
setuptools: 40.6.3
   sklearn: 0.20.1
     numpy: 1.15.4
     scipy: 1.1.0
    Cython: None
    pandas: 0.23.4

<!-- Thanks for contributing! -->
","['Bug', 'help wanted', 'module:metrics']","2019-01-28T12:36:17Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/13058","Logical Bug"
"222","scikit-learn/scikit-learn","RadiusNeighborsRegression is inconsistent when extrapolation occurs","#### Description
The behavior of `RadiusNeighborsRegression` is inconsistent when extrapolation occurs. The behavior depends on the chosen weight-function. 

-`weight=""uniform""` will return `[NaN]`.
-`weight=""distance""`will raise an error
-`weight=lambda d: d`will raise an error


#### Steps/Code to Reproduce
```python
from sklearn.neighbors import RadiusNeighborsRegressor

X = [[1],[2],[3],[4]]
y = [1,2,3,4]

X_predict = [[-100]]

model = RadiusNeighborsRegressor(radius=1.0, weights = ""distance"")
fitm = model.fit(X,y)

# raises ZeroDivisionError
result = fitm.predict(X_predict)  
```

#### Expected Results
No error is raised and `[[NaN]]` is returned. 

#### Actual Results
```
 File ""c:/test.py"", line 12, in <module>
    result = fitm.predict(X_predict)
  File ""C:\...\anaconda3\lib\site-packages\sklearn\neighbors\regression.py"", line 296, in predict
    for (i, ind) in enumerate(neigh_ind)])
  File ""C:\...\anaconda3\lib\site-packages\sklearn\neighbors\regression.py"", line 296, in <listcomp>
    for (i, ind) in enumerate(neigh_ind)])
  File ""C:\...\anaconda3\lib\site-packages\numpy\lib\function_base.py"", line 1158, in average
    ""Weights sum to zero, can't be normalized"")
ZeroDivisionError: Weights sum to zero, can't be normalized
```

#### Versions
```
Windows-8.1-6.3.9600-SP0
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]
NumPy 1.14.3
SciPy 1.1.0
Scikit-Learn 0.19.1
```","['Bug', 'help wanted', 'module:neighbors']","2019-01-11T20:10:27Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/12960","UI/UX Bug"
"223","scikit-learn/scikit-learn","Error while using n_jobs=-1 with uwsgi","Hi I am using sklearn n_jobs = -1 for multi process training of my model but i am getting below error.

sklearn.externals.joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker. The exit codes of the workers are {EXIT(1), EXIT(1), EXIT(1)}","['Bug']","2019-01-04T09:58:22Z","2","1","https://github.com/scikit-learn/scikit-learn/issues/12922","Runtime Error"
"224","scikit-learn/scikit-learn","[MRG] Bugfix for not-yet-confirmed issue #12863: arpack returns singular values in ascending order, the opposite was supposed in sklearn","Bugfix for not-yet-confirmed issue #12863: arpack returns singular values in ascending order, the opposite was supposed in sklearn

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#pull-request-checklist
-->

#### Reference Issues/PRs
Fixes [MRG] not-yet-confirmed issue #12863 

<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

In the sklearn code it was assumed that the scipy wrapper for arpack singular value decomposition returns singular values (and corresponding vectors) in descending order. This is not the case. The ARPACK documentation (https://www.caam.rice.edu/software/ARPACK/UG/node136.html#SECTION001210000000000000000
) clearly states that the singular values are returned in ascending order.

The bug manifests in differing results depending on the solver:

```
import numpy as np
from sklearn import cluster

A= np.array([[-2, -4, 2], [-2, 1, 2], [4, 2, 5]])

sc= cluster.bicluster.SpectralCoclustering(n_clusters= 2, 
                                           svd_method='randomized')

sc.fit(A)
print(sc.column_labels_)
```
gives
```
[0 0 1]
```
, but
```
sc= cluster.bicluster.SpectralCoclustering(n_clusters= 2, 
                                           svd_method='arpack')
sc.fit(A)
print(sc.column_labels_)
```
gives
```
[0 0 0]
```

This bugfix makes the arpack call return the same result as the randomized sv based solution.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
http://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
","['Bug', 'module:cluster', 'module:decomposition']","2019-01-01T13:40:30Z","14","0","https://github.com/scikit-learn/scikit-learn/pull/12898","Logical Bug"
"225","scikit-learn/scikit-learn","LogisticRegressionCV failure when not all classes appear in each fold","#### Description

When not all classes appear in each CV fold, LogisticRegressionCV fails when trying to collect the coefficient array.

#### Steps/Code to Reproduce
```python
import numpy as np
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegressionCV

xtmp, ytmp = make_classification(n_samples=2500, n_informative=10, n_classes=6)

# remove all but 2 instances of class 3, in order to make class 3 not-appear in at least one fold
remove = np.where(ytmp == 3)[0][2:]
keep = np.setdiff1d(np.arange(len(ytmp), dtype='int'), remove)

lr = LogisticRegressionCV(multi_class='multinomial', cv=KFold(5)).fit(xtmp[keep], ytmp[keep])
```

#### Expected Results
No errors; `lr` contains a fitted instance of `LogisticRegressionCV`.

#### Actual Results
```ValueError                                Traceback (most recent call last)
<ipython-input-51-ac4dd6663338> in <module>
      4 remove = np.where(ytmp == 3)[0][2:]  # remove all but 2 instances of class 3
      5 keep = np.setdiff1d(np.arange(len(ytmp), dtype='int'), remove)
----> 6 lr = LogisticRegressionCV(multi_class='multinomial', cv=KFold(5)).fit(xtmp[keep], ytmp[keep])

<redacted>/lib/python3.6/site-packages/sklearn/linear_model/logistic.py in fit(self, X, y, sample_weight)
   1796         if multi_class == 'multinomial':
   1797             multi_coefs_paths, Cs, multi_scores, n_iter_ = zip(*fold_coefs_)
-> 1798             multi_coefs_paths = np.asarray(multi_coefs_paths)
   1799             multi_scores = np.asarray(multi_scores)
   1800 

<redacted>/lib/python3.6/site-packages/numpy/core/numeric.py in asarray(a, dtype, order)
    499 
    500     """"""
--> 501     return array(a, dtype, copy=False, order=order)
    502 
    503 

ValueError: could not broadcast input array from shape (10,5,21) into shape (10)
```

If you go inspect the `multi_coefs_paths` object, you'll see that its elements have different shapes, due to the misaligned classes. Only workaround as far as I can tell is to do grid search manually.

#### Versions

```
System:
    python: 3.6.7 |Anaconda, Inc.| (default, Oct 23 2018, 19:16:44)  [GCC 7.3.0]
executable: <redacted>/bin/python
   machine: Linux-3.10.0-514.26.2.el7.x86_64-x86_64-with-redhat-7.3-Maipo

BLAS:
    macros: SCIPY_MKL_H=None, HAVE_CBLAS=None
  lib_dirs: <redacted>/lib
cblas_libs: mkl_rt, pthread

Python deps:
       pip: 18.1
setuptools: 40.6.2
   sklearn: 0.20.1
     numpy: 1.15.4
     scipy: 1.1.0
    Cython: 0.29
    pandas: 0.23.4
```","['Bug', 'help wanted', 'module:linear_model']","2018-12-20T21:41:39Z","17","0","https://github.com/scikit-learn/scikit-learn/issues/12845","Runtime Error"
"226","scikit-learn/scikit-learn","LDA covariance shrinkage 'auto' bug","<!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->
The current implementation of shrinkage 'auto' is to calculate ledoit-wolf shrinkage factor separately for each matrix. 

The within-class covariance matrices are calculated with different shrinkage factors, then summed together to be S_w. Because shrinkage factor depends largely on sample size, the shrinkage factor of every within-class covariance matrix will be much larger than that of the total covariance matrix, meaning diagonal entries are weighted more in S_w than in S_t. This will result in negative diagonal entries for the between-class covariance matrix $S_b=S_t-S_w$

negative variance then leads to weird behavior of eigen-value decomposition, including 1) wrong/meaningless explained_variance 2) questionable eigenvectors

this problem can be resolved by calculating the ledoit-wolf shrinkage factor first and pass it as an argument to the LDA decoder.

I'm not sure whether this is an intended feature or a bug.

If it's the former case, how should we interpret the eigenvectors?

","['Bug', 'module:discriminant_analysis']","2018-11-23T09:46:19Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/12662","Logical Bug"
"227","scikit-learn/scikit-learn","Sklearn.svm.SVC,the ""predict"" method predict all samples to  same one label, which is wrong. ","<!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: https://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
I want to use sklearn.svm.SVC to predict probality of each label. However, when I use the method of ""predict"", the SVC estimator will predict all samples to the same label whether I set the probablity to True or False. 
When I replace SVC with LinearSVC, the result becomes normal.
the following ""Expected Results"" was run by replacing the SVC(probablity=True) with LinearSVC().

#### Steps/Code to Reproduce

```python
from sklearn.metrics import confusion_matrix
from sklearn import datasets, linear_model
from sklearn.svm import SVC, LinearSVC, NuSVC

# some code about to load X and y

clf = SVC(probability=True)
clf.fit(X, y)
y_pred = clf.predict(X)
print(clf.classes_)
print('the true label:',y[:20])
print('the predict label:', clf.predict(X)[:20])
```

#### Expected Results
['pop_and_registration' 'recall' 'repair_process' 'warranty_extension'
 'warranty_lookup' 'warranty_policy']
the true label: ['warranty_lookup' 'warranty_lookup' 'warranty_lookup' 'warranty_lookup'
 'warranty_policy' 'warranty_policy' 'warranty_policy' 'warranty_policy'
 'warranty_policy' 'warranty_policy' 'pop_and_registration'
 'pop_and_registration' 'warranty_policy' 'pop_and_registration'
 'warranty_extension' 'warranty_extension' 'warranty_extension'
 'warranty_policy' 'warranty_lookup' 'warranty_lookup']
the predict label: ['warranty_lookup' 'warranty_lookup' 'warranty_lookup' 'warranty_lookup'
 'warranty_policy' 'warranty_policy' 'warranty_policy' 'warranty_policy'
 'warranty_policy' 'warranty_policy' 'pop_and_registration'
 'pop_and_registration' 'warranty_policy' 'pop_and_registration'
 'warranty_extension' 'warranty_extension' 'warranty_extension'
 'warranty_policy' 'warranty_lookup' 'warranty_lookup']


#### Actual Results
['pop_and_registration' 'recall' 'repair_process' 'warranty_extension'
 'warranty_lookup' 'warranty_policy']
the true label: ['warranty_lookup' 'warranty_lookup' 'warranty_lookup' 'warranty_lookup'
 'warranty_policy' 'warranty_policy' 'warranty_policy' 'warranty_policy'
 'warranty_policy' 'warranty_policy' 'pop_and_registration'
 'pop_and_registration' 'warranty_policy' 'pop_and_registration'
 'warranty_extension' 'warranty_extension' 'warranty_extension'
 'warranty_policy' 'warranty_lookup' 'warranty_lookup']
the predict label: ['pop_and_registration' 'pop_and_registration' 'pop_and_registration'
 'pop_and_registration' 'pop_and_registration' 'pop_and_registration'
 'pop_and_registration' 'pop_and_registration' 'pop_and_registration'
 'pop_and_registration' 'pop_and_registration' 'pop_and_registration'
 'pop_and_registration' 'pop_and_registration' 'pop_and_registration'
 'pop_and_registration' 'pop_and_registration' 'pop_and_registration'
 'pop_and_registration' 'pop_and_registration']



#### Versions
System
------
   machine: Windows-10-10.0.17134-SP0
    python: 3.5.6 |Anaconda, Inc.| (default, Aug 26 2018, 16:05:27) [MSC v.1900 64 bit (AMD64)]
executable: D:\ProgramData\Anaconda3\envs\py35\python.exe

BLAS
----
cblas_libs: cblas
    macros: 
  lib_dirs: 

Python deps

-----------

     numpy: 1.15.2
     scipy: 1.1.0
   sklearn: 0.20.0
setuptools: 40.2.0
    Cython: None
    pandas: 0.23.4
       pip: 10.0.1

","['Bug', 'module:svm']","2018-10-18T04:28:27Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/12408","Logical Bug"
"228","scikit-learn/scikit-learn","[WIP] priority of features in decision to splitting","
#### Reference Issues/PRs
Fixes #12259 . See also  issues #2386 , #8443 and  #12188 

#### What does this implement/fix? Explain your changes.

Basically, Scikit-learn implements decision trees with 2 different types of splitters. A RandomSplitter and a BestSplitter. I focus on BestSplitter, which currently has two flaws:

1. It is random (provide different outputs on the default config). This happens when there are two or more 'best' features with equal criterion value.
2. It doesn't allow to give priors on the features, for example, when there is a Tie. It would be a nice-to-have if we could chose a features that we prefer. Either because it is easier to compute, or because it is less noisy or because it is easy to interpret. The ideal would be to encode it in the index of the column of the feature. Lower index means higher priority. 

We have a supplementary constraint which is the Shuffle constrain. Because currently there is a max_features parameter which allows us to do not test all the features at each node if there are many. So this is a nice feature and I believe we should always shuffle. The problem is that shuffling means non deterministic...


#### So we have:

- Sorted / Unsorted       -> stands for the fact of giving priority to lower index features if and only if there is a Tie.
- Deterministic / Non     -> stands for the fact of always obtaining the same result, at least for the default configuration.
- Shuffled / Looped / No  -> stands for the fact of shuffling the features when we try them. We can shuffle them randomly, we can loop through them or can test them in order (from 0 to N). This is important only if max_featues < nb_features  or if we do not sort the features.



So for me the ideal is to have a a prior on the features (Sorted, saying lower index means I prefer the feature). 
We need to do Shuffle or at least Loop, in order to support  max_features, which is often useful, and necessary for sparse cases.

Then it should be deterministic in the default configuration, so if we want this we have the following possibilities:
 - we can do looping instead of shuffling: this would make random_state useless here... so is not viable
 - we can do shuffle and sort the features to the best of our knowledge: So we do shuffling and if 2 features from the tested features are 'best' we keep the lower index. This will make the default config where `max_features=n_features` stable regardless of the `random_state` because we will test all the features.
 

So I implemented this BestSplitter2 that shuffles, and is deterministic when `max_features=n_features` and gives priority to the lower index features when there is a tie.


Here you can see an example of the previous and the new one:
Previous BestSplitter is on the left and New BestSplitter2 is on the right 

[![Screenshot-from-2018-10-12-11-31-55.png](https://i.postimg.cc/m2wbstBY/Screenshot-from-2018-10-12-11-31-55.png)](https://postimg.cc/ppmNQV9r)

We observe that BestSplitter2 give priority to lower index features.
And when we test stability 


```
import numpy as np
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
x,y  = iris.data, iris.target

dtc1 = DecisionTreeClassifier(random_state=1,splitter='best2')
dtc2 = DecisionTreeClassifier(random_state=2,splitter='best2')

rs = np.random.RandomState(1234)
itr = rs.rand(x.shape[0]) < 0.75

dtc1.fit(x[itr],y[itr])
dtc2.fit(x[itr],y[itr])

print(  (dtc1.predict(x[~itr]) != dtc2.predict(x[~itr])).sum() )
```

We obtain `0` as expected in #12259 


#### Any other comments?

I don't know what do you think, I think having many splitters is not a solution either.
Personally I would replace the current bestSplitter with the one described above, as they are essentially the same but with some improvements.

Thank you very much
","['Bug', 'Stalled', 'help wanted', 'module:tree', 'cython']","2018-10-12T09:37:35Z","10","0","https://github.com/scikit-learn/scikit-learn/pull/12364","UI/UX Bug"
"229","scikit-learn/scikit-learn","Dask Joblib backend cancels required futures","Here is a failing example that uses the Dask joblib backend to parallelize a nested RandomForestClassifier within a RandomSearchCV

```python
from scipy.stats import randint as sp_randint

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.datasets import load_digits
from sklearn.ensemble import RandomForestClassifier

from dask.distributed import Client

def test_random_forest_random_search():
    with Client(processes=False) as client:
        # Taken from http://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html

        # get some data
        digits = load_digits()
        X, y = digits.data, digits.target

        # build a classifier
        clf = RandomForestClassifier(n_estimators=20)
        # specify parameters and distributions to sample from
        param_dist = {""max_depth"": [3, None],
                      ""max_features"": sp_randint(1, 11),
                      ""min_samples_split"": sp_randint(2, 11),
                      ""bootstrap"": [True, False],
                      ""criterion"": [""gini"", ""entropy""]}

        n_iter_search = 20
        random_search = RandomizedSearchCV(clf, param_distributions=param_dist,
                                           n_iter=n_iter_search, cv=5)
        from sklearn.externals import joblib
        with joblib.parallel_backend('dask'):
            random_search.fit(X, y)
```

### What happened

By placing a breakpoint here:

```diff
diff --git a/distributed/client.py b/distributed/client.py
index f9cf7f94..e47d0d16 100644
--- a/distributed/client.py
+++ b/distributed/client.py
@@ -314,6 +314,8 @@ class Future(WrappedKey):
     def release(self, _in_destructor=False):
         # NOTE: this method can be called from different threads
         # (see e.g. Client.get() or Future.__del__())
+        if not default_client().cluster.scheduler.story(self.key):
+            import pdb; pdb.set_trace()
         if not self._cleared and self.client.generation == self._generation:
             self._cleared = True
             try:
```

We find that Joblib has stopped the computation

https://github.com/scikit-learn/scikit-learn/blob/3804ccd2770ac4c026a823d019091917e4a2c70e/sklearn/externals/joblib/parallel.py#L1002-L1004

Which then goes ahead and clears out live futures

https://github.com/scikit-learn/scikit-learn/blob/3804ccd2770ac4c026a823d019091917e4a2c70e/sklearn/externals/joblib/_dask.py#L156-L159

This then fails in retrieve when we get the job result

https://github.com/scikit-learn/scikit-learn/blob/3804ccd2770ac4c026a823d019091917e4a2c70e/sklearn/externals/joblib/parallel.py#L901

I'm curious why we're stopping things while we still have live futures.  Have we found a ""good enough"" result and are stopping early?  If so, should we be getting the results of the submitted-but-discarded futures?  Is there something that Dask can do to help here?","['Bug']","2018-10-06T14:42:27Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/12315","Logical Bug"
"230","scikit-learn/scikit-learn","DecisionTreeClassifier behaviour when there are 2 or more best splitter (a tie among splitters) ","#### Description

My issue has been discussed in the past issues #2386  and #8443

It concerns `DecisionTreeClassifier` and the `splitter` parameter
In the [docs](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.decision_path) this parameter has two possible values  `splitter=""best""` and `splitter=""random""`


The current behaviour when  `splitter=""best""` is to shuffle the features at each step and take the best feature to split. In case there is a tie, we take a random one.

In some cases, there is a prior on the feature importance or in the ease of the interpretation. 

For example, some features can be noisy (due to possible data errors or any other cause) and shouldn't be used for splitting unless there is no 'good quality' feature available that does the job. Similarly, some features can be easy to understand and some others can be obscure and shouldn't be used for splitting when possible.


The current implementation doesn't allow having this kind of prior. So when two features tie as the best splitter one get chosen at random. I believe this could be an important improvement. Specially when the trees do not have a `max_depth`, because they tend to over-fit on random features while there may be some ""better"" features (in terms of prior) that do the work.

Having an option such as `splitter=""best_no_shuffle""` would allow the user to provide the features in order of importance and then when two or more features tie as the best splitters, it would systemically chose the features with the lowest index. This was once proposed [here](https://github.com/scikit-learn/scikit-learn/issues/2386#issuecomment-23148454) 


If you believe that this can be a valuable improvement I can try to implement it.
Or maybe you know a better workaround that solves my problem ! 






#### Steps/Code to Reproduce

I have recovered the code from  #8443 because it is simple and explains the issue


```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
x,y  = iris.data, iris.target

dtc1 = DecisionTreeClassifier(random_state=1)
dtc2 = DecisionTreeClassifier(random_state=2)

rs = np.random.RandomState(1234)
itr = rs.rand(x.shape[0]) < 0.75

dtc1.fit(x[itr],y[itr])
dtc2.fit(x[itr],y[itr])

print(  (dtc1.predict(x[~itr]) != dtc2.predict(x[~itr])).sum() )
```


#### Expected Results
`Should print 0`

#### Actual Results
`Prints 1`

#### Versions

System
------
   machine: Linux-4.4.0-135-generic-x86_64-with-Ubuntu-16.04-xenial
executable: /usr/bin/python3
    python: 3.5.2 (default, Nov 23 2017, 16:37:01)  [GCC 5.4.0 20160609]

BLAS
----
cblas_libs: openblas, openblas
    macros: HAVE_CBLAS=None
  lib_dirs: /usr/lib

Python deps
-----------
     scipy: 0.19.0
   sklearn: 0.21.dev0
    pandas: 0.20.3
     numpy: 1.13.3
    Cython: 0.26
setuptools: 36.7.2
       pip: 9.0.1



","['Bug', 'help wanted', 'module:tree']","2018-10-03T14:25:36Z","7","1","https://github.com/scikit-learn/scikit-learn/issues/12259","UI/UX Bug"
"231","scikit-learn/scikit-learn","Unavoidable ""y_true and y_pred contain different number of classes"" error inside a CV loop","#### Description
During cross-validation on a multi-class problem, it's technically possible to have classes present in the test data that don't appear in the training data.

#### Steps/Code to Reproduce

```python
import numpy as np
from sklearn.metrics import make_scorer, log_loss
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from sklearn.naive_bayes import BernoulliNB

rs = np.random.RandomState(1389057)

y = [
    'cow',
    'hedgehog',
    'fox',
    'fox',
    'hedgehog',
    'fox',
    'hedgehog',
    'cow',
    'cow',
    'fox'
]

x = rs.normal([0, 0], [1, 1], size=(len(y), 2))

model = BernoulliNB()

cv = StratifiedKFold(4, shuffle=True, random_state=rs)

param_dist = {
    'alpha': np.logspace(np.log(0.1), np.log(1), 20)
}

search = RandomizedSearchCV(model, param_dist, 5,
                            scoring=make_scorer(log_loss, needs_proba=True), cv=cv)

search.fit(x, y)
```

#### Expected Results

Either:
1. Predicted classes from `predict_proba` are aligned with classes in the full training data, not just the in-fold subset.
2. Classes not in the training data are ignored in the test data.

#### Actual Results

Predicted classes from `predict_proba` are aligned with classes in the in-fold subset only, but classes not in the training data are still used in the test data, causing the error.

I understand that this is normatively ""correct"" behavior, but it makes it hard/impossible to use in cross-validation with the existing APIs.

From my perspective, the best solution would be to have `RandomizedSearchCV` pass a `labels=self.classes_` argument to its scorer. I'm not sure how well that generalizes.

#### Versions

```
Linux-3.10.0-514.26.2.el7.x86_64-x86_64-with-redhat-7.3-Maipo
Python 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 17:14:51) [GCC 7.2.0]
NumPy 1.15.0
SciPy 1.1.0
Scikit-Learn 0.19.1
```
","['Bug', 'module:model_selection']","2018-08-07T20:48:34Z","15","1","https://github.com/scikit-learn/scikit-learn/issues/11777","UI/UX Bug"
"232","scikit-learn/scikit-learn","Graphical lasso Issue with graph_lasso() method","#### Description
 Dear All,
 I am working on replicating a paper titled Improving Mean Variance Optimization through Sparse Hedging Restriction. The authors idea is to use Graphical Lasso algorithm to infuse some bias in the estimation process of the inverse of the sample covariance matrix. The graphical lasso algorithm works perfectly fine in R, but when I use python on the same data with the same parameters I get two sorts of errors:
 
1-  If I use coordinate descent (cd ) mode as a solver, I get a floating point error saying that: the matrix is not symmetric positive definite and that the system is too ill-conditioned  for this solver. FloatingPointError: Non SPD result: the system is too ill-conditioned for this solver. The system is too ill-conditioned for this solver  (The thing that bugs me is that I tried this solver on a simulated Positive definite matrix and It game me this error)
 
2-  If I use the Least Angle Regression (LARS) mode (Which is less stable but recommended for ill-conditioned matrices) I get an  Overflow error stating that the integer is too large to be converted to a float OverflowError: int too large to convert to float
 
To my knowledge, unlike C++ and other languages, python is not restricted by an upper maximum for integer numbers (besides the capacity of the machine itself). Whereas the floats are restricted. I think this might be the source of the later problem. (I have also heard in the past that R is much more robust in terms of dealing ill-conditioned matrices). I would be glad to hear you experience with graph lasso in R or python. 
With this email, I have attached a little python code that simulates this problem in a few lines. Any input will be of great appreciation.
Thank you all,
 
Skander


#### Steps/Code to Reproduce

from sklearn.covariance import graph_lasso
from sklearn.datasets import make_spd_matrix

symetric_PD_mx= make_spd_matrix(100)

glout = graph_lasso(emp_cov=symetric_PD_mx, alpha=0.01,mode=""lars"")

#### Actual Results
<!-- ---------------------------------------------------------------------------
OverflowError                             Traceback (most recent call last)
<ipython-input-10-9ac318eec6e7> in <module>()
----> 1 glout = graph_lasso(emp_cov=symetric_PD_mx, alpha=0.01,mode=""lars"")

~\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\sklearn\covariance\graph_lasso_.py in graph_lasso(emp_cov, alpha, cov_init, mode, tol, enet_tol, max_iter, verbose, return_costs, eps, return_n_iter)
    222                             sub_covariance, row, Xy=row, Gram=sub_covariance,
    223                             alpha_min=alpha / (n_features - 1), copy_Gram=True,
--> 224                             eps=eps, method='lars', return_path=False)
    225                 # Update the precision matrix
    226                 precision_[idx, idx] = (

~\AppData\Local\Continuum\anaconda3\envs\my_env\lib\site-packages\sklearn\linear_model\least_angle.py in lars_path(X, y, Xy, Gram, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)
    358                 L_ = L[:n_active, :n_active].copy()
    359                 while not np.isfinite(AA):
--> 360                     L_.flat[::n_active + 1] += (2 ** i) * eps
    361                     least_squares, info = solve_cholesky(
    362                         L_, sign_active[:n_active], lower=True)

OverflowError: int too large to convert to float -->

#### Versions
<!--
Please run the following snippet and paste the output below.
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import sklearn; print(""Scikit-Learn"", sklearn.__version__)
-->
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]
NumPy 1.14.3
SciPy 1.1.0
Scikit-Learn 0.19.1

<!-- Thanks for contributing! -->
","['Bug', 'module:covariance']","2018-07-03T19:39:12Z","1","1","https://github.com/scikit-learn/scikit-learn/issues/11417","Runtime Error"
"233","scikit-learn/scikit-learn","f_regression takes square root of negative values when constant columns are present","#### Description
`sklearn.feature_selection.f_regression` raises `RuntimeWarning: invalid value encountered in sqrt` when an array with any constant column is passed in. 

If this is the expected behavior, this issue should be closed. However, it seem suspicious that this only happens when `center` is `True`, and even so it may be worthwhile to check for constant columns and raise a more helpful warning.

If the call to `f_regression` below is replaced with
`f_regression(X, y, center=False)` then all values are returned with no NaN's or warnings.

I'm still not sure what conditions are needed for the error to occur. A constant column usually creates a division by zero warning, which appears to be the correct behavior. This specific array always results in square root of a negative, which is unexpected.

#### Steps/Code to Reproduce
```python
import numpy as np
from sklearn.feature_selection import f_regression

X = np.array([[  0.92  ,   0.2674,  54.934 ,  26.    ],
              [  0.92  ,   0.2674,  54.934 ,  28.    ],
              [  0.92  ,   0.2674,  54.934 ,  23.    ],
              [  0.92  ,   0.2674,  54.934 ,  21.    ],
              [  0.92  ,   0.2674,  54.934 ,  29.    ],
              [  0.92  ,   0.2674,  54.934 ,  25.    ],
              [  0.92  ,   0.2674,  54.934 ,  23.    ],
              [  0.92  ,   0.2674,  54.934 ,  28.    ],
              [  0.92  ,   0.2674,  54.934 ,  27.    ],
              [  0.92  ,   0.2674,  54.934 ,  27.    ]])
y = np.array([ 0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92,  0.92])

f_regression(X, y)
```

#### Expected Results
No warning raised and no NaN's in output
#### Actual Results

```
/usr/local/miniconda3/envs/testml/lib/python3.6/site-packages/scikit_learn-0.19.1-py3.6-macosx-10.7-x86_64.egg/sklearn/feature_selection/univariate_selection.py:292: RuntimeWarning: invalid value encountered in sqrt
  n_samples * X_means ** 2)
/usr/local/miniconda3/envs/testml/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in greater
  return (self.a < x) & (x < self.b)
/usr/local/miniconda3/envs/testml/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:879: RuntimeWarning: invalid value encountered in less
  return (self.a < x) & (x < self.b)
/usr/local/miniconda3/envs/testml/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1821: RuntimeWarning: invalid value encountered in less_equal
  cond2 = cond0 & (x <= self.a)
```
```python
(array([        nan, -8.        , -8.        , -8.07593079]),
 array([ nan,   1.,   1.,   1.]))
```
#### Another example without the error
```python
...
f_regression(X, y, center=False)
```
returns 
```
(array([ -2.02661983e+16,  -2.02661983e+16,  -2.02661983e+16,
          9.57231884e+02]),
 array([  1.00000000e+00,   1.00000000e+00,   1.00000000e+00,
          1.88654093e-10]))
```

#### Versions
Darwin-15.6.0-x86_64-i386-64bit
Python 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) 
[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]
NumPy 1.12.1
SciPy 1.1.0
Scikit-Learn 0.19.1","['Bug', 'help wanted', 'module:feature_selection']","2018-06-30T21:19:10Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/11395","Runtime Error"
"234","scikit-learn/scikit-learn","SGDRegressor gets poor fit with sparse matrix","If a sparse matrix is passed into the function, it doesn't throw any error but gets a poor fit **sometimes**. I think it should either throw an error when the parameter is a sparse matrix or convert it into a dense matrix like https://github.com/scikit-learn/scikit-learn/pull/535

Below is some code to compare the differences. (It fits well on boston housing price even when the parameter is a sparse matrix but fails on diabetes.)

```python
import numpy
from scipy.sparse import csr_matrix
from sklearn.datasets import load_diabetes
from sklearn.linear_model import SGDRegressor, LinearRegression
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

diabetes = load_diabetes()
X = diabetes.data
# X = csr_matrix(X)  # Uncomment this line to use a sparse matrix
y = numpy.asarray(diabetes.target)

scaler = StandardScaler(with_mean=False)
scaler.fit(X)
X = scaler.transform(X)

estimator = SGDRegressor()
# estimator = LinearRegression()  # LinearRegression works with sparse matrix
estimator.fit(X, y)
predicted = estimator.predict(X)

fig, ax = plt.subplots()
ax.scatter(y, predicted, edgecolors=(0, 0, 0))
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
plt.show()
```
Just stating in the documentation that the input should be a dense matrix is not enough, it may cause surprises like this.

NumPy 1.14.2
SciPy 1.1.0
Scikit-Learn 0.19.1
","['Bug', 'help wanted', 'module:linear_model']","2018-05-31T15:53:38Z","24","0","https://github.com/scikit-learn/scikit-learn/issues/11178","Runtime Error"
"235","scikit-learn/scikit-learn","Bug function: reconstruct_from_patches_2d","<!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: http://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
Function `reconstruct_from_patches_2d` is getting the right answer!!!

#### Steps/Code to Reproduce

Code:
```python
from time import time
import numpy as np
from sklearn.feature_extraction.image import extract_patches_2d
from sklearn.feature_extraction.image import reconstruct_from_patches_2d

img = np.ones((128, 256))

height, width = img.shape


# Extract all reference patches from the left half of the image
print('Extracting reference patches...')
t0 = time()
patch_size = (128, 128)
data = extract_patches_2d(img, patch_size)
#data = data.reshape(data.shape[0], -1)
#intercept = np.mean(data, axis=0)
#data -= intercept
print('done in %.2fs.' % (time() - t0))

b = reconstruct_from_patches_2d(data, img.shape)
```


#### Actual Results
b should be all ones!!!, The result is NOT!!!

#### Versions

Windows-10-10.0.16299-SP0
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]
NumPy 1.14.2
SciPy 1.0.1
Scikit-Learn 0.19.1


<!-- Thanks for contributing! -->
","['Bug', 'help wanted', 'module:feature_extraction']","2018-04-03T08:24:10Z","10","0","https://github.com/scikit-learn/scikit-learn/issues/10910","UI/UX Bug"
"236","scikit-learn/scikit-learn","BUG Median not always being calculated correctly for DecisionTrees in the WeightedMedianCalculator","It appears the existing ""_WeightedMedianCalculator.update_median_parameters_post_remove_(...)"" is not working correctly...  Here is what I am seeing with an illustrated example...

```
y = [0.14038694, 0.4173048, 0.55868983]
sample_weights = [2, 1, 2]
```

in effect, the above is equivalent to:

> 0.14038694, 0.14038694, 0.4173048, 0.55868983, 0.55868983


The process starts with these items being added to the _WeightedMedianCalculator_...  
I have outputted key values after each movement (after _update_median_parameters_post_push_ and after _update_median_parameters_post_remove_ method calls):


`> PUSH, data: 0.4173048;  orig_median: 0.0; new_median: 0.4173048;....`
This tells us that the sample line with y = 0.4173048 was added to a new group.  Since there is only one item this is the same as the median!


`> PUSH;  data: 0.55868983;  orig_median: 0.4173048; new_median: 0.55868983;....`
Next the 0.55868983 sample line is added to the group **(note that this has a weight of 2).**  We now have in effect:
> 0.4174048, **0.55868983**, 0.55868983
 
so the new median becomes 0.55868983 as expected.


`> PUSH;  data: 0.14038694;  orig_median: 0.55868983; new_median: 0.4173048;....`
Next the 0.14038694 sample line is added to the group.  We now have in effect:
> 0.14038694, 0.14038694, **0.4173048**, 0.55868983, 0.55868983

so the new median becomes 0.4173048 as expected.


`> POP/REMOVE;   data: 0.4173048;  orig_median: 0.4173048; new_median: 0.55868983;....`

The data equated to the sample line 0.4173048 is removed.  We now have:
> 0.14038694, **0.14038694, 0.55868983**, 0.55868983

The new median looks incorrect!  Should it not be:
= (0.14038694 + 0.55868983) / 2
= 0.349538385
?

Steps to reproduce:
Unfortunately it is difficult to provide a python script to demonstrate the above, as it requires debugging private values but can be achieved this way:
/tree/utils.pyx -> WeightedMedianCalculator class:

```
    cdef int remove(self, DOUBLE_t data, DOUBLE_t weight) nogil:
        """"""Remove a value from the MedianHeap, removing it
        from consideration in the median calculation
        """"""
        cdef int return_value
        cdef DOUBLE_t original_median

        if self.size() != 0:
            original_median = self.get_median()

        #FOR DEBUG:
        cdef int original_k
        cdef DOUBLE_t original_sum_w_0_k
        cdef DOUBLE_t original_total_weight
        original_k = self.k
        original_sum_w_0_k = self.sum_w_0_k
        original_total_weight = self.total_weight

        return_value = self.samples.remove(data, weight)
        self.update_median_parameters_post_remove(data, weight,
                                                  original_median)

        with gil:
            print ("" "")
            print (""POP/REMOVE, data: "" + str(data)
                + "";  orig_median: "" + str(original_median) 
                + ""; new_median: "" + str(self.get_median()) 
                + "";  size: "" + str(self.size())
                + "";  weight: "" + str(weight)
                + "";  orig_total_weight: "" + str(original_total_weight)
                + "";  self.total_weight: "" + str(self.total_weight)
                + "";  orig_k: "" + str(original_k)               
                + "";  self.k: "" + str(self.k)
                + "";  orig_sum_w_0_k: "" + str(original_sum_w_0_k)
                + "";  self.sum_w_0_k: "" + str(self.sum_w_0_k)
                )

        return return_value
```
```
        cdef int push(self, DOUBLE_t data, DOUBLE_t weight) nogil except -1:
        """"""Push a value and its associated weight to the WeightedMedianCalculator

        Return -1 in case of failure to allocate memory (and raise MemoryError)
        or 0 otherwise.
        """"""
        cdef int return_value
        cdef DOUBLE_t original_median

        #FOR DEBUG:
        cdef int original_k
        cdef DOUBLE_t original_sum_w_0_k
        cdef DOUBLE_t original_total_weight
        original_k = self.k
        original_sum_w_0_k = self.sum_w_0_k
        original_total_weight = self.total_weight

        if self.size() != 0:
            original_median = self.get_median()
        # samples.push (WeightedPQueue.push) uses safe_realloc, hence except -1
        return_value = self.samples.push(data, weight)
        self.update_median_parameters_post_push(data, weight,
                                                original_median)

        with gil:
            print ("" "")
            print (""PUSH, data: "" + str(data) 
                + "";  orig_median: "" + str(original_median) 
                + ""; new_median: "" + str(self.get_median()) 
                + "";  size: "" + str(self.size())
                + "";  weight: "" + str(weight)
                + "";  orig_total_weight: "" + str(original_total_weight)
                + "";  self.total_weight: "" + str(self.total_weight)
                + "";  orig_k: "" + str(original_k)               
                + "";  self.k: "" + str(self.k)
                + "";  orig_sum_w_0_k: "" + str(original_sum_w_0_k)
                + "";  self.sum_w_0_k: "" + str(self.sum_w_0_k)
                )

        return return_value
```

After compilation, calling this script:

```
from sklearn.tree import DecisionTreeRegressor

train_y = [0.4173048,0.55868983,0.14038694]
train_X = [[ 15., 9.],
           [ 19., 35.],
           [ 40., 54.]]
sample_weight = [1,2,2]

depth = 1

wineTree = DecisionTreeRegressor(max_depth=depth, criterion='mae', random_state=1)
wineTree.fit(train_X, train_y
    , sample_weight=sample_weight
    )
```

I think I know where the problem lies and how to fix this issue, but first wanted to ensure this behaviour isn't desired especially considering sample_weights could be represented as floats etc?


","['Bug', 'help wanted', 'module:tree']","2018-02-28T10:23:42Z","8","0","https://github.com/scikit-learn/scikit-learn/issues/10725","Logical Bug"
"237","scikit-learn/scikit-learn","NMF n_components are not properly getting reflected in output when using Grid Search CV","When I executed the example code from this [link ](http://scikit-learn.org/0.18/auto_examples/plot_compare_reduction.html
) 
and analyzed the grid search output (grid.cv_results_['params']), n_components are not properly getting reflected in output.

Posting a small snippet of output of grid.cv_results_['params']:
```
{'classify__C': 1000,
  'reduce_dim': NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,
    **n_components=None,** random_state=None, shuffle=False, solver='cd',
    tol=0.0001, verbose=0),
  'reduce_dim__n_components': 2},
 {'classify__C': 1000,
  'reduce_dim': NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,
    **n_components=None**, random_state=None, shuffle=False, solver='cd',
    tol=0.0001, verbose=0),
  'reduce_dim__n_components': 4},
 {'classify__C': 1000,
  'reduce_dim': NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,
    **n_components=None**, random_state=None, shuffle=False, solver='cd',
    tol=0.0001, verbose=0),
  'reduce_dim__n_components': 8},
```
where reduce_dim__n_components are updating for NMF but not the actual n_components in NMF

Thanks,
Pat

","['Bug', 'help wanted', 'module:decomposition', 'module:pipeline']","2017-12-15T13:21:52Z","25","0","https://github.com/scikit-learn/scikit-learn/issues/10329","Logical Bug"
"238","scikit-learn/scikit-learn","Problems with score_samples in BayesianGaussianMixture","I have noticed that on many problems the `score_samples()` function in `sklearn.mixture.BayesianGaussianMixture` consistently gives lower test set likelihoods than just taking the posterior mean parameters from the mixture and plugging them into a regular mixture of Gaussians likelihood.

I have coded up a demo on a toy problem to illustrate:

```python
import numpy as np
import scipy.stats as ss
from scipy.special import logsumexp
from sklearn.mixture import BayesianGaussianMixture, GaussianMixture


def loglik_mixture(X, w, means, covs):
    N = X.shape[0]

    w = w / np.sum(w)  # Just to be sure normalized

    loglik = np.zeros((N, len(w)))
    for ii in range(len(w)):
        mu = means[ii, :]
        S = covs[ii, :, :]
        gauss_part = ss.multivariate_normal.logpdf(X, mu, S)
        loglik[:, ii] = np.log(w[ii]) + gauss_part
    loglik = logsumexp(loglik, axis=1)
    return loglik


def simple_data():
    x = np.random.randn(1000, 2) + 1.0
    idx = np.random.rand(1000) <= 0.5
    x[idx, :] = -1 * x[idx, :]
    return x

np.random.seed(1234)

delta = []
for _ in range(500):
    x_train = simple_data()
    x_test = simple_data()

    gmm = GaussianMixture(n_components=2, covariance_type='full')
    gmm.fit(x_train)
    loglik0 = gmm.score_samples(x_test)
    w, means, covs = gmm.weights_, gmm.means_, gmm.covariances_
    loglik1 = loglik_mixture(x_test, w, means, covs)
    # Demonstrate that loglik_mixture() is correct
    np.testing.assert_allclose(loglik0, loglik1)
    print('-' * 10)
    print(f'MLE GMM loglik {np.mean(loglik0)}')

    bgmm = BayesianGaussianMixture(n_components=2, covariance_type='full')
    bgmm.fit(x_train)
    loglik_bayes0 = bgmm.score_samples(x_test)
    w, means, covs = bgmm.weights_, bgmm.means_, bgmm.covariances_
    loglik_bayes1 = loglik_mixture(x_test, w, means, covs)
    print(f'VB GMM loglik (built-in) {np.mean(loglik_bayes0)}')
    print(f'improvement {np.mean(loglik_bayes0) - np.mean(loglik0)}')
    print(f'VB GMM loglik {np.mean(loglik_bayes1)}')
    print(f'improvement {np.mean(loglik_bayes1) - np.mean(loglik0)}')

    delta.append(np.mean(loglik_bayes1) - np.mean(loglik_bayes0))
print(np.mean(delta))
print(np.mean(np.array(delta) > 0))
```
This gives for instance:
```
MLE GMM loglik -3.362221
VB GMM loglik (built-in) -3.367131
improvement -0.004911
VB GMM loglik -3.361099
improvement 0.001121
```
The built in score for VBGMM is consistently worse than MLE GMM but often better when the posterior mean is plugged in as a point estimate in loglik_mixture().  On this problem, the loglik_mixture() likelihood is always around 0.006 nats higher than the built in score function.

This makes me wonder if there is some normalization issue in the likelihood in _estimate_log_prob().  I am not sure where the derivation for the code in lines 690-698 is.

The documentation says it implements Blei and Jordan (2006), which has the posterior predictive in eqn (23).  But that is only an approximation and not necessarily even normalized.

If score_samples is used for evaluation purposes, it is not fair to compare VB-GMM to other models if its likelihood is not even normalized!  Some more investigation is needed here.","['Bug', 'module:mixture']","2017-11-15T20:39:20Z","2","1","https://github.com/scikit-learn/scikit-learn/issues/10148","UI/UX Bug"
"239","scikit-learn/scikit-learn","Fitting ExpSineSquared kernel to large dimension data throws a ValueError on MacOS","<!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: http://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
<!-- Example: Joblib Error thrown when calling fit on LatentDirichletAllocation with evaluate_every > 0-->
ValueError: ""array must not contain infs or NaNs"" thrown when fitting an ExpSineSquared kernel to large dimension data

#### Steps/Code to Reproduce
<!--
Example:
```python
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

docs = [""Help I have a bug"" for i in range(1000)]

vectorizer = CountVectorizer(input=docs, analyzer='word')
lda_features = vectorizer.fit_transform(docs)

lda_model = LatentDirichletAllocation(
    n_topics=10,
    learning_method='online',
    evaluate_every=10,
    n_jobs=4,
)
model = lda_model.fit(lda_features)
```
If the code is too long, feel free to put it in a public gist and link
it in the issue: https://gist.github.com
-->
```py
import sklearn.gaussian_process as gp
from sklearn.gaussian_process.kernels import ExpSineSquared

while True:
    xtr = np.random.rand(25,25)
    ytr = np.random.rand(25)
    model = gp.GaussianProcessRegressor(kernel=ExpSineSquared(),
                                                alpha=1e-5,
                                                n_restarts_optimizer=10,
                                                normalize_y=True)
    model.fit(xtr, ytr)
```

#### Expected Results
<!-- Example: No error is thrown. Please paste or describe the expected results.-->
No error is thrown

#### Actual Results
<!-- Please paste or specifically describe the actual output or traceback. -->

```
  File ""<ipython-input-39-119d71c23c9f>"", line 1, in <module>
    runfile('/Users/eric/Google Drive/Spatial conquest/Neural Network/GitHub Repository/tensorlayer/benchmarks/bug.py', wdir='/Users/eric/Google Drive/Spatial conquest/Neural Network/GitHub Repository/tensorlayer/benchmarks')

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 710, in runfile
    execfile(filename, namespace)

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 101, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/Users/eric/Google Drive/Spatial conquest/Neural Network/GitHub Repository/tensorlayer/benchmarks/bug.py"", line 19, in <module>
    model.fit(xtr, ytr)

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py"", line 232, in fit
    bounds))

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py"", line 455, in _constrained_optimization
    if convergence_dict[""warnflag""] != 0:

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py"", line 193, in fmin_l_bfgs_b
    **opts)

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py"", line 328, in _minimize_lbfgsb
    f, g = func_and_grad(x)

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/scipy/optimize/lbfgsb.py"", line 278, in func_and_grad
    f = fun(x, *args)

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/scipy/optimize/optimize.py"", line 292, in function_wrapper
    return function(*(wrapper_args + args))

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/scipy/optimize/optimize.py"", line 63, in __call__
    fg = self.fun(x, *args)

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py"", line 209, in obj_func
    theta, eval_gradient=True)

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/sklearn/gaussian_process/gpr.py"", line 419, in log_marginal_likelihood
    except np.linalg.LinAlgError:

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/scipy/linalg/decomp_cholesky.py"", line 81, in cholesky
    check_finite=check_finite)

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/scipy/linalg/decomp_cholesky.py"", line 20, in _cholesky
    a1 = asarray_chkfinite(a)

  File ""/Users/eric/Applications/anaconda/envs/NN_on_GPU/lib/python3.6/site-packages/numpy/lib/function_base.py"", line 1215, in asarray_chkfinite
    ""array must not contain infs or NaNs"")

ValueError: array must not contain infs or NaNs
```

#### Versions
<!--
Please run the following snippet and paste the output below.
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import sklearn; print(""Scikit-Learn"", sklearn.__version__)
-->
Darwin-16.7.0-x86_64-i386-64bit
Python 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:14:59) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]
NumPy 1.13.1
SciPy 0.19.1
Scikit-Learn 0.19.0

#### Additional comment
This problem seems MacOS specific as it does not occur on my linux system with exactly the same versions
Linux-4.10.0-33-generic-x86_64-with-debian-stretch-sid
Python 3.6.2 |Continuum Analytics, Inc.| (default, Jul 20 2017, 13:51:32) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
NumPy 1.13.1
SciPy 0.19.1
Scikit-Learn 0.19.0

In terms of investigation, the furthest i got was to see that `kernel(self.X_train_)` in sklearn/gaussian_process/gpr.py"", line 419, log_marginal_likelihood was diverging to infinity

<!-- Thanks for contributing! -->
","['Bug', 'module:gaussian_process']","2017-09-23T18:10:22Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/9824","Runtime Error"
"240","scikit-learn/scikit-learn","Bad results for LabelPropagation (and LabelSpreading) when using any kernel other than ""rbf"" or ""knn""","<!--
If your issue is a usage question, submit it here instead:
- StackOverflow with the scikit-learn tag: http://stackoverflow.com/questions/tagged/scikit-learn
- Mailing List: https://mail.python.org/mailman/listinfo/scikit-learn
For more information, see User Questions: http://scikit-learn.org/stable/support.html#user-questions
-->

<!-- Instructions For Filing a Bug: https://github.com/scikit-learn/scikit-learn/blob/master/CONTRIBUTING.md#filing-bugs -->

#### Description
Poor results always obtained when using sklearn.semi_supervised.LabelPropagation (or LabelSpreading) with any (callable) kernel other than the predefined ""knn"", ""rbf"", or sklearn.metrics.pairwise.rbf_kernel

#### Steps/Code to Reproduce
Example:
```python
from sklearn.semi_supervised import LabelPropagation, LabelSpreading
from sklearn.metrics.pairwise import pairwise_kernels, rbf_kernel

def my_kernel(X, Z):
        # This works good :
	return rbf_kernel(X, Z)
        # This seems to always work bad for any choice of metric (and parameters), except for ""rbf"" :
	return pairwise_kernels(X, Z, metric=""cosine"")

h = LabelSpreading(kernel=my_kernel, n_neighbors=7, alpha=0.2).fit(X_labelled+X_unlabelled, Y_labelled+[-1 for _ in X_unlabelled])
print ""Result:"", 100*accuracy_score( Y_test, h.predict(X_test) )
```

Here is a complete other example using the optical digits dataset:
```python
from sklearn.metrics import accuracy_score
from sklearn import datasets 
from sklearn.semi_supervised import LabelPropagation, LabelSpreading
from sklearn.utils import shuffle
from sklearn.metrics.pairwise import pairwise_kernels, rbf_kernel

def my_kernel(X, Z):
	# return rbf_kernel(X, Z, gamma=0.1) # works good
	return pairwise_kernels(X, Z, metric=""cosine"")  # works bad
	
if __name__ == ""__main__"":
	digits = datasets.load_digits()
	X, Y = shuffle( list(digits.data), list(digits.target) )
	Lx = X[:50]; Ly = Y[:50]; Ux = X[50:]; Uy = Y[50:]
	
	print Ly
	h = LabelSpreading(kernel=my_kernel, n_neighbors=7, alpha=0.2).fit(Lx+Ux, Ly+[-1 for _ in Ux])
	print ""Result:"", 100*accuracy_score( Uy, h.predict(Ux) )
```

#### Expected Results
I expected kernels such as: sigmoid, polynomial, poly, linear, cosine (tested with various parameters) to perform reasonably good, but all of those give extremely poor results.

#### Actual Results
Only kernel = ""knn"", or ""rbf"" (or a callable corresponding to an rbf kernel) gives good results.

#### Versions
Linux-3.10.0-514.6.1.el7.x86_64-x86_64-with-centos-7.3.1611-Core
('Python', '2.7.5 (default, Nov  6 2016, 00:28:07) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)]')
('NumPy', '1.11.2')
('SciPy', '0.18.1')
('Scikit-Learn', '0.19.0')


<!-- Thanks for contributing! -->
","['Bug', 'module:metrics', 'module:semi_supervised']","2017-09-09T21:05:59Z","10","1","https://github.com/scikit-learn/scikit-learn/issues/9722","UI/UX Bug"
"241","scikit-learn/scikit-learn","least_angle.py in lars_path - shapes not aligned on properly formatted data with specific alpha","Encountered a similar error to https://github.com/scikit-learn/scikit-learn/issues/5873 - when running the following code:

`a_selection = RandomizedLasso(alpha=0.025, normalize=False, n_jobs=1, random_state=42)
 a_selection.fit(X=x_sub, y=y_sub)`
    
I can't share the data as it's from clinical trials, but what I have noticed is that the error disappears (for this particular fit) when I remove the alpha parameter. The code takes 2 days to complete so I am worried a different alpha will break a different dataset input. The data frame is properly formatted, the row number fits the labels and there are no NaNs. The error I get is:

`~/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/randomized_l1.py in fit(self, X, y)
    110             n_jobs=self.n_jobs, verbose=self.verbose,
    111             pre_dispatch=self.pre_dispatch, random_state=self.random_state,
--> 112             sample_fraction=self.sample_fraction, **params)
    113 
    114         if scores_.ndim == 1:`

`~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in __call__(self, *args, **kwargs)
    281 
    282     def __call__(self, *args, **kwargs):
--> 283         return self.func(*args, **kwargs)
    284 
    285     def call_and_shelve(self, *args, **kwargs):`

`~/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/randomized_l1.py in _resample_model(estimator_func, X, y, scaling, n_resampling, n_jobs, verbose, pre_dispatch, random_state, sample_fraction, **params)
     52                 verbose=max(0, verbose - 1),
     53                 **params)
---> 54             for _ in range(n_resampling)):
     55         scores_ += active_set
     56 `

`~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self, iterable)
    756             # was dispatched. In particular this covers the edge
    757             # case of Parallel used with an exhausted iterator.
--> 758             while self.dispatch_one_batch(iterator):
    759                 self._iterating = True
    760             else:`

`~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self, iterator)
    606                 return False
    607             else:
--> 608                 self._dispatch(tasks)
    609                 return True
    610 `

`~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self, batch)
    569         dispatch_timestamp = time.time()
    570         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 571         job = self._backend.apply_async(batch, callback=cb)
    572         self._jobs.append(job)
    573 `

`~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self, func, callback)
    107     def apply_async(self, func, callback=None):
    108         """"""Schedule a func to be run""""""
--> 109         result = ImmediateResult(func)
    110         if callback:
    111             callback(result)`

`~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self, batch)
    324         # Don't delay the application, to avoid keeping the input
    325         # arguments in memory
--> 326         self.results = batch()
    327 
    328     def get(self):`

`~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
    132 
    133     def __len__(self):`

`~/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
    132 
    133     def __len__(self):`

`~/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/randomized_l1.py in _randomized_lasso(X, y, weights, mask, alpha, verbose, precompute, eps, max_iter)
    171                                       copy_Gram=False, alpha_min=np.min(alpha),
    172                                       method='lasso', verbose=verbose,
--> 173                                       max_iter=max_iter, eps=eps)
    174 
    175     if len(alpha) > 1:`

`~/miniconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py in lars_path(X, y, Xy, Gram, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)
    442 
    443                 # TODO: this could be updated
--> 444                 residual = y - np.dot(X[:, :n_active], coef[active])
    445                 temp = np.dot(X.T[n_active], residual)
    446 `

`ValueError: shapes (49,17) and (16,) not aligned: 17 (dim 1) != 16 (dim 0)`

Versions:
Linux-4.10.0-32-generic-x86_64-with-debian-stretch-sid
Python 3.6.1 |Continuum Analytics, Inc.| (default, May 11 2017, 13:09:58)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]
NumPy 1.12.1
SciPy 0.19.1
Scikit-Learn 0.18.2

Happy to provide any additional information if needed.

Lastly, I get a message that RandomizedLasso with be deprecated - what will replace it's functionality? Setting n_jobs parameter to anything beyond 1 breaks the code, which was already reported - hope the replacement will fix that.

Many thanks!","['Bug', 'help wanted', 'module:linear_model']","2017-08-22T03:48:20Z","30","0","https://github.com/scikit-learn/scikit-learn/issues/9603","UI/UX Bug"
"242","scikit-learn/scikit-learn","Multioutput-multiclass estimators have broken score method.","It looks to me like the decision trees use ``accuracy_score`` for their ``score`` but ``accuracy_score`` doesn't document that it's supporting ``multiclass-multioutput`` which the trees do.

~~I guess the ``y_true == y_pred`` works in this case, but it should be documented.
There's no list of scores supporting multiclass-multioutput in the docs, and that should be fixed, too.~~

Update:
Calling ``score`` in this case errors :-/","['Bug', 'API', 'module:multioutput', 'module:multiclass']","2017-07-19T16:34:13Z","10","1","https://github.com/scikit-learn/scikit-learn/issues/9414","Runtime Error"
"243","scikit-learn/scikit-learn","PCA, LDA, unexpected explained_variance_ratio","```python
from sklearn import datasets
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

iris = datasets.load_iris()

X = iris.data
y = iris.target
target_names = iris.target_names

#### dimensionality reduction using PCA
pca = PCA(n_components=2)
X_r = pca.fit(X).transform(X)

#### Percentage of variance explained for each components
print('PCA: explained variance ratio (first two components): %s'
      % str(pca.explained_variance_ratio_))

#### dimensionality reduction using LDA
lda = LinearDiscriminantAnalysis(n_components=2)
X_r2 = lda.fit(X, y).transform(X)

print('LDA: explained variance ratio (first two components): %s'
      % str(lda.explained_variance_ratio_))
```
#### Expected Results
The first componet of the PCA has a larger variance ratio than that from the first componet from LDA.

#### Actual Results
PCA: explained variance ratio (first two components): [ 0.925  0.053]
LDA: explained variance ratio (first two components): [ 0.991  0.009]

#### Versions
<!--
Please run the following snippet and paste the output below.
import platform; print(platform.platform())
import sys; print(""Python"", sys.version)
import numpy; print(""NumPy"", numpy.__version__)
import scipy; print(""SciPy"", scipy.__version__)
import sklearn; print(""Scikit-Learn"", sklearn.__version__)
-->

Darwin-14.5.0-x86_64-i386-64bit
Python 3.5.3 |Anaconda custom (x86_64)| (default, Mar  6 2017, 12:15:08) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]
NumPy 1.12.1
SciPy 0.19.1
Scikit-Learn 0.18.2

<!-- Thanks for contributing! -->","['Bug', 'help wanted', 'module:decomposition']","2017-07-18T16:50:13Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/9400","Logical Bug"
"244","scikit-learn/scikit-learn","LatentDirichletAllocation  Perplexity too big on Wiki dump","#### Description
LDA Perplexity too big on Wiki dump

#### Steps/Code to Reproduce
I used `gensim` `WikiCorpus` to obtain the Bag-of-Words for each document, then vectorised it using `scikit-learn` `CountVectorizer`. All is from the En Wiki dump dated 2017-04-10. I'm using the latest `scikit-learn`.

The LDA used non-informative prior,

```python
lda = LatentDirichletAllocation(
    n_topics=20,
    doc_topic_prior=1.0,
    topic_word_prior=1.0,
    max_iter=1000,
    total_samples=n_docs,
    n_jobs=-1
)

for _ in range(runs):
    lda.partial_fit(training_sample)
    print(np.log(lda.perplexity(test_docs)))
```

#### Expected Results
7.x - 8.x, which is what the Spark LDA would give and a personal code as well. As a baseline, the entropy (log-perplexity) of the mean of all the docs is 8.3 (I manually computed the entropy of M1 of the generated corpus for Python, it gives 8.3; for the corpus generated for Spark, it also gave 8.3). Normally after seeing 20% of data it should print out 8.3 or inferior.

#### Actual Results
13.8 all the way

#### Versions
All Python packages are of the latest versions as of reporting.

","['Bug', 'help wanted', 'module:decomposition']","2017-05-27T21:22:58Z","18","0","https://github.com/scikit-learn/scikit-learn/issues/8943","UI/UX Bug"
"245","scikit-learn/scikit-learn","train_test_split fails for too many values (32bit only)","Consider the following code:

```py
import numpy as np
from sklearn.model_selection import train_test_split

n = 10000
y = np.random.randint(0, 2, size=n)

y_train, y_test = train_test_split(y, train_size=int(n/2),
                                   test_size=int(n/2), stratify=y, random_state=123)

print('num train: {}'.format(len(y_train)))
print('train mean: {}'.format(y_train.mean()))
print('num test: {}'.format(len(y_test)))
print('test mean: {}'.format(y_test.mean()))
```

When n=10,000, I correctly obtain:
```py
num train: 5000
train mean: 0.4958
num test: 5000
test mean: 0.4958
```

But for larger n, such as n=100,000, I get the following error:
```pytb
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
/home/scott/Development/scratch/sklearn/stratify.py in <module>()
      6 y = np.random.randint(0, 2, size=n)
      7 
----> 8 y_train, y_test = train_test_split(y, train_size=n/2, test_size=n/2, stratify=y, random_state=123)
      9 
     10 print 'num train: {}'.format(len(y_train))

/home/scott/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc in train_test_split(*arrays, **options)
   1700     train, test = next(cv.split(X=arrays[0], y=stratify))
   1701     return list(chain.from_iterable((safe_indexing(a, train),
-> 1702                                      safe_indexing(a, test)) for a in arrays))
   1703 
   1704 

/home/scott/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc in <genexpr>((a,))
   1700     train, test = next(cv.split(X=arrays[0], y=stratify))
   1701     return list(chain.from_iterable((safe_indexing(a, train),
-> 1702                                      safe_indexing(a, test)) for a in arrays))
   1703 
   1704 

/home/scott/anaconda/lib/python2.7/site-packages/sklearn/utils/__init__.pyc in safe_indexing(X, indices)
    110             return X.take(indices, axis=0)
    111         else:
--> 112             return X[indices]
    113     else:
    114         return [X[idx] for idx in indices]

IndexError: arrays used as indices must be of integer (or boolean) type
```

And for n=1,000,000, I don't get an exception, instead the strange results:
```py
num train: 1785
train mean: 0.414565826331
num test: 894
test mean: 0.414988814318
```

Why is this? Is this a bug? Does train_test_split fail with too many values?","['Bug', 'help wanted', 'module:model_selection']","2017-04-17T23:10:11Z","10","0","https://github.com/scikit-learn/scikit-learn/issues/8755","Runtime Error"
"246","scikit-learn/scikit-learn","Surprising result in BayesianGaussianMixture","I'm surprised by the results of BayesianGaussianMixture
```python
from sklearn.mixture import BayesianGaussianMixture
from sklearn.datasets import make_blobs
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

rng = np.random.RandomState(3)
X, y = make_blobs(n_samples=500, centers=10, random_state=rng, cluster_std=[rng.gamma(1.5) for i in range(10)])

fig, axes = plt.subplots(2, 4)
gammas = np.logspace(-5, 5, 4)
for gamma, ax in zip(gammas, axes.T):
    bgmm = BayesianGaussianMixture(n_components=10, weight_concentration_prior=gamma).fit(X)
    ax[0].scatter(X[:, 0], X[:, 1], s=5, alpha=.6, c=plt.cm.Vega10(bgmm.predict(X)))
    ax[0].set_title(""gamma={:.2f}"".format(gamma))
    ax[1].bar(range(10), bgmm.weights_)
```
![image](https://cloud.githubusercontent.com/assets/449558/24217565/b050eca4-0f16-11e7-9806-ec85f9154ff3.png)

I'm changing gamma pretty drastically but the distribution of weights doesn't seem to change. Is that expected?","['Bug', 'help wanted', 'module:mixture']","2017-03-22T19:46:53Z","12","2","https://github.com/scikit-learn/scikit-learn/issues/8632","UI/UX Bug"
"247","scikit-learn/scikit-learn","Bug: BaggingClassifier for multiclass usage","Hi, 

The BaggingClassifier does not check if the number of classes of the random drawn samples for one of its estimators matches the number of classes in the dataset, resulting in an error message when its predict method is used: 

```
---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
ValueError                                         Thu Feb 16 11:17:13 2017
PID: 25287                                    Python 2.7.5: /usr/bin/python
...........................................................................
.local/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _parallel_predict_proba>
        args = ([SVC(C=100, cache_size=200, class_weight=None, co...5695085, shrinking=True,
  tol=0.001, verbose=10), SVC(C=100, cache_size=200, class_weight=None, co...8919740, shrinking=True,
  tol=0.001, verbose=10)], [array([   0,    1,    2, ..., 2045, 2046, 2047]), array([   0,    1,    2, ..., 2045, 2046, 2047])], memmap([[  1.21488877e-01,  -6.84937861e-01,  -5...022708e-05,   1.09372859e-06,  -8.55540498e-06]]), 1000)
        kwargs = {}
        self.items = [(<function _parallel_predict_proba>, ([SVC(C=100, cache_size=200, class_weight=None, co...5695085, shrinking=True,
  tol=0.001, verbose=10), SVC(C=100, cache_size=200, class_weight=None, co...8919740, shrinking=True,
  tol=0.001, verbose=10)], [array([   0,    1,    2, ..., 2045, 2046, 2047]), array([   0,    1,    2, ..., 2045, 2046, 2047])], memmap([[  1.21488877e-01,  -6.84937861e-01,  -5...022708e-05,   1.09372859e-06,  -8.55540498e-06]]), 1000), {})]
    132
    133     def __len__(self):
    134         return self._size
    135

...........................................................................
.local/lib/python2.7/site-packages/sklearn/ensemble/bagging.py in _parallel_predict_proba(estimators=[SVC(C=100, cache_size=200, class_weight=None, co...5695085, shrinking=True,
  tol=0.001, verbose=10), SVC(C=100, cache_size=200, class_weight=None, co...8919740, shrinking=True,
  tol=0.001, verbose=10)], estimators_features=[array([   0,    1,    2, ..., 2045, 2046, 2047]), array([   0,    1,    2, ..., 2045, 2046, 2047])], X=memmap([[  1.21488877e-01,  -6.84937861e-01,  -5...022708e-05,   1.09372859e-06,  -8.55540498e-06]]), n_classes=1000)
    130     for estimator, features in zip(estimators, estimators_features):
    131         if hasattr(estimator, ""predict_proba""):
    132             proba_estimator = estimator.predict_proba(X[:, features])
    133
    134             if n_classes == len(estimator.classes_):
--> 135                 proba += proba_estimator
        proba = array([[ 0.00130233,  0.00013968,  0.00144125, ....  0.00016293,
         0.00010567,  0.00053245]])
        proba_estimator = array([[  1.02577963e-03,   3.75469340e-04,   9....362413e-05,   1.45631109e-04,   3.04322015e-04]])
    136
    137             else:
    138                 proba[:, estimator.classes_] += \
    139                     proba_estimator[:, range(len(estimator.classes_))]

ValueError: operands could not be broadcast together with shapes (8009,1000) (8009,999) (8009,1000)
___________________________________________________________________________
```

It would be nice to get a warning message, if the number of classes used to train an estimator in the BaggingClassifier would not match the overall number of classes in the trainingsset. 


","['Bug', 'help wanted', 'module:ensemble']","2017-02-20T16:49:11Z","3","0","https://github.com/scikit-learn/scikit-learn/issues/8409","Runtime Error"
"248","scikit-learn/scikit-learn","Error in using multi-label classification in partial_fit() in OvR","- StackOverflow Question: https://stackoverflow.com/questions/42280439/multi-label-out-of-core-learning-for-text-data-valueerror-on-partial-fit

#### Description
When using OneVsRestClassifier() with partial_fit() method, errors are thrown. When using fit(), no errors are thrown and everything works.

#### Steps/Code to Reproduce
```
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.multiclass import OneVsRestClassifier
import numpy as np

categories = ['a','b','c']
X = [""This is a test"", ""This is another attempt"", ""And this is a test too!""]
Y = [['a', 'b'],['b', 'c'],['a', 'b']] 

mlb = MultiLabelBinarizer(classes=categories)
vectorizer = HashingVectorizer(decode_error='ignore', n_features=2 ** 18,         non_negative=True)
clf = OneVsRestClassifier(MultinomialNB(alpha=0.01))

X_train = vectorizer.fit_transform(X)
Y_train = mlb.fit_transform(Y)

- Case1
clf.partial_fit(X_train, Y_train, categories)
- Case2
clf.partial_fit(X_train, Y_train, mlb.transform(Y))
```
#### Description of code

- Case1   Using classes=categories without transforming 
```partial_fit(X_train, Y_train, classes=categories)```

        ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

- Case2   Using classes=mlb.transform(categories) i.e. after transforming from same multilabelbinarizer
```partial_fit(X_train, Y_train, classes=mlb.transform(categories))```

         ValueError: The object was not fitted with multilabel input.

#### Expected Results
No error is thrown as when using fit().

#### Actual Results
- Case1
> Traceback
 (most recent call last):
  File ""/path_to_module/Check.py"", line 18, in <module>
    clf.partial_fit(X_train, Y_train, categories)
  File ""/library/python2.7/dist-packages/sklearn/multiclass.py"", line 260, in partial_fit
    if np.setdiff1d(y, self.classes_):
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

- Case2
> Traceback
 (most recent call last):
  File ""/path_to_module/Check.py"", line 18, in <module>
    clf.partial_fit(X_train, Y_train, mlb.transform(Y))
  File ""/library/python2.7/dist-packages/sklearn/multiclass.py"", line 265, in partial_fit
    Y = self.label_binarizer_.transform(y)
  File ""/library/python2.7/dist-packages/sklearn/preprocessing/label.py"", line 329, in transform
    raise ValueError(""The object was not fitted with multilabel""
ValueError: The object was not fitted with multilabel input.

#### Observation
- In Case1, the error is because https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/multiclass.py#L260 is returning an array of booleans whereas it expects a single boolean value hence the error. But couldnt find what to do about it. How to pass Y or classes into it.

- In Case2, the error occurs because partial_fit() calls the _check_partial_fit_first_call() function which sets the clf.classes_ in a different way using unique_labels() as seen here https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/multiclass.py#L308.
These unique labels are passed to clf.label_binarizer_ in this line https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/multiclass.py#L258, which leads to it assuming type of targets as 'multiclass' whereas actual type of target is multilabel, hence this error.
The fit() method handles classes_ in a different way (doesnt use unique_labels) and hence everything works correctly 

#### Versions
Linux-3.16.0-77-generic-x86_64-with-Ubuntu-14.04-trusty
('Python', '2.7.6 (default, Oct 26 2016, 20:30:19) \n[GCC 4.8.4]')
('NumPy', '1.12.0')
('SciPy', '0.18.1')
('Scikit-Learn', '0.18.1')

","['Bug', 'help wanted', 'module:multiclass']","2017-02-17T11:43:27Z","18","6","https://github.com/scikit-learn/scikit-learn/issues/8381","Runtime Error"
"249","scikit-learn/scikit-learn","Bug in bfgs gradient computation of MLPRegressor with multiple output neurons","When implementing a special Neural Network based on MLPRegressor, I found the following problem when using bfgs training and multiple output neurons (I did not look into the other training methods):
- The 'squared_loss' implementation uses np.mean to compute the overall loss. Thus, the method divides by the number of samples and the number of output neurons/features included in the dimensions of y_true - y_pred.
- The gradient computations do not include the number of output neurons. Gradients are only divided by the number of samples (_compute_loss_grad)
Overall, this leads to the fact, that the gradient has a wrong scaling by the number of output neurons.
As the search direction is still alright, this does not cause too much pain. Still, it should be fixed.

In case this is not clear, I can see that I create a minimal example.

Cheers!

#### Versions
>>> import platform; print(platform.platform())
Linux-3.16.0-4-amd64-x86_64-with-debian-8.5
>>> import sys; print(""Python"", sys.version)
('Python', '2.7.9 (default, Mar  1 2015, 12:57:24) \n[GCC 4.9.2]')
>>> import numpy; print(""NumPy"", numpy.__version__)
('NumPy', '1.10.4')
>>> import scipy; print(""SciPy"", scipy.__version__)
('SciPy', '0.14.0')
>>> import sklearn; print(""Scikit-Learn"", sklearn.__version__)
('Scikit-Learn', '0.18.1')

","['Bug', 'help wanted', 'module:neural_network']","2017-02-13T14:25:25Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/8349","Logical Bug"
"250","scikit-learn/scikit-learn","Bug in metrics.classification._check_targets?","#### Description
I came across this issue while checking a test in `test_matthews_corrcoef`. In this test two label vectors are defined. The groundtruth vector is filled with `a`'s and `b`'s. The prediction vector is filled with `-1`'s and `0`'s. The `_check_targets` in `sklearn.metrics.classification` reports these vectors are binary when it seems to me it should be multiclass instead.

If this is the correct behavior and these pair of vectors should be classified as binary, then there would need to be some association between the numbers in the prediction vector and letters in the groundtruth vector and this association should be captured in the `LabelEncoder` used latter in the `matthews_corrcoef` function, but it is not. So, there is either a bug in `_check_inputs` or the `LabelEncoder`. 


#### Steps/Code to Reproduce

The following code takes the bit of the `test_matthews_corrcoef` and `matthews_corrcoef` functions that produce the odd states. 

```
    from sklearn.preprocessing import label_binarize, LabelEncoder
    from sklearn.metrics.classification import _check_targets
    import numpy as np
    rng = np.random.RandomState(0)
    y_true = [""a"" if i == 0 else ""b"" for i in rng.randint(0, 2, size=20)]
    y_pred = label_binarize(y_true, [""a"", ""b""]) * -1
    y_type, y_true, y_pred = _check_targets(y_true, y_pred)
    lb = LabelEncoder()
    lb.fit(np.hstack([y_true, y_pred]))
    y_true = lb.transform(y_true)
    y_pred = lb.transform(y_pred)
    print('y_type = %r' % (y_type,))
    print('lb.classes_ = %r' % (lb.classes_,))
    print('y_true = %r' % (y_true,))
    print('y_pred = %r' % (y_pred,))
```

The initial values of y_true and y_pred are: 
```
y_true = [u'a', u'b', u'b', u'a', u'b', u'b', u'b', u'b', u'b', u'b', u'b', u'a', u'a', u'b', u'a', u'a', u'a', u'a', u'a', u'b']
y_pred = [[0], [-1], [-1], [0], [-1], [-1], [-1], [-1], [-1], [-1], [-1], [0], [0], [-1], [0], [0], [0], [0], [0], [-1]]
```

After they leave _check_inputs they are
```
y_true = array([u'a', u'b', u'b', u'a', u'b', u'b', u'b', u'b', u'b', u'b', u'b',
       u'a', u'a', u'b', u'a', u'a', u'a', u'a', u'a', u'b'], 
      dtype='<U1')

y_pred =  array([ 0, -1, -1,  0, -1, -1, -1, -1, -1, -1, -1,  0,  0, -1,  0,  0,  0, 0,  0, -1])
```

At the end the values are: 

#### Expected Results
```
y_type = 'multiclass'
lb.classes_ = array([u'-1', u'0', u'a', u'b'], 
      dtype='<U21')
y_true = array([2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3])
y_pred = array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0])
```

OR 

```
y_type = 'binary'
lb.classes_ = array([u'a', u'b'], dtype='<U21')
y_true = array([0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1])
y_pred = array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0])
```

#### Actual Results

```
y_type = 'binary'
lb.classes_ = array([u'-1', u'0', u'a', u'b'], 
      dtype='<U21')
y_true = array([2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3])
y_pred = array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0])
```

It doesn't make sense to me that `_check_inputs` would output that these cases are binary when the LabelEncoder ends up encoding 4 classes and transforming them as such. 

Fortunately, this behavior does not cause `matthews_corrcoef` to produce the wrong value because the vectors are centered in the next line of code which effectively undoes the bug. 

However, this is causing me issues because I'm attempting to extend `matthews_corrcoef` to the multiclass case (in PR #8094) and I need to be able to distinguish this case from the case where a predicted class exists that is not in the groundtruth or a groundtruth class exists that was not predicted. 

#### Thoughts on a fix

My idea for resolving this issue is to have `_check_inputs` check the type of a concatenated version of `y_true` and `y_pred`, however I'm not sure how to go about this with more complex types like `multilabel-indicator`. 

#### Versions
Linux-3.13.0-105-generic-x86_64-with-Ubuntu-14.04-trusty
Python 2.7.6 (default, Jun 22 2015, 17:58:13) 
[GCC 4.8.2]
NumPy 1.11.2
SciPy 0.18.0
Scikit-Learn 0.19.dev0","['Bug', 'module:metrics']","2016-12-21T21:48:35Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/8098","Logical Bug"
"251","scikit-learn/scikit-learn","OneVsOneClassifier decision function shape non-standard","For binary tasks, OvO has a shape of ``(n_samples, 2)`` which pretty much violates our standards.
I'm not sure what the best solution is apart from just breaking it as a bug-fix.
We could add a parameter and deprecate it and then remove the parameter if we really want.

The OVR classifier also has a potential issue where the decision_function for binary is ``(n_samples, 1)`` instead of ``(n_samples,)``. That also seems non-standard. I'm not sure we have other multi-output classifiers with a decision function, so I'm not entirely certain what the standard should be. All the multi-label ones do ``(n_samples,)`` according to the tests.

Found via #8022.","['Bug', 'API', 'help wanted', 'module:multiclass']","2016-12-13T15:56:41Z","15","0","https://github.com/scikit-learn/scikit-learn/issues/8049","Logical Bug"
"252","scikit-learn/scikit-learn","Randomness in LatentDirichletAllocation(learning_method=""batch"")","It looks to me as if there's randomness in the batch variant of the LDA in ``_em_step`` where ``random_init`` is set to ``True``. That seems incorrect to me.

In general I find the logic of the algorithm a bit hard to follow, I'm wondering whether we should either refactor it or implement batch and online in two entirely separate code paths.","['Bug', 'module:decomposition']","2016-12-08T19:54:48Z","8","0","https://github.com/scikit-learn/scikit-learn/issues/8020","Logical Bug"
"253","scikit-learn/scikit-learn","Is there an issue with TimeSeriesSplit and the learning_curve?","#### Description
When using the TimeSeriesSplit (TSS) CV generator with learning_curve(), the learning curve training sets appear to be computed as fractions of the first CV split, as opposed to the whole data set. 

This means that the learning curve is being computed on the basis of a very small training set (just a small fraction of the total training set).

#### Steps/Code to Reproduce

```
from sklearn.datasets import make_classification
from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import learning_curve
from sklearn.tree import DecisionTreeClassifier
import numpy as np

data_set = make_classification(n_samples=2000, n_features=20, n_informative=2, 
                               n_redundant=2, n_repeated=0, n_classes=2, 
                               n_clusters_per_class=2, weights=None, flip_y=0.01, 
                               class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, 
                               shuffle=True, random_state=None)

X, y = data_set

tscv = TimeSeriesSplit(n_splits=10)

estimator = DecisionTreeClassifier()

train_sizes, train_scores, valid_scores = learning_curve(estimator, X, y, 
                                                         train_sizes=(np.linspace(0.1, 1.0, num=10)), 
                                                         cv=tscv, scoring='f1', exploit_incremental_learning=False, 
                                                         n_jobs=-1, pre_dispatch='2*n_jobs', verbose=1)

```

#### Expected Results
learning curve output looks like this: 
`[learning_curve] Training set sizes: [ 181  363  545  727  909 1091 1273 1455 1637 1819]`

#### Actual Results
Training set sizes are calculated as a fraction of the smallest CV iteration - i.e. a fraction of 181, meaning that the learning curve ends up being very small. 

#### Possible fix
It is possible that line 750 in _validation.py needs to change from
`n_max_training_samples = len(cv_iter[0][0])`
To
`n_max_training_samples = len(cv_iter[-1][0])`

This would allow the learning curve to be computed on basis of the full size of the CV sets, rather than just the first (and smallest) fraction.

#### Versions
Windows-10-10.0.14393
('Python', '2.7.12 |Continuum Analytics, Inc.| (default, Jun 29 2016, 11:07:13) [MSC v.1500 64 bit (AMD64)]')
('NumPy', '1.11.2')
('SciPy', '0.18.1')
('Scikit-Learn', '0.18')
","['Bug', 'module:model_selection']","2016-11-07T13:38:53Z","31","0","https://github.com/scikit-learn/scikit-learn/issues/7834","Logical Bug"
"254","scikit-learn/scikit-learn","KernelDensity Score unstable with many samples and changing leaf_size","Hi,

I have the impression that KernelDensity.score_samples() behaves strangely when there are many samples to fit.  
#### Description

When I fit many samples adding a single additional sample has a very large impact on the score of an unchanged test set. I think the influence of a single additional training input should become smaller rather than larger, as the number of training samples goes up (as can be seen below this is initially the case, before strange behaviour occurs at >80 training samples). 

I get the expected behaviour when I implement the KDE by hand.
#### Code

Here's the code to reproduce the behaviour:

``` py
import numpy as np
from sklearn.neighbors import KernelDensity as KD
from sklearn.datasets import load_digits

###Load the digits dataset and split it into a training and a testing set                                                                                                                                            

trainSamp = load_digits()['images']
trainSampF = np.zeros((trainSamp.shape[0],trainSamp.shape[1]*trainSamp.shape[2]))
for i in range(trainSamp.shape[0]):
     trainSampF[i] = trainSamp[i].flatten()

testSamp = trainSampF[-100:]
trainSamp = trainSampF[:-100]
train = trainSamp[:]
test = testSamp[:20]

###Fit a gaussian KDE with k*10 samples and score                                                                                                                                                                    
###compare to a gaussian KDE with one additional sample and score                                                                                                                                                   

for k in range(10):
     n = (k+1)*10
     model = KD(bandwidth=0.175, kernel='gaussian')
     model.fit(train[:n])
     llsv = model.score_samples(test)
     model.fit(train[:(n+1)])
     llsv2 = model.score_samples(test)
     print('evaluating score (noise free) for')
     print(n,'training samples',llsv.mean(), ' and',n+1, 'training samples',llsv2.mean())
     print('difference:',llsv.mean()-llsv2.mean())

     print()
```
#### Expected Result

Decreasing impact of adding a single training sample as training set gets larger.
#### Actual Result

When there are more than ~80 training samples adding a single training sample changes the score quite a lot.

Program output:

```
evaluating score (noise free) for
10 training samples -16898.9523708  and 11 training samples -16779.047681
difference: -119.90468982

evaluating score (noise free) for
20 training samples -15013.1149057  and 21 training samples -15013.1636959
difference: 0.0487901641663

evaluating score (noise free) for
30 training samples -14134.3366974  and 31 training samples -14134.3694872
difference: 0.0327898228261

evaluating score (noise free) for
40 training samples -11150.9509101  and 41 training samples -11150.9756027
difference: 0.0246926125947

evaluating score (noise free) for
50 training samples -11148.725074  and 51 training samples -11148.7448766
difference: 0.0198026272956

evaluating score (noise free) for
60 training samples -10508.091069  and 61 training samples -10508.1075983
difference: 0.0165293019545

evaluating score (noise free) for
70 training samples -10508.2452197  and 71 training samples -10508.2594044
difference: 0.0141846349907

evaluating score (noise free) for
80 training samples -10024.2971185  and 81 training samples -10082.2179183
difference: 57.9207998393

evaluating score (noise free) for
90 training samples -9293.12908873  and 91 training samples -9565.75291888
difference: 272.623830155

evaluating score (noise free) for
100 training samples -9855.60678148  and 101 training samples -10123.3057225
difference: 267.698941066
```
#### Versions

Linux-3.19.0-66-generic-x86_64-with-debian-jessie-sid
('Python', '2.7.12 |Anaconda 4.0.0 (64-bit)| (default, Jul  2 2016, 17:42:40) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]')
('NumPy', '1.11.1')
('SciPy', '0.17.1')
('Scikit-Learn', '0.17.1')
#### What now?

I would be glad if you could help me figure out what I did wrong or what's going wrong. Thanks!

<!-- Thanks for contributing! -->
","['Bug', 'Moderate', 'help wanted', 'module:neighbors', 'Needs Investigation']","2016-09-26T12:49:54Z","6","0","https://github.com/scikit-learn/scikit-learn/issues/7492","UI/UX Bug"
"255","scikit-learn/scikit-learn","Perplexity not monotonically decreasing for batch Latent Dirichlet Allocation","When using the batch method, the perplexity in LDA should be non-increasing in every iteration, right?
I have cases where it does increase. If this is indeed a bug, I'll investigate.
","['Bug', 'help wanted', 'module:decomposition']","2016-05-12T16:27:24Z","14","5","https://github.com/scikit-learn/scikit-learn/issues/6777","UI/UX Bug"
"256","scikit-learn/scikit-learn","Normalization in SVD solver for LinearDiscriminantAnalysis can produce large scaling coefficients","#### Description

The SVD solver contains the following code

``` python
std = Xc.std(axis=0)
# avoid division by zero in normalization
std[std == 0] = 1.
fac = 1. / (n_samples - n_classes)
#2) Within variance scaling
X = np.sqrt(fac) * (Xc / std)
```

This can result in very large scaling coefficients if std contains small values not exactly zero.
#### Steps/Code to Reproduce

Example:

``` python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis()
data = [[ 1, 2, 0, 4],
             [5, 6, 1e-7, 8],
             [9, 10, 0, 12]]
labels = [0,0,1]

lda.fit(data, labels)
print lda.scalings_
```

```
array([[  8.83883476e-02],
       [  8.83883476e-02],
       [  3.53553391e+06],
       [  8.83883476e-02]])
```

The scaling value resulting from the normalization could ( and does ) result in poor classification performance. I haven't as yet checked this with other implementations to compare the results.
#### Versions

Windows-7-6.1.7601-SP1
('Python', '2.7.11 |Anaconda 2.5.0 (32-bit)| (default, Mar  4 2016, 15:18:41) [MSC v.1500 32 bit (Intel)]')
('NumPy', '1.10.4')
('SciPy', '0.17.0')
('Scikit-Learn', '0.17')
","['Bug', 'Enhancement', 'Needs Decision']","2016-04-27T17:35:58Z","7","0","https://github.com/scikit-learn/scikit-learn/issues/6725","Logical Bug"
"257","scikit-learn/scikit-learn","Cloning CalibratedClassifierCV with prefit base_estimator","I recently stumbled upon this issue, I needed to clone a `CalibratedClassifierCV` object when the latter is defined on top on a pre-fit (`cv=""prefit""`) base estimator. Unfortunately, because of the semantic of the clone interface, this cannot be achieved without ""unfitting"" the pre-fit base estimator. 

I think this is somehow related to @jnothman long standing frozen estimator discussion. 
","['Bug', 'API', 'module:calibration']","2016-02-25T20:04:24Z","5","0","https://github.com/scikit-learn/scikit-learn/issues/6451","Logical Bug"
"258","scikit-learn/scikit-learn","SGD classification unnecessarily slow","Multiclass prediction using SGD on sparse data is unnecessarily slow. It seems to be copying large arrays _on every call to `predict`, `predict_proba` etc_, which kills its performance.

I think the main culprit is `safe_sparse_dot`, which uses scipy's ""CSR \* dense"" routine:

``` python
def safe_sparse_dot(a, b, dense_output=False):
    if issparse(a) or issparse(b):
        ret = a * b  # <== this line here: `a` is CSR, `b` is dense clf.coef_.T
        if dense_output and hasattr(ret, ""toarray""):
            ...
```

Because `b` is transposed and scipy's multiplication invokes `b.ravel()`, this is very slow (copies `clf.coef_` internally).

Keeping `clf.coef_.T` as a C-contiguous array [here](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/linear_model/base.py#L252) improved the prediction performance of SGD classifier **7300x** for us (1s vs 137s per call):

``` python
def decision_function(self, X):
    ...
    # before:
    # scores = safe_sparse_dot(X, self.coef_.T, dense_output=True) + self.intercept_

    # after quick fix: self.coef_T = np.ascontiguousarray(self.coef_.T)
    scores = safe_sparse_dot(X, self.coef_T, dense_output=True) + self.intercept_
    ...
```

The exact speedup numbers will vary, depending on `coef_` size (~the number of SGD classes and features).

This could be raised as an issue in scipy as well (I see no good reason for such inefficiency -- that `ravel()` is just too generous), but since the fix seems trivial, maybe it's worth addressing on sklearn side as well?

This is using scipy 0.16.1 and sklearn 0.17.
","['Bug', 'Moderate', 'help wanted', 'module:linear_model']","2016-01-19T08:31:58Z","21","2","https://github.com/scikit-learn/scikit-learn/issues/6186","Performance Issue"
"259","scikit-learn/scikit-learn","Problems with data scaling in sklearn.cross_decomposition.PLSRegression","I am trying to fit some spectral data using PLS, and am having difficulties with the module.

Essentially, when I use the default value of `scale=False`, I get a prediction, BUT all my predictions are scaled, and I just cannot figure out how to convert back to my original data space.  The same is true for the example code.  As you see, the scaled prediction is just off.  I assumed that I should be able to revert back to the ""unscaled"" data using `pls_scaled.y_mean_` and `pls_scaled.y_std_`, but that doesn't seem to work for me.  Any suggestions would be highly appreciated.

```
Y = np.array([[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]])
X = np.array([[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [2.,5.,4.]])
pls_not_scaled=PLSRegression(n_components=2, scale=False)
pls_scaled=PLSRegression(n_components=2, scale=True)
pls_not_scaled.fit(X,Y)
Out[]: PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)
pls_scaled.fit(X,Y)
Out[]: PLSRegression(copy=True, max_iter=500, n_components=2, scale=True, tol=1e-06)
Out[]: PLSRegression(copy=True, max_iter=500, n_components=2, scale=False, tol=1e-06)
Y_not_scaled_pred = pls_not_scaled.predict(X)
Y_scaled_pred = pls_scaled.predict(X)

print(Y)
[[  0.1  -0.2]
 [  0.9   1.1]
 [  6.2   5.9]
 [ 11.9  12.3]]

print(Y_not_scaled_pred)
[[  0.11029323  -0.09323388]
 [  0.86825958   0.77077371]
 [  6.24049125   6.31999396]
 [ 11.88095594  12.1024662 ]]

print(Y_scaled_pred)
[[ 1.52568016  1.47577156]
 [ 2.43367318  2.3584298 ]
 [ 6.25638942  6.26638408]
 [ 8.88425724  8.99941456]]
```

For those with a more visual bend:

```
plt.grid('on')
plt.scatter(Y[:, 0], Y_not_scaled_pred[:, 0], color='blue')
plt.scatter(Y[:, 0], Y_scaled_pred[:, 0], color='red')
plt.xlabel('Predicted')
plt.ylabel('Measured')
plt.legend(['not_scaled','scaled'],'lower right')
```

![figure_1](https://cloud.githubusercontent.com/assets/13950307/11730049/a1fcfa66-9f60-11e5-960f-90fa260eef6a.png)
","['Bug', 'module:cross_decomposition']","2015-12-10T22:14:24Z","19","0","https://github.com/scikit-learn/scikit-learn/issues/6002","UI/UX Bug"
"260","scikit-learn/scikit-learn","BUG: StandardScaler partial_fit overflows","The recent implementation of `partial_fit` for `StandardScaler` can overflow. A use case there is to transform indefinitely long stream of data, but that is problematic with the current implementation. The reason is that to compute the running mean, [we keep track](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/utils/extmath.py#L788) of the sample sum.

Here the code to reproduce the behavior. To simulate long stream of data would take long time; instead, I use samples with very large norm but the effect is the same. The same batch is presented to the transformer many times. The mean should be same.

```python
from sklearn.preprocessing import StandardScaler
import numpy as np

rng = np.random.RandomState(0)

def gen_1d_uniform_batch(min_, max_, n):
    return rng.uniform(min_, max_, size=(n, 1))

max_f = np.finfo(np.float64).max / 1e5
min_f = max_f / 1e2
stream_dim = 100
batch_dim = 500000
print(""mean overflow: batch vs online on %d repetitions"" % stream_dim)

X = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)

scaler = StandardScaler(with_std=False).fit(X)
print(scaler.mean_)
[  1.79769313e+301]

iscaler = StandardScaler(with_std=False)
batch = gen_1d_uniform_batch(min_=min_f, max_=min_f, n=batch_dim)
for _ in range(stream_dim):
    iscaler = iscaler.partial_fit(batch)
RuntimeWarning: overflow encountered in add
  updated_mean = (last_sum + new_sum) / updated_sample_count

print(iscaler.mean_)
[ inf]
```
","['Bug', 'Moderate', 'help wanted', 'module:preprocessing']","2015-10-27T08:59:52Z","8","0","https://github.com/scikit-learn/scikit-learn/issues/5602","Security Vulnerability"
"261","scikit-learn/scikit-learn","sklearn.cluster.AgglomerativeClustering: Can we do without completing the matrix? 'UserWarning:the number of connected components of the connectivity matrix is *>1. Completing it to avoid stopping the tree early.' ","I have tried this both on the latest 0.16.1 version and on the latest bleeding edge version of sklearn '0.17.dev0' and this appears to be an issue in both.

I use `sklearn.cluster.AgglomerativeClustering(affinity='precomputed',connectivity=Cmat,linkage='complete')`
where Cmat is a connectivity matrix in which there are disconnected components. 
As indicated by the source code, I get the error message UserWarning: the number of connected components of the connectivity matrix is *>1. Completing it to avoid stopping the tree early. 

However, reading the source code I see that when completing the connectivity matrix the developers are wondering whether the clustering can take place without completing the matrix:
""""XXX: Can we do without completing the matrix?""""

I am interested exactly in this development. Do you think sklearn is planning to fix this and make it possible to do the clustering without completing the matrix? I think it should not be too hard.

Best,
Zhana
","['Bug', 'module:cluster']","2015-09-29T09:59:42Z","15","2","https://github.com/scikit-learn/scikit-learn/issues/5327","Logical Bug"
"262","scikit-learn/scikit-learn","Dense svm and zeroed weight for samples of entire class","This bug appears in current master, and for any dense svm class.

```
import numpy as np
from sklearn.svm import SVC
X = np.array([[0, 0, 0],
              [0, 0, 1],
              [0, 1, 0],
              [0, 1, 1],
              [1, 0, 0],
              [1, 0, 1],
              [1, 1, 0],
              [1, 1, 1]])
y = np.array([0, 0, 0, 1, 1, 1, 2, 2])
w = np.array([1, 1, 1, 1, 1, 1, 0, 0])


f = SVC(kernel='linear', probability=True, random_state=1)
f.fit(X,y, w)
print(f.classes_)
print(f.predict_proba(X))
```

Output:

```
[0 1 2]
warning: class label 2 specified in weight is not found
[[ 0.28963492  0.71036508]
 [ 0.39180833  0.60819167]
 [ 0.28963492  0.71036508]
 [ 0.39180833  0.60819167]
 [ 0.57544014  0.42455986]
 [ 0.68293573  0.31706427]
 [ 0.57544014  0.42455986]
 [ 0.68293573  0.31706427]]
```

Here we see that svmlib internally have lost 2nd class, at the same time sklean's wrapper class keeps all class labels inside, that's why predict_proba returns matrix of shape (n_samples, 2) instead of (n_sample, 3) (what is expected by bagging classifier implementation). I understand that it's insane usage of weights by itself, but together with bagging and dataset with many labels, bagging randomly zeroes complete classes, and this bug shows itself, because bagging expects that svm's return probability of classes which they hold (e.g. all classes).

I investigated this a little bit, and can try to fix this, if someone will say that all this usage with bagging makes sense (Because i don't really sure about this).
","['Bug', 'module:svm']","2015-08-24T22:24:24Z","19","0","https://github.com/scikit-learn/scikit-learn/issues/5150","Logical Bug"
"263","scikit-learn/scikit-learn","Our R^2 makes odd assumptions, doesn't work with LeaveOneOut","``` python
from sklearn.datasets import make_regression
from sklearn.cross_validation import cross_val_score, LeaveOneOut
from sklearn.linear_model import Ridge

X, y, coef_ = make_regression(random_state=42, noise=1, n_samples=200, coef=True)
cross_val_score(Ridge(), X, y, cv=LeaveOneOut(len(X)))
```

> array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
>         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
>         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
>         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
>         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])

Maybe the single sample in the LOO is not interpreted correctly? :-/
","['Bug', 'API', 'module:model_selection']","2015-08-07T18:55:51Z","20","0","https://github.com/scikit-learn/scikit-learn/issues/5097","UI/UX Bug"
"264","scikit-learn/scikit-learn","has_fit_parameter will not work with **kwargs","Bagging and boosting use `sklearn.utils.validation.has_fit_parameter` to produce a clear error when `sample_weight is not None` and the base estimator does not support `sample_weight`. However, the method of checking the argspec will not work if `sample_weight` is supported as a `**kwargs` in `fit`, thus raising an exception when none applies. (A similar concern exists for the proposed alternative to `sample_weight` as keyword arg, #4696.)
","['Bug', 'help wanted']","2015-06-18T04:02:37Z","2","0","https://github.com/scikit-learn/scikit-learn/issues/4871","Logical Bug"
"265","scikit-learn/scikit-learn","Dictionary learning is slower with n_jobs > 1","Setting n_jobs > 1 in MiniBatchDictionaryLearning (and in function dictionary_learning_online) leads to worse performance.

Multi processing is handled in sklearn.decompositions, function dict_learning, l 249 

``` python
    code_views = Parallel(n_jobs=n_jobs)(
        delayed(_sparse_encode)(
            X[this_slice], dictionary, gram, cov[:, this_slice], algorithm,
            regularization=regularization, copy_cov=copy_cov,
            init=init[this_slice] if init is not None else None,
            max_iter=max_iter)
        for this_slice in slices)
```

Minimal example :
https://gist.github.com/arthurmensch/091d16c135f4a3ba5580

Output n_jobs = 1

```
Distorting image...
Extracting reference patches...
done in 0.05s.
Learning the dictionary...
done in 5.12s.
Extracting noisy patches... 
done in 0.02s.
Lasso LARS...
done in 10.24s.
```

Output n_jobs == 2

```
Distorting image...
Extracting reference patches...
done in 0.05s.
Learning the dictionary...
done in 78.98s.
Extracting noisy patches... 
done in 0.02s.
Lasso LARS...
done in 6.15s.
```

Output n_jobs == 4

```
Distorting image...
Extracting reference patches...
done in 0.05s.
Learning the dictionary...
done in 83.24s.
Extracting noisy patches... 
done in 0.02s.
Lasso LARS...
done in 3.82s.
```

We can see that transform function of MiniBatchDictionaryLearning (relying on sparse_encode function) benefits from multi-processing as expected.

Dictionary learning relies on successive calls of sparse_encode function : slowness may come from this.
","['Bug', 'Performance', 'help wanted', 'module:decomposition']","2015-05-26T07:45:45Z","10","1","https://github.com/scikit-learn/scikit-learn/issues/4769","Performance Issue"
"266","scikit-learn/scikit-learn","Ensure sparse `y` is supported in grid search etc.","As reported at https://github.com/scikit-learn/scikit-learn/issues/1233#issuecomment-73467547, `GridSearchCV` currently breaks if it is fit with sparse `y` (at least for some supported `scipy` versions). This is due to a `len` call, when we should probably be using `check_consistent_length`. In any case, a test is needed, since we are meant to be preferring sparse matrices for multilabel data now...
","['Bug', 'help wanted', 'module:model_selection']","2015-02-09T09:35:14Z","4","0","https://github.com/scikit-learn/scikit-learn/issues/4225","Logical Bug"
"267","scikit-learn/scikit-learn","kneighbors_graph param check ineffective","In `sklearn.neighbors.graph`, the `_check_params` function is called by both of the graph methods in the module. The idea seems to be to ensure that when the neighbors are precomputed that the parameters passed into say, `kneighbors_graph` match what was used in the computation of the `BallTree`. However, if the `BallTree` itself was from a prior fit, the check misses that fact. Demonstration:

``` python
data = np.random.rand(100).reshape(10,10)
n_neighbors = 2
neigh_minkowski = neighbors.NearestNeighbors(n_neighbors, metric='minkowski')
neigh_jaccard = neighbors.NearestNeighbors(n_neighbors, metric='jaccard')
neigh_jaccard.fit(data)

# ""Copies"" _fit_X, _tree, _fit_method
# But it does not copy p, metric, effective_metric, effective_metric_params, etc.
neigh_minkowski.fit(neigh_jaccard)
assert(neigh_minkowski.metric == 'minkowski') # misleading, but explainable

# Succeeds
neighbors.kneighbors_graph(neigh_minkowski, n_neighbors, mode='distance', metric='minkowski')

# Fails: ValueError: Got jaccard for metric, while the estimator has minkowski for the same parameter.
neighbors.kneighbors_graph(neigh_minkowski, n_neighbors, mode='distance', metric='jaccard')
```

I'm not sure any of this matters since `kneighbors_graph` (and the things it calls) doesn't do anything with the name of the metric if the BallTree has been precomputed. But it's certainly the case the `_check_params()` is not having its intended effect. So does this matter? Should `NearestNeighbor._fit()` be modified so that it copies all relevant data over when X is ""precomputed""?

This is a bit contrived, but I ran across it while trying to modify `Isomap` to handle non-Euclidean metrics. The issue is that Isomap initializes a `NearestNeighbors` instance with metric equal to 'minkowski' and `p=2`. So if you call `fit` with a precomputed `NearestNeighbors` using a different metric, then this ""fact"" is not transferred to the initialized instance. This can all be avoided if the API to Isomap was changed, but this particular point of `_check_params()` not always doing an effective check still holds.
","['Bug', 'module:neighbors']","2015-02-01T22:33:00Z","8","0","https://github.com/scikit-learn/scikit-learn/issues/4194","Performance Issue"
"268","scikit-learn/scikit-learn","Ensemble models (and maybe others?) don't check for negative sample_weight","When sample weights are negative, the probabilities can come out negative as well:

```
>>> rng = np.random.RandomState(10)
>>> X = rng.randn(10, 4)
>>> y = rng.randint(0, 2, 10)
>>> sample_weight = rng.randn(10)
>>> clf = RandomForestClassifier().fit(X, y, sample_weight)
>>> clf.predict_proba(X)
array([[ 0.56133774,  0.43866226],
       [ 1.03235924, -0.03235924],
       [ 1.03235924, -0.03235924],
       [ 1.03235924, -0.03235924],
       [ 1.03235924, -0.03235924],
       [ 1.03235924, -0.03235924],
       [ 0.98071868,  0.01928132],
       [ 0.56133774,  0.43866226],
       [ 1.03235924, -0.03235924],
       [ 1.03235924, -0.03235924]])
```
","['Bug', 'module:ensemble']","2014-10-15T12:50:51Z","40","0","https://github.com/scikit-learn/scikit-learn/issues/3774","Logical Bug"