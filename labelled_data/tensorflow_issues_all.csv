issue_number,repository,title,description,labels,created_at,comments,reactions,url,classification
1,tensorflow/tensorflow,`gradient_checker.compute_gradient` can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `Floating point exception` issue in TensorFlow when I used API `gradient_checker.compute_gradient`. I have confirmed that below code would crash on tf-nightly 2.19.0-dev20241025 (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1Ow6DQI7g-s7LpKUM1fy8OXbNcfw1ZH7r?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import math

from absl.testing import parameterized
import numpy as np
import tensorflow as tf
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import errors_impl
from tensorflow.python.framework import tensor_shape
from tensorflow.python.framework import test_util
from tensorflow.python.ops import gen_nn_ops
from tensorflow.python.ops import gradient_checker
from tensorflow.python.ops import gradients_impl
from tensorflow.python.ops import nn_ops
import tensorflow.python.ops.nn_grad  # pylint: disable=unused-import
from tensorflow.python.platform import test
from tensorflow.python.platform import tf_logging
from tensorflow.python.util.compat import collections_abc
from tensorflow.python.eager import context
def DtypesToTest(use_gpu):
  # double datatype is currently not supported for convolution ops
  # on the ROCm platform
  optional_float64 = [] if test.is_built_with_rocm() else [dtypes.float64]
  if use_gpu:
    if not test_util.GpuSupportsHalfMatMulAndConv():
      return optional_float64 + [dtypes.float32]
    else:
      # It is important that float32 comes before float16 here,
      # as we will be using its gradients as reference for fp16 gradients.
      return optional_float64 + [dtypes.float32, dtypes.float16]
  else:
    return optional_float64 + [dtypes.float32, dtypes.float16, dtypes.bfloat16]
def _ConstructAndTestGradientForConfig(
    batch, input_shape, filter_shape, in_depth, out_depth, stride,
    padding, test_input, data_format, use_gpu):
  input_planes, input_rows, input_cols = input_shape
  filter_planes, filter_rows, filter_cols = filter_shape
  input_shape = [batch, input_planes, input_rows, input_cols, in_depth]
  filter_shape = [
      filter_planes, filter_rows, filter_cols, in_depth, out_depth
  ]
  if isinstance(stride, collections_abc.Iterable):
    strides = [1] + list(stride) + [1]
  else:
    strides = [1, stride, stride, stride, 1]
  if padding == ""VALID"":
    output_planes = int(
        math.ceil((input_planes - filter_planes + 1.0) / strides[1]))
    output_rows = int(
        math.ceil((input_rows - filter_rows + 1.0) / strides[2]))
    output_cols = int(
        math.ceil((input_cols - filter_cols + 1.0) / strides[3]))
  else:
    output_planes = int(math.ceil(float(input_planes) / strides[1]))
    output_rows = int(math.ceil(float(input_rows) / strides[2]))
    output_cols = int(math.ceil(float(input_cols) / strides[3]))
  output_shape = [batch, output_planes, output_rows, output_cols, out_depth]
  input_size = 1
  for x in input_shape:
    input_size *= x
  filter_size = 1
  for x in filter_shape:
    filter_size *= x
  input_data = [x * 1.0 / input_size for x in range(0, input_size)]
  filter_data = [x * 1.0 / filter_size for x in range(0, filter_size)]
  for data_type in DtypesToTest(use_gpu=use_gpu):
    # TODO(mjanusz): Modify gradient_checker to also provide max relative
    # error and synchronize the tolerance levels between the tests for forward
    # and backward computations.
    if data_type == dtypes.float64:
      tolerance = 1e-8
    elif data_type == dtypes.float32:
      tolerance = 5e-3
    elif data_type == dtypes.float16:
      tolerance = 5e-3 if test.is_built_with_rocm() else 1e-3
    elif data_type == dtypes.bfloat16:
      tolerance = 1e-2
    sess = tf.compat.v1.Session()
    with sess.as_default():
      orig_input_tensor = constant_op.constant(
          input_data, shape=input_shape, dtype=data_type, name=""input"")
      filter_tensor = constant_op.constant(
          filter_data, shape=filter_shape, dtype=data_type, name=""filter"")
      if data_format == ""NCDHW"":
        input_tensor = test_util.NHWCToNCHW(orig_input_tensor)
        new_strides = test_util.NHWCToNCHW(strides)
      else:
        input_tensor = orig_input_tensor
        new_strides = strides
      conv = nn_ops.conv3d(
          input_tensor,
          filter_tensor,
          new_strides,
          padding,
          data_format=data_format,
          name=""conv"")
      jacob_t, jacob_n = gradient_checker.compute_gradient(
          orig_input_tensor, input_shape, conv, output_shape)

with context.graph_mode():
  _ConstructAndTestGradientForConfig(data_format=""NDHWC"",use_gpu=False,batch=2, input_shape=(3, 7, 6), filter_shape=(3, 3, 3), in_depth=2, out_depth=0, stride=3, padding='VALID', test_input=True)
```

### Relevant log output

```shell
Fatal Python error: Floating point exception
```","['type:bug', 'type:support']",2025-02-11T16:38:56Z,0,0,https://github.com/tensorflow/tensorflow/issues/87063,Runtime Error
2,tensorflow/tensorflow,TPU nan issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux (google colab default)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the 2.18.0 Tensorflow version, occur only when using TPU:
- At the middle of any epoch, loss turns out to be nan for the rest of the epoch 

### Standalone code to reproduce the issue

```shell
Shortest way: connect and connect to the colab tutorial on TPU, i.e. https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/tpu.ipynb#scrollTo=Tce3stUlHN0L
If the issue hasn't been fixed, the training loop will produce nan loss
```

### Relevant log output

```shell
Epoch 1/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 22s 47ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 2/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 3/5
300/300 ━━━━━━━━━━━━━━━━━━━━ 9s 30ms/step - loss: nan - sparse_categorical_accuracy: nan - val_loss: nan - val_sparse_categorical_accuracy: nan
Epoch 4/5
231/300 ━━━━━━━━━━━━━━━━━━━━ 1s 23ms/step - loss: nan - sparse_categorical_accuracy: nan
```","['type:bug', 'comp:tpus', 'TF 2.18']",2025-02-10T15:03:57Z,2,0,https://github.com/tensorflow/tensorflow/issues/86953,Logical Bug
3,tensorflow/tensorflow,"`tf.summary_ops.write` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.write` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/13pz-hZNK_BaOnbYo_9CT2YMdoncPXEA3?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import summary_ops_v2 as summary_ops
from tensorflow.python.ops import variables
writer = summary_ops.create_file_writer_v2(""/tmp"")
mystep = variables.Variable(1, dtype=dtypes.int64)
with writer.as_default(step=[3, 0, 0, 2]):
    summary_ops.write('tag', 1.0)
```

### Relevant log output

```shell
2025-02-09 04:47:35.482562: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor
Aborted (core dumped)
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-09T04:49:39Z,1,0,https://github.com/tensorflow/tensorflow/issues/86918,Runtime Error
4,tensorflow/tensorflow,"`tf.summary_ops.run_metadata_graphs` aborts with ""Check failed: 1 == NumElements() (1 vs. 4)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.summary_ops.run_metadata_graphs` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1qCZb2sPj2l79IecGA1tz9CywtNlevAjf?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.ops import summary_ops_v2 as summary_ops
from tensorflow.core.protobuf import config_pb2
writer = summary_ops.create_file_writer_v2(""/tmp"")
meta = config_pb2.RunMetadata()
with writer.as_default([3, 0, 0, 2]):
    summary_ops.run_metadata_graphs(name='my_name', data=meta)
```

### Relevant log output

```shell
2025-02-09 04:39:36.666278: F tensorflow/core/framework/tensor.cc:865] Check failed: 1 == NumElements() (1 vs. 4)Must have a one element tensor
Aborted (core dumped)
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-09T04:42:49Z,1,0,https://github.com/tensorflow/tensorflow/issues/86917,Runtime Error
5,tensorflow/tensorflow,"`io_ops.restore_v2` aborts with ""Check failed: size >= 0 (0 vs. -3) ""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.19.0-dev20250207

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `tf.io_ops.restore_v2` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20250207` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/17XwNF4WI3HqVJkLwk2VQjcTDAV42-6Bl?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.ops import io_ops

dtype = dtypes.uint4
with ops.Graph().as_default():
    op = io_ops.restore_v2('model', ['var1', 'var2'], ['', '-3 4 0,1:-'], [dtype, dtype])
```

### Relevant log output

```shell
2025-02-09 04:25:19.857968: F tensorflow/core/framework/tensor_shape.cc:413] Check failed: size >= 0 (0 vs. -3) 
Aborted (core dumped)
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-09T04:34:07Z,1,0,https://github.com/tensorflow/tensorflow/issues/86916,Runtime Error
6,tensorflow/tensorflow,Bug sur TensorFlow 2.13 multi-GPU : freeze lors du fitting,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Macos 15.3 (worker 0) Macos 12.7.6(worker 1)

### Mobile device

_No response_

### Python version

3.8.20

### Bazel version

...

### GCC/compiler version

16.0.0 (apple M3) 14.0.0 (intel iris)

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M3 and Intel Iris Graphics 6100

### Current behavior?

When I train a TensorFlow model on several GPUs (with tf.distribute.MultiWorkerMirroredStrategy), the execution is blocked at  #Build the model under the strategy

No error message is displayed, but the process no longer progresses after

•	 I followed the recommendations of the official documentation, but the problem persists.

### Standalone code to reproduce the issue

```shell
import json
import os
import numpy as np
import tensorflow as tf

TF_CONFIG = {
    ""cluster"": {
        ""worker"": [""192.168.0.68:12345"", ""192.168.0.68:12346""]
    },
    ""task"": {""type"": ""worker"", ""index"": 0}  # Modifier index pour chaque worker
}

os.environ[""TF_CONFIG""] = json.dumps(TF_CONFIG)

# Manually Load the MNIST dataset
data = np.load(""mnist.npz"")
x_train, y_train = data[""x_train""], data[""y_train""]
x_test, y_test = data[""x_test""], data[""y_test""]

# Normalize images
x_train, x_test = x_train / 255.0, x_test / 255.0

# Add a dimension to match TensorFlow's expectations
x_train = x_train[..., np.newaxis]
x_test = x_test[..., np.newaxis]

# Define the distribution strategy
strategy = tf.distribute.MultiWorkerMirroredStrategy()

# Build the model under the strategy
with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D((2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))

test_loss, test_acc = model.evaluate(x_test, y_test)
print(f""Test accuracy: {test_acc:.4f}"")
```

### Relevant log output

```shell
2025-02-08 12:29:20.630247: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3
2025-02-08 12:29:20.630286: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB
2025-02-08 12:29:20.630293: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB
2025-02-08 12:29:20.630324: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-02-08 12:29:20.630338: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-02-08 12:29:20.631249: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-02-08 12:29:20.631259: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-02-08 12:29:20.632347: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:449] Started server with target: grpc://192.168.0.68:12345
2025-02-08 12:29:20.637550: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 14287335759644278642
2025-02-08 12:29:20.637654: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:298] Coordination agent has successfully connected.
2025-02-08 12:29:37.728182: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:535] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 15227140312468372989
```","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.13']",2025-02-08T11:40:33Z,1,0,https://github.com/tensorflow/tensorflow/issues/86897,Runtime Error
7,tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.Rsqrt``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ``tf.raw_ops.Rsqrt``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x_0 = tf.constant([[[1.2031, 0.2168], [0.1177, 0.6758]], 
                    [[1.8203, 0.9688], [0.8828, 0.0767]]], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.Rsqrt(x=x_0)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.Rsqrt(x=x_0)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor(
[[[0.910156 2.14062]
  [2.92188 1.21875]]

 [[0.742188 1.01562]
  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)

Output on GPU: tf.Tensor(
[[[0.914062 2.15625]
  [2.90625 1.21875]]

 [[0.738281 1.01562]
  [1.0625 3.60938]]], shape=(2, 2, 2), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-05T06:12:29Z,1,0,https://github.com/tensorflow/tensorflow/issues/86607,Logical Bug
8,tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.Tan``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

```tf.raw_ops.Tan``` produces inconsistent results between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

real = tf.constant([1.5634333], dtype=tf.float32)
imag = tf.constant([0.020735], dtype=tf.float32)

complex_tensor = tf.complex(real, imag)

with tf.device('/CPU:0'):
    result_cpu = tf.raw_ops.Tan(x=complex_tensor)
    print(result_cpu)

with tf.device('/GPU:0'):
    result_gpu = tf.raw_ops.Tan(x=complex_tensor)
    print(result_gpu)

##Comparing whole complex numbers
max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_cons = tf.experimental.numpy.allclose(result_cpu, result_gpu, rtol=1e-6,  atol=1e-5)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:"", is_cons.numpy())

##Comparing by parts
real_part_cpu = tf.math.real(result_cpu)
real_part_gpu = tf.math.real(result_gpu)
real_part_diff = tf.reduce_max(tf.abs(real_part_cpu - real_part_gpu)).numpy()
real_part_cons = tf.experimental.numpy.allclose(real_part_cpu, real_part_gpu, rtol=1e-6,  atol=1e-5)

imag_part_cpu = tf.math.imag(result_cpu)
imag_part_gpu = tf.math.imag(result_gpu)
imag_part_diff = tf.reduce_max(tf.abs(imag_part_cpu - imag_part_gpu)).numpy()
imag_part_cons = tf.experimental.numpy.allclose(imag_part_cpu, imag_part_gpu, rtol=1e-6,  atol=1e-5)

print(""Real parts absolute difference:"", real_part_diff)
print(""Real parts Consistency check with atol=1e-5 and rtol=1e-6:"", real_part_cons.numpy())

print(""Imag parts absolute difference:"", imag_part_diff)
print(""Imag parts Consistency check with atol=1e-5 and rtol=1e-6:"", imag_part_cons.numpy())
```

### Relevant log output

```shell
tf.Tensor([15.205576+42.834145j], shape=(1,), dtype=complex64)
tf.Tensor([15.205605+42.834225j], shape=(1,), dtype=complex64)

Max absolute difference: 8.5064334e-05
Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False

Real parts absolute difference: 2.861023e-05
Real parts Consistency check with atol=1e-5 and rtol=1e-6: False

Imag parts absolute difference: 8.010864e-05
Imag parts Consistency check with atol=1e-5 and rtol=1e-6: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-04T03:18:15Z,2,0,https://github.com/tensorflow/tensorflow/issues/86506,Logical Bug
9,tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.LogSoftmax``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ``tf.raw_ops.LogSoftmax``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

logits = tf.constant([[0.0664, -2.3906]], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.LogSoftmax(logits=logits)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.LogSoftmax(logits=logits)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor([[-0.0825195 -2.53125]], shape=(1, 2), dtype=bfloat16)

Output on GPU: tf.Tensor([[-0.0825195 -2.54688]], shape=(1, 2), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-03T03:30:58Z,1,0,https://github.com/tensorflow/tensorflow/issues/86434,Logical Bug
10,tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.L2Loss``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 (google colab)

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

result of ```tw.raw_ops.L2Loss``` is inconsistent between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

t = tf.constant([
    [[0.9922, -1.4922], 
     [0.0376,  0.1504], 
     [0.6172,  1.2266]],

    [[-0.1387,  1.3047], 
     [0.3535, -0.0471], 
     [0.0437,  0.2637]]
], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.L2Loss(t=t)
  print(""Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.L2Loss(t=t)
  print(""Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
Output on CPU: tf.Tensor(3.51562, shape=(), dtype=bfloat16)

Output on GPU: tf.Tensor(3.53125, shape=(), dtype=bfloat16)

Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-02T07:05:17Z,2,0,https://github.com/tensorflow/tensorflow/issues/86406,Logical Bug
11,tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.BiasAddGrad``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

```tf.raw_ops.BiasAddGrad``` produces inconsistent results between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

out_backprop = tf.constant([
    [
        [
            [[ 0.2207,  2.1094], [-0.3730, -1.0625], [ 1.7031,  0.7148]], 
            [[ 1.5078, -0.6719], [-0.6367,  0.5039], [-2.3281,  0.5078]]
        ],
        [
            [[-0.3574,  0.0461], [ 2.3750, -2.9688], [-0.5703, -2.0156]],
            [[ 0.8125,  1.7656], [-0.9570,  0.6250], [-0.6914, -0.4746]]
        ],
        [
            [[-0.3750, -0.7383], [ 0.3691,  0.4570], [ 1.1641,  0.2715]],
            [[-1.2969, -0.9844], [-0.4863,  1.0938], [-1.4297,  0.8086]]
        ]
    ],
    [
        [
            [[ 0.3730,  0.8477], [-0.3887,  1.2266], [ 0.0859, -0.5742]],
            [[-0.7383, -0.2432], [-0.7578, -0.8281], [-0.1660, -0.9336]]
        ],
        [
            [[ 1.4297,  0.6797], [-1.6172,  0.4941], [-0.3047, -0.3711]],
            [[-0.6250, -0.7617], [ 0.9453,  0.1064], [ 1.4062, -2.9531]]
        ],
        [
            [[-1.4297, -0.1387], [ 0.0625,  1.0469], [-0.1953,  1.6406]],
            [[-0.3047,  0.5117], [ 1.8125,  1.1797], [-0.8789, -0.4688]]
        ]
    ]
], dtype=tf.bfloat16)

with tf.device('CPU:0'):
  result_cpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")
  print(""BiasAddGrad Output on CPU:"", result_cpu)

with tf.device('GPU:0'):
  result_gpu = tf.raw_ops.BiasAddGrad(out_backprop=out_backprop, data_format=""NCHW"")
  print(""BiasAddGrad Output on GPU:"", result_gpu)

max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
BiasAddGrad Output on CPU: tf.Tensor([0.09375 -3.96875 1.70312], shape=(3,), dtype=bfloat16)

BiasAddGrad Output on GPU: tf.Tensor([0.078125 -4 1.70312], shape=(3,), dtype=bfloat16)

Max absolute difference: 0.03125
Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2025-02-01T10:43:06Z,5,0,https://github.com/tensorflow/tensorflow/issues/86378,Logical Bug
12,tensorflow/tensorflow,inconsistent result of ```tf.raw_ops.BatchMatMulV2``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

getting inconsistent results of ```tf.raw_ops.BatchMatMulV2``` between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x_0 = tf.constant([
    [[[[ -1.3594, -0.3027], [-1.4141,  0.2969]],
      [[ -0.9141,  1.7812], [ 1.2266,  0.8594]]],

     [[[  0.8359, -0.9414], [-1.7969, -0.7461]],
      [[  0.3164,  0.3691], [ 0.7656,  0.2354]]]],

    [[[[ -0.5898,  1.3516], [ 0.4902, -0.1045]],
      [[ -0.1099,  1.5078], [ 0.2852, -0.0957]]],

     [[[-0.9883,  1.3203], [-0.2715, -1.7578]],
      [[ -0.1602, -0.4336], [-0.6875, -0.4492]]]]
], dtype=tf.bfloat16)

y = tf.constant([
    [[[  0.6836, -0.6562], [-0.5508, -0.8438]], 
     [[  1.6094, -0.9883], [-0.1318,  1.1094]]],

    [[[  0.4062, -1.1094], [-0.7188, -1.7578]], 
     [[ -1.0391, -0.6602], [ 0.8359, -0.6562]]]
], dtype=tf.bfloat16) 

with tf.device('CPU:0'):
    result_cpu = tf.raw_ops.BatchMatMulV2(
        x=x_0, 
        y=y,
    )
    print(result_cpu)

with tf.device('GPU:0'):
    result_gpu = tf.raw_ops.BatchMatMulV2(
        x=x_0, 
        y=y,
    )
    print(result_gpu)


max_abs_diff = tf.reduce_max(tf.abs(result_cpu - result_gpu)).numpy()

is_consistent = tf.experimental.numpy.allclose(tf.cast(result_cpu, tf.float32), tf.cast(result_gpu, tf.float32), rtol=1e-3,  atol=1e-2)

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3:"", is_consistent.numpy())
```

### Relevant log output

```shell
tf.Tensor(
[[[[[-0.761719 1.14062]
    [-1.125 0.675781]]

   [[-1.70312 2.875]
    [1.85938 -0.257812]]]


  [[[1.01562 0.726562]
    [-0.193359 3.29688]]

   [[-0.0201416 -0.449219]
    [-0.597656 -0.65625]]]]



 [[[[-1.14062 -0.75]
    [0.392578 -0.233398]]

   [[-0.375 1.78125]
    [0.470703 -0.386719]]]


  [[[-1.34375 -1.21875]
    [1.14844 3.39062]]

   [[-0.195312 0.388672]
    [0.337891 0.746094]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)

tf.Tensor(
[[[[[-0.761719 1.14844]
    [-1.13281 0.675781]]

   [[-1.70312 2.875]
    [1.85938 -0.259766]]]


  [[[1.01562 0.726562]
    [-0.193359 3.3125]]

   [[-0.0201416 -0.451172]
    [-0.597656 -0.660156]]]]



 [[[[-1.14844 -0.753906]
    [0.392578 -0.233398]]

   [[-0.375 1.78125]
    [0.470703 -0.388672]]]


  [[[-1.35156 -1.22656]
    [1.15625 3.39062]]

   [[-0.196289 0.390625]
    [0.337891 0.75]]]]], shape=(2, 2, 2, 2, 2), dtype=bfloat16)


Max absolute difference: 0.015625

Consistency check (CPU vs GPU) with atol=1e-2 and rtol=1e-3: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-02-01T07:13:36Z,2,0,https://github.com/tensorflow/tensorflow/issues/86350,Logical Bug
13,tensorflow/tensorflow,Stateful LSTM bug with batch size,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.18

### Custom code

No

### OS platform and distribution

Windows 11

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running a stateful LSTM one needs to fix a batch size. Previously this was achieved with the `batch_input_shape=(batch_size, sequence_length, num_features)` argument, but with the most recent version of TF batch_input_shape is not recognised as valid. 
Basically the error is the same as this https://github.com/tensorflow/tensorflow/issues/64061

Below is some LLM generated code to reproduce the behaviour. Also, the `model.reset_states()` bit appears broken.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

# Set a fixed batch size
batch_size = 32

# Create some random training data
# We'll have sequences of length 5, with 1 feature per time step
sequence_length = 5
num_features = 1
num_samples = 100  # Total number of samples (must be divisible by batch_size)

# Ensure num_samples is a multiple of batch_size
num_samples = (num_samples // batch_size) * batch_size

X_train = np.random.rand(num_samples, sequence_length, num_features)
y_train = np.random.rand(num_samples, 1)  # Example target values

# Reshape y_train to match expected output shape if needed
y_train = y_train.reshape(-1,1)

# Create the stateful LSTM model
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.LSTM(units=64,  # Number of LSTM units
                               batch_input_shape=(batch_size, sequence_length, num_features),
                               stateful=True,
                               return_sequences=False)) #often false for a final prediction

model.add(tf.keras.layers.Dense(units=1)) # Output layer with 1 unit

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
epochs = 10

for epoch in range(epochs):
    # Shuffle data indices for each epoch (important for stateful LSTMs)
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    X_train = X_train[indices]
    y_train = y_train[indices]

    model.fit(X_train, y_train, batch_size=batch_size, epochs=1, shuffle=False) # Shuffle must be false

    # Reset states after each epoch (essential for stateful LSTMs)
    model.reset_states()
```

### Relevant log output

```shell

```","['stat:awaiting response', 'type:bug', 'comp:keras', 'TF 2.18']",2025-01-31T18:07:07Z,5,0,https://github.com/tensorflow/tensorflow/issues/86310,Syntax Error
14,tensorflow/tensorflow,inconsistent result of ```tf.image.adjust_hue``` on CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.5/9

### GPU model and memory

Tesla T4

### Current behavior?

getting inconsistent results of ```tf.image.adjust_hue``` between CPU and GPU

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

images = tf.constant([
    [[ 1.9720840,  2.1302242, -0.1902120],
     [ 0.6557856, -1.3016001,  1.1452782]],
    
    [[-2.2193234,  0.3198028,  0.9568117],
     [-0.3937407, -0.0503466, -0.3693791]]
], dtype=tf.float32)

delta = tf.constant(-0.7441734, dtype=tf.float32)

with tf.device('CPU:0'):
    adjusted_cpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on CPU:\n"", adjusted_cpu)

with tf.device('GPU:0'):
    adjusted_gpu = tf.image.adjust_hue(images, delta)
    print(""Adjusted Hue on GPU:\n"", adjusted_gpu)


is_consistent = tf.experimental.numpy.allclose(adjusted_cpu, adjusted_gpu, atol=1e-5, rtol=1e-6)

max_abs_diff = tf.reduce_max(tf.abs(adjusted_cpu - adjusted_gpu)).numpy()

print(""Max absolute difference:"", max_abs_diff)
print(""Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6:"", is_consistent.numpy())
```

### Relevant log output

```shell
Adjusted Hue on CPU:
 tf.Tensor(
[[[-0.190212    2.1302242   1.2092681 ]
  [ 1.1452782  -0.48211157 -1.3016001 ]]

 [[ 0.11679006 -2.2193234   0.9568117 ]
  [-0.3937407  -0.25841027 -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Adjusted Hue on GPU:
 tf.Tensor(
[[[-0.19021209  2.1302242   1.209268  ]
  [ 1.1452781  -0.48211193 -1.3016001 ]]

 [[ 0.11678863 -2.2193234   0.95681167]
  [-0.0503466  -0.0503466  -0.0503466 ]]], shape=(2, 2, 3), dtype=float32)

Max absolute difference: 0.3433941
Consistency check (CPU vs GPU) with atol=1e-5 and rtol=1e-6: False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-01-31T07:40:34Z,4,1,https://github.com/tensorflow/tensorflow/issues/86256,Logical Bug
15,tensorflow/tensorflow,Buffer allocation error in Tensorflow Lite with OpenCL backend on certain platforms,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

macOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I noticed a memory allocation error in clCreateBuffer. The issue seems to be caused by this:

1) TFlite tries to alloca 0xa2000000 bytes of memory (value stored as size_t)

2) The call ends up in this function (tensorflow/lite/experimental/litert/runtime/opencl/buffer.cc):

```
absl::Status CreateClBuffer(cl_context context, int size_in_bytes,
                            bool read_only, void* data, cl_mem* result) 
```

where the size is now int (i.e. 32 bit signed integer... so 0xa2000000 is interpreted as a negative value).

3) This function then calls clCreateBuffer, which takes the size argument as size_t again, and thus receives 0xffffffffa2000000, i.e. the signed 32 bit integer first sign extended to 64bit and then interpreted as unsigned, and thus resulting in a huge size.

The issue doesn't seem to appear with the same model on Android, probably because: The max buffer allocation size on macOS (M1) seems to be 9GB (according to clinfo), but on that android device it's only 1GB (so on android tflite never tries to allocate such a huge chunk of memory).

### Standalone code to reproduce the issue

```shell
Unfortunately I'm not allowed to share the code/model, but looking at the function signatures one can see the issue.
```

### Relevant log output

```shell
ERROR: Failed to allocate device memory (clCreateBuffer): Invalid buffer size
```","['type:bug', 'comp:lite', 'TF 2.13']",2025-01-29T12:11:28Z,0,0,https://github.com/tensorflow/tensorflow/issues/86048,Runtime Error
16,tensorflow/tensorflow,TensorFlow warning shows whenever importing it,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802

### Custom code

No

### OS platform and distribution

Linux Ubuntu 24.10 x86_64

### Mobile device

_No response_

### Python version

3.12.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA:12.6

### GPU model and memory

_No response_

### Current behavior?

`cuFFT`, `cuDNN` and `cuBLAS` warnings showing after importing `TensorFlow`

- OS: Ubuntu 24.10 x86_64
- Host: G5 5590
- Kernel: 6.11.0-13-generic
- CPU: Intel i7-9750H (12) @ 4.500GHz
- GPU: NVIDIA GeForce GTX 1650 Mobile / Max-Q
- GPU: Intel CoffeeLake-H GT2 [UHD Graphics 630]
> whenever running the following code it gives that warning also it outputs the predicted output but after the warning:
```python
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```
> output:
```
2025-01-23 21:08:06.468437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737659286.484845  763412 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737659286.489647  763412 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-23 21:08:06.505984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```
> also i know that for that warning (`cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`) i must rebuild the tensor-flow from binaries enabling the AVX2 and FMA instructions but what about the others?

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
```

### Relevant log output

```shell

```","['type:bug', '2.18.rc']",2025-01-23T21:56:39Z,3,0,https://github.com/tensorflow/tensorflow/issues/85604,UI/UX Bug
17,tensorflow/tensorflow,"Tutorial ""Multi-worker training with Keras"" fails to complete","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v1.12.1-120353-gc5bd67bc56f 2.19.0-dev20250107

### Custom code

No

### OS platform and distribution

Debian 6.1.123-1 (2025-01-02) x86_64 GNU/Linux

### Mobile device

_No response_

### Python version

Python 3.12.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the tutorial everything goes well until you start the second worker. Then the below failure occures.

2025-01-20 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-20 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-20 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-20 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-01-20 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""-1""
2025-01-20 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA
2025-01-20 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-01-20 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael
2025-01-20 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael
2025-01-20 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0
2025-01-20 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0
2025-01-20 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0
2025-01-20 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345
2025-01-20 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635
2025-01-20 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect.
2025-01-20 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers:
/job:worker/replica:0/task:1
I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.
2025-01-20 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870
2025-01-20 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect.
/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
  warnings.warn(
2025-01-20 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
Traceback (most recent call last):
  File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in <module>
    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Attempt to convert a value (PerReplica:{
  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=
array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>
}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.

### Standalone code to reproduce the issue

```shell
python main.py &> job_1.log
```

### Relevant log output

```shell
2025-01-20 07:19:35.283801: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-20 07:19:35.290192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1737379175.297785    4595 cuda_dnn.cc:8501] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1737379175.300054    4595 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-20 07:19:35.307721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-20 07:19:36.510476: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2025-01-20 07:19:36.510494: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:167] env: CUDA_VISIBLE_DEVICES=""-1""
2025-01-20 07:19:36.510499: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] CUDA_VISIBLE_DEVICES is set to -1 - this hides all GPUs from CUDA
2025-01-20 07:19:36.510501: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:178] verbose logging is disabled. Rerun with verbose logging (usually --v=1 or --vmodule=cuda_diagnostics=1) to get more diagnostic output from this module
2025-01-20 07:19:36.510505: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:183] retrieving CUDA diagnostic information for host: michael
2025-01-20 07:19:36.510507: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:190] hostname: michael
2025-01-20 07:19:36.510562: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:197] libcuda reported version is: 565.77.0
2025-01-20 07:19:36.510572: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:201] kernel reported version is: 565.77.0
2025-01-20 07:19:36.510574: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:291] kernel version seems to match DSO: 565.77.0
2025-01-20 07:19:36.519175: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:637] Initializing CoordinationService
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1737379176.519611    4595 grpc_server_lib.cc:465] Started server with target: grpc://localhost:12345
2025-01-20 07:19:36.524874: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 4677280066871850635
2025-01-20 07:19:36.524894: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 1/2 tasks to connect.
2025-01-20 07:19:36.524898: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:819] Example stragglers:
/job:worker/replica:0/task:1
I0000 00:00:1737379176.525022    4595 coordination_service_agent.cc:369] Coordination agent has successfully connected.
2025-01-20 07:22:27.996664: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:378] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 13530699364709055870
2025-01-20 07:22:27.996686: I external/local_xla/xla/tsl/distributed_runtime/coordination/coordination_service.cc:816] Waiting for 0/2 tasks to connect.
/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.
  warnings.warn(
2025-01-20 07:22:28.461733: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.
Traceback (most recent call last):
  File ""/home/chad/Documents/McCueFiles/NeuralNetworks/TensorFlowProject/TensorFlowDocExample/main.py"", line 21, in <module>
    multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/chad/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Attempt to convert a value (PerReplica:{
  0: <tf.Tensor: shape=(64, 28, 28), dtype=float32, numpy=
array([[[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       ...,

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]],

       [[0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        ...,
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.],
        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>
}) with an unsupported type (<class 'tensorflow.python.distribute.values.PerReplica'>) to a Tensor.
```","['type:bug', 'TF 2.18']",2025-01-20T14:03:18Z,2,0,https://github.com/tensorflow/tensorflow/issues/85351,Runtime Error
18,tensorflow/tensorflow,Aborted  in `tf.raw_ops.RaggedGather`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.raw_ops.RaggedGather` triggers crash.

### Standalone code to reproduce the issue

```shell
params_nested_splits = tf.constant(0, shape=[3], dtype=tf.int64)
params_dense_values = tf.constant(1, shape=[0], dtype=tf.float32)
indices = tf.constant(0, shape=[], dtype=tf.int64)
OUTPUT_RAGGED_RANK = 1
PARAMS_RAGGED_RANK = 1

tf.raw_ops.RaggedGather(
    params_nested_splits=[params_nested_splits],
    params_dense_values=params_dense_values,
    indices=indices,
    OUTPUT_RAGGED_RANK=1,
    name=None
)
```

### Relevant log output

```shell
2025-01-18 09:30:00.549762: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (9 vs. 1) float expected, got int64
Aborted (core dumped)
```","['type:bug', 'comp:ops', '2.17']",2025-01-18T09:32:16Z,1,0,https://github.com/tensorflow/tensorflow/issues/85242,Runtime Error
19,tensorflow/tensorflow,Segmentation fault (core dumped) in `RaggedTensorToTensor`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs, `tf.raw_ops.RaggedTensorToTensor` triggers crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

shape = tf.constant(-1, shape=[], dtype=tf.int64)
values = tf.constant(0, shape=[0], dtype=tf.int32)
default_value = tf.constant(0, shape=[], dtype=tf.int32)
row_partition_tensors = tf.constant([0, 1, 6], shape=[3], dtype=tf.int64)
row_partition_types = [""ROW_SPLITS""]

tf.raw_ops.RaggedTensorToTensor(
    shape=shape,
    values=values,
    default_value=default_value,
    row_partition_tensors=[row_partition_tensors],
    row_partition_types=row_partition_types)
```

### Relevant log output

```shell
Segmentation fault (core dumped)
```","['type:bug', 'comp:ops', '2.17']",2025-01-18T09:27:19Z,1,0,https://github.com/tensorflow/tensorflow/issues/85240,Runtime Error
20,tensorflow/tensorflow,Seg Fault when iterate dataset created from data service,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Segfault when trying to iterate dataset get from data service.

### Standalone code to reproduce the issue

```shell
# start the data service file start_dataservice.py

import tensorflow as tf

dispatcher = tf.data.experimental.service.DispatchServer(
    tf.data.experimental.service.DispatcherConfig(port=50050), start=True
)
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True
)
print(""Starting Worker"")
worker.join()

# test file test_dataset_service.py
import tensorflow as tf
import numpy as np


flags = tf.compat.v1.app.flags

flags.DEFINE_bool(""local"", False, ""Run data service in process"")
flags.DEFINE_bool(""distribute"", False, ""Run data service in distributed_epoch mode"")
FLAGS = flags.FLAGS


def local_service():
    print(""Starting Local Service"")
    dispatcher = tf.data.experimental.service.DispatchServer(
        tf.data.experimental.service.DispatcherConfig(port=50050), start=True
    )
    dispatcher_address = dispatcher.target.split(""://"")[1]
    worker = tf.data.experimental.service.WorkerServer(
        tf.data.experimental.service.WorkerConfig(dispatcher_address=dispatcher_address), start=True
    )
    print(""Dispatcher target is "", dispatcher.target)
    return dispatcher, worker, dispatcher.target


def apply_transformations(ds_train):
    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    ds_train = ds_train.cache()
    ds_train = ds_train.shuffle(60000)
    ds_train = ds_train.batch(128)
    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)
    return ds_train


(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()
x_train = x_train / np.float32(255)
y_train = y_train.astype(np.int64)
ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))


def normalize_img(image, label):
    """"""Normalizes images: `uint8` -> `float32`.""""""
    return tf.cast(image, tf.float32) / 255.0, label


ds_train = apply_transformations(ds_train)
# Create dataset however you were before using the tf.data service.
dataset = ds_train
if FLAGS.local:
    dispatcher, worker, service = local_service()
else:
    dispatcher_address = ""localhost""
    dispatcher_port = ""50050""
    service = ""grpc://{}:{}"".format(dispatcher_address, dispatcher_port)
if FLAGS.distribute:
    processing_mode = ""distributed_epoch""
else:
    processing_mode = ""parallel_epochs""

# This will register the dataset with the tf.data service cluster so that
# tf.data workers can run the dataset to produce elements. The dataset returned
# from applying `distribute` will fetch elements produced by tf.data workers.
dataset = dataset.apply(
    tf.data.experimental.service.distribute(processing_mode=processing_mode, service=service)
)

for (x1, y1), (x2, y2) in zip(dataset, ds_train):
    np.allclose(x1, x2)
    np.allclose(y1, y2)

print(""verified mnist dataset locally vs over service"")

# script to run 
python -m pip install --upgrade pip
python -m pip install tensorflow==2.18.0
python -m pip install 'protobuf<4'
screen -d -m python start_dataservice.py
python3 test_dataset_service.py --local=False
```

### Relevant log output

```shell
2025-01-14 21:56:19.778399: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736891779.795141    9168 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1736891779.800177    9168 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-14 21:56:19.815971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
I0000 00:00:1736891783.518634    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 889 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1c.0, compute capability: 8.0
I0000 00:00:1736891783.520395    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 37945 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:10:1d.0, compute capability: 8.0
I0000 00:00:1736891783.522012    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 37945 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1c.0, compute capability: 8.0
I0000 00:00:1736891783.523626    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 37945 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:20:1d.0, compute capability: 8.0
I0000 00:00:1736891783.525222    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 37945 MB memory:  -> device: 4, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1c.0, compute capability: 8.0
I0000 00:00:1736891783.526807    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 37945 MB memory:  -> device: 5, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:90:1d.0, compute capability: 8.0
I0000 00:00:1736891783.528377    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 37945 MB memory:  -> device: 6, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1c.0, compute capability: 8.0
I0000 00:00:1736891783.529933    9168 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 37945 MB memory:  -> device: 7, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:a0:1d.0, compute capability: 8.0
/test/bin/testDataservice: line 5:  9168 Segmentation fault      (core dumped) python ${BIN_DIR}/test_dataset_service.py --local=False
```","['type:bug', 'comp:ops', 'TF 2.18']",2025-01-14T21:57:20Z,0,0,https://github.com/tensorflow/tensorflow/issues/84897,Runtime Error
21,tensorflow/tensorflow,GPU Profiling: MemoryProfile do not contain memory events when profile remote worker.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

nightly

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Python version

Python 3.12

### CUDA/cuDNN version

CUDA 12.4

### GPU model and memory

A100 80GB

### Current behavior?

Start a simple any collective training with Tensorflow cluster config. And then use RPC client capture_profile in Tensorboard or tf.profiler.experimental.client.trace. 
No any memory profile events or OP profiler, but only trace view.

### Standalone code to reproduce the issue

**tf_allreduce.py**
```python
import tensorflow as tf
from tensorflow.python.ops.collective_ops import all_reduce, all_reduce_v2
from tensorflow.python.eager import context
from tensorflow.core.protobuf import config_pb2
from tensorflow.python.distribute import cluster_resolver as cluster_resolver_lib

cluster_resolver = cluster_resolver_lib.TFConfigClusterResolver()
cluster = cluster_resolver.cluster_spec()
task_type = cluster_resolver.task_type
task_id = cluster_resolver.task_id

experimental_config = config_pb2.ConfigProto.Experimental(
    share_cluster_devices_in_session=False,
    share_session_state_in_clusterspec_propagation=False
)
config = config_pb2.ConfigProto(experimental=experimental_config)
config.experimental.collective_group_leader = '/job:worker/replica:0/task:0'
server = tf.distribute.Server(cluster,
                              job_name=task_type,
                              task_index=task_id,
                              protocol=""grpc"", # ""grpc+verbs""
                              config=config)
run_options = config_pb2.RunOptions()

with tf.compat.v1.Session(target=server.target, config=config) as sess:
    tensor = tf.Variable(tf.ones([2, 2]), dtype=tf.float32)
    init = tf.compat.v1.global_variables_initializer()
    sess.run(init)
    sess.run(tf.print([""tensor:"",tensor]))

    reduced_tensor = all_reduce(tensor, group_size=2, group_key=4321, instance_key=1234, merge_op='Add', final_op='Id', communication_hint='auto')
    run_options.experimental.collective_graph_key = 6
    while True:
        sess.run(tf.print([""reduced_tensor:"",reduced_tensor]), options=run_options)
```

Run script to start server.
```bash
CUDA_VISIBLE_DEVICES=0 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":0}}' python tf_allreduce.py&
CUDA_VISIBLE_DEVICES=1 TF_CONFIG='{""cluster"":{""worker"":[""localhost:2223"",""localhost:2224""]},""task"":{""type"":""worker"",""index"":1}}' python tf_allreduce.py&
```

 use capture_profile in Tensorboard or tf.profiler.experimental.client.trace.
```python
tf.profiler.experimental.client.trace(
  'grpc://localhost:2223,grpc://localhost:2224',
   '/tmp/my_tb_dir',
   2000,
)
```

Try to convert xplane.pb to memory_profile, nothing show.
```python
from tensorflow.python.profiler.internal import _pywrap_profiler as profiler_wrapper
json = profiler_wrapper.xspace_to_tools_data([""xxx.xplane""], ""memory_profile"")
```

**Relevant log output**
```
{""memoryProfilePerAllocator"":{},""numHosts"":1,""memoryIds"":[]}
```

Relative issue: #48146 ","['type:bug', 'comp:gpu', 'TF 2.18']",2025-01-09T09:26:20Z,0,0,https://github.com/tensorflow/tensorflow/issues/84460,Logical Bug
22,tensorflow/tensorflow,Unable to connect to TPU through Cloud VM (metadata issue?),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

tpu-ubuntu2204-base

### Mobile device

_No response_

### Python version

3.11.2

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am on a VM instance trying to connect to a tpu v4-32 using a test script. I installed tensorflow-tpu on both the VM (in venv) and TPU (globally) as per the instructions from the google website.

It seems like there is an issue with getting TPU metadata.

It is able to connect to the metadata server when I request manually from the VM:

```
$ curl http://169.254.169.254/computeMetadata/v1/ -H ""Metadata-Flavor: Google""
instance/
oslogin/
project/
```

Any help would be appreciated!

### Standalone code to reproduce the issue

```shell
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=tpu_name)
tf.config.experimental_connect_to_cluster(resolver)
try:
    tf.tpu.experimental.initialize_tpu_system(resolver)
    print(""TPU initialized:"", resolver.master())
except Exception as e:
    print(""Failed to initialize TPU:"", e)
```


### Relevant log output

```shell
$ python hello.py
2025-01-08 23:49:33.189260: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-01-08 23:49:33.221197: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:95] Opening library: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2
2025-01-08 23:49:33.221290: I tensorflow/core/tpu/tpu_api_dlsym_initializer.cc:121] Libtpu path is: /home/ucsdwanglab/test_tpu/.venv/lib/python3.11/site-packages/libtpu/libtpu.so
Failed to get TPU metadata (tpu-env) from instance metadata for variable CHIPS_PER_HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable HOST_BOUNDS: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable ALT: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (tpu-env) from instance metadata for variable WRAP: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93
learning/45eac/tfrc/runtime/env_var_utils.cc:50

Failed to get TPU metadata (accelerator-type) from instance metadata for variable TPU_ACCELERATOR_TYPE: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

Failed to find host bounds for accelerator type: WARNING: could not determine TPU accelerator type, please set env var `TPU_ACCELERATOR_TYPE` manually, otherwise libtpu.so may not properly initialize.
Failed to get TPU metadata (agent-worker-number) from instance metadata for variable TPU_WORKER_ID: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

Failed to get TPU metadata (worker-network-endpoints) from instance metadata for variable TPU_WORKER_HOSTNAMES: INTERNAL: Failed to fetch URL after 30 tries (http status: 404); curl status: No error
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/gcp_metadata_utils.cc:93

WARNING: Logging before InitGoogle() is written to STDERR
E0000 00:00:1736380405.363400    3192 common_lib.cc:511] INVALID_ARGUMENT: Error: unexpected worker hostname 'WARNING: could not determine TPU worker hostnames or IP addresses' from env var TPU_WORKER_HOSTNAMES. Expecting a valid hostname or IP address without port number. (Full TPU workers' addr string: WARNING: could not determine TPU worker hostnames or IP addresses, please set env var `TPU_WORKER_HOSTNAMES` manually, otherwise libtpu.so may not properly initialize.)
=== Source Location Trace: === 
learning/45eac/tfrc/runtime/libtpu_init_utils.cc:173
2025-01-08 23:56:48.526584: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1736380609.730442    3192 context_distributed_manager.cc:762] unknown service tensorflow.WorkerService
Additional GRPC error information from remote target /job:worker/replica:0/task:0 while calling /tensorflow.WorkerService/GetStatus:
:{""created"":""@1736380609.730372913"",""description"":""Error received from peer ipv4:10.130.0.3:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""unknown service tensorflow.WorkerService"",""grpc_status"":12}
E0108 23:56:49.730822322    3192 completion_queue.cc:244]    assertion failed: queue.num_items() == 0
https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1ccaf0e04f&map= 
*** SIGABRT received by PID 3192 (TID 3192) on cpu 4 from PID 3192; stack trace: ***
PC: @     0x7f1ccaf5cebc  (unknown)  (unknown)
    @     0x7f1caa302841       1888  (unknown)
    @     0x7f1ccaf0e050   18460496  (unknown)
    @     0x7f1ccaed1c60  (unknown)  (unknown)
https://symbolize.stripped_domain/r/?trace=7f1ccaf5cebc,7f1caa302840,7f1ccaf0e04f,7f1ccaed1c5f&map= 
E0108 23:56:49.732558    3192 coredump_hook.cc:316] RAW: Remote crash data gathering hook invoked.
E0108 23:56:49.732569    3192 coredump_hook.cc:355] RAW: Skipping coredump since rlimit was 0 at process start.
E0108 23:56:49.732575    3192 client.cc:269] RAW: Coroner client retries enabled, will retry for up to 30 sec.
E0108 23:56:49.732580    3192 coredump_hook.cc:411] RAW: Sending fingerprint to remote end.
E0108 23:56:49.732595    3192 coredump_hook.cc:420] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
E0108 23:56:49.732601    3192 coredump_hook.cc:472] RAW: Dumping core locally.
E0108 23:56:49.745981    3192 process_state.cc:805] RAW: Raising signal 6 with default behavior
Aborted
```
","['type:bug', 'comp:tpus', 'TF 2.18']",2025-01-09T00:04:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/84413,Dependency Issue
23,tensorflow/tensorflow,dictionaries in fit method of model load data in wrong order,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17; tf 2.18

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

the code is running in google collab.
The code below is an example of a model with multiple inputs and multiple outputs.
NOT working code with using **dictionaries** in method **fit** of model.

the link to collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing
the link to gist: https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd


### Standalone code to reproduce the issue

```shell
# collab:  https://colab.research.google.com/drive/1q13ZwWqgfFcnY8f5oU_KnK3wVf_Gr1JA?usp=sharing
# gist:      https://gist.github.com/moprules/def9b2bda642a064b35e51b8914a28dd

# fast code
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

vocabulary_size = 10000
num_tags = 100
num_departments = 4

# define three model inputs
title = keras.Input(shape=(vocabulary_size,), name=""title"")
text_body = keras.Input(shape=(vocabulary_size,), name=""text_body"")
tags = keras.Input(shape=(num_tags,), name=""tags"")

features = layers.Concatenate()([title, text_body, tags])
# one intermediate layer
features = layers.Dense(64, activation=""relu"")(features)

# Define two model outputs
priority = layers.Dense(1, activation=""sigmoid"", name=""priority"")(features)
department = layers.Dense(num_departments, activation=""softmax"", name=""department"")(features)

# set the model
model = keras.Model(inputs=[title, text_body, tags],
                    outputs=[priority, department])
# prepare data
num_samples = 1280
# The data is filled in with zeros and ones
title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))
tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))

# priority: [0., 1.]
priority_data = np.random.random(size=(num_samples, 1))
# class of 4 labels
department_data = np.random.randint(0, 2, size=(num_samples, num_departments))

# compile model
model.compile(optimizer=""rmsprop"",
              loss={""priority"": ""mean_squared_error"",
                    ""department"": ""categorical_crossentropy""},
              metrics={""priority"": [""mean_absolute_error""],
                       ""department"": [""accuracy""]})

# It doesn't matter how the model is compiled
# model.compile(optimizer=""rmsprop"",
#               loss=[""mean_squared_error"", ""categorical_crossentropy""],
#               metrics=[[""mean_absolute_error""], [""accuracy""]])


# NOT WORKING
# TRAIN MODEL WITH transferring the DICTIONARY to the method
model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},
          {""priority"": priority_data, ""department"": department_data},
          epochs=1
)

# WORK
# TRAIN MODEL WITHOUT transferring the DICTIONARY to the method
model.fit([title_data, text_body_data, tags_data],
          [priority_data, department_data],
          epochs=1
)

# ALSO WORK
# TRAIN MODEL WITH transferring the DICTIONARY to the method
# REPLACE priority and department
model.fit({""title"": title_data, ""text_body"": text_body_data, ""tags"": tags_data},
          {""priority"": department_data, ""department"": priority_data},
          epochs=1
)
```


### Relevant log output

_No response_","['stat:awaiting response', 'type:bug', 'stale', 'comp:keras', 'TF 2.18']",2025-01-07T13:08:06Z,3,0,https://github.com/tensorflow/tensorflow/issues/84278,Logical Bug
24,tensorflow/tensorflow,keras model.save does not respect `include_optimizer=False`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0-dev20250105

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Saving a model using keras with `include_optizer = False` results in a model being saved with optimizer

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1x5NJs9nFxmExhuy8_f_fOehHmIOmk-CZ?usp=sharing
```


### Relevant log output

_No response_","['type:bug', 'comp:keras', 'TF 2.18']",2025-01-07T10:33:38Z,4,0,https://github.com/tensorflow/tensorflow/issues/84268,Logical Bug
25,tensorflow/tensorflow,Encountered unresolved custom op: XlaDynamicSlice,"Hi, i am doing a task in converting T5 model to TFLite for Android. Currently, i am using T5ModelForConditionalGeneration from Huggingface to convert. The conversion is done with some below logging but when load the `generator` from Interpretor, and run an inference example, i have faced this error. You can reproduce with the colab provided below. AFAIK, this XlaDynamicSlice is in TF ops but why this op cannot be resolved in this cases. 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): PyPI 24.0 on Python 3.11.5
- TensorFlow version (or github SHA if from source): 2.18.0


**Provide the text output from tflite_convert**
In colab version, tflite_convert doesn't log anything, below log is in my local version
```
INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets
INFO:tensorflow:Assets written to: /tmp/tmpaxxybw9x/assets
W0000 00:00:1736157114.568747 1061359 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.
W0000 00:00:1736157114.568765 1061359 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.
2025-01-06 16:51:54.568997: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpaxxybw9x
2025-01-06 16:51:54.645325: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }
2025-01-06 16:51:54.645352: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpaxxybw9x
2025-01-06 16:51:55.085153: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.
2025-01-06 16:51:56.061632: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpaxxybw9x
2025-01-06 16:51:56.517300: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 1948307 microseconds.
2025-01-06 16:52:30.233639: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3825] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):
Flex ops: FlexStridedSlice
Details:
	tf.StridedSlice(tensor<?x?x?xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> (tensor<?x1x?x?xf32>) : {begin_mask = 13 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 13 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}
See instructions: https://www.tensorflow.org/lite/guide/ops_select
2025-01-06 16:52:30.233666: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3836] The following operation(s) need TFLite custom op implementation(s):
Custom ops: XlaDynamicSlice
Details:
	tf.XlaDynamicSlice(tensor<1x12x?x?xf32>, tensor<4xi64>, tensor<4xi64>) -> (tensor<1x12x1x?xf32>) : {device = """"}
See instructions: https://www.tensorflow.org/lite/guide/ops_custom
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
My reproduce code in Colab: https://colab.research.google.com/drive/1Rmhc_vpJSa7M1Vt-4ugV5uORaEfRJMOw?usp=sharing
Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
","['stat:awaiting response', 'type:bug', 'comp:lite', 'TFLiteConverter', 'TF 2.18']",2025-01-06T10:46:54Z,2,0,https://github.com/tensorflow/tensorflow/issues/84203,Dependency Issue
26,tensorflow/tensorflow,MFCC-Example-Model converted from TF to TFlite fails with IsPowerOfTwo-RuntimeError inside rfft2d,"### 1. System information

- OS Platform and Distribution: Linux Mint 6.2.9
- TensorFlow installation: pip
- TensorFlow library: 2.18.0 (latest)

### 2. Code

Below is a minimum example which triggers the rfft2d IsPowerOfTwo RuntimeError.
The MFCC-Calculation was directly taken from the tutorial from [tensorflow.org](https://www.tensorflow.org/api_docs/python/tf/signal/mfccs_from_log_mel_spectrograms#for_example)

```
import tensorflow as tf

class MFCCLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(MFCCLayer, self).__init__(**kwargs)

    def call(self, pcm):
        # A 1024-point STFT with frames of 64 ms and 75% overlap.
        stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256, fft_length=1024)
        spectrograms = tf.abs(stfts)

        # Warp the linear scale spectrograms into the mel-scale.
        num_spectrogram_bins = stfts.shape[-1]
        lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80
        linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
            num_mel_bins,
            num_spectrogram_bins,
            sample_rate,
            lower_edge_hertz,
            upper_edge_hertz,
        )
        mel_spectrograms = tf.tensordot(spectrograms, linear_to_mel_weight_matrix, 1)
        mel_spectrograms.set_shape(
            spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:])
        )

        # Compute a stabilized log to get log-magnitude mel-scale spectrograms.
        log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)

        # Compute MFCCs from log_mel_spectrograms and take the first 13.
        mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[
            ..., :13
        ]
        print(""mfccs.shape: "", mfccs.shape)
        return mfccs


def build_model(input_shape):
    input_layer = tf.keras.layers.Input(shape=input_shape)
    output_layer = MFCCLayer()(input_layer)
    return tf.keras.models.Model(inputs=input_layer, outputs=output_layer)


if __name__ == ""__main__"":
    batch_size, num_samples, sample_rate = 32, 32000, 16000.0
    # A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].
    pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)
    print(""pcm.shape: "", pcm.shape)

    model = build_model(pcm.shape)
    model.summary()

    # Convert to TensorFlow Lite and Save
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    tflite_model = converter.convert()

    with open(""mfcc.tflite"", ""wb"") as f:
        f.write(tflite_model)

    # Load the model and run inference
    with open(""mfcc.tflite"", ""rb"") as f:
        tflite_model = f.read()

    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    pcm = tf.expand_dims(pcm, axis=0)  # Add batch dimension

    interpreter.set_tensor(input_details[0][""index""], pcm)
    interpreter.invoke()  # <-- RuntimeError: tensorflow/lite/kernels/rfft2d.cc:117 IsPowerOfTwo(fft_length_data[1]) was not true.Node number 42 (RFFT2D) failed to prepare.
    mfccs = interpreter.get_tensor(output_details[0][""index""])
    print(""mfccs.shape: "", mfccs.shape)
```


### 3. Failure after conversion
As far as I know, the RuntimeError should't happen, as all supplied stft-function arguments are power of two's?

I am unsure if this is just a user error from myself or this is a bug.
I couldn't find any info online, hence i ask here.

Is a MFCC-calculation model possible with TFlite?

Thanks for all help

","['stat:awaiting response', 'type:bug', 'stale', 'comp:lite', 'TFLiteConverter', 'TF 2.18']",2025-01-05T20:45:45Z,3,0,https://github.com/tensorflow/tensorflow/issues/84171,Runtime Error
27,tensorflow/tensorflow,Broken compatibility with tensorflow-metal in 2.18,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

MacOS 15.2

### Mobile device

_No response_

### Python version

3.11.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

Apple M2 Max GPU 38-cores

### Current behavior?

Apple silicone GPU with tensorflow-metal==1.1.0  and python 3.11 works fine with tensorboard==2.17.0

This is normal output:
```
/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py 
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]

Process finished with exit code 0
```

But if I upgrade tensorflow to 2.18 I'll have error, attached in ""Relevant log output"" issue section

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

if __name__ == '__main__':
    gpus = tf.config.experimental.list_physical_devices('GPU')
    print(gpus)
```


### Relevant log output

```shell
/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/bin/python /Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py 
Traceback (most recent call last):
  File ""/Users/mspanchenko/VSCode/cryptoNN/ml/core_second_window/test_tensorflow_gpus.py"", line 1, in <module>
    import tensorflow as tf
  File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/__init__.py"", line 437, in <module>
    _ll.load_library(_plugin_dir)
  File ""/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/python/framework/load_library.py"", line 151, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: dlopen(/Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN3tsl8internal10LogMessageC1EPKcii
  Referenced from: <D2EF42E3-3A7F-39DD-9982-FB6BCDC2853C> /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow-plugins/libmetal_plugin.dylib
  Expected in:     <2814A58E-D752-317B-8040-131217E2F9AA> /Users/mspanchenko/anaconda3/envs/cryptoNN_ml_core/lib/python3.11/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so

Process finished with exit code 1
```
","['stat:awaiting response', 'type:bug', 'stale', 'comp:gpu', 'TF 2.18']",2025-01-05T17:26:17Z,5,0,https://github.com/tensorflow/tensorflow/issues/84167,Dependency Issue
28,tensorflow/tensorflow,The test case label_image .py of tensorflow2.4.1 source code fails to be execued.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.4.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.7.12

### Bazel version

3.7

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The test case label_image.py fails to be executed,and the message ""module 'tensorfle' has no attribute 'GrapDef'"" is displayed.
![image](https://github.com/user-attachments/assets/e4b7b56d-2589-41fe-8395-1743c941dd49)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
graph_def = tf.GraphDef()
```


### Relevant log output

_No response_","['stat:awaiting response', 'type:bug', 'stale', 'TF 2.4']",2025-01-03T03:03:30Z,6,0,https://github.com/tensorflow/tensorflow/issues/84039,Logical Bug
29,tensorflow/tensorflow,Failing to convert MobileNetV3Large to TFLite w/ Integer q,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Windows 10 WSL
- TensorFlow installation (pip package or built from source):  2.10 (on Win 10) and 2.16.2 (on WSL)

### 2. Code

```
import tensorflow as tf
import numpy as np
from tensorflow.keras.applications import MobileNetV3Large
from tensorflow.keras.applications.mobilenet_v3 import preprocess_input
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Generate one sample image for testing
test_image = np.random.normal(loc=127.5, scale=50, size=(1, 224, 224, 3))
test_image = np.clip(test_image, 0, 255).astype(np.float32)
preprocessed_image = preprocess_input(test_image.copy())

# Load model
model = MobileNetV3Large(
    weights='imagenet',
    include_top=True,
    input_shape=(224, 224, 3)
)

# Get original prediction
original_pred = model.predict(preprocessed_image, verbose=0)

# Convert to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable dynamic range quantization
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]
converter._experimental_disable_per_channel = True
converter.experimental_new_converter = True

# Convert
tflite_model = converter.convert()

# Get TFLite prediction
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

interpreter.set_tensor(input_details[0]['index'], preprocessed_image)
interpreter.invoke()
tflite_pred = interpreter.get_tensor(output_details[0]['index'])

# Calculate correlation
correlation, _ = pearsonr(original_pred.flatten(), tflite_pred.flatten())

# Visualize
plt.figure(figsize=(10, 5))

# Scatter plot
plt.subplot(1, 2, 1)
plt.scatter(original_pred.flatten(), tflite_pred.flatten(), alpha=0.5)
plt.plot([original_pred.min(), original_pred.max()],
         [original_pred.min(), original_pred.max()],
         'r--', label=f'Perfect Correlation\nActual: {correlation:.4f}')
plt.title('Original vs Quantized Predictions')
plt.xlabel('Original Model')
plt.ylabel('Quantized Model')
plt.legend()

# Distribution plot
plt.subplot(1, 2, 2)
plt.hist(np.abs(original_pred.flatten() - tflite_pred.flatten()),
         bins=50, alpha=0.75, label='Prediction Differences')
plt.title('Distribution of Prediction Differences')
plt.xlabel('|Original - Quantized|')
plt.ylabel('Count')
plt.legend()

plt.tight_layout()
plt.show()

print(f""\nResults:"")
print(f""Prediction correlation: {correlation:.4f}"")
print(f""Original model size: {len(model.get_weights()) / 1024 / 1024:.2f} MB"")
print(f""Quantized model size: {len(tflite_model) / 1024 / 1024:.2f} MB"")
print(f""Size reduction: {(1 - len(tflite_model) / len(model.get_weights())) * 100:.1f}%"")
```

### 3. Failure after conversion
1. TF 2.10 in Win10 Log:
Model produces wrong results. See plot made from code:
![image](https://github.com/user-attachments/assets/8823cbfc-88d9-4e2d-9ef7-c8a2adc3ef0a)

2. TF2.16 in WSL:
Model fails to convert. Gets error: `LLVM ERROR: Failed to infer result type(s).` (see log)


### 5. (optional) Any other info / logs
I ran this on 2 systems:

1. TF 2.10 in Win10 Log:
```
import sys; print('Python %s on %s' % (sys.version, sys.platform))
D:\code\ai_dev\venv\Scripts\python.exe ""C:/Program Files/JetBrains/PyCharm 2023.2.4/plugins/python/helpers/pydev/pydevd.py"" --multiprocess --qt-support=auto --client 127.0.0.1 --port 54366 --file C:\Users\Administrator\AppData\Roaming\JetBrains\PyCharm2023.2\scratches\tfmodel_tflite.py 
Connected to pydev debugger (build 232.10203.26)
2024-12-26 15:17:07.215039: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-26 15:17:07.670016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7423 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:0a:00.0, compute capability: 8.6
2024-12-26 15:17:11.111912: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8906
2024-12-26 15:17:12.036555: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 64). These functions will not be directly callable after loading.
2024-12-26 15:17:44.746406: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.
2024-12-26 15:17:44.746529: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.
2024-12-26 15:17:44.747230: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:44.771028: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }
2024-12-26 15:17:44.771129: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:44.886049: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled
2024-12-26 15:17:44.904668: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.
2024-12-26 15:17:45.275249: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: C:\Users\ADMINI~1\AppData\Local\Temp\tmprjeve5nr
2024-12-26 15:17:45.402632: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 655396 microseconds.
2024-12-26 15:17:45.811466: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```

3. TF 2.16.2 in WSL log:
```
/root/ai_dev/.venv/bin/python /root/.pycharm_helpers/pydev/pydevd.py --multiprocess --qt-support=auto --client 127.0.0.1 --port 55955 --file /mnt/c/Users/Administrator/AppData/Roaming/JetBrains/PyCharm2023.2/scratches/tfmodel_tflite.py 
Connected to pydev debugger (build 232.10203.26)
2024-12-26 15:18:56.434366: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-26 15:18:57.803991: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-12-26 15:18:58.473972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-12-26 15:18:58.972456: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-12-26 15:18:58.975123: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-12-26 15:18:59.910805: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-12-26 15:19:06.124533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-12-26 15:19:15.989425: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node
Your kernel may have been built without NUMA support.
2024-12-26 15:19:17.013008: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
W0000 00:00:1735255170.947341  469943 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.
W0000 00:00:1735255170.947401  469943 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.
2024-12-26 15:19:30.948060: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp9lkkwnp9
2024-12-26 15:19:30.953309: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }
2024-12-26 15:19:30.953334: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmp9lkkwnp9
2024-12-26 15:19:31.011901: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
2024-12-26 15:19:31.020594: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.
2024-12-26 15:19:31.231606: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /tmp/tmp9lkkwnp9
2024-12-26 15:19:31.297302: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 349244 microseconds.
2024-12-26 15:19:31.779723: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(fused[""ReadVariableOp:"", callsite(""MobileNetV3Large_1/conv_1/convolution/ReadVariableOp@__inference_serving_default_5035""(""/root/.pycharm_helpers/pydev/pydevd.py"":2199:1) at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":2181:1 at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":1493:1 at callsite(""/root/.pycharm_helpers/pydev/pydevd.py"":1500:1 at callsite(""/root/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py"":18:1 at callsite(""/mnt/c/Users/Administrator/AppData/Roaming/JetBrains/PyCharm2023.2/scratches/tfmodel_tflite.py"":34:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1175:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1129:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1636:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1614:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py"":205:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/tensorflow/lite/python/lite.py"":1537:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/layer.py"":58:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/layer.py"":112:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py"":899:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/operation.py"":46:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":156:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/models/functional.py"":182:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/function.py"":171:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/models/functional.py"":632:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/layer.py"":899:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":117:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/operation.py"":46:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"":156:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"":243:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py"":233:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/ops/nn.py"":1183:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py"":301:1 at callsite(""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/nn.py"":274:1 at ""/root/ai_dev/.venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py"":85:1))))))))))))))))))))))))))))))))]): error: missing attribute 'value'
LLVM ERROR: Failed to infer result type(s).

Process finished with exit code 134
```


","['stat:awaiting tensorflower', 'type:bug', 'comp:lite', 'TFLiteConverter', 'TF 2.16']",2024-12-26T23:21:09Z,10,0,https://github.com/tensorflow/tensorflow/issues/83754,Logical Bug
30,tensorflow/tensorflow,How to run TFLite benchmark with QNN delegate in Android,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

No

### OS platform and distribution

macOS 15.2

### Mobile device

One Plus 7 Pro, Android 11

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have built/installed/run TFLite benchmark following this [instruction](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark#on-android) for Android, and used TensorFlow 2.15.0 according to [issue#66015](https://github.com/tensorflow/tensorflow/issues/66015). I test the benchmark via the following commands and the output result seems correct.
```shell
adb push /Users/handleychen/Github/tensorflow/tensorflow/bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model /data/local/tmp
adb shell chmod +x /data/local/tmp/benchmark_model
adb shell ""mkdir /data/local/tmp/models""
adb push /Users/handleychen/Github/tensorflow/models/*.tflite /data/local/tmp/models
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --num_threads=4 --enable_op_profiling=true
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --use_gpu=true --enable_op_profiling=true
adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224.tflite --use_nnapi=true --enable_op_profiling=true
``` 
[benchmark result.txt](https://github.com/user-attachments/files/18197819/benchmark.result.txt)

Now I want to run the benchmark with QNN delegate. I [setup the on device environment](https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/TfLite-Delegate_setup.html#on-device-environment-setup) and [run a QNN delegate using an external delegate](https://docs.qualcomm.com/bundle/publicresource/topics/80-70015-54/sample-applications.html#run-a-qnn-delegate-using-an-external-delegate). The [model](https://storage.googleapis.com/download.tensorflow.org/models/tflite/task_library/image_classification/android_java/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite) being tested comes from tflite example [image_classification](https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification).  I tested the benchmark using the following commands, but the result was a failure.
```shell
adb shell ""mkdir /data/local/tmp/qnn_delegate""
adb push /Users/handleychen/Github/quic/SDK/qairt/2.26.0.240828/lib/aarch64-android/* /data/local/tmp/qnn_delegate
adb shell
cd /data/local/tmp
export LD_LIBRARY_PATH=/data/local/tmp/qnn_delegate
export ADSP_LIBRARY_PATH=""/data/local/tmp/qnn_delegate""
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:gpu;'
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;'
# I also tried setting htp_precision:1, but the result was the same.
./benchmark_model --graph=/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite --external_delegate_path=/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so --external_delegate_options='backend_type:htp;htp_precision:1'
``` 
```shell
# for gpu delegate
……
INFO: Though EXTERNAL delegate is explicitly applied, the model graph will not be executed by the delegate.
……

# for npu delegate
INFO: STARTING!
INFO: Log parameter values verbosely: [0]
INFO: Graph: [/data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite]
INFO: External delegate path: [/data/local/tmp/qnn_delegate/libQnnTFLiteDelegate.so]
INFO: External delegate options: [backend_type:htp;htp_precision:1]
INFO: Loaded model /data/local/tmp/models/mobilenet_v1_1.0_224_quantized_1_metadata_1.tflite
INFO: Initialized TensorFlow Lite runtime.
INFO: EXTERNAL delegate created.
ERROR: [QNN Delegate] Failed to create device_handle for Backend ID 6, error=1008
ERROR: Restored original execution plan after delegate application failure.
ERROR: Failed to apply EXTERNAL delegate.
ERROR: Benchmarking failed.
``` 
The full output is attached. [benchmarkQNN result.txt](https://github.com/user-attachments/files/18206356/benchmarkQNN.result.txt)

I have also tested it on Android phones equipped with Snapdragon 855 and Gen 3 chips, and the results were the same.

Could anyone tell me how to deal with this?

### Standalone code to reproduce the issue

```shell
as described above
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:lite']",2024-12-19T12:40:25Z,4,0,https://github.com/tensorflow/tensorflow/issues/83344,Runtime Error
31,tensorflow/tensorflow,Aborted (core dumped) in `LearnedUnigramCandidateSampler`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific inputs,`tf.raw_ops.LearnedUnigramCandidateSampler` triggers crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

true_classes = tf.constant([], dtype=tf.int64)
num_true = 3590707793247644003
num_sampled = 126
unique = False
range_max = 186785497093039093
seed = 8997
seed2 = 0

tf.raw_ops.LearnedUnigramCandidateSampler(
    true_classes=true_classes,
    num_true=num_true,
    num_sampled=num_sampled,
    unique=unique,
    range_max=range_max,
    seed=seed,
    seed2=seed2,
    name=None
)
```


### Relevant log output

```shell
2024-12-17 11:36:29.305345: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at candidate_sampler_ops.cc:37 : INVALID_ARGUMENT: Attr num_true has value 3590707793247644003 out of range for an int32
2024-12-17 11:36:29.305378: F external/local_tsl/tsl/lib/random/weighted_picker.cc:28] Check failed: N >= 0 (0 vs. -2090015755)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-12-17T12:05:50Z,1,0,https://github.com/tensorflow/tensorflow/issues/83164,Runtime Error
32,tensorflow/tensorflow,[XLA] `tf.keras.layers.LSTM` behaves differently on GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When executing LSTM on **XLA**, it fails.
However, when executing it without XLA, it passes.
The above failure is on GPU.
If I use CPU as backend, with or without XLA both pass the check.

### Standalone code to reproduce the issue

```python
import os
import tensorflow
import tensorflow as tf
tf.random.set_seed(42)
class RecurrentModel(tf.keras.Model):

    def __init__(self):
        super(RecurrentModel, self).__init__()
        self.lstm = tf.keras.layers.LSTM(units=64, return_sequences=True)

    @tf.function(jit_compile=True)
    def call(self, x):
        return self.lstm(x)


model = RecurrentModel()


input_shape = (10, 20, 1)
x = tf.random.normal(shape=input_shape)

inputs = [x]

output = model(*inputs)
print(output)
```


### Relevant log output

```shell
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-0938fdccd1fa> in <cell line: 24>()
     22 inputs = [x]
     23 
---> 24 output = model(*inputs)
     25 print(output)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Exception encountered when calling RecurrentModel.call().

Detected unsupported operations when trying to compile graph __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: CudnnRNNV3 (No registered 'CudnnRNNV3' OpKernel for XLA_GPU_JIT devices compatible with node {{node lstm_3_1/CudnnRNNV3}}){{node lstm_3_1/CudnnRNNV3}}
The op is created at: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-4-0938fdccd1fa>"", line 24, in <cell line: 24>
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 826, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 1376, in _maybe_build
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py"", line 212, in compute_output_spec
File ""<ipython-input-1-0938fdccd1fa>"", line 13, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py"", line 901, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/ops/operation.py"", line 46, in __call__
File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py"", line 570, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py"", line 406, in call
File ""/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py"", line 537, in inner_loop
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py"", line 841, in lstm
File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py"", line 933, in _cudnn_lstm
	tf2xla conversion failed while converting __inference_call_877[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_call_877]

Arguments received by RecurrentModel.call():
  • x=tf.Tensor(shape=(10, 20, 1), dtype=float32)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'comp:xla', 'TF 2.18']",2024-12-16T13:57:22Z,2,0,https://github.com/tensorflow/tensorflow/issues/83063,Logical Bug
33,tensorflow/tensorflow,`tensorflow.experimental.numpy.kron` not working with multidimensional Arrays,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu

### Mobile device

_No response_

### Python version

3.10.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

This is a new issue in replacement for https://github.com/tensorflow/tensorflow/issues/59761 as suggested by @tilakrayal

I tested the function against numpy and it throws an error when the `ndim` of the input tensors is greater than 2.
I run the code on the latest TensorFlow version on PyPI and the nightly version, and I get the same failures.

Also, I am not getting as much debug information only this error

`UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name: `

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
try:
    a = tf.constant(np.arange(100).reshape(2, 5, 2, 5))
    b = tf.constant(np.arange(24).reshape(2, 3, 4))
    print(a.ndim) # 4
    print(b.ndim) # 3

    y = tf.experimental.numpy.kron(a, b)

    print(y.shape)
except:
    print(""Can't use tf.experimental.numpy.kron on multi-dimensional arrays"")

x = np.arange(100).reshape(2, 5, 2, 5)
y = np.arange(24).reshape(2, 3, 4)

print(x.ndim) # 4
print(y.ndim) # 3

z = np.kron(x, y)

print(z.shape) # (2, 10, 6, 20)
```


### Relevant log output

```shell
UnimplementedError: {{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Broadcast between [2,1,5,1,2,1,5,1] and [1,1,1,2,1,3,1,4] is not supported yet. [Op:Mul] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-12-16T07:22:01Z,3,0,https://github.com/tensorflow/tensorflow/issues/83037,Syntax Error
34,tensorflow/tensorflow,error message is inconsistent with documentation in `tf.raw_ops.MaxPoolGradWithArgmax`,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

According to [Doc tf.raw_ops.MaxPoolGradWithArgmax](https://www.tensorflow.org/api_docs/python/tf/raw_ops/MaxPoolGradWithArgmax), the argument `argmax` can be `int32` or `int64`. However, after actual testing, the parameter `argmax` can only support tensor input of data type `int64`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_tensor = tf.constant(1, shape=[1, 2, 2, 1], dtype=tf.float32)
grad_tensor = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.float32)
argmax_indices = tf.constant(1, shape=[1, 1, 1, 1], dtype=tf.int32)
ksize = [1, 2, 2, 1]
strides = [1, 1, 1, 1]
padding = ""VALID""

output_grad = tf.raw_ops.MaxPoolGradWithArgmax(
    input=input_tensor,
    grad=grad_tensor,
    argmax=argmax_indices,
    ksize=ksize,
    strides=strides,
    padding=padding,
    include_batch_in_index=False
)
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node MaxPoolGradWithArgmax}} = MaxPoolGradWithArgmax[T=DT_FLOAT, Targmax=DT_INT32, include_batch_in_index=false, ksize=[1, 2, 2, 1], padding=""VALID"", strides=[1, 1, 1, 1]]
All kernels registered for op MaxPoolGradWithArgmax:
  device='CPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT32]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Targmax in [DT_INT64]
  device='CPU'; T in [DT_UINT64]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_BFLOAT16]; Targmax in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Targmax in [DT_INT64]
 [Op:MaxPoolGradWithArgmax] name:
```
","['type:docs-bug', 'stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-12-12T13:14:53Z,1,0,https://github.com/tensorflow/tensorflow/issues/82837,Syntax Error
35,tensorflow/tensorflow,Update Python in Docker images to 3.11.x and ditch `3.11.0rc1`,"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18

### Custom code

No

### OS platform and distribution

Docker

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The Python version of the docker images is outdated and should be updated

### Standalone code to reproduce the issue

```shell
docker run -it tensorflow/tensorflow python
```


### Relevant log output

```shell
Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>>
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'TF 2.18']",2024-12-09T13:35:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/82529,Dependency Issue
36,tensorflow/tensorflow,"The warning ""The structure of `inputs` doesn't match the expected structure"" when training a functional model","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.13.1-0-gf841394b1b7 2.13.1 (Nightly: v1.12.1-119104-gf8fd6f53fa3 2.19.0-dev20241204)

### Custom code

Yes

### OS platform and distribution

Windows 11 23H2 22631.4460

### Mobile device

Windows 11 23H2 22631.4460

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the model is functional, not Sequential, the warning has occured:

```
Epoch 1/5
<path-to-python>\lib\site-packages\keras\src\models\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor']. Received: the structure of inputs=*
  warnings.warn(
```

Yes, the warning message has interrupted on parenthesis. When I've run the same code in Nightly, the warning message is:

```
Epoch 1/5
<path-to-python>\lib\site-packages\keras\src\models\functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.
Expected: ['keras_tensor']
Received: inputs=Tensor(shape=(None, 10))
  warnings.warn(msg)
```

After the warning, the training continues normally, but because of this warning, I can't be sure that the model works as I expect.

I've traced the source and found that in `Lib\site-packages\keras\src\tree\optree_impl.py` on line 95 comparasion of expected and actual structure failed. Now I place the traced variables here:

```
>>> a
<tf.Tensor 'data:0' shape=(None, 10) dtype=float64>
>>> b
[<KerasTensor shape=(None, 10), dtype=float32, sparse=False, name=keras_tensor>]
>>> a_structure
PyTreeSpec(*, NoneIsLeaf)
>>> b_structure
PyTreeSpec([*], NoneIsLeaf)
```

The data passed to the `fit` function fully corresponds to the [documentation](https://keras.io/api/models/model_training_apis/#fit-method). The warning appears independently of whether I use numpy array or PyDataset as dataset of `fit` function.



### Standalone code to reproduce the issue

```shell
from keras.models import Model
from keras.layers import Dense, Input, Flatten, Concatenate
from keras import utils
import numpy as np
import tensorflow as tf

class SamplesSet(utils.PyDataset):
    
    def __init__(self, batch_size, **kwargs):
        super().__init__(**kwargs)
        self.batch_size = batch_size
        
    def __len__(self):
        return 1
    
    def __getitem__(self, idx):
        x1 = np.random.uniform(size=10*self.batch_size).reshape((self.batch_size, 10))
        y = np.arange(self.batch_size)
        return x1, y
    
train = SamplesSet(100)
x1_train = np.random.uniform(size=10*100).reshape((100, 10))
y_train = np.arange(100)

input1 = Input(shape=(10,))
l1 = Dense(1)(input1)
d2 = Dense(1, activation='sigmoid')(l1)
model = Model(inputs=[input1], outputs=[d2])
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

history = model.fit(x1_train, y_train, epochs=5, verbose=1)
# In the all cases below warning occures too
# history = Model.fit(train, epochs=5, verbose=1) 
# ret = model.predict(np.arange(10)[np.newaxis,:])
# ret = model.predict(tf.constant([[0,1,2,3,4,5,6,7,8,9]]))
```


### Relevant log output

_No response_","['type:bug', 'comp:keras', 'TF 2.13']",2024-12-06T03:47:02Z,8,0,https://github.com/tensorflow/tensorflow/issues/82372,Logical Bug
37,tensorflow/tensorflow,[XLA] TF XLA outputs abnormal value when compiling `Embedding`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

For `Embedding` operator, when I set the `input_dim=1` (which means it indexed from 0 to 0), the output always returns **0 without XLA**.

After compilation, the outputs are usually some random tensors.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.random.set_seed(42)
x = tf.constant([1])


# uncompiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()

output1 = m(x)


# compiled model
class Model(tf.keras.Model):

    def __init__(self):
        super(Model, self).__init__()
        self.embedding = tf.keras.layers.Embedding(1, 1)

    @tf.function(jit_compile=True)
    def call(self, x):
        output = self.embedding(x)
        return output


m = Model()
output2 = m(x)

print(output1)
print(output2)
```


### Relevant log output

```shell
tf.Tensor([[0.]], shape=(1, 1), dtype=float32)
tf.Tensor([[-0.00567592]], shape=(1, 1), dtype=float32)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.18']",2024-12-05T13:58:40Z,4,0,https://github.com/tensorflow/tensorflow/issues/82317,Logical Bug
38,tensorflow/tensorflow,Are checkpoints broken in >= 2.16?,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16, 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The example given in https://www.tensorflow.org/guide/checkpoint does not seem to work as expected in 2.16 and 2.17, while working fine in 2.15. After restoring and restarting the training process, it starts training from the very beginning.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1n76Mu5BhdBJBSXc7cXYJMr0lMDER2JRa?usp=sharing
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', '2.17']",2024-12-04T15:11:53Z,2,0,https://github.com/tensorflow/tensorflow/issues/82209,Logical Bug
39,tensorflow/tensorflow,TPU not support TensorFlow 2.18 and 2.17.1,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.18 and tf. 2.17.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`import tensorflow as tf` results `segmentation fault core dumped`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:tpus', 'TF 2.18']",2024-12-04T14:56:53Z,7,0,https://github.com/tensorflow/tensorflow/issues/82208,Runtime Error
40,tensorflow/tensorflow,Clarify the `constant_op.constant(2)` statement,"It would be helpful to clarify the `constant_op.constant(2)` statement by explaining the corresponding import statement.

https://github.com/tensorflow/tensorflow/blob/5bc9d26649cca274750ad3625bd93422617eed4b/tensorflow/python/ops/summary_ops_v2.py#L1062-L1066","['type:bug', 'comp:ops']",2024-12-02T18:05:41Z,4,0,https://github.com/tensorflow/tensorflow/issues/81954,UI/UX Bug
41,tensorflow/tensorflow,MixedPrecision + XLA: Seen floating point types of different precisions,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

Google Colab default

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using bilinear interpolation + XLA + mixed_float16 policy issue raises during compilation.
Without bilinear interpolation or without XLA or without mixed_float16 there is no issue.

In Google Colab i have this issue only on CPU with TF 2.17 and both CPU & GPU with TF 2.18.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1joSiScbM7Stc9bn1C4R_4sFkDzakrTsJ?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-4-9cc273be2d5a> in <cell line: 28>()
     26 model.compile(loss='mse', optimizer='adam', run_eagerly=False, jit_compile=True)
     27 
---> 28 model.fit(dataset)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InternalError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>

  File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start

  File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>

  File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 377, in dispatch_queue

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 250, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 748, in __init__

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request

  File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute

  File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes

  File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code

  File ""<ipython-input-4-9cc273be2d5a>"", line 28, in <cell line: 28>

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 368, in fit

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 216, in function

  File ""/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py"", line 129, in multi_step_on_iterator

during context [Unknown]: Seen floating point types of different precisions in %multiply.43589 = f32[2,8,8,1280]{3,2,1,0} multiply(f32[2,8,8,1280]{3,2,1,0} %add.43539, f16[2,8,8,1280]{3,2,1,0} %multiply.43588), metadata={op_type=""Mul"" op_name=""mul_9"" source_file=""/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py"" source_line=1196}, but mixed precision is disallowed.
	 [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_124474]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.18']",2024-11-29T07:11:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/81273,Logical Bug
42,tensorflow/tensorflow,Some operators give different results on CPU and GPU when dealing with complex numbers that include `inf`.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The outputs of TensorFlow mathematical APIs (`sin, cos, tan, sinh, cosh, exp, and reduce_mean`) are inconsistent between the CPU and GPU when applied to complex inputs containing `inf`. 

### Standalone code to reproduce the issue

```shell
python
import tensorflow as tf

test_inputs = [
    tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128),
]

test_apis = [
    tf.math.sin, tf.math.cos, tf.math.tan,
    tf.math.sinh, tf.math.cosh, tf.math.exp, tf.math.reduce_mean
]

for api in test_apis:
    print(f""Testing {api.__name__}"")
    for x in test_inputs:
        try:
            with tf.device('/CPU'):
              cpu_out = api(x)
              print(f""CPU Output: {cpu_out}"")
            with tf.device('/GPU:0'):
              gpu_out = api(x)
              print(f""GPU Output: {gpu_out}"")
        except Exception as e:
            print(f""Error in {api.__name__}: {e}"")
```


### Relevant log output

```shell
Testing sin
CPU Output: [nan +0.j  0.+infj nan+infj]
GPU Output: [nan+nanj nan+infj nan+nanj]
Testing cos
CPU Output: [nan +0.j inf -0.j inf+nanj]
GPU Output: [nan+nanj inf+nanj nan+nanj]
Testing tan
CPU Output: [nan+0.j  0.+1.j  0.+1.j]
GPU Output: [nan+nanj  0. +1.j  0. +1.j]
Testing sinh
CPU Output: [inf +0.j  0.+nanj inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing cosh
CPU Output: [inf +0.j nan +0.j inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing exp
CPU Output: [inf +0.j nan+nanj inf+nanj]
GPU Output: [inf+nanj nan+nanj nan+nanj]
Testing reduce_mean
CPU Output: (inf+infj)
GPU Output: (nan+nanj)
```
","['stat:awaiting response', 'type:bug', 'stale', 'comp:ops', '2.17']",2024-11-27T13:13:05Z,4,0,https://github.com/tensorflow/tensorflow/issues/80947,Logical Bug
43,tensorflow/tensorflow,"When using `tf.math.log1p` and NumPy's `np.log1p` with the same complex input, the outputs are inconsistent.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.17.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow's `tf.math.log1p` produces inconsistent results with NumPy's `np.log1p` for complex inputs containing `inf`, such as `[inf+0.j, 0+inf.j, inf+inf.j]`. TensorFlow outputs `[inf+0.j, nan+nanj, nan+nanj]`, while NumPy returns `[inf+0.j, inf+1.57079633j, inf+0.78539816j]`.

### Standalone code to reproduce the issue

```shell
python
import tensorflow as tf
import numpy as np

test_input = tf.constant([complex(float('inf'), 0), complex(0, float('inf')), complex(float('inf'), float('inf'))], dtype=tf.complex128)

# TensorFlow computation
with tf.device('/CPU:0'):
    cpu_out = tf.math.log1p(test_input)

# NumPy computation
numpy_out = np.log1p(test_input.numpy())

print(f""CPU Output: {cpu_out}"")
print(f""NumPy Output: {numpy_out}"")
```


### Relevant log output

```shell
CPU Output: [inf +0.j nan+nanj nan+nanj]
NumPy Output: [inf+0.j         inf+1.57079633j inf+0.78539816j]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-26T13:36:51Z,3,0,https://github.com/tensorflow/tensorflow/issues/80850,Logical Bug
44,tensorflow/tensorflow,Heap-buffer-overflow in `SparseMatrixSparseCholesky`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When batch_index is scalar, `SparseMatrixSparseCholesky` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

indices = tf.constant([[0, 0], [1, 1], [2, 1], [2, 2], [3, 3]], dtype=tf.int64)
values = tf.constant([1.0, 2.0, 1.0, 3.0, 4.0], tf.float32)
dense_shape = tf.constant([4, 4], dtype=tf.int64)
input = tf.raw_ops.SparseTensorToCSRSparseMatrix(
    indices=indices, values=values, dense_shape=dense_shape, name=None
)
permutation = tf.constant([4,1,1,1], dtype=tf.int32)

tf.raw_ops.SparseMatrixSparseCholesky(
  input=input, permutation=permutation, type=tf.float32
)
```


### Relevant log output

```shell
=================================================================
==3331846==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x60700049d1d0 at pc 0x7faa202d6d87 bp 0x7ffd09bf8fe0 sp 0x7ffd09bf8fd0
WRITE of size 4 at 0x60700049d1d0 thread T0
    #0 0x7faa202d6d86 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2d86)
    #1 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f373a)
    #2 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f84e9)
    #3 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4201d41)
    #4 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x26d8d71)
    #5 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2adab6)
    #6 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cdc14a)
    #7 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1862fe6)
    #8 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x173f306)
    #9 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x179cf74)
    #10 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x17aa8b1)
    #11 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c582eb)
    #12 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aedc2c)
    #13 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31af066a)
    #14 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c2a939)
    #15 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31addde4)
    #16 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31ae1dd4)
    #17 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aebd26)
    #18 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x20ad4c33)
    #19 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c1fe5e)
    #20 0x7fa9f268645b in TFE_Execute (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x1065245b)
    #21 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so+0x3c2274)
    #22 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0xb7ccb)
    #23 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #24 0x51ad66  (/usr/bin/python3.11+0x51ad66)
    #25 0x4e75db in _PyObject_MakeTpCall (/usr/bin/python3.11+0x4e75db)
    #26 0x4fb151 in _PyEval_EvalFrameDefault (/usr/bin/python3.11+0x4fb151)
    #27 0x531822 in _PyFunction_Vectorcall (/usr/bin/python3.11+0x531822)
    #28 0x541194 in PyObject_Call (/usr/bin/python3.11+0x541194)
    #29 0x4fefe0 in _PyEval_EvalFrameDefault (/usr/bin/python3.11+0x4fefe0)
    #30 0x62e1b3  (/usr/bin/python3.11+0x62e1b3)
    #31 0x4f3a66 in PyEval_EvalCode (/usr/bin/python3.11+0x4f3a66)
    #32 0x647c36  (/usr/bin/python3.11+0x647c36)
    #33 0x64534f  (/usr/bin/python3.11+0x64534f)
    #34 0x650d14  (/usr/bin/python3.11+0x650d14)
    #35 0x650a63 in _PyRun_SimpleFileObject (/usr/bin/python3.11+0x650a63)
    #36 0x650832 in _PyRun_AnyFileObject (/usr/bin/python3.11+0x650832)
    #37 0x64f786 in Py_RunMain (/usr/bin/python3.11+0x64f786)
    #38 0x61ee0c in Py_BytesMain (/usr/bin/python3.11+0x61ee0c)
    #39 0x7faaf80d1d8f  (/lib/x86_64-linux-gnu/libc.so.6+0x29d8f)
    #40 0x7faaf80d1e3f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x29e3f)
    #41 0x61ec94 in _start (/usr/bin/python3.11+0x61ec94)

0x60700049d1d0 is located 0 bytes to the right of 80-byte region [0x60700049d180,0x60700049d1d0)
allocated by thread T0 here:
    #0 0x7faaf84bf887 in __interceptor_malloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cpp:145
    #1 0x7faa202d6803 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2803)
    #2 0x7faa4e5b373a in std::_Function_handler<void (long, long), tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&)::{lambda(long, long)#1}>::_M_invoke(std::_Any_data const&, long&&, long&&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f373a)
    #3 0x7faa4e5b84e9 in Eigen::ThreadPoolDevice::parallelFor(long, Eigen::TensorOpCost const&, std::function<long (long)>, std::function<void (long, long)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f84e9)
    #4 0x7faa4e5c1d41 in tsl::thread::ThreadPool::ParallelFor(long, long, std::function<void (long, long)> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4201d41)
    #5 0x7faa4ca98d71 in tensorflow::Shard(int, tsl::thread::ThreadPool*, long, long, std::function<void (long, long)>) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x26d8d71)
    #6 0x7faa202e1ab6 in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2adab6)
    #7 0x7faa4c09c14a in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cdc14a)
    #8 0x7faa4bc22fe6 in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1862fe6)
    #9 0x7faa4baff306 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x173f306)
    #10 0x7faa4bb5cf74 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x179cf74)
    #11 0x7faa4bb6a8b1 in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x17aa8b1)
    #12 0x7faa13c8c2eb in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape> > >*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c582eb)
    #13 0x7faa13b21c2c in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*> > const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aedc2c)
    #14 0x7faa13b2466a in tensorflow::ExecuteNode::Run() (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31af066a)
    #15 0x7faa13c5e939 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c2a939)
    #16 0x7faa13b11de4 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31addde4)
    #17 0x7faa13b15dd4 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31ae1dd4)
    #18 0x7faa13b1fd26 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31aebd26)
    #19 0x7faa02b08c33 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x20ad4c33)
    #20 0x7faa13c53e5e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x31c1fe5e)
    #21 0x7fa9f268645b in TFE_Execute (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x1065245b)
    #22 0x7faa4a034274 in TFE_Py_FastPathExecute_C(_object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../_pywrap_tensorflow_internal.so+0x3c2274)
    #23 0x7fa9ddff2ccb in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(pybind11::args)#61}&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0xb7ccb)
    #24 0x7fa9de109899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #25 0x51ad66  (/usr/bin/python3.11+0x51ad66)

SUMMARY: AddressSanitizer: heap-buffer-overflow (/mnt/venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e2a2d86) in tensorflow::CSRSparseCholeskyCPUOp<float>::Compute(tensorflow::OpKernelContext*)::{lambda(long, long)#1}::operator()(long, long) const
Shadow bytes around the buggy address:
  0x0c0e8008b9e0: 00 00 00 00 00 00 fa fa fa fa fd fd fd fd fd fd
  0x0c0e8008b9f0: fd fd fd fd fa fa fa fa 00 00 00 00 00 00 00 00
  0x0c0e8008ba00: 00 fa fa fa fa fa 00 00 00 00 00 00 00 00 04 fa
  0x0c0e8008ba10: fa fa fa fa 00 00 00 00 00 00 00 00 04 fa fa fa
  0x0c0e8008ba20: fa fa 00 00 00 00 00 00 00 00 04 fa fa fa fa fa
=>0x0c0e8008ba30: 00 00 00 00 00 00 00 00 00 00[fa]fa fa fa fa fa
  0x0c0e8008ba40: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba50: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba60: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba70: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c0e8008ba80: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
==3331846==ABORTING
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-26T12:42:50Z,1,0,https://github.com/tensorflow/tensorflow/issues/80847,Runtime Error
45,tensorflow/tensorflow,Aborted (core dumped) in `RaggedBincount`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value of size is close to the maximum value for the dtype, an integer overflow error will occur in following multiplication calculations.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

splits = tf.constant([0, 3, 5, 9], dtype=tf.int64)
values = tf.constant(1, shape=[3,3], dtype=tf.int64)
size = tf.constant(6522107765268123892, dtype=tf.int64)
weights = tf.constant(1, shape=[3,3], dtype=tf.float32)
counts = tf.raw_ops.RaggedBincount(splits=splits, values=values, size=size, weights=weights)
```


### Relevant log output

```shell
Status: INVALID_ARGUMENT: Encountered overflow when multiplying 3 with 6522107765268123892, result: -1
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-26T03:18:31Z,3,0,https://github.com/tensorflow/tensorflow/issues/80812,Runtime Error
46,tensorflow/tensorflow,This method creates a model with a 100% memory leak loop using model. fit(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

ubuntu 2.2 or mac m1

### Mobile device

ubuntu 2.2 or mac m1

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have tried various methods, but the memory is definitely leaking, it seems that the release of memory cannot keep up. Through the logs, it can be found that there is periodic memory recycling, but with the increase of time, there is still a clear upward trend

### Standalone code to reproduce the issue

```shell
import gc

import keras
import numpy as np
import psutil
from keras.optimizers import Adam
from keras.layers import Dense, Dropout, Input, LSTM
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import time
import json


num_samples = 6
num_features = 3
num_classes = 4
epochs = 50
batch_size = 2
identifier = ""test_model""
num_iterations = 500  

def build_model(X, num_classes):
    model = Sequential()
    model.add(Input(shape=(X.shape[1], X.shape[2])))
    model.add(LSTM(16, return_sequences=True))
    model.add(LSTM(16))
    model.add(Dropout(0.4))
    model.add(Dense(8, activation='tanh'))
    model.add(Dense(num_classes, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    return model


data_X = np.random.rand(num_samples, num_features)
data_Y = np.random.randint(0, num_classes, size=(num_samples, 1))  


data_Y = np.eye(num_classes)[data_Y.flatten()]  
print(type(data_X))

scaler = MinMaxScaler()
data_X_scaled = scaler.fit_transform(data_X)


train_X, test_X, train_Y, test_Y = train_test_split(data_X_scaled, data_Y, train_size=0.6, random_state=42)


train_X = np.expand_dims(train_X, axis=1)
test_X = np.expand_dims(test_X, axis=1)

best_loss = np.inf
best_model_data = None
for iteration in range(num_iterations):
   
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()
    model = build_model(train_X, num_classes)
 
    model_name = f""model_{iteration}""
    early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=0, restore_best_weights=True)
    print(f""Iteration {iteration + 1}/{num_iterations}"")
    process = psutil.Process()
    mem_info = process.memory_info()
    print(f""start Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size
    try:
        history = model.fit(train_X, train_Y, epochs=epochs, batch_size=batch_size, shuffle=True,
                            validation_data=(test_X, test_Y), verbose=0)
        current_loss = history.history['loss'][-1]
        print(f""Training model: {model.name}"")
    
        del model
        tf.keras.backend.clear_session()
        gc.collect()
    except Exception as e:
        print(""err:"", e)
    finally:
        process = psutil.Process()
        mem_info = process.memory_info()
        print(f""end Current memory usage: {mem_info.rss / (1024 * 1024):.2f} MB"")  # RSS - Resident Set Size

print(""end！"")


if best_model_data:
    model_json = best_model_data[""model_architecture""]
    model_weights = json.loads(best_model_data[""model_weights""], object_hook=lambda d: np.array(d))
    model = tf.keras.models.model_from_json(model_json)
    model.set_weights(model_weights)
    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy'])
    print(""ok"")
else:
    print(""not found"")
```


### Relevant log output

```shell
Iteration 1/500
start Current memory usage: 450.69 MB
Training model: sequential
end Current memory usage: 524.41 MB
Iteration 2/500
start Current memory usage: 524.52 MB
Training model: sequential
end Current memory usage: 564.97 MB
Iteration 3/500
start Current memory usage: 564.98 MB
Training model: sequential
end Current memory usage: 598.00 MB
Iteration 4/500
start Current memory usage: 598.03 MB
Training model: sequential
end Current memory usage: 624.69 MB
Iteration 5/500
start Current memory usage: 624.69 MB
Training model: sequential
end Current memory usage: 653.89 MB
Iteration 6/500
start Current memory usage: 653.91 MB
Training model: sequential
end Current memory usage: 679.45 MB
Iteration 7/500
start Current memory usage: 679.45 MB
Training model: sequential
end Current memory usage: 701.59 MB
Iteration 8/500
start Current memory usage: 701.59 MB
Training model: sequential
end Current memory usage: 726.83 MB
Iteration 9/500
start Current memory usage: 726.84 MB
Training model: sequential
end Current memory usage: 749.56 MB
Iteration 10/500
start Current memory usage: 749.56 MB
Training model: sequential
end Current memory usage: 782.56 MB
Iteration 11/500
start Current memory usage: 782.56 MB
Training model: sequential
end Current memory usage: 805.92 MB
Iteration 12/500
start Current memory usage: 805.92 MB
Training model: sequential
end Current memory usage: 833.17 MB
Iteration 13/500
start Current memory usage: 833.17 MB
Training model: sequential
end Current memory usage: 852.84 MB
Iteration 14/500
start Current memory usage: 852.84 MB
Training model: sequential
end Current memory usage: 875.05 MB
Iteration 15/500
start Current memory usage: 875.06 MB
Training model: sequential
end Current memory usage: 901.56 MB
Iteration 16/500
start Current memory usage: 901.56 MB
Training model: sequential
end Current memory usage: 930.62 MB
Iteration 17/500
start Current memory usage: 705.70 MB
Training model: sequential
end Current memory usage: 762.64 MB
Iteration 18/500
start Current memory usage: 762.70 MB
Training model: sequential
end Current memory usage: 798.06 MB
Iteration 19/500
start Current memory usage: 798.17 MB
Training model: sequential
end Current memory usage: 824.98 MB
Iteration 20/500
start Current memory usage: 824.98 MB
Training model: sequential
end Current memory usage: 850.34 MB
Iteration 21/500
start Current memory usage: 850.42 MB
Training model: sequential
end Current memory usage: 876.81 MB
Iteration 22/500
start Current memory usage: 876.81 MB
Training model: sequential
end Current memory usage: 904.02 MB
Iteration 23/500
start Current memory usage: 904.08 MB
Training model: sequential
end Current memory usage: 929.70 MB
Iteration 24/500
start Current memory usage: 929.73 MB
Training model: sequential
end Current memory usage: 952.33 MB
Iteration 25/500
start Current memory usage: 952.34 MB
Training model: sequential
end Current memory usage: 952.28 MB
Iteration 26/500
start Current memory usage: 952.47 MB
Training model: sequential
end Current memory usage: 980.39 MB
Iteration 27/500
start Current memory usage: 978.78 MB
Training model: sequential
end Current memory usage: 999.02 MB
Iteration 28/500
start Current memory usage: 999.05 MB
Training model: sequential
end Current memory usage: 1023.50 MB
Iteration 29/500
start Current memory usage: 1023.53 MB
Training model: sequential
end Current memory usage: 1047.80 MB
Iteration 30/500
start Current memory usage: 1047.83 MB
Training model: sequential
end Current memory usage: 1068.88 MB
Iteration 31/500
start Current memory usage: 1068.94 MB
Training model: sequential
end Current memory usage: 1095.78 MB
Iteration 32/500
start Current memory usage: 1095.78 MB
Training model: sequential
end Current memory usage: 1119.03 MB
Iteration 33/500
start Current memory usage: 1119.03 MB
Training model: sequential
end Current memory usage: 1039.41 MB
Iteration 34/500
start Current memory usage: 1022.78 MB
Training model: sequential
end Current memory usage: 1040.88 MB
Iteration 35/500
start Current memory usage: 1040.70 MB
Training model: sequential
end Current memory usage: 1054.58 MB
Iteration 36/500
start Current memory usage: 1054.58 MB
Training model: sequential
end Current memory usage: 1076.16 MB
Iteration 37/500
start Current memory usage: 1076.19 MB
Training model: sequential
end Current memory usage: 1097.02 MB
Iteration 38/500
start Current memory usage: 1097.03 MB
Training model: sequential
end Current memory usage: 1113.70 MB
Iteration 39/500
start Current memory usage: 1114.12 MB
Training model: sequential
end Current memory usage: 1140.30 MB
Iteration 40/500
start Current memory usage: 1140.33 MB
Training model: sequential
end Current memory usage: 1163.81 MB
Iteration 41/500
start Current memory usage: 1163.86 MB
Training model: sequential
end Current memory usage: 1195.83 MB
Iteration 42/500
start Current memory usage: 1195.83 MB
Training model: sequential
end Current memory usage: 1221.53 MB
Iteration 43/500
start Current memory usage: 1221.55 MB
Training model: sequential
end Current memory usage: 1231.09 MB
Iteration 44/500
start Current memory usage: 1231.14 MB
Training model: sequential
end Current memory usage: 1245.78 MB
Iteration 45/500
start Current memory usage: 1199.55 MB
Training model: sequential
end Current memory usage: 1221.59 MB
Iteration 46/500
start Current memory usage: 1221.59 MB
Training model: sequential
end Current memory usage: 1249.11 MB
Iteration 47/500
start Current memory usage: 1249.22 MB
Training model: sequential
end Current memory usage: 1275.50 MB
Iteration 48/500
start Current memory usage: 1259.83 MB
Training model: sequential
end Current memory usage: 1290.91 MB
Iteration 49/500
start Current memory usage: 1285.67 MB
Training model: sequential
end Current memory usage: 1296.75 MB
Iteration 50/500
start Current memory usage: 1296.75 MB
Training model: sequential
end Current memory usage: 1306.59 MB
Iteration 51/500
start Current memory usage: 1306.59 MB
Training model: sequential
end Current memory usage: 1287.53 MB
Iteration 52/500
start Current memory usage: 1287.53 MB
Training model: sequential
end Current memory usage: 1297.23 MB
Iteration 53/500
start Current memory usage: 1297.25 MB
Training model: sequential
end Current memory usage: 1285.45 MB
Iteration 54/500
start Current memory usage: 1285.45 MB
Training model: sequential
end Current memory usage: 1290.36 MB
Iteration 55/500
start Current memory usage: 1282.14 MB
Training model: sequential
end Current memory usage: 1302.14 MB
Iteration 56/500
start Current memory usage: 1302.14 MB
Training model: sequential
end Current memory usage: 1287.70 MB
Iteration 57/500
start Current memory usage: 1287.75 MB
Training model: sequential
end Current memory usage: 1282.77 MB
Iteration 58/500
start Current memory usage: 1271.38 MB
Training model: sequential
end Current memory usage: 1232.14 MB
Iteration 59/500
start Current memory usage: 1212.70 MB
Training model: sequential
end Current memory usage: 1201.16 MB
Iteration 60/500
start Current memory usage: 1200.53 MB
Training model: sequential
end Current memory usage: 1169.45 MB
Iteration 61/500
start Current memory usage: 1169.45 MB
Training model: sequential
end Current memory usage: 1209.73 MB
Iteration 62/500
start Current memory usage: 1207.19 MB
Training model: sequential
end Current memory usage: 1226.28 MB
Iteration 63/500
start Current memory usage: 1226.28 MB
Training model: sequential
end Current memory usage: 1231.45 MB
Iteration 64/500
start Current memory usage: 1210.11 MB
Training model: sequential
end Current memory usage: 1176.00 MB
Iteration 65/500
start Current memory usage: 1173.97 MB
Training model: sequential
end Current memory usage: 1201.42 MB
Iteration 66/500
start Current memory usage: 1201.42 MB
Training model: sequential
end Current memory usage: 1223.94 MB
Iteration 67/500
start Current memory usage: 1222.50 MB
Training model: sequential
end Current memory usage: 1229.80 MB
Iteration 68/500
start Current memory usage: 1227.14 MB
Training model: sequential
end Current memory usage: 1219.02 MB
Iteration 69/500
start Current memory usage: 1210.48 MB
Training model: sequential
end Current memory usage: 1247.17 MB
Iteration 70/500
start Current memory usage: 1245.94 MB
Training model: sequential
end Current memory usage: 1259.84 MB
Iteration 71/500
start Current memory usage: 1259.86 MB
Training model: sequential
end Current memory usage: 1286.39 MB
Iteration 72/500
start Current memory usage: 1286.53 MB
Training model: sequential
end Current memory usage: 1316.52 MB
Iteration 73/500
start Current memory usage: 1311.53 MB
Training model: sequential
end Current memory usage: 1338.72 MB
Iteration 74/500
start Current memory usage: 1338.75 MB
Training model: sequential
end Current memory usage: 1348.45 MB
Iteration 75/500
start Current memory usage: 1338.30 MB
Training model: sequential
end Current memory usage: 1354.97 MB
Iteration 76/500
start Current memory usage: 1353.83 MB
Training model: sequential
end Current memory usage: 1385.67 MB
Iteration 77/500
start Current memory usage: 1385.69 MB
Training model: sequential
end Current memory usage: 1408.83 MB
Iteration 78/500
start Current memory usage: 1408.88 MB
Training model: sequential
end Current memory usage: 1430.91 MB
Iteration 79/500
start Current memory usage: 1430.94 MB
Training model: sequential
end Current memory usage: 1443.62 MB
Iteration 80/500
start Current memory usage: 1428.00 MB
Training model: sequential
end Current memory usage: 1436.50 MB
Iteration 81/500
start Current memory usage: 1436.64 MB
Training model: sequential
end Current memory usage: 1454.66 MB
Iteration 82/500
start Current memory usage: 1440.91 MB
Training model: sequential
end Current memory usage: 1461.81 MB
Iteration 83/500
start Current memory usage: 1460.47 MB
Training model: sequential
end Current memory usage: 1481.19 MB
Iteration 84/500
start Current memory usage: 1481.19 MB
Training model: sequential
end Current memory usage: 1477.84 MB
Iteration 85/500
start Current memory usage: 1477.84 MB
Training model: sequential
end Current memory usage: 1493.55 MB
Iteration 86/500
start Current memory usage: 1493.58 MB
Training model: sequential
end Current memory usage: 1509.50 MB
Iteration 87/500
start Current memory usage: 1509.50 MB
Training model: sequential
end Current memory usage: 1543.94 MB
Iteration 88/500
start Current memory usage: 1542.83 MB
Training model: sequential
end Current memory usage: 1516.17 MB
Iteration 89/500
start Current memory usage: 1516.20 MB
Training model: sequential
end Current memory usage: 1470.17 MB
Iteration 90/500
start Current memory usage: 1470.22 MB
Training model: sequential
end Current memory usage: 1443.72 MB
Iteration 91/500
start Current memory usage: 1444.36 MB
Training model: sequential
end Current memory usage: 1486.23 MB
Iteration 92/500
start Current memory usage: 1476.41 MB
Training model: sequential
end Current memory usage: 1524.97 MB
Iteration 93/500
start Current memory usage: 1524.97 MB
Training model: sequential
end Current memory usage: 1534.94 MB
Iteration 94/500
start Current memory usage: 1551.98 MB
Training model: sequential
end Current memory usage: 1853.48 MB
Iteration 95/500
start Current memory usage: 1853.48 MB
Training model: sequential
end Current memory usage: 1790.12 MB
Iteration 96/500
start Current memory usage: 1792.27 MB
Training model: sequential
end Current memory usage: 1883.20 MB
Iteration 97/500
start Current memory usage: 1879.05 MB
Training model: sequential
end Current memory usage: 1759.69 MB
Iteration 98/500
start Current memory usage: 1669.66 MB
Training model: sequential
end Current memory usage: 1596.77 MB
Iteration 99/500
start Current memory usage: 1597.12 MB
Training model: sequential
end Current memory usage: 1568.83 MB
Iteration 100/500
start Current memory usage: 1532.98 MB
Training model: sequential
end Current memory usage: 1516.75 MB
Iteration 101/500
start Current memory usage: 1465.98 MB
Training model: sequential
end Current memory usage: 1486.66 MB
Iteration 102/500
start Current memory usage: 1483.34 MB
Training model: sequential
end Current memory usage: 1523.19 MB
Iteration 103/500
start Current memory usage: 1523.14 MB
Training model: sequential
end Current memory usage: 1532.77 MB
Iteration 104/500
start Current memory usage: 1531.14 MB
Training model: sequential
end Current memory usage: 1561.78 MB
Iteration 105/500
start Current memory usage: 1555.67 MB
Training model: sequential
end Current memory usage: 1586.70 MB
Iteration 106/500
start Current memory usage: 1586.75 MB
Training model: sequential
end Current memory usage: 1608.41 MB
Iteration 107/500
start Current memory usage: 1603.81 MB
Training model: sequential
end Current memory usage: 1629.00 MB
Iteration 108/500
start Current memory usage: 1629.05 MB
Training model: sequential
end Current memory usage: 1609.25 MB
Iteration 109/500
start Current memory usage: 1609.31 MB
Training model: sequential
end Current memory usage: 1630.09 MB
Iteration 110/500
start Current memory usage: 1629.20 MB
Training model: sequential
end Current memory usage: 1638.66 MB
Iteration 111/500
start Current memory usage: 1620.30 MB
Training model: sequential
end Current memory usage: 1642.81 MB
Iteration 112/500
start Current memory usage: 1642.94 MB
Training model: sequential
end Current memory usage: 1659.45 MB
Iteration 113/500
start Current memory usage: 1655.17 MB
Training model: sequential
end Current memory usage: 1687.80 MB
Iteration 114/500
start Current memory usage: 1673.33 MB
Training model: sequential
end Current memory usage: 1705.94 MB
Iteration 115/500
start Current memory usage: 1699.95 MB
Training model: sequential
end Current memory usage: 1708.22 MB
Iteration 116/500
start Current memory usage: 1707.88 MB
Training model: sequential
end Current memory usage: 1648.23 MB
Iteration 117/500
start Current memory usage: 1634.03 MB
Training model: sequential
end Current memory usage: 1670.97 MB
Iteration 118/500
start Current memory usage: 1671.97 MB
Training model: sequential
end Current memory usage: 1649.69 MB
Iteration 119/500
start Current memory usage: 1645.14 MB
Training model: sequential
end Current memory usage: 1698.64 MB
Iteration 120/500
start Current memory usage: 1699.69 MB
Training model: sequential
end Current memory usage: 1737.67 MB
Iteration 121/500
start Current memory usage: 1737.67 MB
Training model: sequential
end Current memory usage: 1738.05 MB
Iteration 122/500
start Current memory usage: 1721.47 MB
Training model: sequential
end Current memory usage: 1730.64 MB
Iteration 123/500
start Current memory usage: 1729.53 MB
Training model: sequential
end Current memory usage: 1766.12 MB
Iteration 124/500
start Current memory usage: 1761.22 MB
Training model: sequential
end Current memory usage: 1796.58 MB
Iteration 125/500
start Current memory usage: 1796.73 MB
Training model: sequential
end Current memory usage: 1709.02 MB
Iteration 126/500
start Current memory usage: 1721.67 MB
Training model: sequential
end Current memory usage: 1771.50 MB
Iteration 127/500
start Current memory usage: 1771.50 MB
Training model: sequential
end Current memory usage: 1777.38 MB
Iteration 128/500
start Current memory usage: 1757.58 MB
Training model: sequential
end Current memory usage: 1806.50 MB
Iteration 129/500
start Current memory usage: 1758.81 MB
Training model: sequential
end Current memory usage: 1812.45 MB
Iteration 130/500
start Current memory usage: 1812.86 MB
Training model: sequential
end Current memory usage: 1811.14 MB
Iteration 131/500
start Current memory usage: 1799.61 MB
Training model: sequential
end Current memory usage: 1835.33 MB
Iteration 132/500
start Current memory usage: 1716.38 MB
Training model: sequential
end Current memory usage: 1759.75 MB
Iteration 133/500
start Current memory usage: 1752.44 MB
Training model: sequential
end Current memory usage: 1818.41 MB
Iteration 134/500
start Current memory usage: 1811.42 MB
Training model: sequential
end Current memory usage: 1853.58 MB
Iteration 135/500
start Current memory usage: 1853.70 MB
Training model: sequential
end Current memory usage: 1858.50 MB
Iteration 136/500
start Current memory usage: 1858.56 MB
Training model: sequential
end Current memory usage: 1874.84 MB
Iteration 137/500
start Current memory usage: 1862.92 MB
Training model: sequential
end Current memory usage: 1768.23 MB
Iteration 138/500
start Current memory usage: 1762.73 MB
Training model: sequential
end Current memory usage: 1843.39 MB
Iteration 139/500
start Current memory usage: 1843.52 MB
Training model: sequential
end Current memory usage: 1885.88 MB
Iteration 140/500
start Current memory usage: 1885.95 MB
Training model: sequential
end Current memory usage: 1924.86 MB
Iteration 141/500
start Current memory usage: 1925.05 MB
Training model: sequential
end Current memory usage: 1946.80 MB
Iteration 142/500
start Current memory usage: 1946.69 MB
Training model: sequential
end Current memory usage: 1977.53 MB
Iteration 143/500
start Current memory usage: 1974.27 MB
Training model: sequential
end Current memory usage: 1995.17 MB
Iteration 144/500
start Current memory usage: 1992.41 MB
Training model: sequential
end Current memory usage: 1984.45 MB
Iteration 145/500
start Current memory usage: 1963.42 MB
Training model: sequential
end Current memory usage: 1947.31 MB
Iteration 146/500
start Current memory usage: 1944.47 MB
Training model: sequential
end Current memory usage: 1996.00 MB
Iteration 147/500
start Current memory usage: 1996.08 MB
Training model: sequential
end Current memory usage: 2008.41 MB
Iteration 148/500
start Current memory usage: 1999.69 MB
Training model: sequential
end Current memory usage: 1951.30 MB
Iteration 149/500
start Current memory usage: 1942.98 MB
Training model: sequential
end Current memory usage: 1992.28 MB
Iteration 150/500
start Current memory usage: 1982.86 MB
Training model: sequential
end Current memory usage: 2008.83 MB
Iteration 151/500
start Current memory usage: 2008.83 MB
Training model: sequential
end Current memory usage: 1946.42 MB
Iteration 152/500
start Current memory usage: 1946.92 MB
Training model: sequential
end Current memory usage: 1992.48 MB
Iteration 153/500
start Current memory usage: 1979.52 MB
Training model: sequential
end Current memory usage: 2035.66 MB
Iteration 154/500
start Current memory usage: 2023.91 MB
Training model: sequential
end Current memory usage: 2030.31 MB
Iteration 155/500
start Current memory usage: 1974.39 MB
Training model: sequential
end Current memory usage: 2029.30 MB
Iteration 156/500
start Current memory usage: 1997.42 MB
Training model: sequential
end Current memory usage: 2000.31 MB
Iteration 157/500
start Current memory usage: 1964.38 MB
Training model: sequential
end Current memory usage: 1979.45 MB
Iteration 158/500
start Current memory usage: 1973.12 MB
Training model: sequential
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF 2.18']",2024-11-25T12:12:55Z,7,1,https://github.com/tensorflow/tensorflow/issues/80753,Runtime Error
47,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.SparseConcat`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, `SparseConcat` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

indices1 = tf.constant(2, shape=[3,3], dtype=tf.int64)
values1 = tf.constant(""aaaabaaacaaadaaaeaaafaaagaaahaaaiaaajaaakaaalaaamaaanaaaoaaapaaaqaaaraaasaaataaauaaavaaawaaaxaaayaaazaabbaabcaabdaabeaabfaabgaabhaabiaabjaabkaablaabmaabnaaboaabpaabqaabraabsaabtaabuaabvaabwaabxaabyaabzaacbaaccaacdaaceaacfaacgaachaaciaacjaackaaclaacmaacnaacoaacpaacqaacraacsaactaacuaacvaacwaacxaacyaac"",
    shape=[3], dtype=tf.string)
shapes1 = tf.constant([5, 2, 2147483647], dtype=tf.int64)

indices2 = tf.constant(-2, shape=[4,3], dtype=tf.int64)
values2 = tf.constant("" "", shape=[4], dtype=tf.string)
shapes2 = tf.constant([5,1879048192,536870912], dtype=tf.int64)

concat_dim = 1
tf.raw_ops.SparseConcat(
    indices=[indices1, indices2], values=[values1, values2], shapes=[shapes1, shapes2], concat_dim=concat_dim, name=None
)
```


### Relevant log output

```shell
2024-11-24 06:36:10.994508: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements() status: INVALID_ARGUMENT: Shape [5,1879048194,2147483647] results in overflow when computing number of elements
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-24T06:40:09Z,3,0,https://github.com/tensorflow/tensorflow/issues/80669,Runtime Error
48,tensorflow/tensorflow,Floating point exception (core dumped) in `tf.raw_ops.Reshape`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs,  `tf.raw_ops.Reshape` triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tensor = tf.constant(-3.5e+35, shape=[5], dtype=tf.float32)
shape = tf.constant([0, 1879048192, 100000000, 1610612736, -1], dtype=tf.int32)

tf.raw_ops.Reshape(tensor=tensor, shape=shape)
```


### Relevant log output

```shell
Floating point exception (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-22T02:58:01Z,2,0,https://github.com/tensorflow/tensorflow/issues/80529,Runtime Error
49,tensorflow/tensorflow,The documentation in `data_performance.ipynb` uses `py_function()` without an explanation,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.18

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the [""Better performance with the tf.data API"" guide](https://www.tensorflow.org/guide/data_performance), `tf.py_function()` is used several times in the mapping function. There are various performance boosts demonstrated by the guide with said mapping functions. However, there does not seem to be any practical reason why `tf.py_function()` is used inside the mapping functions. In fact, if you remove those, the behavior is the same; in other words, there doesn't seem to be a need for them at all.

Curiously, if you remove them from the examples and then perform the time measurements, the speedup goes away. For example, consider the following mapping function from [the guide](https://github.com/tensorflow/docs/blob/bbc0b9c70fc0bd4411793d1b0bcc56ef1dbc2405/site/en/guide/data_performance.ipynb#L447-L450):

```python
def mapped_function(s):
    # Do some hard pre-processing
    tf.py_function(lambda: time.sleep(0.03), [], ())
    return s
```

In the sequential case, it is shown that the following mapping code results in a time measurement of 0.49222556600034295:

```python
benchmark(
    ArtificialDataset()
    .map(mapped_function)
)
```

And, the optimized (parallel) version as follows results in a time measurement of 0.36392719900049997:

```python
benchmark(
    ArtificialDataset()
    .map(
        mapped_function,
        num_parallel_calls=tf.data.AUTOTUNE
    )
)
```

But, if I remove the `tf.py_function()` from the mapping function, I get comparable measurements from both examples, namely, 0.22448736599994845 and 0.2266392660001202:

```python
def mapped_function(s):
    # Do some hard pre-processing
    lambda: time.sleep(0.03), [], ()
    return s
```

In fact, that measurement is even better, which makes me believe that this example is contrived to show a performance benefit by using `num_parallel_calls` when in fact TF is already optimizing the code without it. Frivolously wrapping the function in `tf.py_function()` is most likely  causing TensorFlow *not* to optimize the function. Thus, is `num_parallel_calls` even needed to achieve better performance?

### Standalone code to reproduce the issue

```shell
Mentioned in the above description.
```


### Relevant log output

_No response_","['type:docs-bug', 'stat:awaiting tensorflower', 'type:bug']",2024-11-20T16:01:24Z,3,0,https://github.com/tensorflow/tensorflow/issues/80365,UI/UX Bug
50,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.MatrixSolve`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the matrix argument is empty and the gpu is available, tf.raw_ops.MatrixSolve triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.MatrixSolve(matrix=tf.random.uniform([], dtype=tf.dtypes.double, maxval=1000000000), rhs=tf.random.uniform([1, 2], dtype=tf.dtypes.double, maxval=1000000000), adjoint=True)
```


### Relevant log output

```shell
2024-11-20 14:51:08.714846: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 14:51:08.775383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 14:51:08.852267: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 14:51:08.876168: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 14:51:08.931206: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 14:51:16.385650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 535 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 14:51:16.387914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 14:51:16.542383: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-20T06:53:21Z,2,0,https://github.com/tensorflow/tensorflow/issues/80331,Runtime Error
51,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.MatrixInverse`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the shape of the input argument is empty and the gpu is available, tf.raw_ops.MatrixInverse triggers a crash.
It can be reproduced on tf-nightly when the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.raw_ops.MatrixInverse(input=tf.cast(tf.random.uniform([], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex128),adjoint=True)
```


### Relevant log output

```shell
2024-11-20 10:46:15.940818: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-11-20 10:46:16.001155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-11-20 10:46:16.076386: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-11-20 10:46:16.100080: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-20 10:46:16.154057: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-11-20 10:46:23.889652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21903 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-11-20 10:46:23.891964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 71 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-11-20 10:46:24.756574: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-11-20T02:47:58Z,2,0,https://github.com/tensorflow/tensorflow/issues/80316,Runtime Error
52,tensorflow/tensorflow,model.fit fails when the number of rows exceeds Int32.MaxValue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.19.0-dev20241117

### Custom code

Yes

### OS platform and distribution

MacOS 15.1.0

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I would expect model.fit to handle training on extremely large NumPy arrays without limitations.

### Standalone code to reproduce the issue

```shell
import numpy as np
from keras import Sequential
from keras.layers import Dense

n = 2_147_483_648
x = np.zeros(n).astype(np.float32)
y = x

model = Sequential([
    Dense(64, activation=""relu"", input_shape=(1,)),
    Dense(1, activation=""sigmoid"")
])
model.compile(optimizer=""adam"", loss=""binary_crossentropy"")
model.fit(x=x,y=y, epochs=1, batch_size=1024, verbose=1)
```


### Relevant log output

```shell
ValueError: Invalid value in tensor used for shape: -2147483648
```
","['type:bug', 'TF 2.18']",2024-11-19T09:24:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/80241,Logical Bug
53,tensorflow/tensorflow,tf.range still miss some dtypes support,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

No

### Python version

Google Colab default

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Same issue as in https://github.com/tensorflow/tensorflow/issues/72365 but now with unsigned dtypes

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.range(10, delta=1, dtype='uint8')
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-7b6ccd0e0a16> in <cell line: 3>()
      1 import tensorflow as tf
      2 
----> 3 tf.range(10, delta=1, dtype='uint8')

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6000 def raise_from_not_ok_status(e, name) -> NoReturn:
   6001   e.message += ("" name: "" + str(name if name is not None else """"))
-> 6002   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   6003 
   6004 

InvalidArgumentError: Value for attr 'Tidx' of uint8 is not in the list of allowed values: bfloat16, half, float, double, int8, int16, int32, int64, uint16, uint32
	; NodeDef: {{node Range}}; Op<name=Range; signature=start:Tidx, limit:Tidx, delta:Tidx -> output:Tidx; attr=Tidx:type,default=DT_INT32,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_INT8, DT_INT16, DT_INT32, DT_INT64, DT_UINT16, DT_UINT32]> [Op:Range] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-11-14T17:40:39Z,1,0,https://github.com/tensorflow/tensorflow/issues/80039,Logical Bug
54,tensorflow/tensorflow,tf.cast to int8 produce wrong number,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.15 - 2.17

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

input is a long list include random numbers.
tf.cast to int8 output is  different from numpy cast.

The strange thing is if you truncate the long input list to a short list then things works fine.

The code is straight forward, you can reproduce it in the colab

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/18dYjvY6JQk79hVq8JsjcWEwIG5KBuF1v?usp=sharing
```


### Relevant log output

```shell
NumPy Casted values (int8): 
 [   0    0    0    0    1   -1    0    3   -3    2   -2    0   50   21
 -125   62   25   31  127   -9  117   61  123   47  -20   28   52   36
  -43  -45  -84  118   37  -17   -6  -79    1   75  -45  -60 -103  -63
   85 -112   76   96  -56   86  -32 -108 -105 -121   -2  121   86  -54
   91  -55   36 -119   -3  -36   95  127 -105  -60   37   -9 -106    7
  -31  105   13 -103 -123   79   17  -48 -108  -56  -87 -128   35  -94
  -45  118  -91   86  -63  -43   77    1 -127  -16  -71  -73  -76  -15
  -11]
TensorFlow Casted values (int8): 
 [   0 -128    0    0    1   -1    0    3   -3    2   -2    0  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  127  127
  127  127  127  127  127  127  127  127  127  127  127  127  -76  -15
  -11]
```
```
","['type:bug', 'comp:apis', '2.17']",2024-11-08T11:33:35Z,2,0,https://github.com/tensorflow/tensorflow/issues/79689,Logical Bug
55,tensorflow/tensorflow,Unable to register CUDA plug-ins runnung docker image latest-gpu-jypyter,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.18.0

### Custom code

No

### OS platform and distribution

docker desktop 4.35.1 , ubuntu 24.04.1 LTS, WSL 2.3.24.0

### Mobile device

None

### Python version

Python 3.11.0rc1 (provided by tensorflow docker image)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.7

### GPU model and memory

NVIDIA GeForce RTX 4060 Ti 16G

### Current behavior?

I Strictly followed the instructions provided in:
https://www.tensorflow.org/install/docker 
https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
Got correct results running a sample workload (as suggested in nvidia contaner toolkit installation manual)

![image](https://github.com/user-attachments/assets/6f8c74e6-e23d-4395-8ffc-314f69af5bc7)

Downloaded tensorflow/tensorflow latest-gpu-jupyter image and ran the container.
Opend a new jupyter notebook (http://127.0.0.1:8888/tree?token=...)
Importing tensorflow I wanted to check the GPU support.
Got error messages and empy available gpu list.
`2024-11-06 10:31:50.143673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-06 10:31:50.712357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.`



### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tf.__version__

print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
```


### Relevant log output

```shell
2024-11-06 10:31:50.143673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1730889110.283427      23 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1730889110.322596      23 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-11-06 10:31:50.712357: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
'2.18.0'

Num GPUs Available:  0
2024-11-06 10:31:54.748888: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (34)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.18']",2024-11-06T11:15:27Z,7,0,https://github.com/tensorflow/tensorflow/issues/79520,Dependency Issue
56,tensorflow/tensorflow,`tf.math.floormod` not throwing `Integer division by zero` error on GPU for tensor of int64 dtype,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

_No response_

### Current behavior?

Running `tf.math.floormod` with a tensor containing zeroes in the denominator tensor on GPU does not throw a division by zero error when the tensor has a dtype of `int64`.

[colab](https://colab.research.google.com/drive/1e053_hcmu0sHQcMPop-_WZR6ya1bw6dy?usp=sharing)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
A = tf.constant([[2,2],[2,2]], dtype=tf.int64)
B = tf.constant([[0,0],[0,0]], dtype=tf.int64)

with tf.device(""/gpu:0""):
    output_gpu = tf.math.floormod(A, B) # No error
    print(f""\nGPU: {output_gpu}\n"") # GPU: [[2 2] [2 2]]

with tf.device(""/cpu:0""):
    output_cpu = tf.math.floormod(A, B) 
    # InvalidArgumentError: Integer division by zero
```


### Relevant log output

```shell
GPU: [[2 2]
 [2 2]]

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-2-ba17e88e7095> in <cell line: 9>()
      8 
      9 with tf.device(""/cpu:0""):
---> 10     output_cpu = tf.math.floormod(A, B)
     11     # InvalidArgumentError: Integer division by zero

2 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   5981 def raise_from_not_ok_status(e, name) -> NoReturn:
   5982   e.message += ("" name: "" + str(name if name is not None else """"))
-> 5983   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   5984 
   5985 

InvalidArgumentError: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:CPU:0}} Integer division by zero [Op:FloorMod] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-11-01T00:42:37Z,1,0,https://github.com/tensorflow/tensorflow/issues/79162,Logical Bug
57,tensorflow/tensorflow,`tf.linalg.lstsq` producing outputs with large inconsistencies between CPU and GPU with `float32` tensors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3

### GPU model and memory

_No response_

### Current behavior?

Passing tensors of `float32` to 'tf.linalg.lstsq' is producing very different output from CPU and GPU.

### Standalone code to reproduce the issue
[colab](https://colab.research.google.com/drive/1Fyh3HQs39NhGlOLEdzpqPKVkznBe6Fe9?usp=sharing)
```shell
import tensorflow as tf
import numpy as np

A = tf.constant([[[[0.37454012, 0.9507143, 0.7319939, 0.5986585, 0.15601864], [0.15599452, 0.05808361, 0.8661761, 0.601115, 0.7080726,], [0.02058449, 0.96990985, 0.83244264,
                0.21233912, 0.18182497], [0.1834045, 0.30424225, 0.52475643, 0.43194503, 0.29122913], [0.6118529, 0.13949387, 0.29214466, 0.36636186, 0.45606998]]]], dtype=tf.float32)

B = tf.constant([[[[0.59241456, 0.04645041], [0.94888556, 0.965632,], [0.684233, 0.4401525,], [
                0.9093204, 0.25877997], [0.54671025, 0.18485446]]]], dtype=tf.float32)

with tf.device(""/gpu:0""):
    output_gpu = tf.linalg.lstsq(A, B)
    
with tf.device( ""/cpu:0""):
    output_cpu = tf.linalg.lstsq(A, B)

np.testing.assert_allclose(output_cpu, output_gpu.cpu(), atol=100) # AssertionError
```


### Relevant log output

```shell
AssertionError: 
Not equal to tolerance rtol=1e-07, atol=100

Mismatched elements: 2 / 10 (20%)
Max absolute difference: 112.95715
Max relative difference: 0.2963447
 x: array([[[[-170.69518 ,   28.379017],
         [ 164.85085 ,  -28.04222 ],
         [-273.4264  ,   46.952198],...
 y: array([[[[-241.07059 ,   40.25023 ],
         [ 232.80626 ,  -39.505226],
         [-386.38354 ,   66.00629 ],...
```
","['type:bug', 'comp:ops', 'TF 2.18']",2024-10-31T22:52:23Z,1,0,https://github.com/tensorflow/tensorflow/issues/79157,Logical Bug
58,tensorflow/tensorflow,Thread ID in TensorBoard Profiler Trace Viewer Could Be Negative,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17

### Custom code

No

### OS platform and distribution

Linux Mint 21.2

### Mobile device

_No response_

### Python version

3.11

### Bazel version

6.5.0

### GCC/compiler version

clang 14.0.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

While using TensorFlow's profiler, I found that some thread IDs displayed in the Trace Viewer are negative. As shown in the image below, thread names such as `tf_Compute/-1030865605`, `tf_data_private_threadpool/-102013...`, etc., have negative thread IDs.

![image](https://github.com/user-attachments/assets/71d32241-d617-4e52-bcbf-d3140ebef440)

My log file is also attached.

[20241030-161104.zip](https://github.com/user-attachments/files/17587809/20241030-161104.zip)

### Standalone code to reproduce the issue

```shell
(Just the profiler demo code) https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras
```


### Relevant log output

_No response_","['type:bug', 'awaiting PR merge', '2.17']",2024-10-31T13:36:36Z,2,0,https://github.com/tensorflow/tensorflow/issues/79128,Logical Bug
59,tensorflow/tensorflow,Significant Discrepancy in `tf.linalg.triangular_solve` Results Between CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.18.0-rc2-4-g6550e4bd802 2.18.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.4 LTS x86_64

### Mobile device

_No response_

### Python version

3.10.0

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using `tf.linalg.triangular_solve` with large matrices or specific triangular matrix conditions (e.g., `upper=True`, `transpose=True`, `unitriangular=True`), the GPU results significantly differ from the CPU results. 

The discrepancy includes extremely large Mean Absolute Error (MAE) values and infinite Mean Squared Error (MSE) values of the results, indicating a possible issue in the GPU implementation of the function.

### Standalone code to reproduce the issue

[colab](https://colab.research.google.com/drive/1zXcOAkxHV6snaMyWGMeKIukf5IVBd0Bh?usp=sharing)
[Safe Tensors](https://drive.google.com/file/d/1n1JyugXNAS9M2wmnk9u9vCVuHz0uE3R6/view?usp=sharing)

```python
import tensorflow as tf
import numpy as np
from safetensors.torch import load_file

def set_seed(seed_value=42):
    """"""Sets the random seed for reproducibility.""""""
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)

def tensorflow_version(input, cpu=True):
    set_seed()
    if cpu:
        device_string = ""/cpu:0""
    else:
        device_string = ""/gpu:0""

    with tf.device(device_string):
        b_tensor = tf.constant(input[""b""])
        A_tensor = tf.constant(input[""A""])

        upper = input.get(""upper"", True)
        transpose = input.get(""transpose"", False)
        unitriangular = input.get(""unitriangular"", False)

        solution = tf.linalg.triangular_solve(
            A_tensor, b_tensor, lower=not upper, adjoint=transpose
        )

        return {""triangular_solve_solution"": solution.numpy()}

def load_safe_tensor(file_path):
    safe_tensor_data = load_file(file_path)
    A_tensor = safe_tensor_data[""A""]
    b_tensor = safe_tensor_data[""b""]

    return {
        ""A"": tf.convert_to_tensor(A_tensor.numpy()),
        ""b"": tf.convert_to_tensor(b_tensor.numpy()),
    }

def calculate_differences(cpu_result, gpu_result):
    diff = np.abs(cpu_result - gpu_result)
    mae = np.mean(diff)
    mse = np.mean(diff**2)
    rmse = np.sqrt(mse)
    max_diff = np.max(diff)
    mean_relative_diff = np.mean(diff / (np.abs(cpu_result) + 1e-10))

    return {
        ""Mean Absolute Error"": mae,
        ""Mean Squared Error"": mse,
        ""Root Mean Squared Error"": rmse,
        ""Maximum Absolute Difference"": max_diff,
        ""Mean Relative Difference"": mean_relative_diff,
    }

def main():
    file_path = ""tensorflow_triangular_solve_3.safetensors""
    input_data = load_safe_tensor(file_path)
    input_data[""upper""] = True
    input_data[""transpose""] = True
    input_data[""unitriangular""] = True

    result_cpu = tensorflow_version(input_data, cpu=True)
    print(""CPU Result:"")
    print(result_cpu)

    if tf.config.list_physical_devices(""GPU""):
        result_gpu = tensorflow_version(input_data, cpu=False)
        print(""GPU Result:"")
        print(result_gpu)

        if result_gpu:
            cpu_solution = result_cpu[""triangular_solve_solution""]
            gpu_solution = result_gpu[""triangular_solve_solution""]
            differences = calculate_differences(cpu_solution, gpu_solution)
            for key, value in differences.items():
                print(f""{key}: {value}"")
    else:
        print(""GPU not available."")

if __name__ == ""__main__"":
    main()
```

#### **Issue Summary:**
- The `tf.linalg.triangular_solve` function produces vastly different results on the GPU compared to the CPU. The differences include an enormous Mean Absolute Error (MAE) and an infinite Mean Squared Error (MSE), indicating a severe discrepancy between the CPU and GPU results.
- The issue appears to be related to the use of specific arguments like `upper=True`, `transpose=True`, and `unitriangular=True`, suggesting that there might be a numerical precision or stability issue in the GPU implementation.
- It is unclear why these large discrepancies occur, but they point to a critical inconsistency that could impact numerical reliability for users depending on TensorFlow’s triangular solve functionality.


### Relevant log output

```shell
Mean Absolute Error: 1.566790629150662e+21
Mean Squared Error: inf
Root Mean Squared Error: inf
Maximum Absolute Difference: 1.4265324671452624e+26
```
","['type:bug', 'comp:ops', 'TF 2.18']",2024-10-31T04:34:40Z,1,0,https://github.com/tensorflow/tensorflow/issues/79090,Logical Bug
60,tensorflow/tensorflow,`lookup_ops.StaticVocabularyTable` and `lookup_ops.StaticVocabularyTableV1`  can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `segmentation fault issue` in TensorFlow when I used API `lookup_ops.StaticVocabularyTable` or `lookup_ops.StaticVocabularyTableV1` . I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build).
Please find the [gist](https://colab.research.google.com/drive/1NGnFQqFl6Jy_Xt7wBL3ynmSVs9AiFw_l?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import os
from tensorflow.python.ops import lookup_ops

def _createVocabFile(basename, values=('brain', 'salad', 'surgery')):
    vocabulary_file = os.path.join(""/tmp"", basename)
    with open(vocabulary_file, 'w') as f:
        f.write('\n'.join(values) + '\n')
    return vocabulary_file

vocab_file = _createVocabFile('feat_to_id_1.txt')
vocab_size = 9223372036854775807
oov_buckets = 1
table = lookup_ops.StaticVocabularyTable(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)
#lookup_ops.StaticVocabularyTableV1(lookup_ops.TextFileIdTableInitializer(vocab_file, vocab_size=vocab_size), oov_buckets, experimental_is_anonymous=True)
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-10-26T07:19:51Z,2,0,https://github.com/tensorflow/tensorflow/issues/78831,Runtime Error
61,tensorflow/tensorflow,"`gen_list_ops.tensor_list_concat_v2` aborts with ""Check failed: size >= 0 (0 vs. -1)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

Linux Ubuntu 20.04.3 LTS

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)

Please find the [[gist](https://colab.research.google.com/drive/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing)](https://colab.research.google.com/drive/1Y-ZQk0KgIGfzCMfLVdemJ2UqXLeQWKUd?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import gen_list_ops
from tensorflow.python.ops import list_ops

l = list_ops.tensor_list_reserve(element_dtype=dtypes.float32, element_shape=None, num_elements=3)
t = gen_list_ops.tensor_list_concat_v2(l, element_dtype=dtypes.float32, element_shape=list_ops._build_element_shape((None, 3)), leading_dims=[-1, 3, 5])
```


### Relevant log output

```shell
F tensorflow/core/framework/tensor_shape.cc:587] Check failed: size >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-10-26T07:15:16Z,1,0,https://github.com/tensorflow/tensorflow/issues/78830,Runtime Error
62,tensorflow/tensorflow,"`data_flow_ops.Barrier` aborts with ""Check failed: i >= 0 (0 vs. -100)"" ","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly  2.19.0-dev20241025

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that below code would crash on `tf-nightly 2.19.0-dev20241025` (nightly-build)

Please find the [gist](https://colab.research.google.com/drive/1OOOIvZ7brRDjRqshv36CA_55BlPbgI4e?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import data_flow_ops
from tensorflow.python.eager import context
import tensorflow as tf

with context.graph_mode():
    sess = tf.compat.v1.Session()
    with sess.as_default():
        b = data_flow_ops.Barrier((dtypes.float32, dtypes.float32), shapes=((), ()), name='B')
        keys = [b'a', b'b', b'c', b'd']
        values_0 = [10.0, 20.0, 30.0, 40.0]
        values_1 = [100.0, 200.0, 300.0, 400.0]
        insert_1_1_op = b.insert_many(-100, keys[0:2], values_1[0:2]) 
        insert_1_1_op.run()
```


### Relevant log output

```shell
F tensorflow/core/kernels/barrier_ops.cc:286] Check failed: i >= 0 (0 vs. -100)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.18']",2024-10-26T07:04:16Z,1,0,https://github.com/tensorflow/tensorflow/issues/78828,Runtime Error
63,tensorflow/tensorflow,Does TFLite dequantize opertor support constant buffer input,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 24.04
- TensorFlow installed from (source or binary): source 
- TensorFlow version (or github SHA if from source): the latest version

**Standalone code to reproduce the issue** 
<img width=""724"" alt=""image"" src=""https://github.com/user-attachments/assets/849a0952-90ce-42c4-b2dc-9a9e7333fb0b"">
The [tflite model for user input](https://github.com/fujunwei/tensorflow/blob/tflite_label_image/tensorflow/lite/examples/python/dequantize_input.tflite) can work as expected

<img width=""713"" alt=""image"" src=""https://github.com/user-attachments/assets/43cafc84-0ea6-4a06-84b4-4772c6cea8e8"">

The [tflite model with constant buffer](https://github.com/fujunwei/tensorflow/blob/tflite_label_image/tensorflow/lite/examples/python/dequantize_constant.tflite) can't compute, so does [dequantize](https://www.tensorflow.org/mlir/tfl_ops#tfldequantize_tfldequantizeop) support constant input?

**Any other info / logs**

You can also modify [dequantize_tester in the Repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/dequantize_tester.cc#L110) to reproduce this issue like below code:

1, Create a constant buffer
```
  const auto buffer_data = builder.CreateVector( reinterpret_cast<const uint8_t*>(constant_buffer.data()),
                                                                              constant_buffer.size());
  const auto buffer_index = base::checked_cast<uint32_t>(buffers.size());
  buffers.emplace_back(::tflite::CreateBuffer(builder, buffer_data));
```

2, Use the `buffer_index ` when creating tensor:
```
const std::array<flatbuffers::Offset<::tflite::Tensor>, 2> tensors{{
      ::tflite::CreateTensor(
          builder,
          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
          Unsigned() ? ::tflite::TensorType_UINT8 : ::tflite::TensorType_INT8,
          /*buffer=*/buffer_index , /*name=*/0,
          ::tflite::CreateQuantizationParameters(
              builder, /*min=*/0, /*max=*/0,
              builder.CreateVector<float>({InputScale()}),
              builder.CreateVector<int64_t>({InputZeroPoint()}))),
      ::tflite::CreateTensor(
          builder,
          builder.CreateVector<int32_t>(Shape().data(), Shape().size()),
          ::tflite::TensorType_FLOAT32,
          /*buffer=*/0, /*name=*/builder.CreateString(""dequantizeLinearOutput"")),
  }};

```","['stat:awaiting tensorflower', 'type:bug', 'comp:lite', 'ModelOptimizationToolkit', '2.17']",2024-10-25T08:28:42Z,4,0,https://github.com/tensorflow/tensorflow/issues/78748,Logical Bug
64,tensorflow/tensorflow,No kernels registered for op `Conv2DBackpropInputV2`,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The existence of operator `Conv2DBackpropInputV2` is described in the official website document.https://www.tensorflow.org/api_docs/python/tf/raw_ops/Conv2DBackpropInputV2.
However, during my actual execution, the following error message appears:

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_sizes = tf.constant(1, shape=[4], dtype=tf.int32)
filter_tensor = tf.constant(2, shape=[4], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInputV2(
    input=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)
```


### Relevant log output

```shell
2024-10-21 12:30:18.492624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Traceback (most recent call last):
  File ""/mnt/tests/Conv2DBackpropInput.py"", line 10, in <module>
    tf.raw_ops.Conv2DBackpropInputV2(
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/util/tf_export.py"", line 377, in wrapper
    return f(**kwargs)
           ^^^^^^^^^^^
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 2030, in conv2d_backprop_input_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""/mnt/origin/venv/tensorflow-nightly/lib/python3.11/site-packages/tensorflow/python/framework/ops.py"", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Conv2DBackpropInputV2}} = Conv2DBackpropInputV2[T=DT_INT32, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true]
All kernels registered for op Conv2DBackpropInputV2:
  <no registered kernels>
 [Op:Conv2DBackpropInputV2] name:
```
","['type:docs-bug', 'stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-10-21T12:34:35Z,1,0,https://github.com/tensorflow/tensorflow/issues/78431,Logical Bug
65,tensorflow/tensorflow,Overflow and Check fail in `tf.raw_ops.Conv2DBackpropInput`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Overflow : `input_sizes` is a one-dimensional tensor and contains maximum values
Check fail：`input_sizes`'s shape is [2] and The dimension of `filter` is less than 2

### Standalone code to reproduce the issue

```shell
Overflow:

import tensorflow as tf

input_sizes = tf.constant([1, 5, 5, 999999999999], shape=[4], dtype=tf.int32)
filter_tensor = tf.constant(3, shape=[3, 3, 3, 2], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInput(
    input_sizes=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)

Check fail:
```python
import tensorflow as tf

input_sizes = tf.constant(1, shape=[2], dtype=tf.int32)
filter_tensor = tf.constant(2, shape=[1], dtype=tf.float32)
out_backprop = tf.constant(5, shape=[1, 5, 5, 2], dtype=tf.float32)

strides = [1, 1, 1, 1]
padding = ""SAME""

tf.raw_ops.Conv2DBackpropInput(
    input_sizes=input_sizes,
    filter=filter_tensor,
    out_backprop=out_backprop,
    strides=strides,
    padding=padding
)
```
```


### Relevant log output

```shell
Overflow:

2024-10-21 11:40:49.417611: F tensorflow/core/kernels/mkl/mkl_conv_grad_input_ops.cc:578] Non-OK-status: tensor::MakeShape(input_tensor, &input_tf_shape)
Status: INVALID_ARGUMENT: Dimension -727379969 must be >= 0
Aborted (core dumped)
```

Check fail:
```
2024-10-21 11:16:29.937265: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 1)
Aborted (core dumped)
```
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-21T12:17:20Z,3,0,https://github.com/tensorflow/tensorflow/issues/78428,Runtime Error
66,tensorflow/tensorflow,Overflow in `tf.raw_ops.Fill`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Overflow in `tf.raw_ops.Fill` when there are too large values in `dims`.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1GDBN4lheNUIW704hsXVt3lW55UJue97S?usp=sharing
```


### Relevant log output

```shell
Kill
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-21T11:10:34Z,3,0,https://github.com/tensorflow/tensorflow/issues/78427,Runtime Error
67,tensorflow/tensorflow,Multi-threaded execution throws an exception (using GPU).,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.19.0-dev20241018

### Custom code

Yes

### OS platform and distribution

Ubuntu 24.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Multi-threaded execution throws an exception (using GPU).

### Standalone code to reproduce the issue

```shell
import concurrent

import numpy as np
import tensorflow as tf
print(tf.__version__)

executor = concurrent.futures.ThreadPoolExecutor()


def sum(x, axis):
    return tf.reduce_sum(x, axis=axis)


futures = []

for i in range(1000):
    futures.clear()
    for _ in range(4):
        x = tf.convert_to_tensor(np.random.rand(100, 100))
        futures.append(executor.submit(sum, x, 1))
        x = tf.convert_to_tensor(np.random.rand(100))
        futures.append(executor.submit(sum, x, 0))
    concurrent.futures.wait(
        futures, return_when=concurrent.futures.ALL_COMPLETED
    )
    [future.result() for future in futures]
```


### Relevant log output

```shell
W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at reduction_ops_common.h:147 : INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)
I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Invalid reduction dimension (1 for input with 1 dimension(s)

```
```
","['type:bug', 'comp:gpu']",2024-10-19T13:02:32Z,5,0,https://github.com/tensorflow/tensorflow/issues/78338,Runtime Error
68,tensorflow/tensorflow,[Incorrect Result] `tf.math.reciprocal` returns `NaN` on `inf` input on Linux.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17.0

### Custom code

No

### OS platform and distribution

AlmaLinux 9.4

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.math.reciprocal` returns `NaN` on Linux when input is `inf` or `-inf` and has dtype=complex128, shape >= 2.
The output is expected to be 0, since:
1. This behavior is not consistent with dtype=float64, where the output will be 0.
2. When input tensor contains only one value, the output will be 0.
3. The same code snippet will return different result on macOS, where the output is also 0.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

input = tf.constant(np.inf, dtype=tf.float64)
out = tf.math.reciprocal(input)
# tf.Tensor(0.0, shape=(), dtype=float64)
print(out)

input = tf.constant(np.inf, dtype=tf.complex128)
out = tf.math.reciprocal(input)
# tf.Tensor(0j, shape=(), dtype=complex128)
print(out)

input = tf.constant([np.inf, np.inf], dtype=tf.complex128)
out = tf.math.reciprocal(input)
# On Linux: tf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)
# On macOS: tf.Tensor([0.+0.j 0.+0.j], shape=(2,), dtype=complex128)
print(out)
```


### Relevant log output

```shell
AttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'
tf.Tensor(0.0, shape=(), dtype=float64)
tf.Tensor(0j, shape=(), dtype=complex128)
tf.Tensor([nan+nanj nan+nanj], shape=(2,), dtype=complex128)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-19T09:41:36Z,2,0,https://github.com/tensorflow/tensorflow/issues/78298,Logical Bug
69,tensorflow/tensorflow,"tf.linalg.expm fails to support half/float16 data type, which is inconsistent with doc","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

According to the documentation: https://www.tensorflow.org/api_docs/python/tf/linalg/expm
tf.linalg.expm is expected to accept float16 input, but it fails on float16 when actually running the following code.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
input = tf.constant(np.random.randn(1,1), dtype='float16')
out = tf.linalg.expm(input)
```
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node MatrixSolve}} = MatrixSolve[T=DT_HALF, adjoint=false]
All kernels registered for op MatrixSolve:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
 [Op:MatrixSolve] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-17T13:09:08Z,3,0,https://github.com/tensorflow/tensorflow/issues/78107,Logical Bug
70,tensorflow/tensorflow,DLPack with Int32 tensor on the GPU: inconsistent eager mode / graph mode / XLA,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-117097-gecf05620570 2.19.0-dev20241016

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hello,

I realize that `int32` is a special dtype in TensorFlow for historical reasons. It seems that the handling of GPU int32-typed tensors has evolved over time.

Currently, the `device` field of a tensor created with:
```py
with tf.device('gpu'):
    x = tf.constant([0,1,2], tf.int32)
```
*does* indicate it's a GPU tensor: `/job:localhost/replica:0/task:0/device:GPU:0`.

However, when exporting and re-importing it via DLPack, it comes back as a CPU tensor.
There even seems to be a unit test validating this:
https://github.com/tensorflow/tensorflow/blob/d3de971a7348ecaefdbb920e580c37ebde10d780/tensorflow/python/dlpack/dlpack_test.py#L75-L78


However, @jhoydis found that this is *not* consistent between modes. In particular, if the tensor goes through an XLA-compiled function, it will correctly live on the GPU even after a round-trip through DLPack. (See reproducer below).

Would it please be possible to revisit this behavior, so that **exporting an int32 GPU tensor via DLPack does result in a GPU DLPack capsule in all modes, not just XLA?**

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


def f_eager(x):
    return x
f_graph = tf.function()(f_eager)
f_xla = tf.function(jit_compile=True)(f_eager)


with tf.device('gpu'):
    x = tf.constant([0,1,2], tf.int32)
    print(""Original tensor:"", x.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(x)
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Default:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_eager(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Eager:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_graph(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""Graph:"", x_.device)

    dlcapsule = tf.experimental.dlpack.to_dlpack(f_xla(x))
    x_ = tf.experimental.dlpack.from_dlpack(dlcapsule)
    print(""XLA:"", x_.device)
```


### Relevant log output

```shell
Original tensor: /job:localhost/replica:0/task:0/device:GPU:0
Default: /job:localhost/replica:0/task:0/device:CPU:0
Eager: /job:localhost/replica:0/task:0/device:CPU:0
Graph: /job:localhost/replica:0/task:0/device:CPU:0
XLA: /job:localhost/replica:0/task:0/device:GPU:0
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'comp:gpu']",2024-10-17T08:59:51Z,1,0,https://github.com/tensorflow/tensorflow/issues/78091,Logical Bug
71,tensorflow/tensorflow,tf.math.special.bessel_* has inconsistent result with scipy,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Based on the documentation, special function such as bessel_y0 should have consistent result with scipy. However, when receiving `-inf`, it has inconsistent results with scipy. Please check the reproducible for details.

### Standalone code to reproduce the issue

```shell
import scipy
import numpy as np
import tensorflow as tf
x = tf.constant(-np.inf, dtype='float64')
print(""TF:"", tf.math.special.bessel_y1(x))
print(""Scipy: "", scipy.special.y1(x))
print(""TF:"", tf.math.special.bessel_y0(x))
print(""Scipy: "", scipy.special.y0(x))
print(""TF:"", tf.math.special.bessel_k0(x))
print(""Scipy: "", scipy.special.k0(x))
print(""TF:"", tf.math.special.bessel_k1(x))
print(""Scipy: "", scipy.special.k1(x))
```


### Relevant log output

```shell
TF: tf.Tensor(-inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(-inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(inf, shape=(), dtype=float64)
Scipy:  nan
TF: tf.Tensor(inf, shape=(), dtype=float64)
Scipy:  nan
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-14T15:45:19Z,1,0,https://github.com/tensorflow/tensorflow/issues/77864,Logical Bug
72,tensorflow/tensorflow,"tf.math.is_strictly_increasing's behavior is not clear on a (2,2) matrix","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When receiving this input:
```
x = tf.constant([[1,2],[2,3]])
```
`tf.math.is_strictly_increasing` outputs `False` instead of `True`.
Also, for a tensor with shape (1,3,3):
```
x = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],
  [-0.5704009,  -0.2167283,   0.2548743 ],
  [-0.14944994,  2.0107825,  -0.09678416]]])
```
It's output is still `False` instead of `True` even when x's first dimension only has one element.
Based on the description ""Elements of x are compared in row-major order."", it seems that elements in x are compared along row (i.e., the first dimension).
Therefore, to my understanding, if the first dimension contains only one element (such as 1x3x3 shape tensor), the output should be True. If the input is [[1,2],[3,4]], the output should also be `True` since the value in the first dimension is increasing (from `[1,2]` to `[3,4]`)


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([[1,2],[2,3]])
print(tf.math.is_strictly_increasing(x))  # False
x = tf.constant([[[-0.3188535,  -1.6029806,  -1.5352179],
  [-0.5704009,  -0.2167283,   0.2548743 ],
  [-0.14944994,  2.0107825,  -0.09678416]]])
print(tf.math.is_strictly_increasing(x))  # False
```
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-14T15:28:21Z,2,0,https://github.com/tensorflow/tensorflow/issues/77863,Logical Bug
73,tensorflow/tensorflow,argmax returns incorrect result for input containing Minimum number (TensorFlow 2.x),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Current Behavior:
When using tf.math.argmax on an input array that contains -0.0, the result is incorrect. Specifically, the function returns 1 (the index of -0.0) as the position of the maximum value, while the actual maximum value is 1.401298464324817e-45 at index 2.

The same behavior is observed in Keras and JAX, as both use TensorFlow internally for the argmax function.

Expected Behavior:
tf.math.argmax should return 2, as the value at index 2 (1.401298464324817e-45) is greater than both -1.0 and -0.0.
```
import numpy as np
import torch
import tensorflow as tf
import jax.numpy as jnp
from tensorflow import keras

def test_argmax():
    # Input data
    input_data = np.array([-1.0, -0.0, 1.401298464324817e-45], dtype=np.float32)

    # PyTorch argmax
    pytorch_result = torch.argmax(torch.tensor(input_data, dtype=torch.float32)).item()
    print(f""PyTorch argmax result: {pytorch_result}"")

    # TensorFlow argmax
    tensorflow_result = tf.math.argmax(input_data).numpy()
    print(f""TensorFlow argmax result: {tensorflow_result}"")

    # Keras argmax (Keras internally uses TensorFlow, so should be the same)
    keras_result = keras.backend.argmax(input_data).numpy()
    print(f""Keras argmax result: {keras_result}"")

    # JAX argmax
    jax_result = jnp.argmax(input_data)
    print(f""JAX argmax result: {jax_result}"")

if __name__ == ""__main__"":
    test_argmax()

```


### Standalone code to reproduce the issue

```shell
PyTorch argmax result: 2
TensorFlow argmax result: 1
Keras argmax result: 1
JAX argmax result: 1

```
```


### Relevant log output

_No response_","['type:bug', 'comp:ops', 'TF 2.16']",2024-10-14T10:07:59Z,3,0,https://github.com/tensorflow/tensorflow/issues/77853,Logical Bug
74,tensorflow/tensorflow,argsort incorrectly handles very small floating-point numbers and -0.0 compared to other libraries (PyTorch and JAX),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using TensorFlow's argsort function on an array containing small floating-point numbers and both 0.0 and -0.0, the sort order is incorrect compared to other deep learning libraries such as PyTorch and JAX. TensorFlow incorrectly places 1.401298464324817e-45 (a very small positive number) before 0.0 and -0.0.

Expected behavior is that both 0.0 and -0.0 should be treated as equivalent and placed before any positive number, including very small ones like 1.401298464324817e-45. However, TensorFlow does not follow this behavior, whereas PyTorch correctly handles this.
```
import numpy as np
import torch
import tensorflow as tf
import jax.numpy as jnp

def test_argsort():
    # Input data, hardcoded as float32
    input_data = np.array([
        -0.0, 1.401298464324817e-45, 1.100000023841858, -0.0,
        5.960464477539063e-08, -2.0000100135803223, 1000000.0,
        722801.375, 0.0, -1.100000023841858
    ], dtype=np.float32)

    # PyTorch argsort
    pytorch_result = torch.argsort(torch.tensor(input_data, dtype=torch.float32)).numpy()
    print(f""PyTorch argsort result: {pytorch_result}"")

    # TensorFlow argsort
    tensorflow_result = tf.argsort(input_data).numpy().astype(np.int32)
    print(f""TensorFlow argsort result: {tensorflow_result}"")

    # JAX argsort
    jax_result = jnp.argsort(input_data).astype(np.int32)
    print(f""JAX argsort result: {jax_result}"")

if __name__ == ""__main__"":
    test_argsort()

```

### Standalone code to reproduce the issue

```shell
PyTorch argsort result: [5 9 0 3 8 1 4 2 7 6]
TensorFlow argsort result: [5 9 0 1 3 8 4 2 7 6]
JAX argsort result: [5 9 0 1 3 8 4 2 7 6]
```
Expected Behavior:
TensorFlow's argsort should place 0.0 and -0.0 before any positive number, including very small values like 1.401298464324817e-45. PyTorch demonstrates the correct behavior by treating 0.0 and -0.0 as equal and placing them in the correct order relative to other values.

Standalone Code to Reproduce the Issue:
The above Python code demonstrates the issue. It uses the same input data for PyTorch, TensorFlow, and JAX to show the difference in behavior. TensorFlow and JAX produce incorrect results by misplacing the small positive value before 0.0, while PyTorch produces the correct order.

Relevant Log Output:
No error logs are generated, but the incorrect behavior is clearly shown in the sorting results.
```


### Relevant log output

_No response_","['type:bug', 'TF 2.16']",2024-10-14T09:02:22Z,2,0,https://github.com/tensorflow/tensorflow/issues/77849,Logical Bug
75,tensorflow/tensorflow,Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT3D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If the value contained in fft_length is the maximum value, it will cause an abort

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant(0, shape=[2,0,0,0] ,dtype=tf.complex64)
fft_length = tf.constant(1879048192, shape=[3], dtype=tf.int32)

tf.raw_ops.IRFFT3D(input=input, fft_length=fft_length)
```


### Relevant log output

```shell
2024-10-13 13:04:53.308156: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()
Status: INVALID_ARGUMENT: Shape [2,1879048192,1879048192,1879048192] results in overflow when computing number of elements
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-13T13:07:08Z,1,0,https://github.com/tensorflow/tensorflow/issues/77824,Runtime Error
76,tensorflow/tensorflow,Aborted (core dumped) due to Overflow : `tf.raw_ops.IRFFT2D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.17.0 tf2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Since the value in fft_length is a maximum value, it will cause abort

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant(0, shape=[1,4,10,0,0] ,dtype=tf.complex64)
fft_length = tf.constant(2147483647, shape=[2], dtype=tf.int32)

tf.raw_ops.IRFFT2D(input=input, fft_length=fft_length)
```


### Relevant log output

```shell
2024-10-13 12:59:11.295197: F tensorflow/core/framework/tensor_shape.cc:607] Non-OK-status: RecomputeNumElements()
Status: INVALID_ARGUMENT: Shape [1,4,10,2147483647,2147483647] results in overflow when computing number of elements
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-13T13:04:05Z,1,0,https://github.com/tensorflow/tensorflow/issues/77823,Runtime Error
77,tensorflow/tensorflow,Gradients of tf.linalg.expm not supported with JIT compilation,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tested 2.17 and 2.10, both have the issue

### Custom code

Yes

### OS platform and distribution

Ubuntu

### Mobile device

_No response_

### Python version

tested 3.9 and 3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Gradients of `tf.linalg.expm` can not be computed with JIT compilation. 

This is an issue, because tf 2.17 seems to have activated jit compilation for compiled models per default whereas earlier versions did not, breaking existing code.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

A = tf.Variable([[.4, 1.5], [.6, .1]], dtype=tf.float32)

@tf.function(jit_compile=True) #set jit_compile=False to make it work
def f(A):
    with tf.GradientTape() as tape:
        B = tf.linalg.expm(A)
    return tape.gradient(B, A)

f(A)
```


### Relevant log output

```shell
2024-10-11 11:17:27.281304: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: XLA compilation requires a fixed tensor list size. Set the max number of elements. This could also happen if you're using a TensorArray in a while loop that does not have its maximum_iteration set, you can fix this by setting maximum_iteration to a suitable value.

Stack trace for op definition: 
File ""<frozen runpy>"", line 198, in _run_module_as_main
File ""<frozen runpy>"", line 88, in _run_code
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel_launcher.py"", line 18, in <module>
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/traitlets/config/application.py"", line 1075, in launch_instance
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelapp.py"", line 739, in start
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/tornado/platform/asyncio.py"", line 205, in start
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py"", line 641, in run_forever
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/base_events.py"", line 1986, in _run_once
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/asyncio/events.py"", line 88, in _run
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 534, in process_one
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/ipkernel.py"", line 362, in execute_request
File ""/home/beckerf/mambaforge/envs/learnMSAdev2/lib/python3.12/site-packages/ipykernel/kernelbase.py"",
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-11T12:31:35Z,1,0,https://github.com/tensorflow/tensorflow/issues/77693,Logical Bug
78,tensorflow/tensorflow,tf.custom_gradient for function with kwarg shows unexpected behavior,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.5 LTS

### Mobile device

_No response_

### Python version

3.12.7

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3/8

### GPU model and memory

_No response_

### Current behavior?

I have a function that takes two tensors as inputs, one as argument and one as keyword argument.
The function has a custom gradient.

When ``tape.gradient`` for both input tensors with respect to the output of the function is called, TensorFlow throws an error, saying that only one gradient is expect and not two.

When the function is called with both inputs as arguments (and not one of them as kwarg), no error is thrown.


### Standalone code to reproduce the issue

```shell
@tf.custom_gradient
def func(x, y=0):
    z = 2*x + y
    def grad(dz):
        dx = 2*dz
        dy = dz
        return dx, dy
    return z, grad
x = tf.constant(2.)
y = tf.constant(3.)
with tf.GradientTape() as tape:
    tape.watch([x, y])
    z = func(x, y=y) #func(x, y) does not generate the error
grads = tape.gradient(z, [x, y])
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[129], line 14
     12     tape.watch([x, y])
     13     z = func(x, y=y)
---> 14 grads = tape.gradient(z, [x, y])

File ~/.local/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py:1066, in GradientTape.gradient(self, target, sources, output_gradients, unconnected_gradients)
   1060   output_gradients = (
   1061       composite_tensor_gradient.get_flat_tensors_for_gradients(
   1062           output_gradients))
   1063   output_gradients = [None if x is None else ops.convert_to_tensor(x)
   1064                       for x in output_gradients]
-> 1066 flat_grad = imperative_grad.imperative_grad(
   1067     self._tape,
   1068     flat_targets,
   1069     flat_sources,
   1070     output_gradients=output_gradients,
   1071     sources_raw=flat_sources_raw,
   1072     unconnected_gradients=unconnected_gradients)
   1074 if not self._persistent:
   1075   # Keep track of watched variables before setting tape to None
   1076   self._watched_variables = self._tape.watched_variables()

File ~/.local/lib/python3.12/site-packages/tensorflow/python/eager/imperative_grad.py:67, in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
     63 except ValueError:
     64   raise ValueError(
     65       ""Unknown value for unconnected_gradients: %r"" % unconnected_gradients)
---> 67 return pywrap_tfe.TFE_Py_TapeGradient(
     68     tape._tape,  # pylint: disable=protected-access
     69     target,
     70     sources,
     71     output_gradients,
     72     sources_raw,
     73     compat.as_str(unconnected_gradients.value))

File ~/.local/lib/python3.12/site-packages/tensorflow/python/ops/custom_gradient.py:588, in _eager_mode_decorator.<locals>.actual_grad_fn(*result_grad_components)
    585 flat_grads = composite_tensor_gradient.get_flat_tensors_for_gradients(
    586     nest.flatten(input_grads))
    587 if len(flat_grads) != arg_count:
--> 588   raise ValueError(
    589       f""custom_gradient function expected to return {arg_count} ""
    590       f""gradients, but returned {len(flat_grads)} instead."")
    591 return flat_grads + variable_grads

ValueError: custom_gradient function expected to return 1 gradients, but returned 2 instead.
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-10T10:18:54Z,1,0,https://github.com/tensorflow/tensorflow/issues/77559,Logical Bug
79,tensorflow/tensorflow,Backward compatibility issue: failure to load models saved in TensorFlow format (Keras 2) in TensorFlow 2.17,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.9.1 (model saved), 2.17.0 (model loaded)

### Custom code

Yes

### OS platform and distribution

(Official Docker Image) Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.8.10 (model saved),  3.11.0rc1 (model loaded)

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

## Description

I have encountered a backward compatibility issue when loading models saved with Keras 2 in TensorFlow 2.9 into TensorFlow 2.17, which now uses Keras 3 API. This issue impacts various loading methods, and there does not appear to be a straightforward solution to resolve the errors.

## Steps to reproduce

1. **Train and export a model in TensorFlow 2.9 with Keras 2 API**:
   - A simple Keras sequential model is created and trained on random data.
   - The model is saved using both `tf.saved_model.save` and `tf.keras.models.save_model` with `tf` save format (which is unsupported in Keras 3).
   
2. **Attempt to load the models in TensorFlow 2.17 with Keras 3 API**:
   - The models are loaded using TensorFlow’s `tf.saved_model.load`, `keras.layers.TFSMLayer`, and `tf.keras.models.load_model`.

3. **Observe the errors**:
   - When loading using `tf.saved_model.load`, the error `'_UserObject' object has no attribute 'add_slot'` occurs.
   - When loading using `keras.layers.TFSMLayer`, the same `'_UserObject' object has no attribute 'add_slot'` error is triggered.
   - When loading using `tf.keras.models.load_model`, a different error appears: `File format not supported.` Because Keras 3 has dropped support for the default `tf` save format in version 2!

## Expected behavior

While I understand that issues related to loading legacy Keras models saved with the `tf` save format using Keras are out of scope for TensorFlow and should be addressed by the Keras team, the functionality surrounding TensorFlow's `tf.saved_model`, which uses the `SavedModel` bundle, is part of TensorFlow Core. Since this format is shared across different runtimes, it should remain backward compatible. Therefore, models saved in earlier versions of TensorFlow using the `SavedModel` format should load seamlessly in newer TensorFlow versions, without requiring users to rebuild their models or encountering compatibility errors.


### Standalone code to reproduce the issue

## Minimal example to reproduce the issue

A minimal code example to reproduce the issue is available in this repository: [Reproduce TF Model Compatibility Issue.](https://github.com/arianmaghsoudnia/reproduce-tf-model-compat-issue)

Please follow the steps in the README file.



### Relevant log output

_No response_","['type:bug', 'comp:keras', '2.17']",2024-10-09T10:50:39Z,3,0,https://github.com/tensorflow/tensorflow/issues/77356,
80,tensorflow/tensorflow,tf.nn.conv2d terminates process with invalid input shape instead of raising an exception,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorFlow terminates the process when passing an invalid input shape to `tf.nn.conv2d`. Instead of raising a Python exception that can be caught with a try-except block.

I expected TensorFlow to raise a catchable Python exception indicating that the input tensor shape is invalid. This would allow the error to be handled in a try-except block, instead of terminating the process. The error message should clearly explain the shape mismatch issue.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Define invalid input tensor and kernel
input_tensor = [[1.0, 2.0, 3.0]]
kernel = [[0.5, 0.5], [0.5, 0.5]]

try:
    # Create TensorFlow constants
    input_tf = tf.constant(input_tensor, dtype=tf.float32)
    kernel_tf = tf.constant(kernel, dtype=tf.float32)
    
    # Attempt to perform convolution, expecting an error
    output_tf = tf.nn.conv2d(
        tf.expand_dims(input_tf, axis=0), 
        tf.expand_dims(kernel_tf, axis=0), 
        strides=[1, 1, 1, 1], 
        padding='VALID'
    )
    
    print(""TensorFlow Output:"", output_tf.numpy())
except Exception as e:
    print(""TensorFlow Error:"", e)
```


### Relevant log output

```shell
2024-10-09 14:53:31.118589: F ./tensorflow/core/util/tensor_format.h:427] Check failed: index >= 0 && index < num_total_dims Invalid index from the dimension: 3, 0, C Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-09T07:52:38Z,5,0,https://github.com/tensorflow/tensorflow/issues/77336,Logical Bug
81,tensorflow/tensorflow,RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Google Colab with Python 3.10.12
- TensorFlow installation (pip package or built from source):
pip package
- TensorFlow library (version, if pip package or github SHA, if built from source):
v2.17.0

### 2. Code

```
import tensorflow as tf

saved_model_dir = '/content/saved_model'

num_calibration_steps = 100

input = tf.cast(tf.random.normal((1, 640, 640, 3)), tf.float32)
dummy_input = tf.cast(tf.random.normal((1, 2)), tf.int64)

def representative_dataset_gen():
    for _ in range(num_calibration_steps):
        yield [dummy_input, input] #model has 2 input tensors

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
  tf.lite.OpsSet.SELECT_TF_OPS
]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_quant_model = converter.convert()

# Save the quantized model to a local file
with open('quantized_model.tflite', 'wb') as f:
    f.write(tflite_quant_model)
```

### 3. Failure after conversion

After converting the model from ONNX using onnx2tf, I got saved_model, which I tried to convert to int8 quantized model using the code above. When trying to inference the model, after reading the model by the interpreter and calling the function `allocate_tensors()` 
```
interpreter = tf.lite.Interpreter(model_path=""/content/quantized_model.tflite"")
interpreter.allocate_tensors()
```
I get the following error:

```
RuntimeError                              Traceback (most recent call last)
[<ipython-input-7-b6b80a3bdf94>](https://localhost:8080/#) in <cell line: 6>()
      4 interpreter = tf.lite.Interpreter(model_path=""/content/quantized_model.tflite"")
      5 print(interpreter.get_input_details())
----> 6 interpreter.allocate_tensors()

[/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/interpreter.py](https://localhost:8080/#) in allocate_tensors(self)
    535   def allocate_tensors(self):
    536     self._ensure_safe()
--> 537     return self._interpreter.AllocateTensors()
    538 
    539   def _safe_to_run(self):

RuntimeError: failed to create XNNPACK runtimeNode number 2977 (TfLiteXNNPackDelegate) failed to prepare.
```

Could someone give me some advice, suggestions on how to solve this error? I couldn't even find that anyone has solved the same problem. 
The closest to this error is this [issue](https://github.com/tensorflow/tensorflow/issues/61395), but the workaround is to convert the ONNX model to Keras and for my complex model it is not possible to fix.
","['stat:awaiting tensorflower', 'type:bug', 'comp:lite', 'TFLiteConverter', '2.17']",2024-10-08T22:02:42Z,12,0,https://github.com/tensorflow/tensorflow/issues/77293,Runtime Error
82,tensorflow/tensorflow,"tensorflow.python.ops.signal.dct_ops.dct aborts with ""Assertion failure no zero-sized FFTs""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241007

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)
Please find the [gist](https://colab.research.google.com/drive/1oBjZoqp6WZn_VU-CTxZ3bspUc6v9D51s?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.eager import def_function
from tensorflow.python.framework import tensor_spec
from tensorflow.python.ops.signal import dct_ops

def test_with_dynamic_dimensions(dct_type, norm, shape, dtype):
    @def_function.function
    def func(signals):
        return dct_ops.dct(signals, n=norm, type=dct_type, norm=None)
    signals_spec = tensor_spec.TensorSpec([None] * len(shape), dtype)
    f = func.get_concrete_function(signals_spec)
    f(np.zeros([0], dtype=dtype))
test_with_dynamic_dimensions(3, None, [3], np.float32)
```


### Relevant log output

```shell
DUCC FFT c2r failed: 
bazel-out/k8-opt/bin/external/ducc/_virtual_includes/fft/ducc/src/ducc0/fft/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):

Assertion failure
no zero-sized FFTs

Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-10-08T06:57:49Z,1,0,https://github.com/tensorflow/tensorflow/issues/77211,
83,tensorflow/tensorflow,tensorflow.python.ops.parsing_ops.parse_single_sequence_example can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.19.0-dev20241007

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

Linux Ubuntu 20.04.3 LTS

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have confirmed that above code would crash on `tf-nightly 2.19.0-dev20241007` (nightly-build)

Please find the [gist](https://colab.research.google.com/drive/17PzKxkDEr3N8E9D9Kk_mT2A1LyPpoZZe?usp=sharing) to reproduce the issue.

### Standalone code to reproduce the issue

```shell
from tensorflow.core.example import example_pb2
from tensorflow.core.example import feature_pb2
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.ops import parsing_ops
example = example_pb2.Example
feature = feature_pb2.Feature
features = lambda d: feature_pb2.Features(feature=d)
bytes_feature = lambda v: feature(bytes_list=feature_pb2.BytesList(value=v))
int64_feature = lambda v: feature(int64_list=feature_pb2.Int64List(value=v))
float_feature = lambda v: feature(float_list=feature_pb2.FloatList(value=v))
feature_list = lambda l: feature_pb2.FeatureList(feature=l)
feature_lists = lambda d: feature_pb2.FeatureLists(feature_list=d)
sequence_example = example_pb2.SequenceExample

def testSequenceExampleListWithWrongShapeFails():
    original = sequence_example(feature_lists=feature_lists({'a': feature_list([int64_feature([2, 3]), int64_feature([2, 3, 4])])}))
    serialized = original.SerializeToString()
    parsing_ops.parse_single_sequence_example(**
        ({
            'example_name': 'in1',
            'serialized': ops.convert_to_tensor(serialized),
            'sequence_features': {'a': parsing_ops.FixedLenSequenceFeature((0, 0), dtypes.int64)}
        }))
testSequenceExampleListWithWrongShapeFails()
```


### Relevant log output

```shell
Floating point exception (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops']",2024-10-08T06:52:07Z,1,0,https://github.com/tensorflow/tensorflow/issues/77210,Runtime Error
84,tensorflow/tensorflow,`SimpleDynamicBuffer::AddString` is calling `memcpy` with null data,"I've noticed this hitting on our ubsan builds recently:

```
../../third_party/tflite/src/tensorflow/compiler/mlir/lite/utils/string_utils.cc:32:10: runtime error: null pointer passed as argument 1, which is declared to never be null
../../build/linux/debian_bullseye_amd64-sysroot/usr/include/string.h:44:28: note: nonnull attribute specified here
    #0 0x5a36de826450 in mlir::TFL::SimpleDynamicBuffer::AddString(char const*, unsigned long) third_party/tflite/src/tensorflow/compiler/mlir/lite/utils/string_utils.cc:32:3
    #1 0x5a36de825d3e in tflite::DynamicBuffer::AddString(char const*, unsigned long) third_party/tflite/src/tensorflow/lite/string_util.cc:37:28
    #2 0x5a36de82924d in PopulateTensor<std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char> > > third_party/tflite_support/src/tensorflow_lite_support/cc/task/core/task_utils.h:125:13
    #3 0x5a36de82924d in tflite::task::processor::UniversalSentenceEncoderPreprocessor::Preprocess(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/processor/universal_sentence_encoder_preprocessor.cc:58:3
    #4 0x5a36de81d3f7 in tflite::task::text::TextEmbedder::Preprocess(std::__Cr::vector<TfLiteTensor*, std::__Cr::allocator<TfLiteTensor*>> const&, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/text/text_embedder.cc:174:25
    #5 0x5a36de81cd8c in tflite::task::core::BaseTaskApi<tflite::task::processor::EmbeddingResult, std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&>::InferWithFallback(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/core/base_task_api.h:146:5
    #6 0x5a36de81cc40 in tflite::task::text::TextEmbedder::Embed(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&) third_party/tflite_support/src/tensorflow_lite_support/cc/task/text/text_embedder.cc:169:10
    #7 0x5a36d2728c24 in ai_chat::TextEmbedder::EmbedText(std::__Cr::basic_string<char, std::__Cr::char_traits<char>, std::__Cr::allocator<char>> const&, tflite::task::processor::EmbeddingResult&) brave/components/ai_chat/core/browser/text_embedder.cc:271:49
    #8 0x5a36d2728073 in ai_chat::TextEmbedder::EmbedSegments() brave/components/ai_chat/core/browser/text_embedder.cc:287:19
    #9 0x5a36c8c67059 in ai_chat::TextEmbedderUnitTest::EmbedSegments(ai_chat::TextEmbedder*)::'lambda'()::operator()() const brave/components/ai_chat/core/browser/text_embedder_unittest.cc:67:58
    #10 0x5a36c6762969 in base::OnceCallback<void ()>::Run() && base/functional/callback.h:156:12
    #11 0x5a36d4977df2 in base::TaskAnnotator::RunTaskImpl(base::PendingTask&) base/task/common/task_annotator.cc:202:34
    #12 0x5a36d49dcfe9 in RunTask<(lambda at ../../base/task/thread_pool/task_tracker.cc:678:35)> base/task/common/task_annotator.h:90:5
    #13 0x5a36d49dcfe9 in base::internal::TaskTracker::RunTaskImpl(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base/task/thread_pool/task_tracker.cc:677:19
    #14 0x5a36d49dd0f1 in base::internal::TaskTracker::RunSkipOnShutdown(base::internal::Task&, base::TaskTraits const&, base::internal::TaskSource*, base::internal::SequenceToken const&) base/task/thread_pool/task_tracker.cc:662:3
    #15 0x5a36d49dc1f5 in base::internal::TaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base/task/thread_pool/task_tracker.cc:520:5
    #16 0x5a36d4af81fb in base::test::TaskEnvironment::TestTaskTracker::RunTask(base::internal::Task, base::internal::TaskSource*, base::TaskTraits const&) base/test/task_environment.cc:1028:46
    #17 0x5a36d49db5a5 in base::internal::TaskTracker::RunAndPopNextTask(base::internal::RegisteredTaskSource) base/task/thread_pool/task_tracker.cc:415:5
    #18 0x5a36d4a0cabd in base::internal::WorkerThread::RunWorker() base/task/thread_pool/worker_thread.cc:493:36
    #19 0x5a36d4a0c100 in base::internal::WorkerThread::RunPooledWorker() base/task/thread_pool/worker_thread.cc:379:3
    #20 0x5a36d4a0bc86 in base::internal::WorkerThread::ThreadMain() base/task/thread_pool/worker_thread.cc:359:7
    #21 0x5a36d4a3e1ec in base::(anonymous namespace)::ThreadFunc(void*) base/threading/platform_thread_posix.cc:101:13
    #22 0x7695b109ca93 in start_thread nptl/pthread_create.c:447:8
    #23 0x7695b1129c3b in clone3 misc/../sysdeps/unix/sysv/linux/x86_64/clone3.S:78
```","['type:bug', 'comp:lite', 'TFLiteConverter', 'awaiting PR merge']",2024-10-07T18:57:35Z,2,0,https://github.com/tensorflow/tensorflow/issues/77168,Runtime Error
85,tensorflow/tensorflow,NotImplementedError from tf.constant in trivial case,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Trying to make a tensor that has the same value for all items in the batch, see the following bare minimum code. 
I get `NotImplementedError: cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array.`
I am not trying to use numpy, this is an internal error.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import keras
import numpy as np

class CustomModel(keras.models.Model):
    def call(self, inputs):
        inputs_shape = tf.shape(inputs)
        return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # NotImplementedError
        #return 3.0 * tf.ones(shape=(inputs_shape[0], 1), dtype=inputs.dtype)  # OK

model = CustomModel()
model.compile(run_eagerly=False, loss=""mse"")  # OK if run_eagerly=True
model.fit(np.array([[0.0]]), np.array([[0.0]]))
```
```


### Relevant log output

```shell
{
	""name"": ""NotImplementedError"",
	""message"": ""Exception encountered when calling CustomModel.call().

Cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.

Arguments received by CustomModel.call():
  • inputs=tf.Tensor(shape=(None, 1), dtype=float32)"",
	""stack"": ""---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In[6], line 13
     11 model = CustomModel()
     12 model.compile(run_eagerly=False, loss=\""mse\"")  # OK if run_eagerly=True
---> 13 model.fit(np.array([[0.0]]), np.array([[0.0]]))

File /usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

Cell In[6], line 8, in CustomModel.call(self, inputs)
      6 def call(self, inputs):
      7     inputs_shape = tf.shape(inputs)
----> 8     return tf.constant(3.0, shape=(inputs_shape[0], 1), dtype=inputs.dtype)

File /usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3100, in prod(a, axis, dtype, out, keepdims, initial, where)
   2979 @array_function_dispatch(_prod_dispatcher)
   2980 def prod(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,
   2981          initial=np._NoValue, where=np._NoValue):
   2982     \""\""\""
   2983     Return the product of array elements over a given axis.
   2984 
   (...)
   3098     10
   3099     \""\""\""
-> 3100     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
   3101                           keepdims=keepdims, initial=initial, where=where)

File /usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88, in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     85         else:
     86             return reduction(axis=axis, out=out, **passkwargs)
---> 88 return ufunc.reduce(obj, axis, dtype, out, **passkwargs)

NotImplementedError: Exception encountered when calling CustomModel.call().

Cannot convert a symbolic tf.Tensor (custom_model_5_1/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported.

Arguments received by CustomModel.call():
  • inputs=tf.Tensor(shape=(None, 1), dtype=float32)""
}
```
","['type:bug', 'TF 2.16']",2024-10-04T13:38:25Z,3,0,https://github.com/tensorflow/tensorflow/issues/77045,Logical Bug
86,tensorflow/tensorflow,TFlite compilation crashes on MacOS (error: _Float16 is not supported on this target),"### Issue type

Build/Install

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-rc0

### Custom code

No

### OS platform and distribution

MacOS 15.0

### Mobile device

_No response_

### Python version

3.12

### Bazel version

6.5

### GCC/compiler version

Apple clang version 16.0.0 (clang-1600.0.26.3)
XCode 16.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Compiling TF_lite v2.18.0-rc0 from source, using the command suggested in the documentation leads to crash (log below):

```PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native```

### Standalone code to reproduce the issue

```shell
PYTHON=python3 tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh native
```


### Relevant log output

```shell
TF2_BEHAVIOR=1 \
    XCODE_VERSION_OVERRIDE=16.0.0.16A242d \
    ZERO_AR_DATE=1 \
  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' 'DEBUG_PREFIX_MAP_PWD=.' -iquote external/XNNPACK -iquote bazel-out/darwin-opt/bin/external/XNNPACK -iquote external/pthreadpool -iquote bazel-out/darwin-opt/bin/external/pthreadpool -iquote external/FXdiv -iquote bazel-out/darwin-opt/bin/external/FXdiv -iquote external/cpuinfo -iquote bazel-out/darwin-opt/bin/external/cpuinfo -iquote external/FP16 -iquote bazel-out/darwin-opt/bin/external/FP16 -Ibazel-out/darwin-opt/bin/external/pthreadpool/_virtual_includes/pthreadpool -Ibazel-out/darwin-opt/bin/external/FXdiv/_virtual_includes/FXdiv -Ibazel-out/darwin-opt/bin/external/cpuinfo/_virtual_includes/cpuinfo -Ibazel-out/darwin-opt/bin/external/FP16/_virtual_includes/FP16 -isystem external/XNNPACK/include -isystem bazel-out/darwin-opt/bin/external/XNNPACK/include -isystem external/XNNPACK/src -isystem bazel-out/darwin-opt/bin/external/XNNPACK/src -isystem external/pthreadpool/include -isystem bazel-out/darwin-opt/bin/external/pthreadpool/include -isystem external/FXdiv/include -isystem bazel-out/darwin-opt/bin/external/FXdiv/include -isystem external/cpuinfo/include -isystem bazel-out/darwin-opt/bin/external/cpuinfo/include -isystem external/cpuinfo/src -isystem bazel-out/darwin-opt/bin/external/cpuinfo/src -isystem external/FP16/include -isystem bazel-out/darwin-opt/bin/external/FP16/include -MD -MF bazel-out/darwin-opt/bin/external/XNNPACK/_objs/microkernel_configs/cmul-config.d -DPTHREADPOOL_NO_DEPRECATED_API '-DXNN_LOG_LEVEL=0' '-DXNN_ENABLE_CPUINFO=1' '-DXNN_ENABLE_MEMOPT=1' '-DXNN_ENABLE_DWCONV_MULTIPASS=1' '-DXNN_ENABLE_GEMM_M_SPECIALIZATION=1' '-DXNN_ENABLE_SPARSE=1' '-DXNN_ENABLE_ASSEMBLY=1' '-DXNN_ENABLE_ARM_FP16_SCALAR=0' '-DXNN_ENABLE_ARM_FP16_VECTOR=0' '-DXNN_ENABLE_ARM_BF16=0' '-DXNN_ENABLE_ARM_DOTPROD=0' '-DXNN_ENABLE_ARM_I8MM=0' '-DXNN_ENABLE_RISCV_FP16_VECTOR=0' '-DXNN_ENABLE_AVX512VNNIGFNI=1' '-DXNN_ENABLE_AVX512AMX=1' '-DXNN_ENABLE_AVX512FP16=1' '-DXNN_ENABLE_AVXVNNI=1' '-DXNN_ENABLE_AVXVNNIINT8=1' '-DXNN_ENABLE_AVX256SKX=1' '-DXNN_ENABLE_AVX256VNNI=1' '-DXNN_ENABLE_AVX256VNNIGFNI=1' '-DXNN_ENABLE_HVX=0' '-DXNN_ENABLE_KLEIDIAI=0' '-DBAZEL_CURRENT_REPOSITORY=""XNNPACK""' '-frandom-seed=bazel-out/darwin-opt/bin/external/XNNPACK/_objs/microkernel_configs/cmul-config.o' -isysroot __BAZEL_XCODE_SDKROOT__ -F__BAZEL_XCODE_SDKROOT__/System/Library/Frameworks -F__BAZEL_XCODE_DEVELOPER_DIR__/Platforms/MacOSX.platform/Developer/Library/Frameworks -no-canonical-prefixes -pthread -DGRPC_BAZEL_BUILD -w -O3 '-march=native' -Iinclude -Isrc '-DXNN_ENABLE_CPUINFO=1' '-std=c99' -O2 -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -target x86_64-apple-macosx15.0 -c external/XNNPACK/src/configs/cmul-config.c -o bazel-out/darwin-opt/bin/external/XNNPACK/_objs/microkernel_configs/cmul-config.o)
# Configuration: 7ffafbfebd31d6f3229fc3b4603178937ffcc0387347731c7b47fcb97e2cd76d
# Execution platform: @local_execution_config_platform//:platform
ERROR: /private/var/tmp/_bazel_feranick/50b852099a3bf3aaa184abce166f8e34/external/XNNPACK/BUILD.bazel:803:36: Compiling external/XNNPACK/sse_prod_microkernels.c failed: (Exit 1): wrapped_clang failed: error executing command (from target @XNNPACK//:sse_prod_microkernels) external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG '-DNS_BLOCK_ASSERTIONS=1' ... (remaining 109 arguments skipped)
In file included from bazel-out/darwin-opt/bin/external/XNNPACK/sse_prod_microkernels.c:1:
In file included from external/XNNPACK/src/xnnpack/avgpool.h:15:
In file included from external/XNNPACK/src/xnnpack/microparams.h:12:
In file included from external/XNNPACK/src/xnnpack/math.h:21:
In file included from bazel-out/darwin-opt/bin/external/FP16/_virtual_includes/FP16/fp16/fp16.h:10:
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:614:27: error: _Float16 is not supported on this target
  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:614:8: error: _Float16 is not supported on this target
  614 | extern _Float16 __fabsf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:28: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:38: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                                      ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:615:8: error: _Float16 is not supported on this target
  615 | extern _Float16 __hypotf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:616:27: error: _Float16 is not supported on this target
  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:616:8: error: _Float16 is not supported on this target
  616 | extern _Float16 __sqrtf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:617:27: error: _Float16 is not supported on this target
  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:617:8: error: _Float16 is not supported on this target
  617 | extern _Float16 __ceilf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:618:28: error: _Float16 is not supported on this target
  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:618:8: error: _Float16 is not supported on this target
  618 | extern _Float16 __floorf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:619:27: error: _Float16 is not supported on this target
  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                           ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:619:8: error: _Float16 is not supported on this target
  619 | extern _Float16 __rintf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:620:28: error: _Float16 is not supported on this target
  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:620:8: error: _Float16 is not supported on this target
  620 | extern _Float16 __roundf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:621:28: error: _Float16 is not supported on this target
  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                            ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:621:8: error: _Float16 is not supported on this target
  621 | extern _Float16 __truncf16(_Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |        ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:622:31: error: _Float16 is not supported on this target
  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                               ^
/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX15.0.sdk/usr/include/math.h:622:41: error: _Float16 is not supported on this target
  622 | extern _Float16 __copysignf16(_Float16, _Float16) __API_AVAILABLE(macos(15.0), ios(18.0), watchos(11.0), tvos(18.0));
      |                                         ^
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated.
Error in child process '/usr/bin/xcrun'. 1
Target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 119.893s, Critical Path: 3.17s
INFO: 342 processes: 313 internal, 29 local.
FAILED: Build did NOT complete successfully
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'comp:lite', '2.17']",2024-10-02T18:20:47Z,10,0,https://github.com/tensorflow/tensorflow/issues/76976,Runtime Error
87,tensorflow/tensorflow,Jit-compiling `tf.while_loop` inside `tf.vectorized_map` raises `InvalidArgumentError`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

Yes

### OS platform and distribution

Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

MRE
-------
The following mock-up of `cumsum` attempts to JIT compile a `tf.vectorized_map`ped function containing a `tf.scan`:
```python
import tensorflow as tf

def cumsum(xs):
    return tf.scan(
        lambda a, x: a + x, elems=xs
    )

@tf.function(jit_compile=True)
def vec_cumsum(xs):
    return tf.vectorized_map(cumsum, elems=xs)

xs_batched = tf.reshape(tf.range(30), (3, 10))
vec_cumsum(xs_batched)
```

__Expected behaviour__: `vec_cumsum(xs_batched)` returns a batch of cumulative sums.

__Actual behaviour__: Even though all data structures are known statically at JIT compile time, an InvalidArgumentError is raised with ""No registered 'TensorListReserve'"".  The fault is clearly related to `tf.scan`'s use of `tf.while_loop`, as a (longer) example using naked `tf.while_loop(..., max_iterations=n)` will confirm.

In JAX, it is possible to jit-compile a `vmap`ped function containing a `lax.while_loop` indicating that this is possible in HLO.  It seems the `tf.function(jit_compile=True)` machinery may be mis-transpiling to HLO somehow.

May be related to #73367 also involving `tf.vectorized_map` and `tf.while_loop` (albeit with reversed scope)?

### Standalone code to reproduce the issue

```shell
Colab MRE: https://colab.research.google.com/drive/1bmq1t3PdtebCSlNd0t-iEFrXX7Q0qqZp?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-26ea491cd046> in <cell line: 13>()
     11 
     12 # Fails with ""No registered 'TensorListReserve'""""
---> 13 vec_cumsum(xs_batched)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_vec_cumsum_462[_XlaMustCompile=true,config_proto=13561319589895757934,executor_type=11160318154034397263] on XLA_CPU_JIT: TensorListReserve (No registered 'TensorListReserve' OpKernel for XLA_CPU_JIT devices compatible with node {{function_node __inference_while_fn_428}}{{node while_init/TensorArrayV2_4}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: element_dtype=DT_VARIANT, shape_type=DT_INT32){{function_node __inference_while_fn_428}}{{node while_init/TensorArrayV2_4}}
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'comp:ops', '2.17']",2024-10-01T17:01:36Z,1,0,https://github.com/tensorflow/tensorflow/issues/76891,Logical Bug
88,tensorflow/tensorflow,Multithreading is not working with teansorflow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tensorflow==2.15.0.post1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

python:3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am using bert model for classification and serving the model with gunicorn worker_class=gthreads, tf.config.threading.set_intra_op_parallelism_threads(1)
tf.config.threading.set_inter_op_parallelism_threads(1)

when I using above two line of code the code is working fine as expected and if increase the number to more than 1, the code getting blocked at the below line of code

# Make predictions
outputs = model_obj(inputs)

and also inorder to reduce the docker image size I am using 
RUN pip3 install torch==2.0.0+cpu -f https://download.pytorch.org/whl/torch_stable.html


before installing all the dependencies
Flask==2.2.5
g2p-en==2.1.0
gunicorn==21.2.0
jellyfish==1.0.3
kenlm==0.2.0
nltk==3.8.1
numpy==1.26.3
pandas==2.2.0
python-dotenv==1.0.1
requests==2.31.0
scikit-learn==1.4.0
semantic-router==0.0.17
semantic-router[fastembed]
sentence-transformers==2.3.1
tensorflow==2.15.0.post1
tensorflow-hub==0.16.0
theano==1.0.5
transformers==4.37.2
Werkzeug==2.2.2


please tell me why is my code is getting blocked if I use more than 1 thread.

### Standalone code to reproduce the issue

```shell
def intent_prediction(self, sentence, thresold_score):
        logger.info(f""Threshold Score: {thresold_score}"")
        try:
            model_obj = intent_object_dict[self.model]
        except KeyError:
            logger.error(""Model not found in intent_object_dict"")
            model_obj = self.load_model()

        if INTENT_MODEL == ""cohere"":
            score, intent = self.cohere_intent_prediction(sentence, model_obj)
        else:
            score, intent = self.Bert_intent_prediction(
                sentence, model_obj, thresold_score
            )
        return score, intent

def Bert_intent_prediction(self, sentence, model_obj, thresold_score):
        inputs = self.load_BERT_tokenizer(sentence)
        # Make predictions
        outputs = model_obj(inputs)
        # Get predicted class
        probabilities = tf.nn.softmax(outputs.logits, axis=1)
        predicted_class = tf.argmax(probabilities, axis=1).numpy()[0]
        matching_score = probabilities[0][predicted_class].numpy()
        try:
            intent_data = intent_label_dict[self.model]
        except Exception as e:
            logger.error(f""error while getting label: {e}"")
            intent_data = self.get_intent_labels()

        if matching_score >= thresold_score:
            logger.info(
                f""Matched Main Intent:\
    {intent_data[predicted_class]},\
    SCORE :{matching_score}""
            )
            logger.info(f""Matched Sentence: {intent_data[predicted_class]}"")
        else:
            logger.info(f""Intent not matched, score is {matching_score}"")

        intent = intent_data[predicted_class]

        return str(matching_score), intent
```


### Relevant log output

```shell
30-Sep-2024 15:50:42.761|INFO    |__init__|I want my sofa get cleaned|
    __init__.py:171|Enter into PUNC for intent...
30-Sep-2024 15:50:42.761|INFO    |phrase_sim|I want my sofa get cleaned|
    phrase_sim.py:72|Threshold Score: 0.7


after this the code is blocked
```
","['stat:awaiting tensorflower', 'type:bug', 'TF 2.15']",2024-09-30T10:37:06Z,3,0,https://github.com/tensorflow/tensorflow/issues/76794,Logical Bug
89,tensorflow/tensorflow,Segmentation fault (core dumped) in `tf.data.experimental.SqlDataset`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the illegal input to tf.data.experimental.SqlDataset triggered when a crash, and will only come when iteration data.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

data_source_name = ""sqlite:///path/to/correct_database.db""

query = ""SELECT id, name FROM my_table""
output_types = (tf.int64, tf.string)
dataset = tf.data.experimental.SqlDataset(
    'sqlite', data_source_name, query, output_types)

for element in dataset:
    print(element)
```


### Relevant log output

```shell
2024-09-28 21:18:33.844482: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:18:33.907260: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:18:33.986019: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:18:34.009755: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:18:34.068897: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:18:38.768599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:18:38.769172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:18:39.132534: W tensorflow/core/kernels/data/experimental/sql_dataset_op.cc:209] Failed to connect to database: INVALID_ARGUMENT: Sqlite::Open(sqlite:///path/to/correct_database.db) failed: unable to open database file
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T13:20:59Z,1,0,https://github.com/tensorflow/tensorflow/issues/76731,Runtime Error
90,tensorflow/tensorflow,Aborted (core dumped) in `tf.linalg.det/slogdet/logdet/cholesky/inv`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.linalg.det/slogdet/logdet/cholesky/inv triggered a crash when the input is empty. Note that this will only be triggered if the gpu is available.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

invalid_input = tf.zeros([])
tf.linalg.det(invalid_input)    # crash
tf.linalg.slogdet(invalid_input)  # crash
tf.linalg.cholesky(invalid_input)  # crash
tf.linalg.logdet(invalid_input)  # crash
tf.linalg.inv(invalid_input)  # crash
```


### Relevant log output

```shell
2024-09-28 21:11:10.188752: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:11:10.199880: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:11:10.213635: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:11:10.221654: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:11:10.279720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:11:17.015480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:11:17.015957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:11:17.154391: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T13:15:48Z,1,0,https://github.com/tensorflow/tensorflow/issues/76730,Runtime Error
91,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.ResourceScatterNdop`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the type of resource_handle is inconsistent with that of updates,tf.raw_ops.ResourceScatterNdop triggers the crash. As follows:
tf.raw_ops.ResourceScatterNdUpdate
tf.raw_ops.ResourceScatterNdAdd
tf.raw_ops.ResourceScatterNdSub
tf.raw_ops.ResourceScatterNdMax
tf.raw_ops.ResourceScatterNdMin

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

resource_var = tf.Variable(initial_value=tf.zeros([2, 2], dtype=tf.int32), trainable=False)
resource_handle = resource_var.handle

indices = np.array([[2, 1], [1, 2]], dtype=np.int32)
updates = np.array([10, 20], dtype=np.float32)
tf.raw_ops.ResourceScatterNdUpdate(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)

tf.raw_ops.ResourceScatterNdAdd(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdSub(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdMax(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
tf.raw_ops.ResourceScatterNdMin(  # crash
    ref=resource_handle,
    indices=indices,
    updates=updates,
    use_locking=True
)
```


### Relevant log output

```shell
2024-09-28 21:06:23.445185: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 21:06:23.508056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 21:06:23.583640: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 21:06:23.607538: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 21:06:23.664877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 21:06:31.527466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 21:06:31.527985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 21:06:31.782114: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (3 vs. 1) float expected, got int32
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T13:09:06Z,1,0,https://github.com/tensorflow/tensorflow/issues/76729,Runtime Error
92,tensorflow/tensorflow,Aborted (core dumped) in `tf.io.encode_png`/`tf.compat.v1.image.encode_png`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The crash was triggered when an illegal image was passed to tf.io.encode_png/tf.compat.v1.image.encode_png

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

image = tf.cast(tf.tile([[[0, 0, 0, 1]], [[0, 0, 1, 0]]], [0, 0, 1]), tf.uint8)

encoded_image = tf.compat.v1.image.encode_png(image) # crash
tf.io.encode_png(image, compression=-1, name=None) #crash
```


### Relevant log output

```shell
2024-09-28 20:48:36.270008: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:48:36.332972: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:48:36.411391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:48:36.428306: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:48:36.438336: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-09-28 20:48:41.296886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3114 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.92024-09-28 20:48:41.297450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 20:48:41.475588: F tensorflow/core/lib/png/png_io.cc:350] 'image' Must be non NULL
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T12:49:42Z,3,0,https://github.com/tensorflow/tensorflow/issues/76726,Runtime Error
93,tensorflow/tensorflow,Floating point exception (core dumped) in `tf.nn.depth_to_space`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.nn.depth_to_space triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
try:
    # Create an empty tensor
    arg_0_tensor = tf.zeros([0, 2, 3, 12], dtype=tf.float32)
    # arg_0 = tf.identity(arg_0_tensor)
    arg_1 = 536870912
    out = tf.nn.depth_to_space(arg_0_tensor, arg_1)
except Exception as e:
    print(""Error:"", str(e))
```


### Relevant log output

```shell
2024-09-28 20:41:05.888017: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:41:05.950498: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:41:06.028236: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:41:06.052072: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:41:06.111011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 20:41:11.970896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2704 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 20:41:11.973176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
Floating point exception (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T12:41:49Z,1,0,https://github.com/tensorflow/tensorflow/issues/76724,Runtime Error
94,tensorflow/tensorflow,Aborted (core dumped) in `tf.nn.max_pool/tf.nn.max_pool1d`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.nn.max_pool triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

invalid_kernel_size = -1
invalid_operation = tf.nn.max_pool(
    tf.random.normal([1, 32, 32, 3]),
    ksize=[1, invalid_kernel_size, invalid_kernel_size, 1],
    strides=[1, 2, 2, 1],
    padding='SAME'
)
```

```
import tensorflow as tf
import sys

ksize = sys.maxsize + 100  # Set to a value larger than sys.maxsize
input_tensor = tf.random.normal(shape=(2, 10, 4))
result = tf.nn.max_pool1d(input=input_tensor, ksize=ksize, strides=1, padding='SAME')

```


### Relevant log output

```shell
2024-09-28 20:26:47.491907: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-28 20:26:47.554171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:26:47.606570: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:26:47.610539: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-09-28 20:26:47.639739: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-09-28 20:26:54.579839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21471 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-09-28 20:26:54.582099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1724 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
2024-09-28 20:26:55.563477: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
F0000 00:00:1727526415.563805  147227 cuda_dnn.cc:1107] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0) 
*** Check failure stack trace: ***
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T12:28:21Z,1,0,https://github.com/tensorflow/tensorflow/issues/76722,Runtime Error
95,tensorflow/tensorflow,Segmentation fault (core dumped) in `tf.profiler.experimental.Profile`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0-dev20240925

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific inputs, tf.profiler.experimental.Profile triggered a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

profiler_options = tf.profiler.experimental.ProfilerOptions(
    host_tracer_level=999,
    python_tracer_level=-1,
    device_tracer_level=10,
    delay_ms=None
)

with tf.profiler.experimental.Profile(None, options=profiler_options):
    a = tf.constant(1)
    b = tf.constant(2)
    c = a + b
    print(c.numpy())
```


### Relevant log output

```shell
2024-09-28 20:07:36.902909: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-09-28 20:07:36.966049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-09-28 20:07:36.998027: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-09-28 20:07:37.002984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered2024-09-28 20:07:37.055864: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-28T12:11:00Z,2,1,https://github.com/tensorflow/tensorflow/issues/76718,Runtime Error
96,tensorflow/tensorflow,TensorFlow keeps creating threads when multi-GPU training （thread leak）,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.4

### GPU model and memory

Nvidia A800 

### Current behavior?

I was using this machine to train a GPT-2 example from (the data I used can also be found in this link, also here https://github.com/chinese-poetry/chinese-poetry.git) https://keras.io/examples/generative/gpt2_text_generation_with_kerasnlp/

Before we start, I would give a baseline amount of this machine's threads :
![10](https://github.com/user-attachments/assets/53e16381-e50f-4054-8c2a-b790fe2e077b)

When starting with the multi-GPU training of tensorflow by calling the tf.distribute.MirroredStrategy, the training process worked fine as usual. 

But with the time went by, I found the amount of threads increased with the training process going, here is the evidence of thread increasing when processing 3354th batch (I used cat /proc/""this programs' pid""/status to check the number of threads):
![3](https://github.com/user-attachments/assets/56bbfaf0-f59c-48f1-ae19-a98d563ad000)
![tf](https://github.com/user-attachments/assets/c546ea4b-855a-44e5-84c1-95d6d3f3aba4)
![2](https://github.com/user-attachments/assets/0d07b24d-e93e-451e-9e1e-51a26f7db60d)

Then, the evidence of thread increasing when processing 3791st batch (the amount of threads reached 22178):
![6](https://github.com/user-attachments/assets/508fed5c-8aec-4deb-8e68-74dbf6aa5613)
![4](https://github.com/user-attachments/assets/77507790-70fd-4abf-b72c-e9d831565879)
![5](https://github.com/user-attachments/assets/d8945e53-23d9-4fb7-b50d-37962b93a692)

When calculating  the 5054th batch, the training program got an error, and I captured a count of threads before the error (achieved around 31120 threads):
![8](https://github.com/user-attachments/assets/15b433cc-eefb-451f-a31c-2a286e17afec)
![8](https://github.com/user-attachments/assets/813835d2-3834-400a-b8fe-ec27f15d89ad)

I checked an similar issue in https://github.com/tensorflow/tensorflow/issues/62466, but I cannot find a solution, moreover, I have run other examples like diffusion model using this machine and the same tensorflow env with multi-GPU training, which worked fine and no any problems. So, could you please give me a help for this problem, very appreciated.





### Standalone code to reproduce the issue

```shell
Here is my code

import os

os.environ[""KERAS_BACKEND""] = ""tensorflow""  # or ""tensorflow"" or ""torch""

import keras_nlp
import keras
import tensorflow as tf
import time

keras.mixed_precision.set_global_policy(""mixed_float16"")

import os
import json
import datetime

train_ds = (
    tf.data.Dataset.from_tensor_slices(paragraphs)
    .batch(36)
    .cache()
    .prefetch(tf.data.AUTOTUNE)
)

strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1"", ""GPU:2"", ""GPU:3"", ""GPU:4"", ""GPU:5""])
print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))

preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(
    ""gpt2_base_en"",
    sequence_length=128,
)

# Open a strategy scope.
with strategy.scope():
# To speed up training and generation, we use preprocessor of length 128
# instead of full length 1024.
    gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(
        ""gpt2_base_en"", preprocessor=preprocessor
    )
    num_epochs = 5

    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_path,
        save_weights_only=True,
        monitor=""accuracy"",
        # monitor=""i_loss"",
        mode=""min"",
        save_best_only=True,
        save_freq=""epoch""
    )
    learning_rate = 5e-4
    
    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    gpt2_lm.compile(
        optimizer=keras.optimizers.Adam(learning_rate),
        loss=loss,
        weighted_metrics=[""accuracy""],
    )

    gpt2_lm.fit(train_ds, epochs=num_epochs, callbacks=[
        checkpoint_callback,
        tensorboard_callback,
    ],
                )
```


### Relevant log output

```shell
2024-09-21 00:24:25.840848: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:57] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. gpt2_causal_lm/gpt2_backbone/embeddings_dropout/dropout/random_uniform/RandomUniform
2024-09-21 00:24:25.846135: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-09-21 00:24:26.559075: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.572007: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.573183: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.581128: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.581197: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
2024-09-21 00:24:26.588190: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert
12024-09-21 00:25:01.986918: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:02.215951: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2024-09-21 00:25:02.420703: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:02.542130: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:03.595774: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

12024-09-21 00:25:04.071526: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:04.130504: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:325] ptxas warning : Registers are spilled to local memory in function 'fusion_1267'
ptxas warning : Registers are spilled to local memory in function 'fusion_1225'
ptxas warning : Registers are spilled to local memory in function 'fusion_1111'
ptxas warning : Registers are spilled to local memory in function 'fusion_1220'
ptxas warning : Registers are spilled to local memory in function 'fusion_1124'
ptxas warning : Registers are spilled to local memory in function 'fusion_1175'
ptxas warning : Registers are spilled to local memory in function 'fusion_1128'
ptxas warning : Registers are spilled to local memory in function 'fusion_1006'
ptxas warning : Registers are spilled to local memory in function 'fusion_1015'

2024-09-21 00:25:05.333297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
5054/8663 [================>.............] - ETA: 9:14 - loss: 11.8715 - accuracy: 2.2702terminate called after throwing an instance of 'std::system_error'
terminate called recursively
  what():  Resource temporarily unavailable
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:dist-strat', 'TF 2.11']",2024-09-20T17:14:03Z,2,0,https://github.com/tensorflow/tensorflow/issues/76157,Runtime Error
97,tensorflow/tensorflow,"tf.python.ops.array_ops.transpose aborts with ""Check failed: d >= 0 (0 vs. -1)""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly 2.18.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `array_ops.transpose`

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.ops import array_ops

x =np.arange(0, 8).reshape([2, 4]).astype(np.float32)
y = np.array([-1, 0]).astype(np.int32)
array_ops.transpose(x, y,conjugate = False)
```


### Relevant log output

```shell
2024-09-19 16:16:30.137164: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-19T08:31:38Z,3,0,https://github.com/tensorflow/tensorflow/issues/76036,Runtime Error
98,tensorflow/tensorflow,Code error when feature name has multiple `_`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

Yes

### OS platform and distribution

5.15.149-99.162.amzn2.x86_64

### Mobile device

_No response_

### Python version

Python 3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Expect there shouldn't be errors just by changing feature name. 

### Standalone code to reproduce the issue
This is a code sample that will work normally

```
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Concatenate
from tensorflow.keras import Model

import pandas as pd
import numpy as np

df = pd.DataFrame()
numeric_feature_name = 'a' * 27
categorical_feature_name = 'b' * 11
df[numeric_feature_name] = range(1000)
df[categorical_feature_name] = 'a'
df['label'] = 1

numeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')
categorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=""string"")
encoding_layer = get_category_encoding_layer(vocab=['a'])
encoded_categorical_feature = encoding_layer(categorical_feature_layer)

all_inputs = [numeric_feature_layer, categorical_feature_layer]
encoded_features = [numeric_feature_layer, encoded_categorical_feature]
concat_features = Concatenate()(encoded_features)
output = Dense(units=1, activation='sigmoid')(concat_features)
model = Model(inputs=all_inputs, outputs=output)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

dataframe_x = df[[numeric_feature_name, categorical_feature_name]]
dataframe_y = df['label']
df2 = ((dict(dataframe_x), dataframe_y))
ds = tf.data.Dataset.from_tensor_slices(df2)
ds = ds.batch(32)
ds_train = ds

model.fit(
    ds_train,
    epochs=10,
    batch_size=300,
    verbose=1
)
```

However, if I change the feature name, the same code will throw error

```
df = pd.DataFrame()
## Just change the feature name here
numeric_feature_name = 'a_b_c_d_e_f_g' 
categorical_feature_name = 'a_b_c_d_e_f'
df[numeric_feature_name] = range(1000)
df[categorical_feature_name] = 'a'
df['label'] = 1

numeric_feature_layer = tf.keras.Input(shape=(1,), name=numeric_feature_name, dtype='float32')
categorical_feature_layer = tf.keras.Input(shape=(1,), name=categorical_feature_name, dtype=""string"")
encoding_layer = get_category_encoding_layer(vocab=['a'])
encoded_categorical_feature = encoding_layer(categorical_feature_layer)

all_inputs = [numeric_feature_layer, categorical_feature_layer]
encoded_features = [numeric_feature_layer, encoded_categorical_feature]
concat_features = Concatenate()(encoded_features)
output = Dense(units=1, activation='sigmoid')(concat_features)
model = Model(inputs=all_inputs, outputs=output)

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

dataframe_x = df[[numeric_feature_name, categorical_feature_name]]
dataframe_y = df['label']
df2 = ((dict(dataframe_x), dataframe_y))
ds = tf.data.Dataset.from_tensor_slices(df2)
ds = ds.batch(32)
ds_train = ds

model.fit(
    ds_train,
    epochs=10,
    batch_size=300,
    verbose=1
)
```

We have tested that this error is on 2.17.0 and if we are using 2.15 tensorflow, both codes will run smoothly.
```


### Relevant log output

```shell
Epoch 1/10
2024-09-18 05:28:05.240962: W tensorflow/core/framework/op_kernel.cc:1817] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported
---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
Cell In[14], line 8
      5 ds = ds.batch(32)
      6 ds_train = ds
----> 8 model.fit(
      9     ds_train,
     10     epochs=10,
     11     batch_size=300,
     12     verbose=1
     13 )

File ~/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51 try:
     52   ctx.ensure_initialized()
---> 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                       inputs, attrs, num_outputs)
     55 except core._NotOkStatusException as e:
     56   if name is not None:

UnimplementedError: Graph execution error:

Detected at node functional_5_1/Cast defined at (most recent call last):
  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/runpy.py"", line 196, in _run_module_as_main

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/runpy.py"", line 86, in _run_code

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel_launcher.py"", line 18, in <module>

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/traitlets/config/application.py"", line 1075, in launch_instance

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 739, in start

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 205, in start

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/asyncio/events.py"", line 80, in _run

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 545, in dispatch_queue

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 534, in process_one

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 437, in dispatch_shell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 362, in execute_request

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 778, in execute_request

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 449, in do_execute

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 549, in run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3075, in run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3130, in _run_cell

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 128, in _pseudo_sync_runner

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3334, in run_cell_async

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3517, in run_ast_nodes

  File ""/home/jinqi_shen/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3577, in run_code

  File ""/tmp/ipykernel_37779/4021243845.py"", line 8, in <module>

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 320, in fit

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 121, in one_step_on_iterator

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 108, in one_step_on_data

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py"", line 51, in train_step

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/layers/layer.py"", line 901, in __call__

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 117, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/ops/operation.py"", line 46, in __call__

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 156, in error_handler

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 167, in call

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 258, in _standardize_inputs

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/models/functional.py"", line 218, in _convert_inputs_to_tensors

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/ops/core.py"", line 822, in convert_to_tensor

  File ""/home/jinqi_shen/.airconda-environments/production--payments--tensorflow--ray_tf215--v0.0.1/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py"", line 132, in convert_to_tensor

Cast string to float is not supported
	 [[{{node functional_5_1/Cast}}]] [Op:__inference_one_step_on_iterator_6125]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', '2.17']",2024-09-18T17:06:56Z,10,0,https://github.com/tensorflow/tensorflow/issues/75996,Syntax Error
99,tensorflow/tensorflow,`resource_create_op` operation can cause TensorFlow to crash.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 A `Segmentation fault`  could be raised in TensorFlow when using `test_ops.resource_create_op` . The code is as follows:

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.framework import test_ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import array_ops_stack


sess = tf.compat.v1.Session()

@tf.function
def func():
    r1 = test_ops.stub_resource_handle_op(container='a', shared_name='b')
    r2 = test_ops.stub_resource_handle_op(container='a', shared_name='c')
    c = array_ops_stack.stack([r1, r2])
    s = array_ops.strided_slice(c, [1], [2 ** 32])
    with sess.as_default():
        test_ops.resource_create_op(s)

func()
```


### Relevant log output

```shell
> Segmentation fault (core dumped)

The above code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-16T01:45:51Z,3,0,https://github.com/tensorflow/tensorflow/issues/75825,Runtime Error
100,tensorflow/tensorflow,Encountering a `Segmentation fault` when using `data_flow_ops.FIFOQueue` in TensorFlow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 A `Segmentation fault`  could be raised in TensorFlow when using `data_flow_ops.FIFOQueue` . The following code is confirmed to crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes as dtypes_lib
from tensorflow.python.ops import data_flow_ops
import tensorflow as tf

q = data_flow_ops.FIFOQueue(10, dtypes_lib.float32, ())
elems = [10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]
tmp_var64 = elems[4:8]

sess = tf.compat.v1.Session()
with sess.as_default():
    q.dequeue_up_to([])
```


### Relevant log output

```shell
> Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-15T04:17:25Z,1,0,https://github.com/tensorflow/tensorflow/issues/75814,Runtime Error
101,tensorflow/tensorflow,`tf_cond.cond` and `tf.function` could cause an aborted issue,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when using `tf_cond.cond` and `tf.function`. The code is as follows:

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import constant_op
from tensorflow.python.framework import ops
from tensorflow.python.ops import cond as tf_cond
from tensorflow.python.ops import control_flow_assert
from tensorflow.python.ops import control_flow_ops
from tensorflow.python.platform import test

import tensorflow as tf
sess = tf.compat.v1.Session()

@tf.function
def func1():
    with sess.as_default():
        with ops.device(test.gpu_device_name()):
            pred = constant_op.constant([True, False])

        def fn1():
            return control_flow_ops.no_op()

        def fn2():
            with ops.device('/cpu:0'):
                return control_flow_assert.Assert(False, ['Wrong!'])

        r = tf_cond.cond(pred, fn1, fn2)

func1()
```


### Relevant log output

```shell
> 2024-09-12 16:43:14.634438: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
> Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-12T08:31:32Z,1,0,https://github.com/tensorflow/tensorflow/issues/75625,Runtime Error
102,tensorflow/tensorflow,Using `fft_ops`  would cause an `aborted issue`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when using `fft_ops`. The code is as follows:

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.ops.signal import fft_ops
import tensorflow as tf
tf.compat.v1.disable_eager_execution()


def _tf_ifft(x, rank, fft_length=None, feed_dict=None):
    with tf.compat.v1.Session() as sess:
        return sess.run(_tf_ifft_for_rank(rank)(x, fft_length), feed_dict=feed_dict)

def _tf_ifft_for_rank(rank):
    if rank == 1:
        return fft_ops.irfft
    elif rank == 2:
        return fft_ops.irfft2d
    elif rank == 3:
        return fft_ops.irfft3d
    else:
        raise ValueError('invalid rank')


rank = 1
extra_dims = 0
np_rtype = np.float32
np_ctype = np.complex64
dims = rank + extra_dims
x = np.zeros((1,) * dims).astype(np_ctype)
tmp_var22 = _tf_ifft(x, rank).shape
```


### Relevant log output

```shell
DUCC FFT c2r failed:
bazel-out/k8-opt/bin/external/ducc/_virtual_includes/fft/ducc/src/ducc0/fft/fft1d_impl.h: 2948 (static Trpass<Tfs> ducc0::detail_fft::rfftpass<float>::make_pass(size_t, size_t, size_t, const Troots<Tfs> &, bool) [Tfs = float]):

Assertion failure
no zero-sized FFTs

Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-12T08:06:13Z,3,0,https://github.com/tensorflow/tensorflow/issues/75624,Runtime Error
103,tensorflow/tensorflow,Using `gen_random_index_shuffle_ops.random_index_shuffle` with `rounds=-2` can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 A `Segmentation fault`  could be raised in TensorFlow when I used API `gen_random_index_shuffle_ops.random_index_shuffle` with `rounds=-2`. The code is as follows:

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import gen_random_index_shuffle_ops
from tensorflow.python.ops import math_ops

seed = (74, 117)
seed_dtype = dtypes.int32
max_index = 129
index_dtype = dtypes.int32
rounds = 4

seen = (max_index + 1) * [False]
seed = math_ops.cast([seed[0], seed[1], 42], seed_dtype)
for index in range(max_index + 1):
    new_index = gen_random_index_shuffle_ops.random_index_shuffle(math_ops.cast(index, index_dtype), seed, max_index=math_ops.cast(max_index, index_dtype), rounds=rounds)
    # rounds = -2 causes the segmentfault
    new_index = gen_random_index_shuffle_ops.random_index_shuffle(math_ops.cast(index, index_dtype), seed, max_index=math_ops.cast(max_index, index_dtype), rounds=-2)
```


### Relevant log output

```shell
> Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-12T07:51:54Z,1,0,https://github.com/tensorflow/tensorflow/issues/75623,Runtime Error
104,tensorflow/tensorflow,Got one aborted issue when using `data_flow_ops.MapStagingArea` ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when I used API `data_flow_ops.MapStagingArea`. The code is as follows:

### Standalone code to reproduce the issue

```shell
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import data_flow_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.platform import test
import tensorflow as tf


with ops.Graph().as_default() as g:
    with ops.device('/cpu:0'):
        x = array_ops.placeholder(dtypes.float32)
        pi = array_ops.placeholder(dtypes.int64)
        gi = array_ops.placeholder(dtypes.int64)
        v = 2.0 * (array_ops.zeros([]) + x)
    with ops.device(test.gpu_device_name()):
        stager = data_flow_ops.MapStagingArea([dtypes.float32])
        stage = stager.put(pi, [v], [0])
        k, y = stager.get([])
        y = math_ops.reduce_max(math_ops.matmul(y, y))
g.finalize()
with tf.compat.v1.Session(graph=g) as sess:
    sess.run(stage, feed_dict={x: -1, pi: 0})
    for i in range(10):
        _, yval = sess.run([stage, y], feed_dict={x: i, pi: i + 1, gi: i})
```


### Relevant log output

```shell
2024-09-12 15:57:44.445189: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 0)Must have a one element tensor
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-12T07:47:04Z,2,0,https://github.com/tensorflow/tensorflow/issues/75622,Runtime Error
105,tensorflow/tensorflow,An `aborted issue` could be raised in TensorFlow when I used API `math_ops.cast` and `array_ops.split`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 An `aborted issue`  could be raised in TensorFlow when I used API `math_ops.cast` and `array_ops.split` . 

### Standalone code to reproduce the issue

```shell
import numpy as np

from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import math_ops
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
sess = tf.compat.v1.Session()
with sess.as_default():
    a = math_ops.cast([2], dtypes.int32)
    b = math_ops.cast([1], dtypes.int32)
    value = np.random.rand(11, 11)
    array_ops.split(value, [a, b])
```


### Relevant log output

```shell
2024-09-12 15:49:03.711972: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-09-12T07:42:00Z,1,0,https://github.com/tensorflow/tensorflow/issues/75621,Runtime Error
106,tensorflow/tensorflow,check failed: !PyErr_Occurred() when constructing two uint64 tensors,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When running the following code, tensorflow will directly raise program abort with the error message: `./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred()`

```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['CUDA_VISIBLE_DEVICES'] = ''
import warnings
warnings.filterwarnings(""ignore"")
import tensorflow as tf

lower1 = -1
try:
    lower1 = tf.constant(lower1, dtype='uint64')
except:
    ...
lower2 = -2
lower2 = tf.constant(lower2, dtype='uint64')
```

It seems the problem occurs when TensorFlow tries to construct **two uint64 tensors**. Although it is invalid to convert negative int to unsigned, an exception is more proper as program abort will directly kill the process.

Indeed, only constructing one uint64 tensor will properly raises an OverFlow exception. 
This issue only occurs when repeatedly constructing two uint64 tensors.
Another weird thing is that, **if I change the value of `lower2` to either `-1` or `-3` instead of `-2`**, this issue does not occur.


### Standalone code to reproduce the issue

```shell
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['CUDA_VISIBLE_DEVICES'] = ''
import warnings
warnings.filterwarnings(""ignore"")
import tensorflow as tf

lower1 = -1
try:
    lower1 = tf.constant(lower1, dtype='uint64')
except:
    ...
lower2 = -2
lower2 = tf.constant(lower2, dtype='uint64')
```
```


### Relevant log output

```shell
F ./tensorflow/python/eager/pywrap_tensor_conversion.h:58] Check failed: !PyErr_Occurred() 
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:eager', '2.17']",2024-09-09T11:18:01Z,1,0,https://github.com/tensorflow/tensorflow/issues/75400,Logical Bug
107,tensorflow/tensorflow,Gradients can't be computed for keras embeddings,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.1

### Custom code

Yes

### OS platform and distribution

Windows 11, Ubuntu 22.04LTS

### Mobile device

_No response_

### Python version

3.11.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We have a problem in shap for quite some while now with embeddings (see [here](https://github.com/shap/shap/issues/3440)). Since we manipulate the graph to adjust the gradient calculation in order to produce shap values we need the layers to be backpropagatable. This does not seem the case for `tensorflow.keras.layers.Embedding` and we do not know a way around this.

(In a previous version there was the possibility to [manipulate](https://github.com/shap/shap/blob/master/shap/explainers/_deep/deep_tf.py#L412-L416) the [`_IsBackpropagatable` function](https://github.com/tensorflow/tensorflow/blob/v1.10.0/tensorflow/python/ops/gradients_impl.py#L293) but this is no longer possible)

In the example below one can see that the gradients just become `None` if the model contains an embedding layer. 
Is there a way around this, so that we can calculate gradients for embeddings again?

### Standalone code to reproduce the issue

```python
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the IMDb dataset
max_features = 10000  # Only consider the top 10,000 words
maxlen = 100  # Only consider the first 100 words of each movie review

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)
X_train = pad_sequences(X_train, maxlen=maxlen)
X_test = pad_sequences(X_test, maxlen=maxlen)

# Build the model
model = models.Sequential()
embedding_layer = layers.Embedding(input_dim=max_features, output_dim=128, input_length=maxlen)
model.add(embedding_layer)
flat_layer = layers.Flatten()
model.add(flat_layer)
dense_layer = layers.Dense(1, activation='sigmoid')
model.add(dense_layer)

# Build the same model except for the embedding layer
new_model = models.Sequential()
new_model.add(flat_layer)
new_model.add(layers.Dense(1, activation=""sigmoid""))

# Forward pass and gradient extraction
@tf.function
def get_gradients_model(inputs):
    inputs = tf.cast(inputs, tf.float32)  # Convert inputs to float32
    with tf.GradientTape() as tape:
        tape.watch(inputs)  # Watch the input tensor to compute gradients w.r.t. it
        predictions = model(inputs)
    
    gradients = tape.gradient(predictions, inputs)
    
    return predictions, gradients

@tf.function
def get_gradients_new_model(inputs):
    inputs = tf.cast(inputs, tf.float32)  # Convert inputs to float32
    with tf.GradientTape() as tape:
        tape.watch(inputs)  # Watch the input tensor to compute gradients w.r.t. it
        predictions = new_model(inputs)
    
    gradients = tape.gradient(predictions, inputs)
    
    return predictions, gradients

# Example usage
sample_input = X_train[:1]  # Select a sample from the training set
sample_label = y_train[:1]  # Corresponding label

predictions, gradients = get_gradients_model(sample_input)
predictions2, gradients2 = get_gradients_new_model(sample_input)
print(""Gradients for model:"", gradients)
print(""Gradients for new_model:"", gradients2)
```


### Relevant log output

Gradients for model: None
Gradients for new_model: tf.Tensor(
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0.]], shape=(1, 100), dtype=float32)","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-09-08T11:00:16Z,6,0,https://github.com/tensorflow/tensorflow/issues/75351,Logical Bug
108,tensorflow/tensorflow,"Calibrator segfaults trying to log the ""while"" operation","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 22.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.17.0

### 2. Code

[reproducer.zip](https://github.com/user-attachments/files/16894947/reproducer.zip)

### 3. Failure after conversion

Segmentation fault (signal 11) during conversion

### 5. (optional) Any other info / logs

The ""while"" operation does a check if an output tensor of the body subgraph is the same as the corresponding input tensor. If it's the case, it deallocates its own output tensor. The check is done at the prepare stage, so the affected tensor is already included in the ""loggable_outputs"" list by the calibrator. Then the calibrator tries to read the data from the deallocated tensor and segfaults. I've debugged it up to https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/calibration/calibrator.cc#L267 and found that the `tensor.data.f == nullptr`.
The check in question was introduced between 2.13 and 2.14, so it might be considered a regression: https://github.com/tensorflow/tensorflow/commit/7d49fd431ee5cebbb76eda88bc17e48921e10c85
","['awaiting review', 'stat:awaiting tensorflower', 'type:bug', 'comp:lite', 'TFLiteConverter', '2.17']",2024-09-05T01:23:29Z,19,0,https://github.com/tensorflow/tensorflow/issues/75140,Runtime Error
109,tensorflow/tensorflow,tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel raises a program abort,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When setting the `num_bits` in a large integer, this API raises the program abort.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
inputs = tf.constant(0.57681304)
min = tf.constant(2.1311088)
max = tf.constant(2.4402196)
num_bits = 10
narrow_range = False
tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel(inputs=inputs,min=min,max=max,num_bits=num_bits,narrow_range=narrow_range)
```
```


### Relevant log output

```shell
F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-08-31T17:52:07Z,1,0,https://github.com/tensorflow/tensorflow/issues/74932,Runtime Error
110,tensorflow/tensorflow,tensorflow.python.ops.gen_math_ops.sparse_bincount can cause a crash,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a `Segmentation fault ` issue in TensorFlow when using the `gen_math_ops.sparse_bincount` API. I have confirmed that the code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)

### Standalone code to reproduce the issue

```shell
from tensorflow.python.ops import gen_math_ops

values = [0, 1, 2, 2]
binary = False
indices = [[], [], [990, 2], [2, 349]]
dense_shape = []
gen_math_ops.sparse_bincount(indices=indices, values=values, dense_shape=dense_shape, size=3, weights=[],binary_output=binary)
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-08-31T06:18:00Z,1,0,https://github.com/tensorflow/tensorflow/issues/74918,Logical Bug
111,tensorflow/tensorflow, Aborted (core dumped): Check failed: d < dims() (1 vs. 1),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-nightly 2.18.0.dev20240817

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered an `aborted issue` in TensorFlow when I used API `array_ops.scatter_nd` . The code is as follows:

```python
import numpy as np
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops

GRADIENT_TESTS_DTYPES = (dtypes.bfloat16, dtypes.float16, dtypes.float32, dtypes.float64)

def scatter_nd(indices, updates, shape):
    return array_ops.scatter_nd(indices, updates, shape)

def testExtraIndicesDimensions():
    indices = array_ops.zeros((1, 1, 2), dtypes.int32)
    updates = array_ops.zeros([1], dtypes.int32)
    shape = np.array((2, 2))
    scatter = scatter_nd(indices, updates, shape)

testExtraIndicesDimensions()
```

> 2024-08-31 12:17:41.010855: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
> Aborted (core dumped)

### Standalone code to reproduce the issue

```shell
import numpy as np
from tensorflow.python.framework import dtypes
from tensorflow.python.ops import array_ops

GRADIENT_TESTS_DTYPES = (dtypes.bfloat16, dtypes.float16, dtypes.float32, dtypes.float64)

def scatter_nd(indices, updates, shape):
    return array_ops.scatter_nd(indices, updates, shape)

def testExtraIndicesDimensions():
    indices = array_ops.zeros((1, 1, 2), dtypes.int32)
    updates = array_ops.zeros([1], dtypes.int32)
    shape = np.array((2, 2))
    scatter = scatter_nd(indices, updates, shape)

testExtraIndicesDimensions()
```


### Relevant log output

```shell
2024-08-31 12:17:41.010855: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```
I have confirmed that above code would crash on `tf-nightly 2.18.0.dev20240817` (nightly-build)","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-08-31T04:18:19Z,2,0,https://github.com/tensorflow/tensorflow/issues/74912,Runtime Error
112,tensorflow/tensorflow,tf.math.floordiv produces incorrect result when the denominator is `-inf`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Based on the documentation https://www.tensorflow.org/api_docs/python/tf/math/floordiv, `tf.math.floordiv` should be equivalent to python's `//` operator. However, when the `x=1.4` and `y=-np.inf`, `tf.math.floordiv` outputs `-0.0` while `//` outputs `-1.0`.

I also checked Numpy and PyTorch's APIs, both output `-1.0`. 

It seems that the implementation of `tf.math.floordiv` is different from others, it would be nice if you can fix the implementation inconsistency, or make this inconsistency in the documentation.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import torch
import numpy as np
a = 1.4
b = -np.inf
print(""Numpy's result: "", np.floor_divide(a, b))
print(""Python's // result: "", a // b)
print(f""TF's result: {tf.math.floordiv(a, b)}"")
print(f""PyTorch's result: {torch.floor_divide(torch.tensor(a, dtype=torch.float32), torch.tensor(b, dtype=torch.float32))}"")
```
```


### Relevant log output

```shell
Numpy's result:  -1.0
Python's // result:  -1.0
TF's result: -0.0
PyTorch's result: -1.0
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-08-30T18:05:02Z,2,0,https://github.com/tensorflow/tensorflow/issues/74893,Logical Bug
113,tensorflow/tensorflow,tf.raw_ops.Round outputs zeros for any integer tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Expects the same tensor as the input according to the specification https://www.tensorflow.org/api_docs/python/tf/raw_ops/Round

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([-2, -1, 1, 2, 3])
tf.raw_ops.Round(x=x)
```


### Relevant log output

```shell
>>> import tensorflow as tf
>>> x = tf.constant([-2, -1, 1, 2, 3], dtype=tf.int32)
>>> tf.raw_ops.Round(x=x)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 0, 0, 0])>
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'regression issue', '2.17']",2024-08-29T12:24:07Z,2,0,https://github.com/tensorflow/tensorflow/issues/74789,Logical Bug
114,tensorflow/tensorflow,"With the same input and parameter settings, there is a large difference in the output of Dense layer on GPU and CPU.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.12.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We found that when training the model, there is a large difference in the output of the DENSE layer on the cpu and gpu when using the same input tensor and parameter settings.
```
CPU output: [[3.4838054e+34]]
GPU output: [[3.4838057e+34]]
2.4758800785707605e+27
```
We have tried some similar inputs, but no such problems have occurred.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import pickle
import h5py

tf.random.set_seed(42)
tf.config.experimental.enable_op_determinism()

def chebyshev_distance(A: np.ndarray, B: np.ndarray):
    if A is None or B is None:
        return 0.0
    if A.shape != B.shape:
        return 9999999
    else:
        return float(np.max(np.abs(A - B)))


tf.random.set_seed(42)

x_input = np.array([[37.63115]])
# x_input = np.array([[38.63115]])
# x_input = np.array([[3.763115]])
print(x_input)

dense_layer = tf.keras.layers.Dense(units=1, activation='exponential', use_bias=True, activity_regularizer=None, bias_constraint=None, bias_initializer='random_normal', kernel_initializer='he_normal', bias_regularizer=None, kernel_regularizer=None, kernel_constraint=None)

weights = [np.array([[2.112561]], dtype=np.float32), np.array([0.03791478], dtype=np.float32)]

dense_layer.build((1,))  
dense_layer.set_weights(weights)

with tf.device('/CPU:0'):
    x_cpu = tf.constant(x_input, dtype=tf.float32)
    output_cpu = dense_layer(x_cpu)
    print(""CPU output:"", output_cpu.numpy())


if tf.config.list_physical_devices('GPU'):
    with tf.device('/GPU:0'):
        x_gpu = tf.constant(x_input, dtype=tf.float32)
        output_gpu = dense_layer(x_gpu)
        print(""GPU output:"", output_gpu.numpy())
else:
    print(""GPU not available."")
    
output_diff = chebyshev_distance(output_cpu.numpy(), output_gpu.numpy())
print(output_diff)
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.12']",2024-08-29T09:59:58Z,2,0,https://github.com/tensorflow/tensorflow/issues/74783,Logical Bug
115,tensorflow/tensorflow,tf.sparse.reduce_sum error in JIT,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d

### Custom code

Yes

### OS platform and distribution

Ubuntu Mate 22.04

### Mobile device

_No response_

### Python version

Python 3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.3

### GPU model and memory

_No response_

### Current behavior?

Error when using tf.sparse.reduce_sum and JIT compilation.

I have written a layer that passes all my unit tests except when I use in a model with predict or train. 
Would that make sense with the JIT compilation on? I am not fully certain how and when this works.

Anyway, I am not exactly sure why my layer code fails, but I think that this minimum reproducible example captures the issue.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

batch_size = 4
input_shape = (3, 3)

indices = np.array([[0, 0, 0], [0, 0, 1], [1, 0, 0], [1, 0, 1], [2, 0, 0], [2, 0, 1], [3, 0, 0], [3, 0, 1]])
inputs = tf.sparse.SparseTensor(dense_shape=(batch_size, *input_shape),
                                indices=indices,
                                values=[9, 1, 9, 1, 9, 1, 9, 1])


@tf.function(input_signature=[tf.SparseTensorSpec(
    shape=(4, 3, 3),
    dtype=tf.dtypes.int32)], jit_compile=True)
def get_batch_sum(inputs):
    # same problem with tf.sparse.reduce_max
    return tf.sparse.reduce_sum(tf.sparse.reduce_sum(inputs, axis=1, output_is_sparse=True), axis=1)


sum_out = get_batch_sum(inputs)
print(sum_out)
```


### Relevant log output

```shell
2024-08-20 16:31:01.690779: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at xla_ops.cc:577 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263] on XLA_GPU_JIT: SparseReduceSumSparse (No registered 'SparseReduceSumSparse' OpKernel for XLA_GPU_JIT devices compatible with node {{node SparseReduceSumSparse}}){{node SparseReduceSumSparse}}
The op is created at: 
File "".config/JetBrains/PyCharmCE2023.3/scratches/scratch_30.py"", line 21, in <module>
File "".config/JetBrains/PyCharmCE2023.3/scratches/scratch_30.py"", line 18, in get_batch_sum
	tf2xla conversion failed while converting __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
Traceback (most recent call last):
(...)  

File "".config/JetBrains/PyCharmCE2023.3/scratches/scratch_30.py"", line 18, in get_batch_sum
	tf2xla conversion failed while converting __inference_get_batch_sum_15[_XlaMustCompile=true,config_proto=6001324581131673121,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_get_batch_sum_15]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'comp:xla', '2.17']",2024-08-20T14:39:42Z,2,0,https://github.com/tensorflow/tensorflow/issues/74131,Logical Bug
116,tensorflow/tensorflow,HLO for TopK oddly casts uint8 input to uint32 before passing to radix sort. ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2 (HEAD of internal repo)

### Custom code

Yes

### OS platform and distribution

Google-Internal Environment

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

V100 (also reproducible on other GPUs)

### Current behavior?

An Alphabet model invokes `tf.math.top_k` with a tensor of dtype uint8 and shape (1, 1,32768). 
![strange_hlo_text_with_uint32_radix_sort](https://github.com/user-attachments/assets/945af324-b00b-405b-8a68-06b7c33cb1e1)

For this call, XLA ends up calling radix sort. However, the radix sort is sub-optimal because TensorFlow casts the inputs to uint32 (instead of using original dtype uint8). Of course, radix sort is faster across smaller dtypes (with fewer bytes).

> @@(u32[1,32768]{1,0}, s32[1,32768]{1,0}, u8[271871]{0}) custom-call(u32[1,32768]{1,0}, s32[1,32768]{1,0}), custom_call_target=""__cub$DeviceRadixSort

I would expect HLO text more like this, where the uint8 inputs are passed directly:

> (u8[1,32768]{1,0}, s32[1,32768]{1,0}, u8[310527]{0}) custom-call(u8[1,32768]{1,0}, s32[1,32768]{1,0}), custom_call_target=""__cub$DeviceRadixSort""

We actually use TF indirectly via the jax2tf bridge, and I see this comment in the code hinting that uint8 is incompatible with `tf.math.top_k`:
https://github.com/google/jax/blob/0b87bf48f97ace10c7aee19c8f980788891a2df7/jax/experimental/jax2tf/jax2tf.py#L3167

However, recently, @dimitar-asenov (on XLA GPU) has made some changes to XLA sorting logic that provides support for radix-sorting uint8.

Could `tf.math.top_k` lower to HLO that avoids this up-cast to uint32 before radix sort? I believe this would halve the latency of radix sort for uint8.

### Standalone code to reproduce the issue

```shell
To repro, collect an XProf trace for the following snippet. See attached screenshot for sample trace.


import tensorflow as tf

data = tf.random.uniform(
          shape=(1, 32768),
          minval=0,
          maxval=256,
          dtype=tf.int32,
      )
data = tf.cast(data, tf.uint8)
_, box_indices = tf.math.top_k(data, k=1024)


Or feel free to just find and run my internal experimental `benchmark_tf_top_k_uint8` binary on a machine with a GPU.
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-08-18T21:52:59Z,2,0,https://github.com/tensorflow/tensorflow/issues/74035,Logical Bug
117,tensorflow/tensorflow,Cannot dlopen some GPU libraries [can't find cuda driver] in rhel9,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.17.0-rc1-2-gad6d8cc177d 2.17.0

### Custom code

Yes

### OS platform and distribution

Redhat enterprise 9.4 base image

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

Tesla T4, 15360MiB

### Current behavior?

What is your question?

Describe the bug

Error when I run GPU test, wondering if my docker linux kernel version is too low? The reason I am asking is that, my ubuntu22 is working fine.

WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723846435.253301 203 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-08-16 22:13:55.253747: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
""
My Kernel version,

bash-5.1# uname -a
Linux mlops-test-failed 5.10.220-209.869.amzn2.x86_64 https://github.com/rapidsai/cudf/issues/1 SMP Wed Jul 17 15:10:20 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

My OS/Docker image distro,
""
bash-5.1# uname -m && cat /etc/*release
x86_64
NAME=""Red Hat Enterprise Linux""
VERSION=""9.4 (Plow)""
ID=""rhel""
ID_LIKE=""fedora""
VERSION_ID=""9.4""
PLATFORM_ID=""platform:el9""
PRETTY_NAME=""Red Hat Enterprise Linux 9.4 (Plow)""
ANSI_COLOR=""0;31""
LOGO=""fedora-logo-icon""
CPE_NAME=""cpe:/o:redhat:enterprise_linux:9::baseos""
HOME_URL=""https://www.redhat.com/""
DOCUMENTATION_URL=""https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9""
BUG_REPORT_URL=""https://bugzilla.redhat.com/""

REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 9""
REDHAT_BUGZILLA_PRODUCT_VERSION=9.4
REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""
REDHAT_SUPPORT_PRODUCT_VERSION=""9.4""
Red Hat Enterprise Linux release 9.4 (Plow)
Red Hat Enterprise Linux release 9.4 (Plow)
""
Both my ubuntu 22 and rhel9 were showing the nvidia-smi ok like the following.

bash-5.1# nvidia-smi
Fri Aug 16 22:29:43 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06 Driver Version: 535.183.06 CUDA Version: 12.2 |
|-----------------------------------------+----------------------+----------------------+
| GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. |
| | | MIG M. |
|=========================================+======================+======================|
| 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 |
| N/A 33C P8 11W / 70W | 0MiB / 15360MiB | 0% Default |
| | | N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes: |
| GPU GI CI PID Type Process name GPU Memory |
| ID ID Usage |
|=======================================================================================|
| No running processes found |
+---------------------------------------------------------------------------------------+

I am checking the following url,
https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html

and it says the rhel9 kernel version has to be 5.14.0-427.

Your input is appreciated!

### Standalone code to reproduce the issue

```shell
def run_gpu_test():
    gpus = tf.config.list_logical_devices('GPU')
    print(""Num GPUs Available: "", len(gpus))

run_gpu_test()

In Ubuntu22, it prints gpu number 1 with all the gpu information and in rhel9 it does not.

This is a Pod we created in eks, and by exec to the pod, we pasted the debugging information in the above section. I did nvidia-smi and both ubuntu22 and rhel9 shows GPU fine. Ubuntu22 works fine but not rhel9. The node we created is a AWS G4 instance, so it has tensorflow 12.7 and cuda 12.2 and we installed nvidia-plugin. I think this should be very easy to reproduce not sure if the aws kernel version matters.
```


### Relevant log output

```shell
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1723846435.253301 203 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-08-16 22:13:55.253747: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
""
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', '2.17']",2024-08-16T23:39:02Z,3,1,https://github.com/tensorflow/tensorflow/issues/73978,Dependency Issue
118,tensorflow/tensorflow,tritonserver preload trt plugin got warning message and many core files : Failed to compile generated PTX with ptxas. Falling back to compilation by driver.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.2

### Custom code

No

### OS platform and distribution

linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tritonserver preload trt plugin got warning message and many core  dump files
`
2024-08-16 10:09:14.975649: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:16.033970: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:16.701031: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:17.498157: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
2024-08-16 10:09:18.328719: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:190] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.
`

![image](https://github.com/user-attachments/assets/c9cf824b-f27e-456d-aa43-af501aae0694)

I have an ensmble model, the first part is the tf model, the second part is the trt model. I have a trt plugin, and I load it as LD_PRELOAD. It won't be a problem if I load the two models separately. But when I load them at the same time this warning comes up and produces a lot of coredump files. Why is that? I don't understand how the trt plugin will affect tf

### Standalone code to reproduce the issue

```shell
LD_PRELOAD=/app/lib/ops/libtrtplugin.so --model-repository=/opt/model-repo-copy --model-control-mode=explicit --load-model=first_model --load-model=second_model  --load-model=ensmble_model  --log-verbose=0 --http-port=xxx--grpc-port=xxx --metrics-port=xxx --backend-config=tensorflow,version=2 --backend-config=tensorrt,version-compatible=true --disable-auto-complete-config 
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'TF 2.16']",2024-08-16T10:23:45Z,10,0,https://github.com/tensorflow/tensorflow/issues/73922,Dependency Issue
119,tensorflow/tensorflow,The average should not be computed in L2Pool2d,"The [ONNX L2Pool2d](https://github.com/onnx/onnx/blob/main/docs/Operators.md#lppool), [DirectML L2 Pooling Desc](https://docs.microsoft.com/en-us/windows/win32/api/directml/ns-directml-dml_lp_pooling_operator_desc) and [CoreML's l2_pool2d](https://apple.github.io/coremltools/source/coremltools.converters.mil.mil.ops.defs.html#coremltools.converters.mil.mil.ops.defs.iOS15.pool.l2_pool) calculate the l2 pooling by the expression `Y = (X1^2 + X2^2 + ... + Xn^2) ^ (1/2)`,  but [TFLite L2_PooL2d kernel implementation](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/internal/optimized/optimized_ops.h;l=3341?q=src%2Ftensorflow%2Flite%2Fkernels%2Finternal%2Foptimized%2Foptimized_ops.h) has the average with the count of sum elements  `Y=((X1^2 + X2^2 + ... + Xn^2)/n) ^ (1/2)`,  is it an issue of the kernel implementation?

BTW, [the kernel of l2_norm](https://source.chromium.org/chromium/chromium/src/+/main:third_party/tflite/src/tensorflow/lite/kernels/internal/optimized/optimized_ops.h;l=1424?q=L2Normalization&ss=chromium%2Fchromium%2Fsrc) also has no the average.

","['type:bug', 'comp:lite']",2024-08-14T00:51:39Z,3,1,https://github.com/tensorflow/tensorflow/issues/73742,Logical Bug
120,tensorflow/tensorflow,Unable to train/take gradient of integer variable under any condition,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.16.1-19-g810f233968c

### Custom code

Yes

### OS platform and distribution

Debian 11

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.6

### GPU model and memory

_No response_

### Current behavior?

Models with integer variables cannot have gradients computed (or be trained because of this). They fail with `ValueError: No gradients provided for any variable.` Please note that I'm referring to building/training models, _not_ post-training model quantization.

For context, I'm working on a somewhat novel approach to solving a type of engineering problem that includes both discrete and continuous values. In some cases, variables must be one of a set of numeric values (e.g. `1`, `2`, `3000`, etc). This isn't something that can really be split out into separate models and trained independently, as both these variables are used throughout a complex, multiple-input multiple-output layer based model. An approximation cannot be used either, as even a highly accurate approximation can feasibly result in an incorrect result under certain conditions.

Intuitively, a derivative cannot be calculated on integer values because they're discrete, and differentiation requires a continuous function over the domain of differentiation. However, floats (and any limited-precision data type) also suffer from this - but differentiation is generally considered ""viable"" for floats. Under any argument made for floats, integers should also be considered differentiable. There are certainly some issues with this for small number, but they have much less of an impact for larger ones (just like floats very close to 0 vs ""single digit"" floats i.e. `1`, `2`, etc).

If maintainers/triagers/project folks don't want to go in this direction, users like myself should be able to implement this via a `tf.custom_gradient` function. However, even when implementing this function, gradients/training fail in the exact same manner in the exact same place. When using a custom gradient function with an integer variable, TF fails with the aforementioned error message without even calling the custom gradient function. IMO this is pretty clearly a bug. See below for specific code to reproduce this.

### Standalone code to reproduce the issue

This fails (no custom gradient function, int32 type):

```python
import keras
import tensorflow as tf
import numpy as np

variable_dtype = tf.int32
# variable_dtype = tf.float32   # Uncommenting this fixes the issue


class BugTestLayer(keras.layers.Layer):
    # Layer is just y = self.var * x
    def build(self, input_shape):
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=variable_dtype)

    def call(self, input):
        return input * self.var


input_layer = keras.Input((1,), dtype=tf.int32)
test_layer = BugTestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output])

values = np.array([[i] for i in range(1000)])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

# This will always raise a `ValueError: No gradients provided for any variable.`
# when using an integer type
history = model.fit(values, values, batch_size=1, epochs=2)
```

This works (no custom gradient function, float32 type):
```python
import keras
import tensorflow as tf
import numpy as np

# variable_dtype = tf.int32
variable_dtype = tf.float32   # Uncommenting this fixes the issue


class BugTestLayer(keras.layers.Layer):
    # Layer is just y = self.var * x
    def build(self, input_shape):
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=variable_dtype)

    def call(self, input):
        return input * self.var


input_layer = keras.Input((1,), dtype=tf.int32)
test_layer = BugTestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output])

values = np.array([[i] for i in range(1000)])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

# This will always raise a `ValueError: No gradients provided for any variable.`
# when using an integer type
history = model.fit(values, values, batch_size=1, epochs=2)
```

This fails (custom gradient function, int32 type):
```python
# %%

import keras
import tensorflow as tf
import numpy as np

# %%


class TestLayer(keras.layers.Layer):
    def build(self, input_shape):
        # dtype is the problem.
        # integer type results in ""No gradients provided for any variable"", while
        # floats work just fine.
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=tf.int32)

    @tf.custom_gradient
    def op(input, var):
        def grad(upstream, variables=None):
            return tf.squeeze(upstream * tf.cast(var, tf.float32), axis=[1]), \
                tf.squeeze(upstream * tf.cast(input, tf.float32), axis=[1])

        return tf.cast(input, tf.float32) * tf.cast(var, tf.float32), grad

    def call(self, input):
        return TestLayer.op(input, self.var)

# %%


input_layer = keras.Input((1,), name=""input_layer"", dtype=tf.int32)
test_layer = TestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output], name=""model"")

# %%

model.summary()
# Example evaluation (untrained)
model(tf.constant([[100]]))

keras.utils.plot_model(model, ""my_first_model.png"", show_dtype=True,
                       show_layer_names=True, show_shapes=True, show_trainable=True)


# %%

values = np.array([
    [i]
    for i in range(1000)
])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

history = model.fit(values, values, batch_size=1, epochs=10)
```

This works (custom gradient function, float32 type):
```python
# %%

import keras
import tensorflow as tf
import numpy as np

# %%


class TestLayer(keras.layers.Layer):
    def build(self, input_shape):
        # dtype is the problem.
        # integer type results in ""No gradients provided for any variable"", while
        # floats work just fine.
        self.var = self.add_variable(
            (1,), initializer=""zeros"", dtype=tf.int32)

    @tf.custom_gradient
    def op(input, var):
        def grad(upstream, variables=None):
            return tf.squeeze(upstream * tf.cast(var, tf.float32), axis=[1]), \
                tf.squeeze(upstream * tf.cast(input, tf.float32), axis=[1])

        return tf.cast(input, tf.float32) * tf.cast(var, tf.float32), grad

    def call(self, input):
        return TestLayer.op(input, self.var)

# %%


input_layer = keras.Input((1,), name=""input_layer"", dtype=tf.float32)
test_layer = TestLayer()

output = test_layer(input_layer)
model = keras.Model(inputs=[input_layer], outputs=[output], name=""model"")

# %%

model.summary()
# Example evaluation (untrained)
model(tf.constant([[100]]))

keras.utils.plot_model(model, ""my_first_model.png"", show_dtype=True,
                       show_layer_names=True, show_shapes=True, show_trainable=True)


# %%

values = np.array([
    [i]
    for i in range(1000)
])

model.compile(
    loss=[keras.losses.MeanSquaredError()],
    optimizer=keras.optimizers.RMSprop(),
    metrics=[keras.metrics.MeanSquaredError()],
)

history = model.fit(values, values, batch_size=1, epochs=10)
```


### Relevant log output

Logs for the last failure:

```shell
Epoch 1/10
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File /workspaces/power sim 2/test_docs.py:14
      3 values = np.array([
      4     [i]
      5     for i in range(1000)
      6 ])
      8 model.compile(
      9     loss=[keras.losses.MeanSquaredError()],
     10     optimizer=keras.optimizers.RMSprop(),
     11     metrics=[keras.metrics.MeanSquaredError()],
     12 )
---> 14 history = model.fit(values, values, batch_size=1, epochs=10)

File ~/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/.local/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:662, in BaseOptimizer._filter_empty_gradients(self, grads, vars)
    659         missing_grad_vars.append(v.name)
    661 if not filtered_grads:
--> 662     raise ValueError(""No gradients provided for any variable."")
    663 if missing_grad_vars:
    664     warnings.warn(
    665         ""Gradients do not exist for variables ""
    666         f""{list(reversed(missing_grad_vars))} when minimizing the loss.""
    667         "" If using `model.compile()`, did you forget to provide a ""
    668         ""`loss` argument?""
    669     )

ValueError: No gradients provided for any variable.
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-08-12T21:23:47Z,1,0,https://github.com/tensorflow/tensorflow/issues/73631,Logical Bug
121,tensorflow/tensorflow,Segmentation fault (core dumped) in `tf.config.threading.set_inter_op_parallelism_threads`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Crash triggered when input boundary values into tf.config.threading.set_intra_op_parallelism_threads

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/12s6D2GuBFEWAdvFdCvjbs4nK888vLGvm?usp=sharing
```


### Relevant log output

```shell
2024-08-10 21:36:47.636016: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-10 21:36:47.703371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-10 21:36:47.791020: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-10 21:36:47.817978: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-10 21:36:47.883418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-10 21:36:56.611712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22279 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:1f:00.0, compute capability: 8.9
2024-08-10 21:36:56.617085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1717 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:d4:00.0, compute capability: 8.9
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-08-10T13:39:20Z,1,0,https://github.com/tensorflow/tensorflow/issues/73519,Runtime Error
122,tensorflow/tensorflow,Aborted (core dumped) in `tf.config.threading.set_intra_op_parallelism_threads`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Crash triggered when input negative numbers into tf.config.threading.set_intra_op_parallelism_threads.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/17JV6ppGU1XtQg25PKa8itDhihAspgHIz?usp=sharing
```


### Relevant log output

```shell
2024-08-10 21:29:55.538868: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-08-10 21:29:55.869155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-08-10 21:29:55.967845: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-08-10 21:29:56.002644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-08-10 21:29:56.222202: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-08-10 21:30:05.961788: F external/local_tsl/tsl/platform/threadpool.cc:112] Check failed: num_threads >= 1 (1 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-08-10T13:33:57Z,1,0,https://github.com/tensorflow/tensorflow/issues/73518,Runtime Error
123,tensorflow/tensorflow,Issue with softmax warning appearing in Tensorflow 2.17.0,"### TensorFlow version

2.17.0

### OS platform and distribution

Google Colab

### Current behavior?

Hi, everyone.

I am practicing implementing a Transformer model that machine translates English into Korean by reading TensorFlow guides and books. However, I am having trouble because an unknown UserWarning appears during the final translation process. In that issue, I've never used softmax before, but warning me about using it. This problem appears after the model has finished training and when making inferences.

I searched to see if there were any cases similar to mine, but it seems that no solution was found in any of them. 



same issue: https://github.com/tensorflow/tensorflow/issues/67758



This problem did not exist in tensorflow 2.15 but appeared in 2.17.0. I can't even guess what could be causing it. For those of you who are curious about the full code, I am leaving a Colab link. You can easily reproduce it by running it with Ctrl + F9 in Google Colab. The execution time of the entire code is approximately 5 minutes ~ 5 minutes and 30 seconds on a T4 GPU. That issue is at the bottom.

Colab Link: https://colab.research.google.com/drive/1IMFWoJ1s5ReKU9LYENROpAsZ47D6cG8T?usp=sharing

The data I used is 'kor-eng.zip' located at ""https://www.manythings.org/anki/"".

I'm really sorry for not writing the comments in English.

### Standalone code to reproduce the issue

```shell
class Transformer(keras.Model):
    def __init__(self, *, num_layers, encoder_sequence_length, decoder_sequence_length, source_vocab_size, target_vocab_size, embed_dim,
                 dense_dim, num_heads, dropout_rate):
        super().__init__()
        self.encoder = Encoder(num_layers=num_layers, sequence_length=encoder_sequence_length, input_dim=source_vocab_size, embed_dim=embed_dim,
                              dense_dim=dense_dim, num_heads=num_heads, dropout_rate=dropout_rate)
        self.decoder = Decoder(num_layers=num_layers, sequence_length=decoder_sequence_length, input_dim=target_vocab_size, embed_dim=embed_dim,
                              dense_dim=dense_dim, num_heads=num_heads, dropout_rate=dropout_rate)
        self.final_layer = tf.keras.layers.Dense(units=target_vocab_size)

    def call(self, inputs):
        encoder_inputs, decoder_inputs = inputs
        encoder_pad_mask = tf.math.not_equal(encoder_inputs, 0)[:, tf.newaxis]
        decoder_pad_mask = tf.math.not_equal(decoder_inputs, 0)[:, tf.newaxis]

        decoder_sequence_length = tf.shape(decoder_inputs)[1]
        causal_mask = tf.linalg.band_part(tf.ones((decoder_sequence_length, decoder_sequence_length), tf.bool), -1, 0)

        encoder_inputs = self.encoder(inputs=encoder_inputs, encoder_pad_mask=encoder_pad_mask)  # Shape: (batch_size, encoder_sequence_length, embed_dim)
        decoder_inputs = self.decoder(inputs=decoder_inputs, encoder_outputs=encoder_inputs,
                                      encoder_pad_mask=encoder_pad_mask, decoder_pad_mask=decoder_pad_mask,
                                      causal_mask=causal_mask)  # Shape: (batch_size, decoder_sequence_length, embed_dim)

        logits = self.final_layer(decoder_inputs)  # Shape: (batch_size, decoder_sequence_length, target_vocab_size)

        try:
            # losses/metrics가 커지지 않게 keras_mask를 제거
            del logits._keras_mask
        except AttributeError:
            pass

        return logits


class Translator(tf.Module):
    # 데이터 전처리 함수
    @staticmethod
    def preprocess_text(text_: str, max_repeat: int=2) -> str:
        """"""텍스트 문자열 중 일부 규칙적인 부분을 전처리 하는 함수

        Args:
            text_: 텍스트 문자열 -> str
            max_repeat: 똑같은 문자열이 연속해서 반복할 수 있는 최대 횟수 -> int

        Returns:
            text_: 전처리 된 텍스트 문자열 -> str
        """"""

        text_ = text_.lower()
        text_ = re.sub(pattern=rf""[^\w\s{string.punctuation}]"", repl=r"""", string=text_)
        text_ = re.sub(pattern=r""\?+"", repl=r""?"", string=text_) # ?가 2번 이상 연속되면 ?로 수정
        text_ = re.sub(pattern=r""\!+"", repl=r""!"", string=text_) # !가 2번 이상 연속되면 !로 수정
        text_ = re.sub(pattern=r""(?P<char>\D)(?P=char){"" + str(max_repeat - 1) + r"",}"", repl=r""\g<char>"" * max_repeat, string=text_) # 숫자가 아닌 똑같은 문자열이 repeat번 이상 연속되면 repeat만큼으로 수정
        text_ = re.sub(pattern=r""\.{2,}"", repl=r""..."", string=text_) # ..을 ...으로 변경
        text_ = re.sub(pattern=r""\.\.\.(?P<s>\w)"", repl=r""... \g<s>"", string=text_) # 띄어쓰기 교정
        text_ = re.sub(pattern=r""\s+"", repl="" "", string=text_)

        # 영어 축약형을 풀기
        # 's 축약은 is / has 또는 아예 소유격으로 가능해서 제외
        # 'd 축약은 had / would / could 등으로 가능해서 제외
        # 'll 축약은 shall / will로 가능해서 제외
        text_ = re.sub(pattern=r""\bi'm\b"", repl=r""i am"", string=text_)
        text_ = re.sub(pattern=r""\b(?P<subj>you|we|they|there|who|when|where|what|how|why)'re\b"", repl=r""\g<subj> are"", string=text_)
        text_ = re.sub(pattern=r""\b(?P<verb>is|are|was|were|do|does|did|have|has|had|must|should|may|might|could|would|ought|dare|need)n't\b"", repl=r""\g<verb> not"", string=text_)
        text_ = re.sub(pattern=r""\bwon't\b"", repl=r""will not"", string=text_)
        text_ = re.sub(pattern=r""\bcan't\b"", repl=r""can not"", string=text_)
        text_ = re.sub(pattern=r""\bshan't\b"", repl=r""shall not"", string=text_)
        text_ = re.sub(pattern=r""\b(?P<subj>i|you|they|we|should|could|would|must|not)'ve\b"", repl=r""\g<subj> have"", string=text_)

        text_ = text_.strip()
        return text_

    # tensorflow의 바이트 문자열 데이터 전처리 함수
    @tf.function
    def tf_preprocess_text(text_: tf.string, max_repeat: int=2) -> tf.string:
        """"""텍스트 문자열 중 일부 규칙적인 부분을 전처리 하는 함수

        Args:
            text_: 바이트 문자열 -> tf.string
            max_repeat: 똑같은 문자열이 연속해서 반복할 수 있는 최대 횟수 -> int

        Returns:
            text_: 전처리 된 바이트 문자열 -> tf.string
        """"""

        text_ = tf.strings.lower(input=text_)
        text_ = tf.strings.regex_replace(input=text_, pattern=rf""[^\w\s{string.punctuation}]"", rewrite=r"""")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\?+"", rewrite=r""?"") # ?가 2번 이상 연속되면 ?로 수정
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\!+"", rewrite=r""!"") # !가 2번 이상 연속되면 !로 수정

        # 숫자가 아닌 똑같은 문자열이 repeat번 이상 연속되면 repeat만큼으로 수정
        stacks = tf.TensorArray(dtype=tf.string, size=0, dynamic_size=True)
        for s in tf.strings.bytes_split(text_):
            if stacks.size() >= max_repeat:
                if tf.strings.regex_full_match(input=s, pattern=r""\D""):
                    back_s = stacks.gather(indices=tf.range(start=stacks.size() - max_repeat, limit=stacks.size(), delta=1))
                    if tf.math.reduce_all(back_s == s):
                        continue

            stacks = stacks.write(stacks.size(), s)

        text_ = tf.strings.reduce_join(inputs=stacks.stack())

        text_ = tf.strings.regex_replace(input=text_, pattern=r""\.{2,}"", rewrite=r""..."") # ..을 ...으로 변경
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\.\.\.(\w)"", rewrite=r""... \1"") # 띄어쓰기 교정
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\s+"", rewrite=r"" "")

        # 영어 축약형을 풀기
        # 's 축약은 is / has 또는 아예 소유격으로 가능해서 제외
        # 'd 축약은 had / would / could 등으로 가능해서 제외
        # 'll 축약은 shall / will로 가능해서 제외
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bi'm\b"", rewrite=r""i am"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\b(you|we|they|there|who|when|where|what|how|why)'re\b"", rewrite=r""\1 are"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\b(is|are|was|were|do|does|did|have|has|had|must|should|may|might|could|would|ought|dare|need)n't\b"", rewrite=r""\1 not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bwon't\b"", rewrite=r""will not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bcan't\b"", rewrite=r""can not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\bsha't\b"", rewrite=r""shall not"")
        text_ = tf.strings.regex_replace(input=text_, pattern=r""\b(i|you|they|we|should|could|would|must|not)'ve\b"", rewrite=r""\1 have"")

        text_ = tf.strings.strip(input=text_)
        return text_

    def __init__(self, source_tokenizer, target_tokenizer, target_length, model):
        super().__init__()
        self.source_tokenizer = source_tokenizer
        self.target_tokenizer = target_tokenizer
        self.target_length = target_length
        self.model = model

    def __call__(self, sentence):
        sentence = Translator.tf_preprocess_text(text_=sentence)
        sentence_token = self.source_tokenizer.tokenize(sentence)[tf.newaxis]

        encoder_input = sentence_token

        starts = tf.constant(2, dtype=tf.int32)[tf.newaxis]
        ends = tf.constant(3, dtype=tf.int32)[tf.newaxis]
        decoder_token = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)
        decoder_token = decoder_token.write(0, starts)

        for i in tf.range(self.target_length):
            decoder_input = tf.transpose(decoder_token.stack())
            predictions = self.model([encoder_input, decoder_input], training=False)
            predictions = predictions[0, -1, :] # 마지막 토큰으로부터 예측
            predicted_id = tf.argmax(input=predictions, output_type=tf.int32)[tf.newaxis]

            if predicted_id == ends:
                break

            decoder_token = decoder_token.write(i + 1, predicted_id)

        decoder_token = tf.transpose(decoder_token.stack())[0]
        decoder_token = decoder_token[1:]

        return self.target_tokenizer.detokenize(decoder_token)
```


### Relevant log output

```shell
/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 8, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?
  warnings.warn(
English: tom came here to learn french.
Translated Korean: 톰은 프랑스어를 배우러 여기에 왔어. 
Real Korean: <s> 톰은 프랑스어를 배우려고 여기에 왔어. </s>
```
","['type:bug', '2.17']",2024-08-10T07:12:43Z,3,0,https://github.com/tensorflow/tensorflow/issues/73516,Runtime Error
124,tensorflow/tensorflow,Memory leak in training,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1 and 2.17

### Custom code

Yes

### OS platform and distribution

Debian 11

### Mobile device

_No response_

### Python version

Python 3.10.14

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda/12.0.0_gcc-10.4.0 and cudnn/8.9.7.29-12_gcc-10.4.0

### GPU model and memory

different GPU, among which Tesla V100-SXM2-32GB

### Current behavior?

I have a memory leak (GPU memory and RAM constantly increase) during my training. 

**This does not happen with Tensorflow 2.11.1**. 

[Here is the project](https://github.com/deep-finder/tirfm-deepfinder).
Unfortunately, this is not my code and I do not have time to create a minimal standalone code.

### Standalone code to reproduce the issue

```shell
def printMemoryUsage(self):
        gpus = tf.config.list_physical_devices('GPU')             
        for gpu in gpus:
            gpuNameRoot = gpu.name.split(':')[0] + ':'
            memory_info = tf.config.experimental.get_memory_info(gpu.name.replace(gpuNameRoot, ''))
            self.display(f'Memory info of GPU {gpu.name}: current: {memory_info[""current""]/1e9:.2f}, peak: {memory_info[""peak""]/1e9:.2f}')
            try:
                import psutil
                virtual_memory = psutil.virtual_memory()
                print(f'Memory info of CPU: total:{virtual_memory[0]/1e9:.2f}Gb, available: {virtual_memory[1]/1e9:.2f}Gb, percent: {virtual_memory[2]}%')
            except:
                pass

[...]
        # Training loop:
        for e in range(self.epochs):
            # TRAINING:
            start = time.time()
            list_loss_train = []
            list_acc_train = []
            for it in range(self.steps_per_epoch):
                if self.flag_direct_read:
                    batch_data, batch_target = self.generate_batch_direct_read(path_data, path_target, self.batch_size, objlist_train)
                else:
                    batch_data, batch_target, idx_list = self.generate_batch_from_array(data_list, target_list, self.batch_size, objlist_train)

                if self.sample_weights is not None:
                    sample_weight = self.sample_weights[idx_list]
                else:
                    sample_weight = None

                loss_train = self.net.train_on_batch(batch_data, batch_target,
                                                     class_weight=self.class_weight,
                                                     sample_weight=sample_weight)

                self.display('epoch %d/%d - it %d/%d - loss: %0.3f - acc: %0.3f' % (e + 1, self.epochs, it + 1, self.steps_per_epoch, loss_train[0], loss_train[1]))

                self.printMemoryUsage()

                list_loss_train.append(loss_train[0])
                list_acc_train.append(loss_train[1])
                del batch_data
                del batch_target
                if idx_list is not None:
                    del idx_list
                gc.collect()
```


### Relevant log output

```shell
With Tensorflow 2.11.1:


=============================================================
epoch 3/50 - it 1/100 - loss: 2.012 - acc: 0.976
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 2/100 - loss: 2.008 - acc: 0.985
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 3/100 - loss: 2.004 - acc: 0.992
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 4/100 - loss: 2.006 - acc: 0.987
Memory info of GPU /physical_device:GPU:0: current: 0.02, peak: 2.90
Memory info of CPU: total:201.37Gb, available: 191.38Gb, percent: 5.0%
epoch 3/50 - it 5/100 - loss: 2.006 - acc: 0.989
```


With Tensorflow 2.16.1 and 2.17:

```
epoch 1/50 - it 6/100 - loss: 2.473 - acc: 0.305
Memory info of GPU /physical_device:GPU:0: current: 0.18, peak: 2.55
Memory info of CPU: total:201.37Gb, available: 190.78Gb, percent: 5.3%
epoch 1/50 - it 7/100 - loss: 2.470 - acc: 0.394
Memory info of GPU /physical_device:GPU:0: current: 0.21, peak: 2.58
Memory info of CPU: total:201.37Gb, available: 190.58Gb, percent: 5.4%
epoch 1/50 - it 8/100 - loss: 2.466 - acc: 0.463
Memory info of GPU /physical_device:GPU:0: current: 0.24, peak: 2.61
Memory info of CPU: total:201.37Gb, available: 190.38Gb, percent: 5.5%
epoch 1/50 - it 9/100 - loss: 2.461 - acc: 0.519
Memory info of GPU /physical_device:GPU:0: current: 0.27, peak: 2.64
Memory info of CPU: total:201.37Gb, available: 190.11Gb, percent: 5.6%
epoch 1/50 - it 10/100 - loss: 2.455 - acc: 0.564
Memory info of GPU /physical_device:GPU:0: current: 0.29, peak: 2.67
Memory info of CPU: total:201.37Gb, available: 189.94Gb, percent: 5.7%
epoch 1/50 - it 11/100 - loss: 2.448 - acc: 0.603
Memory info of GPU /physical_device:GPU:0: current: 0.32, peak: 2.70
Memory info of CPU: total:201.37Gb, available: 189.67Gb, percent: 5.8%
epoch 1/50 - it 12/100 - loss: 2.437 - acc: 0.634
Memory info of GPU /physical_device:GPU:0: current: 0.35, peak: 2.72
Memory info of CPU: total:201.37Gb, available: 189.48Gb, percent: 5.9%
epoch 1/50 - it 13/100 - loss: 2.425 - acc: 0.662
Memory info of GPU /physical_device:GPU:0: current: 0.38, peak: 2.75
Memory info of CPU: total:201.37Gb, available: 189.21Gb, percent: 6.0%
epoch 1/50 - it 14/100 - loss: 2.408 - acc: 0.685
Memory info of GPU /physical_device:GPU:0: current: 0.41, peak: 2.78
Memory info of CPU: total:201.37Gb, available: 189.03Gb, percent: 6.1%
epoch 1/50 - it 15/100 - loss: 2.389 - acc: 0.705
Memory info of GPU /physical_device:GPU:0: current: 0.44, peak: 2.81
Memory info of CPU: total:201.37Gb, available: 188.79Gb, percent: 6.2%
epoch 1/50 - it 16/100 - loss: 2.369 - acc: 0.723
Memory info of GPU /physical_device:GPU:0: current: 0.46, peak: 2.84
Memory info of CPU: total:201.37Gb, available: 188.54Gb, percent: 6.4%
epoch 1/50 - it 17/100 - loss: 2.350 - acc: 0.739
Memory info of GPU /physical_device:GPU:0: current: 0.49, peak: 2.87
Memory info of CPU: total:201.37Gb, available: 188.37Gb, percent: 6.5%
epoch 1/50 - it 18/100 - loss: 2.332 - acc: 0.753
Memory info of GPU /physical_device:GPU:0: current: 0.52, peak: 2.89
Memory info of CPU: total:201.37Gb, available: 188.10Gb, percent: 6.6%
epoch 1/50 - it 19/100 - loss: 2.315 - acc: 0.765
Memory info of GPU /physical_device:GPU:0: current: 0.55, peak: 2.92
Memory info of CPU: total:201.37Gb, available: 187.87Gb, percent: 6.7%
epoch 1/50 - it 20/100 - loss: 2.300 - acc: 0.776
Memory info of GPU /physical_device:GPU:0: current: 0.58, peak: 2.95
Memory info of CPU: total:201.37Gb, available: 187.64Gb, percent: 6.8%
epoch 1/50 - it 21/100 - loss: 2.286 - acc: 0.786
Memory info of GPU /physical_device:GPU:0: current: 0.61, peak: 2.98
Memory info of CPU: total:201.37Gb, available: 187.49Gb, percent: 6.9%
epoch 1/50 - it 22/100 - loss: 2.274 - acc: 0.795
Memory info of GPU /physical_device:GPU:0: current: 0.63, peak: 3.01
Memory info of CPU: total:201.37Gb, available: 187.25Gb, percent: 7.0%
epoch 1/50 - it 23/100 - loss: 2.262 - acc: 0.804
Memory info of GPU /physical_device:GPU:0: current: 0.66, peak: 3.04
Memory info of CPU: total:201.37Gb, available: 186.97Gb, percent: 7.1%
epoch 1/50 - it 24/100 - loss: 2.251 - acc: 0.811
Memory info of GPU /physical_device:GPU:0: current: 0.69, peak: 3.06
Memory info of CPU: total:201.37Gb, available: 186.81Gb, percent: 7.2%
epoch 1/50 - it 25/100 - loss: 2.241 - acc: 0.818
Memory info of GPU /physical_device:GPU:0: current: 0.72, peak: 3.09
Memory info of CPU: total:201.37Gb, available: 186.59Gb, percent: 7.3%
epoch 1/50 - it 26/100 - loss: 2.232 - acc: 0.825
Memory info of GPU /physical_device:GPU:0: current: 0.75, peak: 3.12
Memory info of CPU: total:201.37Gb, available: 186.36Gb, percent: 7.5%
epoch 1/50 - it 27/100 - loss: 2.224 - acc: 0.831
Memory info of GPU /physical_device:GPU:0: current: 0.78, peak: 3.15
Memory info of CPU: total:201.37Gb, available: 186.09Gb, percent: 7.6%
epoch 1/50 - it 28/100 - loss: 2.216 - acc: 0.836
Memory info of GPU /physical_device:GPU:0: current: 0.80, peak: 3.18
Memory info of CPU: total:201.37Gb, available: 185.90Gb, percent: 7.7%
epoch 1/50 - it 29/100 - loss: 2.209 - acc: 0.841
Memory info of GPU /physical_device:GPU:0: current: 0.83, peak: 3.21
Memory info of CPU: total:201.37Gb, available: 185.65Gb, percent: 7.8%
epoch 1/50 - it 30/100 - loss: 2.202 - acc: 0.846
Memory info of GPU /physical_device:GPU:0: current: 0.86, peak: 3.23
Memory info of CPU: total:201.37Gb, available: 185.46Gb, percent: 7.9%
epoch 1/50 - it 31/100 - loss: 2.196 - acc: 0.851
Memory info of GPU /physical_device:GPU:0: current: 0.89, peak: 3.26
Memory info of CPU: total:201.37Gb, available: 185.24Gb, percent: 8.0%
epoch 1/50 - it 32/100 - loss: 2.190 - acc: 0.855
Memory info of GPU /physical_device:GPU:0: current: 0.92, peak: 3.29
Memory info of CPU: total:201.37Gb, available: 184.99Gb, percent: 8.1%
epoch 1/50 - it 33/100 - loss: 2.185 - acc: 0.859
Memory info of GPU /physical_device:GPU:0: current: 0.95, peak: 3.32
Memory info of CPU: total:201.37Gb, available: 184.75Gb, percent: 8.3%
epoch 1/50 - it 34/100 - loss: 2.180 - acc: 0.863
Memory info of GPU /physical_device:GPU:0: current: 0.97, peak: 3.35
Memory info of CPU: total:201.37Gb, available: 184.51Gb, percent: 8.4%
epoch 1/50 - it 35/100 - loss: 2.174 - acc: 0.866
Memory info of GPU /physical_device:GPU:0: current: 1.00, peak: 3.38
Memory info of CPU: total:201.37Gb, available: 184.36Gb, percent: 8.4%
epoch 1/50 - it 36/100 - loss: 2.170 - acc: 0.870
Memory info of GPU /physical_device:GPU:0: current: 1.03, peak: 3.40
Memory info of CPU: total:201.37Gb, available: 184.06Gb, percent: 8.6%
epoch 1/50 - it 37/100 - loss: 2.165 - acc: 0.873
Memory info of GPU /physical_device:GPU:0: current: 1.06, peak: 3.43
Memory info of CPU: total:201.37Gb, available: 183.89Gb, percent: 8.7%
epoch 1/50 - it 38/100 - loss: 2.161 - acc: 0.876
Memory info of GPU /physical_device:GPU:0: current: 1.09, peak: 3.46
Memory info of CPU: total:201.37Gb, available: 183.66Gb, percent: 8.8%
epoch 1/50 - it 39/100 - loss: 2.157 - acc: 0.879
Memory info of GPU /physical_device:GPU:0: current: 1.11, peak: 3.49
Memory info of CPU: total:201.37Gb, available: 183.42Gb, percent: 8.9%
epoch 1/50 - it 40/100 - loss: 2.153 - acc: 0.881
Memory info of GPU /physical_device:GPU:0: current: 1.14, peak: 3.52
Memory info of CPU: total:201.37Gb, available: 183.26Gb, percent: 9.0%
epoch 1/50 - it 41/100 - loss: 2.150 - acc: 0.884
Memory info of GPU /physical_device:GPU:0: current: 1.17, peak: 3.54
Memory info of CPU: total:201.37Gb, available: 183.02Gb, percent: 9.1%
```
```
","['stat:awaiting tensorflower', 'type:bug', '2.17']",2024-08-09T10:51:49Z,4,0,https://github.com/tensorflow/tensorflow/issues/73457,Performance Issue
125,tensorflow/tensorflow,Gradient computation for `vectorized_map` nested inside `while_loop`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

Yes

### OS platform and distribution

Linux Mint 21.3

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to compute gradients for a `tf.vectorized_map`ped function nested within a call to `tf.while_loop` (and hence also `tf.map_fn`) as in [this (trivial!) Colab example](https://colab.research.google.com/drive/1IEsQAM_AU2H0bfiOfxdphk5dyi9kmS8g?usp=sharing).  The top level function can compute its return value in all three execution modes (eager, graph, XLA).  

I expected to also be able to compute the gradients of the function with respect to its inputs.  This works under eager and graph mode, but not XLA mode where an InvalidArgument exception occurs:

> InvalidArgumentError: Input 1 to node `gradient_tape/while/gradients/while/PartitionedCall_grad/PartitionedCall/gradients/pfor/Tile_grad/Reshape_1` with op Reshape must be a compile-time constant.

In the example, all shapes are well-specified (by hard coding in this trivial example), so it feels like some shape information is getting lost somewhere.  Is this a bug, or a ""feature"" for which there is a workaround, I wonder? 



### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1IEsQAM_AU2H0bfiOfxdphk5dyi9kmS8g?usp=sharing
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-1-151283a3b91b> in <cell line: 36>()
     34 print(""Eager mode:"", value_and_grads(0.1, 0.4))  # Eager mode runs
     35 print(""Graph mode:"", tf.function(lambda: value_and_grads(0.1, 0.4))()) # Graph mode runs
---> 36 print(""XLA mode:"", tf.function(lambda: value_and_grads(0.1, 0.4), jit_compile=True)()) # XLA fails

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Input 1 to node `gradient_tape/while/gradients/while/PartitionedCall_grad/PartitionedCall/gradients/pfor/Tile_grad/Reshape_1` with op Reshape must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.

Stack trace for op definition: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-1-151283a3b91b>"", line 36, in <cell line: 36>
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 30, in value_and_grads

	 [[{{function_node __inference___backward_f_460_482}}{{node gradients/pfor/Tile_grad/Reshape_1}}]]
	tf2xla conversion failed while converting while_body_347_grad_431_const_0[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.

Stack trace for op definition: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-1-151283a3b91b>"", line 36, in <cell line: 36>
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 36, in 
File ""<ipython-input-1-151283a3b91b>"", line 30, in value_and_grads

	 [[gradient_tape/while/while_grad]]
	tf2xla conversion failed while converting __inference_<lambda>_538[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_<lambda>_538]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', '2.17']",2024-08-08T09:36:08Z,1,0,https://github.com/tensorflow/tensorflow/issues/73367,Logical Bug
126,tensorflow/tensorflow,[BUG] Optimizer crash on TPU: `AttributeError: 'NoneType' object has no attribute 'extended'`,"### Issue type: Bug

System info:

- Kaggle TPU VM v3-8
- Python 3.10
- TensorFlow 2.16.1

### Standalone code to reproduce the issue

```python
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    tpu_strategy = tf.distribute.TPUStrategy(tpu)
    print(""TPU setup successful"")
except (ValueError, ImportError) as e:
    tpu_strategy = tf.distribute.get_strategy()

class BertSLPModel(tf.keras.Model):
    def __init__(self):
        super(BertSLPModel, self).__init__()
        self.bert = bert
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        self.classifier = tf.keras.layers.Dense(num_classes)

    def call(self, inputs):
        input_ids, attention_mask = inputs
        bert_output = self.bert(input_ids, attention_mask=attention_mask)
        pooled_output = bert_output.pooler_output
        dropout_output = self.dropout(pooled_output)
        logit = self.classifier(dropout_output)
        return logit

with tpu_strategy.scope():
    model = BertSLPModel()
    model.compile(
        optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-5),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
    )
    model.fit(
        train_dataset,
        validation_data=test_dataset,
        epochs=epochs,
        batch_size=batch_size
    )
```


### Relevant log output

```shell
AttributeError: in user code:

    File ""/tf_keras/src/engine/training.py"", line 1398, in train_function  *
        return step_function(self, iterator)
    File ""/tf_keras/src/engine/training.py"", line 1370, in run_step  *
        outputs = model.train_step(data)
    File ""/tf_keras/src/engine/training.py"", line 1151, in train_step  *
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/tf_keras/src/optimizers/optimizer.py"", line 621, in minimize  *
        self.apply_gradients(grads_and_vars)
    File ""/tf_keras/src/optimizers/optimizer.py"", line 1300, in apply_gradients  *
        return super().apply_gradients(grads_and_vars, name=name)
    File ""/tf_keras/src/optimizers/optimizer.py"", line 715, in apply_gradients  *
        self.build(trainable_variables)
    File ""/tf_keras/src/optimizers/rmsprop.py"", line 121, in build  *
        self._velocities.append(
    File ""/tf_keras/src/optimizers/optimizer.py"", line 1201, in add_variable_from_reference  *
        with strategy.extended.colocate_vars_with(model_variable):

    AttributeError: 'NoneType' object has no attribute 'extended'
```","['stat:awaiting tensorflower', 'type:bug', 'comp:tpus', 'TF 2.16']",2024-08-07T12:09:34Z,9,1,https://github.com/tensorflow/tensorflow/issues/73288,Runtime Error
127,tensorflow/tensorflow,TF strings do not work on the GPU as indices for  `tf.gather` / `tf.nn.embedding_lookup`,"### Issue type

Documentation Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

As the [`tf.gather`](https://www.tensorflow.org/api_docs/python/tf/gather) documentation suggests, the following code will indeed break on the CPU, but work on the GPU

![image](https://github.com/user-attachments/assets/4c3180fb-92ba-4d34-9518-8af5968f490e)

```py
indices= tf.constant([ 31., 117., 180., 255., 127.,  14.], dtype=tf.float32)

print(indices.shape)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

print(labels.shape)

tf.nn.embedding_lookup(indices, labels)
```

But this does not to seem to be the case when indices are of type `tf.string` instead:

```py
indices= tf.constant([""a"", ""b"", ""c"", ""d"", ""e"", ""f""], dtype=tf.string)

print(indices.shape)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

print(labels.shape)

tf.nn.embedding_lookup(indices, labels)
```

As the op indeed runs on the CPU (see log output)

A better approach is to indeed use a [`StaticHashTable`](https://www.tensorflow.org/api_docs/python/tf/lookup/StaticHashTable), and then lookup the attribute labels:

```py
text_values= tf.constant([""a"", ""b"", ""c"", ""d"", ""e"", ""f""], dtype=tf.string)
text_indices = tf.range(tf.size(text_values), dtype=tf.int32)

text_table = tf.lookup.StaticHashTable(
    initializer = tf.lookup.KeyValueTensorInitializer(
        keys=text_indices,
        values=text_values,
        key_dtype=tf.int32,
        value_dtype=tf.string
    ),
    default_value="""",
)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

text_table.lookup(labels)
```

But I think this should be documented somewhere in `tf.gather` / `tf.nn.embedding_lookup`


Here is a notebook that demonstrates this behaviour:

https://colab.research.google.com/drive/15Ig6Sw39lXRkZ40TvZhs4Aq_v96t-caL?usp=sharing


### Standalone code to reproduce the issue

```shell
indices= tf.constant([""a"", ""b"", ""c"", ""d"", ""e"", ""f""], dtype=tf.string)

print(indices.shape)

labels = tf.constant([[1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,
        -1, -1, -1, -1]], dtype=tf.int32)

print(labels.shape)

tf.nn.embedding_lookup(indices, labels)
```


### Relevant log output

```shell
(6,)
(1, 100)
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-9-e96bf33d6da0> in <cell line: 15>()
     13 print(labels.shape)
     14 
---> 15 tf.nn.embedding_lookup(indices, labels)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   5881 def raise_from_not_ok_status(e, name) -> NoReturn:
   5882   e.message += ("" name: "" + str(name if name is not None else """"))
-> 5883   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   5884 
   5885 

InvalidArgumentError: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,1] = -1 is not in [0, 6) [Op:GatherV2] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-07-31T09:35:26Z,1,0,https://github.com/tensorflow/tensorflow/issues/72873,Logical Bug
128,tensorflow/tensorflow,Sparse segment mean/sum gives random result on empty tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.12.1

### Custom code

No

### OS platform and distribution

Linux GPU

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When inputs to `math_ops.sparse_segment_mean` or `math_ops.sparse_segment_sum` are empty. The gradient of the OP is not 0, but some random values from previous tensors. The gradient should be zero when inputs are empty

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.python.client import timeline
from tensorflow.python.ops import math_ops

tf.compat.v1.disable_eager_execution()


def construct_model():
  embed_dim = 128
  cache_size = 2**18 + 1
  with tf.device(""/GPU:0""):
    params = tf.compat.v1.Variable(
        shape=(cache_size, embed_dim),
        dtype=tf.float32,
        initial_value=tf.ones(shape=(cache_size, embed_dim), dtype=tf.float32))

  ids = tf.constant([], dtype=tf.int32)
  segment_ids = tf.constant([], dtype=tf.int32)

  embed_pooling = math_ops.sparse_segment_sum(
      params,
      ids,
      segment_ids,
      num_segments=8192,
      name=""sss"",
  )

  #print(embed_pooling)
  loss = tf.reduce_sum(embed_pooling)
  trainable_vars = tf.compat.v1.trainable_variables()
  grads = tf.gradients(loss, trainable_vars)
  grads_and_vars = [(g, v) for g, v in zip(grads, trainable_vars)]

  sanity_check_ops = [
      #tf.print(grads[0]),
      tf.debugging.check_numerics(grads[0], message=""""),
      tf.debugging.assert_equal(tf.reduce_sum(grads[0]), tf.constant(0.)),
      #CRITICAL step to reproduce
      tf.random.uniform((81920, 1280), 100, 101)
  ]
  adam_opt = tf.compat.v1.train.AdamOptimizer(
      learning_rate=0.001, beta1=0.8, beta2=0.88)
  with tf.control_dependencies(sanity_check_ops):

    step = adam_opt.apply_gradients(grads_and_vars)

  return step


tf.config.threading.set_inter_op_parallelism_threads(32)
tf.config.threading.set_intra_op_parallelism_threads(32)
run_op = construct_model()

profile_options = tf.compat.v1.RunOptions(
    trace_level=tf.compat.v1.RunOptions.FULL_TRACE)
run_metadata = tf.compat.v1.RunMetadata()

with tf.compat.v1.Session() as sess:
  sess.run(tf.compat.v1.global_variables_initializer())
  print(sess._config)

  for i in range(10):
    sess.run(run_op, run_metadata=run_metadata)
```


### Relevant log output

```shell
2024-07-30 14:39:23.458752: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-07-30 14:39:23.505787: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-30 14:39:25.698665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79078 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:e1:00.0, compute capability: 8.0
2024-07-30 14:39:25.701973: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled
2024-07-30 14:39:27.165519: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: assertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]
	 [[{{function_node assert_equal_1_Assert_AssertGuard_false_41}}{{node Assert}}]]
device_count {
  key: ""CPU""
  value: 1
}
device_count {
  key: ""GPU""
  value: 1
}
intra_op_parallelism_threads: 32
inter_op_parallelism_threads: 32
gpu_options {
  visible_device_list: ""0""
  experimental {
  }
}
experimental {
}

Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1378, in _do_call
    return fn(*args)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1361, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1454, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]
	 [[{{function_node assert_equal_1_Assert_AssertGuard_false_41}}{{node Assert}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/tmp/t.py"", line 65, in <module>
    sess.run(run_op, run_metadata=run_metadata)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 968, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

assertion failed: [Condition x == y did not hold element-wise:] [x (Sum_1:0) = ] [16775995] [y (Const_4:0) = ] [0]
	 [[{{node Assert}}]]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.12']",2024-07-30T15:03:03Z,2,0,https://github.com/tensorflow/tensorflow/issues/72785,Logical Bug
129,tensorflow/tensorflow,tf.raw_ops.BlockLSTMV2: tensorflow/core/framework/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9) Aborted (core dumped) ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0.dev20240717

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

Python version: 3.10.14 

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a bug in TensorFlow when I used API `tf.raw_ops.BlockLSTMV2`  . The code is as follows:

```python
import tensorflow as tf

seq_len_max = tf.constant(5, shape=(), dtype=tf.int64)
x = tf.constant([[[8., 1.]],[[7., 6.]]], shape=(2, 1, 2), dtype=tf.float16)
cs_prev = tf.constant([[5., 3.]], shape=(1, 2), dtype=tf.float16)
h_prev = tf.constant([[3., 1.]], shape=(1, 2), dtype=tf.float16)
w = tf.constant([[  3.,  -1.,   7.,  -2.,  -8.,   2.,  -5.,   2.],
                 [ -3.,   2.,  -6.,  -4.,   0.,   0.,   2.,  -6.],
                 [-10.,   0.,   8.,   2.,  -1.,  -5.,   4.,  -7.],
                 [  1., -10.,  -6.,   2.,  -2., -10.,  -3.,   5.]], shape=(4, 8), dtype=tf.float16)

wci = tf.constant([ 7., -7.], shape=(2,), dtype=tf.float16)
wcf = tf.constant([-10., 7.], shape=(2,), dtype=tf.float16)
wco = tf.constant([-3., -5.], shape=(2,), dtype=tf.float16)
b = tf.constant([  1.,   6.,  -4.,  -2.,  -4.,   8., -10.,  -2.], shape=(8,), dtype=tf.float16)


tf.raw_ops.BlockLSTMV2(cell_clip=-10,use_peephole=True,seq_len_max=seq_len_max,x=x,cs_prev=cs_prev,h_prev=h_prev,w=w,wci=wci,wcf=wcf,wco=wco,b=b,)
```

The error message was as follows:

```shell
2024-07-23 20:47:52.963893: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.
2024-07-23 20:47:52.968139: F tensorflow/core/framework/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9)
Aborted (core dumped)
```

The above code would crash on `tf-nightly 2.18.0.dev20240717` (nightly-build). To reproduce the issue, I provided that a [colab notebook](https://colab.research.google.com/drive/1UxYr3Fg7uKd_cZA-ekKrM9WsfJK-ox_Z?usp=sharing) to reproduce the error.

 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

seq_len_max = tf.constant(5, shape=(), dtype=tf.int64)
x = tf.constant([[[8., 1.]],[[7., 6.]]], shape=(2, 1, 2), dtype=tf.float16)
cs_prev = tf.constant([[5., 3.]], shape=(1, 2), dtype=tf.float16)
h_prev = tf.constant([[3., 1.]], shape=(1, 2), dtype=tf.float16)
w = tf.constant([[  3.,  -1.,   7.,  -2.,  -8.,   2.,  -5.,   2.],
                 [ -3.,   2.,  -6.,  -4.,   0.,   0.,   2.,  -6.],
                 [-10.,   0.,   8.,   2.,  -1.,  -5.,   4.,  -7.],
                 [  1., -10.,  -6.,   2.,  -2., -10.,  -3.,   5.]], shape=(4, 8), dtype=tf.float16)

wci = tf.constant([ 7., -7.], shape=(2,), dtype=tf.float16)
wcf = tf.constant([-10., 7.], shape=(2,), dtype=tf.float16)
wco = tf.constant([-3., -5.], shape=(2,), dtype=tf.float16)
b = tf.constant([  1.,   6.,  -4.,  -2.,  -4.,   8., -10.,  -2.], shape=(8,), dtype=tf.float16)


tf.raw_ops.BlockLSTMV2(cell_clip=-10,use_peephole=True,seq_len_max=seq_len_max,x=x,cs_prev=cs_prev,h_prev=h_prev,w=w,wci=wci,wcf=wcf,wco=wco,b=b,)
```


### Relevant log output

```shell
2024-07-23 20:47:52.963893: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.
2024-07-23 20:47:52.968139: F tensorflow/core/framework/tensor.cc:1075] Check failed: 0 <= start (0 vs. -9)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-07-23T12:46:06Z,1,0,https://github.com/tensorflow/tensorflow/issues/72362,Runtime Error
130,tensorflow/tensorflow,tf.strided_slice new_axis_mask inconsistency,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf-nightly, 2.17.0, 2.16.2, 2.16.1

### Custom code

No

### OS platform and distribution

macOS, Linux

### Mobile device

_No response_

### Python version

3.10.12, 3.12.4

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm seeing that when `spec_size` (as described in the docs as the length of the `begin`, `end`, `strides` arrays) is less than the rank of the `input` tensor, the behavior of `strided_slice` differs when `new_axis_mask` for the bits between `len(begin)` and `tf.rank(input)` is specified.

Is this the expected behavior, and if so, is there anything else like this that differs when `spec_size != tf.rank(input)`? 

I'm noting that it's currently permitted for `spec_size > tf.rank(input)`, which yields the same result in this case as when `spec_size == tf.rank(input)`.

### Standalone code to reproduce the issue

```shell
Notebook:
https://colab.research.google.com/drive/1-LCENzCjorhzyDCqq5qfB4IFoxLbHjGI?usp=sharing
Code:

import numpy as np
import tensorflow as tf

@tf.function
def test(t, begin, end, mask):
    return tf.strided_slice(t, begin, end, new_axis_mask=mask)

t = tf.constant(np.arange(0,27).reshape((3,3,3)))
mask = 0b111

# Noting as comments the shape of the result

# shape = (1, 3, 3, 3)
print(test(t, [0], [3], mask))
# shape = (1, 1, 3, 3, 3)
print(test(t, [0, 0], [3, 3], mask))
# shape = (1, 1, 1, 3, 3, 3)
print(test(t, [0, 0, 0], [3, 3, 3], mask))
# shape = (1, 1, 1, 3, 3, 3)
print(test(t, [0, 0, 0, 0], [3, 3, 3, 3], mask))
```
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-07-23T00:44:47Z,1,0,https://github.com/tensorflow/tensorflow/issues/72332,Logical Bug
131,tensorflow/tensorflow,tf.raw_ops.MapUnstage: Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.18.0.dev20240717

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

Python version: 3.10.14 

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I encountered a bug in TensorFlow when I used API `tf.raw_ops.MapUnstage`  with randomly generated tensors. The code is as follows:

```python
import tensorflow as tf

key = tf.random.uniform([15, 8], minval=-10, maxval=10, dtype=tf.int64)
indices = tf.random.uniform([1], minval=-10, maxval=10, dtype=tf.int32)
tf.raw_ops.MapUnstage(capacity=100,memory_limit=100,dtypes=[tf.float64],
                      container="""",shared_name="""",key=key,indices=indices)
```

The error message was as follows:

```shell
2024-07-22 22:05:52.420257: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor
Aborted (core dumped)
```

I have confirmed that above code would crash on `tf-nightly 2.18.0.dev20240717` (nightly-build). Also, I provided that a [colab notebook](https://colab.research.google.com/drive/1PXsrbcckN5x_ooMjZr5Vds6KVKMve7ph?usp=sharing) to reproduce the error.



### Standalone code to reproduce the issue

```shell
import tensorflow as tf

key = tf.random.uniform([15, 8], minval=-10, maxval=10, dtype=tf.int64)
indices = tf.random.uniform([1], minval=-10, maxval=10, dtype=tf.int32)
tf.raw_ops.MapUnstage(capacity=100,memory_limit=100,dtypes=[tf.float64],
                      container="""",shared_name="""",key=key,indices=indices)
```


### Relevant log output

```shell
2024-07-22 22:05:52.420257: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 120)Must have a one element tensor
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-07-22T14:08:43Z,1,0,https://github.com/tensorflow/tensorflow/issues/72295,Runtime Error
132,tensorflow/tensorflow,TF timeline timestamp was shifted. The resultant timeline cannot be shown correctly in chrome tracing viewer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

TF2.12

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12, cuDNN 8

### GPU model and memory

_No response_

### Current behavior?

When profiling on GPU. Timestamp in Chrome trace format was incorrect due to shifting. The relevant commit is https://github.com/tensorflow/tensorflow/commit/701c1e97317ace357621b7f0bd4a2e427f16ed42#r144411330

There was a issue on  https://github.com/tensorflow/profiler/issues/238, but does not fix.

**Incorrect** timeline if `CollectData` was called:
https://github.com/tensorflow/tensorflow/blob/aeeeef0ba125dd2b28b59c5d144dd0a237a780c4/tensorflow/core/profiler/lib/device_profiler_session.h#L56-L59

**Correct** timelie if `CollectDataInternal` was called:
https://github.com/tensorflow/tensorflow/blob/a3e2c692c18649329c4210cf8df2487d2028e267/tensorflow/core/profiler/lib/device_profiler_session.h#L56-L59

The shifting was called in `PostProcessSingleHostXSpace` in `CollectData`
https://github.com/tensorflow/tensorflow/blob/aeeeef0ba125dd2b28b59c5d144dd0a237a780c4/third_party/xla/third_party/tsl/tsl/profiler/lib/profiler_session.cc#L81-L88
Is shifting expected behavior? If that, how to use the profiler session correctly?

### Standalone code to reproduce the issue

```shell
from absl import app
import logging

import tensorflow as tf
from tensorflow.python.client import timeline

tf.compat.v1.disable_eager_execution()

def test_profiler():
  shape = tf.constant([1000, 1000], dtype=tf.int64)
  x = tf.random.normal(shape)
  y = x ** 2
  z = y ** 2
  with tf.compat.v1.Session() as sess:
    run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)
    run_metadata = tf.compat.v1.RunMetadata()
    #for i in range(3):
    sess.run(z, options=run_options, run_metadata=run_metadata)
    #logging.info(""run_metadata.step_stats:%s"", run_metadata.step_stats)
    tl = timeline.Timeline(run_metadata.step_stats)
    ctf = tl.generate_chrome_trace_format()
    with open('timeline.json', 'w') as f:
      f.write(ctf)

def main(argv):
  test_profiler()


if __name__ == ""__main__"":
  app.run(main)
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:core', 'TF 2.12']",2024-07-19T06:52:16Z,1,0,https://github.com/tensorflow/tensorflow/issues/72156,Runtime Error
133,tensorflow/tensorflow,Build Error on aarch64 AWS Graviton3 with Ubuntu 22.04 for TensorFlow v2.17.0 with mkl_aarch64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.11.9

### Bazel version

6.5.0

### GCC/compiler version

clang version 17.0.6

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm encountering build errors when trying to build TensorFlow v2.17.0 from source on an AWS Graviton3 instance (aarch64 architecture) running Ubuntu 22.04. The build fails with errors related to the **MakeOneDnnStream** function.

Expected behavior: The build should complete successfully without any errors.

### Standalone code to reproduce the issue

```shell
1. Set up an AWS Graviton3 instance with Ubuntu 22.04.
2. Clone the TensorFlow repository and checkout version 2.17.0:
   
   git clone https://github.com/tensorflow/tensorflow.git
   cd tensorflow
   git checkout v2.17.0
   
3. Install Bazel and other dependencies as per the TensorFlow build documentation.
4. Run the build command:
   ```bash
   bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --features=-layering_check --config=mkl_aarch64 --config=opt --copt=-march=armv8-a+sve --copt=-msve-vector-bits=256 --copt=-O3 --copt=-Wno-gnu-offsetof-extensions --copt=-Wno-unused-but-set-variable --jobs=48 --local_cpu_resources=26 --verbose_failures
   ```

Any guidance or fixes to resolve this build error would be greatly appreciated.
```


### Relevant log output

```shell
(tf-venv) user@ip-xxx-xx-x-xxx:~/work_dirr/tensorflow$ taskset -c 6-31 bazel build //tensorflow/tools/pip_package:wheel --repo_env=WHEEL_NAME=tensorflow_cpu --features=-layering_check --config=mkl_aarch64 --config=opt --copt=-march=armv8-a+sve --copt=-msve-vector-bits=256 --copt=-O3 --copt=-Wno-gnu-offsetof-extensions --copt=-Wno-unused-but-set-variable  --jobs=48 --local_cpu_resources=26 --verbose_failures
INFO: Reading 'startup' options from /home/deepesh/work_dirr/tensorflow/.bazelrc: --windows_enable_symlinks
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=113
INFO: Reading rc options for 'build' from /home/deepesh/work_dirr/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/deepesh/work_dirr/tensorflow/.bazelrc:
  'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --features=-force_no_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --incompatible_enforce_config_setting_visibility
INFO: Reading rc options for 'build' from /home/deepesh/work_dirr/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/deepesh/work_dirr/tf-venv/bin/python3 --action_env PYTHON_LIB_PATH=/home/deepesh/work_dirr/tf-venv/lib/python3.11/site-packages --python_path=/home/deepesh/work_dirr/tf-venv/bin/python3 --action_env CLANG_COMPILER_PATH=/usr/lib/llvm-17/bin/clang --repo_env=CC=/usr/lib/llvm-17/bin/clang --repo_env=BAZEL_COMPILER=/usr/lib/llvm-17/bin/clang --copt=-Wno-gnu-offsetof-extensions
INFO: Found applicable config definition build:short_logs in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:mkl_aarch64 in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --define=build_with_mkl_aarch64=true --define=build_with_openmp=true --define=build_with_acl=true -c opt
INFO: Found applicable config definition build:opt in file /home/deepesh/work_dirr/tensorflow/.tf_configure.bazelrc: --copt=-Wno-sign-compare --host_copt=-Wno-sign-compare
INFO: Found applicable config definition build:linux in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --copt=-Wswitch --copt=-Werror=switch --copt=-Wno-error=unused-but-set-variable --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --experimental_guard_against_concurrent_changes
INFO: Found applicable config definition build:dynamic_kernels in file /home/deepesh/work_dirr/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
INFO: Analyzed target //tensorflow/tools/pip_package:wheel (1 packages loaded, 3758 targets configured).
INFO: Found 1 target...
ERROR: /home/deepesh/.cache/bazel/_bazel_deepesh/c2d183634e6ef66bd4ea91e213a12542/external/local_xla/xla/service/cpu/BUILD:1665:11: Compiling xla/service/cpu/onednn_matmul.cc failed: (Exit 1): clang failed: error executing command (from target @local_xla//xla/service/cpu:onednn_matmul) 
  (cd /home/deepesh/.cache/bazel/_bazel_deepesh/c2d183634e6ef66bd4ea91e213a12542/execroot/org_tensorflow && \
  exec env - \
    CLANG_COMPILER_PATH=/usr/lib/llvm-17/bin/clang \
    PATH=/home/deepesh/.cache/bazelisk/downloads/bazelbuild/bazel-6.5.0-linux-arm64/bin:/home/deepesh/work_dirr/tf-venv/bin:/home/deepesh/.vscode-server/cli/servers/Stable-f1e16e1e6214d7c44d078b1f0607b2388f29d729/server/bin/remote-cli:/home/deepesh/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/deepesh/work_dirr/tf-venv/bin/python3 \
    PYTHON_LIB_PATH=/home/deepesh/work_dirr/tf-venv/lib/python3.11/site-packages \
    TF2_BEHAVIOR=1 \
  /usr/lib/llvm-17/bin/clang -U_FORTIFY_SOURCE -fstack-protector -Wall -Wthread-safety -Wself-assign -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++14' -MD -MF bazel-out/aarch64-opt/bin/external/local_xla/xla/service/cpu/_objs/onednn_matmul/onednn_matmul.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/local_xla/xla/service/cpu/_objs/onednn_matmul/onednn_matmul.pic.o' -fPIC '-DEIGEN_MAX_ALIGN_BYTES=64' -DEIGEN_ALLOW_UNALIGNED_SCALARS '-DEIGEN_USE_AVX512_GEMM_KERNELS=0' -DHAVE_SYS_UIO_H -DTF_USE_SNAPPY '-DLLVM_ON_UNIX=1' '-DHAVE_BACKTRACE=1' '-DBACKTRACE_HEADER=<execinfo.h>' '-DLTDL_SHLIB_EXT="".so""' '-DLLVM_PLUGIN_EXT="".so""' '-DLLVM_ENABLE_THREADS=1' '-DHAVE_DEREGISTER_FRAME=1' '-DHAVE_LIBPTHREAD=1' '-DHAVE_PTHREAD_GETNAME_NP=1' '-DHAVE_PTHREAD_H=1' '-DHAVE_PTHREAD_SETNAME_NP=1' '-DHAVE_REGISTER_FRAME=1' '-DHAVE_SETENV_R=1' '-DHAVE_STRERROR_R=1' '-DHAVE_SYSEXITS_H=1' '-DHAVE_UNISTD_H=1' -D_GNU_SOURCE '-DHAVE_LINK_H=1' '-DHAVE_MALLINFO=1' '-DHAVE_SBRK=1' '-DHAVE_STRUCT_STAT_ST_MTIM_TV_NSEC=1' -DHAVE_BUILTIN_THREAD_POINTER '-DLLVM_NATIVE_ARCH=""AArch64""' '-DLLVM_NATIVE_ASMPARSER=LLVMInitializeAArch64AsmParser' '-DLLVM_NATIVE_ASMPRINTER=LLVMInitializeAArch64AsmPrinter' '-DLLVM_NATIVE_DISASSEMBLER=LLVMInitializeAArch64Disassembler' '-DLLVM_NATIVE_TARGET=LLVMInitializeAArch64Target' '-DLLVM_NATIVE_TARGETINFO=LLVMInitializeAArch64TargetInfo' '-DLLVM_NATIVE_TARGETMC=LLVMInitializeAArch64TargetMC' '-DLLVM_NATIVE_TARGETMCA=LLVMInitializeAArch64TargetMCA' '-DLLVM_HOST_TRIPLE=""aarch64-unknown-linux-gnu""' '-DLLVM_DEFAULT_TARGET_TRIPLE=""aarch64-unknown-linux-gnu""' '-DLLVM_VERSION_MAJOR=19' '-DLLVM_VERSION_MINOR=0' '-DLLVM_VERSION_PATCH=0' '-DLLVM_VERSION_STRING=""19.0.0git""' -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS -D__STDC_FORMAT_MACROS '-DBLAKE3_USE_NEON=0' -DBLAKE3_NO_AVX2 -DBLAKE3_NO_AVX512 -DBLAKE3_NO_SSE2 -DBLAKE3_NO_SSE41 -DENABLE_NEON -DARM_COMPUTE_CPU_ENABLED -DARM_COMPUTE_ENABLE_NEON -DARM_COMPUTE_ENABLE_I8MM -DENABLE_FP32_KERNELS -DENABLE_QASYMM8_KERNELS -DENABLE_QASYMM8_SIGNED_KERNELS -DENABLE_QSYMM16_KERNELS -DENABLE_INTEGER_KERNELS -DENABLE_NHWC_KERNELS -DENABLE_NCHW_KERNELS -DARM_COMPUTE_GRAPH_ENABLED -DARM_COMPUTE_ENABLE_SVEF32MM -DARM_COMPUTE_ENABLE_FIXED_FORMAT_KERNELS -D_GLIBCXX_USE_NANOSLEEP -DARM_COMPUTE_OPENMP_SCHEDULER '-DDNNL_AARCH64_USE_ACL=1' '-DBAZEL_CURRENT_REPOSITORY=""local_xla""' -iquote external/local_xla -iquote bazel-out/aarch64-opt/bin/external/local_xla -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt/bin/external/com_google_protobuf -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/local_tsl -iquote bazel-out/aarch64-opt/bin/external/local_tsl -iquote external/ml_dtypes -iquote bazel-out/aarch64-opt/bin/external/ml_dtypes -iquote external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/aarch64-opt/bin/external/snappy -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/aarch64-opt/bin/external/farmhash_archive -iquote external/llvm-project -iquote bazel-out/aarch64-opt/bin/external/llvm-project -iquote external/zlib -iquote bazel-out/aarch64-opt/bin/external/zlib -iquote external/mkl_dnn_acl_compatible -iquote bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible -iquote external/compute_library -iquote bazel-out/aarch64-opt/bin/external/compute_library -Ibazel-out/aarch64-opt/bin/external/ml_dtypes/_virtual_includes/float8 -Ibazel-out/aarch64-opt/bin/external/ml_dtypes/_virtual_includes/intn -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/ArithCanonicalizationIncGen -Ibazel-out/aarch64-opt/bin/external/llvm-project/mlir/_virtual_includes/AsmParserTokenKinds -Ibazel-out/aarch64-opt/bin/external/compute_library/include/_virtual_includes/include -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/eigen_archive/mkl_include -isystem bazel-out/aarch64-opt/bin/external/eigen_archive/mkl_include -isystem external/ml_dtypes -isystem bazel-out/aarch64-opt/bin/external/ml_dtypes -isystem external/ml_dtypes/ml_dtypes -isystem bazel-out/aarch64-opt/bin/external/ml_dtypes/ml_dtypes -isystem external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/farmhash_archive/src -isystem bazel-out/aarch64-opt/bin/external/farmhash_archive/src -isystem external/llvm-project/llvm/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/llvm/include -isystem external/zlib -isystem bazel-out/aarch64-opt/bin/external/zlib -isystem external/llvm-project/mlir/include -isystem bazel-out/aarch64-opt/bin/external/llvm-project/mlir/include -isystem external/mkl_dnn_acl_compatible/include -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/include -isystem external/mkl_dnn_acl_compatible/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src -isystem external/mkl_dnn_acl_compatible/src/common -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/common -isystem external/mkl_dnn_acl_compatible/src/cpu -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu -isystem external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/src -isystem external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/xbyak_aarch64 -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/aarch64/xbyak_aarch64/xbyak_aarch64 -isystem external/mkl_dnn_acl_compatible/src/cpu/gemm -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn_acl_compatible/src/cpu/gemm -isystem external/compute_library/arm_compute/runtime -isystem bazel-out/aarch64-opt/bin/external/compute_library/arm_compute/runtime -isystem external/compute_library/src/core/NEON/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/assembly -isystem external/compute_library/src/core/NEON/kernels/convolution/common -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/convolution/common -isystem external/compute_library/src/core/NEON/kernels/convolution/winograd -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/convolution/winograd -isystem external/compute_library/src/core/cpu/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/cpu/kernels/assembly -isystem external/compute_library/src/cpu/kernels/assembly -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/cpu/kernels/assembly -isystem external/compute_library/src/core/NEON/kernels/arm_conv -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/arm_conv -isystem external/compute_library/src/core/NEON/kernels/arm_gemm -isystem bazel-out/aarch64-opt/bin/external/compute_library/src/core/NEON/kernels/arm_gemm -Wno-all -Wno-extra -Wno-deprecated -Wno-deprecated-declarations -Wno-ignored-attributes -Wno-array-bounds -Wunused-result '-Werror=unused-result' -Wswitch '-Werror=switch' '-Wno-error=unused-but-set-variable' -DAUTOLOAD_DYNAMIC_KERNELS -Wno-gnu-offsetof-extensions -Wno-sign-compare '-march=armv8-a+sve' '-msve-vector-bits=256' -O3 -Wno-gnu-offsetof-extensions -Wno-unused-but-set-variable '-std=c++17' -DEIGEN_AVOID_STL_ARRAY -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DTENSORFLOW_USE_XLA=1' -DINTEL_MKL -DENABLE_ONEDNN_V3 '-DDNNL_AARCH64_USE_ACL=1' -DENABLE_ONEDNN_OPENMP '-DXLA_CPU_USE_ACL=1' -fexceptions -pthread -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/local_xla/xla/service/cpu/onednn_matmul.cc -o bazel-out/aarch64-opt/bin/external/local_xla/xla/service/cpu/_objs/onednn_matmul/onednn_matmul.pic.o)
# Configuration: 389168362472d4b0f209a68c4f568adffc1a33297c68acb48d71fb4bd10724d2
# Execution platform: @local_execution_config_platform//:platform
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:199:9: warning: 'LOG' macro redefined [-Wmacro-redefined]
  199 | #define LOG(severity) ABSL_LOG_INTERNAL_LOG_IMPL(_##severity)
      |         ^
external/local_tsl/tsl/platform/default/logging.h:165:9: note: previous definition is here
  165 | #define LOG(severity) _TF_LOG_##severity
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:237:9: warning: 'LOG_EVERY_N' macro redefined [-Wmacro-redefined]
  237 | #define LOG_EVERY_N(severity, n) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:278:9: note: previous definition is here
  278 | #define LOG_EVERY_N(severity, n)                       \
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:245:9: warning: 'LOG_FIRST_N' macro redefined [-Wmacro-redefined]
  245 | #define LOG_FIRST_N(severity, n) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:284:9: note: previous definition is here
  284 | #define LOG_FIRST_N(severity, n)                       \
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:253:9: warning: 'LOG_EVERY_POW_2' macro redefined [-Wmacro-redefined]
  253 | #define LOG_EVERY_POW_2(severity) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:290:9: note: previous definition is here
  290 | #define LOG_EVERY_POW_2(severity)                         \
      |         ^
In file included from external/local_xla/xla/service/cpu/onednn_matmul.cc:31:
In file included from external/local_xla/xla/hlo/ir/hlo_instructions.h:37:
In file included from external/local_xla/xla/hlo/ir/hlo_computation.h:31:
external/com_google_absl/absl/log/log.h:265:9: warning: 'LOG_EVERY_N_SEC' macro redefined [-Wmacro-redefined]
  265 | #define LOG_EVERY_N_SEC(severity, n_seconds) \
      |         ^
external/local_tsl/tsl/platform/default/logging.h:300:9: note: previous definition is here
  300 | #define LOG_EVERY_N_SEC(severity, n_seconds)                      \
      |         ^
external/local_xla/xla/service/cpu/onednn_matmul.cc:270:24: error: no matching function for call to 'MakeOneDnnStream'
  270 |   auto onednn_stream = MakeOneDnnStream(cpu_engine, thread_pool.get());
      |                        ^~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/onednn_util.h:57:14: note: candidate function not viable: no known conversion from 'pointer' (aka 'tsl::OneDnnThreadPool *') to 'dnnl::threadpool_interop::threadpool_iface *' for 2nd argument
   57 | dnnl::stream MakeOneDnnStream(
      |              ^
   58 |     const dnnl::engine& cpu_engine,
   59 |     dnnl::threadpool_interop::threadpool_iface* thread_pool);
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/onednn_matmul.cc:359:24: error: no matching function for call to 'MakeOneDnnStream'
  359 |   auto onednn_stream = MakeOneDnnStream(cpu_engine, thread_pool.get());
      |                        ^~~~~~~~~~~~~~~~
external/local_xla/xla/service/cpu/onednn_util.h:57:14: note: candidate function not viable: no known conversion from 'pointer' (aka 'tsl::OneDnnThreadPool *') to 'dnnl::threadpool_interop::threadpool_iface *' for 2nd argument
   57 | dnnl::stream MakeOneDnnStream(
      |              ^
   58 |     const dnnl::engine& cpu_engine,
   59 |     dnnl::threadpool_interop::threadpool_iface* thread_pool);
      |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
5 warnings and 2 errors generated.
Target //tensorflow/tools/pip_package:wheel failed to build
INFO: Elapsed time: 720.177s, Critical Path: 142.22s
INFO: 3322 processes: 82 internal, 3240 local.
FAILED: Build did NOT complete successfully
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'comp:mkl', 'subtype: ubuntu/linux', '2.17']",2024-07-18T09:02:02Z,2,0,https://github.com/tensorflow/tensorflow/issues/72081,Runtime Error
134,tensorflow/tensorflow,`tf.data.Dataset.prefetch()` error with basic usage,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The most basic usage of `tf.data.Dataset.prefetch()` raises an error. 

The `buffer_size` argument is documented as requiring a int64 tensor:

> `buffer_size` 	
> A [tf.int64](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the maximum number of elements that will be buffered when prefetching. If the value [tf.data.AUTOTUNE](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE) is used, then the buffer size is dynamically tuned.

https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch

Probably related to https://github.com/tensorflow/tensorflow/issues/71744, The ""eager fallback"" codepath is broken.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

x = np.arange(5)
y = np.arange(5)

ds = tf.data.Dataset.from_tensor_slices((x, y)).batch(2)

ds.prefetch(tf.constant(1, dtype = 'int64'))
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1259, in prefetch
    return prefetch_op._prefetch(  # pylint: disable=protected-access
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/prefetch_op.py"", line 28, in _prefetch
    return _PrefetchDataset(input_dataset, buffer_size, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/prefetch_op.py"", line 46, in __init__
    variant_tensor = gen_dataset_ops.prefetch_dataset(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 6001, in prefetch_dataset
    return prefetch_dataset_eager_fallback(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 6072, in prefetch_dataset_eager_fallback
    legacy_autotune = _execute.make_bool(legacy_autotune, ""legacy_autotune"")
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/eager/execute.py"", line 172, in make_bool
    raise TypeError(""Expected bool for argument '%s' not %s."" %
TypeError: Expected bool for argument 'legacy_autotune' not <tf.Tensor: shape=(), dtype=bool, numpy=False>.
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'regression issue', '2.17']",2024-07-16T15:02:16Z,1,0,https://github.com/tensorflow/tensorflow/issues/71937,Runtime Error
135,tensorflow/tensorflow,`tf.data.Dataset.from_tensor_slices` allocates GPU RAM,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v2.17.0

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.4 LTS

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Passing a numpy array to `tf.data.Dataset.from_tensor_slices()` attempts to allocate the dataset as a tensor on the GPU device, and raises an exception if there is not enough GPU RAM available. 

This only started with TF 2.17.0. In all previous TF versions, all `tf.data.Dataset` operations were always pinned to the CPU.

To reproduce, create a numpy array larger than can fit on the GPU, and attempt to create a `tf.data.Dataset` from it.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

gpu_ram_gb = 12 # adjust for size of GPU 

gb = gpu_ram_gb+1; dtype = ""float64""
size = (gb * 1024**3) // tf.dtypes.as_dtype(dtype).size

x = np.zeros((size,), dtype = dtype)

tf.data.Dataset.from_tensor_slices(x)
```


### Relevant log output

```shell
2024-07-12 08:20:42.771788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-07-12 08:20:42.784893: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-07-12 08:20:42.788889: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-07-12 08:20:42.798174: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-07-12 08:20:43.492876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1720786850.494498   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.527964   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.531351   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.535502   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.538786   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.541878   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.710210   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.711607   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
I0000 00:00:1720786850.712908   45046 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-07-12 08:20:50.714170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 34 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:03:00.0, compute capability: 7.5
2024-07-12 08:20:50.715606: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 13958643712 exceeds 10% of free system memory.
2024-07-12 08:21:05.997032: W external/local_tsl/tsl/framework/bfc_allocator.cc:482] Allocator (GPU_0_bfc) ran out of memory trying to allocate 13.00GiB (rounded to 13958643712)requested by op _EagerConst
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2024-07-12 08:21:05.997054: I external/local_tsl/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc
2024-07-12 08:21:05.997064: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (256): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997072: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (512): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997079: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1024): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997087: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2048): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997093: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4096): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997100: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8192): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997107: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16384): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997114: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (32768): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997121: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (65536): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997128: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (131072): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997135: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (262144): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997142: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (524288): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997148: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997155: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997162: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997169: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997176: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997183: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997190: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997197: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997203: I external/local_tsl/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2024-07-12 08:21:05.997212: I external/local_tsl/tsl/framework/bfc_allocator.cc:1062] Bin for 13.00GiB was 256.00MiB, Chunk State: 
2024-07-12 08:21:05.997219: I external/local_tsl/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: 
2024-07-12 08:21:05.997225: I external/local_tsl/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 0B
2024-07-12 08:21:05.997232: I external/local_tsl/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 0 memory_limit_: 35651584 available bytes: 35651584 curr_region_allocation_bytes_: 35651584
2024-07-12 08:21:05.997241: I external/local_tsl/tsl/framework/bfc_allocator.cc:1114] Stats: 
Limit:                        35651584
InUse:                               0
MaxInUse:                            0
NumAllocs:                           0
MaxAllocSize:                        0
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2024-07-12 08:21:05.997248: W external/local_tsl/tsl/framework/bfc_allocator.cc:494] <allocator contains no memory>

Traceback (most recent call last):
  File ""/home/tomasz/github/rstudio/keras3/test.py"", line 16, in <module>
    tf.data.Dataset.from_tensor_slices(x)
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 826, in from_tensor_slices
    return from_tensor_slices_op._from_tensor_slices(tensors, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py"", line 25, in _from_tensor_slices
    return _TensorSliceDataset(tensors, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py"", line 33, in __init__
    element = structure.normalize_element(element)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/data/util/structure.py"", line 134, in normalize_element
    ops.convert_to_tensor(t, name=""component_%d"" % i, dtype=dtype))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/ops.py"", line 713, in convert_to_tensor
    return tensor_conversion_registry.convert(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 234, in convert
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_tensor_conversion.py"", line 29, in _constant_tensor_conversion_function
    return constant_op.constant(v, dtype=dtype, name=name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py"", line 142, in wrapper
    return op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 276, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 289, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 301, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/tomasz/.virtualenvs/r-keras/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py"", line 108, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:data', '2.17']",2024-07-12T12:24:35Z,4,0,https://github.com/tensorflow/tensorflow/issues/71744,Runtime Error
136,tensorflow/tensorflow,Advisory GHSA-84mw-34w6-2q43 contains wrong POC codes,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

N/A

### Custom code

No

### OS platform and distribution

N/A

### Mobile device

N/A

### Python version

N/A

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

It seems that description of Gtihub advisory [GHSA-84mw-34w6-2q43](https://github.com/advisories/GHSA-84mw-34w6-2q43) contains POC codes from another advisory [GHSA-772p-x54p-hjrv](https://github.com/advisories/GHSA-772p-x54p-hjrv). This causes issues in understanding the vulnerability.

### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:core']",2024-07-02T07:45:33Z,1,0,https://github.com/tensorflow/tensorflow/issues/70724,UI/UX Bug
137,tensorflow/tensorflow,tf.matmul gives inconsistent results for same data,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.8.0

### Custom code

Yes

### OS platform and distribution

windows 11

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

cuda 11.8

### GPU model and memory

_No response_

### Current behavior?

I want to do matmul(A,transpose(A)) and matmul(B,transpose(B)). Here, B is just matrix A with additional rows. A is 8 by 512 and b is 64 by 512 with first 8 rows exactly as that of A.

let’s call
A_out = A matmul transpose(A), this is 8 by 8,
B_out = B matmul transpose(B), this is 64 by 64.

So the above matmul just dots each row of a matrix with all the other rows of the same matrix. So if the first 8 rows in B are same as A then the top-left, 8 by 8 sub matrix of B_out should exactly be same as A_out.

tf.matmul operation doesnt produce this. however numpy does.

Please look into this

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras.layers import Embedding

# Set up
embedding = Embedding(36711,512,mask_zero=True)
example_sequence = tf.constant([36710,  5095,   466, 16678,     5,     3,  5152, 36711] + [0]*(64-8))

pad_embed    = example_sequence # input with padding       # has shape (64,)
no_pad_embed = example_sequence[:8] # input without padding # has shape (8,)

no_pad_embed = embedding(no_pad_embed) # shape (8,512)  # A
pad_embed    = embedding(pad_embed)    # shape (64,512) # B : has same first 8 rows as A


# Real problem: I want to use matmul to do A @ A.T and B @ B.T
pad_out    = tf.matmul(pad_embed,pad_embed, transpose_b=True)
no_pad_out = tf.matmul(no_pad_embed,no_pad_embed, transpose_b=True)
#print(pad_out[:8,:8])
#print(no_pad_out)

print(tf.reduce_all(pad_out[:8,:8]==no_pad_out)) # False, meaning that the upper left 8 by 8 submatrix of pad_out is not equal to no_pad_out

import numpy as np

# Real problem: I want to use matmul to do A @ A.T and B @ B.T
pad_embed   = pad_embed.numpy()
no_pad_embed= no_pad_embed.numpy()

pad_out    = np.matmul(pad_embed,pad_embed.T)
no_pad_out = np.matmul(no_pad_embed,no_pad_embed.T)
#print(pad_out[:8,:8])
#print(no_pad_out)

print(tf.reduce_all(pad_out[:8,:8]==no_pad_out)) # True, meaning 8 by 8 submatrix of pad_out equals no_pad_out
```


### Relevant log output

```shell
tf.Tensor(False, shape=(), dtype=bool)
tf.Tensor(True, shape=(), dtype=bool)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.8']",2024-07-01T06:09:26Z,1,0,https://github.com/tensorflow/tensorflow/issues/70672,Logical Bug
138,tensorflow/tensorflow,Kubernetes cluster resolver fails when running from within a K8S cluster.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

linux

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If trying to create a cluster spec from a pod running within a K8s cluster, [this](https://github.com/tensorflow/tensorflow/blob/v2.16.1/tensorflow/python/distribute/cluster_resolver/kubernetes_cluster_resolver.py#L90) try block fails because it can't find the kubectl config file.

The quick is rather straightforward:

```python

if override_client is None:
    try:
      from kubernetes import config as k8sconfig  # pylint: disable=g-import-not-at-top

      k8sconfig.load_kube_config()
    except ImportError:
      if not override_client:
        raise ImportError('The Kubernetes Python client must be installed '
                          'before using the Kubernetes Cluster Resolver. '
                          'To install the Kubernetes Python client, run '
                          '`pip install kubernetes` on your command line.')

...


```

Happy to open a MR for this.

### Standalone code to reproduce the issue

main.py
```python
import os
import tensorflow as tf

from absl import logging
from kubernetes import client, config

logging.set_verbosity(logging.DEBUG)
logging.info(""TF version: %s"", tf.__version__)

config.load_incluster_config()
k8s_cli = client.CoreV1Api()

# Fails here despite providing an override client for talking with the k8s APIs.
cluster_resolver = tf.distribute.cluster_resolver.KubernetesClusterResolver(
    {""worker"": [""job-name=mobileye-0"", ""job-name=mobileye-1""]}, override_client=k8s_cli
)
task_index = int(os.environ.get(""TASK_INDEX""))
cluster_resolver.task_type = ""worker""
cluster_resolver.task_id = task_index

logging.info(""Cluster spec: %s"", cluster_resolver.cluster_spec().as_dict())
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
    cluster_resolver=cluster_resolver
)
```

job.yaml
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: mobileye-0
spec:
  template:
    metadata:
      name: mobileye-training
    spec:
      containers:
      - name: tensorflow
        image: europe-west4-docker.pkg.dev/msteiner-kubeflow/mobileye-test/test-tf-image:latest 
        resources:
          limits:
            cpu: ""1""
            memory: 3Gi
        env:
          - name: TASK_INDEX
            value: ""0""
      restartPolicy: Never
  parallelism: 1
---
apiVersion: batch/v1
kind: Job
metadata:
  name: mobileye-1
spec:
  template:
    metadata:
      name: mobileye-training
    spec:
      containers:
      - name: tensorflow
        image: europe-west4-docker.pkg.dev/msteiner-kubeflow/mobileye-test/test-tf-image:latest 
        resources:
          limits:
            cpu: ""1""
            memory: 3Gi
        env:
          - name: TASK_INDEX
            value: ""1""
      restartPolicy: Never
  parallelism: 1
```
```


### Relevant log output

```shell
2024-06-28 13:23:10.980 CEST
2024-06-28 11:23:10.979832: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-28 13:23:10.991 CEST
2024-06-28 11:23:10.991166: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-28 13:23:11.109 CEST
2024-06-28 11:23:11.108950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
2024-06-28 13:23:11.109 CEST
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-28 13:23:14.057 CEST
2024-06-28 11:23:14.056964: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-06-28 13:23:18.311 CEST
INFO:absl:TF version: 2.16.1
2024-06-28 13:23:18.312 CEST
INFO:absl:PATH: /usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
2024-06-28 13:23:18.312 CEST
INFO:absl:HOSTNAME: mobileye-0-v7lkr
2024-06-28 13:23:18.312 CEST
INFO:absl:LANG: C.UTF-8
2024-06-28 13:23:18.312 CEST
INFO:absl:GPG_KEY: A035C8C19219BA821ECEA86B64E628F8D684696D
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_VERSION: 3.11.8
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_PIP_VERSION: 24.0
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_SETUPTOOLS_VERSION: 65.5.1
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_GET_PIP_URL: https://github.com/pypa/get-pip/raw/dbf0c85f76fb6e1ab42aa672ffca6f0a675d9ee4/public/get-pip.py
2024-06-28 13:23:18.312 CEST
INFO:absl:PYTHON_GET_PIP_SHA256: dfe9fd5c28dc98b5ac17979a953ea550cec37ae1b47a5116007395bfacff2ab9
2024-06-28 13:23:18.312 CEST
INFO:absl:TASK_INDEX: 0
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_SERVICE_PORT: 443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_SERVICE_PORT_HTTPS: 443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT: tcp://34.118.224.1:443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP: tcp://34.118.224.1:443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP_PROTO: tcp
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP_PORT: 443
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_PORT_443_TCP_ADDR: 34.118.224.1
2024-06-28 13:23:18.313 CEST
INFO:absl:KUBERNETES_SERVICE_HOST: 34.118.224.1
2024-06-28 13:23:18.313 CEST
INFO:absl:HOME: /root
2024-06-28 13:23:18.313 CEST
INFO:absl:TF2_BEHAVIOR: 1
2024-06-28 13:23:18.313 CEST
INFO:absl:TPU_ML_PLATFORM: Tensorflow
2024-06-28 13:23:18.315 CEST
Traceback (most recent call last):   File ""//src/main.py"", line 20, in <module>     cluster_resolver = tf.distribute.cluster_resolver.KubernetesClusterResolver(
2024-06-28 13:23:18.316 CEST
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-06-28 13:23:18.316 CEST
  File ""/usr/local/lib/python3.11/site-packages/tensorflow/python/distribute/cluster_resolver/kubernetes_cluster_resolver.py"", line 93, in __init__
2024-06-28 13:23:18.317 CEST
    k8sconfig.load_kube_config()
2024-06-28 13:23:18.317 CEST
  File ""/usr/local/lib/python3.11/site-packages/kubernetes/config/kube_config.py"", line 819, in load_kube_config
2024-06-28 13:23:18.318 CEST
    loader = _get_kube_config_loader(
2024-06-28 13:23:18.318 CEST
             ^^^^^^^^^^^^^^^^^^^^^^^^
2024-06-28 13:23:18.318 CEST
  File ""/usr/local/lib/python3.11/site-packages/kubernetes/config/kube_config.py"", line 776, in _get_kube_config_loader
2024-06-28 13:23:18.319 CEST
    raise ConfigException(
2024-06-28 13:23:18.319 CEST
kubernetes.config.config_exception.ConfigException: Invalid kube-config file. No configuration found.
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:dist-strat', 'TF 2.16']",2024-06-28T11:41:32Z,0,0,https://github.com/tensorflow/tensorflow/issues/70581,Runtime Error
139,tensorflow/tensorflow,bucketize -function wrong results on GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.18.0-dev20240624

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Bucketize function returns wrong results when executed on GPU.   

Tested with  2.9.3, 2.15.0 and 2.18.0-dev20240624 and observed same incorrect behavior.

See the example below.   The CPU and GPU results are different so both can't be correct. GPU result seems to be wrong.

Can reproduce in [colab](https://colab.research.google.com/drive/1hSHgt5f31eUG-OsAy0FJ0zlcBhMgPV-c?authuser=0#scrollTo=nCDoKVOFw3ML)





### Standalone code to reproduce the issue

```shell
from tensorflow.python.ops.gen_math_ops import bucketize
import tensorflow as tf

print(tf.__version__)
gpus = tf.config.list_physical_devices('GPU')
assert gpus

x = [0,1,2,3]
boundaries = [0.1, 1.1]
with tf.device(""/CPU:0""):
    print(bucketize(x, boundaries=boundaries))
with tf.device(""/GPU:0""):
    print(bucketize(x, boundaries=boundaries))
```


### Relevant log output

```shell
2.18.0-dev20240624
tf.Tensor([0 1 2 2], shape=(4,), dtype=int32)
tf.Tensor([1 2 2 2], shape=(4,), dtype=int32)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.17']",2024-06-25T10:15:06Z,5,1,https://github.com/tensorflow/tensorflow/issues/70343,Logical Bug
140,tensorflow/tensorflow,`SIGSEGV ` (Address boundary error) in `tf.io.gfile` with `TF_USE_MODULAR_FILESYSTEM=1`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.16.1, 2.17.0rc0, 2.18.0.dev20240617

### Custom code

No

### OS platform and distribution

macOS 14.5

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A `SIGSEGV` (Address boundary error) fault can be caused by trying to call tensorflow gfile API when a `TF_USE_MODULAR_FILESYSTEM=1` is set. This can be reproduced in version 2.16.1 and 2.17.0rc0 and the latest nightly.

### Standalone code to reproduce the issue

```shell
TF_USE_MODULAR_FILESYSTEM=1 python -c 'import tensorflow as tf;tf.io.gfile.exists(""gs://tfds-data/dataset_info/mnist/3.0.1/dataset_info.json"")'
```


### Relevant log output

```shell
Job 1, 'TF_USE_MODULAR_FILESYSTEM=1 pyt…' terminated by signal SIGSEGV (Address boundary error)
```
","['stat:awaiting tensorflower', 'type:bug', 'TF 2.16']",2024-06-20T12:52:32Z,2,0,https://github.com/tensorflow/tensorflow/issues/70101,Runtime Error
141,tensorflow/tensorflow,Segmentation fault in `tf.raw_ops.CollectiveAllToAllV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, tf.raw_ops.CollectiveGatherV2 encounters ""Segmentation fault (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

test = tf.Variable([1.0, 2.0, 3.0])

input_dict = {
    'input': tf.constant([], shape=(1, 1, 0), dtype=tf.float64),
    'group_size': tf.constant(1, dtype=tf.int32),
    'group_key': tf.constant(30631, dtype=tf.int32), 
    'instance_key': tf.constant(2, dtype=tf.int32), 
    'ordering_token': [test.handle], 
}


tf.raw_ops.CollectiveAllToAllV2(
    input=input_dict['input'],
    group_size=input_dict['group_size'],
    group_key=input_dict['group_key'],
    instance_key=input_dict['instance_key'],
    ordering_token=input_dict['ordering_token'],
    communication_hint='auto',
    timeout_seconds=0,
    is_stateless=False,
    name=None
)
```


### Relevant log output

```shell
2024-06-13 14:37:50.256604: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-13T14:38:21Z,1,0,https://github.com/tensorflow/tensorflow/issues/69700,Runtime Error
142,tensorflow/tensorflow,Segmentation fault in `tf.raw_ops.CollectiveGatherV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, tf.raw_ops.CollectiveGatherV2 encounters ""Segmentation fault (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

test = tf.Variable([1.0, 2.0, 3.0])

input_dict = {
    'input': tf.constant([], shape=(1, 1, 0), dtype=tf.float64),
    'group_size': tf.constant(1, dtype=tf.int32),
    'group_key': tf.constant(30631, dtype=tf.int32), 
    'instance_key': tf.constant(2, dtype=tf.int32), 
    'ordering_token': [test.handle], 
}


result = tf.raw_ops.CollectiveGatherV2(
    input=input_dict['input'],
    group_size=input_dict['group_size'],
    group_key=input_dict['group_key'],
    instance_key=input_dict['instance_key'],
    ordering_token=input_dict['ordering_token'],
    communication_hint='auto',
    timeout_seconds=0,
    is_stateless=False
)
```


### Relevant log output

```shell
2024-06-13 14:32:37.690567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-13T14:35:21Z,1,0,https://github.com/tensorflow/tensorflow/issues/69699,Runtime Error
143,tensorflow/tensorflow,"""invalid static_cast"" on AVX512FP16 (e.g. Sapphire Rapids)","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.1

### Custom code

No

### OS platform and distribution

RHEL 8

### Mobile device

_No response_

### Python version

3.10

### Bazel version

6.1.0

### GCC/compiler version

GCC 13.1

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Failing build with

```
external/eigen_archive/Eigen/src/Core/MathFunctions.h:429:12: error: invalid ‘static_cast’ from type ‘const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>’ to type ‘__vector(16) float’
429 | return static_cast(x);
```

when compiling for CPUs supporting AVX512 FP16 extensions, e.g. Sapphire Rapids

This is the same issue as #62047 reported by @OH-AU which was closes by @mihaimaruseac because an unsupported Python version was used which is however unrelated to this failure.

After some analysis it turns out to be an issue in Eigen: https://gitlab.com/libeigen/eigen/-/issues/2829 which might be fixed by https://gitlab.com/libeigen/eigen/-/merge_requests/1639

So TF can either use that MR as a patch or update Eigen once the MR is merged

### Standalone code to reproduce the issue

```shell
TF_PYTHON_VERSION=3.10 CFLAGS=""-O3 -march=native -fPIC"" CXXFLAGS=$CFLAGS LIBRARY_PATH=$LD_RUN_PATH LD_LIBRARY_PATH=$LD_RUN_PATH \
LDFLAGS=""-fPIC  -Wl,--disable-new-dtags -Wl,--rpath -Wl,${LD_RUN_PATH}"" bazel build -j 24 --config=opt -c opt  --copt=-march=native \
--config=mkl --config=tensorrt  //tensorflow/tools/pip_package:build_pip_package --repo_env=TF_PYTHON_VERSION=3.10
```


### Relevant log output

```shell
In file included from external/eigen_archive/Eigen/Core:182,
                 from tensorflow/core/kernels/linalg/matrix_inverse_op.cc:22:
external/eigen_archive/Eigen/src/Core/MathFunctions.h: In instantiation of 'static NewType Eigen::internal::cast_impl<OldType, NewType, EnableIf>::run(const OldType&) [with OldType = Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>; NewType = __vector(16) float; EnableIf = void]':
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h:268:48:   required from 'static TgtPacket Eigen::internal::pcast_generic<SrcPacket, TgtPacket, false, false>::run(const SrcPacket&) [with SrcPacket = Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>; TgtPacket = __vector(16) float]'
external/eigen_archive/Eigen/src/Core/GenericPacketMath.h:289:50:   required from 'TgtPacket Eigen::internal::pcast(const SrcPacket&) [with SrcPacket = eigen_packet_wrapper<__vector(4) long long int, 1>; TgtPacket = __vector(16) float]'
external/eigen_archive/Eigen/src/Core/CoreEvaluators.h:789:52:   required from 'DstPacketType Eigen::internal::unary_evaluator<Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<SrcType, DstType>, ArgType>, Eigen::internal::IndexBased>::packet(Eigen::Index) const [with int LoadMode = 0; DstPacketType = __vector(16) float; typename std::enable_if<Eigen::internal::find_packet_by_size<SrcType, Eigen::internal::unpacket_traits<DstPacketType>::size>::value, bool>::type <anonymous> = true; SrcType = Eigen::half; DstType = float; ArgType = const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >; typename Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<SrcType, DstType>, ArgType>::Scalar = float; Eigen::Index = long int]'
external/eigen_archive/Eigen/src/Core/AssignEvaluator.h:706:110:   required from 'void Eigen::internal::generic_dense_assignment_kernel<DstEvaluatorTypeT, SrcEvaluatorTypeT, Functor, Version>::assignPacket(Eigen::Index) [with int StoreMode = 64; int LoadMode = 0; Packet = __vector(16) float; DstEvaluatorTypeT = Eigen::internal::evaluator<Eigen::Matrix<float, -1, -1, 1, -1, -1> >; SrcEvaluatorTypeT = Eigen::internal::evaluator<Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > > >; Functor = Eigen::internal::assign_op<float, float>; int Version = 0; Eigen::Index = long int]'
external/eigen_archive/Eigen/src/Core/AssignEvaluator.h:462:75:   required from 'static constexpr void Eigen::internal::dense_assignment_loop<Kernel, 3, 0>::run(Kernel&) [with Kernel = Eigen::internal::generic_dense_assignment_kernel<Eigen::internal::evaluator<Eigen::Matrix<float, -1, -1, 1, -1, -1> >, Eigen::internal::evaluator<Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > > >, Eigen::internal::assign_op<float, float>, 0>]'
external/eigen_archive/Eigen/src/Core/AssignEvaluator.h:810:37:   [ skipping 3 instantiation contexts, use -ftemplate-backtrace-limit=0 to disable ]
external/eigen_archive/Eigen/src/Core/AssignEvaluator.h:883:27:   required from 'void Eigen::internal::call_assignment(Dst&, const Src&, const Func&, std::enable_if_t<(! evaluator_assume_aliasing<Src>::value), void*>) [with Dst = Eigen::Matrix<float, -1, -1, 1, -1, -1>; Src = Eigen::CwiseUnaryOp<core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Func = assign_op<float, float>; std::enable_if_t<(! evaluator_assume_aliasing<Src>::value), void*> = void*; typename evaluator_traits<SrcXprType>::Shape = Eigen::DenseShape]'
external/eigen_archive/Eigen/src/Core/AssignEvaluator.h:861:18:   required from 'void Eigen::internal::call_assignment(Dst&, const Src&) [with Dst = Eigen::Matrix<float, -1, -1, 1, -1, -1>; Src = Eigen::CwiseUnaryOp<core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >]'
external/eigen_archive/Eigen/src/Core/PlainObjectBase.h:771:32:   required from 'Derived& Eigen::PlainObjectBase<Derived>::_set(const Eigen::DenseBase<OtherDerived>&) [with OtherDerived = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Derived = Eigen::Matrix<float, -1, -1, 1, -1, -1>]'
external/eigen_archive/Eigen/src/Core/Matrix.h:227:24:   required from 'Eigen::Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>& Eigen::Matrix<Scalar_, Rows_, Cols_, Options_, MaxRows_, MaxCols_>::operator=(const Eigen::DenseBase<OtherDerived>&) [with OtherDerived = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; Scalar_ = float; int Rows_ = -1; int Cols_ = -1; int Options_ = 1; int MaxRows_ = -1; int MaxCols_ = -1]'
external/eigen_archive/Eigen/src/LU/PartialPivLU.h:135:12:   required from 'Eigen::PartialPivLU<MatrixType, PermutationIndex>& Eigen::PartialPivLU<MatrixType, PermutationIndex>::compute(const Eigen::EigenBase<OtherDerived>&) [with InputType = Eigen::CwiseUnaryOp<Eigen::internal::core_cast_op<Eigen::half, float>, const Eigen::Map<const Eigen::Matrix<Eigen::half, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >; MatrixType_ = Eigen::Matrix<float, -1, -1, 1, -1, -1>; PermutationIndex_ = int]'
tensorflow/core/kernels/linalg/matrix_inverse_op.cc:113:31:   required from here
external/eigen_archive/Eigen/src/Core/MathFunctions.h:429:12: error: invalid 'static_cast' from type 'const Eigen::internal::eigen_packet_wrapper<__vector(4) long long int, 1>' to type '__vector(16) float'
  429 |     return static_cast<NewType>(x);
      |            ^~~~~~~~~~~~~~~~~~~~~~~
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'comp:core', 'TF 2.15']",2024-06-13T07:23:50Z,7,1,https://github.com/tensorflow/tensorflow/issues/69674,Runtime Error
144,tensorflow/tensorflow,Aborted (core dumped) in `tf.experimental.numpy.diag/tf.compat.v1.linalg.diag/tf.experimental.numpy.diagflat/tf.keras.ops.diag`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, tf.experimental.numpy.diag/tf.compat.v1.linalg.diag/tf.experimental.numpy.diagflat/tf.keras.ops.diag encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_dict = {
    'diagonal': tf.constant([1,2], dtype=tf.int32),
    'k': tf.constant([1, 2], dtype=tf.int32)
}

# crash
tf.experimental.numpy.diag(
    v=tf.constant([1,2], dtype=tf.int32), k=tf.constant([1, 2], dtype=tf.int32)
)

# crash
# tf.compat.v1.linalg.diag(
#     diagonal=tf.constant([1,2], dtype=tf.int32),
#     name='diag',
#     k= tf.constant([1, 2], dtype=tf.int32),
#     num_rows=-1,
#     num_cols=-1,
#     padding_value=0,
#     align='RIGHT_LEFT'
# )

# crash
# tf.experimental.numpy.diagflat(
#     v=tf.constant([1,2], dtype=tf.int32), k=tf.constant([1, 2], dtype=tf.int32)
# )

# crash
# tf.keras.ops.diag(
#     x=tf.constant([1,2], dtype=tf.int32), k=tf.constant([1, 2], dtype=tf.int32)
# )
```


### Relevant log output

```shell
2024-06-10 13:23:09.021306: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-10 13:23:10.029041: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-10T13:24:51Z,1,0,https://github.com/tensorflow/tensorflow/issues/69471,Runtime Error
145,tensorflow/tensorflow,Crash in `tf.raw_ops.SparseCountSparseOutput `,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TensorFlow Nightly

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.SparseCountSparseOutput will output ""The session crashed because it took up all available RAM."" 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

binary_output = True
indices = tf.constant(0, shape=[3456,2], dtype=tf.int64)
values = tf.constant(536870912, shape=[3456], dtype=tf.int32)
dense_shape = tf.constant([125099989676412,125099989676412], shape=[2], dtype=tf.int64)
weights = tf.constant(51, shape=[3456], dtype=tf.int32)


tf.raw_ops.SparseCountSparseOutput(
   indices=indices, values=values, dense_shape=dense_shape, weights=weights, binary_output=binary_output,
    minlength=0,
    maxlength=0,
    name=None
)
```


### Relevant log output

```shell
Jun 10, 2024, 11:31:57 AM	WARNING	WARNING:root:kernel 7c5d2f95-9ab7-4b7c-99c0-d8949477c9f1 restarted
Jun 10, 2024, 11:31:57 AM	INFO	KernelRestarter: restarting kernel (1/5), keep random ports
Jun 10, 2024, 11:31:29 AM	WARNING	2024-06-10 03:31:29.145870: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Jun 10, 2024, 11:31:26 AM	WARNING	To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Jun 10, 2024, 11:31:26 AM	WARNING	2024-06-10 03:31:26.481147: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
Jun 10, 2024, 11:31:26 AM	WARNING	2024-06-10 03:31:26.469832: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
Jun 10, 2024, 11:31:26 AM	WARNING	2024-06-10 03:31:26.467591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
Jun 10, 2024, 11:31:26 AM	WARNING	2024-06-10 03:31:26.467524: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
Jun 10, 2024, 11:31:18 AM	INFO	Kernel started: 7c5d2f95-9ab7-4b7c-99c0-d8949477c9f1, name: python3
Jun 10, 2024, 11:30:19 AM	INFO	Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-10T03:32:42Z,1,0,https://github.com/tensorflow/tensorflow/issues/69455,Runtime Error
146,tensorflow/tensorflow,GPU MaxPool gradient ops do not yet have a deterministic XLA implementation,"### Issue type

Feature Request

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9.19

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.4/8.9.7.29

### GPU model and memory

NVIDIA GeForce RTX 3090

### Current behavior?

When TF deterministic was set, runtime exception was thrown at MaxPooling2D().

### Standalone code to reproduce the issue

```shell
When TF deterministic was set, runtime exception was thrown at MaxPooling2D().
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/ws/miniconda3/envs/tf216/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3526, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-3dda39ff370e>"", line 1, in <module>
    runfile('/mnt/projects/Projects/Test_Classification/train_model.py', wdir='/mnt/projects/Projects/Test_Classification')
  File ""/opt/pycharm-community-2024.1/plugins/python-ce/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/opt/pycharm-community-2024.1/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/mnt/projects/Projects/Test_Classification/train_model.py"", line 956, in <module>
    history = model.fit(x_train, y_train,
  File ""/home/ws/miniconda3/envs/tf216/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py"", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/ws/miniconda3/envs/tf216/lib/python3.9/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:
Detected at node gradient_tape/functional_1_1/max_pooling2d_4_1/MaxPool2d/MaxPoolGrad defined at (most recent call last):
<stack traces unavailable>
GPU MaxPool gradient ops do not yet have a deterministic XLA implementation.
	 [[{{node gradient_tape/functional_1_1/max_pooling2d_4_1/MaxPool2d/MaxPoolGrad}}]]
	tf2xla conversion failed while converting __inference_one_step_on_data_13588[]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions.
	 [[StatefulPartitionedCall]] [Op:__inference_one_step_on_iterator_14045]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-08T06:18:41Z,22,1,https://github.com/tensorflow/tensorflow/issues/69417,Runtime Error
147,tensorflow/tensorflow,Crash in `tf.raw_ops.ResizeNearestNeighbor/ResizeNearestNeighborGrad/ResizeArea/ResizeBicubic/ResizeBilinear`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TensorFlow Nightly

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, these APIs will output ""The session crashed because it took up all available RAM."" The affected APIs are listed below:

1. tf.raw_ops.ResizeBilinear
2. tf.raw_ops.ResizeBicubic
3. tf.raw_ops.ResizeArea
4. tf.raw_ops.ResizeNearestNeighbor
5. tf.raw_ops.ResizeNearestNeighborGrad

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1a6B_lYMfENTbO4ifQMLYe8Hq6CHcc6hP?usp=sharing
```


### Relevant log output

```shell
Timestamp	Level	Message
Jun 6, 2024, 6:57:43 PM	WARNING	WARNING:root:kernel d0444564-3fb8-4d42-ac83-1338e6842d5a restarted
Jun 6, 2024, 6:57:43 PM	INFO	KernelRestarter: restarting kernel (1/5), keep random ports
Jun 6, 2024, 6:57:25 PM	WARNING	2024-06-06 10:57:25.421101: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 51536461872 exceeds 10% of free system memory.
Jun 6, 2024, 6:57:21 PM	WARNING	2024-06-06 10:57:21.507228: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Jun 6, 2024, 6:57:17 PM	WARNING	To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Jun 6, 2024, 6:57:17 PM	WARNING	2024-06-06 10:57:17.995922: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
Jun 6, 2024, 6:56:13 PM	WARNING	WARNING:root:kernel d0444564-3fb8-4d42-ac83-1338e6842d5a restarted
Jun 6, 2024, 6:56:13 PM	INFO	KernelRestarter: restarting kernel (1/5), keep random ports
Jun 6, 2024, 6:55:56 PM	WARNING	2024-06-06 10:55:56.764126: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 103072923744 exceeds 10% of free system memory.
Jun 6, 2024, 6:55:52 PM	WARNING	2024-06-06 10:55:52.277507: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-06T14:30:10Z,3,0,https://github.com/tensorflow/tensorflow/issues/69322,Runtime Error
148,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.ResourceApplyCenteredRMSProp/tf.raw_ops.ResourceSparseApplyCenteredRMSProp`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceApplyCenteredRMSProp/tf.raw_ops.ResourceSparseApplyCenteredRMSProp triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceSparseApplyCenteredRMSProp(var=var.handle, mg=accum.handle, ms=accum2.handle, mom=var.handle,
                                              lr=lr, rho=lr,
                                              momentum=momentum,
                                              epsilon=0.9, grad=tf.constant([0.1, 0.2, 0.3]),
                                              indices=tf.constant([1,2,2]),
                                              use_locking=False)
# crash
# tf.raw_ops.ResourceApplyCenteredRMSProp(var=var.handle, mg=accum.handle, ms=accum2.handle, mom=var.handle,
#                                               lr=lr, rho=lr,
#                                               momentum=momentum,
#                                               epsilon=0.9, grad=tf.constant([0.1, 0.2, 0.3]),
#                                               use_locking=False)
```


### Relevant log output

```shell
2024-06-06 01:23:02.820585: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-06 01:23:02.853830: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 01:23:03.911976: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-06T01:24:46Z,1,0,https://github.com/tensorflow/tensorflow/issues/69286,Runtime Error
149,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.ResourceApplyAdagrad/tf.raw_ops.ResourceApplyAdagradDA/tf.raw_ops.ResourceApplyAdagradV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceApplyAdagrad/tf.raw_ops.ResourceApplyAdagradDA/tf.raw_ops.ResourceApplyAdagradV2 triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceApplyAdagrad(
    var=var.handle,
    accum=accum.handle,
    lr=lr,
    grad=grad,
    use_locking=False,
)

# crash
# tf.raw_ops.ResourceApplyAdagradDA(var=var.handle,
#               gradient_accumulator=accum.handle,
#               gradient_squared_accumulator=accum2.handle,
#               grad=grad,  lr=lr,
#               l1=lr, l2=lr, global_step=1000,
#               use_locking=False)

# crash
# tf.raw_ops.ResourceApplyAdagradV2(var=var.handle,
#                 accum=accum.handle,
#                 epsilon = 0.9,
#                 grad=grad, lr=lr,
#                 use_locking=False,update_slots=False)
```


### Relevant log output

```shell
2024-06-06 01:18:44.797518: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-06 01:18:44.829812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 01:18:45.875807: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-06T01:19:37Z,1,0,https://github.com/tensorflow/tensorflow/issues/69285,Runtime Error
150,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.ResourceSparseApplyAdagrad/tf.raw_ops.ResourceSparseApplyAdagradDA/tf.raw_ops.ResourceSparseApplyAdagradV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceSparseApplyAdagrad/tf.raw_ops.ResourceSparseApplyAdagradDA/tf.raw_ops.ResourceSparseApplyAdagradV2 triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceSparseApplyAdagrad(
    var=var.handle,
    accum=accum.handle,
    lr=lr,
    grad=grad,
    indices=tf.constant([1,2,3]),
    use_locking=False,
)


# crash only when gpu is available 
# tf.raw_ops.ResourceSparseApplyAdagradDA(var=var.handle,
#                     gradient_accumulator=accum.handle,
#                     gradient_squared_accumulator=accum2.handle,
#                     grad=grad, indices=tf.constant([1,2,3]), lr=lr,
#                     l1=lr, l2=lr, global_step=1000,
#                     use_locking=False)

# crash
# tf.raw_ops.ResourceSparseApplyAdagradV2(var=var.handle,
#                     accum=accum.handle,
#                     epsilon = 0.9,
#                     grad=grad, indices=tf.constant([1,2,3]), lr=lr,
#                     use_locking=False,update_slots=False)
```


### Relevant log output

```shell
2024-06-06 01:14:57.455749: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-06 01:14:57.489757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 01:14:58.552846: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-06T01:15:35Z,1,0,https://github.com/tensorflow/tensorflow/issues/69284,Runtime Error
151,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.ResourceSparseApplyAdadelta/tf.raw_ops.ResourceApplyAdadelta`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceSparseApplyAdadelta/tf.raw_ops.ResourceApplyAdadelta triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceSparseApplyAdadelta(
    var=var.handle,
    accum=accum.handle,
    accum_update=accum2.handle,
    lr=lr,
    rho=0.9,
    epsilon=0.9,
    grad=grad,
    indices=tf.constant([1,2,2]),
    use_locking=False,
)
# crash
# tf.raw_ops.ResourceApplyAdadelta(
#     var=var.handle,
#     accum=accum.handle,
#     accum_update=accum2.handle,
#     lr=lr,
#     rho=0.9,
#     epsilon=0.9,
#     grad=grad,
#     use_locking=False,
# )
```


### Relevant log output

```shell
2024-06-06 01:10:09.399119: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-06-06 01:10:09.432483: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 01:10:10.501032: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-06T01:10:53Z,1,0,https://github.com/tensorflow/tensorflow/issues/69283,Runtime Error
152,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.ResourceApplyRMSProp/tf.raw_ops.ResourceSparseApplyRMSProp`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceApplyRMSProp/tf.raw_ops.ResourceSparseApplyRMSProp triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. The cause of the crash may be the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceApplyRMSProp(
    var=var.handle,
    ms=accum.handle,
    mom=accum2.handle,
    lr=lr,
    rho=lr,
    momentum=momentum,
    epsilon=0.9,
    grad=grad,
    use_locking=False,
)

# tf.raw_ops.ResourceSparseApplyRMSProp(
#     var=var.handle,
#     ms=accum.handle,
#     mom=accum2.handle,
#     lr=lr,
#     rho=lr,
#     momentum=momentum,
#     indices=tf.constant([1,2,2]),
#     epsilon=0.9,
#     grad=grad,
#     use_locking=False,
# )
```


### Relevant log output

```shell
2024-06-06 01:07:00.757682: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 01:07:01.835541: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-06T01:07:46Z,1,0,https://github.com/tensorflow/tensorflow/issues/69281,Runtime Error
153,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.ResourceApplyKerasMomentum/tf.raw_ops.ResourceSparseApplyKerasMomentum`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceApplyKerasMomentum/tf.raw_ops.ResourceSparseApplyKerasMomentum triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. We analyzed it, and the cause of this crash is probably the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceApplyKerasMomentum(
    var=var.handle,
    accum=accum.handle,
    grad=grad,
    lr=lr,
    momentum=0.1,
    use_locking=False,
    use_nesterov=False,
    name=None
)

# crash
# tf.raw_ops.ResourceSparseApplyKerasMomentum(
#     var=var.handle,
#     accum=accum.handle,
#     grad=grad,
#     lr=lr,
#     indices=tf.constant([1,2,2]),
#     momentum=0.1,
#     use_locking=False,
#     use_nesterov=False,
#     name=None
# )
```


### Relevant log output

```shell
2024-06-06 00:59:43.994008: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 00:59:44.992853: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-06T01:01:58Z,1,0,https://github.com/tensorflow/tensorflow/issues/69279,Runtime Error
154,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.ResourceApplyFtrl/tf.raw_ops.ResourceApplyFtrlV2/tf.raw_ops.ResourceSparseApplyFtrl/tf.raw_ops.ResourceSparseApplyFtrlV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On specific input, tf.raw_ops.ResourceApplyFtrl/tf.raw_ops.ResourceApplyFtrlV2/tf.raw_ops.ResourceSparseApplyFtrl/tf.raw_ops.ResourceSparseApplyFtrlV2 triggers ""Aborted (core dumped)"". We ran the code on colab's latest TensorFlow Nightly, which also triggers the crash. We analyzed it, and the cause of this crash is probably the grad parameter.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
accum2 = tf.Variable([0, 1.2, -3.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

# crash
tf.raw_ops.ResourceApplyFtrl(
    var=var.handle,
    accum=accum.handle,
    linear=var.handle,
    grad=grad,
    lr=lr,
    l1=0.1,
    l2=0.1,
    lr_power=1,
    use_locking=False,
    multiply_linear_by_lr=False,
    name=None
)
# crash
# tf.raw_ops.ResourceApplyFtrlV2(
#     var=var.handle,
#     accum=accum.handle,
#     linear=var.handle,
#     grad=grad,
#     lr=lr,
#     l1=0.1,
#     l2=0.1,
#     lr_power=1,
#     l2_shrinkage=1,
#     use_locking=False,
#     multiply_linear_by_lr=False,
#     name=None
# )
# crash
# tf.raw_ops.ResourceSparseApplyFtrl(
#    var=var.handle,
#     accum=accum.handle,
#     linear=var.handle,
#     grad=grad,
#     lr=lr,
#     l1=0.1,
#     l2=0.1,
#     lr_power=-1,
#     use_locking=False,
#     indices=tf.constant([1,2,2]),
#     multiply_linear_by_lr=False,
#     name=None
# )
# crash
# tf.raw_ops.ResourceSparseApplyFtrlV2(
#     var=var.handle,
#     accum=accum.handle,
#     linear=var.handle,
#     grad=grad,
#     lr=lr,
#     l1=0.1,
#     l2=0.1,
#     lr_power=-1,
#     use_locking=False,
#     indices=tf.constant([1,2,2]),
#     multiply_linear_by_lr=False,
#     l2_shrinkage=1,
#     name=None
# )
```


### Relevant log output

```shell
2024-06-06 00:53:09.467458: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-06 00:53:10.440397: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-06T00:53:19Z,1,0,https://github.com/tensorflow/tensorflow/issues/69278,Runtime Error
155,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.QuantizeAndDequantizeV3`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A crash is triggered when an unintended value is passed to the input parameter of the tf.raw_ops.QuantizeAndDequantizeV3 function.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = tf.random.uniform(shape=[2, 1], maxval=9.0)
input_min = tf.constant([0, 1.0], dtype=tf.dtypes.float32)
input_max = tf.constant([1.0], dtype=tf.dtypes.float32)
# crash
tf.raw_ops.QuantizeAndDequantizeV3(input=input_data, input_min=input_min, input_max=input_max, num_bits=8, signed_input=True, range_given=True, narrow_range=False, axis=(- 1))
```


### Relevant log output

```shell
2024-06-05 10:39:35.285635: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-05 10:39:36.361982: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-05T10:42:09Z,1,0,https://github.com/tensorflow/tensorflow/issues/69220,Runtime Error
156,tensorflow/tensorflow,Aborted (core dumped) in `tf.transpose`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.9.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

A crash is triggered when some boundary values are passed to the perm parameter of these transpose functions. the affected APIs are as follows:

1. tf.transpose
2. tf.experimental.numpy.transpose
3. tf.compat.v1.transpose
4. tf.keras.ops.transpose
5. tf.raw_ops.transpose
6. tf.raw_ops.ConjugateTranspose

tf.raw_ops.ConjugateTranspose is a Segmentation fault crash, all other API crashes are Aborted. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

x = tf.random.normal(shape=(3, 3))
# crash
x = tf.transpose(x, perm=((- 1), (- 2))) 
 
# crash 
# tf.experimental.numpy.transpose(     
#     x, axes=((- 1), (- 2))
# )

# crash 
# tf.compat.v1.transpose(  
#     x, perm=((- 1), (- 2)), name='transpose', conjugate=False
# )

# crash
# tf.keras.ops.transpose(
#     x, axes=((- 1), (- 2))
# )

# crash
# tf.raw_ops.ConjugateTranspose(
#     x=x, perm=((- 1), (- 2))
# )

# crash
# tf.raw_ops.Transpose(
#     x=x, perm=((- 1), (- 2))
# )
```


### Relevant log output

```shell
2024-06-05 09:16:55.482645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-05 09:16:56.526139: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)

2024-06-05 09:17:47.967585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-06-05T09:19:56Z,1,0,https://github.com/tensorflow/tensorflow/issues/69213,Runtime Error
157,tensorflow/tensorflow,segmentation fault when tf.histogram_fixed_width receives large `value_range` and `nbins` on CPU mode ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When setting the `nbins` to a large value and the value range to [-np.inf, np.inf], the API `tf.histogram_fixed_width` will raises a segmentation fault.
Interestingly, I found this issue only occurs on CPU backend while this API works fine on GPU.

### Standalone code to reproduce the issue

```shell
import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''
import numpy as np
import tensorflow as tf
input = tf.constant([1.,2,3])
bins = 23
out = tf.histogram_fixed_width(input, [-np.inf, np.inf], nbins=bins)  # Segmentation fault (core dumped)
print(out)
```
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-29T19:33:25Z,1,0,https://github.com/tensorflow/tensorflow/issues/68836,Runtime Error
158,tensorflow/tensorflow,Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence when call some methods of `tf.data`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3 LTS (x86_64)

### Mobile device

_No response_

### Python version

3.12.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When calling and iterating over the results of some of tf.data's methods it outputs ""Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def dataset_test():
 
    dataset = tf.data.Dataset.from_tensor_slices([""apple"", ""banana"", ""cherry""])
    # dataset = tf.data.TextLineDataset([""file1.txt"", ""file2.txt""]).range(10)             # same output
    # dataset = tf.data.TFRecordDataset([""file1.tfrecords"", ""file2.tfrecords""]).range(10) # same output
    # dataset = tf.data.Dataset.from_tensors([1,2,3]).range(10)                           # same output

    print(list(dataset.as_numpy_iterator()))

if __name__ == ""__main__"":
    dataset_test()
```


### Relevant log output

```shell
2024-05-24 06:23:06.268182: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-24 06:23:06.309884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-24 06:23:06.908161: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-24 06:23:07.558225: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22137 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:18:00.0, compute capability: 8.6
2024-05-24 06:23:07.558795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22453 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:af:00.0, compute capability: 8.6
2024-05-24 06:23:07.917142: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.16']",2024-05-24T11:24:11Z,21,0,https://github.com/tensorflow/tensorflow/issues/68593,Runtime Error
159,tensorflow/tensorflow,TFLite ConvTranspose3D implemented typo,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

in tflite convtranspose3d optimized kernel, pad depth seems typo error
![image](https://github.com/tensorflow/tensorflow/assets/30307463/39bdf859-f236-48f9-b02a-c8231d11cb88)

And also compared with same convtransposed3d with torch interface, tflite have mismatch

### Standalone code to reproduce the issue

```shell
in tflite convtranspose3d optimized kernel, pad depth seems typo error
![image](https://github.com/tensorflow/tensorflow/assets/30307463/39bdf859-f236-48f9-b02a-c8231d11cb88)

here, seems 
const int spatial_dim_1_padding_after =
      params.padding_values.depth + params.padding_values.depth_offset;
```


### Relevant log output

_No response_","['type:bug', 'comp:lite', 'awaiting PR merge', 'TF 2.16']",2024-05-21T07:27:49Z,2,0,https://github.com/tensorflow/tensorflow/issues/68319,Logical Bug
160,tensorflow/tensorflow,ValueError: as_list() is not defined on an unknown TensorShape. during training,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.17.0-dev20240517

### Custom code

Yes

### OS platform and distribution

macOS Sonoma 

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I get the following error when training a transformer for translation:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[9], [line 6](vscode-notebook-cell:?execution_count=9&line=6)
      [1](vscode-notebook-cell:?execution_count=9&line=1) model.compile(optimizer='adam', 
      [2](vscode-notebook-cell:?execution_count=9&line=2)     loss='sparse_categorical_crossentropy', 
      [3](vscode-notebook-cell:?execution_count=9&line=3)     metrics=['accuracy'],
      [4](vscode-notebook-cell:?execution_count=9&line=4)     run_eagerly=False
      [5](vscode-notebook-cell:?execution_count=9&line=5)     )
----> [6](vscode-notebook-cell:?execution_count=9&line=6) model.fit(dataset_train, epochs=10)

File ~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    [119](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:119)     filtered_tb = _process_traceback_frames(e.__traceback__)
    [120](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:120)     # To get the full stack trace, call:
    [121](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:121)     # `keras.config.disable_traceback_filtering()`
--> [122](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122)     raise e.with_traceback(filtered_tb) from None
    [123](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123) finally:
    [124](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:124)     del filtered_tb

File ~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/optree/ops.py:594, in tree_map(func, tree, is_leaf, none_is_leaf, namespace, *rests)
    [592](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/optree/ops.py:592) leaves, treespec = _C.flatten(tree, is_leaf, none_is_leaf, namespace)
    [593](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/optree/ops.py:593) flat_args = [leaves] + [treespec.flatten_up_to(r) for r in rests]
--> [594](https://file+.vscode-resource.vscode-cdn.net/Users/glo/Desktop/deep_learning_training-main/~/opt/anaconda3/envs/tensorflow_latest/lib/python3.10/site-packages/optree/ops.py:594) return treespec.unflatten(map(func, *flat_args))

ValueError: as_list() is not defined on an unknown TensorShape.
```

### Standalone code to reproduce the issue

```shell
Here is a colab link to the code:
https://colab.research.google.com/drive/1BQ4lhaZPP5XGb_IUe-rVacMiFPWCfm9Y?usp=sharing

Running eagerly works fine but an error occurs in graph mode.
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-18T15:26:10Z,6,0,https://github.com/tensorflow/tensorflow/issues/68217,Runtime Error
161,tensorflow/tensorflow,"Strange finding: When the global seed and @tf.function decorator are used, the random sampling values of the two adjacent periods are equal","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When I set the global seed, applied the @tf.function decorator, and performed gradient updates every two periods, I observed an unexpected phenomenon: the error values were randomly sampled from two consecutive periods with identical values. However, my expectation was that the error values should differ when sampled in each period instead of being equal in adjacent periods. Furthermore, I noticed that removing the @tf.function decorator from the model function prevented this issue of having identical error values in consecutive periods. What could be causing this phenomenon? How should one handle this situation when using @tf.function?""

### Standalone code to reproduce the issue

```shell
import os
import random
import numpy as np
import tensorflow as tf

# set seeds
SEED = 0
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)
os.environ['PYTHONHASHSEED'] = str(SEED)
tf.keras.utils.set_random_seed(SEED)
tf.config.experimental.enable_op_determinism()

@tf.function
def model(x):
    err = tf.random.uniform(shape=(1,))
    loss = x + err
    return err, loss

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
training_periods = 10

# initialize x
x = tf.Variable(tf.random.uniform(shape=(1,)), trainable=True)

for tt in range(training_periods):
    if tt % 2 == 0:
        with tf.GradientTape() as tape:
            err, loss = model(x)
        
        gradients = tape.gradient(loss, [x])  # suppose x need to be optimized
        optimizer.apply_gradients(zip(gradients, [x]))  # update x
        
        print(f""Period: {tt}, err (trained): {err.numpy()}"")
        
    else:
        err, loss = model(x)
        
        print(f""Period: {tt}, err (not trained): {err.numpy()}"")

# outcomes
Period: 0, err (trained): [0.01975703]
Period: 1, err (not trained): [0.01975703]
Period: 2, err (trained): [0.5400312]
Period: 3, err (not trained): [0.5400312]
Period: 4, err (trained): [0.51667833]
Period: 5, err (not trained): [0.51667833]
Period: 6, err (trained): [0.4683528]
Period: 7, err (not trained): [0.4683528]
Period: 8, err (trained): [0.14856052]
Period: 9, err (not trained): [0.14856052]
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'comp:tf.function', 'TF 2.16']",2024-05-18T14:09:09Z,3,0,https://github.com/tensorflow/tensorflow/issues/68215,Runtime Error
162,tensorflow/tensorflow,Exit code 137 in `tf.raw_ops.ResizeNearestNeighborGrad`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value in the input `size` is too large, program exceeds the memory limit.

### Standalone code to reproduce the issue

```shell
# Signal --4;2024-05-14 00:40:39.895950: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-05-14 00:40:39.896316: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-14 00:40:39.900744: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-14 00:40:39.956390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-05-14 00:40:40.842339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

# ResizeNearestNeighborOpGrad

import tensorflow as tf

align_corners = True
half_pixel_centers = False
grads = tf.constant(1.5e+300, shape=[1,8,16,3], dtype=tf.float64)
size = tf.constant([65534,65534], shape=[2], dtype=tf.int32)
tf.raw_ops.ResizeNearestNeighborGrad(grads=grads, size=size, align_corners=align_corners, half_pixel_centers=half_pixel_centers)
```


### Relevant log output

```shell
(tensorflow-2.16.1-orig) root@b29bda27c601:/mnt# python tests/ResizeNearestNeighborGrad.py 
2024-05-15 08:43:01.439844: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-15 08:43:01.440211: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-05-15 08:43:01.444264: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-05-15 08:43:01.498275: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-15 08:43:02.744313: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
Killed
```
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-15T09:47:39Z,1,0,https://github.com/tensorflow/tensorflow/issues/67626,Performance Issue
163,tensorflow/tensorflow,Missing return in error check in mlir::TFTPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Possible null pointer dereference may occur because of missing return in error check:
https://github.com/tensorflow/tensorflow/blob/63bb7339a2bc98af97c61e86eedbbd60c123529c/tensorflow/compiler/mlir/tensorflow/transforms/extract_tpu_copy_with_dynamic_shape_op.cc#L61-L65

### Standalone code to reproduce the issue

```shell
Bug was found with Svace static analyzer
```


### Relevant log output

_No response_","['awaiting review', 'type:bug', 'TF 2.16']",2024-05-14T12:28:06Z,0,0,https://github.com/tensorflow/tensorflow/issues/67551,Runtime Error
164,tensorflow/tensorflow,Segmentation fault (core dumped) in `tf.raw_ops.FusedResizeAndPadConv2D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Illegal `size` will trigger a segfault.

### Standalone code to reproduce the issue

```shell
# Signal --4;2024-05-13 05:48:16.272992: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.2024-05-13 05:48:16.273352: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-13 05:48:16.277715: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.2024-05-13 05:48:16.331241: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.2024-05-13 05:48:17.210828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT2024-05-13 05:48:17.792175: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.

# FusedResizeConv2DUsingGemmOp

import tensorflow as tf

mode = ""REFLECT""
strides = [1, 1, 1, 1]
padding = ""VALID""
resize_align_corners = False
input = tf.constant(0, shape=[1,2,3,2], dtype=tf.float16)
size = tf.constant([65534,65534], shape=[2], dtype=tf.int32)
paddings = tf.constant(0, shape=[4,2], dtype=tf.int32)
filter = tf.constant(0, shape=[1,2,2,2], dtype=tf.float16)
tf.raw_ops.FusedResizeAndPadConv2D(input=input, size=size, paddings=paddings, filter=filter, mode=mode, strides=strides, padding=padding, resize_align_corners=resize_align_corners)
```


### Relevant log output

```shell
2024-05-14 08:31:20.675971: E tensorflow/core/util/util.cc:131] oneDNN supports DT_HALF only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.
Segmentation fault (core dumped)
```
ASAN report:
```
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
=================================================================
AddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer==2060653==ERROR: AddressSanitizer: SEGV on unknown address 0x7fbb6ce8a800 (pc 0x7fc21cc305a9 bp 0x7fc1b01c05c0 sp 0x7fc1b01c03b0 T93)
:DEADLYSIGNAL
:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
==2060653==The signal is caused by a WRITE memory access.
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizerAddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
AddressSanitizer:DEADLYSIGNAL
:DEADLYSIGNAL
:DEADLYSIGNAL
    #0 0x7fc21cc305a9 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::pack_rhs(long, long) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x394ae5a9)
    #1 0x7fc21cc31692 in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::enqueue_packing_helper(long, long, long, bool) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x394af692)
    #2 0x7fc24fd0b121 in Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41fd121)
    #3 0x7fc24fd00326 in void absl::lts_20230802::internal_any_invocable::RemoteInvoker<false, void, tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}&>(absl::lts_20230802::internal_any_invocable::TypeErasedState*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41f2326)
    #4 0x7fc24e62625c in tsl::(anonymous namespace)::PThread::ThreadFn(void*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2b1825c)
    #5 0x7fc2f9885ac2  (/lib/x86_64-linux-gnu/libc.so.6+0x94ac2)
    #6 0x7fc2f9916a03 in __clone (/lib/x86_64-linux-gnu/libc.so.6+0x125a03)

AddressSanitizer can not provide additional info.
SUMMARY: AddressSanitizer: SEGV (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x394ae5a9) in Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::EvalParallelContext<Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::TensorMap<Eigen::Tensor<Eigen::half const, 2, 1, long>, 16, Eigen::MakePointer> const, Eigen::NoOpOutputKernel const> const, Eigen::ThreadPoolDevice>::NoCallback, true, true, false, 0>::pack_rhs(long, long)
Thread T93 created by T0 here:
    #0 0x7fc2f9bab685 in __interceptor_pthread_create ../../../../src/libsanitizer/asan/asan_interceptors.cpp:216
    #1 0x7fc24e633fbf in tsl::(anonymous namespace)::PThread::PThread(tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, absl::lts_20230802::AnyInvocable<void ()>) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2b25fbf)
    #2 0x7fc24e63450a in tsl::(anonymous namespace)::PosixEnv::StartThread(tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, absl::lts_20230802::AnyInvocable<void ()>) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x2b2650a)
    #3 0x7fc24fd093a8 in Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::ThreadPoolTempl(int, bool, tsl::thread::EigenEnvironment) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41fb3a8)
    #4 0x7fc24fd0d498 in tsl::thread::ThreadPool::ThreadPool(tsl::Env*, tsl::ThreadOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int, bool, Eigen::Allocator*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x41ff498)
    #5 0x7fc24d7ef278 in tensorflow::LocalDevice::EigenThreadPoolInfo::EigenThreadPoolInfo(tensorflow::SessionOptions const&, int, tsl::Allocator*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1ce1278)
    #6 0x7fc24d7f05c5 in tensorflow::LocalDevice::LocalDevice(tensorflow::SessionOptions const&, tensorflow::DeviceAttributes const&) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1ce25c5)
    #7 0x7fc24d7e3111 in tensorflow::ThreadPoolDevice::ThreadPoolDevice(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, tsl::gtl::IntType<tensorflow::Bytes_tag_, long>, tensorflow::DeviceLocality const&, tsl::Allocator*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cd5111)
    #8 0x7fc24d7dc1dd in tensorflow::ThreadPoolDeviceFactory::CreateDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1cce1dd)
    #9 0x7fc24d9a667c in tensorflow::DeviceFactory::AddCpuDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1e9867c)
    #10 0x7fc24d9a6bbd in tensorflow::DeviceFactory::AddDevices(tensorflow::SessionOptions const&, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::vector<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> >, std::allocator<std::unique_ptr<tensorflow::Device, std::default_delete<tensorflow::Device> > > >*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x1e98bbd)
    #11 0x7fc1f3dd6db8 in TFE_NewContext (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x10654db8)
    #12 0x7fc1df839e70 in pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(TFE_ContextOptions const*)#9}, pybind11::object, TFE_ContextOptions const*, pybind11::name, pybind11::scope, pybind11::sibling, pybind11::return_value_policy>(pybind11_init__pywrap_tfe(pybind11::module_&)::{lambda(TFE_ContextOptions const*)#9}&&, pybind11::object (*)(TFE_ContextOptions const*), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&, pybind11::return_value_policy const&)::{lambda(pybind11::detail::function_call&)#3}::_FUN(pybind11::detail::function_call&) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1b6e70)
    #13 0x7fc1df851899 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) (/mnt//venv/tensorflow-2.16.1-asan/lib/python3.11/site-packages/tensorflow/python/_pywrap_tfe.so+0x1ce899)
    #14 0x51ad66  (/usr/bin/python3.11+0x51ad66)

==2060653==ABORTING
```
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-14T08:39:21Z,1,0,https://github.com/tensorflow/tensorflow/issues/67529,Runtime Error
165,tensorflow/tensorflow,Aborted (core dumped) in `QuantizedInstanceNorm`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Abort is triggered when the input x does not meet the condition of dimension 4.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

output_range_given = False
given_y_min = 0
given_y_max = 0
variance_epsilon = 1e-05
min_separation = 0.001
x = tf.constant([], shape=[0,0,0,1,1,0,0,0,11,9], dtype=tf.quint8)
x_min = tf.constant(-3.5e+35, shape=[], dtype=tf.float32)
x_max = tf.constant(0, shape=[], dtype=tf.float32)
tf.raw_ops.QuantizedInstanceNorm(x=x, x_min=x_min, x_max=x_max, output_range_given=output_range_given, given_y_min=given_y_min, given_y_max=given_y_max, variance_epsilon=variance_epsilon, min_separation=min_separation)
```


### Relevant log output

```shell
2024-05-14 08:25:37.034690: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (4 vs. 10)Asking for tensor of 4 dimensions from a tensor of 10 dimensions
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-14T08:30:42Z,1,0,https://github.com/tensorflow/tensorflow/issues/67528,Runtime Error
166,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.FusedPadConv2D`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When `input` dimension is less than 4， it will cause abort.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

mode = ""REFLECT""
strides = [1, 1, 1, 1]
padding = ""VALID""
input = tf.constant(1.5e+300, shape=[], dtype=tf.float64)
paddings = tf.constant(65534, shape=[4,2], dtype=tf.int32)
filter = tf.constant([], shape=[1,1,0,1,0], dtype=tf.float64)
tf.raw_ops.FusedPadConv2D(input=input, paddings=paddings, filter=filter, mode=mode, strides=strides, padding=padding)
```


### Relevant log output

```shell
2024-05-14 08:19:37.953832: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 0)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-14T08:23:10Z,1,0,https://github.com/tensorflow/tensorflow/issues/67527,Runtime Error
167,tensorflow/tensorflow,"Numerical precision issue of operators selu, leakyRelu, softplus and their corresponding backward operators on Bfloat16 vs float32","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'd like to bring to attention an issue concerning the numerical precision of several operators (selu, leaky, relu) when operating on Bfloat16 versus float32 data types. I conducted comparisons using 20,000 random tensors for these operators, assessing the outputs in both Bfloat16 and float32 and computing the discrepancies. My observations indicate that differences generated by TensorFlow are generally more pronounced compared to PyTorch. Particularly noteworthy is the significant error produced by the SeluGrad operator. The results are summarized in the table below:

| Operator           | TensorFlow | PyTorch |
|--------------------|------------|---------|
| selu               | 0.24918    | 0.12243 |
| leakyrelu          | 0.01875    | 0.00094 |
| softplus           | 0.05488    | 0.01554 |
| seluGrad      | 10.41794   | 0.12406 |
| leakyreluGrad | 0.01875    | 0.00094 |
| softplusGrad | 0.13502    | 0.12484 |

In a standalone code to reproduce the issue, I provide illustrative instances for seluGrad operators, where the output discrepancy between Bfloat16 and float32 can be as high as 10.4.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

features = tf.convert_to_tensor(np.array([-0.00112915]), dtype=tf.float32)
gradients = tf.convert_to_tensor(np.array([-14.6875]), dtype=tf.float32)

x = tf.Variable(features)
k = tf.constant(gradients)
with tf.GradientTape(persistent=True) as tape:
    y = tf.nn.selu(features=x)
    z = k*y
    fianl = tf.reduce_mean(z)
    
print('float32 gradient:',tape.gradient(z, x))


features = tf.cast(features, dtype=tf.bfloat16)
gradients = tf.cast(gradients, dtype=tf.bfloat16)
x = tf.Variable(features)
k = tf.constant(gradients)
with tf.GradientTape(persistent=True) as tape:
    y = tf.nn.selu(features=x)
    z = k*y
    fianl = tf.reduce_mean(z)
print('float16 gradient:',tape.gradient(z, x))
```


### Relevant log output

```shell
float32 gradient: tf.Tensor([-25.792944], shape=(1,), dtype=float32)
bfloat16 gradient: tf.Tensor([-15.375], shape=(1,), dtype=bfloat16)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-13T12:05:08Z,2,0,https://github.com/tensorflow/tensorflow/issues/67440,Performance Issue
168,tensorflow/tensorflow,Failed to compile on aarch64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

aarch64

### Mobile device

_No response_

### Python version

3.10

### Bazel version

bazelisk using the default version requested by tensorflow

### GCC/compiler version

clang 17 (I also tried with clang16).

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When compiling the tensorflow source code on aarch64, I always get the following error:

```
WARN ERROR: /home/build/tensorflow/BUILD:1263:20: Linking tensorflow/libtensorflow.so.2.16.1 failed: (Exit 1): clang-17 failed: error executing command (from target //tensorflow:libtensorflow.so.2.16.1) /usr/bin/clang-17 @bazel-out/aarch64-opt/bin/tensorflow/libtensorflow.so.2.16.1-2.params
```

However the compilation works on x86_64.

``` 
export PYTHON_BIN_PATH=/usr/bin/python
      export TF_PYTHON_VERSION=3.10
      export USE_DEFAULT_PYTHON_LIB_PATH=1
      export TF_NEED_JEMALLOC=1
      export TF_NEED_KAFKA=1
      export TF_NEED_OPENCL_SYCL=0
      export TF_NEED_AWS=1
      export TF_NEED_GCP=1
      export TF_NEED_HDFS=1
      export TF_NEED_S3=1
      export TF_ENABLE_XLA=1
      export TF_NEED_GDR=0
      export TF_NEED_VERBS=0
      export TF_NEED_OPENCL=0
      export TF_NEED_MPI=0
      export TF_NEED_TENSORRT=0
      export TF_NEED_NGRAPH=0
      export TF_NEED_IGNITE=0
      export TF_NEED_ROCM=0
      export TF_SYSTEM_LIBS=""boringssl,curl,gif,icu,libjpeg_turbo,nasm,png,zlib""
      export TF_SET_ANDROID_WORKSPACE=0

      ./configure

      bazel --bazelrc=.tf_configure.bazelrc build \
        --config=opt \
        --config=mkl_threadpool \
        //tensorflow:libtensorflow.so \
        //tensorflow:libtensorflow_cc.so \
        //tensorflow:install_headers \
        //tensorflow/tools/pip_package:build_pip_package
```

### Standalone code to reproduce the issue

```shell
export PYTHON_BIN_PATH=/usr/bin/python
      export TF_PYTHON_VERSION=3.10
      export USE_DEFAULT_PYTHON_LIB_PATH=1
      export TF_NEED_JEMALLOC=1
      export TF_NEED_KAFKA=1
      export TF_NEED_OPENCL_SYCL=0
      export TF_NEED_AWS=1
      export TF_NEED_GCP=1
      export TF_NEED_HDFS=1
      export TF_NEED_S3=1
      export TF_ENABLE_XLA=1
      export TF_NEED_GDR=0
      export TF_NEED_VERBS=0
      export TF_NEED_OPENCL=0
      export TF_NEED_MPI=0
      export TF_NEED_TENSORRT=0
      export TF_NEED_NGRAPH=0
      export TF_NEED_IGNITE=0
      export TF_NEED_ROCM=0
      export TF_SYSTEM_LIBS=""boringssl,curl,gif,icu,libjpeg_turbo,nasm,png,zlib""
      export TF_SET_ANDROID_WORKSPACE=0

      ./configure

      bazel --bazelrc=.tf_configure.bazelrc build \
        --config=opt \
        --config=mkl_threadpool \
        //tensorflow:libtensorflow.so \
        //tensorflow:libtensorflow_cc.so \
        //tensorflow:install_headers \
        //tensorflow/tools/pip_package:build_pip_package
```


### Relevant log output

```shell
2024/05/09 13:03:51 WARN [14,447 / 14,578] Compiling tensorflow/compiler/jit/xla_platform_info.cc; 20s local ... (16 actions running)
2024/05/09 13:03:52 WARN [14,448 / 14,578] Compiling tensorflow/compiler/jit/xla_platform_info.cc; 21s local ... (16 actions running)
2024/05/09 13:03:55 WARN [14,449 / 14,578] Compiling tensorflow/compiler/jit/xla_platform_info.cc; 24s local ... (16 actions, 15 running)
2024/05/09 13:03:57 WARN [14,450 / 14,578] Compiling tensorflow/compiler/jit/xla_platform_info.cc; 26s local ... (16 actions running)
2024/05/09 13:03:58 WARN [14,452 / 14,578] Compiling tensorflow/compiler/jit/get_compiler_ir.cc; 26s local ... (16 actions running)
2024/05/09 13:04:00 WARN [14,454 / 14,578] Compiling tensorflow/compiler/jit/kernels/xla_ops.cc; 27s local ... (16 actions running)
2024/05/09 13:04:01 WARN [14,457 / 14,578] Compiling tensorflow/compiler/jit/xla_cpu_device.cc; 22s local ... (16 actions running)
2024/05/09 13:04:03 WARN [14,459 / 14,578] Compiling tensorflow/compiler/jit/xla_cpu_device.cc; 24s local ... (16 actions running)
2024/05/09 13:04:04 WARN [14,460 / 14,578] Compiling tensorflow/compiler/jit/xla_cpu_device.cc; 25s local ... (16 actions, 15 running)
2024/05/09 13:04:06 WARN [14,461 / 14,578] Compiling tensorflow/compiler/jit/xla_cpu_device.cc; 27s local ... (16 actions running)
2024/05/09 13:04:07 WARN [14,464 / 14,578] Compiling tensorflow/compiler/tf2xla/mlir_tf2xla.cc; 22s local ... (16 actions running)
2024/05/09 13:04:11 WARN [14,465 / 14,578] Compiling tensorflow/compiler/tf2xla/mlir_tf2xla.cc; 26s local ... (16 actions, 15 running)
2024/05/09 13:04:12 WARN [14,467 / 14,578] Compiling tensorflow/compiler/tf2xla/mlir_tf2xla.cc; 27s local ... (16 actions, 15 running)
2024/05/09 13:04:13 WARN [14,468 / 14,578] Compiling tensorflow/compiler/tf2xla/mlir_tf2xla.cc; 28s local ... (16 actions running)
2024/05/09 13:04:14 WARN [14,470 / 14,578] Compiling tensorflow/compiler/tf2xla/mlir_tf2xla.cc; 29s local ... (16 actions, 15 running)
2024/05/09 13:04:16 WARN [14,473 / 14,578] Compiling tensorflow/compiler/aot/codegen.cc; 19s local ... (16 actions, 15 running)
2024/05/09 13:04:17 WARN [14,476 / 14,578] Compiling tensorflow/core/common_runtime/pluggable_device/pluggable_device_plugin_init.cc; 16s local ... (16 actions running)
2024/05/09 13:04:18 WARN [14,478 / 14,578] Compiling tensorflow/core/common_runtime/pluggable_device/pluggable_device_plugin_init.cc; 17s local ... (16 actions, 15 running)
2024/05/09 13:04:20 WARN [14,479 / 14,578] Compiling tensorflow/core/common_runtime/pluggable_device/pluggable_device_plugin_init.cc; 19s local ... (16 actions, 15 running)
2024/05/09 13:04:21 WARN [14,481 / 14,578] Compiling tensorflow/core/common_runtime/pluggable_device/pluggable_device_plugin_init.cc; 20s local ... (16 actions, 14 running)
2024/05/09 13:04:22 WARN [14,482 / 14,578] Compiling tensorflow/core/common_runtime/pluggable_device/pluggable_device_plugin_init.cc; 21s local ... (16 actions, 15 running)
2024/05/09 13:04:23 WARN [14,486 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 21s local ... (16 actions running)
2024/05/09 13:04:25 WARN [14,487 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 23s local ... (16 actions running)
2024/05/09 13:04:26 WARN [14,489 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 24s local ... (16 actions running)
2024/05/09 13:04:28 WARN [14,491 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 25s local ... (16 actions, 15 running)
2024/05/09 13:04:29 WARN [14,492 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 27s local ... (16 actions, 15 running)
2024/05/09 13:04:30 WARN [14,494 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 28s local ... (16 actions, 15 running)
2024/05/09 13:04:31 WARN [14,497 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 29s local ... (14 actions running)
2024/05/09 13:04:32 WARN [14,498 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 30s local ... (13 actions running)
2024/05/09 13:04:34 WARN [14,500 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 32s local ... (11 actions running)
2024/05/09 13:04:36 WARN [14,505 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 34s local ... (6 actions running)
2024/05/09 13:04:38 WARN [14,506 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 36s local ... (5 actions running)
2024/05/09 13:04:40 WARN [14,508 / 14,578] Compiling tensorflow/compiler/mlir/python/mlir.cc; 38s local ... (3 actions running)
2024/05/09 13:04:40 WARN ERROR: /home/build/tensorflow/BUILD:1263:20: Linking tensorflow/libtensorflow.so.2.16.1 failed: (Exit 1): clang-17 failed: error executing command (from target //tensorflow:libtensorflow.so.2.16.1) /usr/bin/clang-17 @bazel-out/aarch64-opt/bin/tensorflow/libtensorflow.so.2.16.1-2.params
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN /usr/bin/../lib64/gcc/aarch64-unknown-linux-gnu/13.2.0/../../../../aarch64-unknown-linux-gnu/bin/ld.gold: error: Stub is too far away, try a smaller value for '--stub-group-size'. The current value is 0x7ffbffc.
2024/05/09 13:04:40 WARN clang-17: error: linker command failed with exit code 1 (use -v to see invocation)
2024/05/09 13:04:41 WARN INFO: Elapsed time: 4130.234s, Critical Path: 216.57s
2024/05/09 13:04:41 WARN INFO: 14511 processes: 1423 internal, 13088 local.
2024/05/09 13:04:41 WARN FAILED: Build did NOT complete successfully
```
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'TF 2.16']",2024-05-09T13:28:57Z,3,0,https://github.com/tensorflow/tensorflow/issues/67251,Dependency Issue
169,tensorflow/tensorflow,GlobalAveragePooling1D fails with empty inputs and a mask,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.keras.layers.GlobalAveragePooling1D` cannot be called on an empty tensor with an empty mask. This can cause issue when using a model that uses this layer under a distributed strategy, e.g. `MirroredStrategy`, which will distribute the data over multiple GPUs. For instance, for a dataset with 3 samples, a batch_size of 2 and 3 GPUs, one of the GPUs will have empty batches which will cause the error.

This is due to the casting `math_ops.cast(mask, inputs[0].dtype)` in `GlobalAveragePooling1D::call()` which implies a non-empty inputs tensor, while `math_ops.cast(mask, inputs.dtype`) should do the trick without causing the error.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

x = np.random.rand(0, 3, 4)
mask = np.random.rand(0, 3)
print(x, mask)

y = tf.keras.layers.GlobalAveragePooling1D()(x, mask=mask)
print(y)
```


### Relevant log output

```shell
[] []
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-20-0f7c323aaaab> in <cell line: 7>()
      5 print(x, mask)
      6 
----> 7 y = tf.keras.layers.GlobalAveragePooling1D()(x, mask=mask)
      8 print(y)

1 frames
/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   5881 def raise_from_not_ok_status(e, name) -> NoReturn:
   5882   e.message += ("" name: "" + str(name if name is not None else """"))
-> 5883   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   5884 
   5885 

InvalidArgumentError: Exception encountered when calling layer 'global_average_pooling1d_8' (type GlobalAveragePooling1D).

{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: global_average_pooling1d_8/strided_slice/

Call arguments received by layer 'global_average_pooling1d_8' (type GlobalAveragePooling1D):
  • inputs=tf.Tensor(shape=(0, 3, 4), dtype=float32)
  • mask=array([], shape=(0, 3), dtype=float64)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF 2.15']",2024-05-06T14:57:57Z,2,0,https://github.com/tensorflow/tensorflow/issues/67023,Logical Bug
170,tensorflow/tensorflow,Aborted (core dumped) in `TensorScatterOp`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The dimensions of `indices` and `updates` must be equal.If not equal,`tf.raw
_ops.TensorScatterSub,tf.raw
_ops.TensorScatterAdd,tf.raw
_ops.TensorScatterMin,tf.raw
_ops.TensorScatterMax,tf.raw
_ops.TensorScatterUpdate ` will crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tensor = tf.constant(0, shape=[101,32], dtype=tf.float32)
indices = tf.constant([], shape=[0,0,0,6], dtype=tf.int32)
updates = tf.constant([], shape=[0,0], dtype=tf.float32)
tf.raw_ops.TensorScatterAdd(tensor=tensor, indices=indices, updates=updates)
```
```
import tensorflow as tf

tensor = tf.constant(0, shape=[101,32], dtype=tf.float32)
indices = tf.constant([], shape=[0,0,0,6], dtype=tf.int32)
updates = tf.constant([], shape=[0,0], dtype=tf.float32)
tf.raw_ops.TensorScatterMax(tensor=tensor, indices=indices, updates=updates)
```
```
import tensorflow as tf

tensor = tf.constant(0, shape=[101,32], dtype=tf.float32)
indices = tf.constant([], shape=[0,0,0,6], dtype=tf.int32)
updates = tf.constant([], shape=[0,0], dtype=tf.float32)
tf.raw_ops.TensorScatterMin(tensor=tensor, indices=indices, updates=updates)
```
```
import tensorflow as tf

tensor = tf.constant(0, shape=[101,32], dtype=tf.float32)
indices = tf.constant([], shape=[0,0,0,6], dtype=tf.int32)
updates = tf.constant([], shape=[0,0], dtype=tf.float32)
tf.raw_ops.TensorScatterSub(tensor=tensor, indices=indices, updates=updates)
```
```
import tensorflow as tf

tensor = tf.constant(0, shape=[101,32], dtype=tf.float32)
indices = tf.constant([], shape=[0,0,0,6], dtype=tf.int32)
updates = tf.constant([], shape=[0,0], dtype=tf.float32)
tf.raw_ops.TensorScatterUpdate(tensor=tensor, indices=indices, updates=updates)
```
```


### Relevant log output

```shell
2024-05-01 04:00:48.333921: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 2)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-01T04:08:30Z,1,0,https://github.com/tensorflow/tensorflow/issues/66768,Runtime Error
171,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.SparseBincount`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Multiplication of `dense_shape` and `size` causes integer overflow

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

binary_output = True
indices = tf.constant(0, shape=[3456,2], dtype=tf.int64)
values = tf.constant(-536870912, shape=[3456], dtype=tf.int32)
dense_shape = tf.constant([1250999896764,1250999896764], shape=[2], dtype=tf.int64)
size = tf.constant(305032608, shape=[], dtype=tf.int32)
weights = tf.constant(51, shape=[3456], dtype=tf.int32)
tf.raw_ops.SparseBincount(indices=indices, values=values, dense_shape=dense_shape, size=size, weights=weights, binary_output=binary_output)
```


### Relevant log output

```shell
2024-05-01 03:51:04.482001: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 1250999896764 with 305032608, result: -1
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-01T03:53:11Z,1,0,https://github.com/tensorflow/tensorflow/issues/66766,Logical Bug
172,tensorflow/tensorflow,Aborted (core dumped) in `tf.raw_ops.NearestNeighbors`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Parameter k being set to a negative number will cause a crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

points = tf.constant(0.554979503, shape=[1000,100], dtype=tf.float32)
centers = tf.constant(0.554979503, shape=[1000,100], dtype=tf.float32)
k = tf.constant(-1250999896764, shape=[], dtype=tf.int64)
tf.raw_ops.NearestNeighbors(points=points, centers=centers, k=k)
```


### Relevant log output

```shell
2024-05-01 03:49:00.796702: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -1250999896764
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-01T03:50:23Z,1,0,https://github.com/tensorflow/tensorflow/issues/66765,Runtime Error
173,tensorflow/tensorflow,Check fail in `tf.raw_ops.MaxPoolGradWithArgmax`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When argmax is set to a maximum value， `tf.raw_ops.MaxPoolGradWithArgmax` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

ksize = [1, 1, 1, 1]
strides = [1, 1, 1, 1]
padding = ""VALID""
include_batch_in_index = False
input = tf.constant(1, shape=[1,1,1,1], dtype=tf.float32)
grad = tf.constant(1, shape=[1,1,1,1], dtype=tf.float32)
argmax = tf.constant(1250999896764, shape=[1,1,1,1], dtype=tf.int64)
tf.raw_ops.MaxPoolGradWithArgmax(input=input, grad=grad, argmax=argmax, ksize=ksize, strides=strides, padding=padding, include_batch_in_index=include_batch_in_index)
```


### Relevant log output

```shell
2024-05-01 03:42:17.994627: F tensorflow/core/kernels/maxpooling_op.cc:1081] Check failed: grad_out_index >= output_start && grad_out_index < output_end Invalid output gradient index: 1250999896764, 0, 1
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-01T03:46:46Z,1,0,https://github.com/tensorflow/tensorflow/issues/66764,Runtime Error
174,tensorflow/tensorflow,Aborted (core dumped) with `tf.raw_ops.LSTMBlockCellGrad`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The parameter x is expected to be two-dimensional, and an error will occur when x is one-dimensional

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

use_peephole = False
x = tf.constant(0, shape=[17], dtype=tf.float32)
cs_prev = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
h_prev = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
w = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
wci = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
wcf = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
wco = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
b = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
i = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
cs = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
f = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
o = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
ci = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
co = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
cs_grad = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
h_grad = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
tf.raw_ops.LSTMBlockCellGrad(x=x, cs_prev=cs_prev, h_prev=h_prev, w=w, wci=wci, wcf=wcf, wco=wco, b=b, i=i, cs=cs, f=f, o=o, ci=ci, co=co, cs_grad=cs_grad, h_grad=h_grad, use_peephole=use_peephole)
```


### Relevant log output

```shell
2024-05-01 03:41:18.804132: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-01T03:41:25Z,1,0,https://github.com/tensorflow/tensorflow/issues/66763,Runtime Error
175,tensorflow/tensorflow,Aborted (core dumped) with `tf.raw_ops.LSTMBlockCell`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.LSTMBlockCell` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

forget_bias = 1
cell_clip = 0
use_peephole = False
x = tf.constant([], shape=[0], dtype=tf.float32)
cs_prev = tf.constant([], shape=[0], dtype=tf.float32)
h_prev = tf.constant([], shape=[0,0], dtype=tf.float32)
w = tf.constant([], shape=[0], dtype=tf.float32)
wci = tf.constant([], shape=[0,0], dtype=tf.float32)
wcf = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
wco = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
b = tf.constant(0.837607, shape=[28,29], dtype=tf.float32)
tf.raw_ops.LSTMBlockCell(x=x, cs_prev=cs_prev, h_prev=h_prev, w=w, wci=wci, wcf=wcf, wco=wco, b=b, forget_bias=forget_bias, cell_clip=cell_clip, use_peephole=use_peephole)
```


### Relevant log output

```shell
2024-05-01 03:36:12.005999: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-01T03:37:18Z,1,0,https://github.com/tensorflow/tensorflow/issues/66762,Runtime Error
176,tensorflow/tensorflow,Aborted (core dumped) with `tf.raw_ops.LRNGrad`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If  the dimension of `output_image` is less than `input_grads` and `input_image`, tf.raw_ops.tf.raw_ops.LoadAndRemapMatrix encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
# LRNGradOp

import tensorflow as tf

depth_radius = 1
bias = 1.59018219
alpha = 0.117728651
beta = 0.404427052
input_grads = tf.constant([], shape=[0,0,0,0], dtype=tf.float32)
input_image = tf.constant([], shape=[0,0,0,0], dtype=tf.float32)
output_image = tf.constant([], shape=[0], dtype=tf.float32)
tf.raw_ops.LRNGrad(input_grads=input_grads, input_image=input_image, output_image=output_image, depth_radius=depth_radius, bias=bias, alpha=alpha, beta=beta)
```


### Relevant log output

```shell
2024-05-01 03:28:07.055077: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-01T03:33:03Z,1,0,https://github.com/tensorflow/tensorflow/issues/66761,Runtime Error
177,tensorflow/tensorflow,Aborted (core dumped) with `tf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Check Failed in `tf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient` when the input of inputs is scalar, which causes the program to crash.

### Standalone code to reproduce the issue

```shell
# FakeQuantWithMinMaxVarsPerChannelGradientOp

import tensorflow as tf

num_bits = 8
narrow_range = False
gradients = tf.constant(0, shape=[], dtype=tf.float32)
inputs = tf.constant(0, shape=[], dtype=tf.float32)
min = tf.constant(124, shape=[3,1], dtype=tf.float32)
max = tf.constant(124, shape=[3,1], dtype=tf.float32)
tf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient(gradients=gradients, inputs=inputs, min=min, max=max, num_bits=num_bits, narrow_range=narrow_range)
```


### Relevant log output

```shell
2024-05-01 03:10:46.669162: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-01T03:16:02Z,1,0,https://github.com/tensorflow/tensorflow/issues/66759,Runtime Error
178,tensorflow/tensorflow,Aborted (core dumped) with `tf.raw_ops.Dilation2DBackpropFilter`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.Dilation2DBackpropFilter` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

strides = [1, 1, 1, 1]
rates = [1, 1, 1, 1]
padding = ""VALID""
input = tf.constant([], shape=[0,0,0,0], dtype=tf.float32)
filter = tf.constant([], shape=[0,0,0], dtype=tf.float32)
out_backprop = tf.constant([], shape=[0], dtype=tf.float32)
tf.raw_ops.Dilation2DBackpropFilter(input=input, filter=filter, out_backprop=out_backprop, strides=strides, rates=rates, padding=padding)
```


### Relevant log output

```shell
2024-05-01 03:00:39.549898: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-01T03:05:19Z,1,0,https://github.com/tensorflow/tensorflow/issues/66758,Runtime Error
179,tensorflow/tensorflow,Aborted (core dumped) with `tf.raw_ops.CombinedNonMaxSuppression`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

If `max_total_size` is too large , `tf.raw_ops.CombinedNonMaxSuppression` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

pad_per_class = True
clip_boxes = False
boxes = tf.constant(3.5e+35, shape=[], dtype=tf.float32)
scores = tf.constant(0, shape=[], dtype=tf.float32)
max_output_size_per_class = tf.constant(8, shape=[], dtype=tf.int32)
max_total_size = tf.constant(1879048192, shape=[], dtype=tf.int32)
iou_threshold = tf.constant(0.5, shape=[], dtype=tf.float32)
score_threshold = tf.constant(0.5, shape=[], dtype=tf.float32)
tf.raw_ops.CombinedNonMaxSuppression(boxes=boxes, scores=scores, max_output_size_per_class=max_output_size_per_class, max_total_size=max_total_size, iou_threshold=iou_threshold, score_threshold=score_threshold, pad_per_class=pad_per_class, clip_boxes=clip_boxes)
```


### Relevant log output

```shell
2024-05-01 02:52:31.372751: W tensorflow/core/kernels/image/non_max_suppression_op.cc:998] Detected a large value for `max_total_size`. This may cause OOM error. (max_total_size: 1879048192)
2024-05-01 02:52:31.372808: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (2 vs. 0)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-05-01T02:59:49Z,1,0,https://github.com/tensorflow/tensorflow/issues/66757,Runtime Error
180,tensorflow/tensorflow,tf.raw_ops.StringSplitV2 does not limit a number of splits by max_split attribute,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.raw_ops.StringSplitV2 does not limit a number of splits by max_split attribute.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.raw_ops.StringSplitV2(input=['one<>'], sep='<>', maxsplit=1) # outputs two splits 'one' and '' instead of just one split

tf.raw_ops.StringSplitV2(input=['one<>two'], sep='<>', maxsplit=1) # outputs two splits 'one' and 'two' instead of just one split
```


### Relevant log output

```shell
>>> tf.raw_ops.StringSplitV2(input=['one<>'], sep='<>', maxsplit=1)
StringSplitV2(indices=<tf.Tensor: shape=(2, 2), dtype=int64, numpy=
array([[0, 0],
       [0, 1]])>, values=<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'one', b''], dtype=object)>, shape=<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 2])>)

>>> tf.raw_ops.StringSplitV2(input=['one<>two'], sep='<>', maxsplit=1)
StringSplitV2(indices=<tf.Tensor: shape=(2, 2), dtype=int64, numpy=
array([[0, 0],
       [0, 1]])>, values=<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'one', b'two'], dtype=object)>, shape=<tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 2])>)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-28T09:40:56Z,4,0,https://github.com/tensorflow/tensorflow/issues/66583,Logical Bug
181,tensorflow/tensorflow,"Profiler does not Seem to Output Timesteps in xplane.pb - ""No step marker observed and hence the step time is unknown"" from Tensorboard","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0   (cuda120py39hb94c71b_3  from conda-forge)

### Custom code

No

### OS platform and distribution

Ubuntu Jammy in podman Container

### Mobile device

_No response_

### Python version

3.9.18

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.4 (cuda-cupti 12.4.127  @ h59595ed_1   from conda-forge)

### GPU model and memory

RTX 3090, 24GiB

### Current behavior?

I am in the process of writing a [custom loss function]( https://github.com/stellarpower/Soft-DTW/tree/stellapower.AddFeature.BetterParallelisation ), and trying to profile it to see where resources are currently used.

I have installed newer CUDA drives, the latest release of Tensorflow (2.15), the CUDA PTI libraries, and other dependencies needed for the Tensorboard profiler plugin.

I can run an example with the profiler and get what looks like reasonable data. With my own code, I get a warning back from `_pywrap_profiler.xspace_to_tools_data()` that no timesteps are contained in the file, and thus, some of the useful profiling information is absent/unusable. I cut back the example and found that if I use the MSE loss, the profile is complete; if I change to my own loss function, the timesteps are no longer output.

Given that the message is coming back from the [core profiler library]( https://github.com/tensorflow/tensorflow/blob/v2.15.0/tensorflow/core/profiler/utils/diagnostics.cc#L62 ), and is contained within the encoded protocol buffers, I believe that this is an issue with the profiler and the main library, rather than the tensorboard utility or the profiling plugin.

The loss function is reasonably complicated, so I suspected at first the large files might be an issue. However, when reducing down to the toy example above, whilst there's a difference, the overhead in the files makes this difference much closer:
![image](https://github.com/tensorflow/tensorflow/assets/5004545/6fa3ee71-39fd-4791-a9a1-8f8d6f08d091)
I previously had warning s that the processor has dropped frames due to insufficient buffer space, but with this trivially small data size, those are gone.

### Standalone code to reproduce the issue

```shell
#!/usr/bin/env python
    
import os, sys, pprint, numpy
import tensorflow as tf


ScriptDirectory = os.path.dirname(os.path.realpath(__file__))

# https://github.com/stellarpower/Soft-DTW/tree/stellapower.AddFeature.BetterParallelisation
sys.path.insert(0, f""/path/to/Soft-DTW"")
from softdtwkeras.SDTWLoss import SDTWLoss
#############################################################


from tensorflow.keras import layers
from tensorflow import keras

from softdtwkeras.SDTWLoss import SDTWLoss
from datetime import datetime


## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Epochs = 128


ProfilingTensorboardLogsDirectory = f""{ ScriptDirectory }/BugLogs/{ datetime.now().strftime('%Y-%m-%d_%H.%M.%S') }""

tboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir = ProfilingTensorboardLogsDirectory,
    histogram_freq = 1,
    
    profile_batch = (32, 96) # Should be Middle 50%
)



## ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Very small toy data, in case the file size is an issue.
x_train = numpy.zeros((1, 1, 1))


model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(batch_input_shape=(1, 1, 1), name='layers_flatten'),
])

model.compile(
      optimizer = 'adam',
    # This runs okay:
      # loss      =  'mse',
    # This results in no timestep info in the xplane file.
        loss      = SDTWLoss(gamma=0.5, BatchSize = 1)
)


model.fit(
    x         = x_train, 
    y         = x_train, # << trivial identity function
    epochs    = Epochs,
    callbacks = [tboard_callback]
)
```


### Relevant log output

```shell
2024-04-24 23:32:23.290829: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-24 23:32:23.290868: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-24 23:32:23.291486: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-24 23:32:23.296288: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.



2024-04-24 23:32:26.851127: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-04-24 23:32:26.851153: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-04-24 23:32:26.857074: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1883] Profiler found 1 GPUs
2024-04-24 23:32:26.861348: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-04-24 23:32:26.861419: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:2017] CUPTI activity buffer flushed


2024-04-24 23:32:26.871326: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-04-24 23:32:26.903533: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-04-24 23:32:26.903741: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-04-24 23:32:26.906062: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-04-24 23:32:26.906217: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-04-24 23:32:26.906350: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-04-24 23:32:26.982483: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-04-24 23:32:26.982679: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-04-24 23:32:26.982827: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355


2024-04-24 23:32:26.982929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22199 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6


Epoch 1/128
1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00
...
Epoch 31/128
1/1 [==============================] - 0s 6ms/step - loss: 0.0000e+00
Epoch 32/128
2024-04-24 23:32:30.249189: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-04-24 23:32:30.249218: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
1/1 [==============================] - 0s 15ms/step - loss: 0.0000e+00
Epoch 33/128
1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00
...
Epoch 95/128
1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00
Epoch 96/128


2024-04-24 23:32:30.746769: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2024-04-24 23:32:30.747796: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:2017] CUPTI activity buffer flushed
2024-04-24 23:32:30.792759: I external/local_xla/xla/backends/profiler/gpu/cupti_collector.cc:541]  GpuTracer has collected 4216 callback api events and 3979 activity events. 
2024-04-24 23:32:30.840791: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-04-24 23:32:30.841246: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: /BugLogs/2024-04-24_23.32.26/plugins/profile/2024_04_24_23_32_30/the-alchemist.xplane.pb


1/1 [==============================] - 0s 120ms/step - loss: 0.0000e+00
Epoch 97/128
...
```
","['comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.15']",2024-04-24T23:48:35Z,4,0,https://github.com/tensorflow/tensorflow/issues/66410,Performance Issue
182,tensorflow/tensorflow,Denial of Service in tf.raw_ops.Unstage,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?

Hi,
When using `tf.raw_ops.Unstage`, a `Denial of Service` was encountered under normal invocation, with all passed parameters meeting the requirements of the API documentation.
After a long wait, there was still no response.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tensor = tf.constant([1.0, 2.0, 3.0])
stage_op = tf.raw_ops.Stage(values=[tensor])
print(tensor.dtype)  # <dtype: 'float32'>

unstage_op = tf.raw_ops.Unstage(dtypes=[tensor.dtype])  # Hang

with tf.compat.v1.Session() as sess:
    print(""here"")
    sess.run(stage_op)
    result = sess.run(unstage_op)

print(result)  # Expected output: [1. 2. 3.]
```


### Relevant log output

```shell
2024-04-22 02:23:12.166249: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-22 02:23:12.166825: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-22 02:23:12.169923: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-22 02:23:12.215234: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-22 02:23:13.066132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-22 02:23:13.789282: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
<dtype: 'float32'>
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-22T02:27:21Z,1,0,https://github.com/tensorflow/tensorflow/issues/66174,Security Vulnerability
183,tensorflow/tensorflow,tf.function deadlock with multiple multiprocess/threading,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.16

### Custom code

Yes


### Current behavior?

TensorFlow gets stuck when using multiprocessing/threading more than once.

I've observed it in more complicated situations with only once multithreading, but the following is a reproducible, standalone example that illustrates the point.

The code works correctly if either the `tf.function` decorator is removed _or_ if xla compilation `jit_compile` is enabled (!).

### Standalone code to reproduce the issue

```shell
import multiprocessing as mp  # can also be another module for multiprocess/threading
import random

import tensorflow as tf


# if we use jit_compile=True, it will work, magically
# it also works if there is no decorator at all, i.e. only eager mode
@tf.function(jit_compile=False)
def testjit(x):
    return tf.math.reduce_sum(x)


def make_zdata(_=None):
    print('Making data')
    # just to make sure that we recompile the function (different shapes)
    rnd = tf.random.uniform([random.randint(100, 10000)], -1, 1)
    zdata = testjit(rnd)
    print('Made data')
    return zdata

with mp.Pool(1) as executor:
    executor.map(make_zdata, [1])
executor.terminate()

# if we run this, it will fail (if jit_compile=False)
with mp.Pool(1) as executor:
    executor.map(make_zdata, [1])  # here, the code will be stuck
executor.terminate()
```


### Relevant log output

will be, approximately:
```
Making data
Made data
Making data
```
and then it's stuck. Otherwise, if using `jit_compile=True` or not using the tf.function decorator at all, another `Made data` will be printed","['stat:awaiting tensorflower', 'type:bug', 'comp:tf.function', 'TF 2.16']",2024-04-19T22:45:12Z,2,0,https://github.com/tensorflow/tensorflow/issues/66115,Runtime Error
184,tensorflow/tensorflow,Failing Tensorflow unit tests for BF16 hardware,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We have the following unit test failures in Tensorflow github/nightly:

//tensorflow/compiler/tests:conv3d_test_cpu
//tensorflow/compiler/tests:conv3d_test_cpu_mlir_bridge_test
//tensorflow/compiler/tests:stateful_random_ops_test_cpu
//tensorflow/compiler/tests:stateless_random_ops_test_cpu
//tensorflow/compiler/tests:stateless_random_ops_test_cpu_mlir_bridge_test
//tensorflow/compiler/tests:stateful_random_ops_test_cpu_mlir_bridge_test
//tensorflow/compiler/tests:stochastic_cast_op_test_cpu

On investigation, the first commit where this issue is present is https://github.com/tensorflow/tensorflow/commit/a4d7e978701feb9ed485b926b22676b9f7651d4e, tests pass with the commit immediately prior to this.

 The tests do not fail in the upstream CI because it uses N1 cores with no bf16 HW.



### Standalone code to reproduce the issue

```shell
From the directory tensorflow/ci/official, to reproduce the failure for e.g. //tensorflow/compiler/tests:conv3d_test_cpu we want to:
open any.sh and remove cd ""$(dirname ""$0"")/../../""  # tensorflow/
run:
TFCI=py311,linux_arm64  TF_ANY_MODE=test TF_ANY_TARGETS=//tensorflow/compiler/tests:conv3d_test_cpu  ./any.sh
```


### Relevant log output

```shell
An example error from the stochastic_cast test:

FAIL: //tensorflow/compiler/tests:stochastic_cast_op_test_cpu (shard 13 of 20) (see /root/.cache/bazel/_bazel_root/574657b8af23672198530ef061ba4201/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/tests/stochastic_cast_op_test_cpu/shard_13_of_20/test.log)
INFO: From Testing //tensorflow/compiler/tests:stochastic_cast_op_test_cpu (shard 13 of 20):
==================== Test output for //tensorflow/compiler/tests:stochastic_cast_op_test_cpu (shard 13 of 20):
2024-04-15 10:17:41.267151: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Running tests under Python 3.11.6: /root/.cache/bazel/_bazel_root/574657b8af23672198530ef061ba4201/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/tests/stochastic_cast_op_test_cpu.runfiles/python_aarch64-unknown-linux-gnu/bin/python3
WARNING:tensorflow:From /root/.cache/bazel/_bazel_root/574657b8af23672198530ef061ba4201/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/tests/stochastic_cast_op_test_cpu.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py:106: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.
Instructions for updating:
XLA:CPU and XLA:GPU devices are deprecated
W0415 10:17:44.369471 247748062629904 deprecation.py:50] From /root/.cache/bazel/_bazel_root/574657b8af23672198530ef061ba4201/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/tensorflow/compiler/tests/stochastic_cast_op_test_cpu.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py:106: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.
Instructions for updating:
XLA:CPU and XLA:GPU devices are deprecated
[ RUN      ] StochasticCastOpTest.testStochasticCastOpResultProbability_0.125_from_bfloat16_to_int16
INFO:tensorflow:Start test case: StochasticCastOpTest.testStochasticCastOpResultProbability_0.125_from_bfloat16_to_int16
I0415 10:17:44.370704 247748062629904 xla_test.py:231] Start test case: StochasticCastOpTest.testStochasticCastOpResultProbability_0.125_from_bfloat16_to_int16
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1713176264.392647   68553 service.cc:145] XLA service 0xbd1261c1dc00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1713176264.392683   68553 service.cc:153]   StreamExecutor device (0): Host, Default Version
2024-04-15 10:17:44.398999: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled
I0000 00:00:1713176264.411515   73786 xla_device.cc:462] XLA_GPU and XLA_CPU devices are deprecated and will be removed in subsequent releases. Instead, use either @tf.function(jit_compile=True) for must-compile semantics, or run with TF_XLA_FLAGS=--tf_xla_auto_jit=2 for auto-clustering best-effort compilation.
I0000 00:00:1713176264.457087   73787 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
LLVM ERROR: Cannot select: 0xe14f2452b580: v8bf16,ch = masked_load<(load unknown-size from %ir.lsr.iv217, align 2, !alias.scope !4, !noalias !7)> 0xe14f241a0ea0, 0xe14f2452bd60, undef:i64, 0xe14f244faa10, undef:v8bf16
  0xe14f2452bd60: i64,ch = CopyFromReg 0xe14f241a0ea0, Register:i64 %65
    0xe14f2452d220: i64 = Register %65
  0xe14f2452b9e0: i64 = undef
  0xe14f244faa10: v8i16 = AArch64ISD::VASHR 0xe14f24529720, Constant:i32<15>
    0xe14f24529720: v8i16 = AArch64ISD::VSHL 0xe14f2454cb00, Constant:i32<15>
      0xe14f2454cb00: v8i16 = any_extend 0xe14f2452b970
        0xe14f2452b970: v8i8,ch = CopyFromReg 0xe14f241a0ea0, Register:v8i8 %66
          0xe14f2452bdd0: v8i8 = Register %66
      0xe14f245602e0: i32 = Constant<15>
    0xe14f245602e0: i32 = Constant<15>
  0xe14f2452be40: v8bf16 = undef
In function: parallel_fusion
Fatal Python error: Aborted
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'comp:ops']",2024-04-18T09:41:03Z,7,0,https://github.com/tensorflow/tensorflow/issues/65988,Dependency Issue
185,tensorflow/tensorflow,Segmentation fault in `tf.config.experimental_connect_to_cluster` with `@tf.function`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?

When configuring `tf.config.experimental_connect_to_cluster` and using the `@tf.function` decorator on a function, 
even though the function's content is `pass`, a `Segmentation fault (core dumped)` still occurs.

When the `@tf.function` decorator is commented out, the program executes with a return code of 0 and no errors occur.


### Standalone code to reproduce the issue

```shell
This simple code repeats the problem:

import tensorflow as tf

cluster = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                           ""worker1.example.com:2222"",
                                           ""worker2.example.com:2222""],
                                ""ps"": [""ps0.example.com:2222"",
                                       ""ps1.example.com:2222""]})

tf.config.experimental_connect_to_cluster(
    cluster,
    job_name='worker',
    task_index=0
)


@tf.function
def test():
    pass

test()
```


### Relevant log output

```shell
The error message I got:

2024-04-15 13:29:11.263846: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-15 13:29:11.264202: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-15 13:29:11.267063: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-04-15 13:29:11.305386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-15 13:29:11.992670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-04-15 13:29:12.465636: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
E0415 13:29:12.509477560 2432185 server_chttp2.cc:40]        {""created"":""@1713187752.509411414"",""description"":""No address added out of total 1 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":395,""referenced_errors"":[{""created"":""@1713187752.509406740"",""description"":""Failed to add any wildcard listeners"",""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":342,""referenced_errors"":[{""created"":""@1713187752.509393568"",""description"":""Unable to configure socket"",""fd"":15,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1713187752.509389969"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]},{""created"":""@1713187752.509406103"",""description"":""Unable to configure socket"",""fd"":15,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1713187752.509403589"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}
2024-04-15 13:29:12.509541: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server
2024-04-15 13:29:12.509869: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:450] Could not start gRPC server
Segmentation fault (core dumped)
```
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-16T10:53:07Z,2,0,https://github.com/tensorflow/tensorflow/issues/65835,Runtime Error
186,tensorflow/tensorflow,`Segmentation fault` in `tf.raw_ops.RaggedRange` and `tf.ragged.range` when `starts` or `limits` has large (postive/negative) values.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`Segmentation fault` in `tf.raw_ops.RaggedRange` and `tf.ragged.range` when `starts` or `limits` has large (postive/negative) values.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1c5J_tSNwLdPmi0TXMaJNNhUb-Ha__dw8?usp=sharing
```


### Relevant log output

```shell
segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-04-16T05:05:06Z,1,0,https://github.com/tensorflow/tensorflow/issues/65790,Runtime Error
187,tensorflow/tensorflow,`Segmentation fault` in `tf.raw_ops.CollectiveGather` when `input` is empty.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`Segmentation fault` in `tf.raw_ops.CollectiveGather` when `input` is empty.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1lx42HhXrkx5BPwSxLg-bGuJMMCTEIQyt?usp=sharing
```


### Relevant log output

```shell
segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-16T04:43:40Z,1,0,https://github.com/tensorflow/tensorflow/issues/65785,Runtime Error
188,tensorflow/tensorflow,`Segmentation fault` in `tf.raw_ops.LoadAndRemapMatrix` when the value of `num_cols` is too large while `col_remapping` is empty.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value of `num_cols` is too large while `col_remapping` is empty, `tf.raw_ops.LoadAndRemapMatrix` triggers `Segmentation fault`.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1Eba1FTUMG_as_2FtrSUcAt44Z-LWPX4Z?usp=sharing
```


### Relevant log output

```shell
segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-15T16:19:28Z,3,0,https://github.com/tensorflow/tensorflow/issues/65732,Runtime Error
189,tensorflow/tensorflow,`Check failed` in `tf.raw_ops.LoadAndRemapMatrix` when the rank of `ckpt_path` is not equal to the rank of `old_tensor_name`.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the rank of `ckpt_path` is not equal to the rank of `old_tensor_name`, `tf.raw_ops.LoadAndRemapMatrix` triggers `Check fail` error.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1Eba1FTUMG_as_2FtrSUcAt44Z-LWPX4Z?usp=sharing
```


### Relevant log output

```shell
Check failed: NDIMS == dims() (1 vs. 0)Asking for tensor of 1 dimensions from a tensor of 0 dimensions
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-15T16:04:05Z,1,0,https://github.com/tensorflow/tensorflow/issues/65731,Runtime Error
190,tensorflow/tensorflow,`Overflow` in `tf.raw_ops.SerializeManySparse` when there are too large values in `sparse_shape`.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

 When there are too large values in `sparse_shape`  while `sparse_indices` and `sparse_values` are empty, ``tf.raw_ops.SerializeManySparse`` triggers overflow error.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1itdHdvRiR28lfLLBay-2sUuKSX_IzBPC?usp=sharing
```


### Relevant log output

```shell
Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 7777777777777777777 with 3, result: -1
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-15T14:50:33Z,2,0,https://github.com/tensorflow/tensorflow/issues/65727,Logical Bug
191,tensorflow/tensorflow,Some errors in `tf.raw_ops.SparseTensorDenseMatMul` when there are negative or too large values in `a_shape`.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When there are negative values in `a_shape`, `tf.raw_ops.SparseTensorDenseMatMul` triggers `INVALID_ARGUMENT` error. When there are too large values in `a_shape`, `tf.raw_ops.SparseTensorDenseMatMul` triggers `overflow` error.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1_dmg1zT1TuYnmPgtlUWSDq8Gg4eeyq0b?usp=sharing
```


### Relevant log output

```shell
Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Expected shape dimensions to be non-negative, got -1

or

Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 9223372036854775804 with 3, result: -1
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-15T14:40:25Z,2,0,https://github.com/tensorflow/tensorflow/issues/65724,Logical Bug
192,tensorflow/tensorflow,`Check failed` in `f.raw_ops.CropAndResizeGradBoxes` when `boxes` and `box_indices` are empty. ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`Check failed` in `f.raw_ops.CropAndResizeGradBoxes` when `boxes` and `box_indices` are empty, which causes the program to crash. See the colab link below for details.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1jt4YeBZkcM_o_tqxDsIKePHoeFZ-CdCS?usp=sharing
```


### Relevant log output

```shell
Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-04-15T12:48:08Z,1,0,https://github.com/tensorflow/tensorflow/issues/65721,Runtime Error
193,tensorflow/tensorflow,Memory leak in tf.data when iterating over Dataset.from_generator,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-108954-g88310ddcbdd 2.17.0-dev20240412

### Custom code

Yes

### OS platform and distribution

Docker: tensorflow/tensorflow:nightly

### Mobile device

_No response_

### Python version

3.11.0rc1

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I discovered what appears to be a memory leak when iterating over a tf.data.Dataset created with from_generator. Process memory usage continues to grow out of hand. The effect only appears in certain combinations of Tensorflow and Python and it may have appeared in Python 3.11. Here are some examples I've tested:

Python 3.10.10, tensorflow 2.13.0: yes
Python 3.10.10, tensorflow 2.16.1: no
Python 3.10.12, tensorflow v2.15.0-0-g6887368d6d4: no
Python 3.11, tensorflow 2.16.1: yes
Python: 3.11.0rc1, tensorflow v1.12.1-109002-g2c2c0a17f05: yes

Maybe related to https://docs.python.org/3/whatsnew/3.11.html#faster-cpython? I thought maybe Python is re-using the memory and not freeing it, but usage grows ridiculously (I noticed it because it started taking up tens of GB in one case) and it seems like it shouldn't with a generator. Odd that 2.13.0 experiences the problem with Python 3.10.10 too though.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1LmdIqWME19GLFG0E7dsCtRtscLLFF89R?usp=sharing
```


### Relevant log output

```shell
2024-04-14 16:12:10.319053: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Python: 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]
TensorFlow: ('v1.12.1-109002-g2c2c0a17f05', '2.17.0-dev20240413')
Iterations:        10000 Memory use: 489308160
Iterations:        20000 Memory use: 496451584
Iterations:        30000 Memory use: 503353344
Iterations:        40000 Memory use: 510517248
Iterations:        50000 Memory use: 517464064
Iterations:        60000 Memory use: 524591104
...
Iterations:       980000 Memory use: 1173454848
Iterations:       990000 Memory use: 1180618752
Iterations:      1000000 Memory use: 1187770368
2024-04-14 16:19:00.851851: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
379



Output from running valgrind:
PYTHONMALLOC=malloc valgrind --track-origins=yes --leak-check=full --show-leak-kinds=definite --trace-children=yes python ./tfdata_test.py
I stopped it at about 80,000 iterations because it's much slower running under valgrind
==324== 24,434,160 (4,660,032 direct, 19,774,128 indirect) bytes in 72,813 blocks are definitely lost in loss record 188,561 of 188,562
==324==    at 0x4848899: malloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==324==    by 0x4D4AC1: ??? (in /usr/bin/python3.11)
==324==    by 0x5BA613: ??? (in /usr/bin/python3.11)
==324==    by 0x5BB4C4: ??? (in /usr/bin/python3.11)
==324==    by 0x4FE1B5: _PyEval_EvalFrameDefault (in /usr/bin/python3.11)
==324==    by 0x531822: _PyFunction_Vectorcall (in /usr/bin/python3.11)
==324==    by 0x530FB8: ??? (in /usr/bin/python3.11)
==324==    by 0x5DA693: _PyObject_CallMethod_SizeT (in /usr/bin/python3.11)
==324==    by 0x26C7748A: _descriptor_from_pep3118_format (in /usr/local/lib/python3.11/dist-packages/numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so)
==324==    by 0x26C8CBD5: _array_from_buffer_3118.part.0 (in /usr/local/lib/python3.11/dist-packages/numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so)
==324==    by 0x26C8DF4E: _array_from_array_like (in /usr/local/lib/python3.11/dist-packages/numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so)
==324==    by 0x26C6FCD4: PyArray_DiscoverDTypeAndShape_Recursive (in /usr/local/lib/python3.11/dist-packages/numpy/core/_multiarray_umath.cpython-311-x86_64-linux-gnu.so)
==324==
==324== 26,302,320 bytes in 73,062 blocks are definitely lost in loss record 188,562 of 188,562
==324==    at 0x484DA83: calloc (in /usr/libexec/valgrind/vgpreload_memcheck-amd64-linux.so)
==324==    by 0x625128: ??? (in /usr/bin/python3.11)
==324==    by 0x6250BA: PyThreadState_New (in /usr/bin/python3.11)
==324==    by 0x64E44F: PyGILState_Ensure (in /usr/bin/python3.11)
==324==    by 0xA597C58: tensorflow::PyFuncOp::Compute(tensorflow::OpKernelContext*) (in /usr/local/lib/python3.11/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so)
==324==    by 0x92F0F3F: tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) (in /usr/local/lib/python3.11/dist-packages/tensorflow/libtensorflow_framework.so.2)
==324==    by 0x927A539: tensorflow::(anonymous namespace)::ExecutorState<tensorflow::SimplePropagatorState>::Process(tensorflow::SimplePropagatorState::TaggedNode const&, long) (in /usr/local/lib/python3.11/dist-packages/tensorflow/libtensorflow_framework.so.2)
==324==    by 0x9A3D39D: std::_Function_handler<void (), std::_Bind<tensorflow::data::RunnerWithMaxParallelism(std::function<void (std::function<void ()>)>, int)::$_0::operator()(std::function<void (std::function<void ()>)> const&, std::function<void ()>) const::{lambda(std::function<void ()> const&)#1} (std::function<void ()>)> >::_M_invoke(std::_Any_data const&) (in /usr/local/lib/python3.11/dist-packages/tensorflow/libtensorflow_framework.so.2)
==324==    by 0x9C2299F: Eigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int) (in /usr/local/lib/python3.11/dist-packages/tensorflow/libtensorflow_framework.so.2)
==324==    by 0x9C223D0: void std::__invoke_impl<void, tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}&>(std::__invoke_other, tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}&) (in /usr/local/lib/python3.11/dist-packages/tensorflow/libtensorflow_framework.so.2)
==324==    by 0x960E55A: tsl::(anonymous namespace)::PThread::ThreadFn(void*) (in /usr/local/lib/python3.11/dist-packages/tensorflow/libtensorflow_framework.so.2)
==324==    by 0x4A27AC2: start_thread (pthread_create.c:442)
==324==
==324== LEAK SUMMARY:
==324==    definitely lost: 30,984,576 bytes in 146,328 blocks
==324==    indirectly lost: 19,783,920 bytes in 72,736 blocks
==324==      possibly lost: 136,684,089 bytes in 1,082,632 blocks
==324==    still reachable: 17,341,568 bytes in 182,582 blocks
==324==                       of which reachable via heuristic:
==324==                         stdstring          : 341 bytes in 8 blocks
==324==                         newarray           : 104,387 bytes in 320 blocks
==324==                         multipleinheritance: 7,168 bytes in 24 blocks
==324==         suppressed: 0 bytes in 0 blocks
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.16']",2024-04-14T19:02:53Z,5,1,https://github.com/tensorflow/tensorflow/issues/65675,Performance Issue
194,tensorflow/tensorflow,Some `Check Failed` errors in `tf.raw_ops.MatrixSetDiagV2`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Some `Check Failed` errors in `tf.raw_ops.MatrixSetDiagV2`, which causes the program to crash. See the colab link below for details.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1-yHRqhkbV3J77AaBD4GXYh3nSF49Inep?usp=sharing
```


### Relevant log output

```shell
# Check failed: d < dims() (1 vs. 1)
# Check failed: d < dims() (2 vs. 2)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-14T17:41:03Z,1,0,https://github.com/tensorflow/tensorflow/issues/65674,Runtime Error
195,tensorflow/tensorflow,Some `Check Failed` errors in `tf.raw_ops.MatrixSetDiagV3`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Some `Check Failed` errors in `tf.raw_ops.MatrixSetDiagV3`, which causes the program to crash. See the colab link below for details.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/11e72xipp_lz6fHcp7bN2MQy3Kw5KqHpL?usp=sharing
```


### Relevant log output

```shell
# Check failed: d < dims() (1 vs. 1)
# Check failed: d < dims() (2 vs. 2)
```
","['awaiting review', 'type:bug', 'comp:ops', 'TF 2.15']",2024-04-14T17:33:52Z,1,0,https://github.com/tensorflow/tensorflow/issues/65673,Runtime Error
196,tensorflow/tensorflow,`Segmentation Fault` in `tf.raw_ops.TensorListScatter` and `tf.raw_ops.TensorListScatterV2` when the value of `indices` is too large.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the value of `indices` is too large,  `tf.raw_ops.TensorListScatter` and `tf.raw_ops.TensorListScatterV2` triggers `Segmentation Fault` due to RAM being full.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1BOVVbofLSB2x_chg_WuFIdTWZXS9hiw0?usp=sharing
```


### Relevant log output

```shell
segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-04-14T17:14:00Z,1,0,https://github.com/tensorflow/tensorflow/issues/65672,Runtime Error
197,tensorflow/tensorflow,`Check failed` in `tf.raw_ops.TensorScatterSub` and `tf.tensor_scatter_nd_sub` when the rank of `indices` > 2.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`Check failed` in `tf.raw_ops.TensorScatterSub` and `tf.tensor_scatter_nd_sub` when the rank of `indices` > 2, which causes the program to crash. See the colab link below for details.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1wqe_qe71oLucYiq1WS8s9GkZiVsNlaMz?usp=sharing
```


### Relevant log output

```shell
Check failed: d < dims() (1 vs. 1)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-14T16:41:30Z,1,0,https://github.com/tensorflow/tensorflow/issues/65670,Runtime Error
198,tensorflow/tensorflow,`Check failed` in `tf.raw_ops.TensorScatterUpdate` and `tf.tensor_scatter_nd_update` when the rank of `indices` > 2.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`Check failed` in `tf.raw_ops.TensorScatterUpdate` and `tf.tensor_scatter_nd_update` when the rank of `indices` > 2, which causes the program to crash. See the colab link below for details.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1PLVxv-jQDZ15P4lrd3AhyVU-RAesammI?usp=sharing
```


### Relevant log output

```shell
Check failed: d < dims() (1 vs. 1)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-04-14T16:11:54Z,1,0,https://github.com/tensorflow/tensorflow/issues/65667,Runtime Error
199,tensorflow/tensorflow,`Check failed` in `tf.raw_ops.ScatterNd` and `tf.scatter_nd` when the rank of `indices` > 2. ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`Check failed` in `tf.raw_ops.ScatterNd` and `tf.scatter_nd` when the rank of `indices` > 2, which causes the program to crash. See the colab link below for details.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1rr8w5IOzVdjs07YECnyOsnFu3ouUFkBs?usp=sharing
```


### Relevant log output

```shell
Check failed: d < dims() (1 vs. 1)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-14T15:25:42Z,1,0,https://github.com/tensorflow/tensorflow/issues/65666,Runtime Error
200,tensorflow/tensorflow,`segmentation fault` in `tf.raw_ops.AddManySparseToTensorsMap` when `sparse_indices` and `sparse_values` are empty and `sparse_shape` is too large.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`segmentation fault` in `tf.raw_ops.AddManySparseToTensorsMap` when `sparse_indices` and `sparse_values` are empty and `sparse_shape` is too large, which causes the program to crash. See the colab link below for details.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1KNuiyCMsSo32teT3lNwYyYbQkD9tEBET?usp=sharing
```


### Relevant log output

```shell
segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-04-14T04:28:10Z,1,0,https://github.com/tensorflow/tensorflow/issues/65647,Runtime Error
201,tensorflow/tensorflow,`Check failed` in `tf.raw_ops.UnbatchGrad` when the input of `original_input` is empty.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the input of `original_input` is empty,  `tf.raw_ops.UnbatchGrad` will trigger `Check failed` of the input tensor dimension.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1ekseH5mJBvt3_SeK4MuAAM47U8qi5BEB?usp=sharing
```


### Relevant log output

```shell
Check failed: d < dims() (0 vs. 0)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-13T18:25:26Z,1,0,https://github.com/tensorflow/tensorflow/issues/65636,Runtime Error
202,tensorflow/tensorflow,Some `Check Failed` errors in `tf.raw_ops.Unbatch`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Some `Check Failed` errors in `tf.raw_ops.Unbatch`, which causes the program to crash. See the colab link below for details.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1JRw_-UwodqwPx87naMM04nnFLaLaZcXn?usp=sharing
```


### Relevant log output

```shell
Check failed: size >= 0 (0 vs. -1) Aborted (core dumped)
Check failed: d < dims() (1 vs. 1) Aborted (core dumped)
Check failed: new_num_elements == NumElements() (6 vs. 18) Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-04-13T17:50:28Z,2,0,https://github.com/tensorflow/tensorflow/issues/65633,Runtime Error
203,tensorflow/tensorflow,"tf.raw_ops.Unbatch aborts with ""Check failed: d < dims()""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.3 LTS

### Mobile device

_No response_

### Python version

Python 3.11.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.Unbatch` aborts with ""Check failed: d < dims()""

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.raw_ops.Unbatch(batched_tensor=tf.random.uniform([], dtype=tf.dtypes.float32, maxval=100000000),batch_index=tf.random.uniform([1], dtype=tf.dtypes.int64, minval=-100000000, maxval=1000000000),id=tf.random.uniform([4], dtype=tf.dtypes.int64, minval=-100000000, maxval=1000000000),timeout_micros=30,container=""N/J,"",shared_name=""&o0n7W"",)
```


### Relevant log output

```shell
2024-04-10 06:36:02.435578: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-10T06:56:36Z,6,0,https://github.com/tensorflow/tensorflow/issues/65379,Runtime Error
204,tensorflow/tensorflow,[DOCS] Missing complex input for Round op,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.15.0

### Custom code

No

### OS platform and distribution

MacOS Sonoma

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

While testing the `Round` operation, I noticed an inconsistency with the [documentation](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Round). On the official page, it's described that a complex tensor can be passed as input. However, when I try to replicate this in my environment, it doesn't work as expected. I have to apply the `Round` operation separately to the real and imaginary parts of the tensor to achieve the desired result.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([1+2j, 3+4j])
rounded = tf.raw_ops.Round(x=x)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/lucatam/Documents/OpenSourceProject/openvino/opvenv/lib/python3.9/site-packages/tensorflow/python/util/tf_export.py"", line 403, in wrapper
    return f(**kwargs)
  File ""/Users/lucatam/Documents/OpenSourceProject/openvino/opvenv/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 8770, in round
    _ops.raise_from_not_ok_status(e, name)
  File ""/Users/lucatam/Documents/OpenSourceProject/openvino/opvenv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 5883, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.NotFoundError: Could not find device for node: {{node Round}} = Round[T=DT_COMPLEX128]
All kernels registered for op Round:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]
 [Op:Round] name:
```
","['type:docs-bug', 'awaiting review', 'type:bug', 'comp:ops', 'TF 2.16']",2024-04-09T10:54:25Z,1,1,https://github.com/tensorflow/tensorflow/issues/65317,UI/UX Bug
205,tensorflow/tensorflow,`SystemError` in `tf.ensure_shape` and `tf.compat.v1.ensure_shape` when `dtype` of `shape` is `tf.uint64` and its value is too large.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When the `dtype` of `shape` in `tf.ensure_shape` or `tf.v1.ensure_shape` is `tf.uint64` and its value is close to 2^64,`SystemError` and `OverflowError` will be triggered.
Taking `shape = tf.constant([18446743219011059112, 1], dtype=tf.uint64)` as an example, in eager mode, these APIs will trigger a `SystemError` when it is called. I think it is a bug.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1sy4yzSx-n6HqS9oMvPv46WasMtMsCU_R?usp=sharing
```


### Relevant log output

```shell
SystemError: <built-in function isinstance> returned a result with an exception set
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-04-02T12:46:59Z,5,0,https://github.com/tensorflow/tensorflow/issues/64917,Runtime Error
206,tensorflow/tensorflow,Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed. Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0.post1

### Custom code

Yes

### OS platform and distribution

Rocky Linux 8.9

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

8.9.2.26

### GPU model and memory

 NVIDIA A100-SXM4-80GB 

### Current behavior?

I am trying to run a simple denoising autoencoder. my training data and label data are 900 samples of healpy maps with nside 64 resolution, loaded as numpy array. After normalising the maps, I used tf.data.Dataset.from_tensor_slices, to create dataset. when I used random noise to create these maps and ran on jupyter notebook, although took ages to initiate training after doing model.fit(), but it did run and produced some result. knowing that the model works, I tried to run on GPU with real data. this is where the issue started.  it shows the following error: Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed. Aborted (core dumped), and the process stops. 

### Standalone code to reproduce the issue

```shell
Here is a colab link:

https://colab.research.google.com/drive/1odbf3gQT9h-DI6zdh5e1llmG9zhjY45l?usp=sharing

It runs on colab. but it doesn't run in the terminal.
```


### Relevant log output

```shell
2024-03-27 12:14:07.288975: F external/local_tsl/tsl/platform/default/env.cc:74] Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed.
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.15']",2024-03-28T16:15:23Z,4,3,https://github.com/tensorflow/tensorflow/issues/64681,Runtime Error
207,tensorflow/tensorflow,Aborted (core dumped) with `tf.raw_ops.LoadAndRemapMatrix`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16.1

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.tf.raw_ops.LoadAndRemapMatrix` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

num_rows = 7
num_cols = 16
max_rows_in_memory = -1
ckpt_path = tf.constant(""/tmp"", shape=[1], dtype=tf.string)
old_tensor_name = tf.constant(""/tmp"", shape=[1], dtype=tf.string)
row_remapping = tf.constant(-1, shape=[7], dtype=tf.int64)
col_remapping = tf.constant(0, shape=[], dtype=tf.int64)
initializing_values = tf.constant(42, shape=[112], dtype=tf.float32)
tf.raw_ops.LoadAndRemapMatrix(ckpt_path=ckpt_path, old_tensor_name=old_tensor_name, row_remapping=row_remapping, col_remapping=col_remapping, initializing_values=initializing_values, num_rows=num_rows, num_cols=num_cols, max_rows_in_memory=max_rows_in_memory)
```


### Relevant log output

```shell
2024-03-28 07:29:28.849010: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 0)Asking for tensor of 1 dimensions from a tensor of 0 dimensions
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-03-28T07:38:24Z,1,0,https://github.com/tensorflow/tensorflow/issues/64655,Runtime Error
208,tensorflow/tensorflow,Conv2D computes wrongly in Windows OS,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.16

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On Windows OS, Conv2D generates a wrong output in some cases, while performs correctly on some others.
This error does not occur on Linux OS, even with the same code.

An wrong execution example:
![图片](https://github.com/tensorflow/tensorflow/assets/97206823/51fab71a-9bc6-42a9-ab67-19a6c9ec50d0)

You can tell that the result `l(x)` has a wrong shape.

I notice that an exisiting issue #63860 points out the similar error in Conv3D. I guess Conv2D and Conv3D have similar problem since they have the same parent class BaseConv.

### Standalone code to reproduce the issue

```shell
#This test case works fine on linux OS, while goes wrongly on Windows.


from keras.layers import Conv2D
import numpy as np

x=np.random.rand(1,2,2,1)
print(l(x).shape)
print(l.compute_output_shape(x.shape))
```


### Relevant log output

```shell
TensorShape([1, 2, 2, 1])
(1, 0, 0, 1)
```
","['stat:awaiting tensorflower', 'type:bug', 'subtype:windows', 'subtype:cpu-intel', 'TF 2.16']",2024-03-25T07:25:12Z,7,0,https://github.com/tensorflow/tensorflow/issues/64396,Logical Bug
209,tensorflow/tensorflow,Improper use of PluggableDevice data pointer,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

Linux Arch

### Mobile device

_No response_

### Python version

3.11.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

TensorListFromTensor op assumes that the data pointer from a PluggableDevice tensor is to virtual address space and can add an offset to it.
https://github.com/tensorflow/tensorflow/blob/39cb3719012c249ba40bffc4cebaad66311a4669/tensorflow/core/kernels/list_kernels.h#L805-L821

The assumption that all PluggableDevices set the data pointer of Tensor to be virtual address space is false.
https://github.com/tensorflow/tensorflow/blob/39cb3719012c249ba40bffc4cebaad66311a4669/tensorflow/c/experimental/stream_executor/stream_executor.h#L144-L146

Unfortunately, this is not the only occurrence of a bug like this. I have seen similar issues with distributed training offsetting PluggableDevice data pointers on copy as well but I have not taken the time to look into it yet.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

tf.debugging.set_log_device_placement(True)
tf.config.run_functions_eagerly(False)
a = np.zeros((8, 1), dtype=np.float32)

with tf.device(""MY_DEVICE:0""):
    b = tf.raw_ops.TensorListFromTensor(tensor=a, element_shape=[1])

print(b)
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:runtime', 'comp:core', 'TF 2.15']",2024-03-23T23:27:55Z,2,0,https://github.com/tensorflow/tensorflow/issues/64372,Dependency Issue
210,tensorflow/tensorflow,"tf.raw_ops.ApproximateEqual/Erfinv fail support a few data types, inconsistent with the documentation","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi, I want to report two issues: both tf.raw_ops.ApproximateEqual and tf.raw_ops.Erfinv can only support float32, float64, and double and fail on other datatype with the error message `NotFoundError: Could not find device for node: {{node xxx}} = xxx[T=DT_XXX]`

These behaviors are inconsistent with related documentations of these two APIs:

https://www.tensorflow.org/api_docs/python/tf/raw_ops/ApproximateEqual
https://www.tensorflow.org/api_docs/python/tf/raw_ops/Erfinv


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import warnings
warnings.filterwarnings(""ignore"")
dtype_list = [
    'bfloat16', 'bool', 'complex128', 'complex64',
    'double', 'float16', 'float32',
    'float64', 'half', 'int16', 'int32', 'int64', 'int8',
    'uint16', 'uint32', 'uint64', 'uint8'
]
for dtype in dtype_list:
    x = tf.constant(np.random.randint(-50, 50, ()), dtype=dtype)
    y = tf.constant(np.random.randint(-50, 50, ()), dtype=dtype)
    try:
        out = tf.raw_ops.ApproximateEqual(x=x,y=y)
        print(f""ApproximateEqual Success on dtype: {dtype}"")
    except:
        print(f""ApproximateEqual Fail on dtype: {dtype}"")

import tensorflow as tf
import numpy as np
import warnings
warnings.filterwarnings(""ignore"")
dtype_list = [
    'bfloat16', 'bool', 'complex128', 'complex64',
    'double', 'float16', 'float32',
    'float64', 'half', 'int16', 'int32', 'int64', 'int8',
    'uint16', 'uint32', 'uint64', 'uint8'
]
for dtype in dtype_list:
    x = tf.constant(np.random.randint(-50, 50, ()), dtype=dtype)
    try:
        out = tf.raw_ops.Erfinv(x=x)
        print(f""Erfinv Success on dtype: {dtype}"")
    except:
        print(f""Erfinv Fail on dtype: {dtype}"")
```


### Relevant log output

```shell
ApproximateEqual Fail on dtype: bfloat16
ApproximateEqual Fail on dtype: bool
ApproximateEqual Fail on dtype: complex128
ApproximateEqual Fail on dtype: complex64
ApproximateEqual Success on dtype: double
ApproximateEqual Fail on dtype: float16
ApproximateEqual Success on dtype: float32
ApproximateEqual Success on dtype: float64
ApproximateEqual Fail on dtype: half
ApproximateEqual Fail on dtype: int16
ApproximateEqual Fail on dtype: int32
ApproximateEqual Fail on dtype: int64
ApproximateEqual Fail on dtype: int8
ApproximateEqual Fail on dtype: uint16
ApproximateEqual Fail on dtype: uint32
ApproximateEqual Fail on dtype: uint64
ApproximateEqual Fail on dtype: uint8

Erfinv Fail on dtype: bfloat16
Erfinv Fail on dtype: bool
Erfinv Fail on dtype: complex128
Erfinv Fail on dtype: complex64
Erfinv Success on dtype: double
Erfinv Fail on dtype: float16
Erfinv Success on dtype: float32
Erfinv Success on dtype: float64
Erfinv Fail on dtype: half
Erfinv Fail on dtype: int16
Erfinv Fail on dtype: int32
Erfinv Fail on dtype: int64
Erfinv Fail on dtype: int8
Erfinv Fail on dtype: uint16
Erfinv Fail on dtype: uint32
Erfinv Fail on dtype: uint64
Erfinv Fail on dtype: uint8
```
","['awaiting review', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-22T05:21:16Z,4,0,https://github.com/tensorflow/tensorflow/issues/64256,UI/UX Bug
211,tensorflow/tensorflow,Aborted (core dumped) in tf.raw_ops.SparseMatrixZeros,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

7.5.0

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.SparseMatrixZeros` encounters ""Aborted (core dumped)"".
However, the parameters `dense_shape` and `type` meet the requirements in the API documentation. (https://tensorflow.google.cn/api_docs/python/tf/raw_ops/SparseMatrixZeros)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
dense_shape=tf.constant(value=np.random.randint(0,100,size=(1, 1)), shape=(1, 1), dtype=tf.int64)
type=tf.float64
name=None
tf.raw_ops.SparseMatrixZeros(dense_shape=dense_shape, type=type, name=name)
```


### Relevant log output

```shell
2024-03-19 02:18:57.223847: F tensorflow/core/framework/tensor_shape.cc:45] Check failed: NDIMS == dims() (1 vs. 2)Asking for tensor of 1 dimensions from a tensor of 2 dimensions
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2024-03-19T02:26:44Z,2,0,https://github.com/tensorflow/tensorflow/issues/63937,Runtime Error
212,tensorflow/tensorflow,Conv3D Operation Error on Windows platform,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf-2.15.0, tf-2.16.0, tf-2.16.1

### Custom code

Yes

### OS platform and distribution

Windows 10

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I found an output shape inconsistent in Conv3D layer:
```
from tensorflow.keras.layers import Conv3D
import numpy as np

x=np.random.rand(3,5,5,5,4)
l=Conv3D(5,[2,2,3],[1,1,1],'valid','channels_first',[2,2,2],1)
print(l(x).shape)
print(l.compute_output_shape(x.shape))
```
The output is:
```
(3, 5, 5, 5, 4)
(3, 5, 3, 3, 0)
```

However, I failed to reproduce it on google colab. After checking the package and re-running it on a Linux machine and another Windows machine, I found this bug may come from the package 'tensorflow-intel', which exists only on Windows platform.

Could you please check it?

### Standalone code to reproduce the issue

```shell
from tensorflow.keras.layers import Conv3D
import numpy as np

x=np.random.rand(3,5,5,5,4)
l=Conv3D(5,[2,2,3],[1,1,1],'valid','channels_first',[2,2,2],1)
print(l(x).shape)
print(l.compute_output_shape(x.shape))
```
```


### Relevant log output

```shell
(3, 5, 5, 5, 4)
(3, 5, 3, 3, 0)
```
","['stat:awaiting tensorflower', 'type:bug', 'subtype:cpu-intel', 'TF 2.15']",2024-03-18T12:26:37Z,2,0,https://github.com/tensorflow/tensorflow/issues/63860,Logical Bug
213,tensorflow/tensorflow,"tf.linalg.svd([inf, -inf]) throws internal error logs to the console, and generates inconsistent results on different dtypes.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15.0

### Custom code

Yes

### OS platform and distribution

Linux-5.14.0-362.18.1.el9_3.x86_64-x86_64-with-glibc2.34

### Mobile device

AlmaLinux 9

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

An internal error log was thrown to the console.
Also, we found that API generates inconsistent results on different input dtypes.
The same error log has appeared in #56204, but I am not sure whether we are talking about the same problem.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np

input = tf.constant([[np.inf, -np.inf]], dtype=tf.complex64)
out = tf.linalg.svd(input, full_matrices=False, compute_uv=False) # 0

input = tf.constant([[np.inf, -np.inf]], dtype=tf.float64)
out = tf.linalg.svd(input, full_matrices=False, compute_uv=False) # nan
```


### Relevant log output

```shell
2024-03-18 02:28:14.593059: E ./tensorflow/core/kernels/linalg/svd_op_impl.h:110] Eigen::BDCSVD failed with error code 3
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-17T18:58:12Z,4,0,https://github.com/tensorflow/tensorflow/issues/63852,Logical Bug
214,tensorflow/tensorflow,Adding TensorFlow Hub KerasLayer to Sequential Model Raises ValueError,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.1

### Custom code

No

### OS platform and distribution

Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

Python 3.11.0rc1

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3.0

### GPU model and memory

_No response_

### Current behavior?

I can execute the following code without any issues in TensorFlow 2.15.0 and TensorFlow Hub 1.16.1. However, when I upgrade the TensorFlow version to 2.16.0 or above, I encounter an error stating that `KerasLayer` cannot be added to the Sequential model.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_hub as hub

image_size = 224
URL = ""https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1""

model = tf.keras.Sequential([
        hub.KerasLayer(URL, input_shape=(image_size, image_size, 3))
])
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[29], line 1
----> 1 model = tf.keras.Sequential([
      2         feature_extractor,
      3         tf.keras.layers.Dense(2, activation = 'softmax')
      4 ])
      6 model.build([None, image_size, image_size, 3])
      7 model.summary()

File /usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py:70, in Sequential.__init__(self, layers, trainable, name)
     68 if layers:
     69     for layer in layers:
---> 70         self.add(layer, rebuild=False)
     71     self._maybe_rebuild()

File /usr/local/lib/python3.10/dist-packages/keras/src/models/sequential.py:92, in Sequential.add(self, layer, rebuild)
     90         layer = origin_layer
     91 if not isinstance(layer, Layer):
---> 92     raise ValueError(
     93         ""Only instances of `keras.Layer` can be ""
     94         f""added to a Sequential model. Received: {layer} ""
     95         f""(of type {type(layer)})""
     96     )
     97 if not self._is_layer_name_unique(layer):
...
    101         ""the name of a layer in this model. Update the `name` argument ""
    102         ""to pass a unique name.""
    103     )

ValueError: Only instances of `keras.Layer` can be added to a Sequential model. Received: <tensorflow_hub.keras_layer.KerasLayer object at 0x7a4ac7e30f40> (of type <class 'tensorflow_hub.keras_layer.KerasLayer'>)
```
","['type:bug', 'comp:keras', 'awaiting PR merge', 'TF 2.16']",2024-03-17T11:08:55Z,17,3,https://github.com/tensorflow/tensorflow/issues/63849,Logical Bug
215,tensorflow/tensorflow,A check fail can be triggered in tf.raw_ops.StatefulPartitionedCall,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the latest version of TensorFlow, the following code can trigger a crash in `tf.raw_ops.StatefulPartitionedCall` due to check-fail.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = tf.constant([[1, 2, 3], [4, 5, 6]])

@tf.function
def my_function(inputs):
    return tf.reduce_sum(inputs, axis=1)

callable_function = tf.function(my_function).get_concrete_function(tf.TensorSpec(shape=None, dtype=tf.int32))

output = tf.raw_ops.StatefulPartitionedCall(args=[input_data], Tout=None, f=callable_function)

print(output)
```


### Relevant log output

```shell
2024-03-14 15:58:52.804195: F tensorflow/core/framework/op_kernel.cc:989] Check failed: index < outputs_.size() (0 vs. 0)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-14T15:59:00Z,2,0,https://github.com/tensorflow/tensorflow/issues/63721,Runtime Error
216,tensorflow/tensorflow,A check fail can be triggered in tf.raw_ops.ResourceSparseApplyKerasMomentum,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In the latest version of TensorFlow, the following code can trigger a crash in `tf.raw_ops.ResourceSparseApplyKerasMomentum` due to check-fail.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.complex64)  
accum = tf.Variable([[0.1, 0.2], [0.3, 0.4]])
lr = 0.01
grad = tf.constant([[0.1, 0.2], [0.3, 0.4]])
indices = tf.constant([0, 1])  
momentum = 0.9

result = tf.raw_ops.ResourceSparseApplyKerasMomentum(var=var.handle, accum=accum.handle, lr=lr, grad=grad, indices=indices, momentum=momentum)

print(result)
```


### Relevant log output

```shell
2024-03-14 15:57:08.452454: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
","['awaiting review', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-14T15:57:19Z,0,0,https://github.com/tensorflow/tensorflow/issues/63720,Runtime Error
217,tensorflow/tensorflow,tf.raw_ops.ResourceApplyProximalAdagrad: Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.ResourceApplyProximalAdagrad` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3])
lr = 0.01
l1 = 0.1
l2 = 0.01
grad = tf.constant([0.1, 0.2, 0.3], dtype=tf.float64)

var_resource = tf.raw_ops.VarHandleOp(dtype=var.dtype, shape=var.shape)
accum_resource = tf.raw_ops.VarHandleOp(dtype=accum.dtype, shape=accum.shape)

assign_var = tf.raw_ops.AssignVariableOp(resource=var_resource, value=var)
assign_accum = tf.raw_ops.AssignVariableOp(resource=accum_resource, value=accum)

result = tf.raw_ops.ResourceApplyProximalAdagrad(var=var_resource, accum=accum_resource, lr=lr, l1=l1, l2=l2, grad=grad)
```


### Relevant log output

```shell
2024-03-14 05:57:20.787563: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (1 vs. 2) double expected, got float
Aborted (core dumped)
```
","['awaiting review', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-14T05:57:34Z,1,0,https://github.com/tensorflow/tensorflow/issues/63697,Runtime Error
218,tensorflow/tensorflow,tf.raw_ops.ResourceApplyMomentum: Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.ResourceApplyMomentum` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

var = tf.Variable([1.0, 2.0, 3.0])
accum = tf.Variable([0.1, 0.2, 0.3], dtype=tf.complex64) 
lr = 0.01
grad = tf.constant([0.1, 0.2, 0.3])
momentum = 0.9

output = tf.raw_ops.ResourceApplyMomentum(var=var.handle, accum=accum.handle, lr=lr, grad=grad, momentum=momentum)

print(output)
```


### Relevant log output

```shell
2024-03-14 05:55:18.324852: F tensorflow/core/framework/tensor.cc:844] Check failed: dtype() == expected_dtype (8 vs. 1) float expected, got complex64
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-14T05:55:30Z,1,0,https://github.com/tensorflow/tensorflow/issues/63696,Runtime Error
219,tensorflow/tensorflow,tf.raw_ops.QuantizeAndDequantizeV3: Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.QuantizeAndDequantizeV3` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = tf.constant([1.5, 2.5, 3.5, 4.5])
input_min = tf.constant([[[0.0, 0.0], [0.0, 0.0]], [[0.0, 0.0], [0.0, 0.0]], [[0.0, 0.0], [0.0, 0.0]]])

input_max = tf.constant(5.0)
num_bits = tf.constant(8)
signed_input = True
range_given = True
narrow_range = False
axis = -1

output_data = tf.raw_ops.QuantizeAndDequantizeV3(input=input_data, input_min=input_min, input_max=input_max, num_bits=num_bits, signed_input=signed_input, range_given=range_given, narrow_range=narrow_range, axis=axis)

print(output_data)
```


### Relevant log output

```shell
2024-03-14 05:50:03.159345: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-14T05:50:58Z,1,0,https://github.com/tensorflow/tensorflow/issues/63693,Runtime Error
220,tensorflow/tensorflow,tf.raw_ops.ExperimentalDatasetToTFRecord: Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.ExperimentalDatasetToTFRecord` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = tf.data.Dataset.range(10).map(lambda x: tf.strings.as_string(x))
input_data = input_data.batch(2)  

filename = tf.constant(""output.tfrecord"")
compression_type = tf.constant(""GZIP"")
tf.raw_ops.ExperimentalDatasetToTFRecord(input_dataset=input_data._variant_tensor, filename=filename, compression_type=compression_type)
```


### Relevant log output

```shell
2024-03-14 05:47:55.415453: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.16']",2024-03-14T05:48:39Z,1,0,https://github.com/tensorflow/tensorflow/issues/63691,Runtime Error
221,tensorflow/tensorflow,tf.raw_ops.DatasetToTFRecord: Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.DatasetToTFRecord` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = [[1, 2], [3, 4], [5, 6]]

input_data_strings = [[str(d) for d in inner_list] for inner_list in input_data]

dataset = tf.data.Dataset.from_tensor_slices(input_data_strings)

filename = ""output.tfrecord""

tf.raw_ops.DatasetToTFRecord(input_dataset=dataset._variant_tensor, filename=filename, compression_type="""")
```


### Relevant log output

```shell
2024-03-14 05:41:32.476705: F tensorflow/core/framework/tensor.cc:852] Check failed: 1 == NumElements() (1 vs. 2)Must have a one element tensor
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-14T05:41:47Z,1,0,https://github.com/tensorflow/tensorflow/issues/63689,Runtime Error
222,tensorflow/tensorflow,tf.transpose lead to program abortion on a normal value while the perm contains -1 ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Hi, the tf.transpose will lead to a program abortion when receiving an empty input and perm is [-1, -2].
The traceback indicate a check fail:
```
tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.constant([[]])
dim0 = -2
dim1 = -1
tf.transpose(input, perm=[dim1, dim0])
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-14T05:16:01Z,2,0,https://github.com/tensorflow/tensorflow/issues/63685,Runtime Error
223,tensorflow/tensorflow,_pywrap_tensorflow_internal.so library contains RUNPATH entries that look like C++ symbols,"### Issue type

Bug

### TensorFlow version

2.13 - 2.16

### Python version

3.11, 3.12

### Current behavior?

The RUNPATH in the wheel shared objects is weirdly mangled:

```
$ wget https://files.pythonhosted.org/packages/0d/58/d302d19f06d028ad6c2e33c449ef9813058a16dc35c0ca9580740e0dc9ed/tensorflow-2.16.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
$ unzip *.whl
$ objdump -x tensorflow/python/_pywrap_tensorflow_internal.so | grep RUNPATH
  RUNPATH              $ORIGIN/../../_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so_Ucclib___Utensorflow:$ORIGIN/_pywrap_tensorflow_internal.so.runfiles/org_tensorflow/_solib_local/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal.so_Ucclib___Utensorflow:$ORIGIN/../../_solib_local/_Utensorflow:$ORIGIN/_pywrap_tensorflow_internal.so.runfiles/org_tensorflow/_solib_local/_Utensorflow:$ORIGIN/:$ORIGIN/..:$ORIGIN/../../nvidia/nccl/lib:$ORIGIN/../nvidia/nccl/lib:$ORIGIN/../../tensorflow/tsl/python/lib/core
```

It's not just that one, almost every SO is affected, see:
```
find tensorflow/ -name *.so -exec objdump -x {} \; | grep RUNPATH
```

Under a custom NixOS build, this mess seems to further confuse follow-on patchelf invocations, which is requiring additional hackery to correct. Possibly related to #34991; the objdump in that ticket shows the same kind of RUNPATH.

### Standalone code to reproduce the issue

```shell
Download any wheel from PyPI, examine the `so`s contained within for their RUNPATH values.
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'TF 2.16']",2024-03-13T19:50:40Z,1,0,https://github.com/tensorflow/tensorflow/issues/63646,Dependency Issue
224,tensorflow/tensorflow,Incorrect result when using tf.norm to conduct p-norm computation on float16 precision,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

tf.norm outputs incorrect result under float16 computation when the ord is large. Here is the code to reproduce:

```
import tensorflow as tf
input = tf.constant(1.751, dtype=""float16"")
p = 144111111
print(tf.norm(input, p))   # tf.Tensor(1.0, shape=(), dtype=float16)
```
Instead, if I change the dtype to 'float32', tf.norm outputs `inf`. It seems that tf.norm overflows during this computation https://github.com/tensorflow/tensorflow/blob/9ce1d5f2f7e5e8866a55633c182cc2dda1a2d6b8/tensorflow/python/ops/linalg_ops.py#L784

Although fixing this issue by letting tf.norm also outputs `inf` in `float16` (which is consistent with the result in `float32`. I am suggesting another possible fix according to this post https://timvieira.github.io/blog/post/2014/11/10/numerically-stable-p-norms/

Indeed, the original implementation may not be numerical stable, a more stable one is to normalize the input before doing the actual computation. Here is the naive implementation of original tf.norm and the numerical stable one as a possible fix:

```
import tensorflow as tf
input = tf.constant(1.751, dtype=""float16"")
p = 144111111

def origin_norm(x, p):
  result = tf.math.abs(x)
  result = tf.math.pow(
      tf.reduce_sum(tf.math.pow(result, p), axis=None, keepdims=True),
      1.0 / p)
  return result

def refined_norm(x, p):
  a = np.abs(x).max()
  return a * origin_norm(x/a, p)

print(f""The original tf.norm's output is: "", tf.norm(input, p))
print(f""The naive implementation of tf.norm: {origin_norm(input, p)}, the refined norm: {refined_norm(input, p)}"")
```

The `refined_norm` will output correct results (i.e., 1.7509765625) not only for `float16` computation but also for `float32` computation.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
input = tf.constant(1.751, dtype=""float16"")
p = 144111111

def origin_norm(x, p):
  result = tf.math.abs(x)
  result = tf.math.pow(
      tf.reduce_sum(tf.math.pow(result, p), axis=None, keepdims=True),
      1.0 / p)
  return result

def refined_norm(x, p):
  a = np.abs(x).max()
  return a * origin_norm(x/a, p)

print(f""The original tf.norm's output is: "", tf.norm(input, p))
print(f""The naive implementation of tf.norm: {origin_norm(input, p)}, the refined norm: {refined_norm(input, p)}"")
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-13T19:26:58Z,2,0,https://github.com/tensorflow/tensorflow/issues/63643,Performance Issue
225,tensorflow/tensorflow,How can I exit the XLAControlFlowContext when inside a jit_compile tf.function? Exit() function take no effect.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Python version

3.10

### CUDA/cuDNN version

CUDA12.3/cuDNN9.0

### GPU model and memory

GTX4090 24GiB

### Current behavior?

I have a custom op that is a normal OpKernel unable to be compile by XLA cluster. But unfortunately, when the user use this custom op, they have to wrap it into a tf.funtion. So somehow when the use Keras jit_compile or something else, errors happened.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

@tf.function(jit_compile=True)
def test(a):
    b = a + a
    ctx = tf.__internal__.get_enclosing_xla_context()
    ctx.Exit()
    tf.print(b)
    ctx.Enter()
    return b * b

test(tf.constant(1))
```


### Relevant log output

```shell
2024-03-14 02:28:52.720666: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:296 : INVALID_ARGUMENT: Detected unsupported operations when trying to compile graph __inference_test_9[_XlaMustCompile=true,config_proto=9241198235816212909,executor_type=11160318154034397263] on XLA_GPU_JIT: StringFormat (No registered 'StringFormat' OpKernel for XLA_GPU_JIT devices compatible with node {{node StringFormat}}){{node StringFormat}}
The op is created at: 
File ""xla_context_test.py"", line 12, in <module>
  test(tf.constant(1))
File ""xla_context_test.py"", line 8, in test
  tf.print(b)
Traceback (most recent call last):
  File ""xla_context_test.py"", line 12, in <module>
    test(tf.constant(1))
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 54, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Detected unsupported operations when trying to compile graph __inference_test_9[_XlaMustCompile=true,config_proto=9241198235816212909,executor_type=11160318154034397263] on XLA_GPU_JIT: StringFormat (No registered 'StringFormat' OpKernel for XLA_GPU_JIT devices compatible with node {{node StringFormat}}){{node StringFormat}}
The op is created at: 
File ""xla_context_test.py"", line 12, in <module>
  test(tf.constant(1))
File ""xla_context_test.py"", line 8, in test
  tf.print(b) [Op:__inference_test_9]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'comp:tf.function', 'TF 2.15']",2024-03-13T18:29:08Z,3,0,https://github.com/tensorflow/tensorflow/issues/63632,Dependency Issue
226,tensorflow/tensorflow, TypeError: this __dict__ descriptor does not support '_DictWrapper' objects during trivial model save,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No (bug doesn't exist in tf-nightly 2.17.0.dev20240312)

### Source

source

### TensorFlow version

v2.16.1

### Custom code

Yes

### OS platform and distribution

OSX

### Mobile device

_No response_

### Python version

3.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When calling ` tf.saved_model.save(model, saved_model_path)`

we see:

```
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py:190: in list_children
    for name, child in super(_AugmentedGraphView, self).list_children(
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py:75: in list_children
    for name, ref in super(ObjectGraphView,
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/trackable_view.py:85: in children
    ref = converter.convert_to_trackable(ref, parent=obj)
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/trackable/converter.py:31: in convert_to_trackable
    if (tensor_util.is_tf_type(obj) and
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/framework/tensor_util.py:1156: in is_tf_type
    return isinstance(x, tf_type_classes)
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/typing.py:1871: in __instancecheck__
    val = getattr_static(instance, attr)
../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py:1839: in getattr_static
    instance_result = _check_instance(obj, attr)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

obj = DictWrapper({'input_shape': [(None, 16), (None, 32)]}), attr = 'is_tensor_like'

    def _check_instance(obj, attr):
        instance_dict = {}
        try:
>           instance_dict = object.__getattribute__(obj, ""__dict__"")
E           TypeError: this __dict__ descriptor does not support '_DictWrapper' objects

../../../../miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py:1793: TypeError
```

I suspect this is related to #59869 which was supposedly fixed. However, in 2.16.1 TF removes the pin on wrapt and the issue indeed persists. I've even tried downgrading wrapt to 1.14.1 and the issue remains.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from tensorflow.keras import layers
import tempfile

def get_two_tower_models():
    online_features = [layers.Input(shape=(32,)), layers.Input(shape=(16,))]
    offline_features = [layers.Input(shape=(16,)), layers.Input(shape=(32,))]
    all_features = []
    all_features.extend(online_features)
    all_features.extend(offline_features)

    def get_offline_tower(offline_features):
        offline_inputs = layers.concatenate(offline_features, name=""offline_concatenated"")
        offline_hidden = layers.Dense(32, activation=""tanh"", name=""offline_hidden_1"")(offline_inputs)
        offline_hidden = layers.Dense(16, activation=""tanh"", name=""offline_hidden_2"")(offline_hidden)
        offline_final_embed = layers.Dense(8, name=""offline_hidden_3"")(offline_hidden)

        return offline_final_embed

    def get_online_tower(online_features):
        online_inputs = layers.concatenate(online_features, name=""online_concatenated"")
        online_hidden = layers.Dense(32, activation=""tanh"", name=""online_hidden_1"")(online_inputs)
        online_hidden = layers.Dense(16, activation=""tanh"", name=""online_hidden_2"")(online_hidden)
        online_final_embed = layers.Dense(8, name=""online_hidden_3"")(online_hidden)

        return online_final_embed

    offline_tower_embed = get_offline_tower(offline_features)
    online_tower_embed = get_online_tower(online_features)

    # We normalize vectors with L2 norm to make sure we get the cosine similarity
    offline_online_dot = layers.Dot(axes=1, normalize=True)([offline_tower_embed, online_tower_embed])

    offline_tower_model = tf.keras.Model(inputs=offline_features, outputs=offline_tower_embed)
    online_tower_model = tf.keras.Model(inputs=online_features, outputs=online_tower_embed)

    full_model = tf.keras.Model(inputs=all_features, outputs=offline_online_dot)

    return (full_model, offline_tower_model, online_tower_model)

full_model, offline_tower_model, online_tower_model = get_two_tower_models()

with tempfile.TemporaryDirectory() as tmpdirname:
    tf.saved_model.save(online_tower_model, tmpdirname)
```
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/Users/alfredo_luque/repos/git.musta.ch/airbnb/bighead-service/packages/ml-frameworks/tensorflow/tests/minimal_repro.py"", line 44, in <module>
    tf.saved_model.save(online_tower_model, tmpdirname)
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 1392, in save
    save_and_return_nodes(obj, export_dir, signatures, options)
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 1427, in save_and_return_nodes
    _build_meta_graph(obj, signatures, options, meta_graph_def))
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 1642, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 1564, in _build_meta_graph_impl
    saveable_view = _SaveableView(augmented_graph_view, options)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 285, in __init__
    checkpoint_util.objects_ids_and_slot_variables_and_paths(
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/util.py"", line 160, in objects_ids_and_slot_variables_and_paths
    trackable_objects, node_paths = graph_view.breadth_first_traversal()
                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py"", line 124, in breadth_first_traversal
    return self._breadth_first_traversal()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 156, in _breadth_first_traversal
    super(_AugmentedGraphView, self)._breadth_first_traversal())
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py"", line 128, in _breadth_first_traversal
    return super(ObjectGraphView, self)._descendants_with_paths()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/trackable_view.py"", line 111, in _descendants_with_paths
    for name, dependency in self.children(current_trackable).items():
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py"", line 97, in children
    for name, ref in self.list_children(obj, **kwargs):
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/saved_model/save.py"", line 190, in list_children
    for name, child in super(_AugmentedGraphView, self).list_children(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/graph_view.py"", line 75, in list_children
    for name, ref in super(ObjectGraphView,
                     ^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/checkpoint/trackable_view.py"", line 85, in children
    ref = converter.convert_to_trackable(ref, parent=obj)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/trackable/converter.py"", line 31, in convert_to_trackable
    if (tensor_util.is_tf_type(obj) and
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/site-packages/tensorflow/python/framework/tensor_util.py"", line 1156, in is_tf_type
    return isinstance(x, tf_type_classes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/typing.py"", line 1871, in __instancecheck__
    val = getattr_static(instance, attr)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py"", line 1839, in getattr_static
    instance_result = _check_instance(obj, attr)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/alfredo_luque/miniforge3/envs/local--bighead--v0.0.1/lib/python3.12/inspect.py"", line 1793, in _check_instance
    instance_dict = object.__getattribute__(obj, ""__dict__"")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: this __dict__ descriptor does not support '_DictWrapper' objects
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:model', 'TF 2.16']",2024-03-12T19:52:24Z,17,7,https://github.com/tensorflow/tensorflow/issues/63548,Runtime Error
227,tensorflow/tensorflow,tf.tensor_scatter_nd_add: Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.tensor_scatter_nd_add` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Generate input data
input_tensor = tf.zeros([15, 15, 15])
indices = tf.constant([[[0, 0, 0], [1, 1, 1]], [[2, 2, 2], [3, 3, 3]], [[4, 4, 4], [5, 5, 5]], [[6, 6, 6], [7, 7, 7]], [[8, 8, 8], [9, 9, 9]], [[10, 10, 10], [11, 11, 11]], [[12, 12, 12], [13, 13, 13]], [[14, 14, 14], [0, 0, 0]], [[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]], [[5, 5, 5], [6, 6, 6]], [[7, 7, 7], [8, 8, 8]], [[9, 9, 9], [10, 10, 10]], [[11, 11, 11], [12, 12, 12]], [[13, 13, 13], [14, 14, 14]]])
updates = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0])  # Cast updates to float

# Invoke tf.tensor_scatter_nd_add
result = tf.tensor_scatter_nd_add(input_tensor, indices, updates)

# Print the result
print(result)
```


### Relevant log output

```shell
2024-03-10 14:59:51.853766: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-10T15:00:49Z,2,0,https://github.com/tensorflow/tensorflow/issues/63380,Runtime Error
228,tensorflow/tensorflow,tf.raw_ops.TensorScatterSub: Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.TensorScatterSub` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Generate input data
tensor = tf.constant([1, 2, 3, 4, 5])
indices = tf.constant([[[1], [3]], [[0], [2]]])  # Nested structure for indices
updates = tf.constant([10, 20])

# Invoke tf.raw_ops.TensorScatterSub
result = tf.raw_ops.TensorScatterSub(tensor=tensor, indices=indices, updates=updates)

# Print the result
print(result)
```


### Relevant log output

```shell
2024-03-10 14:55:41.958738: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (1 vs. 1)
Aborted (core dumped)
```
","['type:bug', 'comp:ops', 'TF 2.15']",2024-03-10T14:57:36Z,1,0,https://github.com/tensorflow/tensorflow/issues/63378,Runtime Error
229,tensorflow/tensorflow,tf.raw_ops.FusedPadConv2D: Aborted (core dumped),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Under specific input, `tf.raw_ops.FusedPadConv2D` encounters ""Aborted (core dumped)"".

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Generate input data
input_data = tf.random.normal([3, 10, 10])

# Define paddings
paddings = tf.constant([[0, 0], [1, 1], [1, 1]])

# Define filter
filter = tf.random.normal([3, 3, 3, 16])

# Define mode
mode = ""REFLECT""  # Change mode to ""REFLECT"" or ""SYMMETRIC""

# Define strides
strides = [1, 1, 1, 1]

# Define padding
padding = ""VALID""

# Invoke tf.raw_ops.FusedPadConv2D
output = tf.raw_ops.FusedPadConv2D(input=input_data, paddings=paddings, filter=filter, mode=mode, strides=strides, padding=padding)

print(output)
```


### Relevant log output

```shell
2024-03-10 14:49:28.555826: F tensorflow/core/framework/tensor_shape.cc:357] Check failed: d < dims() (3 vs. 3)
Aborted (core dumped)
```
","['type:bug', 'comp:ops', 'TF 2.15']",2024-03-10T14:51:07Z,1,0,https://github.com/tensorflow/tensorflow/issues/63376,Runtime Error
230,tensorflow/tensorflow,core dumped with tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

core dumped error with specific input parameters.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Generate input data
input_data = tf.constant([[1.5, 2.5, 3.5], [4.5, 5.5, 6.5]])

# Define min and max values per channel
min_per_channel = tf.constant([1.0, 2.0, 3.0])
max_per_channel = tf.constant([2.0, 3.0, 4.0])

# Invoke tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel with inputs as 0-dimensional tensor and max as a 1x3 tensor
quantized_output = tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel(inputs=tf.constant(0.0), min=min_per_channel, max=max_per_channel, num_bits=8, narrow_range=False)

# Print the quantized output
print(quantized_output)
```


### Relevant log output

```shell
2024-03-09 15:02:07.858055: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['type:bug', 'comp:ops', 'TF 2.15']",2024-03-09T15:03:18Z,1,0,https://github.com/tensorflow/tensorflow/issues/63355,Runtime Error
231,tensorflow/tensorflow,Aborted (core dumped) with tf.raw_ops.AvgPoolGrad,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

core dumped error with specific input parameters.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Generate input data
input_data = tf.random.normal([1, 28, 28, 3])
grad = tf.random.normal([1, 14, 14, 6])  # Change the number of channels in grad tensor

# Perform average pooling
result = tf.nn.avg_pool2d(input_data, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', data_format='NHWC')

# Compute gradient
grad_result = tf.raw_ops.AvgPoolGrad(orig_input_shape=tf.shape(input_data), grad=grad, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', data_format='NHWC')

print(grad_result)
```


### Relevant log output

```shell
free(): corrupted unsorted chunks
Aborted (core dumped)
```
","['type:bug', 'comp:ops', 'TF 2.15']",2024-03-09T14:54:40Z,1,0,https://github.com/tensorflow/tensorflow/issues/63353,Runtime Error
232,tensorflow/tensorflow,Segmentation fault with tf.raw_ops.AudioSpectrogram,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Segmentation fault error with specific input parameters.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Generate input data
input_data = tf.random.normal([1, 44100], dtype=tf.float32)

# Invoke tf.raw_ops.AudioSpectrogram with a negative window_size
spectrogram = tf.raw_ops.AudioSpectrogram(input=input_data, window_size=-1024, stride=64, magnitude_squared=False)

# Print the spectrogram
print(spectrogram)
```


### Relevant log output

```shell
Segmentation fault (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-09T14:50:26Z,1,0,https://github.com/tensorflow/tensorflow/issues/63352,Runtime Error
233,tensorflow/tensorflow,Aborted (core dumped) in tf.io.TFRecordWriter,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have tested on tensorflow-nightly.
There are five bugs related to `tf.io.TFRecordWriter`. 

BUG 1: `compression_method=3` -> Aborted (core dumped)
Code:
```
import tensorflow as tf
input_data = b'input_data_example'
options = tf.io.TFRecordOptions(compression_type='ZLIB', compression_method=3)
with tf.io.TFRecordWriter('output.tfrecord', options=options) as writer:
    writer.write(input_data)
```

BUG 2: `window_bits=-8` -> Aborted (core dumped)
CODE:
```
import tensorflow as tf
input_data = b'input_data_example'

options = tf.io.TFRecordOptions(
    compression_type='ZLIB',
    window_bits=-8  # negative window_bits
)

with tf.io.TFRecordWriter('output.tfrecord', options=options) as writer:
    writer.write(input_data)
```

BUG3: `compression_level=-9` -> Aborted (core dumped)
CODE:
```
import tensorflow as tf
input_data = b'input_data_example'
options = tf.io.TFRecordOptions(compression_type='ZLIB', compression_level=-9)
with tf.io.TFRecordWriter('output.tfrecord', options=options) as writer:
    writer.write(input_data)
```

BUG 4: `mem_level=-10` -> Aborted (core dumped)
CODE:
```
import tensorflow as tf
input_data = b'input_data_example'
options = tf.io.TFRecordOptions(compression_type='ZLIB', mem_level=-10)
with tf.io.TFRecordWriter('output.tfrecord', options=options) as writer:
    writer.write(input_data)
```

BUG 5: `compression_strategy=-2` -> Aborted (core dumped)
```
import tensorflow as tf

input_data = b'input_data_example'
options = tf.io.TFRecordOptions(compression_type='ZLIB', compression_strategy=-2)
with tf.io.TFRecordWriter('output.tfrecord', options=options) as writer:
    writer.write(input_data)
```

### Standalone code to reproduce the issue

```shell
The reproducible test cases are provided above.
```


### Relevant log output

```shell
All the above codes output `Aborted (core dumped)`.
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-03-09T03:15:46Z,4,0,https://github.com/tensorflow/tensorflow/issues/63337,Runtime Error
234,tensorflow/tensorflow,The result of tf.quantization.fake_quant_with_min_max_args is inconsistent between CPU and GPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.2

### GPU model and memory

_No response_

### Current behavior?

For the same input, `tf.quantization.fake_quant_with_min_max_args ` produces inconsistent results on CPU and GPU.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input_data = tf.constant([0.0, 1.0, 2.0, 3.0, 4.0])

with tf.device('cpu:0'):
    quantized_input_data = tf.quantization.fake_quant_with_min_max_args(input_data, min=-6.0, max=6.0)
    print(quantized_input_data)

with tf.device('gpu:0'):
    quantized_input_data = tf.quantization.fake_quant_with_min_max_args(input_data, min=-6.0, max=6.0)
    print(quantized_input_data)
```


### Relevant log output

```shell
tf.Tensor([0.        0.9882353 2.0235295 3.0117648 4.       ], shape=(5,), dtype=float32)
tf.Tensor([0.        0.9882353 1.9764706 3.0117648 4.       ], shape=(5,), dtype=float32)
```
","['type:bug', 'comp:ops', 'TF 2.15']",2024-03-08T08:28:07Z,1,0,https://github.com/tensorflow/tensorflow/issues/63271,Performance Issue
235,tensorflow/tensorflow,`tf.raw_ops.ArgMax`: Heap buffer overflow,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.ArgMax` can lead to heap buffer overflow.
[Error location](https://github.com/tensorflow/tensorflow/blob/774b0c3e97b5ef60bfc9c54961347dd0bc3660a8/tensorflow/core/kernels/argmax_op.cc#L59):
```C++
    const int32_t dim = internal::SubtleMustCopy(dimension.scalar<int32>()());
```
It copies scalar value of `dimension` without checking exact type. Therefore when `dimension` is an `int16` it reads over the bound.
Note that `int16` is an allowed type for `dimension` according to [opdef](https://github.com/tensorflow/tensorflow/blob/41b93a8b310086f69aab6b6369d2af9d5178881d/tensorflow/core/ops/math_ops.cc#L1153-L1160):
```C++
REGISTER_OP(""ArgMax"")
    .Input(""input: T"")
    .Input(""dimension: Tidx"")
    .Output(""output: output_type"")
    .Attr(""T: {realnumbertype, quantizedtype, bool}"")
    .Attr(""Tidx: {int16, int32, int64} = DT_INT32"")
    .Attr(""output_type: {int16, uint16, int32, int64} = DT_INT64"")
    .SetShapeFn(ArgOpShape);
```

### Standalone code to reproduce the issue

```python
import tensorflow as tf

tf.raw_ops.ArgMax(
    input=tf.random.normal([1,1,1,1]),
    dimension=tf.constant(1,shape=[],dtype=tf.int16),
    output_type=tf.dtypes.int64,
    name=None
)
```


### Relevant log output

The below log needs ASAN build.
```shell
=================================================================
==4008222==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x609000000400 at pc 0x7fa6a0dca809 bp 0x7ffe63b29d90 sp 0x7ffe63b29d88
READ of size 4 at 0x609000000400 thread T0
    #0 0x7fa6a0dca808 in int const tensorflow::internal::SubtleMustCopy<int>(int const&) /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10
    #1 0x7fa6a0dca808 in tensorflow::ArgOp<Eigen::ThreadPoolDevice, float, long, tensorflow::functor::ArgMax<Eigen::ThreadPoolDevice, float, long>>::Compute(tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/kernels/argmax_op.cc:59:25
    #2 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #3 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #4 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #5 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #6 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #7 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #8 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #9 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #10 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #11 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #12 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #13 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #14 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #15 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #16 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #17 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #18 0x7fa6b989418d in TFE_Py_FastPathExecute_C(_object*) /proc/self/cwd/tensorflow/python/eager/pywrap_tfe_src.cc:3979:3
    #19 0x7fa667d2683e in pybind11_init__pywrap_tfe(pybind11::module_&)::$_60::operator()(pybind11::args) const /proc/self/cwd/tensorflow/python/tfe_wrapper.cc:1276:35
    #20 0x7fa667d2683e in pybind11::object pybind11::detail::argument_loader<pybind11::args>::call_impl<pybind11::object, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, 0ul, pybind11::detail::void_type>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&, std::integer_sequence<unsigned long, 0ul>, pybind11::detail::void_type&&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1443:16
    #21 0x7fa667d2683e in std::enable_if<!std::is_void<pybind11::object>::value, pybind11::object>::type pybind11::detail::argument_loader<pybind11::args>::call<pybind11::object, pybind11::detail::void_type, pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&) && /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/cast.h:1411:42
    #22 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::operator()(pybind11::detail::function_call&) const /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:248:69
    #23 0x7fa667d2683e in void pybind11::cpp_function::initialize<pybind11_init__pywrap_tfe(pybind11::module_&)::$_60, pybind11::object, pybind11::args, pybind11::name, pybind11::scope, pybind11::sibling>(pybind11_init__pywrap_tfe(pybind11::module_&)::$_60&&, pybind11::object (*)(pybind11::args), pybind11::name const&, pybind11::scope const&, pybind11::sibling const&)::'lambda'(pybind11::detail::function_call&)::__invoke(pybind11::detail::function_call&) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:223:21
    #24 0x7fa667d67a59 in pybind11::cpp_function::dispatcher(_object*, _object*, _object*) /proc/self/cwd/bazel-out/k8-dbg/bin/external/pybind11/_virtual_includes/pybind11/pybind11/pybind11.h:939:30
    #25 0x528186 in cfunction_call /usr/local/src/conda/python-3.11.7/Objects/methodobject.c:542:18
    #26 0x503a0b in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18
    #27 0x510f32 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:4769:23
    #28 0x538732 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #29 0x538732 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #30 0x538732 in _PyFunction_Vectorcall /usr/local/src/conda/python-3.11.7/Objects/call.c:393:16
    #31 0x5426bb in _PyVectorcall_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:257:24
    #32 0x5426bb in _PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:328:16
    #33 0x5426bb in PyObject_Call /usr/local/src/conda/python-3.11.7/Objects/call.c:355:12
    #34 0x514ff0 in do_call_core /usr/local/src/conda/python-3.11.7/Python/ceval.c:7352:12
    #35 0x514ff0 in _PyEval_EvalFrameDefault /usr/local/src/conda/python-3.11.7/Python/ceval.c:5376:22
    #36 0x5cb559 in _PyEval_EvalFrame /usr/local/src/conda/python-3.11.7/Include/internal/pycore_ceval.h:73:16
    #37 0x5cb559 in _PyEval_Vector /usr/local/src/conda/python-3.11.7/Python/ceval.c:6434:24
    #38 0x5cac2e in PyEval_EvalCode /usr/local/src/conda/python-3.11.7/Python/ceval.c:1148:21
    #39 0x5ebcf6 in run_eval_code_obj /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1710:9
    #40 0x5e788f in run_mod /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1731:19
    #41 0x5fc831 in pyrun_file /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:1626:15
    #42 0x5fbbfe in _PyRun_SimpleFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:440:13
    #43 0x5fb922 in _PyRun_AnyFileObject /usr/local/src/conda/python-3.11.7/Python/pythonrun.c:79:15
    #44 0x5f65cd in pymain_run_file_obj /usr/local/src/conda/python-3.11.7/Modules/main.c:360:15
    #45 0x5f65cd in pymain_run_file /usr/local/src/conda/python-3.11.7/Modules/main.c:379:15
    #46 0x5f65cd in pymain_run_python /usr/local/src/conda/python-3.11.7/Modules/main.c:601:21
    #47 0x5f65cd in Py_RunMain /usr/local/src/conda/python-3.11.7/Modules/main.c:680:5
    #48 0x5bb3d8 in Py_BytesMain /usr/local/src/conda/python-3.11.7/Modules/main.c:734:12
    #49 0x7fa791408d8f in __libc_start_call_main csu/../sysdeps/nptl/libc_start_call_main.h:58:16
    #50 0x7fa791408e3f in __libc_start_main csu/../csu/libc-start.c:392:3
    #51 0x5bb222 in _start (/home/loft/anaconda3/envs/tf-latest-asan/bin/python3.11+0x5bb222)

0x609000000402 is located 0 bytes after 2-byte region [0x609000000400,0x609000000402)
allocated by thread T0 here:
    #0 0x7fa7917ed617 in __interceptor_posix_memalign /home/runner/work/llvm-project/llvm-project/final/llvm-project/compiler-rt/lib/asan/asan_malloc_linux.cpp:145:3
    #1 0x7fa6be4f6902 in tsl::port::AlignedMalloc(unsigned long, int) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_framework.so.2+0x4a18902)
    #2 0x7fa6bc2d3bcc in tsl::(anonymous namespace)::CPUAllocator::AllocateRaw(unsigned long, unsigned long) cpu_allocator_impl.cc
    #3 0x7fa6bc4883bc in short* tensorflow::TypedAllocator::Allocate<short>(tsl::Allocator*, unsigned long, tsl::AllocationAttributes const&) /proc/self/cwd/./tensorflow/core/framework/typed_allocator.h:47:24
    #4 0x7fa6bc4883bc in tensorflow::(anonymous namespace)::Buffer<short>::Buffer(tsl::Allocator*, long, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:574:21
    #5 0x7fa6bc4883bc in tensorflow::Tensor::Tensor(tsl::Allocator*, tensorflow::DataType, tensorflow::TensorShape const&, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/tensor.cc:986:5
    #6 0x7fa6bbef871d in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes, tsl::AllocationAttributes const&) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:764:10
    #7 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_tensor(tensorflow::DataType, tensorflow::TensorShape const&, tensorflow::Tensor*, tsl::AllocatorAttributes) /proc/self/cwd/./tensorflow/core/framework/op_kernel.h:1270:12
    #8 0x7fa6bbef784e in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**, tsl::AllocatorAttributes) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:822:14
    #9 0x7fa6bbef4368 in tensorflow::OpKernelContext::allocate_output(int, tensorflow::TensorShape const&, tensorflow::Tensor**) /proc/self/cwd/tensorflow/core/framework/op_kernel.cc:728:10
    #10 0x7fa6a9bf0207 in tensorflow::CastOpBase::Compute(tensorflow::OpKernelContext*) (/home/loft/anaconda3/envs/tf-latest-asan/lib/python3.11/site-packages/tensorflow/python/platform/../../libtensorflow_cc.so.2+0x3e9fe207)
    #11 0x7fa6bbbfab9f in tensorflow::ThreadPoolDevice::Compute(tensorflow::OpKernel*, tensorflow::OpKernelContext*) /proc/self/cwd/tensorflow/core/common_runtime/threadpool_device.cc:185:14
    #12 0x7fa6bb91644f in tensorflow::(anonymous namespace)::SingleThreadedExecutorImpl::Run(tensorflow::Executor::Args const&) /proc/self/cwd/tensorflow/core/common_runtime/single_threaded_executor.cc:445:15
    #13 0x7fa6bb836ae6 in tensorflow::FunctionLibraryRuntimeImpl::RunSync(tensorflow::FunctionLibraryRuntime::Options, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) /proc/self/cwd/tensorflow/core/common_runtime/function.cc:1339:3
    #14 0x7fa6bb866863 in tensorflow::ProcessFunctionLibraryRuntime::RunMultiDeviceSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, std::function<absl::lts_20230802::Status (tensorflow::ProcessFunctionLibraryRuntime::ComponentFunctionData const&, tensorflow::ProcessFunctionLibraryRuntime::InternalArgs*)>) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:931:16
    #15 0x7fa6bb877e9b in tensorflow::ProcessFunctionLibraryRuntime::RunSync(tensorflow::FunctionLibraryRuntime::Options const&, unsigned long, absl::lts_20230802::Span<tensorflow::Tensor const>, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor>>*) const /proc/self/cwd/tensorflow/core/common_runtime/process_function_library_runtime.cc:1526:21
    #16 0x7fa69db93761 in tensorflow::KernelAndDeviceFunc::Run(tensorflow::ScopedStepContainer*, tensorflow::EagerKernelArgs const&, std::vector<std::variant<tensorflow::Tensor, tensorflow::TensorShape>, std::allocator<std::variant<tensorflow::Tensor, tensorflow::TensorShape>>>*, tsl::CancellationManager*, std::optional<tensorflow::EagerFunctionParams> const&, std::optional<tensorflow::ManagedStackTrace> const&, tsl::CoordinationServiceAgent*) /proc/self/cwd/tensorflow/core/common_runtime/eager/kernel_and_device.cc:464:21
    #17 0x7fa69da529f5 in tensorflow::EagerKernelExecute(tensorflow::EagerContext*, absl::lts_20230802::InlinedVector<tensorflow::TensorHandle*, 4ul, std::allocator<tensorflow::TensorHandle*>> const&, std::optional<tensorflow::EagerFunctionParams> const&, tsl::core::RefCountPtr<tensorflow::KernelAndDevice> const&, tensorflow::GraphCollector*, tsl::CancellationManager*, absl::lts_20230802::Span<tensorflow::TensorHandle*>, std::optional<tensorflow::ManagedStackTrace> const&) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2167:3
    #18 0x7fa69da86630 in tensorflow::ExecuteNode::Run() /proc/self/cwd/./tensorflow/core/common_runtime/eager/execute_node.h:127:12
    #19 0x7fa69db7f849 in tensorflow::EagerExecutor::SyncExecute(tensorflow::EagerNode*) /proc/self/cwd/tensorflow/core/common_runtime/eager/eager_executor.cc:128:13
    #20 0x7fa69da512d9 in tensorflow::(anonymous namespace)::AddOrExecuteNode(tsl::core::RefCountPtr<tensorflow::KernelAndDevice>, tensorflow::EagerOperation*, tensorflow::TensorHandle**) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1685:25
    #21 0x7fa69da512d9 in tensorflow::(anonymous namespace)::EagerLocalExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:1760:14
    #22 0x7fa69da4a107 in tensorflow::DoEagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2127:12
    #23 0x7fa69da54bd7 in tensorflow::EagerExecute(tensorflow::EagerOperation*, tensorflow::TensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/execute.cc:2207:10
    #24 0x7fa690dd1f99 in tensorflow::EagerOperation::Execute(absl::lts_20230802::Span<tensorflow::AbstractTensorHandle*>, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/core.cc:187:10
    #25 0x7fa69db7959e in tensorflow::CustomDeviceOpHandler::Execute(tensorflow::ImmediateExecutionOperation*, tensorflow::ImmediateExecutionTensorHandle**, int*) /proc/self/cwd/tensorflow/core/common_runtime/eager/custom_device_op_handler.cc
    #26 0x7fa6851e18f1 in TFE_Execute /proc/self/cwd/tensorflow/c/eager/c_api.cc:907:62
    #27 0x7fa6b985dcbf in tensorflow::EagerCast(TFE_Context*, TFE_TensorHandle*, TF_DataType, TF_DataType, TSL_Status*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:259:3
    #28 0x7fa6b985e67b in tensorflow::ConvertToEagerTensorUncached(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:317:11
    #29 0x7fa6b985f8a1 in tensorflow::ConvertToEagerTensor(TFE_Context*, _object*, tensorflow::DataType, char const*) /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:405:14
    #30 0x7fa6b9860088 in EagerTensor_init /proc/self/cwd/tensorflow/python/eager/pywrap_tensor.cc:529:18
    #31 0x5039d2 in type_call /usr/local/src/conda/python-3.11.7/Objects/typeobject.c:1103:19
    #32 0x5039d2 in _PyObject_MakeTpCall /usr/local/src/conda/python-3.11.7/Objects/call.c:214:18

SUMMARY: AddressSanitizer: heap-buffer-overflow /proc/self/cwd/./tensorflow/core/framework/bounds_check.h:49:10 in int const tensorflow::internal::SubtleMustCopy<int>(int const&)
Shadow bytes around the buggy address:
  0x609000000180: fa fa fa fa fa fa fa fa fd fa fa fa fa fa fa fa
  0x609000000200: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000280: fd fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000300: 04 fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000380: fd fd fd fd fa fa fa fa fa fa fa fa fa fa fa fa
=>0x609000000400:[02]fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000480: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000500: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000580: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000600: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x609000000680: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
==4008222==ABORTING
```
","['awaiting review', 'type:bug', 'comp:ops', 'TF 2.15']",2024-02-27T13:06:46Z,4,0,https://github.com/tensorflow/tensorflow/issues/63070,Security Vulnerability
236,tensorflow/tensorflow,`tf.raw_ops.ExtractImagePatches`: Assertion failure in shape inference step,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.ExtractImagePatches` can lead to assertion failure in shape inference step.
Error location is [here](https://github.com/tensorflow/tensorflow/blob/f5daeae21404e8d672c785cc0c4c469ddc1b1a8a/tensorflow/core/ops/array_ops.cc#L2686-L2687):
```C++
      TF_RETURN_IF_ERROR(c->Multiply(
          c->Dim(input_shape, 3), ksize_rows * ksize_cols, &output_depth_dim));
```
Because it does not check validity of `ksize_rows * ksize_cols`, the negative value of it is fed to [`Multiply`](https://github.com/tensorflow/tensorflow/blob/83f1804f3427ae888e62b26b5bcba8afc9e24ef7/tensorflow/core/framework/shape_inference.cc#L1096C1-L1098C56):
```C++
Status InferenceContext::Multiply(DimensionHandle first,
                                  DimensionOrConstant second,
                                  DimensionHandle* out)
```
And it ends up with assertion failure at [a constructor of `DimensionOrConstant`](https://github.com/tensorflow/tensorflow/blob/daea84aad67d1fcc8d58c648e1da58ee576445f9/tensorflow/core/framework/shape_inference.h#L891-L896).

### Standalone code to reproduce the issue

```Python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

tf.raw_ops.ExtractImagePatches(
    images=tf.random.normal([1,1,1,1]),
    ksizes=[1,-1,2,1],
    strides=[1,1,1,1],
    rates=[1,1,1,1],
    padding=""VALID"",
    name=None
)
```


### Relevant log output

Release build: Outputs nothing

Debug build:
```shell
2024-02-27 20:05:58.701202: F ./tensorflow/core/framework/shape_inference.h:891] Check failed: val >= 0 || val == InferenceContext::kUnknownDim Dimension must be non-negative or equal to InferenceContext::kUnknownDim but got -2
Aborted (core dumped)
```
","['awaiting review', 'type:bug', 'comp:ops', 'comp:core', 'TF 2.15']",2024-02-27T11:12:14Z,4,0,https://github.com/tensorflow/tensorflow/issues/63067,Logical Bug
237,tensorflow/tensorflow,C++ API `SparseFillEmptyRows` can lead to `std::length_error`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

6.5.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

C++ API `SparseFillEmptyRows` can lead to `std::length_error`.
Error location is [here](https://github.com/tensorflow/tensorflow/blob/9da417e3f63215efe995b83e7b9f9b34115a424e/tensorflow/core/kernels/fill_empty_rows_functor.h#L114C36-L114C46):
```C++
    std::vector<Tindex> csr_offset(dense_rows, 0);
```
Because lack of checking negativity, negative value of `dense_rows` is fed to the constructor of `std::vector`. By integer underflow, the value is converted to extremely large number and it ends up with `std::length_error`.

### Standalone code to reproduce the issue

```C++
#include ""tensorflow/cc/framework/scope.h""
#include ""tensorflow/core/graph/graph.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/cc/ops/array_ops.h""
#include ""tensorflow/cc/ops/standard_ops.h""

using namespace tensorflow;

int main() {
  SessionOptions options;
  std::unique_ptr<tensorflow::Session>
    session(tensorflow::NewSession(options));
  Scope scope = Scope::NewRootScope();

  Input indices = {{1l}};
  Input values = {1};
  Input dense_shape = {-1l};
  Input default_value = 1;
  auto target = ops::SparseFillEmptyRows(scope.WithOpName(""target""), indices, values, dense_shape, default_value);

  Status status;
  GraphDef graph_def;
  status = scope.ToGraphDef(&graph_def);
  if (!status.ok()) {
    LOG(WARNING) << ""Could not build graph: "" << status.message();
  }

  status = session->Create(graph_def);
  if (!status.ok()) {
    LOG(WARNING) << ""Could not create session: "" << status.message();
  }

  std::vector<Tensor> outputs;
  status = session->Run({}, {""target""}, {""target""}, &outputs);
  if (!status.ok()) {
    LOG(WARNING) << ""Could not run session: "" << status.message();
  }

  return 0;
}
```


### Relevant log output

```shell
terminate called after throwing an instance of 'std::length_error'
  what():  cannot create std::vector larger than max_size()
Aborted (core dumped)
```
","['awaiting review', 'type:bug', 'comp:ops', 'TF 2.15']",2024-02-27T10:56:31Z,1,0,https://github.com/tensorflow/tensorflow/issues/63066,Logical Bug
238,tensorflow/tensorflow,`tf.raw_ops.ExtractGlimpse`: Assertion failure in shape inference step,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.ExtractGlimpse` can lead to assertion failure in shape inference step.
Error location is [here](https://github.com/tensorflow/tensorflow/blob/daea84aad67d1fcc8d58c648e1da58ee576445f9/tensorflow/core/ops/image_ops.cc#L57):
```C++
    height = c->MakeDim(vec(0));
```
It makes a dim from input argument without checking negativity.
In debug build, it ends up with assertion failure in [a constructor of `DimensionOrConstant`](https://github.com/tensorflow/tensorflow/blob/daea84aad67d1fcc8d58c648e1da58ee576445f9/tensorflow/core/framework/shape_inference.h#L891-L896).

### Standalone code to reproduce the issue

```Python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

tf.raw_ops.ExtractGlimpse(
    input=tf.random.normal([1,1,1,1]),
    size=[-2,-2],
    offsets=tf.random.normal([1,2]),
    centered=True,
    normalized=True,
    uniform_noise=True,
    noise='uniform',
    name=None
)
```


### Relevant log output

Release build: Outputs nothing

Debug build:
```shell
2024-02-27 19:02:16.866010: F ./tensorflow/core/framework/shape_inference.h:891] Check failed: val >= 0 || val == InferenceContext::kUnknownDim Dimension must be non-negative or equal to InferenceContext::kUnknownDim but got -2
Aborted (core dumped)
```
","['awaiting review', 'type:bug', 'comp:ops', 'comp:core']",2024-02-27T10:11:25Z,3,0,https://github.com/tensorflow/tensorflow/issues/63063,Logical Bug
239,tensorflow/tensorflow,`tf.raw_ops.DecodeAndCropJpeg`: Assertion failure in shape inference step,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.DecodeAndCropJpeg` can lead to assertion failure in shape inference step.
Error location is [here](https://github.com/tensorflow/tensorflow/blob/daea84aad67d1fcc8d58c648e1da58ee576445f9/tensorflow/core/ops/image_ops.cc#L523):
```C++
       w = c->MakeDim(crop_window_vec(3));
```
It makes a dim from an input argument without checking negativity.
In debug build, it ends up with assertion failure in [a constructor of`DimensionOrConstant`](https://github.com/tensorflow/tensorflow/blob/daea84aad67d1fcc8d58c648e1da58ee576445f9/tensorflow/core/framework/shape_inference.h#L891-L896).

### Standalone code to reproduce the issue

```Python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

tf.raw_ops.DecodeAndCropJpeg(
    contents=""abc"",
    crop_window=[0,0,0,-2],
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)
```


### Relevant log output

Release build: Outputs nothing

Debug build:
```shell
2024-02-27 18:41:59.133420: F ./tensorflow/core/framework/shape_inference.h:891] Check failed: val >= 0 || val == InferenceContext::kUnknownDim Dimension must be non-negative or equal to InferenceContext::kUnknownDim but got -2
Aborted (core dumped)
```
","['awaiting review', 'type:bug', 'comp:ops', 'comp:core']",2024-02-27T09:52:15Z,2,0,https://github.com/tensorflow/tensorflow/issues/63062,Logical Bug
240,tensorflow/tensorflow,`tf.raw_ops.AvgPool`: negative kernel size is not checked at shape inference step,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Currently shape inference step of `tf.raw_ops.AvgPool` allows negative kernel size.
Note that debug build rejects it [here](https://github.com/tensorflow/tensorflow/blob/7ba14e559de0112cbf59dc6db6f8c6a18283642a/tensorflow/core/framework/common_shape_fns.cc#L1393-L1394):
```C++
  TF_RETURN_IF_ERROR(GetWindowedOutputSizeFromDims(
      c, in_rows_dim, kernel_rows, stride_rows, padding, &output_rows));
```
, where `kernel_rows` is converted to `DimensionOrConstant` and ends up with assertion failure [here](https://github.com/tensorflow/tensorflow/blob/7ba14e559de0112cbf59dc6db6f8c6a18283642a/tensorflow/core/framework/shape_inference.h#L890-L895):
```C++
inline DimensionOrConstant::DimensionOrConstant(int64_t val) : val(val) {
  DCHECK(val >= 0 || val == InferenceContext::kUnknownDim)
      << ""Dimension must be non-negative or equal to ""
         ""InferenceContext::kUnknownDim but got ""
      << val;
}
```


### Standalone code to reproduce the issue

```Python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()

x = tf.raw_ops.AvgPool(
    value=tf.random.normal([1,1,1,1]),
    ksize=[1,-2,1,1],
    strides=[1,1,1,1],
    padding=""SAME"",
    data_format='NHWC',
    name=None
)

print(x)
```


### Relevant log output

Release Build:
```shell
Tensor(""AvgPool:0"", shape=(1, 1, 1, 1), dtype=float32)
```

Debug Build:
```shell
2024-02-23 22:18:43.783609: F ./tensorflow/core/framework/shape_inference.h:891] Check failed: val >= 0 || val == InferenceContext::kUnknownDim Dimension must be non-negative or equal to InferenceContext::kUnknownDim but got -2
Aborted (core dumped)
```
","['awaiting review', 'type:bug', 'comp:ops', 'TF 2.15']",2024-02-23T13:29:12Z,1,0,https://github.com/tensorflow/tensorflow/issues/63034,Logical Bug
241,tensorflow/tensorflow,`tf.raw_ops.ConjugateTranspose`: negative value of `perm` can lead to out-of-bounds read,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

In `tf.raw_ops.ConjugateTranspose`, negative value of `perm` can lead to out-of-bounds read.
[Here](https://github.com/tensorflow/tensorflow/blob/617b5e97d6a9a71e1972dfe6fead5bf460094658/tensorflow/core/kernels/transpose_functor_cpu.cc#L52),
```C++
        i_idx += ratio * in_strides[perm[i]];
```
as there is no guard which checks validity of `perm`, when its value is -1 `in_strides[perm[i]]` can be an out-of-bounds reading(I guess -1 would be interpreted as an `SIZE_T_MAX` or something).
Note that the below code ends up with absl assertion failure in debug build.

### Standalone code to reproduce the issue

```Python
import tensorflow as tf

tf.raw_ops.ConjugateTranspose(
    x=tf.random.normal([2]),
    perm=[-1])
```


### Relevant log output

Release Build:
```shell
    Outputs nothing
```

Debug Build:
```shell
python: external/com_google_absl/absl/container/inlined_vector.h:363: auto absl::InlinedVector<long, 8>::operator[](size_type)::(anonymous class)::operator()() const [T = long, N = 8, A = std::allocator<long>]: Assertion `false && ""i < size()""' failed.
Aborted (core dumped)
```
","['awaiting review', 'type:bug', 'comp:core', 'TF 2.15']",2024-02-23T13:09:32Z,2,0,https://github.com/tensorflow/tensorflow/issues/63033,Security Vulnerability
242,tensorflow/tensorflow,`tf.raw_ops.Transpose` aborts with negative `perm` value,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.Transpose` aborts with negative `perm` value. [gist](https://colab.research.google.com/drive/1r0oUxDcu-uWHgjQGOU8qvcSzD1z2fv-a?usp=sharing)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.raw_ops.Transpose(
    x=tf.random.normal([2,1]),
    perm=[-1,0],
    name=None
)
```


### Relevant log output

```shell
2024-02-20 19:35:25.981306: F tensorflow/core/framework/tensor_shape.cc:356] Check failed: d >= 0 (0 vs. -1)
Aborted (core dumped)
```
","['awaiting review', 'type:bug', 'comp:ops', 'TF 2.15']",2024-02-20T10:39:53Z,3,0,https://github.com/tensorflow/tensorflow/issues/62995,Runtime Error
243,tensorflow/tensorflow,"v2.15.0 docker image print error messages like ""Unable to register cuDNN/cuFFT... factory ""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.15.0

### Custom code

No

### OS platform and distribution

ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```
$ docker pull tensorflow/tensorflow:2.15.0-gpu
$ docker run -it --gpus all tensorflow/tensorflow:2.15.0-gpu
$import tensorflow as tf
2024-02-18 02:43:26.283147: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-02-18 02:43:26.283204: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-02-18 02:43:26.283926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-02-18 02:43:26.289011: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
Num GPUs Available:  8
```

I tried some examples and it worked. But why this error messages show?

### Standalone code to reproduce the issue

```shell
Ubuntu 18.04 nvidia-driver 535.104.12
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux', 'TF 2.15']",2024-02-18T02:51:35Z,8,1,https://github.com/tensorflow/tensorflow/issues/62987,Dependency Issue
244,tensorflow/tensorflow,C++ API `GatherV2` aborts with inappropriate input,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

C++ API `GatherV2` aborts with inappropriate input.
Suspected code location is [here](https://github.com/tensorflow/tensorflow/blob/e193d8ea7776ef5c6f5d769b6fb9c070213e737a/tensorflow/core/ops/array_ops.cc#L1240-L1242),
```C++
          c->set_output(0, c->UnknownShapeOfRank(c->Rank(params_shape) +
                                                 c->Rank(indices_shape) - 1 -
                                                 batch_dims));
```
which calls `UnknownShapeOfRank` with negative value due to lack of validity check.

### Standalone code to reproduce the issue

```C++
#include ""tensorflow/cc/framework/scope.h""
#include ""tensorflow/core/graph/graph.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/cc/ops/array_ops.h""
#include ""tensorflow/cc/ops/standard_ops.h""

using namespace tensorflow;

int main() {
  SessionOptions options;
  ConfigProto & config = options.config;
  config.set_inter_op_parallelism_threads(1);
  config.set_intra_op_parallelism_threads(1);
  config.set_use_per_session_threads(false);
  std::unique_ptr<tensorflow::Session>
    session(tensorflow::NewSession(options));
  Scope scope = Scope::NewRootScope();

  auto params = ops::RandomUniformInt(scope, {1}, 0, 15);
  auto attrs = 
    ops::GatherV2::Attrs()
      .BatchDims(2);
  auto target = ops::GatherV2(scope.WithOpName(""target""), params, 1, 1, attrs);

  GraphDef graph_def;
  TF_CHECK_OK(scope.ToGraphDef(&graph_def));

  Status status = session->Create(graph_def);
  if (!status.ok()) {
    LOG(WARNING) << ""Could not create session: "" << status.message();
  }

  std::vector<Tensor> outputs;
  status = session->Run({}, {""target""}, {""""}, &outputs);
  if (!status.ok()) {
    LOG(WARNING) << ""Could not run session: "" << status.message();
  }

  return 0;
}
```


### Relevant log output

```shell
2024-02-18 03:14:31.909510: F tensorflow/core/framework/shape_inference.cc:705] Check failed: rank >= 0 (0 vs. -2)rank must not be negative
Aborted (core dumped)
```
","['awaiting review', 'type:bug', 'comp:runtime', 'comp:ops']",2024-02-17T18:19:19Z,3,0,https://github.com/tensorflow/tensorflow/issues/62985,Logical Bug
245,tensorflow/tensorflow,`tf.raw_ops.FakeQuantWithMinMaxArgs` aborts when `min>0` in Debug build,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.FakeQuantWithMinMaxArgs` aborts when `min>0` in Debug build(compiled with --config=dbg).
It crashes [here](https://github.com/tensorflow/tensorflow/blob/e193d8ea7776ef5c6f5d769b6fb9c070213e737a/tensorflow/core/kernels/fake_quant_ops_functor.h#L86), 
```C++
eigen_assert(min <= 0.0f && ""min should be <= 0.0"");
```
and it doesn't seem to be consistent with [document](https://www.tensorflow.org/api_docs/python/tf/raw_ops/FakeQuantWithMinMaxArgs).
Note that `tf.raw_ops.FakeQuantWithMinMaxArgsGradient` has the same situation.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# it aborts only when compiled with `--config=dbg`
tf.raw_ops.FakeQuantWithMinMaxArgs(
    inputs=tf.random.normal([1,1,1]),
    min=1,
    max=6,
    num_bits=8,
    narrow_range=False)
```


### Relevant log output

```shell
python: ./tensorflow/core/kernels/fake_quant_ops_functor.h:86: void tensorflow::FakeQuantWithMinMaxArgsFunctor<Eigen::ThreadPoolDevice>::operator()(const Device &, ConstFlat<float>, const float, const float, const int, const int, Flat<float>) [Device = Eigen::ThreadPoolDevice]: Assertion `min <= 0.0f && ""min should be <= 0.0""' failed.
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'comp:core']",2024-02-17T17:48:24Z,2,0,https://github.com/tensorflow/tensorflow/issues/62983,Logical Bug
246,tensorflow/tensorflow,`tf.raw_ops.LoopCond` aborts in Debug build,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.17

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04 LTS

### Mobile device

_No response_

### Python version

3.11.7

### Bazel version

6.5.0

### GCC/compiler version

clang 16

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

`tf.raw_ops.LoopCond` aborts in Debug build(compiled with `--config=dbg`).
In release build, it does not crash.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# it aborts only when compiled with `--config=dbg`
tf.raw_ops.LoopCond(input=True)
```


### Relevant log output

```shell
2024-02-18 02:37:24.039192: F tensorflow/core/graph/graph_partition.cc:644] Check failed: !frame_name.empty() 
Aborted (core dumped)
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux']",2024-02-17T17:39:07Z,6,0,https://github.com/tensorflow/tensorflow/issues/62982,
247,tensorflow/tensorflow,Could not find device for node GenerateBoundingBoxProposals,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When using tf.image.generate_bounding_box_proposals in CPU, it raises the following error: 

```
NotFoundError: Could not find device for node: {{node GenerateBoundingBoxProposals}} = GenerateBoundingBoxProposals[post_nms_topn=300]
All kernels registered for op GenerateBoundingBoxProposals:
  device='GPU'
 [Op:GenerateBoundingBoxProposals] name: 
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

scores = tf.constant([[0.9, 0.8, 0.7], [0.6, 0.5, 0.4]])
bbox_deltas = tf.constant([[1, 1, 1, 1], [2, 2, 2, 2]])
image_info = tf.constant([100, 100, 1])
anchors = tf.constant([[10, 10, 20, 20], [30, 30, 40, 40], [50, 50, 60, 60]])

result = tf.image.generate_bounding_box_proposals(scores, bbox_deltas, image_info, anchors)

print(result)
```


### Relevant log output

_No response_","['type:bug', 'comp:ops', 'TF 2.15']",2024-02-15T07:24:02Z,2,0,https://github.com/tensorflow/tensorflow/issues/62969,
248,tensorflow/tensorflow,Error with Custom Keras Model,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.13.0-17-gf841394b1b7 2.13.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.9.18

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Expected output (ran using v2.8.3-90-g1b8f5c396f0 2.8.4):

```
{'name': 'custom_model', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 96, 96, 3), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_1'}, 'name': 'input_1', 'inbound_nodes': []}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 8, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'conv2d', 'inbound_nodes': [[['input_1', 0, 0, {}]]]}], 'input_layers': [['input_1', 0, 0]], 'output_layers': [['conv2d', 0, 0]]}
{'name': 'model', 'layers': [{'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 96, 96, 3), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_1'}, 'name': 'input_1', 'inbound_nodes': []}, {'class_name': 'Conv2D', 'config': {'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 8, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'name': 'conv2d', 'inbound_nodes': [[['input_1', 0, 0, {}]]]}], 'input_layers': [['input_1', 0, 0]], 'output_layers': [['conv2d', 0, 0]]}
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
Model: ""custom_model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 96, 96, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 94, 94, 8)         224       
                                                                 
=================================================================
Total params: 224
Trainable params: 224
Non-trainable params: 0
_________________________________________________________________
Model: ""model""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 96, 96, 3)]       0         
                                                                 
 conv2d (Conv2D)             (None, 94, 94, 8)         224       
                                                                 
=================================================================
Total params: 224
Trainable params: 224
Non-trainable params: 0
_________________________________________________________________
```


### Standalone code to reproduce the issue

```shell
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf  # noqa: E402


class CustomModel(tf.keras.Model):
    def __init__(self, **kwargs):
        super(CustomModel, self).__init__(**kwargs)


input_shape = (96, 96, 3)
inputs = tf.keras.layers.Input(shape=input_shape)
outputs = tf.keras.layers.Conv2D(
    filters=8, kernel_size=3, strides=1)(inputs)

cust_model = CustomModel(inputs=inputs, outputs=outputs)
print(cust_model.get_config())

model = tf.keras.Model(inputs=inputs, outputs=outputs)
print(model.get_config())

cust_model_path = '/home/ubuntu/automltraining/my_custom_model.h5'
cust_model.save(cust_model_path)

model_path = '/home/ubuntu/automltraining/my_model.h5'
model.save(model_path)

cust_model2 = tf.keras.models.load_model(
    cust_model_path, custom_objects={""CustomModel"": CustomModel})
model2 = tf.keras.models.load_model(model_path)

cust_model2.summary()
model2.summary()
```


### Relevant log output

```shell
{'name': 'custom_model', 'trainable': True}
{'name': 'model', 'trainable': True, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 96, 96, 3), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'input_1'}, 'registered_name': None, 'name': 'input_1', 'inbound_nodes': []}, {'module': 'keras.layers', 'class_name': 'Conv2D', 'config': {'name': 'conv2d', 'trainable': True, 'dtype': 'float32', 'filters': 8, 'kernel_size': (3, 3), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'groups': 1, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'GlorotUniform', 'config': {'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}, 'registered_name': None, 'build_config': {'input_shape': (None, 96, 96, 3)}, 'name': 'conv2d', 'inbound_nodes': [[['input_1', 0, 0, {}]]]}], 'input_layers': [['input_1', 0, 0]], 'output_layers': [['conv2d', 0, 0]]}
/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.
  saving_api.save_model(
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
Traceback (most recent call last):
  File ""/home/ubuntu/automltraining/get_config_debug.py"", line 28, in <module>
    cust_model2 = tf.keras.models.load_model(
  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/keras/src/saving/saving_api.py"", line 238, in load_model
    return legacy_sm_saving_lib.load_model(
  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/keras/src/engine/training.py"", line 3246, in from_config
    raise TypeError(
TypeError: Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the  constructor to <class '__main__.CustomModel'>, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of CustomModel from its config.

Received config={'name': 'custom_model', 'trainable': True}

Error encountered during deserialization: __init__() missing 2 required positional arguments: 'inputs' and 'outputs'
```
","['type:bug', 'comp:keras', 'TF 2.13']",2024-02-09T05:34:22Z,5,1,https://github.com/tensorflow/tensorflow/issues/62927,
249,tensorflow/tensorflow,MLP subsequence is inconsistent with full sequence,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-105526-gf9a553f735c 2.16.0-dev20240206

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

**Issue:** Given a two layer MLP with a reshape, I notice that `model(data)[:, :end_idx])` does not equal `model(data[:, :end_idx])`.
That is, comparing the output of the model on a subsequence of the data is _not_ the same as the corresponding subsequence of the output, even when the MLP should not have any interactions across the time-dimension.

Can you confirm this is expected behaviour? I notice that GPUs (T4 on colab) has more inconsistencies than CPU, so I assume this is some sort of computional optimizations. 
Is there anyway to guarantee that the outputs match? I have tried  `tf.config.experimental.enable_op_determinism()`.

Thanks!
Erik

### Standalone code to reproduce the issue

```shell
Colab: https://colab.research.google.com/drive/1ffw4XhJKocu71ORANA_70eG7Tl20-cLh?usp=sharing


Code from colab:

import tensorflow as tf

tf.keras.utils.set_random_seed(1)
tf.config.experimental.enable_op_determinism()

class BrokenModel(tf.keras.Model):

    def __init__(self,):
        super().__init__()

        self.dense1 = tf.keras.layers.Dense(
            32, activation=None,
        )
        self.dense2 = tf.keras.layers.Dense(
            256 , activation=None,
        )

    def call(self, x: tf.Tensor):
        dims = tf.shape(x)

        # First dense
        x = self.dense1(x)

        # Flatten last two dimensions
        x = tf.reshape(
            x, (dims[0], dims[1],  -1)
        )

        # Second dense
        x = self.dense2(x)
        return x

def evaluate_subsequence_consistency(
    model,
    data,
):

    orig_output = model(data)

    for end_idx in range(0, data.shape[1]):
        # Slice subsequence
        output = model(data[:, :end_idx])

        # Check if output is the same
        res_str = """"
        for j in range(0, end_idx):
            # Assumes output has shape [B, T, ...]
            # Check if each time step is identical between subsequence and full sequence
            orig_data = orig_output[:, j].numpy()
            new_data = output[:, j].numpy()

            if tf.experimental.numpy.allclose(orig_data, new_data):
                res_str += "".""
            else:
                res_str += ""X""

        print(f""model(data)[:, :{end_idx}] vs model(data[:, :{end_idx}]) => {res_str}"")

model = BrokenModel()
data = tf.ones((1, 313, 33, 3))
evaluate_subsequence_consistency(model, data)
```


### Relevant log output

```shell
X = output does not match, . = output is matching

model(data)[:, :1] vs model(data[:, :1]) => X
model(data)[:, :2] vs model(data[:, :2]) => XX
model(data)[:, :3] vs model(data[:, :3]) => XXX
model(data)[:, :4] vs model(data[:, :4]) => XXXX
model(data)[:, :5] vs model(data[:, :5]) => XXXXX
model(data)[:, :6] vs model(data[:, :6]) => XXXXXX
model(data)[:, :7] vs model(data[:, :7]) => XXXXXXX
model(data)[:, :8] vs model(data[:, :8]) => XXXXXXXX
model(data)[:, :9] vs model(data[:, :9]) => XXXXXXXXX
model(data)[:, :10] vs model(data[:, :10]) => XXXXXXXXXX
model(data)[:, :11] vs model(data[:, :11]) => XXXXXXXXXXX
model(data)[:, :12] vs model(data[:, :12]) => XXXXXXXXXXXX
model(data)[:, :13] vs model(data[:, :13]) => XXXXXXXXXXXXX
model(data)[:, :14] vs model(data[:, :14]) => XXXXXXXXXXXXXX
model(data)[:, :15] vs model(data[:, :15]) => XXXXXXXXXXXXXXX
model(data)[:, :16] vs model(data[:, :16]) => XXXXXXXXXXXXXXXX
model(data)[:, :17] vs model(data[:, :17]) => XXXXXXXXXXXXXXXXX
model(data)[:, :18] vs model(data[:, :18]) => XXXXXXXXXXXXXXXXXX
model(data)[:, :19] vs model(data[:, :19]) => XXXXXXXXXXXXXXXXXXX
model(data)[:, :20] vs model(data[:, :20]) => XXXXXXXXXXXXXXXXXXXX
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2024-02-07T15:08:07Z,2,0,https://github.com/tensorflow/tensorflow/issues/62917,
250,tensorflow/tensorflow,Cannot disable XLA and/or JIT,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.15.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.10.12 (Unrelated)

### Bazel version

6.3.2

### GCC/compiler version

Clang 18

### CUDA/cuDNN version

12.2

### GPU model and memory

A100 and A10

### Current behavior?

After upgrading from `2.12.1` to `2.15.0` we observed a lot **new** logs when starting a **C++** service using Tensorflow, that it calls `ptxas` to compile some generated PTX, a snippet is attached below.

We tried a bunch of options in TF_XLA_FLAGS such as `--tf_xla_auto_jit=-1`, `--tf_mlir_enable_mlir_bridge=0`, `--tf_xla_cpu_global_jit=0`, `--tf_xla_clustering_fuel=0`, etc. but it still compiles those ops in the pass of `CreateGpuKernelToBlobPass` anyway.

I wonder if anything related to XLA / JIT changed between `2.12.1` and `2.15.0`? And is there a way to simply disable all XLA and JIT?

BTW: Both Tensorflow versions were built with XLA and CUDA supports, with `TF_CUDA_COMPUTE_CAPABILITIES=""7.0,7.5,8.0,8.6,9.0""`.

### Standalone code to reproduce the issue

```shell
It is a C++ service which loads model and do inference, we do not currently have a minimal example at this time. However, we can try providing detailed context as much as possible if we can narrow the scenario down to a considerable small portion of the service.
```


### Relevant log output

```shell
2024-01-29 15:14:43.203921: I external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:565] Compile module main_kernel_0 time: 7.33 ms (cumulative: 82.4 ms, max: 9.23 ms, #called: 15)2024-01-29 15:14:43.204021: I external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:565] Compile module main_kernel time: 7.43 ms (cumulative: 89.9 ms, max: 9.23 ms, #called: 16)2024-01-29 15:14:43.204084: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:263] ptx written to: /tmp/tempfile-jscs02-ai-deep-dev-4a10-01-d460ae06-1673-61010665dbba92024-01-29 15:14:43.204112: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:295] /usr/local/cuda-12.2/bin/ptxas /tmp/tempfile-jscs02-ai-deep-dev-4a10-01-d460ae06-1673-61010665dbba9 -o /tmp/tempfile-jscs02-ai-deep-dev-4a10-01-d460ae06-1673-61010665dbc05 -arch=sm_86 --warn-on-spills -v 2024-01-29 15:14:43.204182: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:263] ptx written to: /tmp/tempfile-jscs02-ai-deep-dev-4a10-01-b79066f7-1673-61010665dbc0d2024-01-29 15:14:43.204209: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:295] /usr/local/cuda-12.2/bin/ptxas /tmp/tempfile-jscs02-ai-deep-dev-4a10-01-b79066f7-1673-61010665dbc0d -o /tmp/tempfile-jscs02-ai-deep-dev-4a10-01-b79066f7-1673-61010665dbc66 -arch=sm_86 --warn-on-spills -v2024-01-29 15:14:43.204334: I external/local_xla/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:565] Compile module main_kernel_1 time: 7.71 ms (cumulative: 97.6 ms, max: 9.23 ms, #called: 17)2024-01-29 15:14:43.204499: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:263] ptx written to: /tmp/tempfile-jscs02-ai-deep-dev-4a10-01-7941e65c-1673-61010665dbd48
2024-01-29 15:14:43.204528: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:295] /usr/local/cuda-12.2/bin/ptxas /tmp/tempfile-jscs02-ai-deep-dev-4a10-01-7941e65c-1673-61010665dbd48 -o /tmp/tempfile-jscs02-ai-deep-dev-4a10-01-7941e65c-1673-61010665dbda4 -arch=sm_86 --warn-on-spills -v2024-01-29 15:14:43.236003: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:333] ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function 'main_kernel' for 'sm_86'ptxas info    : Function properties for main_kernel    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loadsptxas info    : Used 12 registers, 452 bytes cmem[0]
2024-01-29 15:14:43.236589: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:333] ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function 'main_kernel' for 'sm_86'ptxas info    : Function properties for main_kernel    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 14 registers, 476 bytes cmem[0]

2024-01-29 15:14:43.237263: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:333] ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function 'main_kernel' for 'sm_86'
ptxas info    : Function properties for main_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 14 registers, 508 bytes cmem[0]

2024-01-29 15:14:43.238685: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:333] ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function 'main_kernel' for 'sm_86'
ptxas info    : Function properties for main_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 14 registers, 532 bytes cmem[0]

2024-01-29 15:14:43.240855: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:333] ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function 'main_kernel' for 'sm_86'
ptxas info    : Function properties for main_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 15 registers, 564 bytes cmem[0]

2024-01-29 15:14:43.241720: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:333] ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function 'main_kernel' for 'sm_86'
ptxas info    : Function properties for main_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 18 registers, 444 bytes cmem[0]

2024-01-29 15:14:43.241731: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:333] ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function 'main_kernel' for 'sm_86'
ptxas info    : Function properties for main_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 16 registers, 444 bytes cmem[0]

2024-01-29 15:14:43.242423: I external/local_xla/xla/stream_executor/gpu/asm_compiler.cc:333] ptxas info    : 0 bytes gmem
ptxas info    : Compiling entry function 'main_kernel' for 'sm_86'
ptxas info    : Function properties for main_kernel
    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
ptxas info    : Used 18 registers, 452 bytes cmem[0]

libunwind: __unw_add_dynamic_fde: bad fde: FDE is really a CIE
2024-01-29 15:14:43.416923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:812] GpuDevice::ComputeHelper scheduled dense/clip_by_value_65 op Maximum on GPU 0 stream[0]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.15']",2024-01-29T16:09:44Z,7,0,https://github.com/tensorflow/tensorflow/issues/62864,
251,tensorflow/tensorflow,tf.concat (and tf.transpose) inside a for loop with tf.range in the context of a GradientTape while using XLA dosn't work,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.15.0.post1, 2.15.0

### Custom code

Yes

### OS platform and distribution

WSL Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2

### GPU model and memory

RTX 3080 Ti

### Current behavior?

### Current behavior
Even tho the shapes of the elements which are concatenated are well defined to compile time, when running the code provided, the execution fails with the error log provided. This is also the case, if you enforce the shapes with using tf.reshape(). 
If you remove either 
- the GradientTape context
- or the for loop with tf.range
- or the XLA compilation 

the code will work as expected.

Unrolling the for loop with range() is not desired since the number of iterations will be >50000 in the final project. Also converting the for loop in a tf.while loop with maximum_iterations specified will result in the same error. Specifying the batchsize to a constant value (also in the input_signature) won't resolve the issue either. Also changing the tf.device between GPU / CPU won't resolve the issue.
The same error arises if you try to use tf.transpose()


### Expected behavior
The arrays with well defined shapes at compile time will be concatenated / transposed when using a for loop with tf.range in the context of a GradientTape while using XLA (jit_compile=True)

### Standalone code to reproduce the issue

```shell
class Model():
    def __init__(self, batchsize):
        self.batchsize = batchsize
        self.g = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=""g"")
        self.m = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=""m"")
        self.d = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=""d"")
        self.k = tf.Variable(initial_value=tf.ones([self.batchsize,1,1]), trainable=True, name=""k"")
        self.y0 = tf.ones([self.batchsize, 2, 1])


    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 2, 1), dtype=tf.float32)],
                 jit_compile=False)
    def compute(self, yi):
        A1 = tf.concat([tf.zeros_like(self.m), tf.ones_like(self.m)], 2)
        A2 = tf.concat([-self.k/self.m, -self.d/self.m], 2)
        A = tf.concat([A1, A2], 1)
        #A = tf.transpose(A, perm=[0, 2, 1])
        B = tf.concat([tf.zeros_like(self.g), self.g], 1)

        dy = tf.linalg.matmul(A, yi) + B
        return dy
    

class Estimator():
    def __init__(self, model):
        self.model = model
        self.dt = tf.constant(0.001)

    @tf.function(jit_compile=True, reduce_retracing=False)
    def estimate(self):
        with tf.GradientTape() as tape:
            yi = self.model.y0
            for i in tf.range(10):
                dyi = self.model.compute(yi)
                yi = yi + dyi*self.dt
        grads = tape.gradient(yi, [self.model.g, self.model.m, self.model.d, self.model.k])
        return grads



device = ""CPU:0""
#device = ""GPU:0""

with tf.device(device):    
    batchsize = 5
    model = Model(batchsize)
    estimator = Estimator(model)
    grads = estimator.estimate()
```


### Relevant log output

```shell
OP_REQUIRES failed at concat_op.cc:168 : INVALID_ARGUMENT: Input 0 to node `gradient_tape/while/gradients/while/StatefulPartitionedCall_grad/PartitionedCall/gradients/concat_3_grad/ConcatOffset` with op ConcatOffset must be a compile-time constant.

XLA compilation requires that operator arguments that represent shapes or dimensions be evaluated to concrete values at compile time. This error means that a shape or dimension argument could not be evaluated at compile time, usually because the value of the argument depends on a parameter to the computation, on a variable, or on a stateful operation such as a random number generator.
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.15']",2024-01-29T09:22:38Z,8,0,https://github.com/tensorflow/tensorflow/issues/62861,
252,tensorflow/tensorflow,Tensorflow distributes training throws exception on mac m2,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

Mac OS 14.2.1

### Mobile device

_No response_

### Python version

3.11.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am trying to run distributed training using `tf.distribute.experimental.MultiWorkerMirroredStrategy()` on two Mac M2 machines. However, training does not start on the GPU, and the code throws the attached exception.
The distributed training works fine if I use CPU only.
I have installed the latest `tensorflow-metal 1.1.0`.

Is `MultiWorkerMirroredStrategy` supported on Mac M2?

### Standalone code to reproduce the issue

```shell
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import Sequential
import tensorflow as tf
from tensorflow.keras.layers import Dense
import datetime
import os
import keras
import json
import glob


print(""Tensforflow version: "", tf.__version__)
print(""Availabe devices: "", devices)
if len(devices) == 0:
    print(""No devices for mac found"")
    exit(1)

    

directory = os.environ['TF_FOLDER']

checkpoint_dir = os.path.join(directory, ""ckpt"")
if not os.path.exists(checkpoint_dir):
    os.makedirs(checkpoint_dir)

path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'
df = read_csv(path, header=None)

X, y = df.values[:, :-1], df.values[:, -1]

X = X.astype('float32')

y = LabelEncoder().fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

n_features = X_train.shape[1]


def get_compiled_model():
       model = tf.keras.Sequential([
            tf.keras.layers.Flatten(input_shape=(n_features,)),
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dense(10)
        ])
       model.summary()
       model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])
       return model

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
print(""Number of devices: {}"".format(strategy.num_replicas_in_sync))

with strategy.scope():
    model = get_compiled_model()

log_dir = os.path.join(directory, ""logs/fit/"") + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)


class CustomModelCheckpoint(keras.callbacks.ModelCheckpoint):
    def __init__(self, filepath, max_to_keep=100, **kwargs):
        super().__init__(filepath, **kwargs)
        self.filepath = filepath
        self.max_to_keep = max_to_keep

    def on_epoch_end(self, epoch, logs=None):
        super().on_epoch_end(epoch, logs)
        files = sorted(glob.glob(self.filepath.format(epoch='*')))
        if len(files) > self.max_to_keep:
            for f in files[:-self.max_to_keep]:
                os.remove(f)


callbacks = [
        CustomModelCheckpoint(
            filepath=checkpoint_dir + ""/ckpt-{epoch}"", save_freq=""epoch"", max_to_keep=10, save_weights_only=True
        ),
       keras.callbacks.TensorBoard('tensorboard_logs')
    ]

latest = tf.train.latest_checkpoint(checkpoint_dir)
if latest:
    print(""Loading model checkpoint {} ...\n"".format(latest))
    model.load_weights(latest)

model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1,callbacks=callbacks)

loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f'Test Accuracy: {acc:.3f}')
```


### Relevant log output

```shell
2024-01-28 14:46:40.395499: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.
Epoch 1/100
2024-01-28 14:46:40.778281: W tensorflow/core/framework/op_kernel.cc:1803] INTERNAL: Failed to build OpKernel for Add : No registered 'Add' OpKernel for 'GPU' devices compatible with node {{node Add}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_INT64, _device=""/job:worker/replica:0/task:0/device:GPU:0""
	.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_BFLOAT16]
  device='DEFAULT'; T in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_STRING]

Traceback (most recent call last):
  File ""/Users/inet11/git/tensorflow/model.py"", line 129, in <module>
    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1,callbacks=callbacks)
  File ""/Users/inet11/git/tensorflow/email/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/Users/inet11/git/tensorflow/email/tf/lib/python3.11/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node Add defined at (most recent call last):
<stack traces unavailable>
2 root error(s) found.
  (0) INTERNAL:  Failed to build OpKernel for Add : No registered 'Add' OpKernel for 'GPU' devices compatible with node {{node Add}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_INT64, _device=""/job:worker/replica:0/task:0/device:GPU:0""
	.  Registered:  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_BFLOAT16]
  device='DEFAULT'; T in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_STRING]

	 [[CollectiveReduceV2]]
  (1) CANCELLED:  Function was cancelled before it was started
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_1260]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:dist-strat', 'TF 2.15']",2024-01-28T23:20:51Z,0,0,https://github.com/tensorflow/tensorflow/issues/62859,
253,tensorflow/tensorflow,RAM memory leak with tf.function when training multiple models in a loop,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04.3 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

When I train multiple models in a loop, if I decorate the `train()` function with `@tf.function`, then the memory usage keeps on increasing after each loop iteration, even when I delete the model at the end of each loop and clear the TensorFlow graph/session.

The memory leak does not occur when `@tf.function` is removed. However, model training performance is significantly slower.

Colab notebook to reproduce issue:
https://colab.research.google.com/drive/1sJsGmcFeZVx6ImNbqnBsgF_LzIyPxXPW?usp=sharing

### Standalone code to reproduce the issue

```python
import gc
import os

import psutil
import tensorflow as tf


class MyModel:
    def __init__(self):
        self.dnn = tf.keras.Sequential([
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(256),
            tf.keras.layers.Dense(1),
        ])
        self.optimizer = tf.optimizers.Adam()
    
    @tf.function   # if we remove this @tf.function decorator, then there is no memory leak
    def train(self, X):
        with tf.GradientTape() as tape:
            loss = tf.reduce_sum(self.dnn(X))
        grads = tape.gradient(loss, self.dnn.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.dnn.trainable_variables))

process = psutil.Process(os.getpid())
rss = int(process.memory_info().rss / 1024 / 1024)  # in MB
print(f'rss: {rss} MB')

X = tf.ones((50, 80))
for i in range(50):
    model = MyModel()
    for _ in range(20):
        model.train(X)

    del model
    tf.keras.backend.clear_session()
    tf.compat.v1.reset_default_graph()
    gc.collect()

    new_rss = int(process.memory_info().rss / 1024 / 1024)
    if new_rss > rss:
        rss_increase = new_rss - rss
        rss = new_rss
        print(f'Iter {i:05d}, rss increase: {rss_increase} MB, rss: {rss} MB')
```


### Relevant log output

```shell
rss: 593 MB
Iter 00000, rss increase: 31 MB, rss: 624 MB
Iter 00001, rss increase: 6 MB, rss: 630 MB
Iter 00002, rss increase: 5 MB, rss: 635 MB
Iter 00003, rss increase: 5 MB, rss: 640 MB
Iter 00004, rss increase: 5 MB, rss: 645 MB
Iter 00005, rss increase: 5 MB, rss: 650 MB
Iter 00006, rss increase: 5 MB, rss: 655 MB
Iter 00007, rss increase: 5 MB, rss: 660 MB
Iter 00008, rss increase: 5 MB, rss: 665 MB
Iter 00009, rss increase: 5 MB, rss: 670 MB
Iter 00010, rss increase: 4 MB, rss: 674 MB
---  <omitting some rows for brevity>  ---
Iter 00040, rss increase: 5 MB, rss: 822 MB
Iter 00041, rss increase: 5 MB, rss: 827 MB
Iter 00042, rss increase: 5 MB, rss: 832 MB
Iter 00043, rss increase: 5 MB, rss: 837 MB
Iter 00044, rss increase: 5 MB, rss: 842 MB
Iter 00045, rss increase: 5 MB, rss: 847 MB
Iter 00046, rss increase: 5 MB, rss: 852 MB
Iter 00047, rss increase: 5 MB, rss: 857 MB
Iter 00048, rss increase: 5 MB, rss: 862 MB
Iter 00049, rss increase: 5 MB, rss: 867 MB
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:tf.function', 'TF 2.15']",2024-01-27T05:06:09Z,1,0,https://github.com/tensorflow/tensorflow/issues/62854,
254,tensorflow/tensorflow,[ tf-opt ] Keras Official Implementation of Stable Diffusion fails to generate stablehlo mlir,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.14.0-rc1-21-g4dacf3f368e 2.14.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

After saving the diffusion model in saved_model format using the code given below, I ran the following command to convert the saved_model into an input mlir. 

```
tf-mlir-translate --savedmodel-objectgraph-to-mlir --tf-savedmodel-exported-names=serving_fn ./signed-model/ -o tf_executor.mlir
```

This actually produces a large mlir file of 6.5 GB . Then I ran this: 
``` tf-opt --tf-executor-graph-pruning --tf-executor-to-functional-conversion --canonicalize --tf-lower-to-mlprogram-and-hlo ./tf_executor.mlir -o stablehlo.mlir```

This process runs for a couple of minutes and ends without any error. However, in the ```stablehlo.mlir``` does not have any mlir in it. Its an empty file with the following in it: ""module{}"". It does not provide any error message at all. 

I was expecting the second command to generate a stablehlo mlir file corresponding to the diffusion model which I could later pass on to the iree-importer for importing into iree. 


### Standalone code to reproduce the issue

```shell
from keras_cv import models
from keras_cv.src.models.stable_diffusion.constants import _ALPHAS_CUMPROD
import tensorflow as tf
from keras_cv.src.backend import ops
import math
from keras_cv.models.stable_diffusion import DiffusionModel

IMG_HEIGHT = 512
IMG_WIDTH = 512
MAX_PROMPT_LENGTH = 77
ALPHAS_CUMPROD_TF = tf.constant(_ALPHAS_CUMPROD)
UNCONDITIONAL_GUIDANCE_SCALE = 7.5
HIDDEN_DIM = 768
SEED = None


signature_dict = {
    ""context"": tf.TensorSpec(shape=[None, MAX_PROMPT_LENGTH, HIDDEN_DIM], dtype=tf.float32, name=""context""),
    ""unconditional_context"": tf.TensorSpec(
        shape=[None, MAX_PROMPT_LENGTH, HIDDEN_DIM], dtype=tf.float32, name=""unconditional_context""
    ),
    ""num_steps"": tf.TensorSpec(shape=[], dtype=tf.int32, name=""num_steps""),
    ""batch_size"": tf.TensorSpec(shape=[], dtype=tf.int32, name=""batch_size""),
}

def diffusion_model_exporter(model: tf.keras.Model):
    @tf.function
    def get_timestep_embedding(timestep, batch_size, dim=320, max_period=10000):
        half = dim // 2
        range = ops.cast(ops.arange(0, half), ""float32"")
        freqs = ops.exp(-math.log(max_period) * range / half)
        args = ops.convert_to_tensor([timestep], dtype=""float32"") * freqs
        embedding = ops.concatenate([ops.cos(args), ops.sin(args)], 0)
        embedding = ops.reshape(embedding, [1, -1])
        return ops.repeat(embedding, batch_size, axis=0)
    
    @tf.function(input_signature=[signature_dict])
    def serving_fn(inputs):
        img_height = tf.cast(tf.math.round(IMG_HEIGHT / 128) * 128, tf.int32)
        img_width = tf.cast(tf.math.round(IMG_WIDTH / 128) * 128, tf.int32)

        batch_size = inputs[""batch_size""]
        num_steps = inputs[""num_steps""]

        context = inputs[""context""]
        unconditional_context = inputs[""unconditional_context""]

        latent = tf.random.normal((batch_size, img_height // 8, img_width // 8, 4))

        timesteps = tf.range(1, 1000, 1000 // num_steps)
        alphas = tf.map_fn(lambda t: ALPHAS_CUMPROD_TF[t], timesteps, dtype=tf.float32)
        alphas_prev = tf.concat([[1.0], alphas[:-1]], 0)

        index = num_steps - 1
        latent_prev = None
        for timestep in timesteps[::-1]:
            latent_prev = latent
            t_emb = get_timestep_embedding(timestep, batch_size)
            unconditional_latent = model(
                [latent, t_emb, unconditional_context], training=False
            )
            latent = model([latent, t_emb, context], training=False)
            latent = unconditional_latent + UNCONDITIONAL_GUIDANCE_SCALE * (
                latent - unconditional_latent
            )
            a_t, a_prev = alphas[index], alphas_prev[index]
            pred_x0 = (latent_prev - tf.math.sqrt(1 - a_t) * latent) / tf.math.sqrt(a_t)
            latent = (
                latent * tf.math.sqrt(1.0 - a_prev) + tf.math.sqrt(a_prev) * pred_x0
            )
            index = index - 1

        return {""latent"": latent}

    return serving_fn

diffuser = DiffusionModel(IMG_HEIGHT, IMG_WIDTH, MAX_PROMPT_LENGTH)

tf.saved_model.save(diffuser, './stable-diffusion/signed-model', signatures={""serving_default"": diffusion_model_exporter(diffuser)} )
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF2.14']",2024-01-25T20:19:45Z,2,0,https://github.com/tensorflow/tensorflow/issues/62847,
255,tensorflow/tensorflow,Multi-GPU training with gradient propagation in FP16 with XLA fails.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.14

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8/8.7

### GPU model and memory

A100 40GB (20GB MiG)

### Current behavior?

Following advice given [here](https://www.youtube.com/watch?v=6ovfZW8pepo), I implemented gradient propagation in FP16 and concurrent backpropagation with gradient propagation. I am using mixed precision with XLA enabled.

The custom training step works fine when XLA is not enabled. However, when I enable it, I get the error that you can see in the log.

This is run on 2 GPUs using MultiWorkerMirroredStrategy.

### Standalone code to reproduce the issue

```python
import json
import os

import tensorflow as tf


def get_replica_hostnames():
    ...


def get_replica_id():
    ...


def set_multiworker_env_config():
    hostnames = get_replica_hostnames()
    replica_index = get_replica_id()

    os.environ[""TF_CONFIG""] = json.dumps(
        {
            ""cluster"": {
                ""worker"": hostnames,
            },
            ""task"": {""type"": ""worker"", ""index"": replica_index},
        }
    )


class Model(tf.keras.models.Model):
    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)

        self._embedder = tf.keras.Sequential(
            [
                tf.keras.layers.Conv2D(
                    filters=8,
                    kernel_size=3,
                    padding=""same"",
                    activation=tf.keras.activations.relu,
                    use_bias=False,
                ),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Conv2D(
                    filters=8,
                    kernel_size=3,
                    padding=""same"",
                    activation=tf.keras.activations.relu,
                    use_bias=False,
                ),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.MaxPool2D(),
                tf.keras.layers.GlobalAveragePooling2D(),
            ]
        )

        self._classifier = tf.keras.layers.Dense(550)

    def call(self, x: tf.Tensor) -> tf.Tensor:
        x = self._embedder(x)
        x = self._classifier(x)
        x = tf.keras.layers.Activation(""linear"", dtype=""float32"")(x)
        return x

    def train_step(self, data):
        x, y, sample_weight = tf.keras.utils.unpack_x_y_sample_weight(data)

        # Run forward pass.
        with tf.GradientTape() as tape:
            y_pred = self(x, training=True)
            loss = self.compute_loss(x, y, y_pred, sample_weight)
            loss = self.optimizer.get_scaled_loss(loss)

        self._validate_target_and_loss(y, loss)

        # Run backward pass.
        grads = tape.gradient(loss, self.trainable_variables)
        grads = [tf.cast(grad, ""float16"") for grad in grads]
        all_reduced_grads = tf.distribute.get_replica_context().all_reduce(
            tf.distribute.ReduceOp.SUM,
            grads,
            tf.distribute.experimental.CommunicationOptions(
                bytes_per_pack=50 * 1024 * 1024,
            ),
        )
        all_reduced_grads = [tf.cast(grad, ""float32"") for grad in all_reduced_grads]
        all_reduced_grads = self.optimizer.get_unscaled_gradients(all_reduced_grads)

        self.optimizer.apply_gradients(
            zip(all_reduced_grads, self.trainable_variables),
            skip_gradients_aggregations=True,
        )

        return self.compute_metrics(x, y, y_pred, sample_weight)


def create_dummy_dataset(batch_size: int) -> tf.data.Dataset:
    X = np.random.rand(batch_size, 384, 640, 1)
    y = np.random.randint(550, size=batch_size)
    return tf.data.Dataset.from_tensor_slices((X, y)).batch(batch_size, True).repeat()


def train():
    tf.keras.mixed_precision.set_global_policy(""mixed_float16"")

    set_multiworker_env_config()
    strategy = tf.distribute.MultiWorkerMirroredStrategy()
    num_replicas = strategy.num_replicas_in_sync

    batch_size = 384 * num_replicas
    dataset = create_dummy_dataset(batch_size)
    dataset = strategy.experimental_distribute_dataset(dataset)

    with strategy.scope():
        model = Model()
        model.compile(
            loss=tf.keras.losses.SparseCategoricalCrossentropy(
                from_logits=True,
            ),
            optimizer=tf.keras.optimizers.Adam(
                learning_rate=1e-3,
                weight_decay=1e-5,
            ),
            metrics=[
                ""accuracy"",
            ],
            jit_compile=True,
        )

    model.fit(
        dataset,
        epochs=10,
        steps_per_epoch=100,
    )


if __name__ == ""__main__"":
    train()
```


### Relevant log output

```shell
2024-01-15 14:06:48.704206: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-15 14:06:48.704294: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-15 14:06:48.704350: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-15 14:06:48.712047: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-15 14:06:49.711421: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-01-15 14:06:51.459926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18184 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:01:00.0, compute capability: 8.0
2024-01-15 14:06:51.469378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:worker/replica:0/task:0/device:GPU:0 with 18184 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:01:00.0, compute capability: 8.0
2024-01-15 14:06:51.490217: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://gen-svc-82ee2ab7-aae8-44b2-a5ab-1f9fd0401075:80
2024-01-15 14:06:51.495668: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:551] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 3072789976930509670
2024-01-15 14:06:51.495948: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:299] Coordination agent has successfully connected.
2024-01-15 14:06:52.263966: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:551] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 14088291762770279744
2024-01-15 14:07:16.278224: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.
2024-01-15 14:07:17.178980: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.
2024-01-15 14:07:17.905980: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.
2024-01-15 14:07:22.444540: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.
Epoch 1/10
2024-01-15 14:07:24.533902: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1509949440 exceeds 10% of free system memory.
2024-01-15 14:07:27.793986: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f080000c190 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-01-15 14:07:27.794120: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB MIG 3g.20gb, Compute Capability 8.0
2024-01-15 14:07:28.936911: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-01-15 14:07:30.163125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8906
2024-01-15 14:08:24.307063: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:328] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_232', 12 bytes spill stores, 12 bytes spill loads

2024-01-15 14:08:24.726556: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:328] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_232', 12 bytes spill stores, 12 bytes spill loads

2024-01-15 14:08:24.789581: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:328] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_232', 8 bytes spill stores, 8 bytes spill loads

2024-01-15 14:08:26.317751: I tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:328] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_392', 92 bytes spill stores, 92 bytes spill loads

2024-01-15 14:08:26.687122: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:624 : UNKNOWN: <unknown>:0: error: loc(""all-reduce-start.1""): 'lmhlo_gpu.all_reduce_start' op requires the same element type for all operands
<unknown>:0: note: loc(""all-reduce-start.1""): see current operation: 
%225 = ""lmhlo_gpu.all_reduce_start""(%222, %8, %224, %18, %220, %222, %8, %224, %18, %220) ({
^bb0(%arg58: tensor<f32>, %arg59: tensor<f32>):
  %251 = ""mhlo.add""(%arg58, %arg59) : (tensor<f32>, tensor<f32>) -> tensor<f32>
  ""mhlo.return""(%251) : (tensor<f32>) -> ()
}) {channel_id = #mhlo.channel_handle<handle = 4294967300, type = 0>, constrain_layout = false, is_sync = true, replica_groups = dense<> : tensor<0x0xi64>, use_global_device_ids = false} : (memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>) -> !mhlo.token

2024-01-15 14:08:26.687331: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10204104846648882243
2024-01-15 14:08:26.687358: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5394287870414942276
2024-01-15 14:08:26.687399: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8969947325169437454
Traceback (most recent call last):
  File ""/kirax_source/train.py"", line 133, in <module>
    train()
  File ""/kirax_source/train.py"", line 125, in train
    model.fit(
  File ""/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.UnknownError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.11/threading.py"", line 1002, in _bootstrap

  File ""/usr/lib/python3.11/threading.py"", line 1045, in _bootstrap_inner

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.11/threading.py"", line 1002, in _bootstrap

  File ""/usr/lib/python3.11/threading.py"", line 1045, in _bootstrap_inner

2 root error(s) found.
  (0) UNKNOWN:  <unknown>:0: error: loc(""all-reduce-start.1""): 'lmhlo_gpu.all_reduce_start' op requires the same element type for all operands
<unknown>:0: note: loc(""all-reduce-start.1""): see current operation: 
%225 = ""lmhlo_gpu.all_reduce_start""(%222, %8, %224, %18, %220, %222, %8, %224, %18, %220) ({
^bb0(%arg58: tensor<f32>, %arg59: tensor<f32>):
  %251 = ""mhlo.add""(%arg58, %arg59) : (tensor<f32>, tensor<f32>) -> tensor<f32>
  ""mhlo.return""(%251) : (tensor<f32>) -> ()
}) {channel_id = #mhlo.channel_handle<handle = 4294967300, type = 0>, constrain_layout = false, is_sync = true, replica_groups = dense<> : tensor<0x0xi64>, use_global_device_ids = false} : (memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>) -> !mhlo.token

         [[{{node StatefulPartitionedCall}}]]
         [[Reshape_3/_24]]
  (1) UNKNOWN:  <unknown>:0: error: loc(""all-reduce-start.1""): 'lmhlo_gpu.all_reduce_start' op requires the same element type for all operands
<unknown>:0: note: loc(""all-reduce-start.1""): see current operation: 
%225 = ""lmhlo_gpu.all_reduce_start""(%222, %8, %224, %18, %220, %222, %8, %224, %18, %220) ({
^bb0(%arg58: tensor<f32>, %arg59: tensor<f32>):
  %251 = ""mhlo.add""(%arg58, %arg59) : (tensor<f32>, tensor<f32>) -> tensor<f32>
  ""mhlo.return""(%251) : (tensor<f32>) -> ()
}) {channel_id = #mhlo.channel_handle<handle = 4294967300, type = 0>, constrain_layout = false, is_sync = true, replica_groups = dense<> : tensor<0x0xi64>, use_global_device_ids = false} : (memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<1xf32>, memref<5630xf16>) -> !mhlo.token

         [[{{node StatefulPartitionedCall}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_2528]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:dist-strat', 'TF2.14']",2024-01-15T14:56:06Z,6,0,https://github.com/tensorflow/tensorflow/issues/62796,
256,tensorflow/tensorflow,tf.data.Dataset.map() makes unnecessary memory allocations,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.15.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.10.13

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When `tf.data.Dataset.map` is called with a function which references tensor(s) (or a nested object with tensors), it seems to be making a copy of these tensors. If these tensors are large, this causes large memory allocations which can cause the process to OOM.

Please see the below code snippet which reproduces the issue ([here](https://colab.research.google.com/drive/1UykDpf0FefrcNyCgPW9-KW-7zCQFbzTg#scrollTo=OtY6ihLwfvNJ) is a reference to the colab). I am allocating a tensor which takes 2GB of memory and then referencing it in the `get_data` function. This function is used in `tf.data.Dataset.map` to construct the dataset. I am chaining multiple `map` calls to exacerbate the bug to cause OOM in colab. Each `map` call allocates a new copy of the original tensor referenced by the passed function.

Please note that this is not a memory leak as these copies are subsequently freed and the memory is released back to the mem allocator. However, depending on the allocator and it's settings, the allocator may hold on to the memory for a long time and not release back to the OS, causing a memory bloat for the process in the best case, and an OOM in the worse case.

It is expected that these tensor copies do not happen as there is no functional need.

It is possible that the root cause of this issue is the same as #61344 , in which case feel free to close this issue and track the underlying bug over there.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

print(tf.version.VERSION)

# Depending on the underlying RAM resources,
# increase this value to see OOM. With 10,
# this code should OOM with total RAM resources of 20GB or less.
NUM_MAP_CALLS=10

# Allocate a large tensor. This will take 2GB of RAM.
t = tf.random.uniform((2048, 1024*256))
    
def get_data(_idx):
  return t[0, 0]
    
ds = tf.data.Dataset.range(1)
for _ in range(NUM_MAP_CALLS):
  ds = ds.map(get_data)

next(iter(ds))
```


### Relevant log output

```shell
Jan 13, 2024, 12:04:55 PM	WARNING	WARNING:root:kernel 0585280c-54b4-4ab5-8a4d-e5502242c92a restarted
Jan 13, 2024, 12:04:55 PM	INFO	KernelRestarter: restarting kernel (1/5), keep random ports
Jan 13, 2024, 12:04:49 PM	WARNING	2024-01-13 17:04:49.124663: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2147483648 exceeds 10% of free system memory.
Jan 13, 2024, 12:04:44 PM	WARNING	2024-01-13 17:04:44.817746: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2147483648 exceeds 10% of free system memory.
Jan 13, 2024, 12:04:39 PM	WARNING	2024-01-13 17:04:39.468439: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2147483648 exceeds 10% of free system memory.
Jan 13, 2024, 12:04:32 PM	WARNING	2024-01-13 17:04:32.994518: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2147483648 exceeds 10% of free system memory.
Jan 13, 2024, 12:04:28 PM	WARNING	2024-01-13 17:04:28.102305: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2147483648 exceeds 10% of free system memory.
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'type:performance', 'TF 2.15']",2024-01-13T17:07:04Z,5,0,https://github.com/tensorflow/tensorflow/issues/62788,
257,tensorflow/tensorflow,Potential memory leak with SymbolicTensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

Manjaro Linux

### Mobile device

_No response_

### Python version

3.11.5

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.3.52

### GPU model and memory

_No response_

### Current behavior?

Hello,

I am working on a TinyML NAS framework, i.e. throughout the execution of my code, hundreds if not thousands of models are created and trained. I have come across a problem that has been starving my system of memory after a couple of days of execution. By using `tracemalloc` I have been able to see that the main contributor appears to be the symbolic tensor created when creating a Conv2D layer. Maybe I am missing something basic in terms of garbage collection in my code but over time the demo code will eventually consume all system memory.

I have also tried `tf.keras.backend.clear_session()` and `gc.collect()` but neither help. 

Any help would be appreciated.

Cheers

### Standalone code to reproduce the issue

```shell
import gc
import tracemalloc, sys, linecache, os
import numpy as np
import tensorflow as tf
from tensorflow import keras

EPOCHS = 5
BS = 512
TEST_LOOPS = 10000


def start_tracemalloc():
    tracemalloc.start()


def display_top(snapshot, key_type=""lineno"", limit=5):
    snapshot = snapshot.filter_traces(
        (
            tracemalloc.Filter(False, ""<frozen importlib._bootstrap>""),
            tracemalloc.Filter(False, ""<unknown>""),
        )
    )
    top_stats = snapshot.statistics(key_type)

    print(""Top %s lines"" % limit)
    for index, stat in enumerate(top_stats[:limit], 1):
        frame = stat.traceback[0]
        # replace ""/path/to/module/file.py"" with ""module/file.py""
        filename = os.sep.join(frame.filename.split(os.sep)[-2:])
        print(
            ""#%s: %s:%s: %.1f KiB"" % (index, filename, frame.lineno, stat.size / 1024)
        )
        line = linecache.getline(frame.filename, frame.lineno).strip()
        if line:
            print(""    %s"" % line)

    other = top_stats[limit:]
    if other:
        size = sum(stat.size for stat in other)
        print(""%s other: %.1f KiB"" % (len(other), size / 1024))
    total = sum(stat.size for stat in top_stats)
    print(""Total allocated size: %.1f KiB"" % (total / 1024))


def display_snapshot():
    snapshot = tracemalloc.take_snapshot()
    display_top(snapshot)


def create_model() -> keras.models.Model:
    inputs = keras.layers.Input(shape=(28, 28, 1))
    x = keras.layers.Conv2D(32, kernel_size=(3, 3), padding=""valid"")(inputs)
    x = keras.layers.MaxPooling2D(pool_size=(2, 2), strides=None)(x)
    x = keras.layers.Flatten()(x)
    x = keras.layers.Dense(128, activation=tf.nn.relu)(x)
    x = keras.layers.Dropout(0.2)(x)
    outputs = keras.layers.Dense(10, activation=tf.nn.softmax)(x)

    model = keras.models.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    return model


def train_model(model, train_images, train_labels):
    model.fit(train_images, train_labels, epochs=EPOCHS, batch_size=BS)


def main() -> int:
    mnist = keras.datasets.mnist
    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()

    train_images = train_images.reshape(train_images.shape[0], train_images.shape[1], train_images.shape[2], 1)
    train_images = train_images.astype(np.float32) / 255.0

    start_tracemalloc()

    for i in range(TEST_LOOPS):
        tf.keras.backend.clear_session()
        gc.collect()

        model = create_model()
        train_model(model, train_images, train_labels)
        display_snapshot()
        del model


if __name__ == '__main__':
    sys.exit(main())
```


### Relevant log output

```shell
Epoch 1/5
...
Top 5 lines
#1: <frozen abc>:123: 1856.2 KiB
#2: python3.11/linecache.py:137: 606.4 KiB
    lines = fp.readlines()
#3: framework/ops.py:245: 197.9 KiB
    return pywrap_tf_session.PyTensor.__new__(
#4: <frozen importlib._bootstrap_external>:729: 158.6 KiB
#5: framework/ops.py:1211: 137.3 KiB
    self._gradient_function = None
952 other: 1407.6 KiB
Total allocated size: 4364.1 KiB
Epoch 1/5
...
Top 5 lines
#1: <frozen abc>:123: 1846.9 KiB
#2: python3.11/linecache.py:137: 620.8 KiB
    lines = fp.readlines()
#3: framework/ops.py:245: 382.3 KiB
    return pywrap_tf_session.PyTensor.__new__(
#4: framework/ops.py:1211: 264.9 KiB
    self._gradient_function = None
#5: framework/ops.py:1161: 254.7 KiB
    self = Operation(c_op, SymbolicTensor)
947 other: 2160.5 KiB
Total allocated size: 5530.0 KiB
Epoch 1/5 
...
Top 5 lines
#1: <frozen abc>:123: 1843.9 KiB
#2: python3.11/linecache.py:137: 620.8 KiB
    lines = fp.readlines()
#3: framework/ops.py:245: 568.5 KiB
    return pywrap_tf_session.PyTensor.__new__(
#4: framework/ops.py:1211: 394.1 KiB
    self._gradient_function = None
#5: framework/ops.py:1161: 378.9 KiB
    self = Operation(c_op, SymbolicTensor)
949 other: 2702.3 KiB
Total allocated size: 6508.5 KiB
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.15']",2024-01-11T12:16:19Z,4,0,https://github.com/tensorflow/tensorflow/issues/62783,
258,tensorflow/tensorflow,Tensorflow numpy_function causes errors when using tf.shape,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.1

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04.6 LTS

### Mobile device

_No response_

### Python version

3.9.18

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Raises an error. 

Expect: No errors.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf  # v2.13.0-17-gf841394b1b7 2.13.1
import tensorflow_datasets as tfds  # 4.9.3
from functools import partial

AUTO = tf.data.experimental.AUTOTUNE

dataset_name = ""mnist""
dataset = tfds.load(
    dataset_name,
    split=""train[:1%]""
)

dataset_len = dataset.reduce(0, lambda x, _: x + 1).numpy()
print(""dataset_len"", dataset_len)

image_size = [28, 28, 1]
new_image_size = [14, 2, 28, 1]


def prepare_input(
    image, labels,
    image_size
):

    image = tf.image.resize(image, image_size)

    split_shape = tf.shape(image)[1] // 2
    image_reshape = tf.reshape(
        image, [split_shape, 2, image_size[1], 1]
    )

    return image_reshape, labels


partial_prepare_input = partial(
    prepare_input,
    image_size=tf.constant(image_size[:2], dtype=tf.int32)
)


def prepare_input_wrapper(sample):
    image = sample[""image""]
    labels = sample[""label""]

    labels_shape = labels.shape

    image, label = tf.numpy_function(
        func=partial_prepare_input,
        inp=[image, labels],
        Tout=[tf.float32, tf.int64],
        name=""numpy_function_1""
    )

    image.set_shape(new_image_size)
    label.set_shape(labels_shape)

    return {
        ""image"": image,
        ""label"": label
    }


dataset = dataset.map(
    lambda sample: prepare_input_wrapper(sample),
    num_parallel_calls=AUTO
)


for x in dataset:
    print(x['image'].shape, x['label'].shape)
```


### Relevant log output

```shell
2024-01-11 02:21:59.538118: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-01-11 02:21:59.575984: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-11 02:22:00.346338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-01-11 02:22:02.022172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-11 02:22:02.058030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-11 02:22:02.061266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-11 02:22:02.064864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-11 02:22:02.067959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-11 02:22:02.070999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-11 02:22:02.820643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-11 02:22:02.821876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-11 02:22:02.822898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2024-01-11 02:22:02.823877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13623 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5
dataset_len 600
2024-01-11 02:22:03.323582: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: slice index 1 of dimension 0 out of bounds.
2024-01-11 02:22:03.323637: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead.
2024-01-11 02:22:03.324399: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: slice index 1 of dimension 0 out of bounds.
2024-01-11 02:22:03.324531: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead.
2024-01-11 02:22:03.329642: W tensorflow/core/framework/op_kernel.cc:1816] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/
Traceback (most recent call last):

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 268, in __call__
    ret = func(*args)

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 643, in wrapper
    return func(*args, **kwargs)

  File ""/home/ubuntu/automltraining/replicate_error.py"", line 27, in prepare_input
    split_shape = tf.shape(image)[1] // 2

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 6656, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access

tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/


2024-01-11 02:22:03.330620: W tensorflow/core/framework/op_kernel.cc:1816] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice/
Traceback (most recent call last):

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 268, in __call__
    ret = func(*args)

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 643, in wrapper
    return func(*args, **kwargs)

  File ""/home/ubuntu/automltraining/replicate_error.py"", line 27, in prepare_input
    split_shape = tf.shape(image)[1] // 2

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 6656, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access

tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice/


2024-01-11 02:22:03.333474: W tensorflow/core/framework/op_kernel.cc:1816] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice/
Traceback (most recent call last):

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 268, in __call__
    ret = func(*args)

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 643, in wrapper
    return func(*args, **kwargs)

  File ""/home/ubuntu/automltraining/replicate_error.py"", line 27, in prepare_input
    split_shape = tf.shape(image)[1] // 2

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 6656, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access

tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice/


2024-01-11 02:22:03.337583: W tensorflow/core/framework/op_kernel.cc:1816] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/
Traceback (most recent call last):

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 268, in __call__
    ret = func(*args)

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 643, in wrapper
    return func(*args, **kwargs)

  File ""/home/ubuntu/automltraining/replicate_error.py"", line 27, in prepare_input
    split_shape = tf.shape(image)[1] // 2

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 6656, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access

tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/


2024-01-11 02:22:03.337628: W tensorflow/core/framework/op_kernel.cc:1816] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice/
Traceback (most recent call last):

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 268, in __call__
    ret = func(*args)

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 643, in wrapper
    return func(*args, **kwargs)

  File ""/home/ubuntu/automltraining/replicate_error.py"", line 27, in prepare_input
    split_shape = tf.shape(image)[1] // 2

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 6656, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access

tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} Expected begin, end, and strides to be 1D equal size tensors, but got shapes [3], [1], and [1] instead. [Op:StridedSlice] name: strided_slice/


Traceback (most recent call last):
  File ""/home/ubuntu/automltraining/replicate_error.py"", line 69, in <module>
    for x in dataset:
  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 814, in __next__
    return self._next_internal()
  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 777, in _next_internal
    ret = gen_dataset_ops.iterator_get_next(
  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3028, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 6656, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.UnknownError: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/
Traceback (most recent call last):

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py"", line 268, in __call__
    ret = func(*args)

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 643, in wrapper
    return func(*args, **kwargs)

  File ""/home/ubuntu/automltraining/replicate_error.py"", line 27, in prepare_input
    split_shape = tf.shape(image)[1] // 2

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None

  File ""/home/ubuntu/miniconda3/envs/automl/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 6656, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access

tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/


         [[{{node numpy_function_1}}]] [Op:IteratorGetNext] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2024-01-11T02:23:23Z,11,0,https://github.com/tensorflow/tensorflow/issues/62778,
259,tensorflow/tensorflow,NCCL + XLA fails for multi-GPU training.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.15

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.2/8.9.4

### GPU model and memory

A100 40GB (20GB MiG)

### Current behavior?

I am trying to run multi-GPU training with an XLA compiled model (simple CNN with a classification head). Without XLA, everything runs fine. With XLA enabled, I get one of two errors in the log, depending on whether I am using 4 GPUs or 2 GPUs. The GPUs are split into 2 MiGs.

I also tried on previous TF/CUDA versions and I get the same result.

### Standalone code to reproduce the issue

```python
import argparse
import json
import os
from typing import Any

import numpy as np
import tensorflow as tf

def get_replica_hostnames():
    ...


def get_replica_id():
    ...


def set_multiworker_env_config():
    hostnames = get_replica_hostnames()
    replica_index = get_replica_id()

    os.environ[""TF_CONFIG""] = json.dumps(
        {
            ""cluster"": {
                ""worker"": hostnames,
            },
            ""task"": {""type"": ""worker"", ""index"": replica_index},
        }
    )


class Model(tf.keras.models.Model):
    def __init__(self, *args: Any, **kwargs: Any) -> None:
        super().__init__(*args, **kwargs)

        self._embedder = tf.keras.Sequential(
            [
                tf.keras.layers.Conv2D(
                    filters=8,
                    kernel_size=3,
                    padding=""same"",
                    activation=tf.keras.activations.relu,
                    use_bias=False,
                ),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.Conv2D(
                    filters=8,
                    kernel_size=3,
                    padding=""same"",
                    activation=tf.keras.activations.relu,
                    use_bias=False,
                ),
                tf.keras.layers.BatchNormalization(),
                tf.keras.layers.MaxPool2D(),
                tf.keras.layers.GlobalAveragePooling2D(),
            ]
        )

        self._classifier = tf.keras.layers.Dense(550)

    def call(self, x: tf.Tensor) -> tf.Tensor:
        x = self._embedder(x)
        x = self._classifier(x)
        x = tf.keras.layers.Activation(""linear"", dtype=""float32"")(x)
        return x


def create_dummy_dataset(batch_size: int) -> tf.data.Dataset:
    X = np.random.rand(batch_size, 384, 640, 1)
    y = np.random.randint(550, size=batch_size)
    return tf.data.Dataset.from_tensor_slices((X, y)).batch(batch_size, True).repeat()


def train():
    set_multiworker_env_config()
    strategy = tf.distribute.MultiWorkerMirroredStrategy()
    num_replicas = strategy.num_replicas_in_sync

    batch_size = 16 * num_replicas
    dataset = create_dummy_dataset(batch_size)
    dataset = strategy.experimental_distribute_dataset(dataset)

    with strategy.scope():
        model = Model()
        model.compile(
            loss=tf.keras.losses.SparseCategoricalCrossentropy(
                from_logits=True,
            ),
            optimizer=tf.keras.optimizers.Adam(
                learning_rate=1e-3,
                weight_decay=1e-5,
            ),
            metrics=[
                ""accuracy"",
            ],
            jit_compile=True,
        )

    model.fit(
        dataset,
        epochs=10,
        steps_per_epoch=100,
    )


if __name__ == ""__main__"":
    train()
```


### Relevant log output

```shell
Log output for 4 GPUs:

2024-01-08 12:48:24.103746: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-08 12:48:24.103807: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-08 12:48:24.104896: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-08 12:48:24.111875: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-08 12:48:24.978525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-01-08 12:48:27.492019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18370 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:01:00.0, compute capability: 8.0
2024-01-08 12:48:27.503592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:worker/replica:0/task:0/device:GPU:0 with 18370 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:01:00.0, compute capability: 8.0
2024-01-08 12:48:27.528772: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://gen-svc-4e03d749-0565-47d7-b7ff-63437f0ab5b3:80
2024-01-08 12:48:27.535501: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service.cc:553] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 9493928235207696637
2024-01-08 12:48:27.535846: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service_agent.cc:304] Coordination agent has successfully connected.
2024-01-08 12:48:28.425710: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service.cc:553] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 17537869834892823189
2024-01-08 12:48:29.509519: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service.cc:553] /job:worker/replica:0/task:2 has connected to coordination service. Incarnation: 11924625857490357420
2024-01-08 12:48:29.766944: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service.cc:553] /job:worker/replica:0/task:3 has connected to coordination service. Incarnation: 8010175117178506894
WARNING:absl:You use TensorFlow DType <dtype: 'string'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to object.
WARNING:absl:You use TensorFlow DType <dtype: 'int64'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int64.
ml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:563 [0] NCCL INFO Bootstrap : Using eth0:10.233.118.112<0>
ml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:563 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
2024-01-08 12:48:32.756935: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-01-08 12:48:32.756993: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-01-08 12:48:32.757173: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1883] Profiler found 1 GPUs
2024-01-08 12:48:32.790801: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-01-08 12:48:32.790934: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:2017] CUPTI activity buffer flushed
Epoch 1/10
2024-01-08 12:48:37.520453: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fe284006a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-01-08 12:48:37.520583: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB MIG 3g.20gb, Compute Capability 8.0
2024-01-08 12:48:37.680983: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-01-08 12:48:38.693930: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1704718137.764316     430 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
ml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.16.5+cudaCUDA_MAJOR.CUDA_MINOR
ml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO Failed to open libibverbs.so[.1]
ml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO NET/Socket : Using [0]eth0:10.233.118.112<0>
ml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO Using network Socket

ml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] external/nccl_archive/src/init.cc:642 NCCL WARN Duplicate GPU detected : rank 0 and rank 2 both on CUDA device 1000
ml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO external/nccl_archive/src/init.cc:1100 -> 5
ml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO external/nccl_archive/src/init.cc:1173 -> 5
ml-wf-receipt-ext-logo-classifier-pipelinenxxxs-train-template:37:1288 [0] NCCL INFO external/nccl_archive/src/init.cc:1209 -> 5
2024-01-08 12:48:58.577448: W external/local_xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:
INTERNAL: external/local_xla/xla/service/gpu/nccl_utils.cc:297: NCCL operation ncclCommInitRank(&comm, nranks, id, rank) failed: invalid usage
2024-01-08 12:48:58.577775: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16898057275935290807
2024-01-08 12:48:58.577798: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7386102362502530449
2024-01-08 12:48:58.577844: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11220456033729140565
Traceback (most recent call last):
  File ""/kirax_source/train.py"", line 346, in <module>
    train_tf(args=args, jit_compile=XLA)
  File ""/kirax_source/train.py"", line 246, in train_tf
    model.fit(
  File ""/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.11/threading.py"", line 1002, in _bootstrap

  File ""/usr/lib/python3.11/threading.py"", line 1045, in _bootstrap_inner

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.11/threading.py"", line 1002, in _bootstrap

  File ""/usr/lib/python3.11/threading.py"", line 1045, in _bootstrap_inner

2 root error(s) found.
  (0) INTERNAL:  Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.all_reduce' failed: external/local_xla/xla/service/gpu/nccl_utils.cc:297: NCCL operation ncclCommInitRank(&comm, nranks, id, rank) failed: invalid usage; current tracing scope: all-reduce-start.4; current profiling annotation: XlaModule:#hlo_module=a_inference_run_step_7562__.4006,program_id=447#.
         [[{{node StatefulPartitionedCall}}]]
         [[Reshape_3/_22]]
  (1) INTERNAL:  Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.all_reduce' failed: external/local_xla/xla/service/gpu/nccl_utils.cc:297: NCCL operation ncclCommInitRank(&comm, nranks, id, rank) failed: invalid usage; current tracing scope: all-reduce-start.4; current profiling annotation: XlaModule:#hlo_module=a_inference_run_step_7562__.4006,program_id=447#.
         [[{{node StatefulPartitionedCall}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_7900]

Log output for 2 GPUs:

2024-01-08 13:32:56.555585: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-01-08 13:32:56.555666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-01-08 13:32:56.557055: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-01-08 13:32:56.563733: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-01-08 13:32:57.475551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-01-08 13:32:59.868467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18370 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:41:00.0, compute capability: 8.0
2024-01-08 13:32:59.880625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:worker/replica:0/task:0/device:GPU:0 with 18370 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:41:00.0, compute capability: 8.0
2024-01-08 13:32:59.903354: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://gen-svc-d871b43e-6f9e-4d8f-9faf-d98a734319f3:80
2024-01-08 13:32:59.912608: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service.cc:553] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 12924650074147221766
2024-01-08 13:32:59.913045: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service_agent.cc:304] Coordination agent has successfully connected.
2024-01-08 13:33:00.820050: I external/local_tsl/tsl/distributed_runtime/coordination/coordination_service.cc:553] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 4023732460757352804
WARNING:absl:You use TensorFlow DType <dtype: 'string'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to object.
WARNING:absl:You use TensorFlow DType <dtype: 'int64'> in tfds.features This will soon be deprecated in favor of NumPy DTypes. In the meantime it was converted to int64.
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:784 [0] NCCL INFO Bootstrap : Using eth0:10.233.118.63<0>
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:784 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
2024-01-08 13:33:03.552619: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2024-01-08 13:33:03.552667: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2024-01-08 13:33:03.552814: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1883] Profiler found 1 GPUs
2024-01-08 13:33:03.587433: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2024-01-08 13:33:03.587611: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:2017] CUPTI activity buffer flushed
Epoch 1/10
2024-01-08 13:33:08.149597: I external/local_xla/xla/service/service.cc:168] XLA service 0x7ff338008540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-01-08 13:33:08.149730: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB MIG 3g.20gb, Compute Capability 8.0
2024-01-08 13:33:08.732338: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-01-08 13:33:09.871101: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1704720809.106117     425 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.16.5+cudaCUDA_MAJOR.CUDA_MINOR
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Failed to open libibverbs.so[.1]
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO NET/Socket : Using [0]eth0:10.233.118.63<0>
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Using network Socket

ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] external/nccl_archive/src/misc/nvmlwrap.cc:183 NCCL WARN nvmlDeviceGetHandleByPciBusId() failed: Not Found
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Setting affinity for GPU 0 to ffffffff,00000000,ffffffff
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 00/02 :    0   1
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 01/02 :    0   1
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO P2P Chunksize set to 131072
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1284 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1284 [0] NCCL INFO NET/Socket: Using 8 threads and 1 sockets per thread
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 00/0 : 1[1000] -> 0[41000] [receive] via NET/Socket/0
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1284 [0] NCCL INFO NET/Socket: Using 8 threads and 1 sockets per thread
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 01/0 : 1[1000] -> 0[41000] [receive] via NET/Socket/0
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 00/0 : 0[41000] -> 1[1000] [send] via NET/Socket/0
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Channel 01/0 : 0[41000] -> 1[1000] [send] via NET/Socket/0
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Connected all rings
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO Connected all trees
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 512 | 512
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
ml-wf-receipt-ext-logo-classifier-pipelinegl897-train-template:31:1280 [0] NCCL INFO comm 0x7fe61cffcb20 rank 0 nranks 2 cudaDev 0 busId 41000 commId 0xe105507e5746b5a2 - Init COMPLETE
2024-01-08 13:33:29.780117: W external/local_xla/xla/service/gpu/runtime/support.cc:58] Intercepted XLA runtime error:
INTERNAL: There was an error before calling cuModuleGetFunction (101): cudaErrorInvalidDevice : invalid device ordinal
2024-01-08 13:33:29.780331: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14229809023376429067
2024-01-08 13:33:29.780349: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10883816187180422187
2024-01-08 13:33:29.780389: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5243788738829094043
Traceback (most recent call last):
  File ""/kirax_source/train.py"", line 346, in <module>
    train_tf(args=args, jit_compile=XLA)
  File ""/kirax_source/train.py"", line 246, in train_tf
    model.fit(
  File ""/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.11/threading.py"", line 1002, in _bootstrap

  File ""/usr/lib/python3.11/threading.py"", line 1045, in _bootstrap_inner

Detected at node StatefulPartitionedCall defined at (most recent call last):
  File ""/usr/lib/python3.11/threading.py"", line 1002, in _bootstrap

  File ""/usr/lib/python3.11/threading.py"", line 1045, in _bootstrap_inner

2 root error(s) found.
  (0) INTERNAL:  Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.func.launch' failed: There was an error before calling cuModuleGetFunction (101): cudaErrorInvalidDevice : invalid device ordinal; current tracing scope: fusion.274; current profiling annotation: XlaModule:#hlo_module=a_inference_run_step_7562__.4006,program_id=447#.
         [[{{node StatefulPartitionedCall}}]]
         [[CollectiveReduceV2_1/_17]]
  (1) INTERNAL:  Failed to execute XLA Runtime executable: run time error: custom call 'xla.gpu.func.launch' failed: There was an error before calling cuModuleGetFunction (101): cudaErrorInvalidDevice : invalid device ordinal; current tracing scope: fusion.274; current profiling annotation: XlaModule:#hlo_module=a_inference_run_step_7562__.4006,program_id=447#.
         [[{{node StatefulPartitionedCall}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_7900]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.15']",2024-01-08T13:54:59Z,8,2,https://github.com/tensorflow/tensorflow/issues/62757,
260,tensorflow/tensorflow,Can no longer run XLA lit tests,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

TF 2.15

### Custom code

No

### Current behavior?

Previously, I could run the XLA unit tests via `bazel test //tensorflow/compiler/xla/...:all`.
However, in TF 2.15 after xla was moved to third_party/xla I am encountering issues.
I updated my command to `bazel test @local_xla//xla/...:all`. While most tests run successfully, it seems there are some hardcoded paths which are preventing the llvm lit tests from running correctly. See the logs below. Probably the lit configs need to be updated?

### Standalone code to reproduce the issue

```shell
Checkout tensorflow. Configure. Run `bazel test @local_xla//xla/...:all`
```


### Relevant log output

```shell
================================================================================
FAIL: @local_xla//xla/mlir/backends/gpu/transforms/tests:gpu_memcpy.mlir.test (see /root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/external/local_xla/xla/mlir/backends/gpu/transforms/tests/gpu_memcpy.mlir.test/test.log)
[27,548 / 27,604] 348 / 447 tests, 216 failed; [Sched] Testing @local_xla//xla/mlir_hlo/tests:Dialect/mhlo/hlo-collapse-elementwise-map.mlir.test; 45s ... (55 actions, 2 running)
INFO: From Testing @local_xla//xla/mlir/backends/gpu/transforms/tests:gpu_memcpy.mlir.test:
==================== Test output for @local_xla//xla/mlir/backends/gpu/transforms/tests:gpu_memcpy.mlir.test:
Running test /root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/local_xla/xla/mlir/backends/gpu/transforms/tests/gpu_memcpy.mlir.test.runfiles/org_tensorflow/../local_xla/xla/mlir/backends/gpu/transforms/tests/gpu_memcpy.mlir.test xla/gpu_memcpy.mlir --config-prefix=runlit -v on GPU 0
lit.py: /root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/external/llvm-raw/llvm/utils/lit/lit/discovery.py:137: warning: unable to find test suite for 'xla/gpu_memcpy.mlir'
lit.py: /root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/external/llvm-raw/llvm/utils/lit/lit/discovery.py:276: warning: input 'xla/gpu_memcpy.mlir' contained no tests
error: did not discover any tests for provided path(s)
================================================================================
FAIL: @local_xla//xla/mlir_hlo/tests:Dialect/mhlo/hlo-collapse-elementwise-map.mlir.test (see /root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/external/local_xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-collapse-elementwise-map.mlir.test/test.log)
INFO: From Testing @local_xla//xla/mlir_hlo/tests:Dialect/mhlo/hlo-collapse-elementwise-map.mlir.test:
==================== Test output for @local_xla//xla/mlir_hlo/tests:Dialect/mhlo/hlo-collapse-elementwise-map.mlir.test:
Running test /root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/local_xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-collapse-elementwise-map.mlir.test.runfiles/org_tensorflow/../local_xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-collapse-elementwise-map.mlir.test -v external/local_xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-collapse-elementwise-map.mlir on GPU 0
lit.py: /root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/external/llvm-raw/llvm/utils/lit/lit/TestingConfig.py:151: fatal: unable to parse config file '/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/local_xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-collapse-elementwise-map.mlir.test.runfiles/org_tensorflow/external/local_xla/xla/mlir_hlo/tests/lit.site.cfg.py', traceback: Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/external/llvm-raw/llvm/utils/lit/lit/TestingConfig.py"", line 139, in load_from_path
    exec(compile(data, path, ""exec""), cfg_globals, None)
  File ""/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/execroot/org_tensorflow/bazel-out/k8-opt/bin/external/local_xla/xla/mlir_hlo/tests/Dialect/mhlo/hlo-collapse-elementwise-map.mlir.test.runfiles/org_tensorflow/external/local_xla/xla/mlir_hlo/tests/lit.site.cfg.py"", line 44, in <module>
    lit_config.load_config(config, ""xla/mlir_hlo/tests/lit.cfg.py"")
  File ""/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/external/llvm-raw/llvm/utils/lit/lit/LitConfig.py"", line 152, in load_config
    config.load_from_path(path, self)
  File ""/root/.cache/bazel/_bazel_root/a8fc6d0749b4f3c43761726a36e8ec4c/external/llvm-raw/llvm/utils/lit/lit/TestingConfig.py"", line 126, in load_from_path
    f = open(path)
FileNotFoundError: [Errno 2] No such file or directory: 'xla/mlir_hlo/tests/lit.cfg.py'
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install']",2024-01-03T22:23:21Z,5,0,https://github.com/tensorflow/tensorflow/issues/62732,
261,tensorflow/tensorflow,Textfile initializer sharing bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.15

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The logic here for TextFileInitializer [sharing](https://github.com/tensorflow/tensorflow/blob/059b23d0dfd34e0d6cdf1f4a65dbc8ed1dfdf54a/tensorflow/python/ops/lookup_ops.py#L790) does not consider key/value dtype. If two text file initializers are made for same vocab file but with different dtypes (tf.int64/tf.string), then sharing will cause one of them to fail and crash.

The fix should be small change to add dtypes to shared_name.

### Standalone code to reproduce the issue

```shell
import os
import tempfile

import tensorflow as tf
from tensorflow.lookup import TextFileIndex

with tempfile.TemporaryDirectory(""w"") as tmp_dir:
    vocab_file = os.path.join(tmp_dir, ""vocab.txt"")
    with open(vocab_file, ""w"") as f:
        f.write(""\n"".join([""1"", ""2"", ""3""]))

    initializer1 = tf.lookup.TextFileInitializer(
        vocab_file, tf.string, TextFileIndex.WHOLE_LINE, tf.int64, TextFileIndex.LINE_NUMBER
    )
    table1 = tf.lookup.StaticHashTable(initializer1, -1)

    initializer2 = tf.lookup.TextFileInitializer(
        vocab_file, tf.int64, TextFileIndex.WHOLE_LINE, tf.int64, TextFileIndex.LINE_NUMBER
    )
    table2 = tf.lookup.StaticHashTable(initializer2, -1)

    _ = table1.lookup(tf.constant([""1"", ""2"", ""3""], dtype=tf.string))
    _ = table2.lookup(tf.constant([1, 2, 3], dtype=tf.int64))
```


### Relevant log output

```shell
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-12-13 13:50:41.113925: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at lookup_table_op.h:94 : INVALID_ARGUMENT: Conflicting key/value dtypes int64->int64 with string-int64 for table hash_table_/var/folders/3g/0v44vhyd05q6n_77rl_jnh6r0000gn/T/tmpiimi4gc8w/vocab.txt_-2_-1
Traceback (most recent call last):
  File ""/Users/pa-loaner/.pyenv/versions/3.9.16/lib/python3.9/pdb.py"", line 1726, in main
    pdb._runscript(mainpyfile)
  File ""/Users/pa-loaner/.pyenv/versions/3.9.16/lib/python3.9/pdb.py"", line 1586, in _runscript
    self.run(statement)
  File ""/Users/pa-loaner/.pyenv/versions/3.9.16/lib/python3.9/bdb.py"", line 580, in run
    exec(cmd, globals, locals)
  File ""<string>"", line 1, in <module>
  File ""/Users/pa-loaner/Snapchat/Dev/training-platform/scratch/text_file_bug.py"", line 1, in <module>
    import os
  File ""/Users/pa-loaner/Snapchat/Dev/.venvs/bento/lib/python3.9/site-packages/tensorflow/python/training/tracking/resource.py"", line 104, in __call__
    return previous_getter(*args, **kwargs)
  File ""/Users/pa-loaner/Snapchat/Dev/.venvs/bento/lib/python3.9/site-packages/tensorflow/python/training/tracking/resource.py"", line 99, in <lambda>
    previous_getter = lambda *a, **kw: default_resource_creator(None, *a, **kw)
  File ""/Users/pa-loaner/Snapchat/Dev/.venvs/bento/lib/python3.9/site-packages/tensorflow/python/training/tracking/resource.py"", line 96, in default_resource_creator
    obj.__init__(*a, **kw)
  File ""/Users/pa-loaner/Snapchat/Dev/.venvs/bento/lib/python3.9/site-packages/tensorflow/python/ops/lookup_ops.py"", line 347, in __init__
    super(StaticHashTable, self).__init__(default_value, initializer)
  File ""/Users/pa-loaner/Snapchat/Dev/.venvs/bento/lib/python3.9/site-packages/tensorflow/python/ops/lookup_ops.py"", line 198, in __init__
    self._resource_handle = self._create_resource()
  File ""/Users/pa-loaner/Snapchat/Dev/.venvs/bento/lib/python3.9/site-packages/tensorflow/python/ops/lookup_ops.py"", line 360, in _create_resource
    table_ref = gen_lookup_ops.hash_table_v2(
  File ""/Users/pa-loaner/Snapchat/Dev/.venvs/bento/lib/python3.9/site-packages/tensorflow/python/ops/gen_lookup_ops.py"", line 466, in hash_table_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""/Users/pa-loaner/Snapchat/Dev/.venvs/bento/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 7164, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Conflicting key/value dtypes int64->int64 with string-int64 for table hash_table_/var/folders/3g/0v44vhyd05q6n_77rl_jnh6r0000gn/T/tmpiimi4gc8w/vocab.txt_-2_-1 [Op:HashTableV2] name: hash_table
Uncaught exception. Entering post mortem debugging
Running 'cont' or 'step' will restart the program
> /Users/pa-loaner/Snapchat/Dev/.venvs/bento/lib/python3.9/site-packages/tensorflow/python/framework/ops.py(7164)raise_from_not_ok_status()
-> raise core._status_to_exception(e) from None  # pylint: disable=protected-access
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2023-12-13T19:51:16Z,4,0,https://github.com/tensorflow/tensorflow/issues/62631,
262,tensorflow/tensorflow,TFlite model signature lost after populating with metadata,"### 1. System information

- Mac OS 14.1.2
- TensorFlow mac 2.13.0
- Tflite support version '0.1.0a1'

### 2. Code

```
def write_metadata(model_path, run_name):
    model_meta = _metadata_fb.ModelMetadataT()
    model_meta.name = ""test model""
    model_meta.description = run_name
    model_meta.version = datetime.now().strftime(""%Y.%m.%d"")
    model_meta.author = ""Test""
    model_meta.license = f""test""\
        ""All rights reserved - test""

    input_meta_image = _metadata_fb.TensorMetadataT()
    input_meta_image.description = ""Input image for which to score doneness.""
    input_meta_image.content = _metadata_fb.ContentT()
    input_meta_image.content.contentProperties = _metadata_fb.ImagePropertiesT()
    input_meta_image.content.contentProperties.colorSpace = _metadata_fb.ColorSpaceType.RGB
    input_meta_image.content.contentPropertiesType = _metadata_fb.ContentProperties.ImageProperties

    input_meta_state = _metadata_fb.TensorMetadataT()
    input_meta_state.description = ""Input state.""
    input_meta_state.name = ""Input state.""

    # Creates output info.
    output_meta_doneness = _metadata_fb.TensorMetadataT()
    output_meta_doneness.description = ""Output doneness score between 0 and 1.""
    output_meta_doneness.name = ""Output Doneness""

    output_meta_state = _metadata_fb.TensorMetadataT()
    output_meta_state.description = ""Output state.""
    output_meta_state.name = ""Output state.""

    subgraph = _metadata_fb.SubGraphMetadataT()

    interpreter = tf.lite.Interpreter(model_path)
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    state_first_input = input_details[0]['shape'][-1] != 3
    state_first_output = output_details[0]['shape'][-1] != 1

    input_meta = [input_meta_state, input_meta_image]
    output_meta = [output_meta_state, output_meta_doneness]

    subgraph.inputTensorMetadata = input_meta if state_first_input else input_meta[::-1]
    subgraph.outputTensorMetadata = output_meta if state_first_output else output_meta[::-1]

    model_meta.subgraphMetadata = [subgraph]
    b = flatbuffers.Builder(0)
    b.Finish(
        model_meta.Pack(b),
        _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)
    metadata_buf = b.Output()

    populator = _metadata.MetadataPopulator.with_model_file(model_path)
    populator.load_metadata_buffer(metadata_buf)
    populator.populate()

```

I am using above function to populate the metadata. without running above function getting signature output like 

```
interpreter.get_signature_list()
{'serving_default': {'inputs': ['input_image', 'input_state'], 'outputs': ['output_1', 'output_2']}}
```

after running metadata function 
```
interpreter.get_signature_list()
{}
```


","['stat:awaiting tensorflower', 'type:bug', 'comp:lite', 'TFLiteConverter', 'TF 2.13']",2023-12-11T21:30:02Z,5,0,https://github.com/tensorflow/tensorflow/issues/62620,
263,tensorflow/tensorflow,Process is aborted (core dumped) when axis is a large negative integer when calling tf.gather ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.16.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am actually using 2.10.0 but I find this issue insists in 2.16.0-dev20231209 (tf-nightly). Here is the code to reproduce:

```
import tensorflow as tf
params = tf.constant([[0.69]])
indices = tf.constant([16])
axis = tf.constant(-9223372036854775808, dtype='int64')
tf.gather(params,indices,axis=axis)
```

The process will directly be killed by the system when running above code.

Here is the related output:

```
2023-12-09 22:48:16.035701: F ./tensorflow/core/framework/tensor.h:832] Check failed: new_num_elements == NumElements() (0 vs. 1)
Aborted (core dumped)
```

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
params = tf.constant([[0.69]])
indices = tf.constant([16])
axis = tf.constant(-9223372036854775808, dtype='int64')
tf.gather(params,indices,axis=axis)
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2023-12-09T14:54:43Z,5,0,https://github.com/tensorflow/tensorflow/issues/62603,
264,tensorflow/tensorflow,[BUG] race condition in local rendezvous,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### TensorFlow version

2.10, 2.12, doesn't really matter

### Custom code

Yes

### OS platform and distribution

ubuntu 20.04

### Python version

3.8, 3.9

### Bazel version

5.3.0

### GCC/compiler version

9.4

### Current behavior?

The story is kinda bit long and it took us months to debug this issue. I'll try to keep it short.

### Background and what the problem is
We run the [DLRM from NV DLE](https://github.com/NVIDIA/DeepLearningExamples/tree/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM) in our CI daily test. 

Very rare, like once every few weeks, the daily test report such error.

`
W tensorflow/core/framework/op_kernel.cc:1874] OP_REQUIRES failed at strided_slice_op.cc:112 : INVALID_ARGUMENT: Expected begin, end, and strides to be 1D equal size tensors, but got shapes [65536], [1], and [1] instead.
`

The[ strided slice op validator](https://github.com/tensorflow/tensorflow/blob/3a029b19c9c156cd68cab671b5ce95bde839f15e/tensorflow/core/util/strided_slice_op.cc#L214) isn't happy about the input parameters.

The problem is like a ghost. It appears once every 1 or 2 weeks which makes it very hard to debug. But one thing is for sure is that it's not a random dram bit flip caused by cosmic rays because the error systoms is stable: one of (begin, end, stride) tensor's shape is incorrectly set to [65535] instead of [1]

### Evidence

I'll skip the lengthy debug process and jump to the last step.

![image](https://github.com/tensorflow/tensorflow/assets/8800468/d73ddf6d-7cc9-4a10-90f8-1e4d3885c91b)
![image](https://github.com/tensorflow/tensorflow/assets/8800468/0b99c252-59b0-490d-8321-924adb70be72)


We observed a malfunctioned SEND/RECV pair.

The item tensor in SENDOP has a shape of [1] but during the local rendezvous process the recevied tensor got a shape of [65536]. This tensor later flows to strided slice op and triggers the grumpy validator.

![image](https://github.com/tensorflow/tensorflow/assets/8800468/7626ef2f-0de4-4eb0-8c25-3cfd9618b359)

We grep the key hash (red highlighted 1632...9087) in the debug log.  There are two threads: 66787 and 63997. Most of the time the rendezvous runs in a SEND-RECV pattern within the same thread. 

But in the green rectangle you can see two threads interleaved and a SEND-SEND-RECV-RECV pattern is observed. Soon after this the bomb exploded.

### Analysis

It seems the root cause is a bug in the rendezvous mechanism. Two unrelated operations generated the same communication Rendezvous key. 

Assuming there are 2 threads, when the CPU load is not heavy, in most cases the scheduling order is

- THREAD A: SEND(KEY, VALUE_A)

- THREAD A: RECV(KEY)

- THREAD B: SEND(KEY, VALUE_B)

- THREAD B: RECV(KEY)

In this way, everyone will be fine even if the keys are the same. It is equivalent to time-division multiplexing of the same KEY.

When the CPU load becomes heavy (which is the case in our daily test scenario), thread scheduling becomes unpredictable. It is possible that such a pattern may occur

- THREAD A: SEND(KEY, VALUE_A)

- THREAD B: SEND(KEY, VALUE_B)

- THREAD A: RECV(KEY)

- THREAD B: RECV(KEY)

This will cause THREAD A to incorrectly receive the data sent by THREAD B.


### Related python code

1. Thread A is running the normal [training loop.](https://github.com/NVIDIA/DeepLearningExamples/blob/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM/main.py#L281)
2. Thread B is created for the RawBinaryDataset class. The pre-processing creates an [asynchronous thread pool.](https://github.com/NVIDIA/DeepLearningExamples/blob/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM/split_binary_dataset.py#L138)
3. Thread A is running[ x = x[self.begin_idx:self.end_idx]](https://github.com/NVIDIA/DeepLearningExamples/blob/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM/model.py#L58). Translated into StridedSlice operation by eager runtime. begin_idx, end_idx and the implicit stride are all constant tensors on CPU, and StridedSlice is an device operator. So these three tensors need to be sent to the device side.
4. Thread B is running[ tensor = tf.expand_dims(tensor, axis=1). ](https://github.com/NVIDIA/DeepLearningExamples/blob/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM/split_binary_dataset.py#L204). Execute expand_dims. Similar to thread A, the expand_dims operator runs on the device side while the input is on CPU, and the tensor needs to be sent to the device. The shape of this tensor is [65536].
5. Due to the defect of Rendezous::CreateKey, the keys generated by these two operators in the eager runtime are exactly the same.
6. Multi-threading + HASH KEY collision + SEND-SEND-RECV-RECV, all three together, BOOOM.

### FIX

![image](https://github.com/tensorflow/tensorflow/assets/8800468/f50ed76a-9cef-4bf7-87b2-e4a298236dd9)

The key string is 
/job:localhost/replica:0/task:0/device:CPU:0;ea74dbce35f0ab7e;/job:localhost/replica:0/task:0/device:MLU:0;edge_2_input;0:0

The rendezvous key consists 5 parts.
- src_device: /job:localhost/replica:0/task:0/device:CPU:0;
- src_incarnation: ea74dbce35f0ab7e;
- dst_device: /job:localhost/replica:0/task:0/device:MLU:0;
- name: edge_2_input;
- frame_inter: 0:0

In this case,  4 out of 5 (src_device, src_incarnation, dst_device_frame_iter) are naturely indistinguishable.

Unless we add new field in the key, the only field we can play with is `name`.

The creation of `edge_2_input` involes another few tons of code.

![image](https://github.com/tensorflow/tensorflow/assets/8800468/b2d85cd7-1e47-492b-be50-1e3d63f13d55)

A quick fix for my current problem is simply add `dst_name` in the `tensor_name_attr` during graph partition. The names will become `edge_2_input_strided_slice` and `edge_2_input_expand_dims` thus my problem is solved.

```diff
diff --git a/tensorflow/core/graph/graph_partition.cc b/tensorflow/core/graph/graph_partition.cc
index a4f09383c63..57a4e919526 100644
--- a/tensorflow/core/graph/graph_partition.cc
+++ b/tensorflow/core/graph/graph_partition.cc
@@ -1147,7 +1147,8 @@ Status Partition(const PartitionOptions& opts, Graph* g,
         tensor_name_attr = opts.get_tensor_name_attr(edge);
       } else {
         tensor_name_attr =
-            strings::StrCat(""edge_"", edge->id(), ""_"", edge->src()->name());
+            strings::StrCat(""edge_"", edge->id(), ""_"", edge->src()->name(),
+            ""_"", edge->dst()->name());
       }
```

But a more approriate and generic fix might be to have a unique src node name. The source name will be like input_xxxxxx and input_yyyyyy. But this sounds like a fundamental change and I'm not sure if this would break too many things. And I'm not sure about the right place to make the change, like manipulating the input name a little bit [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/eager/execute.cc#L988).

I'd like to hear your opinion on this problem and I'd like to file a PR if you could point me the right place to apply the fix.

Cheers
Hengwen

### Standalone code to reproduce the issue

A minimum case derived from the [DLRM from NV DLE](https://github.com/NVIDIA/DeepLearningExamples/tree/3f82b7f982a1d9a9c5392bd110a5b93dfa7e80eb/Tensorflow2/Recommendation/DLRM) 

Click run all and you can reproduce the error in a few seconds.

https://colab.research.google.com/drive/1CUiyuG2Aob-Fj8xllrmbx2w3cNBXFYqU?usp=sharing




","['awaiting review', 'type:bug', 'comp:core', 'awaiting PR merge', 'TF 2.12']",2023-12-01T09:22:59Z,8,0,https://github.com/tensorflow/tensorflow/issues/62523,
265,tensorflow/tensorflow,MemoryError: std::bad_alloc when calling tf.squeeze with a floating tensor axis,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.14.0, tf-nightly

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When calling tf.squeeze with axis to be floating tensor, it will raise memory error. MemoryError looks dangerous to me, a proper input argument handling may be better.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
input = tf.constant([1,2,3], dtype='float32')
axis = tf.constant(1.0, dtype='float32')
out = tf.squeeze(input,axis)
```


### Relevant log output

```shell
/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_array_ops.py in squeeze(input, axis, name)
  10614   if tld.is_eager:
  10615     try:
> 10616       _result = pywrap_tfe.TFE_Py_FastPathExecute(
  10617         _ctx, ""Squeeze"", name, input, ""squeeze_dims"", axis)
  10618       return _result

MemoryError: std::bad_alloc
```
","['awaiting review', 'stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF2.14']",2023-11-29T19:27:17Z,6,0,https://github.com/tensorflow/tensorflow/issues/62504,
266,tensorflow/tensorflow,Model checkpoint not saved to google cloud storage,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14.1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.9.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I'm training a model and I want to save the model checkpoints in `.keras` format to google cloud storage. I'm using the `ModelCheckpoint` callback but nothing is saved. I'm also using the `Tensorboard` callback and the logs are saved correctly in the same bucket.

If I set the file path to a local directory the model checkpoint is saved without any problem.

### Standalone code to reproduce the issue

https://gist.github.com/rcalonso/f12863b6e2c4669be6875deee2ff6dbf


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF2.14']",2023-11-28T16:43:20Z,5,1,https://github.com/tensorflow/tensorflow/issues/62495,
267,tensorflow/tensorflow,Convolution: CPU memory increase with growing number of different sequence lengths,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.14.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA V11.8.89, cuDNN version 8600

### GPU model and memory

NVIDIA GeForce GTX 1080 Ti

### Current behavior?

I noticed a linear increase of CPU memory usage in my setups when using a convolution on raw waveforms (i.e., sequences which are long in time and 1D in feature). I could isolate the issue and it seems to be related to the number of different sequence lengths that occur.  I.e., if the sequence length is fixed to 100k, the memory consumption is constant. If it is randomly sampled from a given range, the memory consumption asymptotically grows towards a larger value as the range gets larger. This can be observed in the plot below. Also note that the memory consumption is not influenced by the absolute sequence length, just by the size of the range.

![image](https://github.com/tensorflow/tensorflow/assets/45091115/cbaeaadf-6d95-434f-bf2f-1a4aec232efc)

I measured the memory consumption using `watch_memory()` from [here](https://github.com/rwth-i6/returnn/blob/c230d1408a2d7620a9d00a5171c998d3876e69dc/returnn/util/watch_memory.py#L13). The different runs in the plot correspond to different `n_time_min` and `n_time_max` in the stand-alone code.

I reproduced the issue with an apptainer image built on top of the [tensorflow 2.14 image from dockerhub](https://hub.docker.com/layers/tensorflow/tensorflow/2.14.0-gpu-jupyter/images/sha256-981372796921ef7bb75f4fe5fbe98c335824d08233bed57586633199028d5e18?context=explore). The image definition file looks as follows:

<details>

```
Bootstrap: docker
From: tensorflow/tensorflow:2.14.0-gpu 
Stage: build

%post
    apt update -y

    # all the fundamental basics, zsh is need because calling the cache manager might launch the user shell
    DEBIAN_FRONTEND=noninteractive apt install -y wget git unzip gzip libssl-dev lsb-release zsh \
        bison libxml2-dev libopenblas-dev libsndfile1-dev libcrypto++-dev libcppunit-dev \
        parallel xmlstarlet python3-lxml htop strace gdb sox python3-pip cmake ffmpeg vim

    cd /usr/local
    git clone https://github.com/rwth-i6/cache-manager.git
    cd bin
    ln -s ../cache-manager/cf cf

    echo /usr/local/lib/python3.11/dist-packages/tensorflow > /etc/ld.so.conf.d/tensorflow.conf
    ldconfig

    apt install -y python3 python3-pip
    pip3 install -U pip setuptools wheel
    pip3 install ipdb
    pip3 install h5py six soundfile librosa==0.10 better-exchook dm-tree psutil
    pip3 install --ignore-installed psutil flask ipython
    pip3 install git+https://github.com/rwth-i6/sisyphus
    pip3 install black==22.3.0 matplotlib typing-extensions typeguard  # sequitur-g2p==1.0.1668.23
    pip3 install memray objgraph Pympler

```

</details>

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

n_feat = 1
n_out = 30
filter_size = 5
n_steps = 100000
n_time_min = 10000
n_time_max = 30000
batch_size_max = 400000

filters = tf.Variable(tf.random.normal((filter_size, n_feat, n_out), stddev=0.01))
for step in range(n_steps):
    n_time = np.random.randint(n_time_min, n_time_max)
    n_batch = batch_size_max // n_time
    x = tf.random.normal((n_batch, n_time, n_feat))
    y = tf.nn.convolution(
        x,
        filters=filters,
        padding=""VALID"",
    )
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'type:performance', 'TF2.14']",2023-11-20T15:51:19Z,3,0,https://github.com/tensorflow/tensorflow/issues/62441,
268,tensorflow/tensorflow,"Tensorflow 2.15 Docker image cannot find the GPU drivers, but nvidia-smi can.","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

TF 2.15.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 12.2

### GPU model and memory

NVIDIA TITAN V

### Current behavior?

Running TensorFlow 2.15.0 from within Docker image does not find GPU drivers.

nvidia-smi reports the GPUs as available.

This works as intended with a TF 2.14 image in the same machine.


### Standalone code to reproduce the issue

```shell
docker run --gpus all -it tensorflow/tensorflow:2.15.0-gpu-jupyter bash

<within the container>

# nvidia-smi

# python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""
```


### Relevant log output

```shell
# nvidia-smi

Thu Nov 16 16:47:22 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA TITAN V                 Off | 00000000:65:00.0 Off |                  N/A |
| 29%   46C    P8              27W / 250W |   1693MiB / 12288MiB |      2%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

# python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""

2023-11-16 16:46:54.131081: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2023-11-16 16:46:54.255566: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-11-16 16:46:54.255623: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-11-16 16:46:54.276648: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-11-16 16:46:54.327586: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2023-11-16 16:46:54.328299: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-16 16:46:56.486851: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.15']",2023-11-16T16:52:27Z,13,17,https://github.com/tensorflow/tensorflow/issues/62412,
269,tensorflow/tensorflow,jit-compiled `tfnp.take_along_axis` shape bug,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf_nightly-2.16.0.dev20231113-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64

### Custom code

Yes

### OS platform and distribution

colab

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

[colab demonstrating the issue](https://colab.research.google.com/drive/1nk6NXUoo7qE7g6NHlFHBgMWkJW0LuTMx?usp=sharing)

`tf.experimental.numpy.take_along_axis` returns tensor with incorrect shape when used with `tf.function(jit_compile=True)`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

x = tf.random.normal((5, 3, 2))
indices = tf.constant([[[-1]]], dtype=""int32"")


def f(x, i):
    return tf.squeeze(tf.experimental.numpy.take_along_axis(x, i, axis=-2), axis=-2)


z = f(x, indices)
print(z.shape)  # (5, 2)

z1 = tf.function(f, jit_compile=True)(x, indices) # errors
print(z1.shape)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-97d5eab12d23> in <cell line: 1>()
----> 1 z1 = tf.function(f, jit_compile=True)(x, indices)
      2 print(z1.shape)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51   try:
     52     ctx.ensure_initialized()
---> 53     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                         inputs, attrs, num_outputs)
     55   except core._NotOkStatusException as e:

InvalidArgumentError: Tried to explicitly squeeze dimension 1 but dimension was not 1: 2

Stack trace for op definition: 
File ""/usr/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
File ""/usr/lib/python3.10/runpy.py"", line 86, in _run_code
File ""/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py"", line 37, in <module>
File ""/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py"", line 992, in launch_instance
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py"", line 619, in start
File ""/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py"", line 195, in start
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
File ""/usr/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
File ""/usr/lib/python3.10/asyncio/events.py"", line 80, in _run
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 685, in <lambda>
File ""/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py"", line 738, in _run_callback
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 825, in inner
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 786, in run
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 361, in process_one
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 261, in dispatch_shell
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py"", line 539, in execute_request
File ""/usr/local/lib/python3.10/dist-packages/tornado/gen.py"", line 234, in wrapper
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py"", line 302, in do_execute
File ""/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py"", line 539, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 2975, in run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3030, in _run_cell
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py"", line 78, in _pseudo_sync_runner
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3257, in run_cell_async
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3473, in run_ast_nodes
File ""/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py"", line 3553, in run_code
File ""<ipython-input-4-97d5eab12d23>"", line 1, in <cell line: 1>
File ""<ipython-input-2-4f365be98111>"", line 8, in f

	 [[{{node Squeeze}}]]
	tf2xla conversion failed while converting __inference_f_283[_XlaMustCompile=true,config_proto=3175580994766145631,executor_type=11160318154034397263]. Run with TF_DUMP_GRAPH_PREFIX=/path/to/dump/dir and --vmodule=xla_compiler=2 to obtain a dump of the compiled functions. [Op:__inference_f_283]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops']",2023-11-14T07:54:57Z,8,0,https://github.com/tensorflow/tensorflow/issues/62391,
270,tensorflow/tensorflow,TypeError: this __dict__ descriptor does not support '_DictWrapper' objects,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

nightly

### Custom code

Yes

### OS platform and distribution

Colab

### Python version

3.10

### Current behavior?

- Install a clean tensorflow environment with the latest `typing-extensions` pip package.
- Assign a dictionary to a `tf.Module` (will wrap it in a `_DictWrapper` for tracking).
- Output that dictionary from a `tf.function`.
- `TypeError: this __dict__ descriptor does not support '_DictWrapper' objects`.

This is somewhat of a zombie bug, see https://github.com/tensorflow/tensorflow/issues/60687.

This is important because it breaks all `keras` functional models with dictionary output, but is not a `keras` bug. This can be reproduced simply with low-level tensorflow.

We either need to continue pinning an older version of typing extensions, or fix tensorflow to work with the latest version of typing extension. The latter seems less likely to keep breaking.

### Standalone code to reproduce the issue

https://colab.research.google.com/gist/mattdangerw/6904dc4ab29ff936ad3c3b966848f463/dict-output-bug.ipynb


### Relevant log output

```shell
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-4-4a26ae41286e> in <cell line: 12>()
     10 x = tf.constant([2, 3])
     11 y = tf.constant([3, -2])
---> 12 f(x, y)

4 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py in from_value(value, context)
    142   if context.is_legacy_signature and isinstance(value, trace.TraceType):
    143     return value
--> 144   elif isinstance(value, trace.SupportsTracingProtocol):
    145     generated_type = value.__tf_tracing_type__(context)
    146     if not isinstance(generated_type, trace.TraceType):

/usr/local/lib/python3.10/dist-packages/typing_extensions.py in __instancecheck__(cls, instance)
    601             for attr in cls.__protocol_attrs__:
    602                 try:
--> 603                     val = inspect.getattr_static(instance, attr)
    604                 except AttributeError:
    605                     break

/usr/lib/python3.10/inspect.py in getattr_static(obj, attr, default)
   1741         if (dict_attr is _sentinel or
   1742             type(dict_attr) is types.MemberDescriptorType):
-> 1743             instance_result = _check_instance(obj, attr)
   1744     else:
   1745         klass = obj

/usr/lib/python3.10/inspect.py in _check_instance(obj, attr)
   1688     instance_dict = {}
   1689     try:
-> 1690         instance_dict = object.__getattribute__(obj, ""__dict__"")
   1691     except AttributeError:
   1692         pass

TypeError: this __dict__ descriptor does not support '_DictWrapper' objects
```
","['stat:awaiting tensorflower', 'type:bug', 'type:support', 'comp:apis']",2023-10-25T01:11:31Z,15,1,https://github.com/tensorflow/tensorflow/issues/62217,
271,tensorflow/tensorflow,"tf.strings.to_number cannot convert positive integers prefixed with ""+"" when out_type is tf.int32 or tf.int64","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.11.0, 2.13.0, 2.14.0

### Custom code

Yes

### OS platform and distribution

MacOS 13.1

### Mobile device

Macbook Pro

### Python version

3.10.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Converting string to numbers with ""+""  throws errors in TF 2.11.0, 2.13.0, 2.14.0 when `out_type=tf.int64` or `out_type=tf.in32`. Not expecting error to be thrown and strings can be correctly converted into integers. For example, I would like to parse timezone information from a string (using substring)

```python 
t = tf.constant([
    ""2023-05-07 17:32:25-08:00"", # utc: next day
    ""2023-05-07 05:32:25+11:00"", # utc: previous day
    ""2023-05-07 05:32:25-08:00"", # utc: same date
    ""2023-02-29 23:32:15-04:00"", # leap year
    ]
)

```

### Standalone code to reproduce the issue

```shell
Code to reproduce


import tensorflow

# these are all okay
tf.string.to_number(tf.constant(""-11""), out_type=tf.int64)
tf.string.to_number(tf.constant(""11""), out_type=tf.int64)
tf.string.to_number(tf.constant(""+11""), out_type=tf.float32)

# this throws the error below
tf.strings.to_number(tf.constant(""+11""), out_type=tf.int64)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/edward/opt/miniforge3/envs/testtf214/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/Users/edward/opt/miniforge3/envs/testtf214/lib/python3.10/site-packages/tensorflow/python/framework/ops.py"", line 5888, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StringToNumber_device_/job:localhost/replica:0/task:0/device:CPU:0}} StringToNumberOp could not correctly convert string: +11 [Op:StringToNumber] name:
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF2.14']",2023-10-21T01:55:21Z,2,1,https://github.com/tensorflow/tensorflow/issues/62191,
272,tensorflow/tensorflow,tf.map_fn and TensorArray do not seem to support backpropagation,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf2.14

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

****
I provide three different snippet implementation of a function. I am calculating multiple losses with respect to different networks. 
The first and third implementation gives `gradients=None` in both graph and eager mode.

The second implementation works in eager mode but not in graph mode. 

It seems that tf.map_fn does not support backpropagation #19897 and this issue could be related to that.

### Standalone code to reproduce the issue

```shell
First snippet ( Gradients are None in both graph and eager mode )


    def _compute_unadjusted_ce(self, label_batch, logits):
        ce = tf.map_fn(
            lambda x: tf.reduce_mean(
                tf.keras.losses.categorical_crossentropy(
                    from_logits=True,
                    y_pred=logits[x, ...],
                    y_true=label_batch
                )
            ),
            elems=tf.range(self.num_learners),
            fn_output_signature=tf.float32,
            parallel_iterations=1,
            back_prop=True
        )
        return ce



# The following function does the work but fails in graph mode (`tf.function`) ( similar to #37512   


 def _compute_unadjusted_ce(self, label_batch, logits):
       ce = list()
         for i in range(self.num_learners):
            ce.append(
                 tf.reduce_mean(
                     tf.keras.losses.categorical_crossentropy(
                         from_logits=True,
                         y_pred=logits[i, ...],
                         y_true=label_batch
                     )
                 )
             )
    
         return ce


# The following function causes gradients as None ( in both eager and graph mode )


 def _compute_unadjusted_ce(self, label_batch, logits):
         ce = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True, clear_after_read=True)
         for i in range(self.num_learners):
             ce = ce.write(
                 ce.size(),
                 tf.reduce_mean(
                     tf.keras.losses.categorical_crossentropy(
                         from_logits=True,
                         y_pred=logits[i, ...],
                         y_true=label_batch
                     )
                 )
             )
    
         return ce.stack()
```


### Relevant log output

_No response_","['stat:awaiting response', 'stat:awaiting tensorflower', 'type:bug', 'type:feature', 'stale', 'comp:ops', 'TF2.14']",2023-10-19T12:34:32Z,12,0,https://github.com/tensorflow/tensorflow/issues/62174,
273,tensorflow/tensorflow,TFLite CNN model quantization error,"### 1. System information

- OS Platform and Distribution : Ubuntu 22.04.3 LTS
- TensorFlow installation: pip install tensorflow (virtual env: venv)
- TensorFlow library: pip package -> tensorflow 2.14.0

### 2. Code

Colab to build the models and reproduce the issue:

[Reproduce the issue](https://colab.research.google.com/drive/1sx7qmXfP5RA1ituF5Fmh8X-cy0-M7Xol?usp=sharing)


### 3. Failure after conversion
Hi, I'm having an issue when trying to use signatures of quantized tflite CNN model.

The conversion and quantization go well, but when I try to use infer or fine_tune signatures, I get the following error which seems to be related to the quantization process:

RuntimeError: tensorflow/lite/kernels/conv.cc:374 affine_quantization->zero_point->data[i] != 0 (-24 != 0)Node number 21 (CONV_2D) failed to prepare.tensorflow/lite/kernels/conv.cc:374 affine_quantization->zero_point->data[i] != 0 (-24 != 0)Node number 41 (CONV_2D) failed to prepare.


**Note**: I don't get this error with the exact same code using linear model instead of CNN.

Thanks for your help","['stat:awaiting tensorflower', 'type:bug', 'TFLiteConverter', 'TF2.14']",2023-10-19T09:39:50Z,6,1,https://github.com/tensorflow/tensorflow/issues/62171,
274,tensorflow/tensorflow,tf.truncatemod does not support half and bfloat16 data type,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the documentation (https://www.tensorflow.org/api_docs/python/tf/truncatemod) tf.truncatemod supports the half and bfloat16 data type but in practical it does not.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import warnings
warnings.filterwarnings(""ignore"")
x = tf.constant(np.random.rand(2,2), dtype='half')
y = tf.constant(np.random.randint(0, 100, ()), dtype='half')
out = tf.truncatemod(x,y)  # crash
print(out)


import tensorflow as tf
import numpy as np
import warnings
warnings.filterwarnings(""ignore"")
x = tf.constant(np.random.rand(2,2), dtype='bfloat16')
y = tf.constant(np.random.randint(0, 100, ()), dtype='bfloat16')
out = tf.truncatemod(x,y)  # crash
print(out)
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node TruncateMod}} = TruncateMod[T=DT_BFLOAT16]
All kernels registered for op TruncateMod:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='DEFAULT'; T in [DT_INT32]
  device='GPU'; T in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
 [Op:TruncateMod] name: 

NotFoundError: Could not find device for node: {{node TruncateMod}} = TruncateMod[T=DT_HALF]
All kernels registered for op TruncateMod:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='DEFAULT'; T in [DT_INT32]
  device='GPU'; T in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
 [Op:TruncateMod] name:
```
","['stat:awaiting response', 'awaiting review', 'type:bug', 'stale', 'comp:ops', 'TF2.14']",2023-10-09T08:36:40Z,4,0,https://github.com/tensorflow/tensorflow/issues/62070,
275,tensorflow/tensorflow,Custom Gradient Computation not working in TF 2.14,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.14.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I am using W&B's Keras callback `WandbCallback`. This callback has a feature to log gradients of each layer at every step. This feature works fine till TF 2.13.0 but is erroring out in TF 2.14.0.

This piece of code works fine in Tf 2.13.0 but errors out in TF 2.14.0:

```
import numpy as np
import tensorflow as tf
print(tf.__version__)
import wandb
from wandb.keras import WandbModelCheckpoint
from wandb.keras import WandbCallback

run = wandb.init(project=""keras"")

x = np.random.randint(255, size=(100, 28, 28, 1))
y = np.random.randint(10, size=(100,))

dataset = (x, y)


def get_model():
    m = tf.keras.Sequential()
    m.add(tf.keras.layers.Conv2D(3, 3, activation=""relu"", input_shape=(28, 28, 1)))
    m.add(tf.keras.layers.Flatten())
    m.add(tf.keras.layers.Dense(10, activation=""softmax""))
    return m


model = get_model()
model.compile(
    loss=""sparse_categorical_crossentropy"",
    optimizer=""sgd"",
    metrics=[""accuracy""],
)

model.fit(
    x,
    y,
    epochs=5,
    validation_data=(x, y),
    callbacks=[
        WandbCallback(
            save_model=False,
            log_gradients=True,
            training_data=(x,y)
        )
    ],
)

```

I investigated further and was able to narrow it down to the gradient logging logic which again works fine for 2.13.0 but not for 2.14.0. 

I think this has to do with the breaking changes with `tf.Tensor`.

The piece of code below is the gradient logging logic which errors out in the latest version.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(tf.__version__)
import wandb
import numpy as np

_training_data_x = np.random.randint(255, size=(100, 28, 28, 1))
_training_data_y = np.random.randint(10, size=(100,))


def get_model():
    m = tf.keras.Sequential()
    m.add(tf.keras.layers.Conv2D(3, 3, activation=""relu"", input_shape=(28, 28, 1)))
    m.add(tf.keras.layers.Flatten())
    m.add(tf.keras.layers.Dense(10, activation=""softmax""))
    return m

model = get_model()
model.compile(
    loss=""sparse_categorical_crossentropy"",
    optimizer=""sgd"",
    metrics=[""accuracy""],
)


def _get_custom_optimizer_parent_class():
    from pkg_resources import parse_version

    if parse_version(tf.__version__) >= parse_version(""2.9.0""):
        custom_optimizer_parent_class = tf.keras.optimizers.legacy.Optimizer
    else:
        custom_optimizer_parent_class = tf.keras.optimizers.Optimizer

    return custom_optimizer_parent_class


_custom_optimizer_parent_class = _get_custom_optimizer_parent_class()
print(_custom_optimizer_parent_class)


class _CustomOptimizer(_custom_optimizer_parent_class):
    def __init__(self):
        super().__init__(name=""CustomOptimizer"")
        self._resource_apply_dense = tf.function(self._resource_apply_dense)
        self._resource_apply_sparse = tf.function(self._resource_apply_sparse)
        tf.print(self._resource_apply_dense)

    def _resource_apply_dense(self, grad, var):
        var.assign(grad)

    # this needs to be implemented to prevent a NotImplementedError when
    # using Lookup layers.
    def _resource_apply_sparse(self, grad, var, indices):
        pass

    def get_config(self):
        return super().get_config()


class _GradAccumulatorCallback(tf.keras.callbacks.Callback):
    """"""Accumulates gradients during a fit() call when used in conjunction with the CustomOptimizer above.""""""

    def set_model(self, model):
        super().set_model(model)
        self.og_weights = model.get_weights()
        self.grads = [np.zeros(tuple(w.shape)) for w in model.trainable_weights]

    def on_batch_end(self, batch, logs=None):
        for g, w in zip(self.grads, self.model.trainable_weights):
            g += w.numpy()
        self.model.set_weights(self.og_weights)

    def get_grads(self):
        return [g.copy() for g in self.grads]


inputs = model.inputs
print(inputs)
outputs = model(inputs)
grad_acc_model = tf.keras.models.Model(inputs, outputs)
grad_acc_model.compile(loss=model.loss, optimizer=_CustomOptimizer())

_grad_accumulator_model = grad_acc_model
_grad_accumulator_model.summary()

_grad_accumulator_callback = _GradAccumulatorCallback()


_grad_accumulator_model.fit(
    _training_data_x,
    _training_data_y,
    verbose=0,
    callbacks=[_grad_accumulator_callback],
)

weights = model.trainable_weights
grads = _grad_accumulator_callback.grads
print(weights)

metrics = {}
for weight, grad in zip(weights, grads):
    metrics[
        ""gradients/"" + weight.name.split("":"")[0] + "".gradient""
    ] = wandb.Histogram(grad)

print(metrics)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/ayushthakur/client/wandb/test_grad_logging.py"", line 88, in <module>
    _grad_accumulator_model.fit(
  File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/tmp/__autograph_generated_file4zq8l42d.py"", line 15, in tf__train_function
    retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
TypeError: in user code:

    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1377, in train_function  *
        return step_function(self, iterator)
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1360, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1349, in run_step  **
        outputs = model.train_step(data)
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/engine/training.py"", line 1130, in train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py"", line 601, in minimize
        return self.apply_gradients(grads_and_vars, name=name)
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py"", line 760, in apply_gradients
        return tf.__internal__.distribute.interim.maybe_merge_call(
    File ""/opt/conda/envs/tf214/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py"", line 844, in _distributed_apply
        with tf.control_dependencies([tf.group(update_ops)]):

    TypeError: 'inputs' should be zero or more (nested) Tensors. Received 'None' with type '<class 'NoneType'>'.
```
","['stat:awaiting response', 'stat:awaiting tensorflower', 'type:bug', 'stale', 'comp:ops', 'regression issue', 'TF2.14']",2023-10-05T09:51:32Z,6,1,https://github.com/tensorflow/tensorflow/issues/62053,
276,tensorflow/tensorflow,`pip install tf-nightly[and-cuda]` fails for recent nightlies,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

nightly

### Custom code

No

### OS platform and distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

**The issue**

Attempting to run `pip install tf-nightly[and-cuda]` will download a ton of nightly candidates before installing one from mid-September (before tf bumped to cuda12).

Attempting to pin the more recent versions shows the error with recent nightlies.

```shell
pip install tf-nightly[and-cuda]==2.15.0.dev20231002
...
ERROR: Could not find a version that satisfies the requirement tensorrt-libs==8.6.1; extra == ""and-cuda"" (from tf-nightly[and-cuda]) (from versions: 9.0.0.post11.dev1, 9.0.0.post12.dev1, 9.0.1.post11.dev4, 9.0.1.post12.dev4)
ERROR: No matching distribution found for tensorrt-libs==8.6.1; extra == ""and-cuda""
```

You can work around this with `pip install tf-nightly[and-cuda] --extra-index-url https://pypi.nvidia.com`.

**What should happen**
`pip install tf-nightly[and-cuda]` should not self conflict, and recent nighties should be installable via PyPI.

### Standalone code to reproduce the issue

https://colab.research.google.com/gist/mattdangerw/00acd58e43aabe7f80a74d595788bd86/tf-nightly-and-cuda.ipynb


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'comp:gpu:tensorrt', 'TF2.14']",2023-10-02T19:25:22Z,7,2,https://github.com/tensorflow/tensorflow/issues/62035,
277,tensorflow/tensorflow,Tensorflow profiler client does not support IPv6,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.11.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04.3

### Mobile device

_No response_

### Python version

3.10.2

### Bazel version

6.0.0

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11/8.2

### GPU model and memory

_No response_

### Current behavior?

[https://github.com/google/tsl/blob/3dee2c5930eb8ee9c6a7486434240dafadf12fb4/tsl/profiler/utils/session_manager.cc#L188]()

`std::vectorabsl::string_view parts = absl::StrSplit(host_port, ':');`

When trying to connect to an IPv6 host-port pair, which is typically denoted as [xxxx:xxxx:blah]:port, `profiler_client.trace()` throws an error. It should have logic that supports both IPv4 and IPv6. 

### Standalone code to reproduce the issue

```shell
File 1:
jax.profiler.start_server(9876) #on IPv6 xxxx:xxxx:x:xxxx:xxxx::xxx

File 2:
from tensorflow.python.profiler import profiler_client
profiler_client.trace(
        '[xxxx:xxxx:x:xxxx:xxxx::xxx]:9876',
        duration_ms=1000,
        logdir='foo',
    )
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""foo.py"", line 2, in _trigger_trace
    profiler_client.trace(
  File "".../foo.runfiles/tensorflow_python_deps_tensorflow/tensorflow/python/profiler/profiler_client.py"", line 129, in trace
    _pywrap_profiler.trace(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Could not interpret ""[xxxx:xxxx:x:xxxx:xxxx::xxx]:9876"" as a host-port pair.
```
","['stat:awaiting response', 'stat:awaiting tensorflower', 'type:bug', 'stale', 'comp:apis', 'TF 2.11']",2023-09-29T17:59:57Z,5,0,https://github.com/tensorflow/tensorflow/issues/62018,
278,tensorflow/tensorflow,tf.train.Checkpoint.restore does not restore the tf.data.Iterator state.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current behavior?

`tf.data.Dataset` iterators are supported in `tf.train.Checkpoint`.  When I try to restore a checkpoint, I always get a `StopIteration` error.  This should not be the case because : 

1. I save the checkpoint only at the end of an epoch.
2. Before saving the checkpoint, I always call `db_iter=iter(dataset)` ,so it should give me a new iterator.

In prior versions of TF there were functions like `make_initializable_iterator()` which are now deprecated. Since, there is no native mechanism to reset a python iterator to beginning, I wonder why `tf.train.Checkpoint.restore()` does not save the iterator state properly.

I also understand well that one can just loop over the dataset like:

```
import tensorflow as tf
ds = tf.data.Dataset.range(50)
for index, data in ds:
    # Do something 
```

However, saving the dataset in the checkpoint has two problems:

a) **I have not explicitly tested this**  For a very large dataset like MSCOCO or OpenImages or even Imagenet, what does saving the dataset mean , if I already have the dataset as TFRecords ? Will it end up saving the whole dataset again inside the  checkpoint ? At least, `tf.data.Dataset.save()` points towards this. In case this is it, I would never like to save a large dataset directly into the checkpoint.

b) **This is fully tested** If I experiment with a small simple dataset ( e.g:- `tf.data.Dataset.range(10)` ) and save it in the checkpoint, then it does not restore the state of the iterator. This observation is the same as #48178 which is still unresolved. 

### Standalone code to reproduce the issue

```shell
import os
import numpy as np
import tensorflow as tf
from absl import app


class Net(tf.keras.Model):
    def __init__(self):
        super(Net, self).__init__()
        self.x = tf.Variable(tf.keras.initializers.GlorotUniform()(shape=[1]))

    def call(self, inputs):
        return inputs


def main(argv):
    del argv
    tf.random.set_seed(123)
    net = Net()
    optim = tf.optimizers.Adam(learning_rate=0.001)

    dataset = tf.data.Dataset.from_tensor_slices((np.arange(10)))
    dataset = dataset.shuffle(10, reshuffle_each_iteration=True)
    dataset = dataset.batch(2)
    db_iter = iter(dataset)
    step = tf.Variable(0)
    checkpoint = tf.train.Checkpoint(
        step=step, optimizer=optim, net=net, db_iter=db_iter
    )
    manager = tf.train.CheckpointManager(checkpoint, ""./ckpts"", max_to_keep=50)
    manager.restore_or_initialize()
    print(step)
    for epoch in range(10):
        manager.restore_or_initialize()
        for _ in range(dataset.cardinality()):
            batch = next(db_iter)
            step.assign_add(1)
            print(batch)
        db_iter = iter(dataset)
        manager.save()
        print(f""Epoch {epoch} finished."")


if __name__ == ""__main__"":
    app.run(main)
```
```


### Relevant log output

```shell
The first time, it will print something like : 

<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=0>
tf.Tensor([3 6], shape=(2,), dtype=int64)
tf.Tensor([4 0], shape=(2,), dtype=int64)
tf.Tensor([1 5], shape=(2,), dtype=int64)
tf.Tensor([7 8], shape=(2,), dtype=int64)
tf.Tensor([2 9], shape=(2,), dtype=int64)
Epoch 0 finished.
tf.Tensor([6 3], shape=(2,), dtype=int64)
tf.Tensor([0 5], shape=(2,), dtype=int64)
tf.Tensor([2 8], shape=(2,), dtype=int64)
tf.Tensor([1 7], shape=(2,), dtype=int64)
tf.Tensor([4 9], shape=(2,), dtype=int64)
Epoch 1 finished.
tf.Tensor([0 3], shape=(2,), dtype=int64)
tf.Tensor([1 4], shape=(2,), dtype=int64)
tf.Tensor([6 7], shape=(2,), dtype=int64)
tf.Tensor([9 8], shape=(2,), dtype=int64)
tf.Tensor([5 2], shape=(2,), dtype=int64)
Epoch 2 finished.
tf.Tensor([3 7], shape=(2,), dtype=int64)
tf.Tensor([0 9], shape=(2,), dtype=int64)
tf.Tensor([4 2], shape=(2,), dtype=int64)
tf.Tensor([1 8], shape=(2,), dtype=int64)
tf.Tensor([5 6], shape=(2,), dtype=int64)

and so on....


From next time onwards:

    raise StopIteration
StopIteration
```
","['stat:awaiting response', 'stat:awaiting tensorflower', 'type:bug', 'stale', 'comp:data', 'TF 2.13']",2023-09-28T15:49:30Z,8,0,https://github.com/tensorflow/tensorflow/issues/62006,
279,tensorflow/tensorflow,TimeDistributed not compatible with multi-output models,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have been attempting to create a custom CNN-RNN hybrid model that incorporates a pretrained custom DenseNet-derived CNN. My model requires from the CNN both its final output and the intermediate outputs of its dense blocks (as I am attempting to create a U-Net like architecture). I attempted to create a new model from the given CNN that would output these intermediate convolutional layers along with the final output value. However, when I connected it to a TimeDistributed layer, I received a strange error with no clear explanation:

`AttributeError: 'list' object has no attribute 'shape'`

After some experimentation, I realized this error was arising because the TimeDistributed layer was not programmed to handle multi-output layers passed to it. I understand that this issue will likely require a fix to Tensorflow. In the meanwhile, any suggestions for workarounds would be appreciated.

### Standalone code to reproduce the issue

```shell
cnn_model = tf.keras.models.load_model(cnn_filepath)
cnn_final_output = cnn_model.layers[-1].output
cnn_intermediate_output = cnn_model.layers[-3].output
new_model = tf.keras.models.Model(inputs=cnn_input, outputs=[cnn_final_output,cnn_intermediate_output])

output = tfl.TimeDistributed(new_model)(input_images, mask=mask)
```


### Relevant log output

```shell
AttributeError                            Traceback (most recent call last)
/Users/[NAME REMOVED]/[NAME REMOVED] DL/TRG_project_recurrent_cnn_residual_instantaneous_train.ipynb Cell 1 line 1
    173 cnn_model.trainable = False
    174 model = RNNCNNResidualModel(N_A, cnn_model, [""average_pooling2d_6"", ""average_pooling2d_19"", ""average_pooling2d_44"", ""activation_239""])
--> 176 model.build([tf.TensorShape([None, None, 101, 101, 1]), tf.TensorShape([None, None, 1])])
    178 #Initialize inputs
    180 plot_model(model, to_file='rnn_model_plot.png', show_shapes=True, show_layer_names=True)

File ~/mambaforge/envs/tensorflow/lib/python3.10/site-packages/keras/src/engine/training.py:521, in Model.build(self, input_shape)
    516     raise ValueError(
    517         ""You can only call `build()` on a model if its ""
    518         ""`call()` method accepts an `inputs` argument.""
    519     )
    520 try:
--> 521     self.call(x, **kwargs)
    522 except (tf.errors.InvalidArgumentError, TypeError) as e:
    523     raise ValueError(
    524         ""You cannot build your model by calling `build` ""
    525         ""if your layers do not support float type inputs. ""
   (...)
    529         f""`call` is: {e}.""
    530     )

File ~/[NAME REMOVED] DL/rnn_cnn_residual_model.py:41, in RNNCNNResidualModel.call(self, inputs, states, return_state, training)
     38 if states is None:
     39   states = self.lstm_cell.get_initial_state(input_timesteps)
---> 41 output = tfl.TimeDistributed(self.cnn_model)(input_images, mask=mask)
     42 fgr_output = output[0]
     43 lstm_input = tfl.Concatenate()([fgr_output, input_timesteps])

File ~/mambaforge/envs/tensorflow/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/mambaforge/envs/tensorflow/lib/python3.10/site-packages/keras/src/backend.py:1534, in int_shape(x)
   1514 """"""Returns shape of tensor/variable as a tuple of int/None entries.
   1515 
   1516 Args:
   (...)
   1531 
   1532 """"""
   1533 try:
-> 1534     shape = x.shape
   1535     if not isinstance(shape, tuple):
   1536         shape = tuple(shape.as_list())

AttributeError: 'list' object has no attribute 'shape'
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.13']",2023-09-28T14:07:56Z,6,0,https://github.com/tensorflow/tensorflow/issues/62004,
280,tensorflow/tensorflow,XLA compiled `floordiv` allows `integer division by zero`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA compiled `floordiv` allows `integer division by zero` where an exception `Integer division by zero` will be raised without XLA compilation.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

""""""
XLA Compiled
""""""
class Model(tf.keras.Model):
    @tf.function(jit_compile=True)
    def call(self, x1):
        x2 = tf.math.floordiv(3, 0)
        x3 = tf.math.multiply(x1, x2)
        return x3

m = Model()
x1 = tf.constant(1, shape=[])
print(m(x1)) # tf.Tensor(-1, shape=(), dtype=int32)

""""""
Without XLA
""""""
class Model(tf.keras.Model):
    def call(self, x1):
        x2 = tf.math.floordiv(3, 0)
        x3 = tf.math.multiply(x1, x2)
        return x3

m = Model()
x1 = tf.constant(1, shape=[])
print(m(x1))
```


### Relevant log output

```shell
InvalidArgumentError: Exception encountered when calling layer 'model_21' (type Model).

{{function_node __wrapped__FloorDiv_device_/job:localhost/replica:0/task:0/device:CPU:0}} Integer division by zero [Op:FloorDiv] name: 

Call arguments received by layer 'model_21' (type Model):
  • x1=tf.Tensor(shape=(), dtype=int32)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'comp:tf.function', 'TF2.14']",2023-09-23T19:51:17Z,1,0,https://github.com/tensorflow/tensorflow/issues/61965,
281,tensorflow/tensorflow,calling Model in a loop would leak memory,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

tf2.13.0

### Custom code

Yes

### OS platform and distribution

Windows Server 2019

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

there is a transformer model when i try to decode messages, translate input sentence to target sentence i get memory blow up, memory is good when i use transformer.fit(), but in a loop like below it blows up memory, tf.keras.backend.clear_session() does’t help, also accuracy decrease when i use that, gc.collect() doesn’t work also
here is my code

```python
def decode_sequence(input_sentence):
    tokenized_input_sentence = input_vectorization([input_sentence])
    decoded_sentence = START_TOKEN
    for i in tf.range(max_decoded_sentence_length):
        tokenized_target_sentence = output_vectorization([decoded_sentence])#[:, :-1]
        
        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])
        
        
        sampled_token_index = np.argmax(predictions[0, i, :])
        sampled_token = output_index_lookup[sampled_token_index]
        decoded_sentence += sampled_token

        if sampled_token == END_TOKEN:
            break
    
   
    gc.collect()
    return decoded_sentence

from tqdm import tqdm
def overall_accuracy(pairs):
    corrects = 0
    inputs = pairs[2739:]
    iter = tqdm(inputs)
    for i, pair in enumerate(iter):
        input_text = pair[0]
        target = pair[1]
        predicted = decode_sequence(input_text)
        #guess = '✓' if predicted == target else '✗'
        #print('Sample Number : ', i, 'Predicted : ', predicted, 'Real : ', target, guess)
        if predicted == target:
            corrects += 1
        iter.set_postfix(corrects=corrects, accuracy=corrects / (i + 1))
    
    return corrects / len(inputs)

print(""Overall Acurracy : "", overall_accuracy(test_pairs))```

### Standalone code to reproduce the issue

```shell
calling model in a loop
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.13']",2023-09-21T16:51:08Z,7,0,https://github.com/tensorflow/tensorflow/issues/61942,
282,tensorflow/tensorflow,tf.linalg.cholesky output normal value on a complex64 matrix that is not positive definite.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

On a randomly generated matrix that is not positive definite, tf.linalg.cholesky outputs nan if the matrix's dtype is float and outputs 0 if the matrix' dtype is complex. It may be not appropriate especially when tf.linalg.cholesky outputs 0 on an invalid input without giving any abnormal behaviors. For your inference, np.linalg.cholesky will directly raises with error message when receiving non-positive definite matrix in float or complex data type.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
import warnings
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings(""ignore"")
np.random.seed(1234)
array = np.random.rand(4,4).astype(""complex64"")
try:
    print(""Numpy's result: "", np.linalg.cholesky(array))
except:
    print(""Numpy crash."")

print(""TensorFlow's result: "", tf.linalg.cholesky(tf.constant(array)))

array = np.random.rand(4,4).astype(""float64"")
try:
    print(""Numpy's result: "", np.linalg.cholesky(array))
except:
    print(""Numpy crash."")

print(""TensorFlow's result: "", tf.linalg.cholesky(tf.constant(array)))
```


### Relevant log output

```shell
Numpy crash.
TensorFlow's result:  tf.Tensor(
[[0.+0.j 0.+0.j 0.+0.j 0.+0.j]
 [0.+0.j 0.+0.j 0.+0.j 0.+0.j]
 [0.+0.j 0.+0.j 0.+0.j 0.+0.j]
 [0.+0.j 0.+0.j 0.+0.j 0.+0.j]], shape=(4, 4), dtype=complex64)
Numpy crash.
TensorFlow's result:  tf.Tensor(
[[nan  0.  0.  0.]
 [nan nan  0.  0.]
 [nan nan nan  0.]
 [nan nan nan nan]], shape=(4, 4), dtype=float64)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-09-19T13:54:42Z,3,0,https://github.com/tensorflow/tensorflow/issues/61916,
283,tensorflow/tensorflow,"Randomization generating repeated sequences with `tf.function`, `tf.random.set_seed` and `tf.cond`","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.15.0-dev20230919

### Custom code

No

### OS platform and distribution

Linux Ubuntu 22.04.2 LTS

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

We have encountered an unexpected issue with `tf.function` compilation combined with the global/graph-level seed setting (`tf.random.set_seed`); under some circumstances the random variables are generating identical sequences within the same function. The issue seems to be connected with TensorFlow conditionals (`tf.cond`) within the function, particularly when the branches contain the randomization calls. Removing `tf.cond` results in the generation of unique random values, as expected.

We understand the following expected behavior under `tf.function` compilation when a global/graph-level seed is set, as per the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/random/set_seed):

> Note that `tf.function` acts like a re-run of a program in this case. When the global seed is set but operation seeds are not set, the sequence of random numbers are the same for each `tf.function`.

However, the problem we are observing arises within a single instance of the function, not across multiple instances. The behavior is as if the internal counters
in `tf.random.uniform` get reset when there is a TensorFlow conditional in the graph. Is this expected behavior?

To illustrate this issue, the example below presents two scenarios. The first one (""SAD"" mode) shows the function producing repeated random sequences when `tf.cond` is present. The second scenario (""HAPPY"" mode) shows the function generating unique random values once `tf.cond` is removed.


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
from pprint import pprint

tf.random.set_seed(42)

var = tf.Variable(0, dtype=tf.int32)
def get_value(happy):
    if happy:
        return tf.stack([tf.random.uniform(()) for _ in range(2)])
    else:
        return tf.cond(
            var == 0,
            lambda: tf.stack([tf.random.uniform(()) for _ in range(2)]),
            lambda: tf.stack([tf.random.uniform(()) for _ in range(2)]),
        )

def randomize(happy):
    return [get_value(happy) for _ in range(3)]

print(""SAD"")
pprint(tf.function(randomize)(False))
print(""HAPPY"")
pprint(tf.function(randomize)(True))
```


### Relevant log output

```shell
SAD
[<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>,
 <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>,
 <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>]
HAPPY
[<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.8354591 , 0.15012848], dtype=float32)>,
 <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.98781276, 0.63789964], dtype=float32)>,
 <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.00857747, 0.02621067], dtype=float32)>]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops']",2023-09-19T10:48:34Z,1,0,https://github.com/tensorflow/tensorflow/issues/61912,
284,tensorflow/tensorflow,`tf.device` context manager does not restore `cudaCurrentDevice` under some conditions,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

12.0

### GPU model and memory

_No response_

### Current behavior?

When using `tf.device` context manager, the current device of cuda runtime remains ""dirty"" even after exiting the context manager. This happens when: 1. tensorflow is initializing GPU context on this line (tf.device), 2. there is no materialization of tensors on GPU.

For context, keeping a clean state of current device context is important to keep tensorflow in sync with other GPU based libraries such as [cuDF](github.com/rapidsai/cuDF). [RMM](github.com/rapidsai/rmm) memory allocators also depends on the assumption that the context stays the same throughout the lifetime of allocations.

### Standalone code to reproduce the issue

```shell
https://gist.github.com/isVoid/9eded87fca35e86a2c2dc85f603383c2
```


### Relevant log output

```shell
# Log output of the first cell. The second and third current device context should be 0.

(<cudaError_t.cudaSuccess: 0>, 0)
(<cudaError_t.cudaSuccess: 0>, 7)
(<cudaError_t.cudaSuccess: 0>, 7)
(<cudaError_t.cudaSuccess: 0>, 0)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'comp:gpu', 'TF 2.13']",2023-09-19T10:21:44Z,0,0,https://github.com/tensorflow/tensorflow/issues/61911,
285,tensorflow/tensorflow,tf.linalg.cholesky fails on half precision,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Following the documentation: https://www.tensorflow.org/api_docs/python/tf/linalg/cholesky, tf.linalg.cholesky is expected to accept tensor in half precision but it fails.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
np.random.seed(2023)

# {'input_ndims': 2}
input = tf.constant(np.random.rand(3,3), dtype='half')
out = tf.linalg.cholesky(input)
```
```


### Relevant log output

```shell
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6654 def raise_from_not_ok_status(e, name):
   6655   e.message += ("" name: "" + str(name if name is not None else """"))
-> 6656   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   6657 
   6658 

NotFoundError: Could not find device for node: {{node Cholesky}} = Cholesky[T=DT_HALF]
All kernels registered for op Cholesky:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_HALF]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
 [Op:Cholesky] name:
```
","['awaiting review', 'type:bug', 'comp:ops', 'TF 2.13']",2023-09-19T05:37:10Z,2,0,https://github.com/tensorflow/tensorflow/issues/61907,
286,tensorflow/tensorflow,XLA doesn't do the DCE as autocluster,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA doesn't do the DCE as autocluster. In the example below, the second line `sliced2 = tf.slice(x1, [0, 1, 0, 0], [-1, -1, 4, -1])` is dead code, which is deleted when enabling `autocluster`. However, the XLA compiled model will still execute this line.

It is expected to delete this line since it is dead code.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os

""""""
Autocluster
""""""

os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit'

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function
  def call(self, x1):
    sliced1 = tf.slice(x1, [0, 0, 1, 0], [-1, -1, 1, -1])
    sliced2 = tf.slice(x1, [0, 1, 0, 0], [-1, -1, 4, -1])
    return sliced1

# Initializing the model
x1_shape = (1, 3, 3, 2)
m = Model()

# Inputs to the model
x1 = tf.range(18)
x1 = tf.reshape(x1, x1_shape)

# Call model
output_tensor = m(x1)

""""""
XLA
""""""
os.environ['TF_XLA_FLAGS'] = ''

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True)
  def call(self, x1):
    sliced1 = tf.slice(x1, [0, 0, 1, 0], [-1, -1, 1, -1])
    sliced2 = tf.slice(x1, [0, 1, 0, 0], [-1, -1, 4, -1])
    return sliced1

# Initializing the model
x1_shape = (1, 3, 3, 2)
m = Model()

# Inputs to the model
x1 = tf.range(18)
x1 = tf.reshape(x1, x1_shape)

# Call model
output_tensor = m(x1)

""""""
InvalidArgumentError: Exception encountered when calling layer 'model_5' (type Model).

Expected size[2] in [0, 3], but got 4
""""""
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF2.14']",2023-09-17T18:49:54Z,2,0,https://github.com/tensorflow/tensorflow/issues/61884,
287,tensorflow/tensorflow,XLA compiled model skip `build` method,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA compiled model skip `build` method, which is not expected since the `build` method is expected to be invoked automatically before the first execution of `call()`invoked automatically before the first execution of call().

The example below shows that the XLA compiled model doesn't invoke `build` since `self.w` is still `[[1, 0], [0, 1]]` instead of the result by `add_weight`.

### Standalone code to reproduce the issue

```shell
""""""
Without XLA
""""""
import tensorflow as tf

class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()
    self.w = tf.Variable([[1., 0.], [0., 1.]])

  def build(self, input_shape):
    super(Model, self).build(input_shape)
    self.w = self.add_weight(""weight"", shape=input_shape[1:], trainable=True)
    
  def call(self, x):
    return tf.matmul(x, self.w), self.w

# Initializing the model
m = Model()
# Input to the model
x1 = tf.constant([[6., 7.], [2., 7.]], shape=[1,2,2])
print(m(x1))
""""""
(<tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=
array([[[4.0300035, 3.5133138],
        [3.804155 , 5.0799932]]], dtype=float32)>, <tf.Variable 'model_4/weight:0' shape=(2, 2) dtype=float32, numpy=
array([[ 0.05646205, -0.3916698 ],
       [ 0.5273187 ,  0.83761895]], dtype=float32)>)
""""""

""""""
With XLA
""""""

class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()
    self.w = tf.Variable([[1., 0.], [0., 1.]])

  def build(self, input_shape):
    super(Model, self).build(input_shape)
    self.w = self.add_weight(""weight"", shape=input_shape[1:], trainable=True)

  @tf.function(jit_compile=True)  
  def call(self, x):
    return tf.matmul(x, self.w), self.w

# Initializing the model
m = Model()
# Input to the model
x1 = tf.constant([[6., 7.], [2., 7.]], shape=[1,2,2])
print(m(x1))

""""""
(<tf.Tensor: shape=(1, 2, 2), dtype=float32, numpy=
array([[[6., 7.],
        [2., 7.]]], dtype=float32)>, <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1., 0.],
       [0., 1.]], dtype=float32)>)
""""""
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:tf.function', 'TF2.14']",2023-09-17T02:11:14Z,2,0,https://github.com/tensorflow/tensorflow/issues/61882,
288,tensorflow/tensorflow,XLA compiled `tf.matmul` can work for two size-incompatible matrices,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

XLA compiled `tf.matmul` can work for two size-incompatible matrices, like `[1, 5]` and `[10, 1]`. By contrast, if we run `tf.matmul` directly without XLA compilation, it will raise the error as expected: `Matrix size-incompatible: In[0]: [1,5], In[1]: [10,1] [Op:BatchMatMulV2] name: `

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
""""""
With XLA
""""""
class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()
    self.w = tf.Variable(tf.random.normal([10,1]),
            shape=tf.TensorShape(None),
            dtype='float32')
  @tf.function(jit_compile=True)
  def call(self, x):
    return tf.matmul(x, self.w)

m = Model()

# Inputs to the model
x1 = tf.constant([1., 2., 3., -3., 2.5], shape=[1, 5])
print(m(x1))
# tf.Tensor([[-11.37509]], shape=(1, 1), dtype=float32)

""""""
Without XLA
""""""
class Model(tf.keras.Model):
  def __init__(self):
    super(Model, self).__init__()
    self.w = tf.Variable(tf.random.normal([10,1]),
            shape=tf.TensorShape(None),
            dtype='float32')

  def call(self, x):
    return tf.matmul(x, self.w)

m = Model()

# Inputs to the model
x1 = tf.constant([1., 2., 3., -3., 2.5], shape=[1, 5])
print(m(x1))
""""""
InvalidArgumentError: Exception encountered when calling layer 'model' (type Model).

{{function_node __wrapped__BatchMatMulV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} Matrix size-incompatible: In[0]: [1,5], In[1]: [10,1] [Op:BatchMatMulV2] name: 

Call arguments received by layer 'model' (type Model):
  • x=tf.Tensor(shape=(1, 5), dtype=float32)
""""""
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.13']",2023-09-16T21:02:17Z,2,0,https://github.com/tensorflow/tensorflow/issues/61881,
289,tensorflow/tensorflow,Invalid `Conv2d` can be executed without compilation,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.15.0-dev20230914

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

An invalid `Conv2d` can be executed without compilation. By contrast, after using `@tf.function(jit_compile=True)`, it will raise error `Negative dimension size caused by subtracting 2 from 1 for '{{node conv2d_6/Conv2D}} ...`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

""""""
Don't use @tf.function(jit_compile=True)
""""""

class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.conv2d = tf.keras.layers.Conv2D(5, 2, activation=tf.nn.relu)

  def call(self, x):
    conv2d = self.conv2d(x)
    return conv2d

# Initializing the model
m = Model()

# Inputs to the model
# This input results in a Conv2D kernel that has the shape (1, 2, 3, 5).
x1 = tf.constant([[[[1., 2., 3.], [4., 5., 6.]]]], shape=[1, 1, 2, 3])

y = m(x1)
print(y.shape)
# (1, 0, 1, 5)

""""""
Using @tf.function(jit_compile=True)
""""""
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()
    self.conv2d = tf.keras.layers.Conv2D(5, 2, activation=tf.nn.relu)
  @tf.function(jit_compile=True)
  def call(self, x):
    conv2d = self.conv2d(x)
    return conv2d
# Initializing the model
m = Model()

# Inputs to the model
# This input results in a Conv2D kernel that has the shape (1, 2, 3, 5).
x1 = tf.constant([[[[1., 2., 3.], [4., 5., 6.]]]], shape=[1, 1, 2, 3])

y = m(x1)
""""""

    ValueError: Exception encountered when calling layer 'conv2d_6' (type Conv2D).
    
    Negative dimension size caused by subtracting 2 from 1 for '{{node conv2d_6/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=""VALID"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x, conv2d_6/Conv2D/ReadVariableOp)' with input shapes: [1,1,2,3], [2,2,3,5].
    
    Call arguments received by layer 'conv2d_6' (type Conv2D):
      • inputs=tf.Tensor(shape=(1, 1, 2, 3), dtype=float32)


Call arguments received by layer 'model_10' (type Model):
  • x=tf.Tensor(shape=(1, 1, 2, 3), dtype=float32)
""""""
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'comp:tf.function']",2023-09-15T23:55:07Z,4,0,https://github.com/tensorflow/tensorflow/issues/61879,
290,tensorflow/tensorflow,Wrong element_spec when mapping ragged dataset,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13

### Custom code

Yes

### OS platform and distribution

Linux, Rocky 9

### Mobile device

_No response_

### Python version

3.10.12

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

```python
ds = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2, 3], [4, 5]]))
ds.element_spec
```

The `element_spec` is a `RaggedTensorSpec`

But after a `map`, like for instance
```
ds.map(tf.square)
````
the `element_spec` is just a `TensorSpec`

### Standalone code to reproduce the issue

```shell
.
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.13']",2023-09-15T07:17:15Z,4,0,https://github.com/tensorflow/tensorflow/issues/61869,
291,tensorflow/tensorflow,StringLookup / tf.lookup resource cleanup on model cleared,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Describe the problem.

Define a simple network with a StringLookup of non-trivial size

Reload the network in a for loop.

Memory goes to infinity.

Describe the current behavior.
Memory goes up every load

Describe the expected behavior.
Memory doesn't go up every load

When using StringLookup: 

![image](https://user-images.githubusercontent.com/4107771/267815955-966e2a02-135a-4e68-a185-c49120e90092.png)

When using Hashing it works:

![image](https://github.com/tensorflow/tensorflow/assets/4107771/2145a1d1-1eaf-4595-808c-8f53a8a6616f)


### Standalone code to reproduce the issue

```shell
import time
import os

import tensorflow as tf
import psutil

def model_fn():
    X = tf.keras.Input(shape=(1,), dtype=tf.string)
    lookup = tf.keras.layers.StringLookup(
        vocabulary=tf.constant([str(x) for x in range(100_000)])
    )(X)
    Y = tf.math.reduce_sum(lookup, axis=1)

    return tf.keras.Model(inputs=[X], outputs=[Y])

model = model_fn()
model.save(""/tmp/test-model"")

loaded_model = None

process = psutil.Process()

def get_current_mem():
    return process.memory_info().rss / 1e6

def load():
    global loaded_model
    loaded_model = tf.saved_model.load(""/tmp/test-model"")

print('==========================================================')
print(""starting process..."")

for i in range(100_000):
    start_mem = get_current_mem()
    start = time.time()

    print(f""i={i} loading..."", end='')
    load()

    curr_mem = get_current_mem()
    end = time.time()
    print(f""done (mem_usage={curr_mem - start_mem}mb took={int(end - start)}s)"")
    time.sleep(0.25)
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.12']",2023-09-14T17:44:28Z,2,0,https://github.com/tensorflow/tensorflow/issues/61866,
292,tensorflow/tensorflow,Compiled x.shape throws Tuple out-of-range error,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.13, 2.15.0-dev20230913

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When a tensor has shape `(2,)`, `x.shape` returns `2`, and throws an error when I try to get `x.shape[0]`. Without `@tf.function(jit_compile=True)` it works well.

### Standalone code to reproduce the issue

```shell
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True)
  def call(self, x1):
    def select_2_or_one(x):
      print(x.shape)
      if int(x.shape[0]) == 2:
        return x
      else:
        return tf.stack([x,x])
    return tf.cond(tf.less(x1[0], x1[1]),
        lambda: select_2_or_one(x1),
        lambda: select_2_or_one(x1[0]))

m = Model()

input_shape = [2,]
x1 = tf.constant([2.,3.], shape=input_shape)

y = m(x1)
print(y)
```


### Relevant log output

```shell
if int(x.shape[0]) == 2:

    IndexError: tuple index out of range


Call arguments received by layer 'model_1' (type Model):
  • x1=tf.Tensor(shape=(2,), dtype=float32)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.13']",2023-09-14T15:39:18Z,2,0,https://github.com/tensorflow/tensorflow/issues/61862,
293,tensorflow/tensorflow,XLA compiled `tf.reshape` throws ValueError: Shape must be rank 1 but is rank 0,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf2.13, 2.15.0-dev20230913

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following model calls `tf.reshape` on an input tensor, and throws ValueError when compiled with XLA. Without XLA compilation, it runs smoothly. 

This issue is observed on TF 2.13 and also nightly. See the [gist](https://colab.research.google.com/gist/dengyinlin/d9368054f16a011f9d1fed3cede96b95/xla-compiled-tf-reshape-throws-valueerror-shape-must-be-rank-1-but-is-rank-0.ipynb) for more detail.


### Standalone code to reproduce the issue

```python
class Model(tf.keras.Model):

  def __init__(self):
    super(Model, self).__init__()

  @tf.function(jit_compile=True)
  def call(self, x1):
    x2 = tf.reshape(x1, (2, 2))
    return tf.reshape(x2, (4))

m = Model()

input_shape = (4)
x1 = tf.constant([4.,5.,6.,7.], shape=input_shape)

y = m(x1)
print(y)
```


### Relevant log output

```shell
ValueError: Shape must be rank 1 but is rank 0 for '{{node Reshape_1}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](Reshape, Reshape_1/shape)' with input shapes: [2,2], [].
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.13']",2023-09-13T21:09:29Z,5,0,https://github.com/tensorflow/tensorflow/issues/61856,
294,tensorflow/tensorflow,TFLite - Cannot execute with NNAPI on Samsung Galaxy Tab S9 (Snapdragon 8 Gen 2). On delegate CPU available,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.12.0

### Custom code

No

### OS platform and distribution

Android 13

### Mobile device

Samsung Galaxy Tab S9

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

clang 14

### CUDA/cuDNN version

_No response_

### GPU model and memory

Snapdragon 8 Gen 2

### Current behavior?

I have a C++/Kotlin Android application which make inference using Tensorflow Lite (TFLite library 2.12.0 get using conan center).
I use NNAPI delegate and it work great on Google Pixel tab and Samsung Galaxy Tab S8.

I try my application on the new tablet, Samsung Galaxy Tab S8 which have a processor 'snapdragon 8 gen 2', and this time, the inference work, BUT it use delegate for CPU and its much slower than S8...

Looking at the problem, when I call the function to build the interperter, I have this message: `INFO: Created TensorFlow Lite delegate for NNAPI.`, so everything since ok until that.

But, when I call `AllocateTensors()` on the interpreter, I have this messages:
```
Access denied finding property ""ro.mediatek.platform""
Created TensorFlow Lite XNNPACK delegate for CPU.
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```

Is there something special to do to used NNAPI on GPU/TPU in that case?

(You can see the full output of the build of interpreter + call to AllocateTensors() below)

### Standalone code to reproduce the issue

```shell
// Load the model
std::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromFile(filename);

// Build the interpreter
tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptr<tflite::Interpreter> interpreter;
tflite::InterpreterBuilder(*model, resolver)(&interpreter);

tflite::StatefulNnApiDelegate::Options options;
tflite::StatefulNnApiDelegate delegate(options);
interpreter->ModifyGraphWithDelegate(&delegate);

// Resize input tensors -> Here the delegate CPU will be used
interpreter->AllocateTensors();
```
```


### Relevant log output

```shell
12329-12329 Manager         I  findAvailableDevices
10477-10477 tflite          I  Created TensorFlow Lite delegate for NNAPI.
10477-10584 ....C++-Error   I  INFO: Created TensorFlow Lite delegate for NNAPI.
10477-10477 libc            W  Access denied finding property ""ro.mediatek.platform""
10477-10477 tflite          I  Created TensorFlow Lite XNNPACK delegate for CPU.
10477-10584 ....C++-Error   I  INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
```
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:lite', 'TFLiteNNAPIDelegate', 'TF 2.12']",2023-09-13T10:38:40Z,14,0,https://github.com/tensorflow/tensorflow/issues/61854,
295,tensorflow/tensorflow,tf.linalg.eigvals outputs UnboundLocalError when receiving a float16 tensor,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Given a float16 tensor, tf.linalg.eigvals outputs `UnboundLocalError: local variable 'out_dtype' referenced before assignment`. If tf.linalg.eigvals does not accept float16 tensor, it would be better if it can be explicit in the documentation and the error message can point this issue out.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
tensor = tf.constant([[1,2],[3,4]], dtype=tf.float16)
tf_out = tf.linalg.eigvals(tf.constant(tensor))
print(""TensorFlow's result: "",tf_out)
```


### Relevant log output

```shell
---------------------------------------------------------------------------
UnboundLocalError                         Traceback (most recent call last)
<ipython-input-69-5a0470cb5082> in <cell line: 3>()
      1 import tensorflow as tf
      2 tensor = tf.constant([[1,2],[3,4]], dtype=tf.float16)
----> 3 tf_out = tf.linalg.eigvals(tf.constant(tensor))
      4 print(""TensorFlow's result: "",tf_out)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/linalg_ops.py in eigvals(tensor, name)
    431   elif tensor.dtype == dtypes.float64 or tensor.dtype == dtypes.complex128:
    432     out_dtype = dtypes.complex128
--> 433   e, _ = gen_linalg_ops.eig(tensor, Tout=out_dtype, compute_v=False, name=name)
    434   return e
    435 

UnboundLocalError: local variable 'out_dtype' referenced before assignment
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-09-10T15:58:12Z,4,0,https://github.com/tensorflow/tensorflow/issues/61827,
296,tensorflow/tensorflow,"Inconsistent result between tf.linalg.eigvals and numpy/scipy on a (3,3) tensor","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Given the same tensor, tf.linalg.eigvals outputs results different from np.linalg.eigvals and scipy.linalg.eig. The latter two APIs are also computing the eigenvalues and it may be expected if tf.linalg.eigvals can output the same results as theirs.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import scipy
import numpy as np

np.random.seed(1234)
tensor = np.random.random((3, 3)).astype(np.float32)

tf_out = tf.linalg.eigvals(tf.constant(tensor))
np_out = np.linalg.eigvals(tensor)
scipy_out = scipy.linalg.eig(tensor)[0]
print(""TensorFlow's result: "",tf_out)
print(""Numpy's result: "", np_out)
print(""Scipy's result: "", scipy_out)
```


### Relevant log output

```shell
TensorFlow's result:  tf.Tensor([-0.20270659+0.j  1.7388148 +0.j  0.3935262 +0.j], shape=(3,), dtype=complex64)
Numpy's result:  [ 1.7388151  -0.20270674  0.39352626]
Scipy's result:  [ 1.7388152 +0.j -0.20270659+0.j  0.39352617+0.j]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-09-10T15:54:18Z,3,0,https://github.com/tensorflow/tensorflow/issues/61826,
297,tensorflow/tensorflow,The `mask` dtype should be asserted to be boolean in `tf.compat.v1.boolean_mask` (and `tf.boolean_mask`),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0-rc1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The `mask` dtype should be asserted to be boolean in `tf.compat.v1.boolean_mask` (and `tf.boolean_mask`).
The documentation [`tf.boolean_mask`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask) states that the `mask` is a K-D boolean Tensor, K <= N and K must be known statically.
However, currently, the `mask` dtype is not asserted to be boolean in `tf.compat.v1.boolean_mask` (and `tf.boolean_mask`) and cast to boolean if it is not a boolean Tensor, which may lead to unexpected results.
Generally, only the `0` and `1` values in the `mask` can be directly treated as `False` and `True` respectively, while other values should be determined by certain rules in different scenarios (_e.g._ negative values can be treated as `False` in some scenarios, or close to `0` values can be treated as `False` in some other scenarios).

I think it's worth updating the documentation to state that the `mask` will be cast to boolean if it is not a boolean Tensor, or warning users that the `mask` dtype should be boolean.
As mentioned in #54412, it is better to raise an InvalidArgumentError Exception when the `mask` dtype is not boolean, which forces users to cast the `mask` to boolean explicitly.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)
print(tf.config.list_physical_devices(), flush=True)


try:
    tensor = [0,1,2,3]
    mask = tf.random.uniform([4], dtype=tf.float64)
    x1 = tf.compat.v1.boolean_mask(tensor, mask) 
    print(x1, flush=True)
except Exception as e:
    print(""Success! Error:"", str(e), flush=True)
else:
    print(""Failed!"", flush=True)
```


### Relevant log output

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
tf.Tensor([0 1 2 3], shape=(4,), dtype=int32)
Failed!
```
","['awaiting review', 'type:bug', 'comp:ops', 'TF2.14']",2023-09-09T09:40:31Z,2,0,https://github.com/tensorflow/tensorflow/issues/61820,
298,tensorflow/tensorflow,Wrong answer from tflite model with certain configurations of depthwise conv2d,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 11
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13.0

### 2. Code

```
import tensorflow as tf
import numpy as np

tf.keras.utils.set_random_seed(0)

# Layer parameter values
imageSize = (8, 8)
numChannels = 2
fmt = 'channels_last'
kernelSize = (3, 1)
padding = 'same'
depthMult = 1
useBias = False
strides = (2, 2)

# Determine input shape
if fmt == 'channels_last':
    inShape = imageSize + (numChannels,)
else:
    inShape = (numChannels,) + imageSize

# Determine input size and create input data
inSize = (1,) + inShape
rng = np.random.default_rng(seed=999)
x = rng.random(size=inSize, dtype=np.float32)

# Construct Keras model
inp = tf.keras.layers.Input(shape=inShape, batch_size=1)
dconv = tf.keras.layers.DepthwiseConv2D(kernelSize, strides=strides, padding=padding, depth_multiplier=depthMult,
                                        data_format=fmt, activation=None, use_bias=useBias)(inp)
kModel = tf.keras.models.Model(inputs=inp, outputs=dconv)
kModel.compile()

# Run Keras model predict
kY = kModel.predict(x)
print(kY)

# Convert model to tflite
converter = tf.lite.TFLiteConverter.from_keras_model(kModel)
tfModel = converter.convert()

# Run tflite interpreter
interpreter = tf.lite.Interpreter(model_content=tfModel)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.set_tensor(input_details[0]['index'], x)
interpreter.invoke()
tfY = interpreter.get_tensor(output_details[0]['index'])
print(tfY)

isequal = np.allclose(kY, tfY, rtol=1e-4, atol=1e-4)
print(isequal)
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results - in this case all 0s

5. (optional) Any other info / logs

Some general observations on when this wrong answer occurs (all of the following must be met):
- One of the kernel_size dimensions is 1 (1xN or Nx1)
- Stride is not 1
- Padding is same

","['stat:awaiting tensorflower', 'type:bug', 'TFLiteConverter', 'TF 2.13']",2023-09-08T14:16:17Z,1,0,https://github.com/tensorflow/tensorflow/issues/61815,
299,tensorflow/tensorflow,`tensorflow/python/compiler/xla/jit_test` fails on PPC with $HOME unset,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.11

### Bazel version

6.3.1

### GCC/compiler version

12.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Running the test `//tensorflow/python/compiler/xla:jit_test_cpu` through Bazels build system with a pre-installed numpy (build from source) yields a segmentation fault I cannot explain.

It happened after upgrading our toolchain from GCC 11 & Python 3.10 to GCC 12 and Python 3.11 but includes also a couple other software versions which changed but trying to narrow them down wasn't successful. But seemingly only Python and OpenSSL (instead of BoringSSL which doesn't work on PPC) as well as the mentioned numpy should be involved.

After a lot of digging I found that omitting `$HOME` when executing the test (which Bazel does) triggers the segmentation fault.   
Adding `--test_env HOME=/non-existing` works around this.

I further traced it to `testJITCreateOpsLambda` in particular keeping only the single `compute` call at https://github.com/tensorflow/tensorflow/blob/v2.13.0/tensorflow/python/compiler/xla/jit_test.py#L76 is enough while `self.compute(False, create_ops)` works which means it is related to XLA compilation.

Also replacing [`random_uniform`](https://github.com/tensorflow/tensorflow/blob/v2.13.0/tensorflow/python/compiler/xla/jit_test.py#L70) by `constant_op.constant(1)` avoids the error, so it is related to how that call is compiled/run but I failed to further follow how that happens

I'm at loss how to debug this further and whether this is an issue with TensorFlows XLA compilation or a bug elsewhere only triggered by that special environment

### Standalone code to reproduce the issue

```shell
`bazel test --config=noaws --config=nogcp --config=nohdfs --compilation_mode=opt --config=opt --copt=""-fPIC"" --action_env=CPATH='/sw/installed/OpenSSL/1.1/include' --host_action_env=CPATH='/sw/installed/OpenSSL/1.1/include' --action_env=LIBRARY_PATH='/sw/installed/OpenSSL/1.1/lib' --host_action_env=LIBRARY_PATH='/sw/installed/OpenSSL/1.1/lib' --action_env=PYTHONPATH --host_action_env=PYTHONPATH --action_env PYTHON_BIN_PATH --action_env PYTHON_LIB_PATH --python_path=$(which python)  -- //tensorflow/python/compiler/xla:jit_test_cpu`

or reduced to the actual invocation after bazel failed:

`export PYTHONPATH=bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/com_google_protobuf/python:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/tblib_archive/src:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/astunparse_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/com_google_protobuf:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/dill_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/gast_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/opt_einsum_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/six_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/tblib_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/termcolor_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/typing_extensions_archive:bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/wrapt:$PYTHONPATH
(cd /dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/bazel-root/9c88b62e77874bb73aea75868f86ebae/execroot/org_tensorflow && \
cd - &&
  exec env - \
    LD_LIBRARY_PATH=$LD_LIBRARY_PATH \
    PATH=$PATH \
    PYTHONNOUSERSITE=1 \
    PYTHONPATH=$PYTHONPATH \
    HOME2='/fake' \
python bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/compiler/xla/jit_test.py)`
```


### Relevant log output

```shell
Running tests under Python 3.11.3: /beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/bin/python
[ RUN      ] JITTest.testJITCreateOpsLambda
2023-09-08 10:32:04.231849: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled
2023-09-08 10:32:04.404724: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x201108083080 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2023-09-08 10:32:04.404765: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2023-09-08 10:32:04.603193: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
Fatal Python error: Segmentation fault

Thread 0x0000200000048800 (most recent call first):
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1455 in _call_tf_sessionrun
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1362 in _run_fn
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1379 in _do_call
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1372 in _do_run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1192 in _run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 969 in run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2059 in run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/compiler/xla/jit_test.py"", line 55 in compute
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/compiler/xla/jit_test.py"", line 81 in testJITCreateOpsLambda
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/case.py"", line 579 in _callTestMethod
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/case.py"", line 623 in run
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/case.py"", line 678 in __call__
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/suite.py"", line 122 in run
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/suite.py"", line 84 in __call__
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/suite.py"", line 122 in run
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/suite.py"", line 84 in __call__
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/runner.py"", line 217 in run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/testing/_pretty_print_reporter.py"", line 86 in run
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/main.py"", line 274 in runTests
  File ""/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/unittest/main.py"", line 102 in __init__
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/app.py"", line 258 in _run_main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/absl_py/absl/app.py"", line 312 in run
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56 in main
  File ""/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/compiler/xla/jit_test.py"", line 321 in <module>

Extension modules: google.protobuf.pyext._message, numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, tensorflow.python.framework.fast_tensor_util (total: 15)
*** Received signal 11 ***
*** BEGIN MANGLED STACK TRACE ***
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_ppc/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2(+0xd76744)[0x200b0de86744]
[0x2000000504d8]
/beegfs/ws/1/s3248973-EasyBuild/easybuild-ml/software/Python/3.11.3-GCCcore-12.3.0/lib/libpython3.11.so.1.0(+0x14090c)[0x2000001b090c]
[0x2000000504d8]
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../libtensorflow_cc.so.2(_ZN3xla3cpu13CpuExecutable22ExecuteComputeFunctionEPKNS_20ExecutableRunOptionsEN4absl12lts_202301254SpanIKNS_23MaybeOwningDeviceMemoryEEEPNS_19HloExecutionProfileE+0x150)[0x200b15956c40]
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../libtensorflow_cc.so.2(+0x5be7c04)[0x200b15957c04]
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../libtensorflow_cc.so.2(_ZN15stream_executor4host10HostStream8WorkLoopEv+0x1ac)[0x200b1d822b0c]
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../libtensorflow_cc.so.2(+0xdab3274)[0x200b1d823274]
/dev/shm/s3248973-EasyBuild/TensorFlow/2.13.0/foss-2022b/TensorFlow/tensorflow-2.13.0/bazel-out/ppc-opt/bin/tensorflow/python/compiler/xla/jit_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/../../../_solib_ppc/_U_S_Stensorflow_Clibtensorflow_Uframework_Uimport_Ulib___Utensorflow/libtensorflow_framework.so.2(+0xd85088)[0x200b0de95088]
/lib64/libpthread.so.0(+0x8b94)[0x2000008c8b94]
/lib64/libc.so.6(clone+0xe4)[0x200000a585f4]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
	tsl::CurrentStackTrace[abi:cxx11]()
	
	__kernel_sigtramp_rt64
	
	__kernel_sigtramp_rt64
	xla::cpu::CpuExecutable::ExecuteComputeFunction(xla::ExecutableRunOptions const*, absl::lts_20230125::Span<xla::MaybeOwningDeviceMemory const>, xla::HloExecutionProfile*)
	
	stream_executor::host::HostStream::WorkLoop()
	
	
	
	clone
*** End stack trace ***
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'TF 2.13']",2023-09-08T08:32:32Z,0,0,https://github.com/tensorflow/tensorflow/issues/61814,
300,tensorflow/tensorflow,Process aborted when running `tf.gather` and `tf.compat.v1.gather` on GPU with large parameters and indices,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0-rc1

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Process aborted when running `tf.gather` and `tf.compat.v1.gather` on GPU with certain parameters and indices, while it throws an Exception on CPU.
It always happens on GPU favor when the `params` and `indices` are large, regardless of the `validate_indices` value.
Besides, the Check failed error still occurs when `validate_indices` is set to `True`, and it should be an Exception instead.
I think the root cause of this behavior should be related to the bug in #60276 and #60756, which face a Check failed error in `tf.raw_ops.Gather/V2` op.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)

try:
    validate_indices = False # True
    params = tf.saturate_cast(tf.random.uniform([13, 15, 7, 13, 14], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.half)
    indices = tf.saturate_cast(tf.random.uniform([11, 12, 6, 15, 11, 3], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int64)
    res = tf.gather(
        validate_indices=validate_indices,
        params=params,
        indices=indices,
    )
    # res = tf.compat.v1.gather(
    #     validate_indices=validate_indices,
    #     params=params,
    #     indices=indices,
    # )
except Exception as e:
    print(""Error:"", str(e), flush=True)
print(""Success!"", flush=True)
```


### Relevant log output

On GPU, it throws a Check failed and core dumped.

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
2023-09-03 13:08:24.487989: F ./tensorflow/core/util/gpu_launch_config.h:160] Check failed: work_element_count >= 0 (0 vs. -549025096)
Aborted (core dumped)
```

On CPU, it throws a Exception.

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
Error: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[7,11,2,8,5,1] = 424 is not in [0, 13) [Op:GatherV2] name: 
Success!
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-09-03T14:31:11Z,4,0,https://github.com/tensorflow/tensorflow/issues/61780,
301,tensorflow/tensorflow,Process aborted when running `tf.keras.layers.MaxPooling1D` on GPU with large `pool_size`,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.14.0-rc1

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Process aborted with Check failed error when running `tf.keras.layers.MaxPooling1D` on GPU with large `pool_size`, while it throws an Exception on CPU.
When I reduce the `pool_size_0` to `1e+18`, it throws an Exception on GPU, which is a more reasonable behavior I think.
This behavior is similar to what described in #61642, which also throws a Check failed error by using a large `pool_size` for `tf.compat.v1.layers.MaxPooling1D` on GPU.


### Standalone code to reproduce the issue

```python
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION, flush=True)

try:
    pool_size_0 = 1e+19
    pool_size = [pool_size_0,]
    strides_0 = 2
    strides = [strides_0,]
    padding = ""same""
    data_format = ""channels_last""
    arg_class = tf.keras.layers.MaxPooling1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)
    # arg_class = tf.keras.layers.MaxPool1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)
    # arg_class = tf.compat.v1.keras.layers.MaxPooling1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)
    # arg_class = tf.compat.v1.keras.layers.MaxPool1D(pool_size=pool_size,strides=strides,padding=padding,data_format=data_format,)
    arg_input_0_tensor = tf.random.uniform([1, 5, 4], dtype=tf.float32)
    arg_input_0 = tf.identity(arg_input_0_tensor)
    arg_input = [arg_input_0,]
    out = arg_class(*arg_input)
except Exception as e:
    print(""Error:"", str(e), flush=True)
print(""Success!"", flush=True)
```


### Relevant log output

On GPU, it throws a Check failed and core dumped.

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
2023-09-03 11:34:09.267078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600
2023-09-03 11:34:09.267132: F tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:1019] Check failed: cudnnSetPoolingNdDescriptor( handle_.get(), (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum ? cudnn_max_pooling_mode : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING), propagate_nans ? CUDNN_PROPAGATE_NAN : CUDNN_NOT_PROPAGATE_NAN, nd, shape.data(), padding.data(), strides.data()) == CUDNN_STATUS_SUCCESS (3 vs. 0)
Aborted (core dumped)
```

On CPU, it throws a Exception.

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
Error: Exception encountered when calling layer 'max_pooling1d' (type MaxPooling1D).

{{function_node __wrapped__MaxPool_device_/job:localhost/replica:0/task:0/device:CPU:0}} Sliding window ksize for dimension 1 was zero. [Op:MaxPool]

Call arguments received by layer 'max_pooling1d' (type MaxPooling1D):
  • inputs=tf.Tensor(shape=(1, 5, 4), dtype=float32)
Success!
```

When I change the `pool_size_0` to `1e+18`, it throws a Exception on GPU.

```text
v2.14.0-rc0-34-gdd01672d9a9 2.14.0-rc1
2023-09-03 11:51:28.441325: W tensorflow/core/framework/op_kernel.cc:1816] OP_REQUIRES failed at maxpooling_op.cc:1260 : INVALID_ARGUMENT: Attr ksize has value 1000000000000000000 out of range for an int32
Error: Exception encountered when calling layer 'max_pooling1d' (type MaxPooling1D).

{{function_node __wrapped__MaxPool_device_/job:localhost/replica:0/task:0/device:GPU:0}} Attr ksize has value 1000000000000000000 out of range for an int32 [Op:MaxPool]

Call arguments received by layer 'max_pooling1d' (type MaxPooling1D):
  • inputs=tf.Tensor(shape=(1, 5, 4), dtype=float32)
Success!
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.13']",2023-09-03T14:26:52Z,3,0,https://github.com/tensorflow/tensorflow/issues/61778,
302,tensorflow/tensorflow,FAILED: //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test on mac arm64 wheels,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.14rc0

### Custom code

No

### OS platform and distribution

macOS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Test is failing.
This checkin the regression started:

```
commit e1e4de39de51064a359320f9b32936bf1599d4a0Author: Laura Pak <lpak@google.com>Date:   Tue Apr 26 16:51:30 2022 -0700    Update highwayhash from fd3d9af80465e4383162e4a7c5e2f406e82dd968 to c13d28517a4db259d738ea4886b1f00352a3cc33.    PiperOrigin-RevId: 444703993
```


Reverting that fixes the issue.

### Standalone code to reproduce the issue

```shell

bazel --bazelrc='./tensorflow/tools/ci_build/osx/arm64/.macos.bazelrc'  test //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test
```


### Relevant log output

```shell
The test is failing with:

INFO: From Testing //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test:==================== Test output for //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test:dyld[1906]: symbol not found in flat namespace 

'__ZNK11highwayhash11HighwayHashILj16EEclERA4_KyPKcmPy'==================================================================================================== 

Test output for //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test:dyld[1942]: symbol not found in flat namespace '__ZNK11highwayhash11HighwayHashILj16EEclERA4_KyPKcmPy'==================================================================================================== Test output for //tensorflow/tools/proto_splitter/cc:saved_model_splitter_test:dyld[1976]: symbol not found in flat namespace '__ZNK11highwayhash11HighwayHashILj16EEclERA4_KyPKcmPy'================================================================================
```
```


cc @learning-to-play , @nitins17 and @mihaimaruseac 
","['stat:awaiting tensorflower', 'type:bug', 'subtype:macOS', 'TF 2.13']",2023-09-01T22:27:27Z,0,0,https://github.com/tensorflow/tensorflow/issues/61774,
303,tensorflow/tensorflow,Full integer quantization not possible with grouped convolution,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 12.5
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.13

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

```
train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=""training"",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

def representative_data_gen():
  for input_value, labels in train_ds:
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_saved_model('./model.pb')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen

tflite_model = converter.convert()

with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```

The error that I get is 

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[15], line 8
      3 #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]
      4 #converter.inference_input_type = tf.int8
      5 #converter.inference_output_type = tf.int8
      6 converter.representative_dataset = representative_data_gen
----> 8 tflite_model = converter.convert()
     10 with open('model.tflite', 'wb') as f:
     11     f.write(tflite_model)

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1065, in _export_metrics.<locals>.wrapper(self, *args, **kwargs)
   1062 @functools.wraps(convert_func)
   1063 def wrapper(self, *args, **kwargs):
   1064   # pylint: disable=protected-access
-> 1065   return self._convert_and_export_metrics(convert_func, *args, **kwargs)

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1042, in TFLiteConverterBase._convert_and_export_metrics(self, convert_func, *args, **kwargs)
   1040 self._save_conversion_params_metric()
   1041 start_time = time.process_time()
-> 1042 result = convert_func(self, *args, **kwargs)
   1043 elapsed_time_ms = (time.process_time() - start_time) * 1000
   1044 if result:

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1390, in TFLiteSavedModelConverterV2.convert(self)
   1384 else:
   1385   self._debug_info = _get_debug_info(
   1386       _convert_debug_info_func(self._trackable_obj.graph_debug_info),
   1387       graph_def,
   1388   )
-> 1390 return self._convert_from_saved_model(graph_def)

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:1257, in TFLiteConverterBaseV2._convert_from_saved_model(self, graph_def)
   1254 converter_kwargs.update(quant_mode.converter_flags())
   1256 result = _convert_saved_model(**converter_kwargs)
-> 1257 return self._optimize_tflite_model(
   1258     result, quant_mode, quant_io=self.experimental_new_quantizer
   1259 )

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    213 except Exception as error:
    214   report_error_message(str(error))
--> 215   raise error from None

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    202 @functools.wraps(func)
    203 def wrapper(*args, **kwargs):
    204   try:
--> 205     return func(*args, **kwargs)
    206   except ConverterError as converter_error:
    207     if converter_error.errors:

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:991, in TFLiteConverterBase._optimize_tflite_model(self, model, quant_mode, quant_io)
    989   q_allow_float = quant_mode.is_allow_float()
    990   q_variable_quantization = quant_mode.enable_mlir_variable_quantization
--> 991   model = self._quantize(
    992       model,
    993       q_in_type,
    994       q_out_type,
    995       q_activations_type,
    996       q_bias_type,
    997       q_allow_float,
    998       q_variable_quantization,
    999   )
   1001 m_in_type = in_type if in_type else _dtypes.float32
   1002 m_out_type = out_type if out_type else _dtypes.float32

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/lite.py:710, in TFLiteConverterBase._quantize(self, result, input_type, output_type, activations_type, bias_type, allow_float, enable_variable_quantization)
    706 calibrate_quantize = _calibrator.Calibrator(
    707     result, custom_op_registerers_by_name, custom_op_registerers_by_func
    708 )
    709 if self._experimental_calibrate_only or self.experimental_new_quantizer:
--> 710   calibrated = calibrate_quantize.calibrate(
    711       self.representative_dataset.input_gen
    712   )
    714 if self._experimental_calibrate_only:
    715   return calibrated

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:215, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    213 except Exception as error:
    214   report_error_message(str(error))
--> 215   raise error from None

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py:205, in convert_phase.<locals>.actual_decorator.<locals>.wrapper(*args, **kwargs)
    202 @functools.wraps(func)
    203 def wrapper(*args, **kwargs):
    204   try:
--> 205     return func(*args, **kwargs)
    206   except ConverterError as converter_error:
    207     if converter_error.errors:

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:254, in Calibrator.calibrate(self, dataset_gen)
    244 @convert_phase(Component.OPTIMIZE_TFLITE_MODEL, SubComponent.CALIBRATE)
    245 def calibrate(self, dataset_gen):
    246   """"""Calibrates the model with specified generator.
    247 
    248   Returns:
   (...)
    252     dataset_gen: A generator that generates calibration samples.
    253   """"""
--> 254   self._feed_tensors(dataset_gen, resize_input=True)
    255   return self._calibrator.Calibrate()

File ~/miniforge3/envs/tf213/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py:143, in Calibrator._feed_tensors(self, dataset_gen, resize_input)
    139     self._calibrator.Prepare(
    140         [list(s.shape) for s in input_array], signature_key
    141     )
    142   else:
--> 143     self._calibrator.Prepare([list(s.shape) for s in input_array])
    144 else:
    145   if signature_key is not None:

RuntimeError: tensorflow/lite/kernels/conv.cc:352 input_channel % filter_input_channel != 0 (2 != 0)Node number 6 (CONV_2D) failed to prepare.
```

The model itself got converted from pytorch to onnx and then to pb. The issue here is that the model has a grouped convolutional layers. This is fixed for dynamic range quantisation but for full integer quantisation using a representative dataset this still seems to fail. Any quick fix possible?
","['stat:awaiting tensorflower', 'type:bug', 'TFLiteConverter', 'ModelOptimizationToolkit', 'TF 2.13']",2023-08-31T08:14:44Z,9,0,https://github.com/tensorflow/tensorflow/issues/61760,
304,tensorflow/tensorflow,tf.data.Dataset.save deadlocks on error ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

Linux

### Mobile device

_No response_

### Python version

3.8.17

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When saving a dataset to disk, TF will silently brick if there's an error in the data input pipeline. Methinks it should rather raise on the save instead of deadlocking.

Here's a reproducible example:

```python
import tensorflow as tf
# '2.13.0'

dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])
dataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, ""error""))
dataset.save('/tmp/hello')
# => deadlock
```

You can add an `ignore_errors` to the pipeline and it'll rightfully ignore, but it's nonintuitive to track down why the input bricks. 

```python
dataset = dataset.ignore_errors()
# => OK save
```

### Standalone code to reproduce the issue

```shell
See above.
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.13']",2023-08-28T22:43:16Z,1,0,https://github.com/tensorflow/tensorflow/issues/61736,
305,tensorflow/tensorflow,ExtensionType does not allow creating tf.function methods,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.13.0, tf 2.12.0

### Custom code

Yes

### OS platform and distribution

Colab, Mac

### Mobile device

_No response_

### Python version

3.11.3

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When calling a method on ExtensionType, it seems that `self` is flattened, making it incompatible with `tf.function` and `input_signature`. Is this the intended behavior?

### Standalone code to reproduce the issue

Colab: https://colab.research.google.com/drive/19b65SIbsUgV4CKGX0DGgS0IcbXeOsOw9?usp=sharing

```shell
import tensorflow as tf

print(""tf.__version__:"", tf.__version__)

class A(tf.experimental.ExtensionType):
    x1: tf.Tensor
    x2: tf.Tensor

_a = A.Spec(tf.TensorSpec((5, 6)), tf.TensorSpec((5, 6)))

class B(tf.experimental.ExtensionType):
    y1: tf.Tensor
    y2: tf.Tensor
    y3: int

_b = B.Spec(tf.TensorSpec((7, 8)), tf.TensorSpec((7, 8)), 9)

class C(tf.experimental.ExtensionType):
    a1: A
    a2: A
    a3: A
    a4: A

    @tf.function(input_signature=[_b])
    def f(self, b: B):
        return b

x = tf.zeros((5, 6))
y = tf.zeros((7, 8))
a = A(x, x)
b = B(y, y, 9)
c = C(a, a, a, a)
c.f(b)
```


### Relevant log output

Colab:
```
tf.__version__: 2.12.0
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-1-6a3d1772c2f1> in <cell line: 33>()
     31 b = B(y, y, 9)
     32 c = C(a, a, a, a)
---> 33 c.f(b)

1 frames
/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/function_spec.py in cast_inputs_to_signature(inputs, input_signature)
    538         check_types=False)  # lists are convert to tuples for `tf.data`.
    539   except ValueError:
--> 540     raise ValueError(""Structure of Python function inputs does not match ""
    541                      ""input_signature:\n""
    542                      f""{format_error_message(inputs, input_signature)}."")

ValueError: Structure of Python function inputs does not match input_signature:
  inputs: (
    C(a1=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>), a2=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>), a3=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>), a4=A(x1=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>, x2=<tf.Tensor: shape=(5, 6), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0.]], dtype=float32)>)),
    B(y1=<tf.Tensor: shape=(7, 8), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, y2=<tf.Tensor: shape=(7, 8), dtype=float32, numpy=
array([[0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>, y3=9))
  input_signature: (
    B.Spec(y1=TensorSpec(shape=(7, 8), dtype=tf.float32, name=None), y2=TensorSpec(shape=(7, 8), dtype=tf.float32, name=None), y3=9)).
```

Mac:
```shell
tf.__version__: 2.13.0
Traceback (most recent call last):
  File ""/tmp/a.py"", line 31, in <module>
    c.f(b)
  File ""/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py"", line 181, in __call__
    raise ValueError(
ValueError: Signature specifies 4 arguments, got: 10.
```

Mac (tf_nightly-2.15.0.dev20230827-cp311-cp311-macosx_12_0_arm64):
```shell
tf.__version__: 2.15.0-dev20230827
Traceback (most recent call last):
  File ""/tmp/a.py"", line 33, in <module>
    c.f(b)
  File ""/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/opt/homebrew/anaconda3/envs/ml/lib/python3.11/site-packages/tensorflow/core/function/polymorphism/function_type.py"", line 391, in unpack_inputs
    p.type_constraint._to_tensors(bound_parameters.arguments[p.name])  # pylint: disable=protected-access
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyError: 'y1'
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:tf.function', 'TF 2.13']",2023-08-27T12:47:18Z,1,0,https://github.com/tensorflow/tensorflow/issues/61712,
306,tensorflow/tensorflow,Null pointer exception in constant folding of grappler.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Segmentation fault.


### Standalone code to reproduce the issue


[NPE.zip](https://github.com/tensorflow/tensorflow/files/12444694/NPE.zip)

Run the poc.py in the zip.
In constant folding, after `FoldNode`, fold nodes will be updated with the new name at 【1】. But later, the old node name will still be referenced in `MergeConcat` (【2】) with `GetNode`(【3】), leading to an NPE.



```C++
//tensorflow/core/grappler/optimizers/constant_folding.cc
Status ConstantFolding::FoldNode(NodeDef* node, GraphDef* output_graph,
                                 bool* result_too_large) {
    ...
    else if (port < static_cast<int>(const_nodes.size()) &&
               !const_nodes[port].name().empty()) {
          //【1】
          node_map_->UpdateInput(output->name(), NodeName(output->input(i)),
                                   const_nodes[port].name());
          *output->mutable_input(i) = const_nodes[port].name();
 } 

bool ConstantFolding::MergeConcat(bool use_shape_info,
                                  GraphProperties* properties,
                                  GraphDef* optimized_graph, NodeDef* node) {
   ……
   for (int i = 0; i < num_regular_inputs - 1; ++i) {
       //【2】
       const NodeDef* input_node = node_map_->GetNode(node->input(i));
       if (!IsReallyConstant(*input_node)) {……}
}

//tensorflow/core/grappler/utils.h
NodeDefT* GetNode(const string& name) const {
    const string node_name = NodeName(name);
    auto it = nodes_.find(node_name);
    if (it == nodes_.end()) {
      VLOG(1) << ""Node could not be found: "" << name;
      return nullptr; //【3】
    }
    return it->second;
 }
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:grappler', 'TF 2.13']",2023-08-26T06:08:07Z,0,0,https://github.com/tensorflow/tensorflow/issues/61708,
307,tensorflow/tensorflow,LLVM conflict with libosmesa6,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

Debian Bullseye 11.7

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Tensorflow 2.13.0 binary causes some conflict with libOSMesa which uses LLVM-11. Tensorflow 2.7.1 didn't have this problem.

libosmesa6 (20.3.5-1) is installed through apt-get
https://packages.debian.org/bullseye/libosmesa6

![image](https://github.com/tensorflow/tensorflow/assets/4276548/d7a109e1-f545-4692-a9e9-3088dbb1ed87)





### Standalone code to reproduce the issue

```shell
#include <stdio.h>
#include <tensorflow/c/c_api.h>

#include <dlfcn.h>

int main() {
  printf(""Hello from TensorFlow C library version %s\n"", TF_Version());

  dlopen(""libOSMesa.so.6"", RTLD_NOW);

  return 0;
}
```


### Relevant log output

```shell
gcc hello_tf.c -ltensorflow -ldl -o hello_tf

./hello_tf
2023-08-25 13:50:27.935004: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Hello from TensorFlow C library version 2.13.0
: CommandLine Error: Option 'debug-counter' registered more than once!
LLVM ERROR: inconsistency in registered CommandLine options
Aborted
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:runtime', 'TF 2.13']",2023-08-25T12:00:28Z,0,1,https://github.com/tensorflow/tensorflow/issues/61696,
308,tensorflow/tensorflow,tf.linalg.pinv isn't ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.6.22

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Pinv should give similar gradient to inv. Here's the code snippet and result.
![image](https://github.com/tensorflow/tensorflow/assets/10631563/de9cdb82-1c93-4a18-a79e-95c0b0834ae6)


![image](https://github.com/tensorflow/tensorflow/assets/10631563/cfdadbb9-5506-4181-a868-69830c5ee96b)


### Standalone code to reproduce the issue

```shell
See figures in behavior section.
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-08-23T13:32:23Z,5,0,https://github.com/tensorflow/tensorflow/issues/61675,
309,tensorflow/tensorflow,oneDNN matmul is not supported to have a dynamic dimension,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

v1.12.1-98433-gb64e3d4ae2e 2.15.0-dev20230812

### Custom code

Yes

### OS platform and distribution

Linux Debian 6.3.11

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following code works for nightly version 2.15.0-dev20230811, but fails for the version 2.15.0-dev20230812. The issue happens when we compile the function.

### Standalone code to reproduce the issue

```shell
import tensorflow.compat.v2 as tf

class _Slice2Idx:
  """"""Utility to convert numpy basic slices into TF scatter_nd indices.""""""

  def __init__(self, tensor):
    self.ranges = [tf.range(d, dtype=tf.int32) for d in tensor.shape]

  def __getitem__(self, slices):
    grid = tf.meshgrid(*(rng[sl] for rng, sl in zip(self.ranges, slices)), indexing='ij')
    return tf.stack(grid, axis=-1)

def no_pivot_ldl(matrix, name='no_pivot_ldl'):
  with tf.name_scope(name) as name:
    matrix = tf.convert_to_tensor(matrix)
    triangular_factor = tf.linalg.band_part(matrix, num_lower=-1, num_upper=0)
    slix = _Slice2Idx(triangular_factor)

    def fn(triangular_factor, i):
      column_tail = triangular_factor[..., i+1:, i]
      x = tf.einsum('...i,...j->...ij', column_tail, column_tail)
      idx = slix[i+1:, i+1:]
      triangular_factor = tf.tensor_scatter_nd_update(triangular_factor, idx, x)
      return triangular_factor

    triangular_factor = tf.foldl(
        fn=fn,
        elems=tf.range(tf.shape(triangular_factor)[-1]),
        initializer=triangular_factor)
    return triangular_factor

inp = tf.Variable([[2., 1.], [1., 2.]])
alt_chol_jit = tf.function(no_pivot_ldl, autograph=False, jit_compile=True)
print(no_pivot_ldl(inp))
alt_chol_jit(inp)
```
```


### Relevant log output

```shell
tf.Tensor(
[[2. 0.]
 [1. 1.]], shape=(2, 2), dtype=float32)
2023-08-23 11:31:54.059605: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5db9ec0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2023-08-23 11:31:54.059681: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2023-08-23 11:31:54.068228: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-08-23 11:31:54.093913: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at xla_ops.cc:625 : UNIMPLEMENTED: CustomCall ""__onednn$matmul"" is not supported to have a dynamic dimension

tensorflow.python.framework.errors_impl.UnimplementedError: CustomCall ""__onednn$matmul"" is not supported to have a dynamic dimension [Op:__inference_no_pivot_ldl_264]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'comp:core']",2023-08-23T11:39:06Z,1,0,https://github.com/tensorflow/tensorflow/issues/61674,
310,tensorflow/tensorflow, Test_on_batch() gives the same loss output on different batches in a single run,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes


### TensorFlow version

tf 2.10.0

### Custom code

Yes

### OS platform and distribution

windows 10

### Python version

3.10.12

### Current behavior?

I noticed the problem when I got a straight horizontal line on plotting the test results on the trained network. I used the sequential models.

I use train_on_batch(), which gives converging losses. When I switch to test_on_batch(), the losses remain the same for different batches. When I restart the test with different test files, it will give a different loss value, which remains the same for all the batches. In other words, the loss from test_on_batch() remains the some for all batches in a single run.

It's a sequential model.

Here is the code of the section:

    print('mfccs3 value = ', tf.keras.backend.eval(mfccs3[1,:]) )               
    #logs = vadModel.train_on_batch(mfccs3, vadLabel)
    logs = vadModel.test_on_batch(mfccs3, vadLabel) 
    print('string logs = ', str(logs))
The result is:
index = 1

```
mfccs3 value = [[-8.2793800e+01 -5.9538417e+00 9.8302096e-01 -3.5255635e-01
3.0392697e-01 -6.4597696e-01 2.2358397e-02 2.5344249e-02
-6.8171650e-01 -3.7053981e-01 -3.4044239e-01 -8.1056818e-02]]
```

string logs = 0.2398043

index = 2

```
mfccs3 value = [[-69.159195 -2.2269542 4.2501264 -1.3486748 0.62957734
-3.2606528 -3.253118 -3.5308673 -1.1313365 -1.1839466
-2.330786 -1.6313086 ]]
```

string logs = 0.2398043

index = 3

```
mfccs3 value = [[-64.894104 -1.892648 0.11392474 -0.81098145 -1.4640433
-1.1901256 -1.7744782 -0.85753983 -0.9694403 -0.8149232
-1.0680746 -1.0442001 ]]
```

string logs = 0.2398043

You can see that the inputs for test_on_batch() have changed. However, the loss remains the same. I use the same code for train_on_batch(), which gives converging losses.



### Standalone code to reproduce the issue

```shell
logs = vadModel.train_on_batch(mfccs3, vadLabel)
"""""" vs.  """"""
    logs = vadModel.test_on_batch(mfccs3, vadLabel) 

it's just these two lines for a sequential model.
```


### Relevant log output

```shell
I even tried latest Tensorflow version. It has the same problem.

tensorflow version: 2.15.0-dev20230817
listOfFiles 1681
2023-08-17 15:42:12.560549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
model length =  7
Model: ""sequential""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 multiple                  2640      
                                                                 
 dense (Dense)               multiple                  210       
                                                                 
 dense_1 (Dense)             multiple                  11        
                                                                 
=================================================================
Total params: 2861 (11.18 KB)
Trainable params: 2861 (11.18 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF2.14']",2023-08-17T23:10:29Z,18,0,https://github.com/tensorflow/tensorflow/issues/61616,
311,tensorflow/tensorflow,"Build: Protobuf fails with ""File already exists in database""","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.1.1

### GCC/compiler version

11.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Using the TF_SYSTEMLIBS version of protobuf (i.e. a preinstalled protobuf) when building TensorFlow from source results in

```
[libprotobuf ERROR /build/protobuf/3.19.4/GCCcore-11.3.0/protobuf-3.19.4/src/google/protobuf/descriptor_database.cc:641] File already exists in database: tensorflow/dtensor/proto/layout.proto
[libprotobuf FATAL /build/protobuf/3.19.4/GCCcore-11.3.0/protobuf-3.19.4/src/google/protobuf/descriptor.cc:2021] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size): 
```

I can't make much sense out of that failure and Google yielded results related to having the protobuf file in loaded in shared libraries multiple times.

All I could do is indeed trace it to `from tensorflow.dtensor.proto import layout_pb2` in tensorflow/dtensor/python/layout.py

### Standalone code to reproduce the issue

```shell
The failing build step invokes `/bin/bash -c 'bazel-out/ppc-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/ppc-opt/bin/tensorflow/_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=default bazel-out/ppc-opt/bin/tensorflow/tf_python_api_gen_v2.params'`
```


### Relevant log output

```shell
# Configuration: 020ca55738349851eb8a5a672fc7b8d08dc15ebd99b324f52fddbad2f32820b8
# Execution platform: @local_execution_config_platform//:platform
ERROR: /build/TensorFlow/tensorflow-2.13.0/tensorflow/BUILD:1646:19: Action tensorflow/_api/v2/v2.py failed: (Aborted): bash failed: error executing command 
  (cd /build/TensorFlow/bazel-root/663b1bf019e1a9ec9827eae691fce071/execroot/org_tensorflow && \
  exec env - \
    CPATH=/sw/installed/cURL/7.83.0-GCCcore-11.3.0/include:/software/double-conversion/3.2.0-GCCcore-11.3.0/include:/software/flatbuffers/2.0.7-GCCcore-11.3.0/include:/sw/installed/giflib/5.2.1-GCCcore-11.3.0/include:/sw/installed/hwloc/2.7.1-GCCcore-11.3.0/include:/sw/installed/ICU/71.1-GCCcore-11.3.0/include:/software/JsonCpp/1.9.5-GCCcore-11.3.0/include:/sw/installed/libjpeg-turbo/2.1.3-GCCcore-11.3.0/include:/sw/installed/libpng/1.6.37-GCCcore-11.3.0/include:/software/nsync/1.25.0-GCCcore-11.3.0/include:/software/protobuf/3.19.4-GCCcore-11.3.0/include:/sw/installed/pybind11/2.9.2-GCCcore-11.3.0/include:/software/snappy/1.1.9-GCCcore-11.3.0/include:/sw/installed/SQLite/3.38.3-GCCcore-11.3.0/include:/sw/installed/zlib/1.2.12-GCCcore-11.3.0/include:/sw/installed/OpenSSL/1.1/include \
    LD_LIBRARY_PATH=/software/RE2/2022-06-01-GCCcore-11.3.0/lib:/software/snappy/1.1.9-GCCcore-11.3.0/lib:/sw/installed/libpng/1.6.37-GCCcore-11.3.0/lib:/software/nsync/1.25.0-GCCcore-11.3.0/lib:/sw/installed/libjpeg-turbo/2.1.3-GCCcore-11.3.0/lib:/software/JsonCpp/1.9.5-GCCcore-11.3.0/lib:/sw/installed/ICU/71.1-GCCcore-11.3.0/lib:/sw/installed/giflib/5.2.1-GCCcore-11.3.0/lib:/software/flatbuffers/2.0.7-GCCcore-11.3.0/lib:/software/double-conversion/3.2.0-GCCcore-11.3.0/lib:/sw/installed/HDF5/1.12.2-gompi-2022a/lib:/sw/installed/Szip/2.1.1-GCCcore-11.3.0/lib:/sw/installed/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages/numpy/core/lib:/sw/installed/ScaLAPACK/2.2.0-gompi-2022a-fb/lib:/sw/installed/FFTW.MPI/3.3.10-gompi-2022a/lib:/sw/installed/FFTW/3.3.10-GCC-11.3.0/lib:/sw/installed/FlexiBLAS/3.2.0-GCC-11.3.0/lib:/sw/installed/OpenBLAS/0.3.20-GCC-11.3.0/lib:/sw/installed/OpenMPI/4.1.4-GCC-11.3.0/lib:/sw/installed/UCC/1.0.0-GCCcore-11.3.0/lib:/sw/installed/PMIx/4.1.2-GCCcore-11.3.0/lib:/sw/installed/libfabric/1.15.1-GCCcore-11.3.0/lib:/sw/installed/UCX/1.12.1-GCCcore-11.3.0/lib:/sw/installed/libevent/2.1.12-GCCcore-11.3.0/lib:/sw/installed/hwloc/2.7.1-GCCcore-11.3.0/lib:/sw/installed/libpciaccess/0.16-GCCcore-11.3.0/lib:/sw/installed/numactl/2.0.14-GCCcore-11.3.0/lib:/sw/installed/Python/3.10.4-GCCcore-11.3.0/lib:/sw/installed/libffi/3.4.2-GCCcore-11.3.0/lib64:/sw/installed/GMP/6.2.1-GCCcore-11.3.0/lib:/sw/installed/SQLite/3.38.3-GCCcore-11.3.0/lib:/sw/installed/Tcl/8.6.12-GCCcore-11.3.0/lib:/sw/installed/bzip2/1.0.8-GCCcore-11.3.0/lib:/sw/installed/binutils/2.38-GCCcore-11.3.0/lib:/sw/installed/DB/18.1.40-GCCcore-11.3.0/lib:/sw/installed/libreadline/8.1.2-GCCcore-11.3.0/lib:/sw/installed/gettext/0.21-GCCcore-11.3.0/lib:/sw/installed/ncurses/6.3-GCCcore-11.3.0/lib:/sw/installed/libxml2/2.9.13-GCCcore-11.3.0/lib:/sw/installed/XZ/5.2.5-GCCcore-11.3.0/lib:/sw/installed/expat/2.4.8-GCCcore-11.3.0/lib:/sw/installed/cURL/7.83.0-GCCcore-11.3.0/lib:/sw/installed/OpenSSL/1.1/lib:/sw/installed/zlib/1.2.12-GCCcore-11.3.0/lib:/software/protobuf/3.19.4-GCCcore-11.3.0/lib:/sw/installed/Java/11.0.6-ppc64le/lib:/sw/installed/GCCcore/11.3.0/lib64:/usr/local/cuda/lib64 \
    LIBRARY_PATH=/sw/installed/cURL/7.83.0-GCCcore-11.3.0/lib:/software/double-conversion/3.2.0-GCCcore-11.3.0/lib:/software/flatbuffers/2.0.7-GCCcore-11.3.0/lib:/sw/installed/giflib/5.2.1-GCCcore-11.3.0/lib:/sw/installed/hwloc/2.7.1-GCCcore-11.3.0/lib:/sw/installed/ICU/71.1-GCCcore-11.3.0/lib:/software/JsonCpp/1.9.5-GCCcore-11.3.0/lib:/sw/installed/libjpeg-turbo/2.1.3-GCCcore-11.3.0/lib:/sw/installed/libpng/1.6.37-GCCcore-11.3.0/lib:/software/nsync/1.25.0-GCCcore-11.3.0/lib:/software/protobuf/3.19.4-GCCcore-11.3.0/lib:/sw/installed/pybind11/2.9.2-GCCcore-11.3.0/lib:/software/snappy/1.1.9-GCCcore-11.3.0/lib:/sw/installed/SQLite/3.38.3-GCCcore-11.3.0/lib:/sw/installed/zlib/1.2.12-GCCcore-11.3.0/lib:/sw/installed/OpenSSL/1.1/lib \
    PATH=/sw/installed/libpng/1.6.37-GCCcore-11.3.0/bin:/sw/installed/libjpeg-turbo/2.1.3-GCCcore-11.3.0/bin:/sw/installed/NASM/2.15.05-GCCcore-11.3.0/bin:/sw/installed/ICU/71.1-GCCcore-11.3.0/sbin:/sw/installed/ICU/71.1-GCCcore-11.3.0/bin:/sw/installed/giflib/5.2.1-GCCcore-11.3.0/bin:/software/flatbuffers/2.0.7-GCCcore-11.3.0/bin:/software/dill/0.3.6-GCCcore-11.3.0/bin:/sw/installed/HDF5/1.12.2-gompi-2022a/bin:/sw/installed/SciPy-bundle/2022.05-foss-2022a/bin:/sw/installed/FFTW/3.3.10-GCC-11.3.0/bin:/sw/installed/FlexiBLAS/3.2.0-GCC-11.3.0/bin:/sw/installed/OpenMPI/4.1.4-GCC-11.3.0/bin:/sw/installed/UCC/1.0.0-GCCcore-11.3.0/bin:/sw/installed/PMIx/4.1.2-GCCcore-11.3.0/bin:/sw/installed/libfabric/1.15.1-GCCcore-11.3.0/bin:/sw/installed/UCX/1.12.1-GCCcore-11.3.0/bin:/sw/installed/libevent/2.1.12-GCCcore-11.3.0/bin:/sw/installed/hwloc/2.7.1-GCCcore-11.3.0/bin:/sw/installed/numactl/2.0.14-GCCcore-11.3.0/bin:/sw/installed/UnZip/6.0-GCCcore-11.3.0/bin:/sw/installed/pybind11/2.9.2-GCCcore-11.3.0/bin:/sw/installed/Python/3.10.4-GCCcore-11.3.0/bin:/sw/installed/SQLite/3.38.3-GCCcore-11.3.0/bin:/sw/installed/Tcl/8.6.12-GCCcore-11.3.0/bin:/sw/installed/bzip2/1.0.8-GCCcore-11.3.0/bin:/sw/installed/binutils/2.38-GCCcore-11.3.0/bin:/sw/installed/git/2.36.0-GCCcore-11.3.0-nodocs/bin:/sw/installed/Perl/5.34.1-GCCcore-11.3.0/bin:/sw/installed/DB/18.1.40-GCCcore-11.3.0/bin:/sw/installed/gettext/0.21-GCCcore-11.3.0/bin:/sw/installed/ncurses/6.3-GCCcore-11.3.0/bin:/sw/installed/libxml2/2.9.13-GCCcore-11.3.0/bin:/sw/installed/XZ/5.2.5-GCCcore-11.3.0/bin:/sw/installed/expat/2.4.8-GCCcore-11.3.0/bin:/sw/installed/cURL/7.83.0-GCCcore-11.3.0/bin:/sw/installed/OpenSSL/1.1/bin:/software/protobuf/3.19.4-GCCcore-11.3.0/bin:/software/Bazel/5.1.1-GCCcore-11.3.0/bin:/sw/installed/Java/11.0.6-ppc64le:/sw/installed/Java/11.0.6-ppc64le/bin:/sw/installed/GCCcore/11.3.0/bin:/home/s3248973/.local/EasyBuildDev/easybuild-framework:/home/s3248973/.yarn/bin:/home/s3248973/.config/yarn/global/node_modules/.bin:/home/s3248973/.local/bin:/usr/local/cuda/bin:/usr/lib64/qt-3.3/bin:/sw/taurus/tools/slurmtools/default/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/opt/ibutils/bin:/opt/puppetlabs/bin \
    PYTHONNOUSERSITE=1 \
    PYTHONPATH=/software/TensorFlow/2.13.0-foss-2022a/lib/python3.10/site-packages:/software/TensorFlow/2.13.0-foss-2022a/lib/python3.10/site-packages:/software/protobuf-python/3.19.4-GCCcore-11.3.0/lib/python3.10/site-packages:/software/flatbuffers/2.0.7-GCCcore-11.3.0/lib/python3.10/site-packages:/software/dill/0.3.6-GCCcore-11.3.0/lib/python3.10/site-packages:/software/h5py/3.7.0-foss-2022a/lib/python3.10/site-packages:/sw/installed/SciPy-bundle/2022.05-foss-2022a/lib/python3.10/site-packages:/sw/installed/pybind11/2.9.2-GCCcore-11.3.0/lib/python3.10/site-packages:/sw/installed/Python/3.10.4-GCCcore-11.3.0/easybuild/python \
    PYTHON_BIN_PATH=/sw/installed/Python/3.10.4-GCCcore-11.3.0/bin/python \
    PYTHON_LIB_PATH=/software/TensorFlow/2.13.0-foss-2022a/lib/python3.10/site-packages \
    TF2_BEHAVIOR=1 \
    TF_SYSTEM_LIBS=absl_py,astor_archive,astunparse_archive,boringssl,com_google_protobuf,curl,cython,dill_archive,double_conversion,flatbuffers,functools32_archive,gast_archive,gif,hwloc,icu,jsoncpp_git,libjpeg_turbo,nasm,nsync,opt_einsum_archive,org_sqlite,pasta,png,pybind11,six_archive,snappy,tblib_archive,termcolor_archive,typing_extensions_archive,wrapt,zlib \
  /bin/bash -c 'bazel-out/ppc-opt/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/ppc-opt/bin/tensorflow/_api/v2/ --apiname=tensorflow --apiversion=2  --compat_apiversion=1 --compat_apiversion=2  --compat_init_template=tensorflow/compat_template_v1.__init__.py --compat_init_template=tensorflow/compat_template.__init__.py --packages=tensorflow.python,tensorflow.dtensor.python.accelerator_util,tensorflow.dtensor.python.api,tensorflow.dtensor.python.config,tensorflow.dtensor.python.d_checkpoint,tensorflow.dtensor.python.d_variable,tensorflow.dtensor.python.input_util,tensorflow.dtensor.python.layout,tensorflow.dtensor.python.mesh_util,tensorflow.dtensor.python.tpu_util,tensorflow.dtensor.python.save_restore,tensorflow.lite.python.analyzer,tensorflow.lite.python.lite,tensorflow.lite.python.authoring.authoring,tensorflow.python.modules_with_exports --output_package=tensorflow._api.v2 --use_relative_imports=True --loading=default bazel-out/ppc-opt/bin/tensorflow/tf_python_api_gen_v2.params')
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'TF 2.13']",2023-08-16T14:24:31Z,3,1,https://github.com/tensorflow/tensorflow/issues/61593,
312,tensorflow/tensorflow,Segmentation fault when running tensorflow.python.framework.kernels.get_registered_kernels_for_op,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.11.0

### Custom code

Yes

### OS platform and distribution

22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

nvidia-cudnn-cu11==8.6.0.163, cudatoolkit=11.8.0

### GPU model and memory

_No response_

### Current behavior?

Probably due to feeding None argument

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import os
import numpy as np
from tensorflow.python.framework import kernels
try:
  arg_0 = None
  out = kernels.get_registered_kernels_for_op(arg_0,)
except Exception as e:
  print(""Error:""+str(e))

```
```


### Relevant log output

```shell
2023-08-12 13:41:10.388491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
Segmentation fault

```
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-08-12T17:41:26Z,2,0,https://github.com/tensorflow/tensorflow/issues/61531,
313,tensorflow/tensorflow,quantized range of fake_quant_with_min_max_args is -2**num_bits + 1 to 2 ** num_bits.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13

### Custom code

Yes

### OS platform and distribution

ubuntu 22.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I implemented `fake_quant_with_min_max_args` in numpy according to the source code to get the quantized values. However the quantized range is `-2**num_bits + 1` to `2 ** num_bits` and the quantized weights cannot be represented by `num_bits` int.  What should I do if I have to use signed type to represent the weights?

### Standalone code to reproduce the issue

```shell
def fake_quant_with_min_max_args(inputs, min=-0.99, max=0.99, num_bits=8, narrow_range=False):
    assert min < 0 < max
    quant_min = 1 if narrow_range else 0
    quant_max = (1 << num_bits) - 1
    quant_min_float = np.float32(quant_min)
    quant_max_float = np.float32(quant_max)
    scale = np.float32(max - min) / (quant_max_float - quant_min_float)
    inv_scale = (quant_max_float - quant_min_float) / np.float32(max - min)

    zero_point_from_min = quant_min - min / scale
    if zero_point_from_min < quant_min:
        nudged_zero_point = quant_min
    elif zero_point_from_min > quant_max:
        nudged_zero_point = quant_max
    else:
        nudged_zero_point = np.round(zero_point_from_min)

    nudged_min = (quant_min_float - nudged_zero_point) * scale
    nudged_max = (quant_max_float - nudged_zero_point) * scale

    quant_zero = np.floor(-nudged_min * inv_scale + 0.5)
    # print(quant_min, quant_max, nudged_zero_point)
    # print(nudged_min, nudged_max, scale, inv_scale, quant_zero)
    clamped = np.clip(inputs, nudged_min, nudged_max)
    clamp_shifted = clamped - nudged_min
    # quant = np.clip(np.floor(clamp_shifted * inv_scale - quant_zero + 0.5),
    #                 quant_min - 2 ** (num_bits - 1), quant_max - 2 ** (num_bits - 1))
    quant = np.floor(clamp_shifted * inv_scale - quant_zero + 0.5)
    dequant = quant * scale
    return quant, dequant


if __name__ == '__main__':
    import numpy as np
    import tensorflow as tf
    np.random.seed(12345)
    data = np.random.uniform(-1, 1, (2,))
    data = np.r_[1, -1, data]

    d = fake_quant_with_min_max_args(data, -1, 1, 8, False)
    print(d[0])
    print(d[1])
    print(tf.quantization.fake_quant_with_min_max_args(data, -1, 1, 8, False))
```
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-08-12T09:04:02Z,2,0,https://github.com/tensorflow/tensorflow/issues/61529,
314,tensorflow/tensorflow,Incorrect tf.nn.max_pool2d outputs for NCHW and NHWC in the same thread.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13

### Custom code

No

### OS platform and distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

There is an incorrect output of tf.nn.max_pool2d when calling with the input of the same batch size, height, width and depth first in NCHW, then in NHWC format with MKL support. \
Output of tf.nn.max_pool2d on the same input (except data format) must be the same (except data format).

### Standalone code to reproduce the issue

Colab to reproduce https://colab.research.google.com/drive/1Vuo7txDPDq-YXAuCFQcPNowHgOwbT7Oi?usp=sharing



### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:mkl', 'TF 2.13']",2023-08-04T22:24:18Z,4,0,https://github.com/tensorflow/tensorflow/issues/61482,
315,tensorflow/tensorflow,Tensorflow 2.13.0 cannot be imported after install via poetry due to missing wheel metadata,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

macOS-13.3-arm64-arm-64bit

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

After installing tensorflow  2.13.0 with poetry it cannot be imported

### Standalone code to reproduce the issue
See https://github.com/python-poetry/poetry/issues/8271

According to @dimbleby, the issue happens due to missing wheel metadata.

> please encourage the tensorflow folk to publish consistent metadata in all of their wheels, with platform-specific variations described by markers

Tensorflow folks, please publish consistent metadata ;)

```shell
poetry add tensorflow==2.13.0
poetry shell
python3 
import tensorflow
```


### Relevant log output

```shell
ModuleNotFoundError: No module named 'tensorflow
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype:macOS', 'TF 2.13']",2023-08-04T15:23:34Z,6,8,https://github.com/tensorflow/tensorflow/issues/61477,
316,tensorflow/tensorflow,How/Where decision is made to choose between eager execution & graph execution for Tensorflow kernel OPs,"I ran HuggingFace BERT model which uses tensorflow 2.13v with oneDNN support on intel machine and recorded its execution logs by setting TF_CPP_MAX_VLOG_LEVEL=2 & ONEDNN_VERBOSE=1 in file.

**Observation :** I have observing logs that are produced after model creation and its weight loading. Since model.fit() always run in graph model, all tensorflow kernel OPs (onednn's mkl kernel op and non-mkl kernel ops) should run in graph mode. But i observe only for non-mkl kernel ops (like ADDV2, Mul) are executing in eager mode followed by graph mode. I dont see any mkl kernel ops(like _MklMatMul) running in eager mode.

**Questions:** I want to know the reason and file where decision making is made for which op there should be eager mode. Since model.fit() runs in graph mode, why I am seeing eager mode execution for all non-mkl ops?

Sample Logs for model.fit() for ADDV2 kernel op:

```
2023-07-31 03:48:44.632289: I tensorflow/core/common_runtime/eager/execute.cc:1678] Executing op AddV2 in device /job:localhost/replica:0/task:0/device:CPU:0 --> executing addv2 eagerly After some other logs in between, I see below log:

2023-07-31 03:50:01.968512: I tensorflow/core/common_runtime/executor.cc:841] Process node: 8127 step -4458402160563696089 {{node tf_bert_for_sequence_classification/bert/encoder/layer_._0/output/LayerNorm/batchnorm/add_1}} = AddV2[T=DT_FLOAT, _XlaHasReferenceVars=false, device=""/job:localhost/replica:0/task:0/device:CPU:0""](tf_bert_for_sequence_classification/bert/encoder/layer.0/output/LayerNorm/batchnorm/mul_1, tf_bert_for_sequence_classification/bert/encoder/layer._0/output/LayerNorm/batchnorm/sub) device: 
```
/job:localhost/replica:0/task:0/device:CPU:0 --> executing addv2 in graph mode i assume

**Expected to happen:** All kerenl ops should execute in graph mode.
","['stat:awaiting tensorflower', 'type:bug', 'comp:core', 'TF 2.13']",2023-08-04T12:25:08Z,6,0,https://github.com/tensorflow/tensorflow/issues/61476,
317,tensorflow/tensorflow,BUG: reference count leak in function `RegisterForwardAccumulatorCleanup` (static analyzer report),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

commit faad219fc46032a0ae9576ccc3076612cc1f5f72

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

Our static analyzer uses Clang 13 as its parser

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

https://github.com/tensorflow/tensorflow/blob/53fb0130851ad40d544105432f414b4ebe9e729d/tensorflow/python/eager/pywrap_tfe_src.cc#L2378-L2379

API `PyCFunction_New` does not steal a reference for the second argument.
API `PyLong_FromLong` will return a new reference.
Calling `PyLong_FromLong` directly as the second argument of `PyCFunction_New` will lead to a reference count leak for the PyObject returned by `PyLong_FromLong`.

Internal report ID: c13984

### Standalone code to reproduce the issue

```shell
Unnecessary. Whenever this function is called, the problem will be triggered.
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:eager']",2023-08-03T08:05:54Z,0,0,https://github.com/tensorflow/tensorflow/issues/61462,
318,tensorflow/tensorflow,tf.compat.v1.train.MonitoredTrainingSession failed to restore checkpoint_dir variables from s3,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf 2.7.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.7.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.2

### GPU model and memory

_No response_

### Current behavior?

[TOC]

Our project uses tf.compat.v1.train.MonitoredTrainingSession to create a training session. Typically, we need to restore a pre-trained model from S3.

## 1. Error encountered in my project
Before switching to TensorFlow 1, we used TensorFlow 1.15.1 and passed the S3 path to `checkpoint_dir` like this:
```python
import tensorflow as tf
.....
checkpoint_dir = ""s3://xxx/xx/""
tf.compat.v1.train.MonitoredTrainingSession(...., checkpoint_dir=checkpoint_dir, ...)
```
`checkpoint_dir` contains everything needed to restore variables, including checkpoint, graph.pbtxt, etc. Everything works fine.

After switching to TensorFlow 2.7.0, we realized that the Modular File System has been introduced into TensorFlow. So, we installed TensorFlow-io version 0.23.0, which is compatible with TensorFlow 2.7.0. The code becomes:
```python
import tensorflow as tf
import tensorflow_io as tfio
.....
checkpoint_dir = ""s3://xxx/xx/""
tf.compat.v1.train.MonitoredTrainingSession(...., checkpoint_dir=checkpoint_dir, ...)
```
However, it no longer works, and an error is reported:
```
.....
2023-08-02 16:02:40.147093: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:207 : DATA_LOSS: truncated block read
Traceback (most recent call last):
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1380, in _do_call
    return fn(*args)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1364, in _run_fn
    target_list, run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1458, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.
  (0) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
         [[save/RestoreV2/_1]]
  (1) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
0 successful operations.
0 derived errors ignored.
.....
```

## 2. Reproduce the issue using simple code
To rule out the possibility that the issue is caused by the complexity of the model in my project, I reproduced it using a very simple code.
### 2.1 Step 1: Train the model
First, I used the following code to train a very simple model and save it in a local directory:
```python
import tensorflow as tf

tf.compat.v1.disable_eager_execution()
x = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""x"")
y = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""y"")

W = tf.Variable(tf.zeros([1, 1]), name=""W"")
b = tf.Variable(tf.zeros([1]), name=""b"")

y_pred = tf.matmul(x, W) + b
loss = tf.reduce_mean(tf.square(y - y_pred))

optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01)

global_step = tf.compat.v1.train.get_or_create_global_step()
train_op = optimizer.minimize(loss, global_step=global_step)

x_train = [[1], [2], [3], [4]]
y_train = [[0], [-1], [-2], [-3]]

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
hooks = [tf.compat.v1.train.StopAtStepHook(last_step=500)]

checkpoint_dir = './checkpoints'

with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,
                                                 config=config,
                                                 hooks=hooks) as sess:
    while not sess.should_stop():
        sess.run(train_op, feed_dict={x: x_train, y: y_train})
```
### 2.2 Step 2: Upload the model to S3
Then, I used S3 tools to upload all materials in `./checkpoints` to a remote S3 path:
```
s3cmd put ./checkpoints/ s3://xxxx/xxx/checkpoints/
```
### 2.3 Step 3: Restore the model from S3 (error)
Finally, I restored the model training using the following code, and an error was reported:
```python
import tensorflow as tf
import tensorflow_io as tfio

tf.compat.v1.disable_eager_execution()

x = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""x"")
y = tf.compat.v1.placeholder(tf.float32, shape=(None, 1), name=""y"")

W = tf.Variable(tf.zeros([1, 1]), name=""W"")
b = tf.Variable(tf.zeros([1]), name=""b"")

y_pred = tf.matmul(x, W) + b
loss = tf.reduce_mean(tf.square(y - y_pred))

optimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01)

global_step = tf.compat.v1.train.get_or_create_global_step()

train_op = optimizer.minimize(loss, global_step=global_step)

x_train = [[1], [2], [3], [4]]
y_train = [[0], [-1], [-2], [-3]]

config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True

checkpoint_dir = 's3://xxxx/xxx/checkpoints/'

hooks = [tf.compat.v1.train.StopAtStepHook(last_step=2000)]
with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
    while not sess.should_stop():
        sess.run(train_op, feed_dict={x: x_train, y: y_train})
```
The full log is shown below:
```

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:401: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.
2023-08-02 16:43:08.483327: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-02 16:43:09.090602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38415 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:0e:00.0, compute capability: 8.0
2023-08-02 16:43:09.854875: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at save_restore_v2_ops.cc:207 : DATA_LOSS: truncated block read
Traceback (most recent call last):
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1380, in _do_call
    return fn(*args)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1364, in _run_fn
    target_list, run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1458, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.
  (0) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
         [[save/RestoreV2/_1]]
  (1) DATA_LOSS: truncated block read
         [[{{node save/RestoreV2}}]]
0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_s3.py"", line 36, in <module>
    with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 616, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1062, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 761, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1267, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1272, in _create_session
    return self._sess_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 914, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 681, in create_session
    init_fn=self._scaffold.init_fn)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py"", line 321, in prepare_session
    config=config)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py"", line 251, in _restore_checkpoint
    sess, saver, ckpt.model_checkpoint_path)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers
    saver.restore(sess, path)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 1405, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 971, in run
    run_metadata_ptr)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1194, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1374, in _do_run
    run_metadata)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py"", line 1399, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.DataLossError: 2 root error(s) found.
  (0) DATA_LOSS: truncated block read
         [[node save/RestoreV2
 (defined at train_s3.py:36)
]]
         [[save/RestoreV2/_1]]
  (1) DATA_LOSS: truncated block read
         [[node save/RestoreV2
 (defined at train_s3.py:36)
]]
0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node save/RestoreV2:
In[0] save/Const:
In[1] save/RestoreV2/tensor_names:
In[2] save/RestoreV2/shape_and_slices:

Operation defined at: (most recent call last)
>>>   File ""train_s3.py"", line 36, in <module>
>>>     with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
>>> 

Input Source operations connected to node save/RestoreV2:
In[0] save/Const:
In[1] save/RestoreV2/tensor_names:
In[2] save/RestoreV2/shape_and_slices:

Operation defined at: (most recent call last)
>>>   File ""train_s3.py"", line 36, in <module>
>>>     with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
>>> 

Original stack trace for 'save/RestoreV2':
  File ""train_s3.py"", line 36, in <module>
    with tf.compat.v1.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as sess:
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 616, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1062, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 761, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1267, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1272, in _create_session
    return self._sess_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 914, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 672, in create_session
    self._scaffold.finalize()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize
    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 625, in _get_saver_or_default
    saver = Saver(sharded=True, allow_empty=True)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 923, in __init__
    self.build()
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 935, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 973, in _build
    build_restore=build_restore)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 528, in _build_internal
    restore_sequentially, reshape)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 407, in _AddShardedRestoreOps
    name=""restore_shard""))
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 354, in _AddRestoreOps
    restore_sequentially)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py"", line 601, in bulk_restore
    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1504, in restore_v2
    name=name)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 746, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3705, in _create_op_internal
    op_def=op_def)
  File ""/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 2101, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)

```

## 3. Test tf.io and s3 connectivity
I also use the following code to test if tf.io can access s3
```
import tensorflow as tf
import tensorflow_io as tfio
s3_path = ""s3://xxxxx/xxx/checkpoints/checkpoint""
ret = tf.io.read_file(s3_path)
print(ret)
```
And it works fine:
```
2023-08-02 16:48:17.619754: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-08-02 16:48:18.226059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38415 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:0e:00.0, compute capability: 8.0
tf.Tensor(b'model_checkpoint_path: ""model.ckpt-1000""\nall_model_checkpoint_paths: ""model.ckpt-0""\nall_model_checkpoint_paths: ""model.ckpt-500""\nall_model_checkpoint_paths: ""model.ckpt-1000""\n', shape=(), dtype=string)
```



### Standalone code to reproduce the issue

```shell
see above
```


### Relevant log output

```shell
see above
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:core', 'TF 2.13']",2023-08-02T09:01:20Z,9,0,https://github.com/tensorflow/tensorflow/issues/61449,
319,tensorflow/tensorflow,Unexpected differences in outputs of Conv2D copy with exact subset of weights,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

MacOS 13.4.1 (c) (22F770820d)

### Mobile device

_No response_

### Python version

3.10.6

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Given a `Conv2D` layer that has some channel's weights completely as zeros (e.g. as a result of structured pruning), when creating a new layer from the previous one with those channels missing, and calling both layers on some input, while the relevant parts of weights are exactly the same, that's not the case for the layers' outputs.

I would expect them to be exactly the same as well, but get differences up to `~1.7e-7`.


Note, I've also observed this behavior for Dense layers.

My best guess is that a different mechanism is used under the hood that produces slightly different results?

### Standalone code to reproduce the issue

```shell
from itertools import combinations

import numpy as np
import tensorflow as tf


def reduce_to_relevant_part(array: np.ndarray, channel_indices: list[int], channel_axis: int = -1):
    if channel_indices is not None:
        array = np.take(
            array,
            [i for i in range(array.shape[channel_axis]) if i not in channel_indices],
            axis=channel_axis,
        )
    return array


def get_all_combinations(n: int) -> list[list[int]]:
    result = []
    for i in range(n):
        for subset in combinations(range(n), i):
            result.append(list(subset))
    return result


def print_summary_statistics(array: np.ndarray):
    print(""mean: {}, std: {}, max: {}"".format(np.mean(array), np.std(array), np.max(array)))


def get_layer(n_filters, kernel_size, input_shape):
    layer = tf.keras.layers.Conv2D(
        filters=n_filters,
        kernel_size=kernel_size,
        padding='same',
        use_bias=False,
        kernel_initializer='he_normal',
        strides=kernel_size,
    )
    layer.build(input_shape)
    return layer


def set_channel_weights_to_zero(layer, channel_indices):
    weights = layer.get_weights()
    weights[0][..., channel_indices] = 0
    layer.set_weights(weights)
    return layer


def get_copy_of_layer_without_zero_channels(layer, channel_indices):
    config = layer.get_config()
    config[""filters""] -= len(channel_indices)
    weights = layer.get_weights()
    weights[0] = reduce_to_relevant_part(weights[0], channel_indices)
    config[""weights""] = weights
    new_layer = tf.keras.layers.Conv2D.from_config(config)
    return new_layer


def main():
    for n_filters in range(2, 9):
        kernel_size = (4, 4)
        input_shape = (1, 8, 12, 1)
        for channel_indices in get_all_combinations(n_filters):
            layer = get_layer(n_filters, kernel_size, input_shape)
            layer = set_channel_weights_to_zero(layer, channel_indices)
            new_layer = get_copy_of_layer_without_zero_channels(layer, channel_indices)

            x = tf.random.uniform(input_shape)
            output = layer(x).numpy()
            output_subset = reduce_to_relevant_part(output, channel_indices)
            new_output = new_layer(x).numpy()
            weights_subset = reduce_to_relevant_part(layer.get_weights()[0], channel_indices)
            new_weights = new_layer.get_weights()[0]
            if not np.array_equal(weights_subset, new_weights):
                raise ValueError()  # never triggered
            if not np.array_equal(output_subset, new_output):
                print(
                    ""Outputs differ for channel indices {} @ {} filters"".format(
                        channel_indices, n_filters
                    )
                )


if __name__ == '__main__':
    main()


Find below the output of the above script. Note that for 3 filters for example, `[2]` and `[0, 1]` do not show up, because in those cases the outputs actually are exactly the same.
```


### Relevant log output

```shell
Outputs differ for channel indices [0] @ 2 filters
Outputs differ for channel indices [1] @ 2 filters
Outputs differ for channel indices [0] @ 3 filters
Outputs differ for channel indices [1] @ 3 filters
Outputs differ for channel indices [0, 2] @ 3 filters
Outputs differ for channel indices [1, 2] @ 3 filters
Outputs differ for channel indices [0] @ 4 filters
Outputs differ for channel indices [1] @ 4 filters
Outputs differ for channel indices [2] @ 4 filters
Outputs differ for channel indices [3] @ 4 filters
Outputs differ for channel indices [0, 1, 2] @ 4 filters
Outputs differ for channel indices [0, 1, 3] @ 4 filters
Outputs differ for channel indices [0, 2, 3] @ 4 filters
Outputs differ for channel indices [1, 2, 3] @ 4 filters
Outputs differ for channel indices [0] @ 5 filters
Outputs differ for channel indices [1] @ 5 filters
Outputs differ for channel indices [2] @ 5 filters
Outputs differ for channel indices [3] @ 5 filters
Outputs differ for channel indices [0, 4] @ 5 filters
Outputs differ for channel indices [1, 4] @ 5 filters
Outputs differ for channel indices [2, 4] @ 5 filters
Outputs differ for channel indices [3, 4] @ 5 filters
Outputs differ for channel indices [0, 1, 2] @ 5 filters
Outputs differ for channel indices [0, 1, 3] @ 5 filters
Outputs differ for channel indices [0, 2, 3] @ 5 filters
Outputs differ for channel indices [1, 2, 3] @ 5 filters
Outputs differ for channel indices [0, 1, 2, 4] @ 5 filters
Outputs differ for channel indices [0, 1, 3, 4] @ 5 filters
Outputs differ for channel indices [0, 2, 3, 4] @ 5 filters
Outputs differ for channel indices [1, 2, 3, 4] @ 5 filters
Outputs differ for channel indices [0] @ 6 filters
Outputs differ for channel indices [1] @ 6 filters
Outputs differ for channel indices [2] @ 6 filters
Outputs differ for channel indices [3] @ 6 filters
Outputs differ for channel indices [4] @ 6 filters
Outputs differ for channel indices [5] @ 6 filters
Outputs differ for channel indices [0, 1, 2] @ 6 filters
Outputs differ for channel indices [0, 1, 3] @ 6 filters
Outputs differ for channel indices [0, 1, 4] @ 6 filters
Outputs differ for channel indices [0, 1, 5] @ 6 filters
Outputs differ for channel indices [0, 2, 3] @ 6 filters
Outputs differ for channel indices [0, 2, 4] @ 6 filters
Outputs differ for channel indices [0, 2, 5] @ 6 filters
Outputs differ for channel indices [0, 3, 4] @ 6 filters
Outputs differ for channel indices [0, 3, 5] @ 6 filters
Outputs differ for channel indices [0, 4, 5] @ 6 filters
Outputs differ for channel indices [1, 2, 3] @ 6 filters
Outputs differ for channel indices [1, 2, 4] @ 6 filters
Outputs differ for channel indices [1, 2, 5] @ 6 filters
Outputs differ for channel indices [1, 3, 4] @ 6 filters
Outputs differ for channel indices [1, 3, 5] @ 6 filters
Outputs differ for channel indices [1, 4, 5] @ 6 filters
Outputs differ for channel indices [2, 3, 4] @ 6 filters
Outputs differ for channel indices [2, 3, 5] @ 6 filters
Outputs differ for channel indices [2, 4, 5] @ 6 filters
Outputs differ for channel indices [3, 4, 5] @ 6 filters
Outputs differ for channel indices [0, 1, 2, 3, 4] @ 6 filters
Outputs differ for channel indices [0, 1, 2, 3, 5] @ 6 filters
Outputs differ for channel indices [0, 1, 2, 4, 5] @ 6 filters
Outputs differ for channel indices [0, 1, 3, 4, 5] @ 6 filters
Outputs differ for channel indices [0, 2, 3, 4, 5] @ 6 filters
Outputs differ for channel indices [1, 2, 3, 4, 5] @ 6 filters
Outputs differ for channel indices [0] @ 7 filters
Outputs differ for channel indices [1] @ 7 filters
Outputs differ for channel indices [2] @ 7 filters
Outputs differ for channel indices [3] @ 7 filters
Outputs differ for channel indices [4] @ 7 filters
Outputs differ for channel indices [5] @ 7 filters
Outputs differ for channel indices [0, 6] @ 7 filters
Outputs differ for channel indices [1, 6] @ 7 filters
Outputs differ for channel indices [2, 6] @ 7 filters
Outputs differ for channel indices [3, 6] @ 7 filters
Outputs differ for channel indices [4, 6] @ 7 filters
Outputs differ for channel indices [5, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 2] @ 7 filters
Outputs differ for channel indices [0, 1, 3] @ 7 filters
Outputs differ for channel indices [0, 1, 4] @ 7 filters
Outputs differ for channel indices [0, 1, 5] @ 7 filters
Outputs differ for channel indices [0, 2, 3] @ 7 filters
Outputs differ for channel indices [0, 2, 4] @ 7 filters
Outputs differ for channel indices [0, 2, 5] @ 7 filters
Outputs differ for channel indices [0, 3, 4] @ 7 filters
Outputs differ for channel indices [0, 3, 5] @ 7 filters
Outputs differ for channel indices [0, 4, 5] @ 7 filters
Outputs differ for channel indices [1, 2, 3] @ 7 filters
Outputs differ for channel indices [1, 2, 4] @ 7 filters
Outputs differ for channel indices [1, 2, 5] @ 7 filters
Outputs differ for channel indices [1, 3, 4] @ 7 filters
Outputs differ for channel indices [1, 3, 5] @ 7 filters
Outputs differ for channel indices [1, 4, 5] @ 7 filters
Outputs differ for channel indices [2, 3, 4] @ 7 filters
Outputs differ for channel indices [2, 3, 5] @ 7 filters
Outputs differ for channel indices [2, 4, 5] @ 7 filters
Outputs differ for channel indices [3, 4, 5] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 3, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 4, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 2, 3, 6] @ 7 filters
Outputs differ for channel indices [0, 2, 4, 6] @ 7 filters
Outputs differ for channel indices [0, 2, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 3, 4, 6] @ 7 filters
Outputs differ for channel indices [0, 3, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [1, 2, 3, 6] @ 7 filters
Outputs differ for channel indices [1, 2, 4, 6] @ 7 filters
Outputs differ for channel indices [1, 2, 5, 6] @ 7 filters
Outputs differ for channel indices [1, 3, 4, 6] @ 7 filters
Outputs differ for channel indices [1, 3, 5, 6] @ 7 filters
Outputs differ for channel indices [1, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [2, 3, 4, 6] @ 7 filters
Outputs differ for channel indices [2, 3, 5, 6] @ 7 filters
Outputs differ for channel indices [2, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [3, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 3, 4] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 3, 5] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 4, 5] @ 7 filters
Outputs differ for channel indices [0, 1, 3, 4, 5] @ 7 filters
Outputs differ for channel indices [0, 2, 3, 4, 5] @ 7 filters
Outputs differ for channel indices [1, 2, 3, 4, 5] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 3, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 2, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 1, 3, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [0, 2, 3, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [1, 2, 3, 4, 5, 6] @ 7 filters
Outputs differ for channel indices [0] @ 8 filters
Outputs differ for channel indices [1] @ 8 filters
Outputs differ for channel indices [2] @ 8 filters
Outputs differ for channel indices [3] @ 8 filters
Outputs differ for channel indices [4] @ 8 filters
Outputs differ for channel indices [5] @ 8 filters
Outputs differ for channel indices [6] @ 8 filters
Outputs differ for channel indices [7] @ 8 filters
Outputs differ for channel indices [0, 1] @ 8 filters
Outputs differ for channel indices [0, 2] @ 8 filters
Outputs differ for channel indices [0, 3] @ 8 filters
Outputs differ for channel indices [0, 4] @ 8 filters
Outputs differ for channel indices [0, 5] @ 8 filters
Outputs differ for channel indices [0, 6] @ 8 filters
Outputs differ for channel indices [0, 7] @ 8 filters
Outputs differ for channel indices [1, 2] @ 8 filters
Outputs differ for channel indices [1, 3] @ 8 filters
Outputs differ for channel indices [1, 4] @ 8 filters
Outputs differ for channel indices [1, 5] @ 8 filters
Outputs differ for channel indices [1, 6] @ 8 filters
Outputs differ for channel indices [1, 7] @ 8 filters
Outputs differ for channel indices [2, 3] @ 8 filters
Outputs differ for channel indices [2, 4] @ 8 filters
Outputs differ for channel indices [2, 5] @ 8 filters
Outputs differ for channel indices [2, 6] @ 8 filters
Outputs differ for channel indices [2, 7] @ 8 filters
Outputs differ for channel indices [3, 4] @ 8 filters
Outputs differ for channel indices [3, 5] @ 8 filters
Outputs differ for channel indices [3, 6] @ 8 filters
Outputs differ for channel indices [3, 7] @ 8 filters
Outputs differ for channel indices [4, 5] @ 8 filters
Outputs differ for channel indices [4, 6] @ 8 filters
Outputs differ for channel indices [4, 7] @ 8 filters
Outputs differ for channel indices [5, 6] @ 8 filters
Outputs differ for channel indices [5, 7] @ 8 filters
Outputs differ for channel indices [6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2] @ 8 filters
Outputs differ for channel indices [0, 1, 3] @ 8 filters
Outputs differ for channel indices [0, 1, 4] @ 8 filters
Outputs differ for channel indices [0, 1, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3] @ 8 filters
Outputs differ for channel indices [0, 2, 4] @ 8 filters
Outputs differ for channel indices [0, 2, 5] @ 8 filters
Outputs differ for channel indices [0, 2, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 4] @ 8 filters
Outputs differ for channel indices [0, 3, 5] @ 8 filters
Outputs differ for channel indices [0, 3, 6] @ 8 filters
Outputs differ for channel indices [0, 3, 7] @ 8 filters
Outputs differ for channel indices [0, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3] @ 8 filters
Outputs differ for channel indices [1, 2, 4] @ 8 filters
Outputs differ for channel indices [1, 2, 5] @ 8 filters
Outputs differ for channel indices [1, 2, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 4] @ 8 filters
Outputs differ for channel indices [1, 3, 5] @ 8 filters
Outputs differ for channel indices [1, 3, 6] @ 8 filters
Outputs differ for channel indices [1, 3, 7] @ 8 filters
Outputs differ for channel indices [1, 4, 5] @ 8 filters
Outputs differ for channel indices [1, 4, 6] @ 8 filters
Outputs differ for channel indices [1, 4, 7] @ 8 filters
Outputs differ for channel indices [1, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 4] @ 8 filters
Outputs differ for channel indices [2, 3, 5] @ 8 filters
Outputs differ for channel indices [2, 3, 6] @ 8 filters
Outputs differ for channel indices [2, 3, 7] @ 8 filters
Outputs differ for channel indices [2, 4, 5] @ 8 filters
Outputs differ for channel indices [2, 4, 6] @ 8 filters
Outputs differ for channel indices [2, 4, 7] @ 8 filters
Outputs differ for channel indices [2, 5, 6] @ 8 filters
Outputs differ for channel indices [2, 5, 7] @ 8 filters
Outputs differ for channel indices [2, 6, 7] @ 8 filters
Outputs differ for channel indices [3, 4, 5] @ 8 filters
Outputs differ for channel indices [3, 4, 6] @ 8 filters
Outputs differ for channel indices [3, 4, 7] @ 8 filters
Outputs differ for channel indices [3, 5, 6] @ 8 filters
Outputs differ for channel indices [3, 5, 7] @ 8 filters
Outputs differ for channel indices [3, 6, 7] @ 8 filters
Outputs differ for channel indices [4, 5, 6] @ 8 filters
Outputs differ for channel indices [4, 5, 7] @ 8 filters
Outputs differ for channel indices [4, 6, 7] @ 8 filters
Outputs differ for channel indices [5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 5] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 5] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 5] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [2, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [2, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [2, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 5] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [2, 3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 5, 6] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 5, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 4, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 3, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 2, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 1, 3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [0, 2, 3, 4, 5, 6, 7] @ 8 filters
Outputs differ for channel indices [1, 2, 3, 4, 5, 6, 7] @ 8 filters
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'comp:ops', 'TF 2.13']",2023-07-28T13:02:04Z,4,0,https://github.com/tensorflow/tensorflow/issues/61422,
320,tensorflow/tensorflow,tf.debugging.experimental.enable_dump_debug_info (Debugger V2) error with TPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

tf 2.12 (but also 2.14 nightly)

### Custom code

Yes

### OS platform and distribution

Google Colab

### Mobile device

no

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

N/A (using TPU)

### GPU model and memory

_No response_

### Current behavior?

I'm trying to use the `tf.debugging.experimental.enable_dump_debug_info(...)` function with TPU.

I've tested my code without the TPU strategy bit, and it works, I can use the Debugger V2 fine.

I've also tested the code without the debugger bit and it trains fine as well.

But together it gives me this error:

```
Traceback (most recent call last):
  File ""/content/tbscript.py"", line 47, in <module>
    model = train()
  File ""/content/tbscript.py"", line 40, in train
    model.fit(x=x_train, 
  File ""/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/debug/lib/dumping_callback.py"", line 579, in <listcomp>
    output_tensor_device_ids = [writer.RegisterDeviceAndGetId(output.device)
ValueError: Cannot assign a device for operation IteratorGetNextAsOptional: Could not satisfy explicit device specification '' because the node {{colocation_node IteratorGetNextAsOptional}} was colocated with a group of nodes that required incompatible device '/job:worker/replica:0/task:0/device:TPU:0'. All available devices [/job:worker/replica:0/task:0/device:CPU:0, /job:worker/replica:0/task:0/device:TPU:0, /job:worker/replica:0/task:0/device:TPU:1, /job:worker/replica:0/task:0/device:TPU:2, /job:worker/replica:0/task:0/device:TPU:3, /job:worker/replica:0/task:0/device:TPU:4, /job:worker/replica:0/task:0/device:TPU:5, /job:worker/replica:0/task:0/device:TPU:6, /job:worker/replica:0/task:0/device:TPU:7, /job:worker/replica:0/task:0/device:TPU_SYSTEM:0, /job:worker/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:COMPOSITE:0]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=2 requested_device_name_='/job:worker/replica:0/task:0/device:TPU:0' assigned_device_name_='/job:worker/replica:0/task:0/device:TPU:0' resource_device_name_='/job:worker/replica:0/task:0/device:TPU:0' supported_device_types_=[CPU] possible_devices_=[]
OptionalGetValue: CPU TPU XLA_CPU 
DebugNumericSummaryV2: CPU 
IteratorGetNext: CPU TPU XLA_CPU 
Identity: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
Switch: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
IteratorGetNextAsOptional: CPU TPU XLA_CPU 
DebugIdentityV2: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
OptionalHasValue: CPU TPU XLA_CPU 
_Arg: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 

Colocation members, user-requested devices, and framework assigned devices, if any:
  iterator_1 (_Arg)  framework assigned device=/job:worker/replica:0/task:0/device:TPU:0
  IteratorGetNextAsOptional (IteratorGetNextAsOptional) 
  OptionalHasValue (OptionalHasValue) 
  cond/IteratorGetNextAsOptional/_5 (Switch) 
  cond/iterator_1/_13 (Switch) 
  Func/cond/then/_0/input/_39 (Identity) 
  Func/cond/then/_0/input/_47 (Identity) 
  cond/then/_0/cond/OptionalHasValue (OptionalHasValue) 
  Func/cond/else/_1/input/_73 (Identity) 
  Func/cond/else/_1/input/_81 (Identity) 
  cond/else/_1/cond/IteratorGetNext (IteratorGetNext) 
  cond/else/_1/cond/IteratorGetNext/DebugNumericSummaryV2 (DebugNumericSummaryV2) 
  cond/else/_1/cond/IteratorGetNext/DebugIdentityV2_1511 (DebugIdentityV2) 
  cond/then/_0/cond/cond/Func/cond/then/_0/input/_39/_111 (Switch) 
  Func/cond/then/_0/cond/cond/then/_106/input/_179 (Identity) 
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue (OptionalGetValue) /job:worker/replica:0/task:0/device:TPU:0
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue/DebugNumericSummaryV2 (DebugNumericSummaryV2) /job:worker/replica:0/task:0/device:TPU:0
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue/DebugIdentityV2_1369 (DebugIdentityV2) /job:worker/replica:0/task:0/device:TPU:0
  Func/cond/then/_0/cond/cond/else/_107/input/_184 (Identity) 

	 [[{{node IteratorGetNex ... [truncated]
Exception ignored in atexit callback: <function async_wait at 0x7ecd97fc5d80>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py"", line 2796, in async_wait
    context().sync_executors()
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py"", line 742, in sync_executors
    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation IteratorGetNextAsOptional: Could not satisfy explicit device specification '' because the node {{colocation_node IteratorGetNextAsOptional}} was colocated with a group of nodes that required incompatible device '/job:worker/replica:0/task:0/device:TPU:0'. All available devices [/job:worker/replica:0/task:0/device:CPU:0, /job:worker/replica:0/task:0/device:TPU:0, /job:worker/replica:0/task:0/device:TPU:1, /job:worker/replica:0/task:0/device:TPU:2, /job:worker/replica:0/task:0/device:TPU:3, /job:worker/replica:0/task:0/device:TPU:4, /job:worker/replica:0/task:0/device:TPU:5, /job:worker/replica:0/task:0/device:TPU:6, /job:worker/replica:0/task:0/device:TPU:7, /job:worker/replica:0/task:0/device:TPU_SYSTEM:0, /job:worker/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:COMPOSITE:0]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=2 requested_device_name_='/job:worker/replica:0/task:0/device:TPU:0' assigned_device_name_='/job:worker/replica:0/task:0/device:TPU:0' resource_device_name_='/job:worker/replica:0/task:0/device:TPU:0' supported_device_types_=[CPU] possible_devices_=[]
OptionalGetValue: CPU TPU XLA_CPU 
DebugNumericSummaryV2: CPU 
IteratorGetNext: CPU TPU XLA_CPU 
Identity: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
Switch: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
IteratorGetNextAsOptional: CPU TPU XLA_CPU 
DebugIdentityV2: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 
OptionalHasValue: CPU TPU XLA_CPU 
_Arg: CPU TPU TPU_SYSTEM XLA_CPU COMPOSITE 

Colocation members, user-requested devices, and framework assigned devices, if any:
  iterator_1 (_Arg)  framework assigned device=/job:worker/replica:0/task:0/device:TPU:0
  IteratorGetNextAsOptional (IteratorGetNextAsOptional) 
  OptionalHasValue (OptionalHasValue) 
  cond/IteratorGetNextAsOptional/_5 (Switch) 
  cond/iterator_1/_13 (Switch) 
  Func/cond/then/_0/input/_39 (Identity) 
  Func/cond/then/_0/input/_47 (Identity) 
  cond/then/_0/cond/OptionalHasValue (OptionalHasValue) 
  Func/cond/else/_1/input/_73 (Identity) 
  Func/cond/else/_1/input/_81 (Identity) 
  cond/else/_1/cond/IteratorGetNext (IteratorGetNext) 
  cond/else/_1/cond/IteratorGetNext/DebugNumericSummaryV2 (DebugNumericSummaryV2) 
  cond/else/_1/cond/IteratorGetNext/DebugIdentityV2_1511 (DebugIdentityV2) 
  cond/then/_0/cond/cond/Func/cond/then/_0/input/_39/_111 (Switch) 
  Func/cond/then/_0/cond/cond/then/_106/input/_179 (Identity) 
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue (OptionalGetValue) /job:worker/replica:0/task:0/device:TPU:0
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue/DebugNumericSummaryV2 (DebugNumericSummaryV2) /job:worker/replica:0/task:0/device:TPU:0
  cond/then/_0/cond/cond/then/_106/cond/cond/OptionalGetValue/DebugIdentityV2_1369 (DebugIdentityV2) /job:worker/replica:0/task:0/device:TPU:0
  Func/cond/then/_0/cond/cond/else/_107/input/_184 (Identity) 

	 [[{{node IteratorGetNex ... [truncated]
2023-07-28 06:05:18.174660: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:59] Ignoring an error encountered when deleting remote tensors handles: INVALID_ARGUMENT: Unable to find the relevant tensor remote_handle: Op ID: 900, Output num: 0
Additional GRPC error information from remote target /job:worker/replica:0/task:0 while calling /tensorflow.eager.EagerService/Enqueue:
:{""created"":""@1690524318.171303502"",""description"":""Error received from peer ipv4:10.15.76.74:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 900, Output num: 0"",""grpc_status"":3} [type.googleapis.com/tensorflow.core.platform.ErrorSourceProto='\x08\x05']
```

yeah very long.

oh and I've tried `tf.config.set_soft_device_placement(True)` but no result

### Standalone code to reproduce the issue

```shell
Here's a reproducible test case for getting the error:
https://colab.research.google.com/drive/169agwcqy3-M8hQnSAx5EAr64ya2bSMl3?usp=sharing

but you're not supposed to run it on colab, as I've noticed the `tf.debugging.experimental.enable_dump_debug_info` function generally doesn't work with it. So I put it in a .py script and run it with !python3. Without the TPU part it works fine.
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:tpus', 'TF 2.12']",2023-07-28T06:27:06Z,4,0,https://github.com/tensorflow/tensorflow/issues/61421,
321,tensorflow/tensorflow,tf.keras.callbacks.SidecarEvaluatorModelExport doc page looks broken.,"### Issue type

Documentation Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The raw html tag is displayed and the display is collapsed when you access this link. https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/SidecarEvaluatorModelExport

### Standalone code to reproduce the issue

```shell
Please access from your browser.If it is not reproduced, I will share my detailed environment.
```


### Relevant log output

_No response_","['type:docs-bug', 'awaiting review', 'type:bug', 'TF 2.13']",2023-07-25T05:08:46Z,1,0,https://github.com/tensorflow/tensorflow/issues/61375,
322,tensorflow/tensorflow,tf.io.gfile.rename not working for directories in S3,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following code used to work in Tensorflow < 2.6. Upon Tensorflow 2.6, we had to import tensorflow_io. However, the tf.io.gfile.rename function which used to work on directories in S3 no longer works. I would like to update to a newer version of Tensorflow but this issue is preventing our organization from doing so, as some libraries we use use tf.io.gfile.rename to change folder names during training.

tf.io.gfile.rename should work on directories according to the Tensorflow documentation

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tensorflow_io as tfio

SOURCE_DIR = 's3://.../old_name/'
DEST_DIR = 's3://.../new_name/'

tf.io.gfile.rename(SOURCE_DIR, DEST_DIR)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/home/eigen/.config/JetBrains/PyCharm2023.1/scratches/tf2_s3_rename_test.py"", line 9, in <module>
    tf.io.gfile.rename(SOURCE_DIR, DEST_DIR)
  File ""/home/eigen/venvs/eigen-ml-tf2-12/lib/python3.10/site-packages/tensorflow/python/lib/io/file_io.py"", line 622, in rename_v2
    _pywrap_file_io.RenameFile(
tensorflow.python.framework.errors_impl.FailedPreconditionError: Source is a directory or empty file
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.12']",2023-07-24T14:39:54Z,7,0,https://github.com/tensorflow/tensorflow/issues/61364,
323,tensorflow/tensorflow,Unbounded Memory leak when using tf.py_function in tf.data.Dataset.map(),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.13.0-rc2-7-g1cb1a030a62 2.13.0

### Custom code

Yes

### OS platform and distribution

Linux Ubuntu 20.04, Google Colab

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8 / 8.6

### GPU model and memory

various, e.g. 2080ti, 3080ti mobile, Colab T4

### Current behavior?

Using tf.py_function in a function that is applied to a tf.data.Dataset via its map() function causes a (C++-level) memory leak.

In my real training with more complex code inside the py_function, this lead to the python script eventually consuming upwards of 30 GB of RAM during a model.fit() loop, despite taking less that 3GB of RAM during the initial epoch.

tf.py_function also more generally causes memory leaks in all kinds of places. See the flags at the top of the linked Collab for details.



### Standalone code to reproduce the issue

```shell
see Collab: https://colab.research.google.com/drive/1auVJPyHApl4__4FF-rV3xNcJrqYZc38R?usp=sharing

Iterating through a dataset with a tf.py_function in it causes unbounded linear memory consumption growth.
```


### Relevant log output

```shell
**Batch 0**
Memory usage: 1732120576
Delta: 1651.88 MiB
**Batch 200**
Memory usage: 1736859648
Delta: 4.52 MiB
**Batch 400**
Memory usage: 1740644352
Delta: 3.61 MiB
**Batch 600**
Memory usage: 1744699392
Delta: 3.87 MiB
Average Delta since start: 3.87 MiB/iteration
Estimated growth per 1000 steps: 19.34 MiB
**Batch 800**
Memory usage: 1748750336
Delta: 3.86 MiB
Average Delta since start: 3.87 MiB/iteration
Estimated growth per 1000 steps: 19.33 MiB
**Batch 1000**
Memory usage: 1752805376
Delta: 3.87 MiB
Average Delta since start: 3.87 MiB/iteration
Estimated growth per 1000 steps: 19.33 MiB
**Batch 1200**
Memory usage: 1757401088
Delta: 4.38 MiB
Average Delta since start: 4.00 MiB/iteration
Estimated growth per 1000 steps: 19.98 MiB
**Batch 1400**
Memory usage: 1761456128
Delta: 3.87 MiB
Average Delta since start: 3.97 MiB/iteration
Estimated growth per 1000 steps: 19.85 MiB
**Batch 1600**
Memory usage: 1765240832
Delta: 3.61 MiB
Average Delta since start: 3.91 MiB/iteration
Estimated growth per 1000 steps: 19.55 MiB
**Batch 1800**
Memory usage: 1769025536
Delta: 3.61 MiB
Average Delta since start: 3.87 MiB/iteration
Estimated growth per 1000 steps: 19.33 MiB
**Batch 2000**
Memory usage: 1773621248
Delta: 4.38 MiB
Average Delta since start: 3.93 MiB/iteration
Estimated growth per 1000 steps: 19.66 MiB
**Batch 2200**
Memory usage: 1777676288
Delta: 3.87 MiB
Average Delta since start: 3.92 MiB/iteration
Estimated growth per 1000 steps: 19.62 MiB
**Batch 2400**
Memory usage: 1781731328
Delta: 3.87 MiB
Average Delta since start: 3.92 MiB/iteration
Estimated growth per 1000 steps: 19.59 MiB
**Batch 2600**
Memory usage: 1785786368
Delta: 3.87 MiB
Average Delta since start: 3.91 MiB/iteration
Estimated growth per 1000 steps: 19.57 MiB
```
","['stat:awaiting tensorflower', 'type:bug', 'type:performance', 'TF 2.13']",2023-07-20T20:36:51Z,6,0,https://github.com/tensorflow/tensorflow/issues/61344,
324,tensorflow/tensorflow,Random predictions with intel-tensorflow when OMP_THREAD_LIMIT is set,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

The problem arises when using *intel-tensorflow*

### Source

binary

### TensorFlow version

unknown, 2.13.0 (package intel_tensorflow-2.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl)

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04

### Python version

3.8, 3.9, 3.10, 3.11

### Current behavior?

This problem only happens when using the *intel-tensorflow* package which uses the mkl library. When the environment variable OMP_THREAD_LIMIT is set, the predictions of some standard models become random when running on cpu.

During the run, a warning from OMP is shown:
```
OMP: Warning #96: Cannot form a team with 36 threads, using 12 instead.
OMP: Hint Consider unsetting KMP_DEVICE_THREAD_LIMIT (KMP_ALL_THREADS), KMP_TEAMS_THREAD_LIMIT, and OMP_THREAD_LIMIT (if any are set).
```
This warning suggests to unset the variable _OMP_THREAD_LIMIT_ but I think a randomness/instability in the prediction of models needs more than a warning with a ""Hint"".

To reproduce the problem:
* Create an environment with *intel-tensorflow* installed
* Copy the code in the following section into a file *test_script.py*
* Run the following commands:
  * `python test_script.py`
  * `OMP_THREAD_LIMIT=2 python test_script.py`  (edited)

### Standalone code to reproduce the issue

```python
import tensorflow as tf


if __name__ == ""__main__"":
    with tf.device(""/cpu:0""):
        model = tf.keras.applications.efficientnet.EfficientNetB0()
        img = tf.ones((1, 224, 224, 3))
        pred = model(img)

        print(f""Result: {pred[0, :5]}"")
```


### Relevant log output

```shell
# Run without OMP_THREAD_LIMIT
...
Result: [0.00066389 0.00075261 0.00108045 0.00210853 0.00316559]

# Run with OMP_THREAD_LIMIT
...
OMP: Warning #96: Cannot form a team with 6 threads, using 2 instead.
OMP: Hint Consider unsetting KMP_DEVICE_THREAD_LIMIT (KMP_ALL_THREADS), KMP_TEAMS_THREAD_LIMIT, and OMP_THREAD_LIMIT (if any are set).
Result: [0.00030835 0.00062532 0.00057299 0.00088017 0.00140722]
```
","['stat:awaiting tensorflower', 'type:bug', 'subtype:cpu-intel', 'TF 2.13']",2023-07-19T12:58:52Z,2,0,https://github.com/tensorflow/tensorflow/issues/61328,
325,tensorflow/tensorflow,TensorFlow Lite Converter wraps unpack operator with dequantize/quantize,"### System information

- Linux Ubuntu 20.04
- TensorFlow installed from: pip source
- TensorFlow versions: 2.12.0-current master 

### Code

Provide code to help us reproduce your issues using one of the following options:

```
import tensorflow as tf
from tensorflow.keras.layers import Layer, Input
from tensorflow.keras.models import Model
import numpy as np

class UnstackLayer(Layer):
    def __init__(self, **kwargs):
        super(UnstackLayer, self).__init__(**kwargs)

    def call(self, inputs):
        unstacked = tf.unstack(inputs, axis=1)
        # only last output is used as input to the add operator
        return unstacked[-1]

input_tensor = Input(shape=(4, 16, 32))
x = UnstackLayer()(input_tensor)
output_tensor = tf.add(x, 1)

model = Model(inputs=input_tensor, outputs=output_tensor)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

def representative_data_gen():
  for input_value in [np.random.randn(1, 4, 16, 32).astype(np.float32) for _ in range(10)]:
    yield [input_value]
converter.representative_dataset = representative_data_gen

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_types = {tf.int8}

converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_model = converter.convert()


# Save the TFLite model to a .tflite file
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)
```

### Failure after conversion

Input Model:

![Screenshot from 2023-07-19 11-05-35](https://github.com/tensorflow/tensorflow/assets/112401409/dabb3863-54c3-4148-bcab-e4afd39a4ac5)

Output Model:

![Screenshot from 2023-07-19 11-28-39](https://github.com/tensorflow/tensorflow/assets/112401409/f42419d0-0365-4738-bd0c-0bf9dc24b798)


Behaviour in TF 2.11 and below is that no dequantize/quantize ops appear, which is expected.  

### Other info

The conversion started failing with version TF 2.12.0. Note that the conversion succeeds intermittently when converting the same network many times, but on average it fails. This intermittent behaviour is still present if one runs the converter on a single core and keeps the representative dataset constant. Similar issues seems to be present when converting split operators as well. 


","['stat:awaiting tensorflower', 'type:bug', 'TFLiteConverter', 'ModelOptimizationToolkit', 'TF 2.12']",2023-07-19T09:31:11Z,12,0,https://github.com/tensorflow/tensorflow/issues/61323,
326,tensorflow/tensorflow,snapshot op wrongly changes data fingerprint ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75

### Custom code

Yes

### OS platform and distribution

Linux 4e51bcd72cb8 5.15.109 (Colab)

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

When constructing a tabular dataset from a given file, I see the snapshot fingerprint changing with repeated attempts. The pipeline and data don't change tough. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

traffic_volume_csv_gz = tf.keras.utils.get_file(
    'Metro_Interstate_Traffic_Volume.csv.gz', 
    ""https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz"",
    cache_dir='.', cache_subdir='traffic'
)

ds = tf.data.experimental.make_csv_dataset(
    traffic_volume_csv_gz,
    batch_size=256,
    label_name='traffic_volume',
    num_epochs=1,
    compression_type=""GZIP""
)

ds = ds.enumerate()
ds = ds.snapshot('ds.tfsnap')
ds = ds.map(lambda i,x: x).repeat(10)

for i,_ in enumerate(ds):
  pass

print(i)
```


### Relevant log output

In Colab Notebook, several runs of this code generate many snapshots
```console
ls ds.tfsnap
11819476836996993959  2128571372330446365  8272381159243496395
14899783750259964653  3924588669394259065  9335410099383931136
```","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.13']",2023-07-18T15:50:06Z,7,0,https://github.com/tensorflow/tensorflow/issues/61317,
327,tensorflow/tensorflow,TensorFlow 2.13 distributed training fail,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

binary

### TensorFlow version

2.13.0

### Custom code

No

### OS platform and distribution

Linux Ubuntu 20.04.3

### Mobile device

Linux Ubuntu 20.04.3

### Python version

3.8.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.7, cuDNN 8.6

### GPU model and memory

3x NVIDIA GeForce RTX 3090

### Current behavior?

When trying to run multiple distributed trainings one after another, one of them fails with an `Collective ops is aborted by: ...` error. 

The reproducer attached to this issue produces the following error:
```
Collective ops is aborted by: Device /job:localhost/replica:0/task:0/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)
The error could be from a previous operation. Restart your program to reset.
	 [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_5585]
```
When run with TF 2.12 there is no such error.

The original code where I have encountered this problem results in
```
E                                           Collective ops is aborted by: Shape mismatch in the collective instance 100. Op at device /job:localhost/replica:0/task:0/device:GPU:1 expected shape [517169] but another member in the group expected shape [516734]. This is likely due to different input shapes at different members of the collective op.
E                                           The error could be from a previous operation. Restart your program to reset.
E                                           	 [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_49105]
```
but I wasn't able to reproduce this with a small code snippet.

### Standalone code to reproduce the issue

```shell
import pytest
import tensorflow as tf
import tensorflow_datasets as tfds


@pytest.mark.parametrize(""devices"", [1, 3, 2])
def test_distributed_fit(devices):
    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
    mnist_train, mnist_test = datasets['train'], datasets['test']

    if devices == 1:
        strategy = tf.distribute.OneDeviceStrategy(""/gpu:0"")
    else:
        strategy = tf.distribute.MirroredStrategy([f""/gpu:{i}"" for i in range(devices)])

    batch_size = 64 * strategy.num_replicas_in_sync
    train_dataset = mnist_test.cache().shuffle(10000).batch(batch_size)

    with strategy.scope():
        model = tf.keras.Sequential([
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(10)
        ])

        model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                      optimizer=tf.keras.optimizers.Adam(),
                      metrics=['accuracy'])

    model.fit(train_dataset, epochs=1)


if __name__ == '__main__':
    test_distributed_fit(1)
    test_distributed_fit(3)
    test_distributed_fit(2)
```


### Relevant log output

```shell
/home/nsavel/venvs/nncf_tf_213/bin/python /home/nsavel/workspace/nncf_tf_213/reproducer.py 
2023-07-18 16:47:21.693862: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-18 16:47:21.722428: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:7630] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-07-18 16:47:21.722456: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-07-18 16:47:21.722481: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-07-18 16:47:21.728124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-18 16:47:22.211027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:tensorflow:From /home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
WARNING:tensorflow:From /home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.
Instructions for updating:
The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.
2023-07-18 16:47:24.321508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1833] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22292 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:17:00.0, compute capability: 8.6
2023-07-18 16:47:24.322042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1833] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22292 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:65:00.0, compute capability: 8.6
2023-07-18 16:47:24.322425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1833] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22292 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:b3:00.0, compute capability: 8.6
2023-07-18 16:47:24.602273: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2023-07-18 16:47:25.946425: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcf358b4470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-07-18 16:47:25.946450: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2023-07-18 16:47:25.946455: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2023-07-18 16:47:25.946458: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA GeForce RTX 3090, Compute Capability 8.6
2023-07-18 16:47:25.950178: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-07-18 16:47:26.074588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Loaded cuDNN version 8600
2023-07-18 16:47:26.171621: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
157/157 [==============================] - 2s 5ms/step - loss: 25.9054 - accuracy: 0.6873
2023-07-18 16:47:27.474184: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2023-07-18 16:47:30.690312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Loaded cuDNN version 8600
2023-07-18 16:47:30.822607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:434] Loaded cuDNN version 8600
53/53 [==============================] - 3s 7ms/step - loss: 43.9234 - accuracy: 0.5655
2023-07-18 16:47:31.372876: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:552] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2023-07-18 16:47:32.398894: E tensorflow/core/common_runtime/base_collective_executor.cc:249] BaseCollectiveExecutor::StartAbort INTERNAL: Device /job:localhost/replica:0/task:0/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)
2023-07-18 16:47:32.398950: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7416489994643074752
2023-07-18 16:47:32.399024: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1224112818691547746
2023-07-18 16:47:32.399044: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10338356286700713842
2023-07-18 16:47:32.399063: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6809993284794892577
2023-07-18 16:47:32.399081: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12460047264292639245
2023-07-18 16:47:32.399097: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8051515006773529005
Traceback (most recent call last):
  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)
  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)
  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node CollectiveReduceV2 defined at (most recent call last):
  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1376, in train_function
    return step_function(self, iterator)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1376, in train_function
    return step_function(self, iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1359, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 35, in <module>
    test_distributed_fit(2)

  File ""/home/nsavel/workspace/nncf_tf_213/reproducer.py"", line 29, in test_distributed_fit
    model.fit(train_dataset, epochs=1)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py"", line 65, in error_handler
    return fn(*args, **kwargs)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1782, in fit
    tmp_logs = self.train_function(iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1376, in train_function
    return step_function(self, iterator)

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/engine/training.py"", line 1359, in step_function
    outputs = model.distribute_strategy.run(run_step, args=(data,))

  File ""/home/nsavel/venvs/nncf_tf_213/lib/python3.8/site-packages/keras/src/optimizers/utils.py"", line 175, in _all_reduce_sum_fn
    return distribution.extended.batch_reduce_to(

Collective ops is aborted by: Device /job:localhost/replica:0/task:0/device:GPU:1 is joining a group with size2, but that group has size 3 (group_key=1)
The error could be from a previous operation. Restart your program to reset.
	 [[{{node CollectiveReduceV2}}]] [Op:__inference_train_function_5585]

Process finished with exit code 1
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:dist-strat', 'TF 2.13']",2023-07-18T14:56:01Z,15,5,https://github.com/tensorflow/tensorflow/issues/61314,
328,tensorflow/tensorflow,snapshoting failure on dataset made from generator,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75

### Custom code

Yes

### OS platform and distribution

Linux 4e51bcd72cb8 5.15.109 (Colab)

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Simple snapshoting op with sharding fails when applied to a generator-based dataset.

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

np.random.seed(1234)

IMG_SHAPE = (224,224,3)

def gen_img(shape=IMG_SHAPE):
  while True:
    img = np.random.randint(0,256,size=IMG_SHAPE)
    lab = np.random.randint(0,10)
    yield (img,lab)

ds = tf.data.Dataset.from_generator(
      gen_img,
      output_signature=(
        tf.TensorSpec(shape=IMG_SHAPE, dtype=tf.int32),
        tf.TensorSpec(shape=(), dtype=tf.int32)
      )
)
ds = ds.take(int(1e3)).batch(32)
ds = ds.enumerate()
ds = ds.snapshot('./my_cached_dataset', shard_func = lambda i,x: i%10)
ds = ds.map(lambda i,x: x).repeat(2) # error disappears under 1 epoch !

for i,(img,lab) in enumerate(ds):
  pass
```


### Relevant log output

```shell
ResourceExhaustedError: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Output buffer(size: 262144 bytes) too small. Should be larger than 19267611 bytes. [Op:IteratorGetNext]
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.12']",2023-07-18T14:37:30Z,2,0,https://github.com/tensorflow/tensorflow/issues/61313,
329,tensorflow/tensorflow,CTC Loss errors on TPU,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.12

### Custom code

Yes

### OS platform and distribution

Kaggle notebook

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Keras Model with LSTM+ CTC loss is running normally on GPU,
 but on VM TPU it  prints grappler errors. The errors usually don't stop the execution but the loss is nan.
It sometimes also crashes with coredump, but it's not consistent.

I created a public kaggle notebook with the code producing the issue here:
https://www.kaggle.com/code/shaironen/ctc-example/notebook
The grappler errors are:

E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.
2023-07-17 09:06:37.445931: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.
E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.

In addition, I read it's recommended to use model.compile(jit_compile=True) while on GPU to pre-diagnose TPU issues. It gives similar errors and terminates.
(with jit_compile=False it run normally on GPU only).

According to  tf.nn.ctc_loss documentation it should work on tpu.


### Standalone code to reproduce the issue

```shell
https://www.kaggle.com/code/shaironen/ctc-example/notebook
```


### Relevant log output

```shell
WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py:1512: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.

WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py:1512: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.

WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py:1495: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.

WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/tensorflow/python/ops/ctc_ops.py:1495: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.
2023-07-17 09:29:45.795306: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.
2023-07-17 09:29:46.168913: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.

 99/100 [============================>.] - ETA: 0s - loss: nan

2023-07-17 09:30:01.881305: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall.
2023-07-17 09:30:02.118121: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node StatefulPartitionedCall.
F0717 09:30:03.134464    3526 throw_delegate.cc:121] RAW: absl::container_internal::raw_hash_map<>::at
    @     0x78bd48329338  (unknown)
    @     0x78bd483271e6  (unknown)
    @     0x78bd465e0f20  (unknown)
    @     0x78bd465d6231  (unknown)
    @     0x78bd465d5b59  (unknown)
    @     0x78bd476ce3c9  (unknown)
    @     0x78bd476cf3d7  (unknown)
    @     0x78bd476ccfba  (unknown)
    @     0x78bd40ef00e6  (unknown)
    @     0x78bd465d4e1f  (unknown)
    @     0x78bd465d6919  (unknown)
    @     0x78bd465d662c  (unknown)
    @     0x78bd46345238  (unknown)
    @     0x78bd430f4143  (unknown)
    @     0x78bd46d5f6b4  (unknown)
    @     0x78bd46d5dc50  (unknown)
    @     0x78bd46d5d54e  (unknown)
    @     0x78bd430cee43  (unknown)
    @     0x78bd430de27a  (unknown)
    @     0x78bd430ee05b  (unknown)
    @     0x78bd430ee945  (unknown)
    @     0x78bd424648a3  (unknown)
    @     0x78bd4245e54c  (unknown)
    @     0x78bd41562909  (unknown)
    @     0x78bd41563a59  (unknown)
    @     0x78bd40f32a1c  TpuCompile_CompileAndBuild
    @     0x78bd515e0225  tensorflow::tpu::TpuProgramGroup::CompileAndBuild()
    @     0x78bd51530d6f  tensorflow::tpu::TpuCompileOpKernelImpl::Compile()
    @     0x78bd515e34df  tensorflow::tpu::TpuCompileOpKernelCommon::CompileLocallyAndFillHostCacheInternal()
    @     0x78bd515e3aa1  tensorflow::tpu::TpuCompileOpKernelCommon::CompileLocallyAndFillHostCache()
    @     0x78bd515e3c8b  tensorflow::tpu::TpuCompileOpKernelCommon::ComputeInternal()::{lambda()#3}::operator()()
    @     0x78bd515e3d6c  std::_Function_handler<>::_M_invoke()
    @     0x78bd515cc070  tensorflow::tpu::TpuCompilationCacheExternal::InitializeEntry()
    @     0x78bd51612e72  tensorflow::tpu::TpuCompilationCacheInterface::CompileIfKeyAbsentHelper()
    @     0x78bd5161391a  tensorflow::tpu::TpuCompilationCacheInterface::CompileIfKeyAbsent()
    @     0x78bd515e4323  tensorflow::tpu::TpuCompileOpKernelCommon::ComputeInternal()
    @     0x78bd515e6fc4  tensorflow::tpu::TpuCompileOpKernelCommon::Compute()
    @     0x78bd62de6b6d  tensorflow::ThreadPoolDevice::Compute()
    @     0x78bd62ecfe2c  tensorflow::(anonymous namespace)::ExecutorState<>::Process()
    @     0x78bd62eb9272  std::_Function_handler<>::_M_invoke()
    @     0x78bd621b7275  Eigen::ThreadPoolTempl<>::WorkerLoop()
    @     0x78bd621b41c7  std::_Function_handler<>::_M_invoke()
    @     0x78bd62cefabf  tsl::(anonymous namespace)::PThread::ThreadFn()
    @     0x78be31abeea7  start_thread
https://symbolize.stripped_domain/r/?trace=78bd48329338,78bd483271e5,78bd465e0f1f,78bd465d6230,78bd465d5b58,78bd476ce3c8,78bd476cf3d6,78bd476ccfb9,78bd40ef00e5,78bd465d4e1e,78bd465d6918,78bd465d662b,78bd46345237,78bd430f4142,78bd46d5f6b3,78bd46d5dc4f,78bd46d5d54d,78bd430cee42,78bd430de279,78bd430ee05a,78bd430ee944,78bd424648a2,78bd4245e54b,78bd41562908,78bd41563a58,78bd40f32a1b,78bd515e0224,78bd51530d6e,78bd515e34de,78bd515e3aa0,78bd515e3c8a,78bd515e3d6b,78bd515cc06f,78bd51612e71,78bd51613919,78bd515e4322,78bd515e6fc3,78bd62de6b6c,78bd62ecfe2b,78bd62eb9271,78bd621b7274,78bd621b41c6,78bd62cefabe,78be31abeea6&map=aef7fe2e538f701f46d88df9ee3b51d79ec62b1e:78bd6181e000-78bd637f9728,8f79f803f683427be94b1cfeea32716e6ef365e4:78bd48eb8000-78bd60e0d830,1278088d049ad36cb636fbbc76303cb3:78bd3cc43000-78bd484907c0 
https://symbolize.stripped_domain/r/?trace=78be31b11ce1,78be31b11d5f,78bd48329337,78bd483271e5,78bd465e0f1f,78bd465d6230,78bd465d5b58,78bd476ce3c8,78bd476cf3d6,78bd476ccfb9,78bd40ef00e5,78bd465d4e1e,78bd465d6918,78bd465d662b,78bd46345237,78bd430f4142,78bd46d5f6b3,78bd46d5dc4f,78bd46d5d54d,78bd430cee42,78bd430de279,78bd430ee05a,78bd430ee944,78bd424648a2,78bd4245e54b,78bd41562908,78bd41563a58,78bd40f32a1b,78bd515e0224,78bd51530d6e,78bd515e34de,78bd515e3aa0,78bd515e3c8a&map=8f79f803f683427be94b1cfeea32716e6ef365e4:78bd48eb8000-78bd60e0d830,1278088d049ad36cb636fbbc76303cb3:78bd3cc43000-78bd484907c0 
*** SIGABRT received by PID 2614 (TID 3526) on cpu 51 from PID 2614; ***
E0717 09:30:03.650583    3526 coredump_hook.cc:414] RAW: Remote crash data gathering hook invoked.
E0717 09:30:03.650603    3526 client.cc:278] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.
E0717 09:30:03.650607    3526 coredump_hook.cc:512] RAW: Sending fingerprint to remote end.
E0717 09:30:03.650615    3526 coredump_socket.cc:120] RAW: Stat failed errno=2 on socket /var/google/services/logmanagerd/remote_coredump.socket
E0717 09:30:03.650619    3526 coredump_hook.cc:518] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] Missing crash reporting socket. Is the listener running?
E0717 09:30:03.650623    3526 coredump_hook.cc:580] RAW: Dumping core locally.
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:tpus', 'TF 2.12']",2023-07-17T09:34:24Z,12,1,https://github.com/tensorflow/tensorflow/issues/61297,
330,tensorflow/tensorflow,Unable to hide TPUs,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

2.12

### Custom code

No

### OS platform and distribution

Kaggle Notebooks

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Unable to hide TPUs from TensorFlow. The consequence of this is that if we want to use JAX along with TensorFlow, only one of them will be able to initialize the TPU system, and the other will fail. We won't be able to use `tfds`, `tf.image` or any TF operation per se if we can't hide TPUs from being used by TF. I want all these operations to run on CPU only, and leverage JAX for TPU. Here is the code to test it on a TPU machine:

```python
import tenorflow as tf

tf.config.set_visible_devices([], device_type=""TPU_SYSTEM"")
tf.config.set_visible_devices([], device_type=""TPU"")

print(tf.config.list_logical_devices())

# output:
# [LogicalDevice(name='/device:CPU:0', device_type='CPU'),
#  LogicalDevice(name='/device:TPU_SYSTEM:0', device_type='TPU_SYSTEM'),
#  LogicalDevice(name='/device:TPU:0', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:1', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:2', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:3', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:4', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:5', device_type='TPU'),
#  LogicalDevice(name='/device:TPU:6', device_type='TPU'),
# LogicalDevice(name='/device:TPU:7', device_type='TPU')]
```

This also doesn't work:

```python
physical_devices = tf.config.list_physical_devices()
tf.config.set_visible_devices(physical_devices[0], 'CPU')
```

### Standalone code to reproduce the issue

```shell
import tenorflow as tf

tf.config.set_visible_devices([], device_type=""TPU_SYSTEM"")
tf.config.set_visible_devices([], device_type=""TPU"")

print(tf.config.list_logical_devices())

# This also doesn't work:
physical_devices = tf.config.list_physical_devices()
tf.config.set_visible_devices(physical_devices[0], 'CPU')



### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:tpus', 'TF 2.12']",2023-07-17T05:53:45Z,10,1,https://github.com/tensorflow/tensorflow/issues/61293,
331,tensorflow/tensorflow,ValueError: No gradients provided for any variable,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.12

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have run a PyTorch code that computes the gradient of the gradient w.r.t some computation. It works just fine. Now, I want to translate PyTorch code into TensorFlow but got some errors.

## Standalone code to reproduce the issue

Here is the reproducible code. [Gist](https://colab.research.google.com/drive/1GPhctZNrXynrCQ0qNbLyMDmuixQtC0fw?usp=sharing).

The above collab is small and quickly reproduces the run of PyTorch and TensorFlow. PyTorch runs as expected but TensorLow doesn't. Below is the main spot to look at:


**Main Part**

In PyTorch, 

```python
rand_model = Rnadom()
model = Model()
ran_optim = torch.optim.SGD(
    ran_model.parameters()
)

model_params = model.parameters()
loss_mod  = model.forward(x)
loss_rand = model.forward(y)

model_grad = torch.autograd.grad(loss_mod, model_params)
rand_grad  = torch.autograd.grad(
    loss_rand, 
    model_params, 
    create_graph=True
)
    
loss = some_method(model_grad, rand_grad)   
rand_model.zero_grad()
loss.backward()
ran_optim.step()
```
In `pytorch`, the above `create_graph=True` is crucial. 

In TensorFlow, I tried 

```python
ran_model = Random()
ran_optim = tf.keras.optimizers.SGD()

model = Model()
model.build(input_shape=(1, 784))
optim = tf.keras.optimizers.SGD(0.01)
model_params = model.trainable_variables

with tf.GradientTape(persistent=True) as tape:
    tape.watch(ran_model.trainable_variables)
    loss_mod = tf.reduce_mean(tf.math.log(model(x)[:, i]))
    loss_rand = tf.reduce_mean(tf.math.log(model(y)[:, i]))
grads_mod = tape.gradient(loss_mod, model_params)
grads_rand = tape.gradient(loss_rand, model_params)

loss = some_method(model_grad, rand_grad)  
ran_model_grads = tape.gradient(loss, ran_model.trainable_variables)
ran_optim.apply_gradients(
  zip(ran_model_grads, ran_model.trainable_variables)
)
```

The `tf` code gives the following error. 

```yaml
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-3-01562609cda8> in <cell line: 33>()
     44         loss += tf.reduce_sum(tf.stack([a, b], axis=0))
     45     ran_model_grads = tape.gradient(loss, ran_model.trainable_variables)
---> 46     ran_optim.apply_gradients(zip(ran_model_grads, ran_model.trainable_variables))
     47 
     48 

3 frames
/usr/local/lib/python3.10/dist-packages/keras/optimizers/utils.py in filter_empty_gradients(grads_and_vars)
     75     if not filtered:
     76         variable = ([v.name for _, v in grads_and_vars],)
---> 77         raise ValueError(
     78             f""No gradients provided for any variable: {variable}. ""
     79             f""Provided `grads_and_vars` is {grads_and_vars}.""

ValueError: No gradients provided for any variable: (['Variable:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'Variable:0' shape=(10, 1, 784) dtype=float32, numpy=
```

- This is probably because the `ran_model_grads, ran_model.trainable_variables` are not connected. As mentioned in this [doc](https://www.tensorflow.org/guide/autodiff), 

> When a **target** is not connected to a **source**, the gradient will return `None`

- In PyTorch, `create_graph=True` is used to compute the gradient of the gradient in the later part. To compute [grad-of-grad](https://www.tensorflow.org/guide/advanced_autodiff#example_input_gradient_regularization), but didn't work (shown below). The reason probably is the same as before, source and target are not connected.

```python
for i in range(5):
    
    with tf.GradientTape() as tape1:
        loss_mod = tf.reduce_mean(tf.math.log(model(x)[:, i]))
    grads_mod = tape1.gradient(loss_mod, model_params)
    
    
    with tf.GradientTape() as tape3:
        with tf.GradientTape() as tape2:
            loss_rand = tf.reduce_mean(tf.math.log(model(y)[:, i]))
        grads_rand = tape2.gradient(loss_rand, model_params)

    loss = 0
    for a, b in zip(grads_mod, grads_rand):
        loss += tf.reduce_sum(tf.stack([a, b], axis=0))
    [ISSUE] > ran_model_grads = tape3.gradient(loss, ran_model.trainable_variables)
    ran_optim.apply_gradients(zip(ran_model_grads, ran_model.trainable_variables))
```

But in this case, how to resolve this in TensorFlow?","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'comp:core', 'TF 2.12']",2023-07-15T11:35:56Z,5,0,https://github.com/tensorflow/tensorflow/issues/61285,
332,tensorflow/tensorflow,keras.models.load_model broken if custom objects present (.keras format),"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

tf2.13

### Custom code

Yes

### OS platform and distribution

All

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Models do not load.  Errors vary, depending on the number of training samples being divisible by batch size or not.

### Standalone code to reproduce the issue

```shell
import keras
import numpy as np


class MyCustomLSTM(keras.layers.LSTM):
    """"""
    Custom LSTM
    """"""


model = keras.models.Sequential([MyCustomLSTM(32), keras.layers.Dense(1)])

model.compile(keras.optimizers.Adam(), keras.losses.mse)

x = np.zeros((1024, 300, 1))
y = np.zeros((1024, 1))

history = model.fit(x, y, batch_size=32, epochs=1)

model.save('test.keras')

model_loaded = keras.models.load_model('test.keras', custom_objects={'MyCustomLSTM': MyCustomLSTM})
# This throws:
# IndexError: list assignment index out of range

# If the number of samples is not divisible by the batch size (e.g. 1025 samples, instead of 1024), the error becomes:
# ValueError: as_list() is not defined on an unknown TensorShape.
```


### Relevant log output

_No response_","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'comp:model', 'TF 2.13']",2023-07-13T10:48:41Z,4,0,https://github.com/tensorflow/tensorflow/issues/61270,
333,tensorflow/tensorflow,tf.linalg.logdet outputs -inf on a matrix with complex data type,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

Given the following matrix:
```
t = np.array([[1 + 1j, 4 + 1j], [4 + 2j, 3 + 1j]])
```
tf.linalg.logdet outputs `-inf`, I think the expected output should be a normal value. For reference, PyTorch's torch.logdet outputs `tensor(2.6688-2.5536j, dtype=torch.complex128)`

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
t = np.array([[1 + 1j, 4 + 1j], [4 + 2j, 3 + 1j]])
tf_res = tf.linalg.logdet(tf.constant(t, dtype=tf.complex128))
torch_res = torch.logdet(torch.tensor(t))
print(tf_res, torch_res)
```
```


### Relevant log output

```shell
tf.Tensor(-inf, shape=(), dtype=float64) tensor(2.6688-2.5536j, dtype=torch.complex128)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-07-12T05:06:25Z,3,0,https://github.com/tensorflow/tensorflow/issues/61249,
334,tensorflow/tensorflow,tf.image.adjust_contrast fails on the tf.half data type,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

2.13.0

### Custom code

Yes

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I was trying tf.image.adjust_contrast but I find that it fails when I set the input tensor's data type to be `float16 (tf.half)`, this API raises error. However, if I set the input data type to be `bfloat16`, this API works properly. 

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
np.random.seed(1234)
t = tf.constant(np.random.rand(1,2,1), dtype=tf.float16)
i = tf.image.adjust_contrast(t, 2.)
print(i)
```
```


### Relevant log output

```shell
NotFoundError: Could not find device for node: {{node AdjustContrastv2}} = AdjustContrastv2[T=DT_HALF]
All kernels registered for op AdjustContrastv2:
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_HALF]
 [Op:AdjustContrastv2] name:
```
","['awaiting review', 'type:bug', 'comp:apis', 'comp:ops', 'TF 2.13']",2023-07-11T16:05:56Z,1,0,https://github.com/tensorflow/tensorflow/issues/61246,
335,tensorflow/tensorflow,"Tensorflow profiler is not showing anything. Gives ""No profile data was found"" text on selecting Profile in Tensorboard","### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.12, tf 2.13, tf-nightly

### Custom code

No

### OS platform and distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I was learning how to use TensorFlow Profiler according to the [guide](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras). I ran the same notebook without altering anything in Google Colab and it does now show Profile tab. Moreover on selecting the Profile option from the right hand side drop down list, It shows 

```
No profile data was found.
If you have a model running on CPU, GPU, or Google Cloud TPU, you may be able to use the above button to capture a profile.

If you're a CPU or GPU user, please use the IP address option. You may want to check out the [tutorial](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_profiling_keras.ipynb) on how to start a TensorFlow profiler server and profile a Keras model on a GPU.

If you're a TPU user, please use the TPU name option and you may want to check out the [tutorial](https://cloud.google.com/tpu/docs/cloud-tpu-tools) on how to interpreting the profiling results.

If you think profiling is done properly, please see the page of [Google Cloud TPU Troubleshooting and FAQ](https://cloud.google.com/tpu/docs/troubleshooting) and consider filing an issue on GitHub.

```

Instead it should have shown the Profile

### Standalone code to reproduce the issue

```shell
Kindly use the official [tensorflow profiler guide](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras)
```


### Relevant log output

```shell
No profile data was found.
If you have a model running on CPU, GPU, or Google Cloud TPU, you may be able to use the above button to capture a profile.

If you're a CPU or GPU user, please use the IP address option. You may want to check out the [tutorial](https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_profiling_keras.ipynb) on how to start a TensorFlow profiler server and profile a Keras model on a GPU.

If you're a TPU user, please use the TPU name option and you may want to check out the [tutorial](https://cloud.google.com/tpu/docs/cloud-tpu-tools) on how to interpreting the profiling results.

If you think profiling is done properly, please see the page of [Google Cloud TPU Troubleshooting and FAQ](https://cloud.google.com/tpu/docs/troubleshooting) and consider filing an issue on GitHub.

```
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.13']",2023-07-07T13:01:22Z,8,0,https://github.com/tensorflow/tensorflow/issues/61212,
336,tensorflow/tensorflow,Tensorflow Profiler does not work on WSL2: Failed to load libcupti (is it installed and accessible?) ,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

binary

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom code

Yes

### OS platform and distribution

Windows 10 WSL Ubuntu

### Mobile device

Ubuntu 22.04

### Python version

3.9

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

11.8/8.6

### GPU model and memory

_No response_

### Current behavior?

After following exactly the steps mentioned in https://www.tensorflow.org/install/pip for installing Tensorflow on WSL2, and installing the latest version of the profiler plugin, the Tensorboard profiler does not seem to work.

This is with a fresh WSL2 install, miniconda install, etc.

![image](https://github.com/tensorflow/tensorflow/assets/11645696/adbaf3c1-4f03-43fe-80d7-39c484ac91a7)

```
Failed to load libcupti (is it installed and accessible?) 

No step marker observed and hence the step time is unknown. This may happen if (1) training steps are not instrumented (e.g., if you are not using Keras) or (2) the profiling duration is shorter than the step time. For (1), you need to add step instrumentation; for (2), you may try to profile longer. 
```

The problem does not seem to be that lubcupti fails to load (despite what is indicated by Tensorboard); libcupti seems to be found just fine, but there may be some problems - See the attached log output for possible clues as to what's happening. 

### Standalone code to reproduce the issue

```shell
# The below code is copied directly from https://github.com/keras-team/keras-io/blob/master/examples/vision/mnist_convnet.py - with the single addition of adding Tensorboard profiling.

import numpy as np
from tensorflow import keras
from tensorflow.keras import layers

""""""
## Prepare the data
""""""

# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# Load the data and split it between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Scale images to the [0, 1] range
x_train = x_train.astype(""float32"") / 255
x_test = x_test.astype(""float32"") / 255
# Make sure images have shape (28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print(""x_train shape:"", x_train.shape)
print(x_train.shape[0], ""train samples"")
print(x_test.shape[0], ""test samples"")


# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

""""""
## Build the model
""""""

model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation=""relu""),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation=""softmax""),
    ]
)

model.summary()

""""""
## Train the model
""""""

batch_size = 128
epochs = 15

model.compile(loss=""categorical_crossentropy"", optimizer=""adam"", metrics=[""accuracy""])

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=[
    keras.callbacks.TensorBoard(profile_batch=[20, 30])
])

""""""
## Evaluate the trained model
""""""

score = model.evaluate(x_test, y_test, verbose=0)
print(""Test loss:"", score[0])
print(""Test accuracy:"", score[1])
```


### Relevant log output

```shell
17/422 [>.............................] - ETA: 2s - loss: 2.0755 - accuracy: 0.38602023-07-07 15:45:09.034256: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2023-07-07 15:45:09.034284: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2023-07-07 15:45:09.034300: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-07-07 15:45:09.034304: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:186] cuptiSubscribe: ignored due to a previous error.
2023-07-07 15:45:09.034307: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.
2023-07-07 15:45:09.034309: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1730] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error 
 25/422 [>.............................] - ETA: 2s - loss: 1.8839 - accuracy: 0.46882023-07-07 15:45:09.105818: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2023-07-07 15:45:09.105940: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:142] cuptiFinalize: ignored due to a previous error.
2023-07-07 15:45:09.105943: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:459] cuptiGetResultString: ignored due to a previous error.
2023-07-07 15:45:09.105945: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1822] function cupti_interface_->Finalize()failed with error 
2023-07-07 15:45:09.107652: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-07-07 15:45:09.107660: E tensorflow/compiler/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.
2023-07-07 15:45:09.107663: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_collector.cc:541]  GpuTracer has collected 0 callback api events and 0 activity events. 
2023-07-07 15:45:09.107809: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
```
","['stat:awaiting tensorflower', 'type:bug', 'type:others', 'wsl2']",2023-07-07T05:56:10Z,5,0,https://github.com/tensorflow/tensorflow/issues/61210,
337,tensorflow/tensorflow,//tensorflow/compiler/xla/service/gpu:fusion_merger_test fails on AARCH64,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.17

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

//tensorflow/compiler/xla/service/gpu:fusion_merger_test unit test fails when run on AARCH64 machine.

### Standalone code to reproduce the issue

```shell
bazel test --cache_test_results=no --config=mkl_aarch64_threadpool --jobs=75 --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --build_tests_only -- //tensorflow/compiler/xla/service/gpu:fusion_merger_test
```


### Relevant log output

```shell
FAIL: //tensorflow/compiler/xla/service/gpu:fusion_merger_test (see /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/xla/service/gpu/fusion_merger_test/test.log)
INFO: From Testing //tensorflow/compiler/xla/service/gpu:fusion_merger_test:
==================== Test output for //tensorflow/compiler/xla/service/gpu:fusion_merger_test:
[==========] Running 21 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 21 tests from FusionMergerTest
[ RUN      ] FusionMergerTest.MergeSharedFusionInstruction
[       OK ] FusionMergerTest.MergeSharedFusionInstruction (9 ms)
[ RUN      ] FusionMergerTest.MoreMemoryAccessIfFused
[       OK ] FusionMergerTest.MoreMemoryAccessIfFused (1 ms)
[ RUN      ] FusionMergerTest.LessMemoryAccessIfFused
[       OK ] FusionMergerTest.LessMemoryAccessIfFused (1 ms)
[ RUN      ] FusionMergerTest.WillMergeIntoInputFusion
[       OK ] FusionMergerTest.WillMergeIntoInputFusion (1 ms)
[ RUN      ] FusionMergerTest.WillMergeIntoUnfusedConsumer
[       OK ] FusionMergerTest.WillMergeIntoUnfusedConsumer (1 ms)
[ RUN      ] FusionMergerTest.WillNotMergeReduceUnfriendlyLayouts
[       OK ] FusionMergerTest.WillNotMergeReduceUnfriendlyLayouts (0 ms)
[ RUN      ] FusionMergerTest.WillMergeReduceNotTooUnfriendlyLayouts
[       OK ] FusionMergerTest.WillMergeReduceNotTooUnfriendlyLayouts (1 ms)
[ RUN      ] FusionMergerTest.AvoidsLargeFusion
[       OK ] FusionMergerTest.AvoidsLargeFusion (1 ms)
[ RUN      ] FusionMergerTest.WillNotMergeIfFusionEmitterIsInefficient
[       OK ] FusionMergerTest.WillNotMergeIfFusionEmitterIsInefficient (1 ms)
[ RUN      ] FusionMergerTest.WillMergeSliceIntoReusingConsumer
[       OK ] FusionMergerTest.WillMergeSliceIntoReusingConsumer (0 ms)
[ RUN      ] FusionMergerTest.WillMergeExpensiveFusionsIfSavesMemory
[       OK ] FusionMergerTest.WillMergeExpensiveFusionsIfSavesMemory (1 ms)
[ RUN      ] FusionMergerTest.WillMergeExpensiveFusionsWithSingleConsumer
[       OK ] FusionMergerTest.WillMergeExpensiveFusionsWithSingleConsumer (0 ms)
[ RUN      ] FusionMergerTest.WillNotMergeExpensiveFusionsWithReusingConsumer
[       OK ] FusionMergerTest.WillNotMergeExpensiveFusionsWithReusingConsumer (0 ms)
[ RUN      ] FusionMergerTest.NoMergeWithBitcast
[       OK ] FusionMergerTest.NoMergeWithBitcast (1 ms)
[ RUN      ] FusionMergerTest.CostBasedMerge
[       OK ] FusionMergerTest.CostBasedMerge (1 ms)
[ RUN      ] FusionMergerTest.CostBasedNoMerge
[       OK ] FusionMergerTest.CostBasedNoMerge (4 ms)
[ RUN      ] FusionMergerTest.NoMergeBecauseTooManyBasicBlockSplits
[       OK ] FusionMergerTest.NoMergeBecauseTooManyBasicBlockSplits (4 ms)
[ RUN      ] FusionMergerTest.CommonElementwiseUsedParameter
[       OK ] FusionMergerTest.CommonElementwiseUsedParameter (1 ms)
[ RUN      ] FusionMergerTest.IncompatibleNonTrivialHeroes
[       OK ] FusionMergerTest.IncompatibleNonTrivialHeroes (0 ms)
[ RUN      ] FusionMergerTest.DoNotMergeDUSFusions
[       OK ] FusionMergerTest.DoNotMergeDUSFusions (1 ms)
[ RUN      ] FusionMergerTest.MergeDUSFusionWithElementwiseFusion
tensorflow/compiler/xla/service/gpu/fusion_merger_test.cc:1127: Failure
Value of: fusion_merger_.Run(module.get()).value()
  Actual: false
Expected: true
[  FAILED  ] FusionMergerTest.MergeDUSFusionWithElementwiseFusion (0 ms)
[----------] 21 tests from FusionMergerTest (41 ms total)

[----------] Global test environment tear-down
[==========] 21 tests from 1 test suite ran. (42 ms total)
[  PASSED  ] 20 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] FusionMergerTest.MergeDUSFusionWithElementwiseFusion

 1 FAILED TEST
================================================================================
Target //tensorflow/compiler/xla/service/gpu:fusion_merger_test up-to-date:
  bazel-bin/tensorflow/compiler/xla/service/gpu/fusion_merger_test
INFO: Elapsed time: 392.381s, Critical Path: 276.82s
INFO: 1497 processes: 613 internal, 884 local.
INFO: Build completed, 1 test FAILED, 1497 total actions
//tensorflow/compiler/xla/service/gpu:fusion_merger_test                 FAILED in 1.2s
  /home/andrew/src/tf_test/tensorflow-git/bazel-ci_build-cache/.cache/bazel/_bazel_andrew/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/testlogs/tensorflow/compiler/xla/service/gpu/fusion_merger_test/test.log

Executed 1 out of 1 test: 1 fails locally.
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux']",2023-07-05T16:03:35Z,2,0,https://github.com/tensorflow/tensorflow/issues/61180,
338,tensorflow/tensorflow,//tensorflow/python/distribute:parameter_server_strategy_v2_test_cpu is flaky,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

//tensorflow/python/distribute:parameter_server_strategy_v2_test_cpu timeouts sometimes

x86 log
https://source.cloud.google.com/results/invocations/75eaa47f-92bd-47d5-9c7a-020a0c67dda9/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5399333727/jobs/9806323589#step:5:7344

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
TIMEOUT: //tensorflow/python/distribute:parameter_server_strategy_v2_test_cpu (Summary)
      /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu/test.log
INFO: From Testing //tensorflow/python/distribute:parameter_server_strategy_v2_test_cpu:
==================== Test output for //tensorflow/python/distribute:parameter_server_strategy_v2_test_cpu:
2023-07-04 08:53:48.311167: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-04 08:53:48.373167: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] ClusterTypeNameTest.testArbitraryCurrentTaskType
INFO:tensorflow:Using local port 38369
I0704 08:53:53.254792 140378756265792 test_util.py:3796] Using local port 38369
INFO:tensorflow:Using local port 44357
I0704 08:53:53.255300 140378756265792 test_util.py:3796] Using local port 44357
INFO:tensorflow:Using local port 46287
I0704 08:53:53.255505 140378756265792 test_util.py:3796] Using local port 46287
INFO:tensorflow:time(__main__.ClusterTypeNameTest.testArbitraryCurrentTaskType): 0.0s
I0704 08:53:53.256115 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testArbitraryCurrentTaskType): 0.0s
[       OK ] ClusterTypeNameTest.testArbitraryCurrentTaskType
[ RUN      ] ClusterTypeNameTest.testArbitraryJobName
INFO:tensorflow:Using local port 43625
I0704 08:53:53.256868 140378756265792 test_util.py:3796] Using local port 43625
INFO:tensorflow:Using local port 38445
I0704 08:53:53.257118 140378756265792 test_util.py:3796] Using local port 38445
INFO:tensorflow:Using local port 33687
I0704 08:53:53.257305 140378756265792 test_util.py:3796] Using local port 33687
INFO:tensorflow:Using local port 43947
I0704 08:53:53.257477 140378756265792 test_util.py:3796] Using local port 43947
INFO:tensorflow:time(__main__.ClusterTypeNameTest.testArbitraryJobName): 0.0s
I0704 08:53:53.258199 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testArbitraryJobName): 0.0s
[       OK ] ClusterTypeNameTest.testArbitraryJobName
[ RUN      ] ClusterTypeNameTest.testLessThanOnePs
INFO:tensorflow:Using local port 37051
I0704 08:53:53.258810 140378756265792 test_util.py:3796] Using local port 37051
INFO:tensorflow:Using local port 44491
I0704 08:53:53.259010 140378756265792 test_util.py:3796] Using local port 44491
INFO:tensorflow:time(__main__.ClusterTypeNameTest.testLessThanOnePs): 0.0s
I0704 08:53:53.259491 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testLessThanOnePs): 0.0s
[       OK ] ClusterTypeNameTest.testLessThanOnePs
[ RUN      ] ClusterTypeNameTest.testLessThanOneWorker
INFO:tensorflow:Using local port 33303
I0704 08:53:53.260033 140378756265792 test_util.py:3796] Using local port 33303
INFO:tensorflow:Using local port 33109
I0704 08:53:53.260214 140378756265792 test_util.py:3796] Using local port 33109
INFO:tensorflow:time(__main__.ClusterTypeNameTest.testLessThanOneWorker): 0.0s
I0704 08:53:53.260598 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testLessThanOneWorker): 0.0s
[       OK ] ClusterTypeNameTest.testLessThanOneWorker
[ RUN      ] ClusterTypeNameTest.testMoreThanOneChief
INFO:tensorflow:Using local port 36459
I0704 08:53:53.261005 140378756265792 test_util.py:3796] Using local port 36459
INFO:tensorflow:Using local port 33799
I0704 08:53:53.261153 140378756265792 test_util.py:3796] Using local port 33799
INFO:tensorflow:Using local port 39497
I0704 08:53:53.261297 140378756265792 test_util.py:3796] Using local port 39497
INFO:tensorflow:Using local port 36657
I0704 08:53:53.261423 140378756265792 test_util.py:3796] Using local port 36657
INFO:tensorflow:Using local port 37059
I0704 08:53:53.261545 140378756265792 test_util.py:3796] Using local port 37059
INFO:tensorflow:time(__main__.ClusterTypeNameTest.testMoreThanOneChief): 0.0s
I0704 08:53:53.261911 140378756265792 test_util.py:2464] time(__main__.ClusterTypeNameTest.testMoreThanOneChief): 0.0s
[       OK ] ClusterTypeNameTest.testMoreThanOneChief
[ RUN      ] ClusterTypeNameTest.test_session
[  SKIPPED ] ClusterTypeNameTest.test_session
INFO:tensorflow:Now creating a MultiProcessCluster with num_workers=2, num_ps=3.
I0704 08:53:53.262382 140378756265792 multi_worker_test_base.py:335] Now creating a MultiProcessCluster with num_workers=2, num_ps=3.
INFO:tensorflow:Using local port 34725
I0704 08:53:53.262561 140378756265792 test_util.py:3796] Using local port 34725
INFO:tensorflow:Using local port 41389
I0704 08:53:53.262695 140378756265792 test_util.py:3796] Using local port 41389
INFO:tensorflow:Using local port 42953
I0704 08:53:53.262822 140378756265792 test_util.py:3796] Using local port 42953
INFO:tensorflow:Using local port 39597
I0704 08:53:53.262954 140378756265792 test_util.py:3796] Using local port 39597
INFO:tensorflow:Using local port 40735
I0704 08:53:53.263074 140378756265792 test_util.py:3796] Using local port 40735
2023-07-04 08:53:54.005149: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-04 08:53:54.092559: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-07-04 08:53:54.232593: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-07-04 08:53:54.295324: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
-- Test timed out at 2023-07-04 08:58:46 UTC --
Thread 0x00007fac735de700 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fac765a0700 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fac025a4700 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fabffda3700 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 527 in _process_watchdog
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fabff5a2700 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Current thread 0x00007fac79e6b740 (most recent call first):
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 379 in _recv
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 414 in _recv_bytes
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 250 in recv
  File ""/usr/lib/python3.9/multiprocessing/managers.py"", line 810 in _callmethod
  File ""/usr/lib/python3.9/multiprocessing/managers.py"", line 1085 in wait
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_worker_test_base.py"", line 270 in start
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_worker_test_base.py"", line 348 in create_multi_process_cluster
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/parameter_server_strategy_v2_test.py"", line 62 in setUpClass
  File ""/usr/lib/python3.9/unittest/suite.py"", line 166 in _handleClassSetUp
  File ""/usr/lib/python3.9/unittest/suite.py"", line 114 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/runner.py"", line 184 in run
  File ""/usr/lib/python3.9/unittest/main.py"", line 271 in runTests
  File ""/usr/lib/python3.9/unittest/main.py"", line 101 in __init__
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/app.py"", line 258 in _run_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/absl_py/absl/app.py"", line 312 in run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/test.py"", line 25 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 167 in test_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 1455 in test_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/parameter_server_strategy_v2_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/parameter_server_strategy_v2_test.py"", line 713 in <module>
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux']",2023-07-04T10:39:13Z,1,0,https://github.com/tensorflow/tensorflow/issues/61169,
339,tensorflow/tensorflow,//tensorflow/python/data/experimental/kernel_tests/service:worker_tags_test is flaky,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

//tensorflow/python/data/experimental/kernel_tests/service:worker_tags_test fails occasionally

x86 log
https://source.cloud.google.com/results/invocations/0bc426bf-e5ae-4fb6-993f-6199c00d1139/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5411569978/jobs/9834485829#step:5:8675

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
======================================================================
ERROR: testMultipleTags_test_mode_eager_tfapiversion_2 (__main__.WorkerTagsTest)
WorkerTagsTest.testMultipleTags_test_mode_eager_tfapiversion_2
testMultipleTags_test_mode_eager_tfapiversion_2(mode='eager', tf_api_version=2)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test
    return test_method(self, **testcase_params)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360, in decorated
    execute_test_method()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343, in execute_test_method
    test_method(**kwargs_to_pass)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.py"", line 179, in testMultipleTags
    cluster = multi_process_cluster.MultiProcessCluster(
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 81, in __init__
    self._start_local_workers(num_local_workers, worker_tags)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 100, in _start_local_workers
    self.start_local_worker(worker_tags)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 114, in start_local_worker
    worker.start()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/test_base.py"", line 110, in start
    self._server.start()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/service/server_lib.py"", line 415, in start
    self._server.start()
NotImplementedError: Failed to get dispatcher version from dispatcher running at localhost:45069:

----------------------------------------------------------------------
Ran 6 tests in 6.597s

FAILED (errors=1)
Exception ignored in: <function MultiProcessCluster.__del__ at 0x7f20f09658b0>
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 160, in __del__
    self._stop()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/worker_tags_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 155, in _stop
    for (_, worker_process) in self._remote_workers:
AttributeError: 'MultiProcessCluster' object has no attribute '_remote_workers'
2023-07-02 09:48:51.833944: I tensorflow/core/data/service/server_lib.cc:94] Shut down DispatchServer server running at port 45069
2023-07-02 09:48:51.934278: I tensorflow/core/data/service/server_lib.cc:94] Shut down WorkerServer server running at port 44313
================================================================================
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux']",2023-07-03T16:32:51Z,1,0,https://github.com/tensorflow/tensorflow/issues/61166,
340,tensorflow/tensorflow,//tensorflow/dtensor/mlir/tests:spmd_expansion.mlir.test is flaky,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

git HEAD

### Custom code

No

### OS platform and distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current behavior?

//tensorflow/dtensor/mlir/tests:spmd_expansion.mlir.test unit test fails occasionally

x86 log
https://source.cloud.google.com/results/invocations/88910868-abeb-4aa0-8954-df2b79ef5a26/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5420661339/jobs/9855097285

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=rbe --config=pycpp --config=build_event_export
```


### Relevant log output

```shell
FAIL: //tensorflow/dtensor/mlir/tests:spmd_expansion.mlir.test (see /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test/test.log)
INFO: From Testing //tensorflow/dtensor/mlir/tests:spmd_expansion.mlir.test:
==================== Test output for //tensorflow/dtensor/mlir/tests:spmd_expansion.mlir.test:
-- Testing: 1 tests, 1 workers --
FAIL: MLIR tests :: spmd_expansion.mlir (1 of 1)
******************** TEST 'MLIR tests :: spmd_expansion.mlir' FAILED ********************
Script:
--
: 'RUN: at line 1';   /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/dtensor-opt -- /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir -split-input-file -dtensor-annotate-global-shape -dtensor-spmd-expansion -verify-diagnostics | /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/llvm-project/llvm/FileCheck /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir
--
Exit Code: 1

Command Output (stderr):
--
2023-07-03 09:20:51.133345: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
TensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.
Stack dump:
0.	Program arguments: /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/dtensor-opt /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/dtensor-opt /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir -split-input-file -dtensor-annotate-global-shape -dtensor-spmd-expansion -verify-diagnostics
1.	Program arguments: /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/dtensor-opt /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir -split-input-file -dtensor-annotate-global-shape -dtensor-spmd-expansion -verify-diagnostics
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
0  libtensorflow_framework.so.2 0x00007f49dfe75e6e llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 46
1  libtensorflow_framework.so.2 0x00007f49dfe73f07 llvm::sys::RunSignalHandlers() + 87
2  libtensorflow_framework.so.2 0x00007f49dfe76692
3  libpthread.so.0              0x00007f49dea05420
4  dtensor-opt                  0x0000562b92fdd978
5  dtensor-opt                  0x0000562b92fdd11a
6  dtensor-opt                  0x0000562b92fde2b9
7  dtensor-opt                  0x0000562b92f94499
8  dtensor-opt                  0x0000562b92fb7b91
9  dtensor-opt                  0x0000562b92fb85bf
10 dtensor-opt                  0x0000562b92eca421
11 dtensor-opt                  0x0000562b94e800d4
12 dtensor-opt                  0x0000562b94e82d17
13 dtensor-opt                  0x0000562b94e82ae5
14 dtensor-opt                  0x0000562b948df256
15 dtensor-opt                  0x0000562b948de53a
16 dtensor-opt                  0x0000562b9502d603
17 dtensor-opt                  0x0000562b9502d34e
18 dtensor-opt                  0x0000562b948da857
19 dtensor-opt                  0x0000562b948dad3d
20 dtensor-opt                  0x0000562b92d80fcc
21 libc.so.6                    0x00007f49de626083 __libc_start_main + 243
22 dtensor-opt                  0x0000562b92d80d39
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
0  libtensorflow_framework.so.2 0x00007f49dfe75e6e llvm::sys::PrintStackTrace(llvm::raw_ostream&, int) + 46
1  libtensorflow_framework.so.2 0x00007f49dfe73f37 llvm::sys::RunSignalHandlers() + 135
2  libtensorflow_framework.so.2 0x00007f49dfe76692
3  libpthread.so.0              0x00007f49dea05420
4  dtensor-opt                  0x0000562b92fdd978
5  dtensor-opt                  0x0000562b92fdd11a
6  dtensor-opt                  0x0000562b92fde2b9
7  dtensor-opt                  0x0000562b92f94499
8  dtensor-opt                  0x0000562b92fb7b91
9  dtensor-opt                  0x0000562b92fb85bf
10 dtensor-opt                  0x0000562b92eca421
11 dtensor-opt                  0x0000562b94e800d4
12 dtensor-opt                  0x0000562b94e82d17
13 dtensor-opt                  0x0000562b94e82ae5
14 dtensor-opt                  0x0000562b948df256
15 dtensor-opt                  0x0000562b948de53a
16 dtensor-opt                  0x0000562b9502d603
17 dtensor-opt                  0x0000562b9502d34e
18 dtensor-opt                  0x0000562b948da857
19 dtensor-opt                  0x0000562b948dad3d
20 dtensor-opt                  0x0000562b92d80fcc
21 libc.so.6                    0x00007f49de626083 __libc_start_main + 243
22 dtensor-opt                  0x0000562b92d80d39
/b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir:277:17: error: CHECK-LABEL: expected string not found in input
// CHECK-LABEL: module @test_spmd_softmax_rank_3
                ^
<stdin>:153:45: note: scanning from here
module @test_spmd_softmax_last_dim_unsharded {
                                            ^
<stdin>:156:303: note: possible intended match here
 %0 = ""tf.Softmax""(%arg0) {_global_shape = [#tf_type.shape<32x32>], _layout = [""sharding_specs:x,unsharded, mesh:TPU|x=2,y=2|0,1,2,3|0,1,2,3|/job:localhost/task:0/device:TPU:0,/job:localhost/task:0/device:TPU:1,/job:localhost/task:0/device:TPU:2,/job:localhost/task:0/device:TPU:3""]} : (tensor<16x32xf32>) -> tensor<16x32xf32>
                                                                                                                                                                                                                                                                                                              ^

Input file: <stdin>
Check file: /b/f/w/bazel-out/k8-opt/bin/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir.test.runfiles/org_tensorflow/tensorflow/dtensor/mlir/tests/spmd_expansion.mlir

-dump-input=help explains the following input dump.

Input was:
<<<<<<
             .
             .
             .
           148:  }
           149: }
           150:
           151:
           152: // -----
           153: module @test_spmd_softmax_last_dim_unsharded {
label:277'0                                                 X~~ error: no match found
           154:  func.func @main(%arg0: tensor<16x32xf32> {tf._global_shape = #tf_type.shape<32x32>, tf._layout = ""sharding_specs:x,unsharded, mesh:TPU|x=2,y=2|0,1,2,3|0,1,2,3|/job:localhost/task:0/device:TPU:0,/job:localhost/task:0/device:TPU:1,/job:localhost/task:0/device:TPU:2,/job:localhost/task:0/device:TPU:3""}) {
label:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           155:  ""tf_device.cluster""() ({
label:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~
           156:  %0 = ""tf.Softmax""(%arg0) {_global_shape = [#tf_type.shape<32x32>], _layout = [""sharding_specs:x,unsharded, mesh:TPU|x=2,y=2|0,1,2,3|0,1,2,3|/job:localhost/task:0/device:TPU:0,/job:localhost/task:0/device:TPU:1,/job:localhost/task:0/device:TPU:2,/job:localhost/task:0/device:TPU:3""]} : (tensor<16x32xf32>) -> tensor<16x32xf32>
label:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
label:277'1                                                                                                                                                                                                                                                                                                                   ?                         possible intended match
           157:  tf_device.return {_global_shape = [], _layout = []}
label:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           158:  }) {_global_shape = [], _mesh = ""TPU|x=2,y=2|0,1,2,3|0,1,2,3|/job:localhost/task:0/device:TPU:0,/job:localhost/task:0/device:TPU:1,/job:localhost/task:0/device:TPU:2,/job:localhost/task:0/device:TPU:3""} : () -> ()
label:277'0     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
           159:  return
label:277'0     ~~~~~~~~
           160:  }
label:277'0     ~~~
           161: }
label:277'0     ~~
           162:
label:277'0     ~
>>>>>>

--

********************
********************
Failed Tests (1):
  MLIR tests :: spmd_expansion.mlir
```
","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux']",2023-07-03T16:06:21Z,1,0,https://github.com/tensorflow/tensorflow/issues/61164,
341,tensorflow/tensorflow,Vectorizing a mapping function containing tf.io.read_file in tf.data pipeline is not working.,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

No

### Source

source

### TensorFlow version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom code

Yes

### OS platform and distribution

RHEL8 .8

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

I have a function that reads input files, does some processing and returns the files. Upon using that function to do processing without batching first works fine. i.e. `dataset.map(some_func).batch(batch_size)`

I was trying to speed up the data pipeline by vectorizing the processing part by batching the dataset and then mapping it to the tf.py_function i.e `dataset.batch(batch_size).map(tf.py_func_wrapped_function)`

I followed the tensorflow guide for [optimizing pipeline performance](https://www.tensorflow.org/guide/data_performance#vectorizing_mapping)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import pathlib
import os
import matplotlib.pyplot as plt

flowers = tf.keras.utils.get_file(
    'flower_photos',
    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    untar=True)

flowers = pathlib.Path(flowers)


list_ds = tf.data.Dataset.list_files(str(flowers/'*/*'),shuffle=False)

list_ds

for f in list_ds.take(10):
    print(f.numpy())


# Reads an image from a file, decodes it into a dense tensor, and resizes it
# to a fixed shape.
def parse_image(filename):
  parts = tf.strings.split(filename, os.sep)
  label = parts[-2]

  image = tf.io.read_file(filename)
  image = tf.image.decode_jpeg(image)
  image = tf.image.convert_image_dtype(image, tf.float32)
  image = tf.image.resize(image, [128, 128])
  return image, label


dataset1 = list_ds.map(parse_image).batch(32)#map then batch, scalar mapping

el = next(iter(dataset1))

plt.imshow(el[0][0])
plt.title(el[1][0].numpy().decode('utf-8'))

dataset2 = list_ds.batch(32).map(parse_image) #batch then map(vectorized mapping), should work
```


### Relevant log output

```shell
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[38], line 1
----> 1 dataset2 = list_ds.batch(32).map(parse_image) #batch then map(vectorized mapping), should work

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2240, in DatasetV2.map(self, map_func, num_parallel_calls, deterministic, name)
   2236 # Loaded lazily due to a circular dependency (dataset_ops -> map_op ->
   2237 # dataset_ops).
   2238 # pylint: disable=g-import-not-at-top,protected-access
   2239 from tensorflow.python.data.ops import map_op
-> 2240 return map_op._map_v2(
   2241     self,
   2242     map_func,
   2243     num_parallel_calls=num_parallel_calls,
   2244     deterministic=deterministic,
   2245     name=name)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:37, in _map_v2(input_dataset, map_func, num_parallel_calls, deterministic, name)
     34   if deterministic is not None and not debug_mode.DEBUG_MODE:
     35     warnings.warn(""The `deterministic` argument has no effect unless the ""
     36                   ""`num_parallel_calls` argument is specified."")
---> 37   return _MapDataset(
     38       input_dataset, map_func, preserve_cardinality=True, name=name)
     39 else:
     40   return _ParallelMapDataset(
     41       input_dataset,
     42       map_func,
   (...)
     45       preserve_cardinality=True,
     46       name=name)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:107, in _MapDataset.__init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)
    105 self._use_inter_op_parallelism = use_inter_op_parallelism
    106 self._preserve_cardinality = preserve_cardinality
--> 107 self._map_func = structured_function.StructuredFunctionWrapper(
    108     map_func,
    109     self._transformation_name(),
    110     dataset=input_dataset,
    111     use_legacy_function=use_legacy_function)
    112 self._name = name
    113 variant_tensor = gen_dataset_ops.map_dataset(
    114     input_dataset._variant_tensor,  # pylint: disable=protected-access
    115     self._map_func.function.captured_inputs,
   (...)
    118     preserve_cardinality=self._preserve_cardinality,
    119     **self._common_args)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:261, in StructuredFunctionWrapper.__init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
    254       warnings.warn(
    255           ""Even though the `tf.config.experimental_run_functions_eagerly` ""
    256           ""option is set, this option does not apply to tf.data functions. ""
    257           ""To force eager execution of tf.data functions, please use ""
    258           ""`tf.data.experimental.enable_debug_mode()`."")
    259     fn_factory = trace_tf_function(defun_kwargs)
--> 261 self._function = fn_factory()
    262 # There is no graph to add in eager mode.
    263 add_to_graph &= not context.executing_eagerly()

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:232, in TracingCompiler.get_concrete_function(self, *args, **kwargs)
    223 def get_concrete_function(self, *args, **kwargs):
    224   """"""Returns a `ConcreteFunction` specialized to inputs and execution context.
    225 
    226   Args:
   (...)
    230       `tf.Tensor` or `tf.TensorSpec`.
    231   """"""
--> 232   concrete_function = self._get_concrete_function_garbage_collected(
    233       *args, **kwargs)
    234   concrete_function._garbage_collector.release()  # pylint: disable=protected-access
    235   return concrete_function

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:202, in TracingCompiler._get_concrete_function_garbage_collected(self, *args, **kwargs)
    199   self._function_spec.make_canonicalized_monomorphic_type(args, kwargs)
    201 with self._lock:
--> 202   concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
    203   seen_names = set()
    204   concrete_function._arg_keywords = []  # pylint: disable=protected-access

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:166, in TracingCompiler._maybe_define_concrete_function(self, args, kwargs)
    163   args = self.input_signature
    164   kwargs = {}
--> 166 return self._maybe_define_function(args, kwargs)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:396, in TracingCompiler._maybe_define_function(self, args, kwargs)
    393   args = placeholder_bound_args.args
    394 kwargs = placeholder_bound_args.kwargs
--> 396 concrete_function = self._create_concrete_function(
    397     args, kwargs, func_graph)
    399 # TODO(b/263520817): Remove access to private attribute.
    400 graph_capture_container = concrete_function.graph._function_captures  # pylint: disable=protected-access

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:300, in TracingCompiler._create_concrete_function(self, args, kwargs, func_graph)
    296 else:
    297   arg_names = base_arg_names
    299 concrete_function = monomorphic_function.ConcreteFunction(
--> 300     func_graph_module.func_graph_from_py_func(
    301         self._name,
    302         self._python_function,
    303         args,
    304         kwargs,
    305         None,
    306         func_graph=func_graph,
    307         autograph=self._autograph,
    308         autograph_options=self._autograph_options,
    309         arg_names=arg_names,
    310         capture_by_value=self._capture_by_value,
    311         create_placeholders=False),
    312     self._function_attributes,
    313     spec=self.function_spec,
    314     # Tell the ConcreteFunction to clean up its graph once it goes out of
    315     # scope. This is not the default behavior since it gets used in some
    316     # places (like Keras) where the FuncGraph lives longer than the
    317     # ConcreteFunction.
    318     shared_func_graph=False)
    319 return concrete_function

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1214, in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)
   1211 else:
   1212   _, original_func = tf_decorator.unwrap(python_func)
-> 1214 func_outputs = python_func(*func_args, **func_kwargs)
   1216 # invariant: `func_outputs` contains only Tensors, CompositeTensors,
   1217 # TensorArrays and `None`s.
   1218 func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:238, in StructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn(*args)
    232 @eager_function.defun_with_attributes(
    233     input_signature=structure.get_flat_tensor_specs(
    234         self._input_structure),
    235     autograph=False,
    236     attributes=defun_kwargs)
    237 def wrapped_fn(*args):  # pylint: disable=missing-docstring
--> 238   ret = wrapper_helper(*args)
    239   ret = structure.to_tensor_list(self._output_structure, ret)
    240   return [ops.convert_to_tensor(t) for t in ret]

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:169, in StructuredFunctionWrapper.__init__.<locals>.wrapper_helper(*args)
    167 if not _should_unpack(nested_args):
    168   nested_args = (nested_args,)
--> 169 ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
    170 ret = variable_utils.convert_variables_to_tensors(ret)
    171 if _should_pack(ret):

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:692, in convert.<locals>.decorator.<locals>.wrapper(*args, **kwargs)
    690 except Exception as e:  # pylint:disable=broad-except
    691   if hasattr(e, 'ag_error_metadata'):
--> 692     raise e.ag_error_metadata.to_exception(e)
    693   else:
    694     raise

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689, in convert.<locals>.decorator.<locals>.wrapper(*args, **kwargs)
    687 try:
    688   with conversion_ctx:
--> 689     return converted_call(f, args, kwargs, options=options)
    690 except Exception as e:  # pylint:disable=broad-except
    691   if hasattr(e, 'ag_error_metadata'):

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439, in converted_call(f, args, kwargs, caller_fn_scope, options)
    437 try:
    438   if kwargs is not None:
--> 439     result = converted_f(*effective_args, **kwargs)
    440   else:
    441     result = converted_f(*effective_args)

File /tmp/__autograph_generated_filezgyxt99w.py:12, in outer_factory.<locals>.inner_factory.<locals>.tf__parse_image(filename)
     10 parts = ag__.converted_call(ag__.ld(tf).strings.split, (ag__.ld(filename), ag__.ld(os).sep), None, fscope)
     11 label = ag__.ld(parts)[-2]
---> 12 image = ag__.converted_call(ag__.ld(tf).io.read_file, (ag__.ld(filename),), None, fscope)
     13 image = ag__.converted_call(ag__.ld(tf).image.decode_jpeg, (ag__.ld(image),), None, fscope)
     14 image = ag__.converted_call(ag__.ld(tf).image.convert_image_dtype, (ag__.ld(image), ag__.ld(tf).float32), None, fscope)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:331, in converted_call(f, args, kwargs, caller_fn_scope, options)
    329 if conversion.is_in_allowlist_cache(f, options):
    330   logging.log(2, 'Allowlisted %s: from cache', f)
--> 331   return _call_unconverted(f, args, kwargs, options, False)
    333 if ag_ctx.control_status_ctx().status == ag_ctx.Status.DISABLED:
    334   logging.log(2, 'Allowlisted: %s: AutoGraph is disabled in context', f)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459, in _call_unconverted(f, args, kwargs, options, update_cache)
    457 if kwargs is not None:
    458   return f(*args, **kwargs)
--> 459 return f(*args)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/ops/io_ops.py:133, in read_file(filename, name)
     96 @tf_export(""io.read_file"", v1=[""io.read_file"", ""read_file""])
     97 def read_file(filename, name=None):
     98   """"""Reads the contents of file.
     99 
    100   This operation returns a tensor with the entire contents of the input
   (...)
    131     A tensor of dtype ""string"", with the file contents.
    132   """"""
--> 133   return gen_io_ops.read_file(filename, name)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/ops/gen_io_ops.py:586, in read_file(filename, name)
    584     pass  # Add nodes to the TensorFlow graph.
    585 # Add nodes to the TensorFlow graph.
--> 586 _, _, _op, _outputs = _op_def_library._apply_op_helper(
    587       ""ReadFile"", filename=filename, name=name)
    588 _result = _outputs[:]
    589 if _execute.must_record_gradient():

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py:795, in _apply_op_helper(op_type_name, name, **keywords)
    790 must_colocate_inputs = [val for arg, val in zip(op_def.input_arg, inputs)
    791                         if arg.is_ref]
    792 with _MaybeColocateWith(must_colocate_inputs):
    793   # Add Op to graph
    794   # pylint: disable=protected-access
--> 795   op = g._create_op_internal(op_type_name, inputs, dtypes=None,
    796                              name=scope, input_types=input_types,
    797                              attrs=attr_protos, op_def=op_def)
    799 # `outputs` is returned as a separate return value so that the output
    800 # tensors can the `op` per se can be decoupled so that the
    801 # `op_callbacks` can function properly. See framework/op_callbacks.py
    802 # for more details.
    803 outputs = op.outputs

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:707, in FuncGraph._create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
    705   inp = self.capture(inp)
    706   captured_inputs.append(inp)
--> 707 return super()._create_op_internal(  # pylint: disable=protected-access
    708     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,
    709     compute_device)

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:3814, in Graph._create_op_internal(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)
   3811 # _create_op_helper mutates the new Operation. `_mutation_lock` ensures a
   3812 # Session.run call cannot occur between creating and mutating the op.
   3813 with self._mutation_lock():
-> 3814   ret = Operation(
   3815       node_def,
   3816       self,
   3817       inputs=inputs,
   3818       output_types=dtypes,
   3819       control_inputs=control_inputs,
   3820       input_types=input_types,
   3821       original_op=self._default_original_op,
   3822       op_def=op_def)
   3823   self._create_op_helper(ret, compute_device=compute_device)
   3824 return ret

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:2112, in Operation.__init__(***failed resolving arguments***)
   2109     control_input_ops.append(control_op)
   2111 # Initialize c_op from node_def and other inputs
-> 2112 c_op = _create_c_op(g, node_def, inputs, control_input_ops, op_def=op_def)
   2113 self._init_from_c_op(c_op=c_op, g=g)
   2115 self._original_op = original_op

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    151 except Exception as e:
    152   filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153   raise e.with_traceback(filtered_tb) from None
    154 finally:
    155   del filtered_tb

File ~/anaconda3/envs/tf2_12/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1973, in _create_c_op(graph, node_def, inputs, control_inputs, op_def, extract_traceback)
   1970   c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
   1971 except errors.InvalidArgumentError as e:
   1972   # Convert to ValueError for backwards compatibility.
-> 1973   raise ValueError(e.message)
   1975 # Record the current Python stack trace as the creating stacktrace of this
   1976 # TF_Operation.
   1977 if extract_traceback:

ValueError: in user code:

    File ""/tmp/ipykernel_26855/3929380215.py"", line 28, in parse_image  *
        image = tf.io.read_file(filename)

    ValueError: Shape must be rank 0 but is rank 1 for '{{node ReadFile}} = ReadFile[](args_0)' with input shapes: [?].
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'comp:data', 'TF 2.12']",2023-07-03T13:08:00Z,1,0,https://github.com/tensorflow/tensorflow/issues/61162,
342,tensorflow/tensorflow,[MLIR] Fix tf.StridedSlice lowering to tosa with new_axis_mask/shrink_axis_mask,"### Issue type

Bug

### Have you reproduced the bug with TensorFlow Nightly?

Yes

### Source

source

### TensorFlow version

tf 2.13.0rc2

### Custom code

Yes

### OS platform and distribution

Linux Ubunto 18.04

### Mobile device

_No response_

### Python version

3.9.1

### Bazel version

6.1.2

### GCC/compiler version

clang 15.0.2

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current behavior?

The following MLIR input is not support:

```
func.func @test_strided_slice_new_axis_mask(%arg0: tensor<1x14x8xf32>) -> tensor<1x14x8x1xf32> {
  %strides = ""tf.Const""() {device = """", value = dense<1> : tensor<4xi32>} : () -> tensor<4xi32>
  %begin_end = ""tf.Const""() {device = """", value = dense<0> : tensor<4xi32>} : () -> tensor<4xi32>
  %res = ""tf.StridedSlice""(%arg0, %begin_end, %begin_end, %strides) {begin_mask = 7 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 8 : i64, shrink_axis_mask = 0 : i64} : (tensor<1x14x8xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<1x14x8x1xf32>
  func.return %res : tensor<1x14x8x1xf32>
}

// -----

func.func @test_strided_slice_shrink_axis_mask(%arg0: tensor<1x14x8x1xf32>) -> tensor<1x14x8xf32> {
  %strides = ""tf.Const""() {device = """", value = dense<1> : tensor<4xi32>} : () -> tensor<4xi32>
  %begin = ""tf.Const""() {device = """", value = dense<0> : tensor<4xi32>} : () -> tensor<4xi32>
  %end = ""tf.Const""() {device = """", value = dense<[0, 0, 0, 1]> : tensor<4xi32>} : () -> tensor<4xi32>
  %res = ""tf.StridedSlice""(%arg0, %begin, %end, %strides) {begin_mask = 7 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 7 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 8 : i64} : (tensor<1x14x8x1xf32>, tensor<4xi32>, tensor<4xi32>, tensor<4xi32>) -> tensor<1x14x8xf32>
  func.return %res : tensor<1x14x8xf32>
}
```

### Standalone code to reproduce the issue

```shell
Use MLIR new tests in https://github.com/tensorflow/tensorflow/pull/60939 to see the error.
```


### Relevant log output

```shell
I create pull request to fix the issue:
https://github.com/tensorflow/tensorflow/pull/60939
```
","['type:bug', 'comp:ops', 'awaiting PR merge', 'TF 2.13']",2023-07-03T07:56:16Z,3,0,https://github.com/tensorflow/tensorflow/issues/61157,
343,tensorflow/tensorflow,Internal error: Error applying delegate when trying to use TensorFlow Lite NNAPI delegate on Google Pixel 7,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tensorflow-lite 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

Google Pixel 7

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When I try to use the NNAPI delegate to run the OpenAI whisper model on a Pixel 7, it crashes when trying to initialize the TFLite interpreter.

### Standalone code to reproduce the issue

```shell
MainActivity.kt
class MainActivity : ComponentActivity() {
    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)

        setContent {
            NNAPIWhisperTestTheme {
                // A surface container using the 'background' color from the theme
                Surface(
                    modifier = Modifier.fillMaxSize(),
                    color = MaterialTheme.colorScheme.background
                ) {
                    Test(applicationContext)
                }
            }
        }
    }
}

@Composable
fun Test(applicationContext: Context, modifier: Modifier = Modifier, viewModel: NnApiViewModel = viewModel()) {
    Scaffold { innerPadding ->
        Column {
            Button(modifier = Modifier.padding(innerPadding), onClick = { viewModel.initialize(applicationContext) }) {
                Text(text = ""INITIALIZE"")
            }
            Button(
                onClick = {
                    viewModel.runInference()
                },
                modifier = Modifier.padding(innerPadding)
            ) {
                Text(viewModel.output + viewModel.elapsed)
            }
        }
    }
}



NnApiViewModel.kt
class NnApiViewModel : ViewModel() {

    var options = Interpreter.Options()
    var nnApiDelegate: NnApiDelegate? = null
    var tfLite: Interpreter? = null
    var output by mutableStateOf("""")
    var elapsed by mutableStateOf(0L)

    // Initialize interpreter with NNAPI delegate for Android Pie or above
    fun initialize(applicationContext: Context) {
        nnApiDelegate = NnApiDelegate()
        options.useNNAPI = true
        NnApiDelegate.Options().useNnapiCpu = false
        options.addDelegate(nnApiDelegate)

        val model = applicationContext.assets.open(""models/whisper-tiny.tflite"")
        val file = model.readBytes()

        val fileName = ""whisper-tiny.tflite""
        applicationContext.openFileOutput(fileName, Context.MODE_PRIVATE).use {
            it.write(file)
        }

        val modelFile = File(applicationContext.filesDir,""whisper-tiny.tflite"")

        // Initialize TFLite interpreter
        try {
            tfLite = Interpreter(modelFile, options)
        } catch (e: Exception) {
            throw RuntimeException(e)
        }
    }

    fun runInference() {
        try {
            val outputShape = tfLite?.getOutputTensor(0)
            val input = TensorBuffer.createFixedSize(intArrayOf(1, 80, 3000), DataType.FLOAT32)
            val output = TensorBuffer.createFixedSize(outputShape?.shape(), DataType.FLOAT32)

            val start = System.currentTimeMillis()
            tfLite?.run(input.buffer, output.buffer)
            elapsed = System.currentTimeMillis() - start
        } catch (e: RuntimeException) {
            throw RuntimeException(e)
        }
    }

    fun unload() {
        tfLite?.close()
        nnApiDelegate?.close()
    }
}
```


### Relevant log output

```shell
I  Loaded native library: tensorflowlite_jni
I  Didn't load native library: tensorflowlite_jni_gms_client
I  Initialized TensorFlow Lite runtime.
I  DeviceManager::DeviceManager
I  findAvailableDevices
E  Error opening trace file: No such file or directory (2)
I  Found interface google-edgetpu (version = 2.0)
I  Found interface google-armnn (version = ArmNN)
I  Created TensorFlow Lite delegate for NNAPI.
E  FATAL EXCEPTION: main                                                                                                                                                                                                   java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Error applying delegate:
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'TFLiteNNAPIDelegate', 'TF 2.12']",2023-06-30T04:21:02Z,18,0,https://github.com/tensorflow/tensorflow/issues/61126,
344,tensorflow/tensorflow,`tf.math.reduce_prod` produces wrong second-order gradient,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0-dev20230628

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

`tf.math.reduce_prod` produces wrong second-order gradient when the input tensor has a `0` element. 

In the following example, `d2z_dydx` should be `1`, but the output value is `0`. Note that, if we replace `z = tf.math.reduce_prod(tf.stack([x, y]))` with `z = x * y`, the assertion passes.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.Variable(0.0)
y = tf.Variable(1.0)
with tf.GradientTape(persistent=True) as t1:
    t1.watch(x)
    t1.watch(y)
    with tf.GradientTape(persistent=True) as t2:
        t2.watch(x)
        t2.watch(y)
        z = tf.math.reduce_prod(tf.stack([x, y]))
    print('z = x * y: ', z)  
    dz_dx = t2.gradient(z, x) # dz_dx = y
    print('dz_dx: ', dz_dx)
    dz_dy = t2.gradient(z, y) # dz_dy = x
    print('dz_dy: ', dz_dy)

d2z_dx2 = t1.gradient(dz_dx, x) # d2z_dx2 = 0
print('d2z_dx2', d2z_dx2)
d2z_dxdy = t1.gradient(dz_dx, y) # d2z_dxdy = 1
print('d2z_dxdy', d2z_dxdy)
assert d2z_dxdy == 1
d2z_dydx = t1.gradient(dz_dy, x) # d2z_dydx = 1
print('d2z_dydx', d2z_dydx)
assert d2z_dydx == 1, 'd2z_dydx = {}'.format(d2z_dydx)
# AssertionError: d2z_dydx = 0.0
```


### Relevant log output

```shell
AssertionError: d2z_dydx = 0.0
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-06-28T16:17:39Z,1,0,https://github.com/tensorflow/tensorflow/issues/61117,
345,tensorflow/tensorflow,//tensorflow/python/distribute/experimental/rpc:rpc_ops_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute/experimental/rpc:rpc_ops_test sometimes fails.

x86 log
https://source.cloud.google.com/results/invocations/e7ac5e31-66d5-4f2e-a095-045cb52cc20f/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5348094575/jobs/9697499180#step:5:8263

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
FAIL: //tensorflow/python/distribute/experimental/rpc:rpc_ops_test (shard 4 of 7) (see /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/distribute/experimental/rpc/rpc_ops_test/shard_4_of_7/test.log)
INFO: From Testing //tensorflow/python/distribute/experimental/rpc:rpc_ops_test (shard 4 of 7):
==================== Test output for //tensorflow/python/distribute/experimental/rpc:rpc_ops_test (shard 4 of 7):
2023-06-27 21:51:01.865535: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-27 21:51:01.915964: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] RpcOpsTest.test_client_timeout
E0627 21:51:07.067311728 3191958 server_chttp2.cc:40]        {""created"":""@1687902667.067282680"",""description"":""Only 1 addresses added out of total 2 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":404,""referenced_errors"":[{""created"":""@1687902667.067278835"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::1]:34603""}]}
2023-06-27 21:51:07.067457: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:347] Server listening on: localhost:34603
2023-06-27 21:51:08.565889: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:314] Shutting down server listening on: localhost:34603
INFO:tensorflow:time(__main__.RpcOpsTest.test_client_timeout): 2.71s
I0627 21:51:08.574481 139721505019712 test_util.py:2464] time(__main__.RpcOpsTest.test_client_timeout): 2.71s
[  FAILED  ] RpcOpsTest.test_client_timeout
[ RUN      ] RpcOpsTest.test_queue_resource
/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/python/saved_model/nested_structure_coder.py:458: UserWarning: Encoding a StructuredValue with type tf.NoneTensorSpec; loading this StructuredValue will require that this type be imported and registered.
  warnings.warn(""Encoding a StructuredValue with type %s; loading this ""
E0627 21:51:08.630379336 3163442 server_chttp2.cc:40]        {""created"":""@1687902668.630352103"",""description"":""Only 1 addresses added out of total 2 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":404,""referenced_errors"":[{""created"":""@1687902668.630348956"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::1]:43377""}]}
2023-06-27 21:51:08.630532: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:347] Server listening on: localhost:43377
2023-06-27 21:51:08.686495: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:314] Shutting down server listening on: localhost:43377
INFO:tensorflow:time(__main__.RpcOpsTest.test_queue_resource): 0.14s
I0627 21:51:08.712013 139721505019712 test_util.py:2464] time(__main__.RpcOpsTest.test_queue_resource): 0.14s
[       OK ] RpcOpsTest.test_queue_resource
[ RUN      ] RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods
E0627 21:51:08.774584099 3163442 server_chttp2.cc:40]        {""created"":""@1687902668.774551355"",""description"":""Only 1 addresses added out of total 2 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":404,""referenced_errors"":[{""created"":""@1687902668.774547071"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::1]:39177""}]}
2023-06-27 21:51:08.774736: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:347] Server listening on: localhost:39177
2023-06-27 21:51:08.798979: I tensorflow/distribute/experimental/rpc/kernels/rpc_ops.cc:314] Shutting down server listening on: localhost:39177
INFO:tensorflow:time(__main__.RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods): 0.09s
I0627 21:51:08.802805 139721505019712 test_util.py:2464] time(__main__.RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods): 0.09s
[       OK ] RpcOpsTest.test_rpc_ops_non_blocking_convenience_methods
======================================================================
ERROR: test_client_timeout (__main__.RpcOpsTest)
RpcOpsTest.test_client_timeout
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.py"", line 487, in test_client_timeout
    client = rpc_ops.GrpcClient(
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/python/distribute/experimental/rpc/rpc_ops.py"", line 342, in __init__
    self._client_handle, methods = gen_rpc_ops.rpc_client(
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/distribute/experimental/rpc/kernels/gen_rpc_ops.py"", line 333, in rpc_client
    return rpc_client_eager_fallback(
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/distribute/experimental/rpc/kernels/gen_rpc_ops.py"", line 407, in rpc_client_eager_fallback
    _result = _execute.execute(b""RpcClient"", 2, inputs=_inputs_flat,
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/experimental/rpc/rpc_ops_test.runfiles/org_tensorflow/tensorflow/python/eager/execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnavailableError: {{function_node __wrapped__RpcClient_device_/job:localhost/replica:0/task:0/device:CPU:0}} GOAWAY received
Additional GRPC error information while calling /tensorflow.rpc.RpcService/List:
:{""created"":""@1687902668.574079798"",""description"":""Error received from peer ipv4:127.0.0.1:34603"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""GOAWAY received"",""grpc_status"":14} [Op:RpcClient]
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'subtype: ubuntu/linux', 'TF 2.13']",2023-06-28T15:54:29Z,1,0,https://github.com/tensorflow/tensorflow/issues/61115,
346,tensorflow/tensorflow,//tensorflow/python/data/experimental/kernel_tests/service:local_workers_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/data/experimental/kernel_tests/service:local_workers_test sometimes fails or timeouts.

x86 log
https://source.cloud.google.com/results/invocations/e41a9dd4-19a3-4298-b34f-6a32eca50e08/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5383187293/jobs/9769619751#step:5:8335

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
INFO: From Testing //tensorflow/python/data/experimental/kernel_tests/service:local_workers_test (shard 1 of 24):
==================== Test output for //tensorflow/python/data/experimental/kernel_tests/service:local_workers_test (shard 1 of 24):
2023-06-27 23:02:53.397107: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-27 23:02:53.528865: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] LocalTaskGarbageCollectTest.testMultipleEpochsSharedJob_test_mode_eager_tfapiversion_1_numremoteworkers_0
[  SKIPPED ] LocalTaskGarbageCollectTest.testMultipleEpochsSharedJob_test_mode_eager_tfapiversion_1_numremoteworkers_0
[ RUN      ] LocalTaskGarbageCollectTest.testReadFromDeletedTask_test_mode_eager_tfapiversion_1_numremoteworkers_0
[  SKIPPED ] LocalTaskGarbageCollectTest.testReadFromDeletedTask_test_mode_eager_tfapiversion_1_numremoteworkers_0
[ RUN      ] LocalWorkersTest.testAnonymousJobWithDifferentTargetWorkers_test_mode_graph_tfapiversion_2
INFO:tensorflow:Using local port 43055
I0627 23:02:57.097305 139801719478080 test_util.py:3796] Using local port 43055
2023-06-27 23:02:57.099356: I tensorflow/core/data/service/dispatcher_impl.cc:223] Attempting to restore dispatcher state from journal in /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/_tmp/dbc16dd8be682dbdabf0b589b8e98f16j99fj6xd/tmptm5ix2lv/tf_data_dispatcher_journal
2023-06-27 23:02:57.099421: I tensorflow/core/data/service/dispatcher_impl.cc:230] No journal found. Starting dispatcher from new state.
2023-06-27 23:02:57.099574: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data DispatchServer running at 0.0.0.0:43055
INFO:tensorflow:Using local port 44899
I0627 23:02:57.099842 139801719478080 test_util.py:3796] Using local port 44899
2023-06-27 23:02:57.101683: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:57.101828: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:44899
INFO:tensorflow:Using local port 41615
I0627 23:02:57.102149 139801719478080 test_util.py:3796] Using local port 41615
2023-06-27 23:02:57.103351: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:57.103477: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:41615
INFO:tensorflow:Using local port 37691
I0627 23:02:57.103664 139801719478080 test_util.py:3796] Using local port 37691
2023-06-27 23:02:57.104726: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:57.104854: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:37691
INFO:tensorflow:Using local port 39883
I0627 23:02:57.107191 139801719478080 test_util.py:3796] Using local port 39883
2023-06-27 23:02:57.584682: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-27 23:02:57.619502: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-27 23:02:57.634234: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-27 23:02:57.788004: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-27 23:02:59.175789: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:59.176006: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:39883
INFO:tensorflow:Using local port 34849
I0627 23:02:59.176878 139801719478080 test_util.py:3796] Using local port 34849
2023-06-27 23:02:59.252223: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:59.252462: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:34849
INFO:tensorflow:Using local port 46439
I0627 23:02:59.253159 139801719478080 test_util.py:3796] Using local port 46439
2023-06-27 23:02:59.287010: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:43055
2023-06-27 23:02:59.287229: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:46439
/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/data/ops/dataset_ops.py:458: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.
  warnings.warn(""To make it possible to preserve tf.data options across ""
WARNING:tensorflow:From /usr/lib/python3.9/contextlib.py:87: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
W0627 23:02:59.486639 139801719478080 deprecation.py:364] From /usr/lib/python3.9/contextlib.py:87: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
2023-06-27 23:02:59.508867: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:382] MLIR V1 optimization pass is not enabled
-- Test timed out at 2023-06-27 23:07:52 UTC --
Current thread 0x00007f261fd41740 (most recent call first):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1477 in _call_tf_sessionrun
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1384 in _run_fn
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1401 in _do_call
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1394 in _do_run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1214 in _run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 971 in run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2061 in run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2691 in evaluate
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/data/kernel_tests/test_base.py"", line 237 in assertDatasetProduces
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.py"", line 238 in testAnonymousJobWithDifferentTargetWorkers
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343 in execute_test_method
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360 in decorated
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314 in bound_param_test
  File ""/usr/lib/python3.9/unittest/case.py"", line 550 in _callTestMethod
  File ""/usr/lib/python3.9/unittest/case.py"", line 592 in run
  File ""/usr/lib/python3.9/unittest/case.py"", line 651 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/runner.py"", line 184 in run
  File ""/usr/lib/python3.9/unittest/main.py"", line 271 in runTests
  File ""/usr/lib/python3.9/unittest/main.py"", line 101 in __init__
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/app.py"", line 258 in _run_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/absl_py/absl/app.py"", line 312 in run
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/eager/test.py"", line 25 in main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 167 in test_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 165 in test_main
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/local_workers_test.py"", line 441 in <module>
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux', 'TF 2.13']",2023-06-28T15:34:17Z,1,0,https://github.com/tensorflow/tensorflow/issues/61113,
347,tensorflow/tensorflow,//tensorflow/python/distribute:vars_test_2gpu is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute:vars_test_2gpu timeouts sometimes.

x86 log
https://source.cloud.google.com/results/invocations/769764d8-8dc9-46fa-a284-78062efe3bd9/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5363572382/jobs/9731245377#step:5:9313

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=rbe --config=pycpp --config=build_event_export
```


### Relevant log output

```shell
[ RUN      ] SyncOnReadScatterReplicaTest.testScatterMax_test_aggregation_VariableAggregationMEAN_distribution_MultiWorkerMirrored2x1CPU_mode_graph_usevarpolicy_True
W0628 01:02:37.513197 140343714740032 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.
W0628 01:02:37.513532 140343714740032 collective_all_reduce_strategy.py:394] Collective ops is not configured at program startup. Some performance features may not be enabled.
INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)
I0628 01:02:37.516878 140343714740032 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/device:CPU:0',)
INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
I0628 01:02:37.517337 140343714740032 collective_all_reduce_strategy.py:446] Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
[chief-0]:     W0628 01:02:37.520201 140325400614720 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
I0628 01:02:37.521484 140343714740032 multi_process_runner.py:989] Waiting for the result from chief-0
[worker-0]:    W0628 01:02:37.523296 140325400614720 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
[chief-0]:     W0628 01:02:37.521138 140325400614720 context.py:881] Enabling collective ops after program startup may cause error when accessing previously created tensors.
[worker-0]:    W0628 01:02:37.524588 140325400614720 context.py:881] Enabling collective ops after program startup may cause error when accessing previously created tensors.
[chief-0]:     INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:chief/replica:0/task:0/device:CPU:0', '/job:chief/replica:0/task:0/device:CPU:1']
[chief-0]:     I0628 01:02:37.525059 140325400614720 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['/job:chief/replica:0/task:0/device:CPU:0', '/job:chief/replica:0/task:0/device:CPU:1']
[worker-0]:    INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:CPU:1']
[worker-0]:    I0628 01:02:37.527691 140325400614720 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:CPU:1']
[chief-0]:     INFO:tensorflow:Using MirroredStrategy with devices ('/job:chief/task:0/device:CPU:0',)
[chief-0]:     I0628 01:02:37.529053 140325400614720 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/job:chief/task:0/device:CPU:0',)
[chief-0]:     INFO:tensorflow:Check health not enabled.
[chief-0]:     I0628 01:02:37.529462 140325400614720 collective_all_reduce_strategy.py:574] Check health not enabled.
[chief-0]:     INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('/job:chief/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[chief-0]:     I0628 01:02:37.529720 140325400614720 collective_all_reduce_strategy.py:576] MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'chief', task_id = 0, num_workers = 2, local_devices = ('/job:chief/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[worker-0]:    INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0/device:CPU:0',)
[worker-0]:    I0628 01:02:37.531570 140325400614720 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/job:worker/task:0/device:CPU:0',)
[worker-0]:    INFO:tensorflow:Check health not enabled.
[worker-0]:    I0628 01:02:37.532161 140325400614720 collective_all_reduce_strategy.py:574] Check health not enabled.
[worker-0]:    INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[worker-0]:    I0628 01:02:37.532691 140325400614720 collective_all_reduce_strategy.py:576] MultiWorkerMirroredStrategy with cluster_spec = {'chief': ['localhost:17555'], 'worker': ['localhost:24042']}, task_type = 'worker', task_id = 0, num_workers = 2, local_devices = ('/job:worker/task:0/device:CPU:0',), communication = CommunicationImplementation.AUTO
[worker-0]:    2023-06-28 01:02:37.540481: I tensorflow/core/distributed_runtime/master.cc:240] Scanning workers for devices: 1 total workers
[chief-0]:     2023-06-28 01:02:37.541763: I tensorflow/core/distributed_runtime/master.cc:240] Scanning workers for devices: 1 total workers
-- Test timed out at 2023-06-28 01:07:29 UTC --
Thread 0x00007fa429ef8700 (most recent call first):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fa42a779700 (most recent call first):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 527 in _process_watchdog
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Thread 0x00007fa44d0fc700 (most recent call first):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 258 in _continuously_readline_from_sub
  File ""/usr/lib/python3.9/threading.py"", line 917 in run
  File ""/usr/lib/python3.9/threading.py"", line 980 in _bootstrap_inner
  File ""/usr/lib/python3.9/threading.py"", line 937 in _bootstrap

Current thread 0x00007fa451437740 (most recent call first):
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 379 in _recv
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 414 in _recv_bytes
  File ""/usr/lib/python3.9/multiprocessing/connection.py"", line 250 in recv
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 991 in run
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/combinations.py"", line 580 in decorator
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343 in execute_test_method
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360 in decorated
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/parameterized.py"", line 314 in bound_param_test
  File ""/usr/lib/python3.9/unittest/case.py"", line 550 in _callTestMethod
  File ""/usr/lib/python3.9/unittest/case.py"", line 592 in run
  File ""/usr/lib/python3.9/unittest/case.py"", line 651 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.9/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.9/unittest/runner.py"", line 184 in run
  File ""/usr/lib/python3.9/unittest/main.py"", line 271 in runTests
  File ""/usr/lib/python3.9/unittest/main.py"", line 101 in __init__
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2537 in _run_and_get_tests_result
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2568 in run_tests
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2156 in _run_in_app
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/testing/absltest.py"", line 2049 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/app.py"", line 258 in _run_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/absl_py/absl/app.py"", line 312 in run
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 56 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/eager/test.py"", line 25 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 167 in test_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 1455 in test_main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/test_util.py"", line 138 in main
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/distribute/vars_test_2gpu.runfiles/org_tensorflow/tensorflow/python/distribute/vars_test.py"", line 1336 in <module>
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux']",2023-06-28T15:03:57Z,1,0,https://github.com/tensorflow/tensorflow/issues/61112,
348,tensorflow/tensorflow,//tensorflow/python/distribute/failure_handling:gce_failure_handler_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute/failure_handling:gce_failure_handler_test sometimes fails or timeouts.

x86 log
https://source.cloud.google.com/results/invocations/66d8ddaa-3dbe-4464-837c-053157b11659/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5354839739/jobs/9712379821#step:5:12470

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
INFO:tensorflow:time(__main__.GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker): 5.11s
I0628 05:40:30.221971 140074262497088 test_util.py:2464] time(__main__.GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker): 5.11s
[       OK ] GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_True_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker
======================================================================
FAIL: test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker (__main__.GceFailureHandlingTest)
GceFailureHandlingTest.test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker
test_multiple_workers_preempted_consecutively_test_apiwrappingtrain_False_graceperiod_7_inputarg_checkpoint_strategyoption_MWMSmultiworker(api_wrapping_train=False, grace_period=7, input_arg='checkpoint', strategy_option='MWMS_multi_worker')
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 314, in bound_param_test
    return test_method(self, **testcase_params)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 360, in decorated
    execute_test_method()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/framework/test_combinations.py"", line 343, in execute_test_method
    test_method(**kwargs_to_pass)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/combinations.py"", line 559, in decorator
    test_method(self, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.py"", line 417, in test_multiple_workers_preempted_consecutively
    mpr.join(timeout=250)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 649, in join
    self._reraise_if_subprocess_error(process_statuses)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 565, in _reraise_if_subprocess_error
    six.reraise(*process_status.exc_info)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/six_archive/six.py"", line 719, in reraise
    raise value
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 1060, in _run_contained
    return_value = fn(*args, **kwargs)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.py"", line 211, in worker_fn
    self.assertNotEmpty(checkpoint_index)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/absl_py/absl/testing/absltest.py"", line 972, in assertNotEmpty
    self.fail('{!r} has length of 0.'.format(container), msg)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/gce_failure_handler_test.runfiles/absl_py/absl/testing/absltest.py"", line 1814, in fail
    return super(TestCase, self).fail(self._formatMessage(prefix, msg))
  File ""/usr/lib/python3.9/unittest/case.py"", line 676, in fail
    raise self.failureException(msg)
AssertionError: [] has length of 0.
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux', 'TF 2.13']",2023-06-28T14:07:42Z,1,0,https://github.com/tensorflow/tensorflow/issues/61111,
349,tensorflow/tensorflow,//tensorflow/python/data/experimental/kernel_tests/service:auto_shard_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/data/experimental/kernel_tests/service:auto_shard_test will sometimes timeout.

x86 log
https://source.cloud.google.com/results/invocations/75230523-dc04-40ad-bcf3-06df3b94a119/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5383235016/jobs/9769729147#step:5:8442

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
TIMEOUT: //tensorflow/python/data/experimental/kernel_tests/service:auto_shard_test (Summary)
      /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test/shard_6_of_32/test.log
INFO: From Testing //tensorflow/python/data/experimental/kernel_tests/service:auto_shard_test (shard 6 of 32):
==================== Test output for //tensorflow/python/data/experimental/kernel_tests/service:auto_shard_test (shard 6 of 32):
2023-06-28 08:01:34.863480: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 08:01:34.953626: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] AutoShardTest.testBatchDataset_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA
[  SKIPPED ] AutoShardTest.testBatchDataset_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA
[ RUN      ] AutoShardTest.testEnumerateShardingPolicies_test_mode_graph_tfapiversion_1_shardingpolicy_ShardingPolicyDYNAMIC
[  SKIPPED ] AutoShardTest.testEnumerateShardingPolicies_test_mode_graph_tfapiversion_1_shardingpolicy_ShardingPolicyDYNAMIC
[ RUN      ] AutoShardTest.testRangeDataset_AutoShard_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA
[  SKIPPED ] AutoShardTest.testRangeDataset_AutoShard_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyFILEORDATA
[ RUN      ] AutoShardTest.testRangeDataset_ShardHintUsedInWrongShardingPolicy_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyOFF
[  SKIPPED ] AutoShardTest.testRangeDataset_ShardHintUsedInWrongShardingPolicy_test_mode_eager_tfapiversion_1_shardingpolicy_ShardingPolicyOFF
[ RUN      ] AutoShardTest.testTFRecordDataset_FewerFilesThanWorkers_DataShard_test_mode_eager_tfapiversion_2
INFO:tensorflow:Using local port 35531
I0628 08:02:03.552320 139750982424384 test_util.py:3796] Using local port 35531
2023-06-28 08:02:03.553738: I tensorflow/core/data/service/dispatcher_impl.cc:223] Attempting to restore dispatcher state from journal in /root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/_tmp/df4595e40f65712a12ed239eda341855ao10glmf/tmplcxh6ash/tf_data_dispatcher_journal
2023-06-28 08:02:03.553799: I tensorflow/core/data/service/dispatcher_impl.cc:230] No journal found. Starting dispatcher from new state.
2023-06-28 08:02:03.553987: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data DispatchServer running at 0.0.0.0:35531
INFO:tensorflow:Using local port 43623
I0628 08:02:03.556421 139750982424384 test_util.py:3796] Using local port 43623
2023-06-28 08:02:04.452927: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 08:02:04.495706: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 08:02:04.557281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-28 08:02:04.586634: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-28 08:02:06.406403: I tensorflow/core/data/service/worker_impl.cc:186] Worker registered with dispatcher running at localhost:35531
2023-06-28 08:02:06.406887: I tensorflow/core/data/service/server_lib.cc:82] Started tf.data WorkerServer running at 0.0.0.0:43623
INFO:tensorflow:Using local port 40059
I0628 08:02:06.407576 139750982424384 test_util.py:3796] Using local port 40059
E0628 08:02:06.423661297  858602 server_chttp2.cc:40]        {""created"":""@1687939326.423469587"",""description"":""No address added out of total 1 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":395,""referenced_errors"":[{""created"":""@1687939326.423463864"",""description"":""Failed to add any wildcard listeners"",""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":342,""referenced_errors"":[{""created"":""@1687939326.423438207"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::]:40059""},{""created"":""@1687939326.423463182"",""description"":""Unable to configure socket"",""fd"":8,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1687939326.423455518"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}
Process _RemoteWorkerProcess-2:
Traceback (most recent call last):
  File ""/usr/lib/python3.9/multiprocessing/process.py"", line 315, in _bootstrap
    self.run()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 54, in _run_with_absl
    app.run(lambda _: self._run_impl())
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/absl_py/absl/app.py"", line 312, in run
    _run_main(main, args)
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/absl_py/absl/app.py"", line 258, in _run_main
    sys.exit(main(argv))
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 54, in <lambda>
    app.run(lambda _: self._run_impl())
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 42, in run
    self.start_worker()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/multi_process_cluster.py"", line 50, in start_worker
    self._worker.start()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/kernel_tests/service/test_base.py"", line 110, in start
    self._server.start()
  File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/data/experimental/kernel_tests/service/auto_shard_test.runfiles/org_tensorflow/tensorflow/python/data/experimental/service/server_lib.py"", line 415, in start
    self._server.start()
RuntimeError: Could not start gRPC server
-- Test timed out at 2023-06-28 08:06:34 UTC --
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux', 'subtype:bazel']",2023-06-28T13:20:03Z,1,0,https://github.com/tensorflow/tensorflow/issues/61109,
350,tensorflow/tensorflow,//tensorflow/python/distribute:cross_device_ops_test_2gpu is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute:cross_device_ops_test_2gpu fails or timeouts sometimes

x86 log
https://source.cloud.google.com/results/invocations/3008a6bc-b49f-4776-871c-1c5ae046a470/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5379162649/jobs/9759994992#step:5:9913

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
[ RUN      ] CollectiveOpsTest.testBatchReduceDense_test_implementation_CommunicationImplementationRING_numprocesses_2_preferuniqueinstancekey_False_reduceop_ReduceOpSUM_requiredgpus_0
[worker-0]:    W0628 09:23:51.698908 140256382134080 context.py:881] Enabling collective ops after program startup may cause error when accessing previously created tensors.
I0628 09:23:51.700115 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-0
I0628 09:23:51.702625 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-0
I0628 09:23:51.702917 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-1
I0628 09:23:51.711031 139955121788736 multi_process_runner.py:989] Waiting for the result from worker-0
[worker-0]:    2023-06-28 09:23:51.734731: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:38213
[worker-1]:    E0628 09:23:51.740952892 1194361 server_chttp2.cc:40]        {""created"":""@1687944231.740904797"",""description"":""No address added out of total 1 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":395,""referenced_errors"":[{""created"":""@1687944231.740902919"",""description"":""Failed to add any wildcard listeners"",""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":342,""referenced_errors"":[{""created"":""@1687944231.740878363"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::]:36443""},{""created"":""@1687944231.740901905"",""description"":""Unable to configure socket"",""fd"":9,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1687944231.740897648"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}
[worker-1]:    2023-06-28 09:23:51.741121: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server
[worker-1]:    2023-06-28 09:23:51.741306: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:703] Could not start gRPC server
-- Test timed out at 2023-06-28 09:28:39 UTC --
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux']",2023-06-28T12:15:43Z,3,0,https://github.com/tensorflow/tensorflow/issues/61107,
351,tensorflow/tensorflow,//tensorflow/python/distribute:moving_averages_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute:moving_averages_test fails sometimes.

x86 log
https://source.cloud.google.com/results/invocations/1e048065-76db-4dab-b1a5-093dd542b27d/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5383507734/jobs/9770356325#step:5:8417

Looks like a network port conflict issue

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
INFO: From Testing //tensorflow/python/distribute:moving_averages_test_cpu (shard 3 of 5):
==================== Test output for //tensorflow/python/distribute:moving_averages_test_cpu (shard 3 of 5):
2023-06-28 10:24:27.228811: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 10:24:27.325938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph
INFO:tensorflow:time(__main__.AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph): 0.03s
I0628 10:24:32.088060 139996622530368 test_util.py:2464] time(__main__.AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph): 0.03s
[  SKIPPED ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MirroredCPUAndGPU_mode_graph
[ RUN      ] AssignMovingAveragesTest.testAssignVariable_test_distribution_MultiWorkerMirrored4x1CPU_mode_graph
W0628 10:24:32.126455 139996622530368 context.py:773] Configuring coordination service type may not be effective because the context is already initialized.
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.
W0628 10:24:32.126971 139996622530368 collective_all_reduce_strategy.py:394] Collective ops is not configured at program startup. Some performance features may not be enabled.
INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)
I0628 10:24:32.164073 139996622530368 mirrored_strategy.py:423] Using MirroredStrategy with devices ('/device:CPU:0',)
INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
I0628 10:24:32.171577 139996622530368 collective_all_reduce_strategy.py:446] Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO
INFO:tensorflow:Using local port 43531
I0628 10:24:32.173116 139996622530368 test_util.py:3796] Using local port 43531
INFO:tensorflow:Using local port 34115
I0628 10:24:32.173517 139996622530368 test_util.py:3796] Using local port 34115
INFO:tensorflow:Using local port 34873
I0628 10:24:32.173699 139996622530368 test_util.py:3796] Using local port 34873
INFO:tensorflow:Using local port 40455
I0628 10:24:32.173828 139996622530368 test_util.py:3796] Using local port 40455
2023-06-28 10:24:32.849027: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 10:24:32.904538: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-28 10:24:32.917529: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-28 10:24:32.995322: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[chief-0]:     I0628 10:24:34.534415 140662937323328 multi_process_runner.py:840] Subprocess with PID 1244 (chief, 0) is now being started.
[chief-0]:     I0628 10:24:34.534823 140662937323328 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""chief"": [""localhost:43531""], ""worker"": [""localhost:34115"", ""localhost:34873"", ""localhost:40455""]}, ""task"": {""type"": ""chief"", ""index"": 0}, ""rpc_layer"": ""grpc""}'
I0628 10:24:34.561606 139996622530368 multi_process_runner.py:989] Waiting for the result from chief-0
[worker-1]:    I0628 10:24:34.596723 140662937323328 multi_process_runner.py:840] Subprocess with PID 1479 (worker, 1) is now being started.
[worker-1]:    I0628 10:24:34.597120 140662937323328 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""chief"": [""localhost:43531""], ""worker"": [""localhost:34115"", ""localhost:34873"", ""localhost:40455""]}, ""task"": {""type"": ""worker"", ""index"": 1}, ""rpc_layer"": ""grpc""}'
[worker-1]:    2023-06-28 10:24:34.655212: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:34873
[worker-1]:    INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
[worker-1]:    I0628 10:24:34.661444 140662937323328 collective_all_reduce_strategy.py:531] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:1/device:CPU:0']
[chief-0]:     E0628 10:24:34.621842982    1244 server_chttp2.cc:40]        {""created"":""@1687947874.621805165"",""description"":""No address added out of total 1 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":395,""referenced_errors"":[{""created"":""@1687947874.621803913"",""description"":""Failed to add any wildcard listeners"",""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":342,""referenced_errors"":[{""created"":""@1687947874.621786452"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::]:43531""},{""created"":""@1687947874.621803346"",""description"":""Unable to configure socket"",""fd"":9,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1687947874.621800946"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}
[chief-0]:     2023-06-28 10:24:34.621944: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server
[worker-0]:    I0628 10:24:34.608072 140662937323328 multi_process_runner.py:840] Subprocess with PID 1472 (worker, 0) is now being started.
[chief-0]:     2023-06-28 10:24:34.622255: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:703] Could not start gRPC server
[chief-0]:     Process _Process-3:
[chief-0]:     Traceback (most recent call last):
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/strategy_combinations.py"", line 207, in skip_if_cannot_start_grpc_server
[chief-0]:         return _create_multi_worker_mirrored()
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/strategy_combinations.py"", line 189, in _create_multi_worker_mirrored
[chief-0]:         strategy = CollectiveAllReduceStrategy(cluster_resolver=resolver)
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 186, in __init__
[chief-0]:         CollectiveAllReduceExtended(
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 339, in __init__
[chief-0]:         self._initialize_strategy(self._cluster_resolver, devices=devices)
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 358, in _initialize_strategy
[chief-0]:         self._initialize_multi_worker(cluster_resolver)
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 530, in _initialize_multi_worker
[chief-0]:         context.context().ensure_initialized()
[chief-0]:       File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/moving_averages_test_cpu.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 610, in ensure_initialized
[chief-0]:         pywrap_tfe.TFE_EnableCollectiveOps(context_handle, server_def_str)
[chief-0]:     tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux']",2023-06-28T11:56:02Z,1,0,https://github.com/tensorflow/tensorflow/issues/61105,
352,tensorflow/tensorflow,//tensorflow/python/distribute/failure_handling:failure_handler_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

6.1.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/distribute/failure_handling:failure_handler_test will timeout sometimes

x86 log
https://source.cloud.google.com/results/invocations/302ca1a4-593b-430c-b278-038351228670/log

AARCH64 log
https://github.com/tensorflow/tensorflow/actions/runs/5363735926/jobs/9731502578#step:5:11034

Looks like a network port conflict.

### Standalone code to reproduce the issue

```shell
bazel --bazelrc=/usertools/cpu.bazelrc test --config=pycpp --config=build_event_export --remote_cache=https://storage.googleapis.com/tensorflow-devinfra-bazel-cache/norbe --google_default_credentials
```


### Relevant log output

```shell
INFO: From Testing //tensorflow/python/distribute/failure_handling:failure_handler_test (shard 4 of 8):
==================== Test output for //tensorflow/python/distribute/failure_handling:failure_handler_test (shard 4 of 8):
2023-06-22 16:46:57.933478: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-22 16:46:58.014195: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Running tests under Python 3.9.17: /usr/bin/python3
[ RUN      ] PreemptionCheckpointTest.test_grace_period_continue_training_test_inputarg_checkpoint_strategyoption_MWMSmultiworker
INFO:tensorflow:Using local port 38723
I0622 16:47:02.418146 140436227843904 test_util.py:3796] Using local port 38723
INFO:tensorflow:Using local port 41053
I0622 16:47:02.418907 140436227843904 test_util.py:3796] Using local port 41053
INFO:tensorflow:Using local port 45087
I0622 16:47:02.419116 140436227843904 test_util.py:3796] Using local port 45087
INFO:tensorflow:Using local port 38125
I0622 16:47:02.419290 140436227843904 test_util.py:3796] Using local port 38125
2023-06-22 16:47:03.130184: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-22 16:47:03.185580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-06-22 16:47:03.185763: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-06-22 16:47:03.241253: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
INFO:tensorflow:Cluster starting.
I0622 16:47:04.817156 140436227843904 failure_handler_test.py:432] Cluster starting.
[worker-0]:    I0622 16:47:04.842670 140652189284160 multi_process_runner.py:840] Subprocess with PID 588346 (worker, 0) is now being started.
[worker-0]:    I0622 16:47:04.842952 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""worker"": [""localhost:38723"", ""localhost:41053"", ""localhost:45087"", ""localhost:38125""]}, ""task"": {""type"": ""worker"", ""index"": 0}, ""rpc_layer"": ""grpc""}'
[worker-1]:    I0622 16:47:04.858144 140652189284160 multi_process_runner.py:840] Subprocess with PID 588646 (worker, 1) is now being started.
[worker-1]:    I0622 16:47:04.858554 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""worker"": [""localhost:38723"", ""localhost:41053"", ""localhost:45087"", ""localhost:38125""]}, ""task"": {""type"": ""worker"", ""index"": 1}, ""rpc_layer"": ""grpc""}'
[worker-0]:    2023-06-22 16:47:04.907068: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:38723
[worker-2]:    I0622 16:47:04.927917 140652189284160 multi_process_runner.py:840] Subprocess with PID 588933 (worker, 2) is now being started.
[worker-0]:    2023-06-22 16:47:04.943758: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:551] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 14520256932767538069
[worker-0]:    2023-06-22 16:47:04.944905: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:299] Coordination agent has successfully connected.
[worker-2]:    I0622 16:47:04.928328 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""worker"": [""localhost:38723"", ""localhost:41053"", ""localhost:45087"", ""localhost:38125""]}, ""task"": {""type"": ""worker"", ""index"": 2}, ""rpc_layer"": ""grpc""}'
[worker-1]:    2023-06-22 16:47:04.995414: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:457] Started server with target: grpc://localhost:41053
[worker-0]:    2023-06-22 16:47:04.997341: I tensorflow/tsl/distributed_runtime/coordination/coordination_service.cc:551] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 7622272584115739518
[worker-1]:    2023-06-22 16:47:04.998064: I tensorflow/tsl/distributed_runtime/coordination/coordination_service_agent.cc:299] Coordination agent has successfully connected.
[worker-3]:    I0622 16:47:05.006252 140652189284160 multi_process_runner.py:840] Subprocess with PID 589461 (worker, 3) is now being started.
[worker-2]:    E0622 16:47:05.032748121  588933 server_chttp2.cc:40]        {""created"":""@1687452425.032699864"",""description"":""No address added out of total 1 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":395,""referenced_errors"":[{""created"":""@1687452425.032698162"",""description"":""Failed to add any wildcard listeners"",""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_posix.cc"",""file_line"":342,""referenced_errors"":[{""created"":""@1687452425.032676237"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::]:45087""},{""created"":""@1687452425.032697070"",""description"":""Unable to configure socket"",""fd"":9,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":216,""referenced_errors"":[{""created"":""@1687452425.032694857"",""description"":""Address already in use"",""errno"":98,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/tcp_server_utils_posix_common.cc"",""file_line"":189,""os_error"":""Address already in use"",""syscall"":""bind""}]}]}]}
[worker-2]:    2023-06-22 16:47:05.032844: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:608] UNKNOWN: Could not start gRPC server
[worker-2]:    2023-06-22 16:47:05.033076: E tensorflow/core/common_runtime/eager/context_distributed_manager.cc:703] Could not start gRPC server
[worker-3]:    I0622 16:47:05.006778 140652189284160 multi_process_runner.py:842] TF_CONFIG: '{""cluster"": {""worker"": [""localhost:38723"", ""localhost:41053"", ""localhost:45087"", ""localhost:38125""]}, ""task"": {""type"": ""worker"", ""index"": 3}, ""rpc_layer"": ""grpc""}'
[worker-2]:    Process _Process-4:
[worker-2]:    Traceback (most recent call last):
[worker-2]:      File ""/usr/lib/python3.9/multiprocessing/process.py"", line 315, in _bootstrap
[worker-2]:        self.run()
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 755, in _run_with_setenv
[worker-2]:        return self._actual_run()
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 54, in _run_with_absl
[worker-2]:        app.run(lambda _: self._run_impl())
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/absl_py/absl/app.py"", line 312, in run
[worker-2]:        _run_main(main, args)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/absl_py/absl/app.py"", line 258, in _run_main
[worker-2]:        sys.exit(main(argv))
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_lib.py"", line 54, in <lambda>
[worker-2]:        app.run(lambda _: self._run_impl())
[worker-2]:      File ""/usr/lib/python3.9/multiprocessing/process.py"", line 108, in run
[worker-2]:        self._target(*self._args, **self._kwargs)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 866, in __call__
[worker-2]:        six.reraise(*info.exc_info)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/six_archive/six.py"", line 719, in reraise
[worker-2]:        raise value
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/multi_process_runner.py"", line 1060, in _run_contained
[worker-2]:        return_value = fn(*args, **kwargs)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/failure_handling/failure_handler_test.py"", line 146, in worker_fn
[worker-2]:        strategy = collective_all_reduce_strategy.CollectiveAllReduceStrategy()
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 186, in __init__
[worker-2]:        CollectiveAllReduceExtended(
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 339, in __init__
[worker-2]:        self._initialize_strategy(self._cluster_resolver, devices=devices)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 358, in _initialize_strategy
[worker-2]:        self._initialize_multi_worker(cluster_resolver)
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/distribute/collective_all_reduce_strategy.py"", line 530, in _initialize_multi_worker
[worker-2]:        context.context().ensure_initialized()
[worker-2]:      File ""/root/.cache/bazel/_bazel_root/fbac33eb30dbfb6b11b15a7ff5ac830d/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/distribute/failure_handling/failure_handler_test.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 610, in ensure_initialized
[worker-2]:        pywrap_tfe.TFE_EnableCollectiveOps(context_handle, server_def_str)
[worker-2]:    tensorflow.python.framework.errors_impl.UnknownError: Could not start gRPC server
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux']",2023-06-28T10:57:28Z,1,0,https://github.com/tensorflow/tensorflow/issues/61104,
353,tensorflow/tensorflow,tf.data.Dataset prefetch not fetching data asynchronously,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

Debian/Linux 11

### Mobile device

_No response_

### Python version

3.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

After implementing a data pipeline using tf.data.Dataset to pull image data from Google Cloud Storage, TensorBoard profiler shows that the GPU compute and CPU prefetch are running synchronously. I used data.Dataset.AUTOTUNE to determine the appropriate prefetch batch size. Monitoring GPU usage while the model is running confirms this with the GPU at 0% utilization to actually computing something for about a 2:1 ratio, which is reflected in the profiler. CPU usage when monitored does not appear to max out.

I expected the prefetch to occur concurrently with GPU processing as described in the data.Dataset documentation and tutorials.

![ch](https://github.com/tensorflow/tensorflow/assets/79778984/e96ad312-12b0-4bbb-b06e-f4e4976714b3)

![cp](https://github.com/tensorflow/tensorflow/assets/79778984/f52959d1-23fd-45ed-ba57-5a532afd0972)

![gp](https://github.com/tensorflow/tensorflow/assets/79778984/2b2e15b6-2cf7-40d8-89b1-98889f151863)


### Standalone code to reproduce the issue

```shell
os.environ[""CUDA_VISIBLE_DEVICES""] = ""0""
os.environ['TF_GPU_ALLOCATOR'] = ""cuda_malloc_async""
config = ConfigProto()
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

def get_label(file_path):
    parts = tf.strings.split(file_path, os.path.sep)
    one_hot = parts[-2] == class_names
    return tf.argmax(one_hot)

def decode_img(img):
    img = tf.io.decode_image(img, channels=3, expand_animations = False)
    img = tf.image.resize(img, [244, 244])
    img = tf.cast(img, tf.float32)
    return img

def process_path(file_path):
    label = get_label(file_path)
    img = tf.io.read_file(file_path)
    img = decode_img(img)
    return img, label

def configure_for_performance(ds):
    ds = ds.batch(128)
    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)
    return ds

files = tf.data.Dataset.list_files((data_dir + '/*/*.png'), shuffle=False)
files = files.shuffle(image_count, reshuffle_each_iteration=False)

val_size = int(image_count * 0.2)

train_files = files.skip(val_size)
val_files = files.take(val_size)

train_ds = train_files.interleave(lambda x: tf.data.Dataset.from_tensor_slices([x]), cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)

val_ds = val_files.interleave(lambda x: tf.data.Dataset.from_tensor_slices([x]), cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE)
val_ds = val_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)

train_ds = configure_for_performance(train_ds)
val_ds = configure_for_performance(val_ds)
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'type:performance', 'TF 2.11']",2023-06-26T19:39:17Z,4,1,https://github.com/tensorflow/tensorflow/issues/61084,
354,tensorflow/tensorflow,"MHLO -> HLO does not respect sharding, inserting a tuple without sharding","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2e896fbe1e0ea4df33fbcfe780a1036f431b4e89

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubunto 20.04

### Mobile device

_No response_

### Python version

3.11

### Bazel version

_No response_

### GCC/Compiler version

10.3

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Converting an MHLO program to HLO that is fully annotated with shardings, results in HLO that has a tuple instruction that is without sharding.

Input MLIR `sharding-not-respected-mhlo-to-hlo.mlir`:
```mlir
func.func @main(%arg0: tensor<2x2xi32> {mhlo.sharding = ""{devices=[2,1]0,1}""}) -> (tensor<2x2xi32> {mhlo.sharding = ""{devices=[2,1]0,1}""}) {
  %0 = mhlo.add %arg0, %arg0 {mhlo.sharding = ""{devices=[2,1]0,1}""} : tensor<2x2xi32>
  return %0 : tensor<2x2xi32>
}
```

Command:
```
xla-translate -mlir-hlo-to-hlo-text sharding-not-respected-mhlo-to-hlo.mlir
```

Result:
```hlo
HloModule main, entry_computation_layout={(s32[2,2]{1,0})->s32[2,2]{1,0}}

ENTRY %main.5 (Arg_0.1: s32[2,2]) -> s32[2,2] {
  %Arg_0.1 = s32[2,2] parameter(0), sharding={devices=[2,1]0,1}
  %add.2 = s32[2,2] add(s32[2,2] %Arg_0.1, s32[2,2] %Arg_0.1), sharding={devices=[2,1]0,1}, metadata={source_file=""sharding-not-respected-mhlo-to-hlo.mlir"" source_line=2}
  %tuple.3 = (s32[2,2]) tuple(s32[2,2] %add.2)
  ROOT %get-tuple-element.4 = s32[2,2] get-tuple-element((s32[2,2]) %tuple.3), index=0, sharding={devices=[2,1]0,1}
}
```

You can see that a tuple instruction has been inserted that has no sharding annotation.
```hlo
  %tuple.3 = (s32[2,2]) tuple(s32[2,2] %add.2)
```

This [test](https://github.com/tensorflow/tensorflow/blob/2e896fbe1e0ea4df33fbcfe780a1036f431b4e89/tensorflow/compiler/xla/translate/mhlo_to_hlo/tests/sharding.mlir#L21) expects a tuple without sharding. Is this really the expected behavior?
For example this tuple instruction would cause a problem if you want to do SPMD partitioning. Then the partitioner would insert an unwanted all-gather instruction.

If you do a conversion without sharding of the same MLIR
```mlir
func.func @main(%arg0: tensor<2x2xi32>) -> tensor<2x2xi32> {
  %0 = mhlo.add %arg0, %arg0 : tensor<2x2xi32>
  return %0 : tensor<2x2xi32>
}
```
You get a nicer result without the redundant `tuple` and `get-tuple-element` instructions
```hlo
HloModule main, entry_computation_layout={(s32[2,2]{1,0})->s32[2,2]{1,0}}

ENTRY %main.3 (Arg_0.1: s32[2,2]) -> s32[2,2] {
  %Arg_0.1 = s32[2,2] parameter(0)
  ROOT %add.2 = s32[2,2] add(s32[2,2] %Arg_0.1, s32[2,2] %Arg_0.1), metadata={source_file=""sharding-not-respected-mhlo-to-hlo.mlir"" source_line=2}
}
```

I can see two solutions here. 
1. Make the tuple instruction inherit the correct sharding.
2. Return directly the result of the `add` operation.

### Standalone code to reproduce the issue

```shell
See the description.
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:runtime', 'comp:xla']",2023-06-26T10:50:30Z,2,0,https://github.com/tensorflow/tensorflow/issues/61080,
355,tensorflow/tensorflow,Errors when custom gradients are being used in TPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

v2.12.0-rc1-12-g0db597d0d75 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When writing custom gradient modules in TensorFlow (not using tf.GradientTape()) and applying it using an existing optimizer causes errors. I don't see any visible difference in tf.GradientShape() gradients and custom ones.

### Standalone code to reproduce the issue

```shell
with strategy.scope():
  with tf.GradientTape() as tape:
    model = ... #using any model
    loss = ... #using any loss function here
    loss_n = loss(y_batch, model(x_batch))

  grads = tape.gradient(loss_n, model.trainable_weights)
  new_grads = []
  
  for g in grads:
    new_grads += [tf.ones((tf.shape(g)))]
  
  optimizer.apply_gradients(zip(new_grads, model.trainable_weights)) #error here
```
```


### Relevant log output

```shell
Should be similar to below:


AttributeError                            Traceback (most recent call last)
<ipython-input-25-c9f1f22a039f> in <cell line: 1>()
     50       grads = tape.gradient(l, model.trainable_weights)
     51       c = grads[0]
---> 52       opt.apply_gradients(zip(grads, model.trainable_weights))
     53       print('Loss this batch: ' + closure().numpy())
     54       opt.next_steps()

2 frames
/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py in apply_gradients(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)
   1171         )
   1172         if not skip_gradients_aggregation and experimental_aggregate_gradients:
-> 1173             grads_and_vars = self.aggregate_gradients(grads_and_vars)
   1174         return super().apply_gradients(grads_and_vars, name=name)
   1175 

/usr/local/lib/python3.10/dist-packages/keras/optimizers/optimizer.py in aggregate_gradients(self, grads_and_vars)
   1137           List of (gradient, variable) pairs.
   1138         """"""
-> 1139         return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)
   1140 
   1141     def apply_gradients(

/usr/local/lib/python3.10/dist-packages/keras/optimizers/utils.py in all_reduce_sum_gradients(grads_and_vars)
     40         else:
     41             # TODO(b/183257003): Remove this branch
---> 42             reduced = tf.distribute.get_replica_context().merge_call(
     43                 _all_reduce_sum_fn, args=(filtered_grads_and_vars,)
     44             )

AttributeError: 'NoneType' object has no attribute 'merge_call'
```
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.12']",2023-06-17T04:52:55Z,5,0,https://github.com/tensorflow/tensorflow/issues/60906,
356,tensorflow/tensorflow,"TF throws: ""'visible_device_list' listed an invalid Device id"" when using non-GPU PluggableDevices","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux CentOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current Behaviour?

When using PluggableDevice API together with GPU devices, TF crashes with `tensorflow.python.framework.errors_impl.InvalidArgumentError: 'visible_device_list' listed an invalid Device id '2' but visible device count is 2` when calling `tf.device('/GPU:0')`

**For reproducing the error, it is necessary to have a PluggableDevice Plugin loaded and to have GPUs within the same system!!!**

Here the list of devices within my system:
```python3
# tf.config.list_physical_devices()
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:VE:0', device_type='VE'), PhysicalDevice(name='/physical_device:VE:1', device_type='VE'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```

I traced down the error to be thrown here. It gets thrown in
https://github.com/tensorflow/tensorflow/blob/e32f5b90ec16e88b23be8a5189e52ea9a420e999/tensorflow/tsl/framework/device_id_utils.cc#L46

However, it is caused by wrong values stored in the gpu_options, which get initialized here:
https://github.com/tensorflow/tensorflow/blob/0db597d0d758aba578783b5bf46c889700a45085/tensorflow/python/eager/context.py#L1206

The list of gpu_devices and ALL pluggable_devices get combined, even if they are not of the same device_type. So the list of `compatible_devices` will be:
```
[
	PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),
	PhysicalDevice(name='/physical_device:VE:0', device_type='VE'),
	PhysicalDevice(name='/physical_device:VE:1', device_type='VE')
]
```

This causes the `visible_device_list` to be `['0', '1', '2']`, which contains invalid GPU device indices. These then get passed to `ParseVisibleDeviceList`, which throws this error.

To fix this error, it suffices to change this line:
https://github.com/tensorflow/tensorflow/blob/0db597d0d758aba578783b5bf46c889700a45085/tensorflow/python/eager/context.py#L1216

and replace it to:
```python
if dev not in gpu_devices and dev.device_type == ""GPU"":
```

This way, the list of `compatible_devices` will only populated with other GPUs, not with any other device types.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
print(*tf.config.list_physical_devices(), sep='\n')
tf.device('/GPU:0')
```


### Relevant log output

```shell
PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')
PhysicalDevice(name='/physical_device:VE:0', device_type='VE')
PhysicalDevice(name='/physical_device:VE:1', device_type='VE')
PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/.../.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 5577, in device_v2
    return device(device_name)
  File ""/.../.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 5526, in device
    return context.device(device_name_or_function)
  File ""/.../.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 2348, in device
    ensure_initialized()
  File ""/.../.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 2143, in ensure_initialized
    context().ensure_initialized()
  File ""/.../.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py"", line 583, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InvalidArgumentError: 'visible_device_list' listed an invalid Device id '2' but visible device count is 2
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.13']",2023-06-16T07:46:52Z,4,0,https://github.com/tensorflow/tensorflow/issues/60895,
357,tensorflow/tensorflow,Spurious(?) type inference failed warning for flattened tf.data.Dataset with a RaggedTensor,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Mac OS

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

As far as I can tell the code still runs as expected, but a seemly spurious warning is still issued.

The issue is pretty niche, removing the `_merge` function or the `vals` part to the initial dataset will make the error go away.

This occurs on 2.11, 2.12, and nightly on mac.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

labels = tf.ragged.constant([[""a"", ""b""], [""a""]])
vals = tf.constant([0.1, 0.2])
ds = tf.data.Dataset.from_tensors(dict(labels=labels, vals=vals, other=vals))

parts = [""labels"", ""vals""]

def _flatten(ex):
  flat_ds = tf.data.Dataset.from_tensor_slices({k: ex[k] for k in parts})

  def _merge(_flat_ex):
    _flat_ex[""other""] = tf.constant([0.1, 0.2])
    return _flat_ex

  return flat_ds.map(_merge)
ds = ds.flat_map(_flatten)

for ex in ds.as_numpy_iterator():
  print(ex)
```


### Relevant log output

```shell
2023-06-15 13:44:43.909272: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: type mismatch for node 'TensorSliceDataset': expected a subtype of:
type_id: TFT_PRODUCT
args {
  type_id: TFT_DATASET
  args {
    type_id: TFT_PRODUCT
    args {
      type_id: TFT_TENSOR
      args {
        type_id: TFT_LEGACY_VARIANT
      }
    }
    args {
      type_id: TFT_TENSOR
      args {
        type_id: TFT_FLOAT
      }
    }
  }
}

  got:
type_id: TFT_PRODUCT
args {
  type_id: TFT_DATASET
  args {
    type_id: TFT_PRODUCT
    args {
      type_id: TFT_RAGGED
      args {
        type_id: TFT_STRING
      }
    }
    args {
    }
  }
}

  
	while updating its output type.
{'labels': array([b'a', b'b'], dtype=object), 'vals': 0.1, 'other': array([0.1, 0.2], dtype=float32)}
{'labels': array([b'a'], dtype=object), 'vals': 0.2, 'other': array([0.1, 0.2], dtype=float32)}
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.12']",2023-06-15T20:54:26Z,3,0,https://github.com/tensorflow/tensorflow/issues/60890,
358,tensorflow/tensorflow,Unnecessary memcopies between CPU and GPU when using tf.function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Linux CentOS

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current Behaviour?

We recognized that TensorFlow creates unnecessary copies between CPU and GPU. To reproduce, save the code below as `error.py` and execute it using `nvprof --print-gpu-trace --openacc-profiling off python3 error.py MODE`, with mode being 0, 1, 2 or 3.

Here what we have observed, I simplyfied the profiler output and commented inline:

## Mode 0 (generates input on GPU, runs model on GPU using tf.function):
```python
    Size Name
## RANDOM NUMBER GENERATION ##
      8B [CUDA memcpy HtoD]
1.0039KB [CUDA memset]
       - void tensorflow::functor::FillPhiloxRandomKernelLaunch...
      8B [CUDA memcpy HtoD]
      8B [CUDA memcpy DtoH]
      8B [CUDA memcpy HtoD]
      4B [CUDA memcpy HtoD]
      8B [CUDA memcpy HtoD]
## HERE THE COMPUTATION STARTS
18.375MB [CUDA memcpy DtoH]	## TF copies data to CPU
18.375MB [CUDA memcpy HtoD] ## 1. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [928]
      8B [CUDA memcpy DtoD]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 2. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [963]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 3. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [997]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 4. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1031]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 5. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1063]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 6. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1099]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 7. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1133]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 8. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1167]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 9. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1201]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 10. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1235]
       - void Eigen::internal::EigenMetaKernel...
## Merging of all results into a single Tensor and ten copies back to host
     30B [CUDA memcpy HtoD]
       - void tensorflow::functor::ColumnReduceMax16ColumnsKernel...
       - void tensorflow::functor::BlockReduceKernel...
      1B [CUDA memcpy DtoH]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
183.75MB [CUDA memcpy DtoH]
```

So we see that TF copies the data, that is already on the GPU to the CPU, and then in every iteration copies back to GPU, instead of just using the data that is already on the GPU.

## Mode 1 (generates input on CPU, runs model on GPU using tf.function):
```python
    Size Name
      8B [CUDA memcpy HtoD]
1.0039KB [CUDA memset]
      8B [CUDA memcpy HtoD]
      8B [CUDA memcpy DtoH]
      8B [CUDA memcpy HtoD]
      4B [CUDA memcpy HtoD]
      8B [CUDA memcpy HtoD]
## HERE THE COMPUTATION STARTS
18.375MB [CUDA memcpy HtoD] ## 1. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [911]
      8B [CUDA memcpy DtoD]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 2. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [946]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 3. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [980]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 4. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1012]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 5. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1046]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 6. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1080]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 7. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1116]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 8. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1150]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 9. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1184]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy HtoD] ## 10. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1218]
       - void Eigen::internal::EigenMetaKernel...
## Merging of all results into a single Tensor and ten copies back to host
     30B [CUDA memcpy HtoD]
       - void tensorflow::functor::ColumnReduceMax16ColumnsKernel...
       - void tensorflow::functor::BlockReduceKernel...
      1B [CUDA memcpy DtoH]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
18.375MB [CUDA memcpy DtoD]
183.75MB [CUDA memcpy DtoH]
```

Nearly identical to Mode 0, but here it is actually expected that the data gets copied over from CPU to GPU in every iteration.

## Mode 2 (generates input on GPU, runs model on GPU using tf.function): 
```python
    Size Name
## RANDOM NUMBER GENERATION ##
      8B [CUDA memcpy HtoD]
1.0039KB [CUDA memset]
       - void tensorflow::functor::FillPhiloxRandomKernelLaunch...
## HERE THE COMPUTATION STARTS
18.375MB [CUDA memcpy DtoH] ## TF copies data to CPU
      4B [CUDA memcpy HtoD]
      8B [CUDA memcpy HtoD]
18.375MB [CUDA memcpy HtoD] ## 1. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [907]
      8B [CUDA memcpy DtoD]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 1. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 2. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [991]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 2. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 3. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1076]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 3. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 4. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1161]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 4. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 5. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1246]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 5. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 6. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1333]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 6. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 7. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1420]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 7. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 8. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1509]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 8. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 9. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1594]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 9. TF copies result to CPU
18.375MB [CUDA memcpy HtoD] ## 10. TF copies data to GPU
       - Mul_GPU_DT_FLOAT_DT_FLOAT_kernel [1679]
       - void Eigen::internal::EigenMetaKernel...
18.375MB [CUDA memcpy DtoH] ## 10. TF copies result to CPU
```

Similar to mode 0, but here the results get immediatly copied back to the host.

## Mode 3 (generates input on GPU, runs model on GPU using eager mode): 
```python
    Size Name
## RANDOM NUMBER GENERATION ##
      8B [CUDA memcpy HtoD]
1.0039KB [CUDA memset]
       - void tensorflow::functor::FillPhiloxRandomKernelLaunch...
## HERE THE COMPUTATION STARTS
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [806]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [810]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [814]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [818]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [822]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [826]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [830]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [834]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [838]
       - AddV2_GPU_DT_FLOAT_DT_FLOAT_kernel [842]
```

In this case no uncessary memcopies occur, data is kept on GPU all the time.

## Summary
When we use `model.predict(...)` or `model.predict_on_batch()` (or any other of these Keras.Model functions that use `tf.function`), then the input data ALWAYS gets copied to the host first, and then in every iteration back to the GPU. This causes an significant performance penalty.

I have not been able to find any documentation about this behavior, if it is intended, or a way to prevent this to happen.

Here also the `tf.debugging.set_log_device_placement(True)` output. I think the line `input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0` indicates that for whatever reason the `tf.function`'s input is expected to be on the CPU.

```python
2023-06-15 14:44:14.959681: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960017: I tensorflow/core/common_runtime/placer.cc:114] resource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960041: I tensorflow/core/common_runtime/placer.cc:114] VarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960475: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960895: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
value: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960922: I tensorflow/core/common_runtime/placer.cc:114] value: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.960945: I tensorflow/core/common_runtime/placer.cc:114] AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.961429: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.963362: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
components_0: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.963837: I tensorflow/core/common_runtime/placer.cc:114] components_0: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0
TensorDataset: (TensorDataset): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.963872: I tensorflow/core/common_runtime/placer.cc:114] TensorDataset: (TensorDataset): /job:localhost/replica:0/task:0/device:CPU:0
handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.963889: I tensorflow/core/common_runtime/placer.cc:114] handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.964369: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op TensorDataset in device /job:localhost/replica:0/task:0/device:CPU:0
input__dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.980981: I tensorflow/core/common_runtime/placer.cc:114] input__dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
FlatMapDataset: (FlatMapDataset): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.981006: I tensorflow/core/common_runtime/placer.cc:114] FlatMapDataset: (FlatMapDataset): /job:localhost/replica:0/task:0/device:CPU:0
handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.981024: I tensorflow/core/common_runtime/placer.cc:114] handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.982115: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op FlatMapDataset in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.982841: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
input__dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.983325: I tensorflow/core/common_runtime/placer.cc:114] input__dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
buffer__size: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.983348: I tensorflow/core/common_runtime/placer.cc:114] buffer__size: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
PrefetchDataset: (PrefetchDataset): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.983387: I tensorflow/core/common_runtime/placer.cc:114] PrefetchDataset: (PrefetchDataset): /job:localhost/replica:0/task:0/device:CPU:0
handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.983403: I tensorflow/core/common_runtime/placer.cc:114] handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.984096: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:CPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.984658: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.984692: I tensorflow/core/common_runtime/placer.cc:114] ReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:CPU:0
value_RetVal: (_DeviceRetval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.984710: I tensorflow/core/common_runtime/placer.cc:114] value_RetVal: (_DeviceRetval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.985212: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.985724: I tensorflow/core/common_runtime/placer.cc:114] input: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.985761: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.985778: I tensorflow/core/common_runtime/placer.cc:114] output_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.986346: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.986722: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.986812: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.988278: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0
resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.988585: I tensorflow/core/common_runtime/placer.cc:114] resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.988624: I tensorflow/core/common_runtime/placer.cc:114] value: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.988643: I tensorflow/core/common_runtime/placer.cc:114] AssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.989152: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.989472: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.989562: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:14.989661: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.989731: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0
handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.990139: I tensorflow/core/common_runtime/placer.cc:114] handle_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
AnonymousIteratorV3: (AnonymousIteratorV3): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.990173: I tensorflow/core/common_runtime/placer.cc:114] AnonymousIteratorV3: (AnonymousIteratorV3): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.990613: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op AnonymousIteratorV3 in device /job:localhost/replica:0/task:0/device:CPU:0
dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.990976: I tensorflow/core/common_runtime/placer.cc:114] dataset: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
iterator: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.990998: I tensorflow/core/common_runtime/placer.cc:114] iterator: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
MakeIterator: (MakeIterator): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.991016: I tensorflow/core/common_runtime/placer.cc:114] MakeIterator: (MakeIterator): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.991473: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:14.992481: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32
         [[{{node Placeholder/_0}}]]
args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.002874: I tensorflow/core/common_runtime/placer.cc:114] args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
GeneratorDataset: (GeneratorDataset): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.002917: I tensorflow/core/common_runtime/placer.cc:114] GeneratorDataset: (GeneratorDataset): /job:localhost/replica:0/task:0/device:CPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.002935: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.002966: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.002983: I tensorflow/core/common_runtime/placer.cc:114] FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.003007: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.007846: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op AnonymousIteratorV3 in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.007991: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:CPU:0
args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011294: I tensorflow/core/common_runtime/placer.cc:114] args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
GeneratorDataset: (GeneratorDataset): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011320: I tensorflow/core/common_runtime/placer.cc:114] GeneratorDataset: (GeneratorDataset): /job:localhost/replica:0/task:0/device:CPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011339: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011355: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011373: I tensorflow/core/common_runtime/placer.cc:114] FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.011389: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.018501: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.018640: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op Identity in device /job:localhost/replica:0/task:0/device:GPU:0
iterator: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.046638: I tensorflow/core/common_runtime/placer.cc:114] iterator: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
assignaddvariableop_resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046661: I tensorflow/core/common_runtime/placer.cc:114] assignaddvariableop_resource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0
IteratorGetNext: (IteratorGetNext): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.046678: I tensorflow/core/common_runtime/placer.cc:114] IteratorGetNext: (IteratorGetNext): /job:localhost/replica:0/task:0/device:CPU:0
model/tf.__operators__.add/AddV2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046695: I tensorflow/core/common_runtime/placer.cc:114] model/tf.__operators__.add/AddV2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0
AssignAddVariableOp: (AssignAddVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046711: I tensorflow/core/common_runtime/placer.cc:114] AssignAddVariableOp: (AssignAddVariableOp): /job:localhost/replica:0/task:0/device:GPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046726: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046759: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046783: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0
Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.046798: I tensorflow/core/common_runtime/placer.cc:114] Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
2023-06-15 14:44:15.051534: I tensorflow/core/common_runtime/eager/execute.cc:1525] Executing op __inference_predict_function_68 in device /job:localhost/replica:0/task:0/device:GPU:0
args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054027: I tensorflow/core/common_runtime/placer.cc:114] args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054141: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054201: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054272: I tensorflow/core/common_runtime/placer.cc:114] FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054342: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.054415: I tensorflow/core/common_runtime/placer.cc:114] PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061278: I tensorflow/core/common_runtime/placer.cc:114] args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061356: I tensorflow/core/common_runtime/placer.cc:114] PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061419: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061476: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061556: I tensorflow/core/common_runtime/placer.cc:114] FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.061612: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.067674: I tensorflow/core/common_runtime/placer.cc:114] args_0: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0
PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.067767: I tensorflow/core/common_runtime/placer.cc:114] PyFunc: (PyFunc): /job:localhost/replica:0/task:0/device:CPU:0
NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.067834: I tensorflow/core/common_runtime/placer.cc:114] NoOp: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.067905: I tensorflow/core/common_runtime/placer.cc:114] Identity: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.067969: I tensorflow/core/common_runtime/placer.cc:114] FakeSink0: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
2023-06-15 14:44:15.068020: I tensorflow/core/common_runtime/placer.cc:114] identity_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:CPU:0
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import sys

assert len(sys.argv) == 2, ""needs to be run as `python3 error.py MODE`""

mode = int(sys.argv[1])

inp = tf.keras.Input((3, 224, 224))
out = inp + inp
model = tf.keras.Model(inp, out)

class Sequence(tf.keras.utils.Sequence):
        def __init__(self, x):          self.x = x
        def __len__(self):              return 10
        def __getitem__(self, idx):     return self.x

with tf.device('/GPU:0' if mode != 1 else '/CPU:0'):
        data = tf.random.uniform((32, 3, 224, 224))

with tf.device('/GPU:0'):
        if mode == 2:
                for _ in range(10):
                        model.predict_on_batch(data)
        elif mode == 3:
                for _ in range(10):
                        model(data)
        else:
                seq = Sequence(data)
                model.predict(seq)
```


### Relevant log output

```shell
see above
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:performance', 'TF 2.12']",2023-06-15T12:51:29Z,3,0,https://github.com/tensorflow/tensorflow/issues/60883,
359,tensorflow/tensorflow,tf.data debug mode breaks dataset.save(),"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

2.13.0rc0

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

dataset.save() does not work if the experimental debug mode has been enabled for the datasets. A simple reproducer and the exception are below. I think the issue is due to the following code in `set_save_dataset_attributes` function in `tensorflow/python/data/ops/save_op.py`:

`shard_func = lambda *x: None  # a dummy function that will not be used`

Replacing this line with e.g.

`shard_func = lambda *x: 0`

seems to fix this issue, so apparently returning `None` doesn't work in the debug mode. Btw the comment on this line is a bit misleading, because this function is still traced, so it's not completely unused.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import tempfile
tf.data.experimental.enable_debug_mode()
ds = tf.data.Dataset.from_tensor_slices([1.0, 2.0, 3.0])
with tempfile.TemporaryDirectory() as tmpdir:
     ds.save(tmpdir)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1042, in convert
    x = ops.convert_to_tensor_or_composite(x)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1547, in convert_to_tensor_or_composite
    return internal_convert_to_tensor_or_composite(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1582, in internal_convert_to_tensor_or_composite
    return convert_to_tensor(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped
    return func(*args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1443, in convert_to_tensor
    return tensor_conversion_registry.convert(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 209, in convert
    return overload(dtype, name)  #  pylint: disable=not-callable
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 2177, in __tf_tensor__
    raise TypeError(""can't convert Operation '{}' to Tensor"".format(self.name))
TypeError: can't convert Operation 'EagerPyFunc' to Tensor

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1746, in save
    return save_op._save(self, path, compression, shard_func, checkpoint_args)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/data/ops/save_op.py"", line 57, in _save
    dataset, shard_func, use_shard_func, path = set_save_dataset_attributes(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/data/ops/save_op.py"", line 103, in set_save_dataset_attributes
    wrapped_func = structured_function.StructuredFunctionWrapper(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/data/ops/structured_function.py"", line 272, in __init__
    self._function = fn_factory()
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1189, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1169, in _get_concrete_function_garbage_collected
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 694, in _initialize
    self._variable_creation_fn    # pylint: disable=protected-access
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 176, in _get_concrete_function_internal_garbage_collected
    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 171, in _maybe_define_concrete_function
    return self._maybe_define_function(args, kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 398, in _maybe_define_function
    concrete_function = self._create_concrete_function(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 305, in _create_concrete_function
    func_graph_module.func_graph_from_py_func(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1060, in func_graph_from_py_func
    func_outputs = nest.map_structure(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/util/nest.py"", line 624, in map_structure
    return nest_util.map_structure(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py"", line 1054, in map_structure
    return _tf_core_map_structure(func, *structure, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py"", line 1094, in _tf_core_map_structure
    [func(*x) for x in entries],
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/util/nest_util.py"", line 1094, in <listcomp>
    [func(*x) for x in entries],
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 1044, in convert
    raise TypeError(
TypeError: To be compatible with tf.function, Python functions must return zero or more Tensors or ExtensionTypes or None values; in compilation of <function StructuredFunctionWrapper.__init__.<locals>.trace_py_function.<locals>.wrapped_fn at 0x7f88c9224c10>, found return value of type Operation, which is not a Tensor or ExtensionType.
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.12']",2023-06-14T14:21:08Z,3,0,https://github.com/tensorflow/tensorflow/issues/60861,
360,tensorflow/tensorflow,"CopyTensor::ViaDMA function, allocator type sometimes not match actual input underlying memory type","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0rc0

### Custom Code

Yes

### OS Platform and Distribution

CentOS Linux 7

### Mobile device

_No response_

### Python version

3.7.5

### Bazel version

bazel 3.7.2

### GCC/Compiler version

gcc-9

### CUDA/cuDNN version

cuda 11, cudnn 8

### GPU model and memory

Tesla V100S

### Current Behaviour?

In `CopyTensor::ViaDMA`, `alloc_attr `decides the direction of memory copy. However, sometimes `alloc_attr `does not keep the same as the Tensor pointer's underlying memory type. In my case,`src_alloc_attr.on_host()` is `True`, but `input->GetMemoryType()` equals to `kDevice`. So this results the memory copy direction in this function is cpu->gpu, but actually the direction should be gpu -> gpu. 
I think this bug does not reveal is because the cuda driver api, like `cuMemcpyHtoD()`, does not care about the direction if it's H to D or others, it only cares about the pointer attribute, if the src pointer is on device and dst pointer is also on device, even if we call `cuMemcpyHtoD()`, cuda driver would still do D to D copy. This feature would cover many bugs. 

I haven't figured out where did the `on_host `attribute is set. From my understanding so far, same allocator object would be reused on different tensors, but the `on_host `attribute is one-way, once it's been set `on_host`, it cannot be unset later. This might cause some issue? Also, why wouln't we just use `input->GetMemoryType()` to decieds the memory copy direction, instead of the `on_host `attribute of `alloc_attr`

I meet this issue when I run horovod unit test case. Add some log message in 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/copy_tensor.cc#L219
, such as 
```
if(!src_alloc_attr.on_host() && (input->GetMemoryType()==AllocatorMemoryType::kHostPageable || input->GetMemoryType()==AllocatorMemoryType::kHostPinned)){
    std::cout<<""!!!!!!! src alloc not on host, but input mem type is on host""<< std::endl;
  }
  if( src_alloc_attr.on_host() && input->GetMemoryType()==AllocatorMemoryType::kDevice) {
    std::cout<<""!!!!!!! src alloc on host, but input mem is on device"" << std::endl;
  }

  if(!dst_alloc_attr.on_host() && (output->GetMemoryType()==AllocatorMemoryType::kHostPageable || output->GetMemoryType()==AllocatorMemoryType::kHostPinned)){
    std::cout<<""!!!!!!! dst alloc not on host, but output mem type is on host""<< std::endl;
  }
  if( dst_alloc_attr.on_host() && output->GetMemoryType()==AllocatorMemoryType::kDevice) {
    std::cout<<""!!!!!!! dst alloc on host, but output mem is on device"" << std::endl;
  }

```
For me, I ran horovod `alltoall Op` unit test case to reproduce this issue. But this issue might reveal in other cases.

### Standalone code to reproduce the issue

```shell
Run horovod unit test case can reproduce this issue:
https://github.com/horovod/horovod/blob/master/test/parallel/test_tensorflow.py
horovodrun --mpi -np 2 pytest -s -v test_tensorflow.py::TensorFlowTests::test_horovod_alltoall_gpu.
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:runtime', 'TF 2.12']",2023-06-14T03:09:42Z,0,0,https://github.com/tensorflow/tensorflow/issues/60856,
361,tensorflow/tensorflow,OMP_PROC_BIND or OMP_PLACES either ignored or respected incorrectly,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.11

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.4

### Bazel version

5.1.1

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

N/A

### GPU model and memory

N/A

### Current Behaviour?

When I run TensorFlow on CPU and try to enable core binding with `OMP_PROC_BIND=close`, all threads get bound to core 0 (rather than thread 0 to core 0, thread 1 to core 1, etc).

Expected output:

```
$ OMP_PROC_BIND=true OMP_PLACES=cores python tf_example.py
Inter_op_threads: 1
Intra_op_threads: 4
Thread count: 4
Child affinity is {96}
Child affinity is {97}
Child affinity is {98}
Child affinity is {99}
```
I.e. I'd expect four threads, bound to subsequent cores. Now, I guess TensorFlow simply uses some threads for management of the framework. Though I'm surprised by the large number of threads, 11 extra threads on top of the Intra_op_thread count, see the log output for the custom TF-2.11 case, this is not really an 'issue' (although one might wonder how management threads ought to behave, they should probably remain unbound even if the compute threads are bound 1 per core).

What I'm seeing however is that with our custom built TF-2.11, all threads are bound to the first core in my cgroup (core ID 96 in this case). For tf-nightly, all binding is completely ignored.

Not sure if it's useful, but the Custom built had this build command:

<details>
<summary>build command</summary>

```
bazel --output_user_root=/tmp/jenkins/build/TensorFlow/2.11.0/foss-2022a/TensorFlow/bazel-root --local_startup_timeout_secs=300 --host_jvm_args=-Xms512m --host_jvm_args=-Xmx4096m build --config=noaws --config=nogcp --config=nohdfs --compilation_mode=opt --config=opt --subcommands --verbose_failures --jobs=128 --copt=""-fPIC"" --distinct_host_configuration=false --action_env=CPATH='/sw/arch/RHEL8/EB_production/2022/software/cURL/7.83.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/double-conversion/3.2.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/flatbuffers/2.0.7-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/giflib/5.2.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/hwloc/2.7.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/ICU/71.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/JsonCpp/1.9.5-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/libjpeg-turbo/2.1.3-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/libpng/1.6.37-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/LMDB/0.9.29-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/nsync/1.25.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/protobuf/3.19.4-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/pybind11/2.9.2-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/snappy/1.1.9-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/SQLite/3.38.3-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/zlib/1.2.12-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/OpenSSL/1.1/include' --host_action_env=CPATH='/sw/arch/RHEL8/EB_production/2022/software/cURL/7.83.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/double-conversion/3.2.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/flatbuffers/2.0.7-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/giflib/5.2.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/hwloc/2.7.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/ICU/71.1-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/JsonCpp/1.9.5-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/libjpeg-turbo/2.1.3-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/libpng/1.6.37-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/LMDB/0.9.29-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/nsync/1.25.0-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/protobuf/3.19.4-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/pybind11/2.9.2-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/snappy/1.1.9-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/SQLite/3.38.3-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/zlib/1.2.12-GCCcore-11.3.0/include:/sw/arch/RHEL8/EB_production/2022/software/OpenSSL/1.1/include' --action_env=LIBRARY_PATH='/sw/arch/RHEL8/EB_production/2022/software/cURL/7.83.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/double-conversion/3.2.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/flatbuffers/2.0.7-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/giflib/5.2.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/hwloc/2.7.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/ICU/71.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/JsonCpp/1.9.5-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/libjpeg-turbo/2.1.3-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/libpng/1.6.37-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/LMDB/0.9.29-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/nsync/1.25.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/protobuf/3.19.4-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/pybind11/2.9.2-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/snappy/1.1.9-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/SQLite/3.38.3-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/zlib/1.2.12-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/OpenSSL/1.1/lib' --host_action_env=LIBRARY_PATH='/sw/arch/RHEL8/EB_production/2022/software/cURL/7.83.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/double-conversion/3.2.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/flatbuffers/2.0.7-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/giflib/5.2.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/hwloc/2.7.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/ICU/71.1-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/JsonCpp/1.9.5-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/libjpeg-turbo/2.1.3-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/libpng/1.6.37-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/LMDB/0.9.29-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/nsync/1.25.0-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/protobuf/3.19.4-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/pybind11/2.9.2-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/snappy/1.1.9-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/SQLite/3.38.3-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/zlib/1.2.12-GCCcore-11.3.0/lib:/sw/arch/RHEL8/EB_production/2022/software/OpenSSL/1.1/lib' --action_env=PYTHONNOUSERSITE='1' --host_action_env=PYTHONNOUSERSITE='1' --action_env=PYTHONPATH --host_action_env=PYTHONPATH  //tensorflow/tools/pip_package:build_pip_package
```

</details>

I'm not 100% sure of all the details of the custom build, I used EasyBuild to build TF from sources, and the build recipy wasn't made by me.

I don't understand enough of the TensorFlow threading model to know how to debug this issue. My specific questions would be:

- Why does my custom TF 2.11 build bind all threads to one core? Other GOMP-based packages (e.g. `scipy`) do show correct binding on my system, with the same (OMP_) environment variables.
- Why does tf-nightly not bind threads at all?

Any general explanations of which potential threading models can be used in TF are also welcome.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

tf.config.threading.set_inter_op_parallelism_threads(1)
tf.config.threading.set_intra_op_parallelism_threads(4)

print(f""Inter_op_threads: {tf.config.threading.get_inter_op_parallelism_threads()}"")
print(f""Intra_op_threads: {tf.config.threading.get_intra_op_parallelism_threads()}"")

A = tf.random.normal([20000,20000])
for i in range(0,1):
    B = tf.multiply(A,A)

import psutil
import os

current_process = psutil.Process()
threads = current_process.threads()
print(f""Thread count: {len(threads)}"")
for thread in threads:
    print('Child affinity is {}'.format(os.sched_getaffinity(thread.id)))
```


### Relevant log output

```shell
# output for custom build TF-2.11
$ OMP_PROC_BIND=true OMP_PLACES=cores python tf_example.py
Inter_op_threads: 1
Intra_op_threads: 4
Thread count: 15
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}
Child affinity is {96}

# output for tf-nightly
# Not sure what threading model tf-nightly uses, so I've set both OMP and KMP variables:
$ OMP_PROC_BIND=true OMP_PLACES=cores KMP_AFFINITY=granularity=fine,verbose,compact,1,0 python tf_example.py

Inter_op_threads: 1
Intra_op_threads: 4
Thread count: 14
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}
Child affinity is {96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127}


# Note that this is generated on an HPC system in which I'm in a CGROUP with access to core 96-127, hence the core IDs start at 96.
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:performance', 'TF 2.11']",2023-06-12T16:23:43Z,2,0,https://github.com/tensorflow/tensorflow/issues/60843,
362,tensorflow/tensorflow,Duplicate logging inside custom training loop,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

TF 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

Windows WSL Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

cuDNN version 8600

### GPU model and memory

GTX 1070

### Current Behaviour?

Hey guys.
I am experiencing a strange behavior when I try to print something from inside my custom training loop.
I want to run the code example as a .ipynb from a Windows machine with WSL Ubuntu 20.04. (https://github.com/m5k5/tf-example). 
The code loops over the dataset and prints out the log statements but it somehow resets the steps after a while:
""
Log at step 80
train step at step  81
train step at step  82
train step at step  83
train step at step  84
train step at step  85
train step at step  86
train step at step  0
Log at step 0
train step at step  1
train step at step  2
train step at step  3
train step at step  4
train step at step  5
""

Originally, I have a project where I need to use the WSL to get the GPU processing within Windows. The print statements are used for logging metrics like accuracy and loss. The values themselves are calculated correctly but there are a lot of duplicates that I can not explain.
Does anyone have similar issues?
Thanks in advance!

### Standalone code to reproduce the issue

```shell
https://github.com/m5k5/tf-example
```


### Relevant log output

```shell
Start of epoch 1
2023-06-10 19:30:47.154065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]
	 [[{{node Placeholder/_1}}]]
2023-06-10 19:30:47.154065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]
	 [[{{node Placeholder/_1}}]]
2023-06-10 19:30:47.766468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600
2023-06-10 19:30:47.154065: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype uint8 and shape [60000]
	 [[{{node Placeholder/_1}}]]
2023-06-10 19:30:47.766468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600
2023-06-10 19:30:48.254207: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f2e52d1a840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-06-10 19:30:48.254240: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1
2023-06-10 19:30:48.256794: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-06-10 19:30:48.348961: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
train step at step  0
Log at step 0
train step at step  1
train step at step  2
train step at step  3
train step at step  4
train step at step  5
train step at step  6
train step at step  7
train step at step  8
train step at step  9
train step at step  10
train step at step  11
train step at step  12
train step at step  13
train step at step  14
train step at step  15
train step at step  16
train step at step  17
train step at step  18
train step at step  19
train step at step  20
Log at step 20
train step at step  21
train step at step  22
train step at step  23
train step at step  24
train step at step  25
train step at step  26
train step at step  27
train step at step  28
train step at step  29
train step at step  30
train step at step  31
train step at step  32
train step at step  33
train step at step  34
train step at step  35
train step at step  36
train step at step  37
train step at step  38
train step at step  39
train step at step  40
Log at step 40
train step at step  41
train step at step  42
train step at step  43
train step at step  44
train step at step  45
train step at step  46
train step at step  47
train step at step  48
train step at step  49
train step at step  50
train step at step  51
train step at step  52
train step at step  53
train step at step  54
train step at step  55
train step at step  56
train step at step  57
train step at step  58
train step at step  59
train step at step  60
Log at step 60
train step at step  61
train step at step  62
train step at step  63
train step at step  64
train step at step  65
train step at step  66
train step at step  67
train step at step  68
train step at step  69
train step at step  70
train step at step  71
train step at step  72
train step at step  73
train step at step  74
train step at step  75
train step at step  76
train step at step  77
train step at step  78
train step at step  79
train step at step  80
Log at step 80
train step at step  81
train step at step  82
train step at step  83
train step at step  84
train step at step  85
train step at step  86
train step at step  0
Log at step 0
train step at step  1
train step at step  2
train step at step  3
train step at step  4
train step at step  5
train step at step  6
train step at step  7
train step at step  8
train step at step  9
train step at step  10
train step at step  11
train step at step  12
train step at step  13
train step at step  14
train step at step  15
train step at step  16
train step at step  17
train step at step  18
train step at step  19
train step at step  20
Log at step 20
train step at step  21
train step at step  22
train step at step  23
train step at step  24
train step at step  25
train step at step  26
train step at step  27
train step at step  28
train step at step  29
train step at step  30
train step at step  31
train step at step  32
train step at step  33
train step at step  34
train step at step  35
train step at step  36
train step at step  37
train step at step  38
train step at step  39
train step at step  40
Log at step 40
train step at step  41
train step at step  42
train step at step  43
train step at step  44
train step at step  45
train step at step  46
train step at step  47
train step at step  48
train step at step  49
train step at step  50
train step at step  51
train step at step  52
train step at step  53
train step at step  54
train step at step  55
train step at step  56
train step at step  57
train step at step  58
train step at step  59
train step at step  60
Log at step 60
train step at step  61
train step at step  62
train step at step  63
train step at step  64
train step at step  65
train step at step  66
train step at step  67
train step at step  68
train step at step  69
train step at step  70
train step at step  71
train step at step  72
train step at step  73
train step at step  74
train step at step  75
train step at step  76
train step at step  77
train step at step  78
train step at step  79
train step at step  80
Log at step 80
train step at step  81
train step at step  82
train step at step  83
train step at step  84
train step at step  85
train step at step  86
train step at step  87
train step at step  88
train step at step  89
train step at step  90
train step at step  91
train step at step  92
train step at step  93
train step at step  94
train step at step  95
train step at step  96
train step at step  97
train step at step  98
train step at step  99
train step at step  100
Log at step 100
train step at step  101
train step at step  102
train step at step  103
train step at step  104
train step at step  105
train step at step  106
train step at step  107
train step at step  108
train step at step  109
train step at step  110
train step at step  111
train step at step  112
train step at step  113
train step at step  114
train step at step  115
train step at step  116
train step at step  117
train step at step  118
train step at step  119
train step at step  120
Log at step 120
train step at step  121
train step at step  122
train step at step  123
train step at step  124
train step at step  125
train step at step  126
train step at step  127
train step at step  128
train step at step  129
train step at step  130
train step at step  131
train step at step  132
train step at step  133
train step at step  134
train step at step  135
train step at step  136
train step at step  137
train step at step  138
train step at step  139
train step at step  140
Log at step 140
train step at step  141
train step at step  142
train step at step  143
train step at step  144
train step at step  145
train step at step  146
train step at step  147
train step at step  148
train step at step  149
train step at step  150
train step at step  151
train step at step  152
train step at step  153
train step at step  154
train step at step  155
train step at step  156
train step at step  157
train step at step  158
train step at step  159
train step at step  160
Log at step 160
train step at step  161
train step at step  162
train step at step  163
train step at step  164
train step at step  165
train step at step  166
train step at step  167
train step at step  168
train step at step  169
train step at step  170
train step at step  171
train step at step  172
train step at step  173
train step at step  174
train step at step  0
Log at step 0
train step at step  1
train step at step  2
train step at step  3
train step at step  4
train step at step  5
train step at step  6
train step at step  7
train step at step  8
train step at step  9
train step at step  10
train step at step  11
train step at step  12
train step at step  13
train step at step  14
train step at step  15
train step at step  16
train step at step  17
train step at step  18
train step at step  19
train step at step  20
Log at step 20
train step at step  21
train step at step  22
train step at step  23
train step at step  24
train step at step  25
train step at step  26
train step at step  27
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:core', 'TF 2.12']",2023-06-10T17:49:40Z,9,0,https://github.com/tensorflow/tensorflow/issues/60833,
363,tensorflow/tensorflow,Unable to run RNN Model,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

WSL2 on Windows 11

### Mobile device

_No response_

### Python version

3.8.12

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1.0

### GPU model and memory

NVIDIA Quadro P4200, 16gb

### Current Behaviour?

After setting up the system to use gpu from WSL2 on windows 11, I am able to connect to GPU. But when I run  a tensorflow model, I am able to run CNN model on GPU but when i try to run RNN model I get the below mentioned message and the model does not run:

""W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at partitioned_function_ops.cc:115 : INVALID_ARGUMENT: No OpKernel was registered to support Op 'CudnnRNN' used by {{node CudnnRNN}} with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=""linear_input"", direction=""unidirectional"", rnn_mode=""lstm"", seed2=0, is_training=true]
Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>

	 [[CudnnRNN]]""

### Standalone code to reproduce the issue

```shell
Trying RNN model with LSTM
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'comp:apis', 'wsl2', 'TF 2.10']",2023-06-09T12:01:54Z,5,0,https://github.com/tensorflow/tensorflow/issues/60826,
364,tensorflow/tensorflow,"tf.keras.Model.predict passing x=tf.keras.utils.Sequence causing exceptions  ValueError: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```Python
tf_model.predict(test_generator)
```

will cause exceptions

```Python
ValueError: in user code:

    File ""/DATA/home/yuehc/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 2169, in predict_function  *
        return step_function(self, iterator)
    File ""/DATA/home/yuehc/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 2155, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/DATA/home/yuehc/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 2143, in run_step  **
        outputs = model.predict_step(data)
    File ""/DATA/home/yuehc/.local/lib/python3.8/site-packages/keras/engine/training.py"", line 2110, in predict_step
        x, _, _ = data_adapter.unpack_x_y_sample_weight(data)
    File ""/DATA/home/yuehc/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py"", line 1775, in unpack_x_y_sample_weight
        raise ValueError(error_msg)

    ValueError: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: (<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, None, None) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, None, None) dtype=float32>)
```

The codes works well back in tf 2.4.0 and should still be working per release notes and the latest documentations.

the `test_generator` is a sub-class of `tf.keras.utils.Sequence` which works normally in tf 2.4.0

The return format for `__getitem__` is a tuple with a List element that indicate multiple inputs of the model
```Python
def __getitem__(self, i) -> Tuple[List[numpy.array]]:
    ...
```

For an example, see below
![image](https://github.com/tensorflow/tensorflow/assets/45007045/b8948609-693e-4918-8f1b-990fb1fa8bc8)
![image](https://github.com/tensorflow/tensorflow/assets/45007045/0c41d2ce-7460-405b-867a-103e6a794cd2)
![image](https://github.com/tensorflow/tensorflow/assets/45007045/51b80f7a-9c39-4ea4-93ee-7812c9765fa7)

I assume some behavors behind the `predict` interface are changed after the 2.4.0 to 2.12.0 upgrades.

Any idea how to fix it? Thx in advance.

### Standalone code to reproduce the issue

```Python
import tensorflow as tf
import math
import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Dense, Input

class BasicGenerator(tf.keras.utils.Sequence):

    def __init__(self, x, y=None, w=None, *, obj=None, batch_size=256, **kwargs):
        """"""
        a simple example of data generator by utilizing tf.keras.utils.Sequence

        Parameters
        ----------
        x: List[np.array], input data, the array size: (n_samples, n_features)

        y: np.arrya, input label, size: (n_samples, n_labels)

        w: sample weights, default None

        batch_size: batch_size, default 256
        """"""
        super(BasicGenerator, self).__init__()

        # basic params
        self.x = x
        self.y = y
        self.w = w
        self.batch_size = batch_size
        
    def __len__(self):
        return math.ceil(self.x[0].shape[0] / self.batch_size)

    def __getitem__(self, index):
        
        res = ()

        b_x = [_x[index * self.batch_size:(index + 1) * self.batch_size] for _x in self.x]
        res += (b_x,)

        if self.y is not None:
            b_y = self.y[index * self.batch_size:(index + 1) * self.batch_size]
            res += (b_y,)

        if self.w is not None:
            b_w = self.w[index * self.batch_size:(index + 1) * self.batch_size]
            res += (b_w,)
        
        return res

# Define the model
input1 = Input(shape=(3, 1))
input2 = Input(shape=(3, 1))

lstm1 = LSTM(50, activation='relu')(input1)
lstm2 = LSTM(50, activation='relu')(input2)

concat = tf.keras.layers.concatenate([lstm1, lstm2])
output = Dense(1)(concat)

model = Model(inputs=[input1, input2], outputs=output)
model.compile(optimizer='adam', loss='mse')

# Generate some fake data
n_samples = 10000
X_1 = np.random.rand(n_samples, 3, 1)
X_2 = np.random.rand(n_samples, 3, 1)
y = np.random.rand(n_samples, 1)

# generator sub-classing from utils.Sequence
train_generator = BasicGenerator(x=[X_1, X_2], y=y)
test_generator = BasicGenerator(x=[X_1, X_2])

# fit
model.fit(train_generator)

# the problem occurs
model.predict(
    test_generator,
    verbose=1,
    max_queue_size=30,
    workers=1,
    use_multiprocessing=False
)
```


### Relevant log output

```Python
40/40 [==============================] - 7s 14ms/step - loss: 0.1985
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-13-82ba4c04abb5> in <module>
     76 
     77 # the problem occurs
---> 78 model.predict(
     79     test_generator,
     80     verbose=1,

/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---> 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/training.py in tf__predict_function(iterator)
     13                 try:
     14                     do_return = True
---> 15                     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16                 except:
     17                     do_return = False

ValueError: in user code:

    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/training.py"", line 2169, in predict_function  *
        return step_function(self, iterator)
    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/training.py"", line 2155, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/training.py"", line 2143, in run_step  **
        outputs = model.predict_step(data)
    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/training.py"", line 2111, in predict_step
        return self(x, training=False)
    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/utils/traceback_utils.py"", line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File ""/DATA1/anaconda3/envs/py38tf2.4/lib/python3.8/site-packages/keras/engine/input_spec.py"", line 219, in assert_input_compatibility
        raise ValueError(

    ValueError: Layer ""model_3"" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, None, None) dtype=float32>]
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF 2.12']",2023-06-08T10:16:56Z,4,0,https://github.com/tensorflow/tensorflow/issues/60813,
365,tensorflow/tensorflow,Exception in tf.function due to operation in inactive condition branch,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev20230606

### Custom Code

Yes

### OS Platform and Distribution

Windows 11 x64

### Mobile device

NA

### Python version

3.10

### Bazel version

NA

### GCC/Compiler version

NA

### CUDA/cuDNN version

NA

### GPU model and memory

NA

### Current Behaviour?

A function decorated with `@tf.function` may fail to execute if it contains a conditional operation where the non-executing branch cannot be executed correctly for the current inputs. Whether the issue arises or not may depend on the input signature given to `tf.function`. See example for clarity.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

@tf.function(input_signature=[tf.TensorSpec([None], tf.int32)])
def f(x):
    return tf.cond(tf.size(x) == 1,
                   # The reshape in this branch can only execute properly when the condition is true
                   lambda: tf.fill(tf.shape(x), tf.reshape(x, ())),
                   lambda: x)

# Works: this input is valid for both condition branches
tf.print(f(tf.constant([1])))
# [1]

# Fails: this input is only valid for the false branch, which is the active one
tf.print(f(tf.constant([1, 2])))
# tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error: (see below)


# If the shape in the function input signature is left completely undefined it works
# If the tf.function is defined with no input signature it works correctly as well

@tf.function(input_signature=[tf.TensorSpec(None, tf.int32)])
def f(x):
    return tf.cond(tf.size(x) == 1,
                   lambda: tf.fill(tf.shape(x), tf.reshape(x, ())),
                   lambda: x)

tf.print(f(tf.constant([1])))
# [1]

tf.print(f(tf.constant([1, 2])))
# [1 2]
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ...
  File ""...\site-packages\tensorflow\python\util\traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""...\site-packages\tensorflow\python\eager\execute.py"", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:

Detected at node cond/Reshape defined at (most recent call last):
  ...

Input to reshape is a tensor with 2 values, but the requested shape has 1
         [[{{node cond/Reshape}}]] [Op:__inference_f_23]
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:tf.function', 'TF 2.12']",2023-06-07T16:12:49Z,4,0,https://github.com/tensorflow/tensorflow/issues/60805,
366,tensorflow/tensorflow,could not run GPU on jupyter ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I could not get Tensorflow to run on GPUs.

TF sees the GPUs on terminal, but not on jupyter lab.

**Edited**
Found a solution to see it on jupyterlab, but must manually repeat . Still, erratic misconfiguration seems to happen.


### Standalone code to reproduce the issue

```shell
I could not get Tensorflow to run on GPUs.

TF sees the GPUs on terminal, but not on jupyter lab.

** edited **


Eventually, after hours, I found a temporary solution in setting the paths each time, before I launch `jupyter lab` :



CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib

```

NB : If I was to set 
```
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo 'CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
```

as in `https://www.tensorflow.org/install/pip`

Somehow it messy with jupyter, bcause the selected kernel from jupyter would not correspond to the kernel set from the script.

Now I can see the GPUs on jupyter, but still - it crashes! And before, on CPU, it was not.

When I run this script, using this library :

https://pypi.org/project/umap-learn/

installed as in the description:

```
embedder = ParametricUMAP(
    ## all the params ...
)

# now launch on GPU
with tf.device('/GPU:0'):
    embedding =  embedder.fit_transform(np.array([t.ravel() for t in train_data]))
```

the code fails with the log output below.

If I close the jupyter lab connection, go back on the conda environment, 
set again:

```
CUDNN_PATH=$(dirname $(python -c ""import nvidia.cudnn;print(nvidia.cudnn.__file__)""))
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib
```
I
and 

```
# Install NVCC
conda install -c nvidia cuda-nvcc=11.3.58
# Configure the XLA cuda directory
mkdir -p $CONDA_PREFIX/etc/conda/activate.d
printf 'export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/\n' >> $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
source $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
# Copy libdevice file to the required path
mkdir -p $CONDA_PREFIX/lib/nvvm/libdevice
cp $CONDA_PREFIX/lib/libdevice.10.bc $CONDA_PREFIX/lib/nvvm/libdevice/

```

And relaunch jupterlab, this time I get the GPU seen also in jupyter lab.

Running 

```
with tf.device('/GPU:0'):
    spec_embedding = spec_embedder.fit_transform(np.array([t.ravel() for t in train_data]))

```

It yields

```
2023-06-06 21:55:04.944257: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32
	 [[{{node gradients/split_grad/concat/split/split_dim}}]]
```

And cannot understand why it raise warning or error with `CPU` device, if I run on `GPU`. 

Not even sure it falled back to CPU, actually.


---

Please advice how to sync jupter lab and conda.
I did follow up with `ikernel` but it seems, after a lot of checking, that environment variables are not correctly read. Not sure if `kernel.json` fails to be updated properly, with thepath of the cuda libraries.

Please consider add a guide on:
https://www.tensorflow.org/install/pip

for running TF on jupyter.

My situation is that I need to run from a remote cluster, and I think it is a frequent situation.
hope this feedback is useful.
```


### Relevant log output

```shell
2023-06-06 21:40:08.385763: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32
	 [[{{node gradients/split_grad/concat/split/split_dim}}]]
2023-06-06 21:40:10.938429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600
2023-06-06 21:40:11.641291: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f89f489b8d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-06-06 21:40:11.641337: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5
2023-06-06 21:40:11.641346: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5
2023-06-06 21:40:11.641353: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5
2023-06-06 21:40:11.641359: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5
2023-06-06 21:40:11.646324: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-06-06 21:40:11.673008: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:530] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.
Searched for CUDA in the following directories:
  ./cuda_sdk_lib
  /usr/local/cuda-11.8
  /usr/local/cuda
  .
You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2023-06-06 21:40:11.673259: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.673658: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.673685: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: libdevice not found at ./libdevice.10.bc
	 [[{{node StatefulPartitionedCall_16}}]]
2023-06-06 21:40:11.700045: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.700414: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.729242: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.729590: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.755959: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.756308: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.783041: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.783397: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.809786: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.810134: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.836777: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.837129: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.864411: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.864767: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.892388: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.892738: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.919296: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.919647: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.946506: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.946866: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:11.974287: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:11.974699: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.245782: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.246188: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.272303: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.272657: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.322559: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.322899: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.350255: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.350736: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.379576: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.379966: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-06-06 21:40:12.406780: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:274] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-06-06 21:40:12.407233: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: libdevice not found at ./libdevice.10.bc

---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
Cell In[33], line 31
      7 spec_embedder = ParametricUMAP(
      8     metric = 'euclidean',
      9     min_dist = 0.1, 
   (...)
     27     n_training_epochs=1
     28 )
     30 with tf.device('/GPU:0'):
---> 31     spec_embedding = spec_embedder.fit_transform(np.array([t.ravel() for t in train_data]))

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py:217, in ParametricUMAP.fit_transform(self, X, y, precomputed_distances)
    215     return super().fit_transform(precomputed_distances, y)
    216 else:
--> 217     return super().fit_transform(X, y)

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/umap_.py:2772, in UMAP.fit_transform(self, X, y)
   2742 def fit_transform(self, X, y=None):
   2743     """"""Fit X into an embedded space and return that transformed
   2744     output.
   2745 
   (...)
   2770         Local radii of data points in the embedding (log-transformed).
   2771     """"""
-> 2772     self.fit(X, y)
   2773     if self.transform_mode == ""embedding"":
   2774         if self.output_dens:

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py:202, in ParametricUMAP.fit(self, X, y, precomputed_distances)
    200     return super().fit(precomputed_distances, y)
    201 else:
--> 202     return super().fit(X, y)

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/umap_.py:2684, in UMAP.fit(self, X, y)
   2681     print(ts(), ""Construct embedding"")
   2683 if self.transform_mode == ""embedding"":
-> 2684     self.embedding_, aux_data = self._fit_embed_data(
   2685         self._raw_data[index],
   2686         self.n_epochs,
   2687         init,
   2688         random_state,  # JH why raw data?
   2689     )
   2690     # Assign any points that are fully disconnected from our manifold(s) to have embedding
   2691     # coordinates of np.nan.  These will be filtered by our plotting functions automatically.
   2692     # They also prevent users from being deceived a distance query to one of these points.
   2693     # Might be worth moving this into simplicial_set_embedding or _fit_embed_data
   2694     disconnected_vertices = np.array(self.graph_.sum(axis=1)).flatten() == 0

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py:462, in ParametricUMAP._fit_embed_data(self, X, n_epochs, init, random_state)
    459     validation_data = None
    461 # create embedding
--> 462 history = self.parametric_model.fit(
    463     edge_dataset,
    464     epochs=self.loss_report_frequency * self.n_training_epochs,
    465     steps_per_epoch=steps_per_epoch,
    466     max_queue_size=100,
    467     validation_data=validation_data,
    468     **self.keras_fit_kwargs
    469 )
    470 # save loss history dictionary
    471 self._history = history.history

File ~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/miniconda3/envs/conda01/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50 try:
     51   ctx.ensure_initialized()
---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                       inputs, attrs, num_outputs)
     54 except core._NotOkStatusException as e:
     55   if name is not None:

InternalError: Graph execution error:

Detected at node 'StatefulPartitionedCall_16' defined at (most recent call last):
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/runpy.py"", line 86, in _run_code
      exec(code, run_globals)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel_launcher.py"", line 17, in <module>
      app.launch_new_instance()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/traitlets/config/application.py"", line 992, in launch_instance
      app.start()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/kernelapp.py"", line 711, in start
      self.io_loop.start()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/tornado/platform/asyncio.py"", line 215, in start
      self.asyncio_loop.run_forever()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/asyncio/base_events.py"", line 603, in run_forever
      self._run_once()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/asyncio/base_events.py"", line 1909, in _run_once
      handle._run()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/asyncio/events.py"", line 80, in _run
      self._context.run(self._callback, *self._args)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 510, in dispatch_queue
      await self.process_one()
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 499, in process_one
      await dispatch(*args)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 406, in dispatch_shell
      await result
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/kernelbase.py"", line 729, in execute_request
      reply_content = await reply_content
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/ipkernel.py"", line 411, in do_execute
      res = shell.run_cell(
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/ipykernel/zmqshell.py"", line 531, in run_cell
      return super().run_cell(*args, **kwargs)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3006, in run_cell
      result = self._run_cell(
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3061, in _run_cell
      result = runner(coro)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/async_helpers.py"", line 129, in _pseudo_sync_runner
      coro.send(None)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3266, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3445, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/IPython/core/interactiveshell.py"", line 3505, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""/tmp/ipykernel_3110734/3903349550.py"", line 31, in <module>
      spec_embedding = spec_embedder.fit_transform(np.array([t.ravel() for t in train_data]))
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py"", line 217, in fit_transform
      return super().fit_transform(X, y)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/umap_.py"", line 2772, in fit_transform
      self.fit(X, y)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py"", line 202, in fit
      return super().fit(X, y)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/umap_.py"", line 2684, in fit
      self.embedding_, aux_data = self._fit_embed_data(
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py"", line 462, in _fit_embed_data
      history = self.parametric_model.fit(
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py"", line 65, in error_handler
      return fn(*args, **kwargs)
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/engine/training.py"", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/engine/training.py"", line 1284, in train_function
      return step_function(self, iterator)
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/engine/training.py"", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/engine/training.py"", line 1249, in run_step
      outputs = model.train_step(data)
    File ""/home/h21/luas6629/miniconda3/envs/conda01/lib/python3.10/site-packages/umap/parametric_umap.py"", line 1150, in train_step
      self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1174, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 650, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1200, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1250, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File ""/home/h21/luas6629/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py"", line 1245, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_16'
libdevice not found at ./libdevice.10.bc
	 [[{{node StatefulPartitionedCall_16}}]] [Op:__inference_train_function_4496]
```
</details>","['type:bug', 'type:feature', 'comp:gpu', 'TF 2.12']",2023-06-06T20:06:53Z,11,0,https://github.com/tensorflow/tensorflow/issues/60790,
367,tensorflow/tensorflow,"Inference model error when xla enabled with error message ""OP_REQUIRES failed at xla_ops.cc:462 : NOT_FOUND: could not find registered platform with id: 0x7f7537df9c24""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf2.9.2

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 16.04

### Mobile device

_No response_

### Python version

Python 3.10.6

### Bazel version

5.3.2

### GCC/Compiler version

gcc (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0

### CUDA/cuDNN version

11.6

### GPU model and memory

_No response_

### Current Behaviour?

A bug happened!

### Standalone code to reproduce the issue

```shell
inference model error in c++ running with xla enabled. when xla disabled, all works fine.

run with option:
export XLA_FLAGS=""--xla_dump_to=/tmp/generated --xla_hlo_profile""
export TF_XLA_FLAGS=""--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit""
```


### Relevant log output

```shell
2023-06-06 13:54:27.650731: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:462 : NOT_FOUND: could not find registered platform with id: 0x7f7537df9c2
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:runtime', 'comp:xla', 'TF 2.9']",2023-06-06T07:25:18Z,3,0,https://github.com/tensorflow/tensorflow/issues/60785,
368,tensorflow/tensorflow,inconsistent .proto file package names break gRPC message/field parsing in Wireshark,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

As noted in #12445, there is inconsistency among the package names in the TensorFlow `.proto` files. Searching for `.proto` file package declarations within the codebase reveals a wide variety of package names, including `tensorflow.dummy`.
https://github.com/search?q=repo%3Atensorflow%2Ftensorflow+%22package+tensorflow%22&type=code&p=2

This has a problematic effect when trying to parse the Protobuf fields in TensorFlow gRPC messages within the [Wireshark](https://www.wireshark.org/) network capturing tool. In Wireshark, the built-in parsing functionality requires the package/service names within the `.proto` files to match the package/service names in the captured gRPC messages, so currently, [CoordinationService](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tsl/protobuf/coordination_service.proto) (`package tensorflow`) messages parse properly, while message types and field names in [WorkerService](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/worker_service.proto) (`package tensorflow.grpc`) messages cannot be parsed, and appear as _unknown_.

Current workaround: using a script to replace all instances of `""tensorflow.grpc""` with `""tensorflow""` in the `.proto` files.

### Standalone code to reproduce the issue

```shell
https://github.com/tensorflow/tensorflow/blob/d0863698de84277282df6f2865795aaa1e22ace5/tensorflow/tsl/protobuf/coordination_service.proto#L3

https://github.com/tensorflow/tensorflow/blob/d0863698de84277282df6f2865795aaa1e22ace5/tensorflow/core/protobuf/worker_service.proto#L18
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:core', 'TF 2.12']",2023-06-03T20:22:40Z,0,0,https://github.com/tensorflow/tensorflow/issues/60773,
369,tensorflow/tensorflow,Inconsistency-bug in `tf.raw_ops.AddN` between jit mode and normal mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

There is an inconsistency bug between **jit compile mode** and **normal mode** in `tf.raw_ops.AddN` which result in inconsistent computational result especially when the data type is `half`. But expectedly, the computational result have to be the same.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

inputs = 44 * [tf.random.uniform([1, 2, 4, 3], dtype=tf.dtypes.half, maxval=1000)] # the half datatype!

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.AddN(
        inputs = inputs
    )
    return y

def fuzz_normal():
    y = tf.raw_ops.AddN(
        inputs = inputs
    )
    return y

y1 = fuzz_jit()
print('[+] JIT ok')
y2 = fuzz_normal()
print('[+] Normal ok')
np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)
```


### Relevant log output

```shell
% python test.py
[+] JIT ok
[+] Normal ok
Traceback (most recent call last):
  File ""test.py"", line 25, in <module>
    np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)
  File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 1530, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=0.0001, atol=0.0001

Mismatched elements: 20 / 24 (83.3%)
Max absolute difference: 160.
Max relative difference: 0.004887
 x: array([[[[14864., 41344., 39872.],
         [ 2492.,  6852., 17664.],
         [17344., 33216., 39968.],...
 y: array([[[[14912., 41376., 39808.],
         [ 2488.,  6824., 17696.],
         [17408., 33280., 40096.],...
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-06-02T10:59:11Z,1,0,https://github.com/tensorflow/tensorflow/issues/60765,Runtime Error
370,tensorflow/tensorflow,Inconsistency-bug in `tf.raw_ops.Acos` between jit mode and normal mode,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

There is an inconsistency bug between **jit compile mode** and **normal mode** in `tf.raw_ops.Acos` which result in inconsistent computational result (mainly occur while dtype is complex, it seems that there are something wrong in the support of complex number).
But expectedly, the computational result have to be the same.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

x = tf.cast(tf.random.uniform([2, 1, 3, 4], dtype=tf.dtypes.float32, maxval=60000), dtype=tf.complex128)

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.Acos(
        x = x
    )
    return y

def fuzz_normal():
    y = tf.raw_ops.Acos(
        x = x
    )
    return y

y1 = fuzz_jit()
print('[+] JIT ok')
y2 = fuzz_normal()
print('[+] Normal ok')
np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)
```


### Relevant log output

```shell
% python test.py
[+] JIT ok
[+] Normal ok
Traceback (most recent call last):
  File ""test.py"", line 25, in <module>
    np.testing.assert_allclose(y1.numpy(), y2.numpy(), rtol=1e-4, atol=1e-4)
  File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 1530, in assert_allclose
    assert_array_compare(compare, actual, desired, err_msg=str(err_msg),
  File ""/usr/local/lib/python3.8/dist-packages/numpy/testing/_private/utils.py"", line 844, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Not equal to tolerance rtol=0.0001, atol=0.0001

Mismatched elements: 24 / 24 (100%)
Max absolute difference: 23.34874158
Max relative difference: 2.00000002
 x: array([[[[4.020810e-07+11.649195j, 3.918355e-07+11.63629j ,
          6.167066e-09 +9.56048j , 2.750275e-10 +8.005427j],
         [3.286194e-07+11.548319j, 8.673153e-08+10.882277j,...
 y: array([[[[0.-11.649196j, 0.-11.63629j , 0. -9.56048j , 0. -8.005427j],
         [0.-11.548319j, 0.-10.882278j, 0.-10.521193j, 0.-10.125081j],
         [0.-10.732039j, 0. -7.92731j , 0.-11.674371j, 0.-11.332818j]]],...
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.12']",2023-06-02T10:50:29Z,2,0,https://github.com/tensorflow/tensorflow/issues/60764,Runtime Error
371,tensorflow/tensorflow,Configure script automatically selects CUDA/cuDNN path instead of waiting for user input,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

TF 2.10

### Custom Code

No

### OS Platform and Distribution

Fedora 37

### Mobile device

_No response_

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

12.3.1

### CUDA/cuDNN version

11.8,12.1/8.0

### GPU model and memory

GTX 1660 Ti, 6 GB

### Current Behaviour?

I am having multiple CUDA versions, and I am trying to build Tensorflow from source with CUDA support.

Now the problem lays when I try to configure the build system using `./configure`. It will asks for relevant information for the build system. This includes:

1. Python path
2. Python packages path
3. Whether to support mROC
4. Whether to support CUDA
5. Whether to support TensorRT

Now, when I select CUDA support. the script seems to automatically selects my CUDA/cuDNN versions, and does not give me the possibility to select it manually, which is contradictory to what the documentation suggests at  [https://www.tensorflow.org/install/source#gpu_support](url):  _""If your system has multiple versions of CUDA or cuDNN installed, explicitly set the version instead of relying on the default""_

Now, I was able to trace the issue exactly to the `configure.py` file. 
In fact, I strongly suspects that there is a logical error on the section that parses the user input (Line 1244 on branch r2.11):
```python
  environ_save = dict(environ_cp)
  for _ in range(_DEFAULT_PROMPT_ASK_ATTEMPTS):
    if validate_cuda_config(environ_cp):
      cuda_env_names = [
          'TF_CUDA_VERSION',
          'TF_CUBLAS_VERSION',
          'TF_CUDNN_VERSION',
          'TF_TENSORRT_VERSION',
          'TF_NCCL_VERSION',
          'TF_CUDA_PATHS',
          # Items below are for backwards compatibility when not using
          # TF_CUDA_PATHS.
          'CUDA_TOOLKIT_PATH',
          'CUDNN_INSTALL_PATH',
          'NCCL_INSTALL_PATH',
          'NCCL_HDR_PATH',
          'TENSORRT_INSTALL_PATH'
      ]
      # Note: set_action_env_var above already writes to bazelrc.
      for name in cuda_env_names:
        if name in environ_cp:
          write_action_env_to_bazelrc(name, environ_cp[name])
      break

    # Restore settings changed below if CUDA config could not be validated.
    environ_cp = dict(environ_save)

    set_tf_cuda_version(environ_cp)
    set_tf_cudnn_version(environ_cp)
    if is_windows():
      set_tf_tensorrt_version(environ_cp)
    if is_linux():
      set_tf_tensorrt_version(environ_cp)
      set_tf_nccl_version(environ_cp)

    set_tf_cuda_paths(environ_cp)
```

Now, from my understanding, the script will validate the given environment, and then if that fails will ask for user input.
With that, on the first iteration of the loop, the validation will not contain the required environment variables.

I was able to solve the issue by swapping the order as follow:
```python
    environ_save = dict(environ_cp)
    for _ in range(_DEFAULT_PROMPT_ASK_ATTEMPTS):
      # Restore settings changed below if CUDA config could not be validated.
      environ_cp = dict(environ_save)

      set_tf_cuda_version(environ_cp)
      set_tf_cudnn_version(environ_cp)
      if is_windows():
        set_tf_tensorrt_version(environ_cp)
      if is_linux():
        set_tf_tensorrt_version(environ_cp)
        set_tf_nccl_version(environ_cp)

      set_tf_cuda_paths(environ_cp)
      if validate_cuda_config(environ_cp):
        cuda_env_names = [
            'TF_CUDA_VERSION',
            'TF_CUBLAS_VERSION',
            'TF_CUDNN_VERSION',
            'TF_TENSORRT_VERSION',
            'TF_NCCL_VERSION',
            'TF_CUDA_PATHS',
            # Items below are for backwards compatibility when not using
            # TF_CUDA_PATHS.
            'CUDA_TOOLKIT_PATH',
            'CUDNN_INSTALL_PATH',
            'NCCL_INSTALL_PATH',
            'NCCL_HDR_PATH',
            'TENSORRT_INSTALL_PATH'
        ]
        # Note: set_action_env_var above already writes to bazelrc.
        for name in cuda_env_names:
          if name in environ_cp:
            write_action_env_to_bazelrc(name, environ_cp[name])
        break
```


### Standalone code to reproduce the issue

```shell
Assumption: Multiple CUDA versions on /usr/local

Command:
./configure

Input Example:
1. [Default Setting]
2. [Default Setting]
3. N
4. y
5. N
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'TF 2.10']",2023-06-02T04:15:37Z,4,0,https://github.com/tensorflow/tensorflow/issues/60760,Runtime Error
372,tensorflow/tensorflow,Check fail can be triggered in `tf.raw_ops.EmptyTensorList` due to overflow under jit compile mode.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Check fail can be triggered in `tf.raw_ops.EmptyTensorList` under jit compile mode. While in normal mode, it won't be triggered but through an InvalidArgumentError.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

element_shape=tf.random.uniform([3], dtype=tf.dtypes.int64, minval=-100000000, maxval=1000000000)
max_num_elements=tf.random.uniform([], dtype=tf.dtypes.int32, minval=-100000, maxval=1000000)
element_dtype=tf.dtypes.int32

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.EmptyTensorList(element_shape=element_shape, max_num_elements=max_num_elements, element_dtype=element_dtype)
    return y

def fuzz_normal():
    y = tf.raw_ops.EmptyTensorList(element_shape=element_shape, max_num_elements=max_num_elements, element_dtype=element_dtype)
    return y

y1 = fuzz_jit() # trigger the check fail under jit compile mode.
print('[+] JIT ok')
y2 = fuzz_normal() # if you run y2 first, it will through error rather than check fail.
print('[+] Normal ok')
```


### Relevant log output

```shell
% python test.py
2023-06-02 11:41:06.810728: F tensorflow/core/framework/tensor_shape.cc:201] Non-OK-status: InitDims(dim_sizes) status: INVALID_ARGUMENT: Encountered overflow when multiplying 246417692688323817 with 562326056, result: -1
zsh: abort (core dumped)  python test.py
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-06-02T03:41:51Z,3,0,https://github.com/tensorflow/tensorflow/issues/60758,Runtime Error
373,tensorflow/tensorflow,Check fail can be triggered in `tf.raw_ops.GatherV2` under jit compile mode.,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

Ubuntu 20.04

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Check fail can be triggered in `tf.raw_ops.GatherV2` under jit compile mode. While in normal mode, it won't be triggered but through an `InvalidArgumentError`.

### Standalone code to reproduce the issue

```shell
import os 
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
import numpy as np

params=tf.random.uniform([4], dtype=tf.dtypes.float32, maxval=100000000)
indices=tf.random.uniform([4, 0], dtype=tf.dtypes.int32, minval=-10000, maxval=60000)
axis=tf.random.uniform([], dtype=tf.dtypes.int64, minval=-100000000, maxval=1000000000)
batch_dims=11282

@tf.function(jit_compile=True)
def fuzz_jit():
    y = tf.raw_ops.GatherV2(params=params, indices=indices, axis=axis, batch_dims=batch_dims)
    return y

def fuzz_normal():
    y = tf.raw_ops.GatherV2(params=params, indices=indices, axis=axis, batch_dims=batch_dims)
    return y

y1 = fuzz_jit()  # trigger the check fail under jit compile mode.
print('[+] JIT ok')
y2 = fuzz_normal()  # if you run y2 first, it will through error rather than check fail.
print('[+] Normal ok')
```


### Relevant log output

```shell
% python test.py
2023-06-02 11:10:06.042625: F tensorflow/core/framework/shape_inference.cc:705] Check failed: rank >= 0 (0 vs. -11280)rank must not be negative
zsh: abort (core dumped)  python test.py
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-06-02T03:14:31Z,4,0,https://github.com/tensorflow/tensorflow/issues/60756,Runtime Error
374,tensorflow/tensorflow,[TF 2.0] Signature for unranked tensor not working as intended,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

trying to save a model where one of the functions in the model is defined as:

```
class XYZ(tf.keras.models.Model):

  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])
  def coefficients(self, input_tensor: tf.Tensor):
  ....
  ....
```


When I train the model, and try to build it, I get the following error:

```
File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/vr/perception/computational_photography/ml/video_enhancement/nets/hdrnet.py"", [line 320](https://cs.corp.google.com/piper///depot/google3/vr/perception/computational_photography/ml/video_enhancement/nets/hdrnet.py?l=320&ws=mnatraj/3402&snapshot=40), in call  *
        grid, _ = self.coefficients(inputs[0])
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/util/traceback_utils.py"", [line 141](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/util/traceback_utils.py?l=141&ws=mnatraj/3402&snapshot=40), in error_handler  **
        return fn(*args, **kwargs)
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", [line 820](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?l=820&ws=mnatraj/3402&snapshot=40), in __call__
        result = self._call(*args, **kwds)
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", [line 864](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?l=864&ws=mnatraj/3402&snapshot=40), in _call
        self._initialize(args, kwds, add_initializers_to=initializers)
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", [line 687](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/eager/polymorphic_function/polymorphic_function.py?l=687&ws=mnatraj/3402&snapshot=40), in _initialize
        self._variable_creation_fn.get_concrete_function(args, kwds)
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", [line 182](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/eager/polymorphic_function/tracing_compiler.py?l=182&ws=mnatraj/3402&snapshot=40), in get_concrete_function
        args, kwargs = function_type_utils.bind_function_inputs(
    File ""/build/work/83bb824f51c38f677e7ad84666ae2d5f4deb/google3/runfiles/google3/third_party/tensorflow/python/eager/polymorphic_function/function_type_utils.py"", [line 451](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/python/eager/polymorphic_function/function_type_utils.py?l=451&ws=mnatraj/3402&snapshot=40), in bind_function_inputs
        raise TypeError(

    TypeError: Binding inputs to tf.function failed due to `Can not cast TensorSpec(shape=(1, 256, 256, 3), dtype=tf.float32, name=None) to TensorSpec(shape=(None,), dtype=tf.float32, name=None)`. Received args: (<tf.Tensor 'Placeholder:0' shape=(1, 256, 256, 3) dtype=float32>,) and kwargs: {} for signature: (input_tensor: TensorSpec(shape=(None,), dtype=tf.float32, name=None)).
```


Shouldn't specifying input signature as `None` imply that you can pass any input into this function? (unranked tensor)

### Standalone code to reproduce the issue

```shell
(no standalone code)
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:apis']",2023-05-30T22:11:27Z,2,0,https://github.com/tensorflow/tensorflow/issues/60732,Runtime Error
375,tensorflow/tensorflow,[TFLite C++] Signature calculating CategoricalCrossentropy loss produces wrong result,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

5.3.0

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I've created a simple model in Python (TF version 2.10) and converted it for tflite. The model has two signatures, one for inference and other for training. When I run those signatures in Python, everything works correctly, I get good inference result and good training loss. When I load the converted tflite model with the C++ TFLite API (built from source, from branch r2.13) and run those signatures: inference works as intended, training works as intended (the accuracy on the test set is steadily rising), but the reported loss is totally random. At first I thought that loss might be accumulated since it is rising to five digits, but that is not the case since it rises and falls in a random fashion. It seems like there is some bug in the ops used for CategoricalCrossentropy C++ TFLite implementation.

I've tried building tensorflow from r2.12 and r2.13 and I get the same behavior. I've tried r2.10 also but then I couldn't even run the signatures with C++ TFLite API, I was getting bunch of segmentation faults. I couldn't find anywhere the documentation on what ops for backward prop are available in C++ TFLite API, maybe some of those which are used in CategoricalCrossentropy loss calculation are not yet available, or there is a bug in their implementation.

### Standalone code to reproduce the issue

Here is a Python code I am using to create model with signatures:
```
IMG_SIZE = 28

class Model(tf.Module):
    def __init__(self):
        self.model = tf.keras.Sequential([
            tf.keras.layers.Flatten(input_shape=(IMG_SIZE, IMG_SIZE), name='flatten'),
            tf.keras.layers.Dense(
                units=10,
                kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05),
                bias_initializer=tf.keras.initializers.Ones(),
                name='dense'
            ),
        ])

        opt = tf.keras.optimizers.SGD(learning_rate=0.1)
        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
        self.model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])

    # The `train` function takes a batch of input images and labels.
    @tf.function(input_signature=[
        tf.TensorSpec([32, IMG_SIZE, IMG_SIZE], tf.float32),
        tf.TensorSpec([32, 10], tf.float32),
    ])
    def train(self, x, y):
        with tf.GradientTape() as tape:
            prediction = self.model(x)
            loss = self.model.loss(y, prediction)
        gradients = tape.gradient(loss, self.model.trainable_variables)
        self.model.optimizer.apply_gradients(
            zip(gradients, self.model.trainable_variables))
        result = {""loss"": loss}
        return result

    @tf.function(input_signature=[
        tf.TensorSpec([1, IMG_SIZE, IMG_SIZE], tf.float32),
    ])
    def infer(self, x):
        logits = self.model(x)
        probabilities = tf.nn.softmax(logits, axis=-1)
        return {
            ""output"": probabilities,
            ""logits"": logits
        }
```

And here is the C++ code I am using to run the tflite model:
```
std::unique_ptr<tflite::FlatBufferModel> model =
    tflite::FlatBufferModel::BuildFromFile(tflite_model_path);
if (model == nullptr)
{
    std::cout << ""Failed to load model"" << std::endl;
    return;
}

tflite::ops::builtin::BuiltinOpResolver resolver;
tflite::InterpreterBuilder builder(*model, resolver);
std::unique_ptr<tflite::Interpreter> interpreter;
builder(&interpreter);
if (interpreter == nullptr)
{
    std::cout << ""Failed to create interpreter"" << std::endl;
    return;
}

if (interpreter->AllocateTensors() != kTfLiteOk)
{
    std::cout << ""Failed to alocate interpreter tensors"" << std::endl;
    return;
}

tflite::SignatureRunner* train_runner = interpreter->GetSignatureRunner(""train"");

TfLiteTensor* input_data_tensor = train_runner->input_tensor(train_runner->input_names()[0]);
float* input_data = input_data_tensor->data.f;
TfLiteTensor* input_labels_tensor = train_runner->input_tensor(train_runner->input_names()[1]);
float* input_labels = input_labels_tensor->data.f;

// Here I fill in the input data and labels, code redacted for brevity.

if (train_runner->Invoke() != kTfLiteOk)
{
    std::cout << ""Error invoking train interpreter signature"" << std::endl;
    return;
}

const TfLiteTensor* output_tensor = train_runner->output_tensor(train_runner->output_names()[0]);
float* output = output_tensor->data.f;
std::cout << ""Training finished with loss: "" << output[0] << std::endl;
```

Please let me know if you need more details, or full source code.


### Relevant log output

Here are the losses from batch to batch, as you can see they are too high and pretty much random. I repeat: the model is training correctly which I can see because the accuracy on the test set is steadily rising, so these loss values do not make sense.
```
Training of batch 1 finished with loss: 172.813
Training of batch 2 finished with loss: 30406.2
Training of batch 3 finished with loss: 35372.7
Training of batch 4 finished with loss: 30955.9
Training of batch 5 finished with loss: 30645.5
Training of batch 6 finished with loss: 39069.4
Training of batch 7 finished with loss: 25181.5
Training of batch 8 finished with loss: 28106.7
Training of batch 9 finished with loss: 12969.1
Training of batch 10 finished with loss: 3079.69
Training of batch 11 finished with loss: 3693.12
Training of batch 12 finished with loss: 3314.77
Training of batch 13 finished with loss: 4591.12
Training of batch 14 finished with loss: 5880.76
Training of batch 15 finished with loss: 5654.75
Training of batch 16 finished with loss: 10133.1
Training of batch 17 finished with loss: 9301.94
Training of batch 18 finished with loss: 11654.5
Training of batch 19 finished with loss: 11827.8
Training of batch 20 finished with loss: 22028.1
Training of batch 21 finished with loss: 8553.58
```

</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:lite', 'type:performance', 'TF 2.13']",2023-05-27T22:40:40Z,16,0,https://github.com/tensorflow/tensorflow/issues/60718,Runtime Error
376,tensorflow/tensorflow,Incorrect gradient in divide_no_nan and reciprocal_no_nan when divide by 0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf-nightly

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When we perform divide by zero in `tf.math.divide_no_nan` and `tf.math.reciprocal_no_nan`, the theoretical and numerical gradient given by `tf.test.compute_gradient` do not match. However, if the input is valid (without dividing by zero), the gradients are fine.

More examples in [gist here](https://colab.research.google.com/drive/1U102ToL3El9wHduuDyjQXtpsZkaByDg5?usp=sharing)

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
x = tf.constant([10.0, 20.0])
y = tf.constant([2.0, 0.0])
th, nu= tf.test.compute_gradient(tf.math.divide_no_nan, [x, y])
print(th)
print(nu)
print(tf.experimental.numpy.allclose(th, nu, atol=1e-3))


import tensorflow as tf
x = tf.constant([2.0, 0.0])
th, nu= tf.test.compute_gradient(tf.math.reciprocal_no_nan, [x])
print(th)
print(nu)
print(tf.experimental.numpy.allclose(th, nu, atol=1e-3))
```


### Relevant log output

```shell
(array([[0.5, 0. ],
       [0. , 0. ]], dtype=float32), array([[-2.5,  0. ],
       [-0. ,  0. ]], dtype=float32))
(array([[0.5, 0. ],
       [0. , 0. ]], dtype=float32), array([[-2.5002441e+00,  0.0000000e+00],
       [ 0.0000000e+00,  2.0971520e+07]], dtype=float32))
tf.Tensor(False, shape=(), dtype=bool)



(array([[-0.25,  0.  ],
       [-0.  ,  0.  ]], dtype=float32),)
(array([[-2.500000e-01,  0.000000e+00],
       [ 0.000000e+00,  1.048576e+06]], dtype=float32),)
tf.Tensor(False, shape=(), dtype=bool)
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops']",2023-05-26T17:59:07Z,2,0,https://github.com/tensorflow/tensorflow/issues/60715,Logical Bug
377,tensorflow/tensorflow,`tf.split` or `tf.transpose` cause errors for quantize-aware training with `quantize_apply`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.7 & 2.12

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 18.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

We are trying to implement some network like [ShuffleNetV2](https://arxiv.org/abs/1807.11164) but encounter some error when `quantize_apply` the model.

![image](https://user-images.githubusercontent.com/22385182/233325404-e2e502fe-1151-4f4c-9377-570a01681848.png)

I believe ShuffleNet or related ideas are popular in edge devices, please kindly help us to resolve this proble.

Any advice is welcome.

_I apologize for this should have been posted as an issue on Tensorflow Model Optimization. However, since it seems that this problem is not unique to me, I'm posting it here in the hope of receiving appropriate suggestions or assistance._


### System information

TensorFlow version (installed from source or binary): 2.7.0

TensorFlow Model Optimization version (installed from source or binary): 0.7.0

Python version: 3.8.13

**We also try on latest release of both module, but also not working.**

### Describe the current behavior

When running the provided code, either the `tf.transpose` or `tf.split` will cause error to Tensorflow Model Optimization.

The error message due to `tf.split` before convolution layers:

```
ValueError: Exception encountered when calling layer ""bn3"" (type BatchNormalization).

Shape must be rank 4 but is rank 5 for '{{node bn3/FusedBatchNormV3}} = FusedBatchNormV3[T=DT_FLOAT, U=DT_FLOAT, data_format=""NHWC"", epsilon=0.001, exponential_avg_factor=1, is_training=false](Placeholder, bn3/ReadVariableOp, bn3/ReadVariableOp_1, bn3/FusedBatchNormV3/ReadVariableOp, bn3/FusedBatchNormV3/ReadVariableOp_1)' with input shapes: [1,?,128,128,32], [32], [32], [32], [32].
```

The error message due to `tf.transpose`:

```
ValueError: Exception encountered when calling layer ""tf.compat.v1.transpose"" (type TFOpLambda).

Dimension must be 6 but is 5 for '{{node tf.compat.v1.transpose/transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](tf.compat.v1.transpose/transpose/a, tf.compat.v1.transpose/transpose/perm)' with input shapes: [1,?,128,128,2,32], [5].
```


### Standalone code to reproduce the issue

Just run the following code you will get the error message due to `tf.split`.

```python
from __future__ import annotations

from typing import Callable, Optional

import tensorflow as tf
import tensorflow_model_optimization as tfmot
from tensorflow.keras import layers


SKIP_LAYER = [
    ""resize"",
    ""Resize"",
    ""reshape"",
    ""Reshape"",
    ""concat"",
    ""Concat"",
    ""ExpandDims"",
    ""Repeats"",
    ""Shape"",
    ""strided_slice"",
    ""Tile"",
]


def quantize_model(
    model: tf.keras.Model,
    annotate: Optional[Callable] = None,
    quantize_scope: Optional[dict[str, tf.keras.layers.Layer]] = None,
) -> tf.keras.Model:
    quantize_scope = {} if quantize_scope is None else quantize_scope

    def annotate(layer):
        if any([name in layer.name for name in SKIP_LAYER]):
            return layer
        else:
            return tfmot.quantization.keras.quantize_annotate_layer(layer)

    anno_model = tf.keras.models.clone_model(model, clone_function=annotate)
    with tfmot.quantization.keras.quantize_scope(quantize_scope):
        model = tfmot.quantization.keras.quantize_apply(anno_model)

    return model


def channel_shuffle(tensor: tf.Tensor, groups: int = 2) -> tf.Tensor:
    """"""Channel shuffle operation.""""""
    _, height, width, num_channels = tensor.shape.as_list()
    assert num_channels % groups == 0

    tensor = tf.reshape(tensor, [-1, height, width, groups, num_channels // groups])
    tensor = tf.transpose(tensor, [0, 1, 2, 4, 3])
    tensor = tf.identity(tensor, name=""channel_shuffle"")

    tensor = tf.reshape(tensor, [-1, height, width, num_channels])
    return tensor


def simple_nn(img_input: tf.Tensor) -> tf.Tensor:
    latent = layers.Conv2D(32, 1, padding=""same"", use_bias=False, name=""conv1"")(img_input)
    latent = layers.BatchNormalization(name=""bn1"")(latent)
    latent = layers.ReLU(name=""relu1"")(latent)

    latent = layers.DepthwiseConv2D(3, 1, padding=""same"", name=""conv2"")(img_input)
    latent = layers.BatchNormalization(name=""bn2"")(latent)

    latent = layers.Conv2D(32, 1, padding=""same"", use_bias=False, name=""conv3"")(img_input)
    latent = layers.BatchNormalization(name=""bn3"")(latent)
    latent = layers.ReLU(name=""relu3"")(latent)

    return latent


def split_like_nn(img_input: tf.Tensor) -> tf.Tensor:
    latent = layers.Conv2D(64, 1, padding=""same"", use_bias=False, name=""conv0"")(img_input)
    latent = layers.BatchNormalization(name=""bn0"")(latent)
    latent = layers.ReLU(name=""relu0"")(latent)

    latent_0, latent_1 = tf.split(latent, 2, axis=-1)
    latent_0 = simple_nn(latent_0)
    latent = tf.concat([latent_0, latent_1], axis=-1)

    latent = channel_shuffle(latent)

    return latent


if __name__ == ""__main__"":
    img_input = tf.keras.Input((128, 128, 1), dtype=tf.float32, name=""img"")

    outputs = split_like_nn(img_input)

    model = tf.keras.Model(inputs=img_input, outputs=outputs, name=""PoseNetV2"")
    model.summary()

    model_qat = quantize_model(model)
    model_qat.summary()
```


You can just comment the following three lines of code will get the error message from `tf.transpose`.

```python
 latent_0, latent_1 = tf.split(latent, 2, axis=-1)
 latent_0 = simple_nn(latent_0)
 latent = tf.concat([latent_0, latent_1], axis=-1)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.12']",2023-05-26T17:33:07Z,5,1,https://github.com/tensorflow/tensorflow/issues/60714,Runtime Error
378,tensorflow/tensorflow,Dataset.ragged_batch does not produce correct specs with tf.py_function and tf.numpy_function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

docker container nvcr.io/nvidia/tensorflow:23.04-tf2-py3 on Ubuntu 22.04.2 LTS host

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I'm trying to train an object detection model where images may have a different number of bounding boxes. Also I want to add some augmentations, and since tf does not support augmentations of bounding boxes I choose albumentations to do the job. I can't use albumentations' augmentations directly, so I need to use either `tf.py_function` or `tf.numpy_function`. I used `Dataset.ragged_batch` instead of `Dataset.batch` (because the dimension of bbox tensor may vary), but it did not provide me the correct `element_spec` and I was unable to make it work.

These are three scenarios that should help to understand the issue:

### Scenario 1:
I don't use any augmentations, `ragged_batch` returns the correct element spec, but I really need those augmentations
```
(TensorSpec(shape=(None, 512, 512, 3), dtype=tf.float32, name=None),
 {'classes': RaggedTensorSpec(TensorShape([None, None]), tf.float32, 1, tf.int64),
  'boxes': RaggedTensorSpec(TensorShape([None, None, 4]), tf.float64, 1, tf.int64)})
```
### Scenario 2:
I use `tf.numpy_function` fo perform the augmentations. The spec is incorrect, I can't batch the items
```
(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),
 {'classes': TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),
  'boxes': TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)})
```
### Scenario 3
I use `tf.py_function`, provide something, that looks like correct spec to `Tout` param:
```
Tout=[
    tf.TensorSpec(shape=[None, 512, 512, 3], dtype=tf.float32), 
    tf.RaggedTensorSpec(shape=[None, 4], dtype=tf.float32), 
    tf.RaggedTensorSpec(shape=[None, None], dtype=tf.float32),
],
```
but spec for image still does not look good
```
(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),
 {'classes': RaggedTensorSpec(TensorShape([None, None, None]), tf.float32, 2, tf.int64),
  'boxes': RaggedTensorSpec(TensorShape([None, None, 4]), tf.float32, 2, tf.int64)})
```
and I had to add an extra dimension for labels spec in order to convert it to `RaggedTensor` (which is probably not good as well). Model refuses to be trained because of incorrect image dimensions

The (non)working code is here - [colab link](https://colab.research.google.com/drive/148i78QRnF98guvx1Y0gZXtBVrv-day42?usp=sharing)

### Standalone code to reproduce the issue

```shell
import keras_cv
import numpy as np
import tensorflow as tf
import albumentations as A

def generate_random_data():
    while True:
        image = np.random.randint(0, 256, size=(512, 512, 3), dtype=np.uint8)
        num_bboxes = np.random.randint(1, 200)
        bboxes = []
        labels = []
        for _ in range(num_bboxes):
            x_min, y_min, x_max, y_max = np.sort(np.random.uniform(0, 512, size=4) / 512)
            bbox = [x_min, y_min, x_max, y_max]
            label = np.random.choice([0, 1])
            bboxes.append(bbox)
            labels.append(label)

        data = {
            'image': tf.convert_to_tensor(image),
            'bboxes': {
                'bbox': tf.convert_to_tensor(bboxes),
                'label': tf.convert_to_tensor(labels, dtype=tf.int64),
            }
        }
        yield data

# Create the random dataset
dataset = tf.data.Dataset.from_generator(generate_random_data, output_signature={
    'image': tf.TensorSpec(shape=(512, 512, 3), dtype=tf.uint8),
    'bboxes': {
        'bbox': tf.TensorSpec(shape=(None, 4), dtype=tf.float64),
        'label': tf.TensorSpec(shape=(None,), dtype=tf.int64),
    },
})


# Scenario 1
def preprocess_data_1(inputs):
    bounding_boxes = {
        ""classes"": tf.cast(inputs[""bboxes""][""label""], dtype=tf.float32),
        ""boxes"": inputs[""bboxes""][""bbox""],
    }
    return tf.image.convert_image_dtype(inputs['image'], tf.float32), bounding_boxes
ds_1 = dataset.map(preprocess_data_1).ragged_batch(2)

# Scenario 2
def transform_2(image, bboxes, labels):
    transforms = A.Compose(
        [
            A.Rotate(limit=40),
        ],
        bbox_params=A.BboxParams(
            format='albumentations',
            label_fields=['label'],
        )
    )

    transformed = transforms(
        image=image, 
        label=labels,
        bboxes=bboxes,
    )

    return transformed

def aug_fn_2(image, bboxes, labels):
    aug_data = transform_2(image, bboxes, labels)
    return (
        tf.image.convert_image_dtype(aug_data[""image""], tf.float32), 
        tf.convert_to_tensor(aug_data[""bboxes""], dtype=tf.float32), 
        tf.cast(aug_data[""label""], tf.float32)
    )

def preprocess_data_2(inputs):
    bboxes = inputs['bboxes']['bbox']
    labels = inputs['bboxes']['label']
    aug_image, aug_bboxes, aug_labels = tf.numpy_function(
        func=aug_fn_2, 
        inp=[inputs[""image""], bboxes, labels], 
        Tout=[tf.float32, tf.float32, tf.float32],
    )
    
    bounding_boxes = {
        ""classes"": aug_labels,
        ""boxes"":  aug_bboxes
    }

    return aug_image, bounding_boxes

ds_2 = dataset.map(preprocess_data_2).ragged_batch(2)
for item in ds_2:
    break

# Scenario 3
def transform_3(image, bboxes, labels):
    transforms = A.Compose(
        [
            A.Rotate(limit=40),
        ],
        bbox_params=A.BboxParams(
            format='albumentations',
            label_fields=['label'],
        )
    )

    transformed = transforms(
        image=image.numpy(), 
        label=labels.numpy(),
        bboxes=bboxes.numpy(),
    )

    return transformed

def aug_fn_3(image, bboxes, labels):
    aug_data = transform_3(image, bboxes, labels)

    return (
        tf.image.convert_image_dtype(aug_data[""image""], tf.float32), 
        tf.RaggedTensor.from_tensor(tf.convert_to_tensor(aug_data[""bboxes""], dtype=tf.float32)),
        tf.RaggedTensor.from_tensor(tf.cast([aug_data[""label""]], tf.float32)),
    )

def preprocess_data_3(inputs):
    bboxes = inputs['bboxes']['bbox']
    labels = inputs['bboxes']['label']
    aug_image, aug_bboxes, aug_labels = tf.py_function(
        func=aug_fn_3, 
        inp=[inputs[""image""], bboxes, labels], 
        Tout=[
            tf.TensorSpec(shape=[None, 512, 512, 3], dtype=tf.float32), 
            tf.RaggedTensorSpec(shape=[None, 4], dtype=tf.float32), 
            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.float32),
        ],
    )
    
    bounding_boxes = {
        ""classes"": aug_labels,
        ""boxes"":  aug_bboxes,
    }

    return aug_image, bounding_boxes

ds_3 = dataset.map(preprocess_data_3).ragged_batch(2)
for batch in ds_3:
    break

model = keras_cv.models.RetinaNet.from_preset(
    ""resnet50_imagenet"",
    num_classes=2,
    bounding_box_format=""rel_xyxy"",
)

model.compile(
    classification_loss=""focal"",
    box_loss=""smoothl1"",
    optimizer=tf.optimizers.Adam(),
)

model.fit(
    ds_3.take(1),
    validation_data=ds_3.take(1),
    epochs=100,
)
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.12']",2023-05-25T18:52:03Z,1,1,https://github.com/tensorflow/tensorflow/issues/60710,UI/UX Bug
379,tensorflow/tensorflow,Incorrect gradient after divide operation when result contains inf,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf-nightly

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The gradient of `tf.experimental.numpy.divide` or simply `/` operator is incorrect when the division result contains `inf`. See example below, the gradient of `result[0]` with respect to `x1` should be `1/2` instead of `nan`.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
x1 = tf.constant([3], dtype=tf.float32)
x2 = tf.constant([2, 3, 0, 1], dtype=tf.float32)
with tf.GradientTape() as tape:
  tape.watch(x1)
  tape.watch(x2)
  result = tf.experimental.numpy.divide(x1, x2)    # or result = x1 / x2
actual_grad = tape.jacobian(result, x1)
expected_grad = tf.constant([[1/2], [1/3], [np.inf], [1/1]])
print(actual_grad)
print(expected_grad)
actual_grad == expected_grad


See gist: https://colab.research.google.com/drive/1xbzvQ99nrEehhBabpgCSEiBMSh4qXojR?usp=sharing
```


### Relevant log output

```shell
tf.Tensor(
[[nan]
 [nan]
 [inf]
 [nan]], shape=(4, 1), dtype=float32)
tf.Tensor(
[[0.5       ]
 [0.33333334]
 [       inf]
 [1.        ]], shape=(4, 1), dtype=float32)
<tf.Tensor: shape=(4, 1), dtype=bool, numpy=
array([[False],
       [False],
       [ True],
       [False]])>
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.13']",2023-05-24T17:14:14Z,1,0,https://github.com/tensorflow/tensorflow/issues/60695,Logical Bug
380,tensorflow/tensorflow,typing_extensions >= 4.6.0 causes pip unit test failure,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.10

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//bazel_pip/tensorflow/python/trackable:data_structures_test will fail with typing_extensions >= 4.6.0 installed when run as a pip test against an installed TensorFlow wheel.

### Standalone code to reproduce the issue

```shell
bazel test --build_tests_only --cache_test_results=no --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --jobs=75 --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true //bazel_pip/tensorflow/python/trackable:data_structures_test
```


### Relevant log output

```shell
======================================================================
ERROR: testFunctionCaching (__main__.MappingTests)
MappingTests.testFunctionCaching
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/trackable/data_structures_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/trackable/data_structures_test.py"", line 507, in testFunctionCaching
    second_trace = f.get_concrete_function(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1198, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1189, in _get_concrete_function_garbage_collected
    concrete = self._variable_creation_fn.get_concrete_function(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 197, in get_concrete_function
    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 172, in _maybe_define_concrete_function
    return self._maybe_define_function(args, kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 294, in _maybe_define_function
    function_type_utils.make_canonicalized_monomorphic_type(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py"", line 378, in make_canonicalized_monomorphic_type
    function_type_lib.canonicalize_to_monomorphic(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/polymorphism/function_type.py"", line 481, in canonicalize_to_monomorphic
    _make_validated_mono_param(name, arg, poly_parameter.kind,
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/polymorphism/function_type.py"", line 421, in _make_validated_mono_param
    mono_type = trace_type.from_value(value, type_context)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py"", line 142, in from_value
    elif isinstance(value, trace.SupportsTracingProtocol):
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/typing_extensions.py"", line 605, in __instancecheck__
    val = inspect.getattr_static(instance, attr)
  File ""/usr/lib/python3.8/inspect.py"", line 1596, in getattr_static
    instance_result = _check_instance(obj, attr)
  File ""/usr/lib/python3.8/inspect.py"", line 1543, in _check_instance
    instance_dict = object.__getattribute__(obj, ""__dict__"")
TypeError: this __dict__ descriptor does not support '_DictWrapper' objects

======================================================================
ERROR: testFunctionCaching (__main__.TupleTests)
TupleTests.testFunctionCaching
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/trackable/data_structures_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/trackable/data_structures_test.py"", line 716, in testFunctionCaching
    second_trace = f.get_concrete_function(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1198, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py"", line 1189, in _get_concrete_function_garbage_collected
    concrete = self._variable_creation_fn.get_concrete_function(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 197, in get_concrete_function
    concrete_function, _ = self._maybe_define_concrete_function(args, kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 172, in _maybe_define_concrete_function
    return self._maybe_define_function(args, kwargs)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py"", line 294, in _maybe_define_function
    function_type_utils.make_canonicalized_monomorphic_type(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py"", line 378, in make_canonicalized_monomorphic_type
    function_type_lib.canonicalize_to_monomorphic(
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/polymorphism/function_type.py"", line 481, in canonicalize_to_monomorphic
    _make_validated_mono_param(name, arg, poly_parameter.kind,
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/polymorphism/function_type.py"", line 421, in _make_validated_mono_param
    mono_type = trace_type.from_value(value, type_context)
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py"", line 142, in from_value
    elif isinstance(value, trace.SupportsTracingProtocol):
  File ""/workspace/pip_test/venv_clean/lib/python3.8/site-packages/typing_extensions.py"", line 605, in __instancecheck__
    val = inspect.getattr_static(instance, attr)
  File ""/usr/lib/python3.8/inspect.py"", line 1596, in getattr_static
    instance_result = _check_instance(obj, attr)
  File ""/usr/lib/python3.8/inspect.py"", line 1543, in _check_instance
    instance_dict = object.__getattribute__(obj, ""__dict__"")
TypeError: this __dict__ descriptor does not support '_TupleWrapper' objects

----------------------------------------------------------------------
Ran 74 tests in 1.021s

FAILED (errors=2, skipped=4)
================================================================================
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:support']",2023-05-24T09:25:30Z,11,0,https://github.com/tensorflow/tensorflow/issues/60687,Runtime Error
381,tensorflow/tensorflow,api_compatibility_test fails on Python 3.11,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/tools/api/tests:api_compatibility_test fails on Python 3.11
See https://github.com/tensorflow/tensorflow/actions/runs/5053005537/jobs/9066419128#step:6:5566

### Standalone code to reproduce the issue

```shell
bazel test --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=tf_api_version=2 --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --repo_env=PYTHON_BIN_PATH=/home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/.venv/tf/bin/python --build_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py38,-no_oss_py39,-no_oss_py310 --test_tag_filters=-no_oss,-oss_excluded,-oss_serial,-v1only,-benchmark-test,-no_aarch64,-gpu,-tpu,-no_oss_py38,-no_oss_py39,-no_oss_py310 --local_test_jobs=64 --build_tests_only -- //tensorflow/... -//tensorflow/compiler/tf2tensorrt/... -//tensorflow/compiler/xrt/... -//tensorflow/core/tpu/... -//tensorflow/go/... -//tensorflow/java/... -//tensorflow/python/integration_testing/... -//tensorflow/tools/toolchains/... -//tensorflow/lite/... -//tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test -//tensorflow/python/kernel_tests/nn_ops:conv_ops_test -//tensorflow/compiler/mlir/tfr/examples/mnist:mnist_ops_test -//tensorflow/core/grappler/optimizers:auto_mixed_precision_test_cpu -//tensorflow/core/grappler/optimizers:remapper_test_cpu
```


### Relevant log output

```shell
Running tests under Python 3.11.3: /home/ubuntu/actions-runner/_work/tensorflow/tensorflow/bazel-ci_build-cache/.venv/tf/bin/python
[ RUN      ] ApiCompatibilityTest.testAPIBackwardsCompatibility
ERROR:tensorflow:TensorFlow API backwards compatibility test
This test ensures all changes to the public API of TensorFlow are intended.

If this test fails, it means a change has been made to the public API. Backwards
incompatible changes are not allowed. You can run the test as follows to update
test goldens and package them with your change.

    $ bazel run tensorflow/tools/api/tests:api_compatibility_test \
    #     -- --update_goldens True

You will need an API approval to make changes to the public TensorFlow API. This
includes additions to the API.

E0523 05:00:41.556244 281473269493776 api_compatibility_test.py:370] TensorFlow API backwards compatibility test
This test ensures all changes to the public API of TensorFlow are intended.

If this test fails, it means a change has been made to the public API. Backwards
incompatible changes are not allowed. You can run the test as follows to update
test goldens and package them with your change.

    $ bazel run tensorflow/tools/api/tests:api_compatibility_test \
    #     -- --update_goldens True

You will need an API approval to make changes to the public TensorFlow API. This
includes additions to the API.

ERROR:tensorflow:1 differences found between API and golden.
E0523 05:00:41.556445 281473269493776 api_compatibility_test.py:371] 1 differences found between API and golden.
ERROR:tensorflow:    Change detected in python object: tensorflow.train.
E0523 05:00:41.556506 281473269493776 api_compatibility_test.py:392]     Change detected in python object: tensorflow.train.
ERROR:tensorflow:    
  path: ""tensorflow.train""
  tf_module {
    member {
      name: ""BytesList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Checkpoint""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointManager""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointOptions""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointView""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""ClusterDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ClusterSpec""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Coordinator""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Example""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ExponentialMovingAverage""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Feature""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FeatureList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FeatureLists""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Features""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FloatList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Int64List""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""JobDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""SequenceExample""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ServerDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""TrackableView""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""experimental""
      mtype: ""<type \'module\'>""
    }
    member_method {
      name: ""checkpoints_iterator""
      argspec: ""args=[\'checkpoint_dir\', \'min_interval_secs\', \'timeout\', \'timeout_fn\'], varargs=None, keywords=None, defaults=[\'0\', \'None\', \'None\'], ""
    }
    member_method {
      name: ""get_checkpoint_state""
      argspec: ""args=[\'checkpoint_dir\', \'latest_filename\'], varargs=None, keywords=None, defaults=[\'None\'], ""
    }
    member_method {
      name: ""latest_checkpoint""
      argspec: ""args=[\'checkpoint_dir\', \'latest_filename\'], varargs=None, keywords=None, defaults=[\'None\'], ""
    }
    member_method {
      name: ""list_variables""
      argspec: ""args=[\'ckpt_dir_or_file\'], varargs=None, keywords=None, defaults=None""
    }
    member_method {
      name: ""load_checkpoint""
      argspec: ""args=[\'ckpt_dir_or_file\'], varargs=None, keywords=None, defaults=None""
    }
    member_method {
      name: ""load_variable""
      argspec: ""args=[\'ckpt_dir_or_file\', \'name\'], varargs=None, keywords=None, defaults=None""
    }
  }

E0523 05:00:41.556555 281473269493776 api_compatibility_test.py:393]     
  path: ""tensorflow.train""
  tf_module {
    member {
      name: ""BytesList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Checkpoint""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointManager""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointOptions""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""CheckpointView""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""ClusterDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ClusterSpec""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Coordinator""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Example""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ExponentialMovingAverage""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""Feature""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FeatureList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FeatureLists""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Features""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""FloatList""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""Int64List""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""JobDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""SequenceExample""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""ServerDef""
-     mtype: ""<class \'google.protobuf.internal.python_message.GeneratedProtocolMessageType\'>""
?                                      ---------   ^^^
+     mtype: ""<class \'google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType\'>""
?                                        ++ ^^^^
    }
    member {
      name: ""TrackableView""
      mtype: ""<type \'type\'>""
    }
    member {
      name: ""experimental""
      mtype: ""<type \'module\'>""
    }
    member_method {
      name: ""checkpoints_iterator""
      argspec: ""args=[\'checkpoint_dir\', \'min_interval_secs\', \'timeout\', \'timeout_fn\'], varargs=None, keywords=None, defaults=[\'0\', \'None\', \'None\'], ""
    }
    member_method {
      name: ""get_checkpoint_state""
      argspec: ""args=[\'checkpoint_dir\', \'latest_filename\'], varargs=None, keywords=None, defaults=[\'None\'], ""
    }
    member_method {
      name: ""latest_checkpoint""
      argspec: ""args=[\'checkpoint_dir\', \'latest_filename\'], varargs=None, keywords=None, defaults=[\'None\'], ""
    }
    member_method {
      name: ""list_variables""
      argspec: ""args=[\'ckpt_dir_or_file\'], varargs=None, keywords=None, defaults=None""
    }
    member_method {
      name: ""load_checkpoint""
      argspec: ""args=[\'ckpt_dir_or_file\'], varargs=None, keywords=None, defaults=None""
    }
    member_method {
      name: ""load_variable""
      argspec: ""args=[\'ckpt_dir_or_file\', \'name\'], varargs=None, keywords=None, defaults=None""
    }
  }

[  FAILED  ] ApiCompatibilityTest.testAPIBackwardsCompatibility
INFO:tensorflow:time(__main__.ApiCompatibilityTest.testAPIBackwardsCompatibility): 2.5s
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'comp:mkl', 'subtype: ubuntu/linux']",2023-05-23T16:40:28Z,3,0,https://github.com/tensorflow/tensorflow/issues/60679,Runtime Error
382,tensorflow/tensorflow,//tensorflow/python/ops/ragged:ragged_cross_op_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.8.13

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Test sometimes fails with segfault

### Standalone code to reproduce the issue

```shell
bazel test --build_tests_only --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --local_test_jobs=64 --test_tag_filters=-nopip,-no_pip,-oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 -k -- //bazel_pip/tensorflow/... -//bazel_pip/tensorflow/compiler/tf2tensorrt/... -//bazel_pip/tensorflow/compiler/xrt/... -//bazel_pip/tensorflow/core/tpu/... -//bazel_pip/tensorflow/go/... -//bazel_pip/tensorflow/java/... -//bazel_pip/tensorflow/python/integration_testing/... -//bazel_pip/tensorflow/tools/toolchains/... -//bazel_pip/tensorflow/lite/... -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:conv_ops_test
```


### Relevant log output

```shell
[ RUN      ] RaggedCrossOpTest.testRaggedCrossInvalidValue
INFO:tensorflow:Running testRaggedCrossInvalidValue in GRAPH mode.
I0522 15:40:37.724678 281472914997264 test_util.py:1494] Running testRaggedCrossInvalidValue in GRAPH mode.
Fatal Python error: Segmentation fault

Thread 0x0000ffff851cc010 (most recent call first):
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1477 in _call_tf_sessionrun
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1384 in _run_fn
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1401 in _do_call
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1394 in _do_run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 1214 in _run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/client/session.py"", line 971 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 2061 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 2693 in evaluate
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.py"", line 478 in testRaggedCrossInvalidValue
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/framework/test_util.py"", line 1498 in decorated
  File ""/usr/lib/python3.10/unittest/case.py"", line 549 in _callTestMethod
  File ""/usr/lib/python3.10/unittest/case.py"", line 591 in run
  File ""/usr/lib/python3.10/unittest/case.py"", line 650 in __call__
  File ""/usr/lib/python3.10/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.10/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.10/unittest/suite.py"", line 122 in run
  File ""/usr/lib/python3.10/unittest/suite.py"", line 84 in __call__
  File ""/usr/lib/python3.10/unittest/runner.py"", line 184 in run
  File ""/usr/lib/python3.10/unittest/main.py"", line 271 in runTests
  File ""/usr/lib/python3.10/unittest/main.py"", line 101 in __init__
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2527 in _run_and_get_tests_result
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2561 in run_tests
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2155 in _run_in_app
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/testing/absltest.py"", line 2060 in main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 51 in g_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/app.py"", line 254 in _run_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/absl/app.py"", line 308 in run
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 60 in main_wrapper
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/benchmark.py"", line 489 in benchmarks_main
  File ""/workspace/pip_test/venv_clean/lib/python3.10/site-packages/tensorflow/python/platform/googletest.py"", line 62 in main
  File ""/tmpfs/bazel_output/_bazel_ubuntu/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow/bazel-out/aarch64-opt/bin/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.runfiles/org_tensorflow/bazel_pip/tensorflow/python/ops/ragged/ragged_cross_op_test.py"", line 497 in <module>

Extension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._bounded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, google._upb._message, tensorflow.python.framework.fast_tensor_util, charset_normalizer.md, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.h5r, h5py.utils, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5t, h5py._conv, h5py.h5z, h5py._proxy, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5o, h5py.h5l, h5py._selector, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.sparse.linalg._isolve._iterative, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg._cythonized_array_utils, scipy.linalg._flinalg, scipy.linalg._solve_toeplitz, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_lapack, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_update, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flow, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.ndimage._nd_image, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, _ni_label, scipy.ndimage._ni_label (total: 72)
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'comp:mkl', 'subtype: ubuntu/linux']",2023-05-23T09:42:28Z,3,0,https://github.com/tensorflow/tensorflow/issues/60670,Runtime Error
383,tensorflow/tensorflow,ROCm: Importing PyTorch before TensorFlow causes TensorFlow to fail completely,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

v2.11.1-3812-gef4eebff7d4 2.11.1

### Custom Code

No

### OS Platform and Distribution

Arch Linux (EndeavourOS)

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

ROCm 5.4.3

### GPU model and memory

Radeon VII/16 GiB

### Current Behaviour?

The title suffices. A workaround is to ensure `tensorflow` is imported before `torch`.

### Standalone code to reproduce the issue

```shell
python -c ""import torch; import tensorflow as tf; tf.zeros(1)""
```


### Relevant log output

```shell
2023-05-19 22:22:33.756830: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
v2.11.1-3812-gef4eebff7d4 2.11.1
(sd) [habbasi@hameer-imacpro11 kohya-trainer]$ python -c ""import tensorflow as tf; import torch; tf.zeros(1)""
2023-05-19 22:25:50.708366: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-19 22:25:53.172076: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-19 22:25:53.172177: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-19 22:25:53.172216: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-19 22:25:53.172542: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-19 22:25:53.173978: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-19 22:25:53.174095: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2023-05-19 22:25:53.174137: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:843] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/habbasi/mambaforge/envs/sd/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/habbasi/mambaforge/envs/sd/lib/python3.10/site-packages/tensorflow/python/eager/context.py"", line 588, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: hipGetDevice() failed. Status: invalid device ordinal
```
</details>

xref pytorch/pytorch#101900","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'TF 2.11']",2023-05-19T20:26:46Z,2,0,https://github.com/tensorflow/tensorflow/issues/60642,Runtime Error
384,tensorflow/tensorflow,control_flow_ops_test unit test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

//tensorflow/python/ops/parallel_for:control_flow_ops_test fails occasionally due to difference exceeding tolerance.

See https://github.com/tensorflow/tensorflow/actions/runs/5012758324/jobs/8985082872#step:5:29789



### Standalone code to reproduce the issue

```shell
bazel test --build_tests_only --config=mkl_aarch64_threadpool --copt=-flax-vector-conversions --test_env=TF_ENABLE_ONEDNN_OPTS=1 --test_env=TF2_BEHAVIOR=1 --define=no_tensorflow_py_deps=true --test_lang_filters=py --test_size_filters=small,medium --test_output=errors --verbose_failures=true --test_keep_going --notest_verbose_timeout_warnings --local_test_jobs=64 --test_tag_filters=-nopip,-no_pip,-oss_serial,-no_oss,-oss_excluded,-v1only,-benchmark-test,-no_aarch64,-no_oss_py38,-no_oss_py39,-no_oss_py310 -k -- //bazel_pip/tensorflow/... -//bazel_pip/tensorflow/compiler/tf2tensorrt/... -//bazel_pip/tensorflow/compiler/xrt/... -//bazel_pip/tensorflow/core/tpu/... -//bazel_pip/tensorflow/go/... -//bazel_pip/tensorflow/java/... -//bazel_pip/tensorflow/python/integration_testing/... -//bazel_pip/tensorflow/tools/toolchains/... -//bazel_pip/tensorflow/lite/... -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:atrous_conv2d_test -//bazel_pip/tensorflow/python/kernel_tests/nn_ops:conv_ops_test
```


### Relevant log output

```shell
AssertionError: 
Not equal to tolerance rtol=0.0001, atol=1e-05
Mismatched value: a is different from b. 
not close where = (array([0]), array([0]), array([0]), array([1]), array([4]), array([0]))
not close lhs = [0.]
not close rhs = [0.77603436]
not close dif = [0.77603436]
not close tol = [8.760343e-05]
dtype = float32, shape = (3, 3, 2, 12, 12, 3)
Mismatched elements: 1 / 7776 (0.0129%)
Max absolute difference: 0.77603436
Max relative difference: 1.
 x: array([[[[[[0.      , 0.      , 0.712515],
           [0.      , 0.889897, 0.      ],
           [0.      , 0.      , 0.      ],...
 y: array([[[[[[0.      , 0.      , 0.712515],
           [0.      , 0.889897, 0.      ],
           [0.      , 0.      , 0.      ],...
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'comp:ops', 'subtype: ubuntu/linux']",2023-05-18T15:22:49Z,3,0,https://github.com/tensorflow/tensorflow/issues/60629,Runtime Error
385,tensorflow/tensorflow,Numpy and tf experimental Numpy differ in vander matrix creation case for N=0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04 jammy

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.6

### GPU model and memory

_No response_

### Current Behaviour?

The behaviour of `tf.experimental.numpy.vander` is different than `np.vander` for `N=0` where both value and shape of the output differ.

### Standalone code to reproduce the issue

```shell
import numpy as np        # 1.23.5
import tensorflow as tf   # 2.11.0

xn = np.array([1], dtype=np.int32)
x = tf.constant([1], dtype=tf.int32)
print(np.vander(xn, 0))
print()
print(tf.experimental.numpy.vander(x, 0))
```


### Relevant log output

```shell
[]

tf.Tensor([[1]], shape=(1, 1), dtype=int32)
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.11']",2023-05-18T15:11:22Z,1,0,https://github.com/tensorflow/tensorflow/issues/60628,Logical Bug
386,tensorflow/tensorflow,Weird memory usage of shuffling in `tf.data.Dataset` ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

binary

### Tensorflow Version

tf 2.12

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Hi, I've read the [tf.data doc](https://www.tensorflow.org/guide/data#randomly_shuffling_input_data), which says using a large `buffer size` in data shuffling is not recommended, but still the shuffling behavior costs way more memory than I would expect. For example, I expect the full shuffling of 50,000,000 `int` data may only use 1 GB of memory, but the following code after `print` essentially uses 10 GB.

This leads to some practical concerns. If I set `buffer_size=1024` in data shuffling, would the *actual* memory usage of the buffer size be 10 times that of 1024 elements?

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

data_size = 50000000
tf_dataset = tf.data.Dataset.from_tensor_slices(np.arange(data_size))
tf_dataset = iter(tf_dataset.shuffle(data_size))
print(next(tf_dataset))
```


### Relevant log output

```shell
<tf.Tensor: shape=(), dtype=int64, numpy=24774043>
```
</details>","['type:bug', 'comp:data', 'type:performance', 'TF 2.12']",2023-05-15T14:54:49Z,4,0,https://github.com/tensorflow/tensorflow/issues/60599,Performance Issue
387,tensorflow/tensorflow,rejection_resample loses track of ragged tensors,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0-dev20230512

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04.6

### Mobile device

_No response_

### Python version

3.8.14

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

A `tf.data.Dataset` initialized from RaggedTensors normally will successfully batch into ragged batches. However after passing it through `rejection_resample`, it loses track of which input tensors were ragged, and so batching fails.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf

input = tf.ragged.constant([[1,2,3], [4,5], [7,8,9]])
ds = tf.data.Dataset.from_tensor_slices(input)
tf.random.set_seed(0)
# Removing this line makes everything work fine
ds = ds.rejection_resample(
    class_func=lambda t: 1,
    target_dist=(0.1, 0.9),
)
ds = ds.batch(2)
ds.take(1).get_single_element()
```


### Relevant log output

```shell
tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DatasetToSingleElement_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 1. First element had shape [3] and element 1 had shape [2]. [Op:DatasetToSingleElement] name:
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.12']",2023-05-12T10:16:39Z,2,0,https://github.com/tensorflow/tensorflow/issues/60583,Runtime Error
388,tensorflow/tensorflow,Large inconsistencies in tf.signal.stft's and tf.signal.inverse_stft's results with @tf.function decorator for certain inputs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0-dev20230509

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?

`tf.signal.stft` and `tf.signal.inverse_stft` has large inconsistencies in their results with or without @tf.function for some inputs. This issue seems to be unrelated to precision errors, as previously discussed under issues (#57960 and #57961), given that the inconsistencies can reach very high values, such as 7.530909102308483e+252+6.2143661415679e-310j. I open this issue because the behavior still exists in the latest nightly version of tensorfow.

Further investigation finds that it is because the results are different during each run and thus the inconsistencies are different, where sometimes the discrepancies are extremely large, while at other times they are relatively small. It appears that the inconsistencies are non-deterministic, which indicates a potential issue with the underlying implementation.

I rerun the reproduction code several times and record the large inconsistencies below in the log file.

The reproduction colab links are here:
For tf.signal.stft, https://colab.research.google.com/drive/1WleKXby71iZXOL12r8nIN8B_jd2wJQks?usp=sharing.
For tf.signal.inverse_stft, https://colab.research.google.com/drive/1MhNfkZgltqQqHw8kKG2zQi8ivwvKfRoj?usp=sharing.


### Standalone code to reproduce the issue

```shell
# for tf.signal.stft
import tensorflow as tf
import numpy as np

print(tf.__version__)

input = {'fft_length': 46, 'frame_step': 19, 'frame_length': 0, 'signals': np.array([[[[-8.75314539e+307, -4.03838038e+307,  8.23775798e+307, -1.32627219e+307,  1.19815521e+307,  4.57117750e+307],
                                                                              [-4.74761327e+307, -4.71580522e+307, -5.88832102e+307, -6.48759076e+307, -4.36028464e+307, -4.77775171e+307],
                                                                              [ 1.20113701e+307, -7.60106094e+307,  7.22716917e+307, 2.17687950e+307, -5.25271143e+306,  5.41182394e+307]]]])}

output1 = tf.signal.stft(**input)

@tf.function
def fun_wrapper(x):
    return tf.signal.stft(**x)

output2 = fun_wrapper(input)

print(np.allclose(output1, output2))
print(np.max(np.subtract(output1, output2)))

# for tf.signal.inverse_stft
import tensorflow as tf
import numpy as np

print(tf.__version__)

input = {'frame_step': 29343, 'frame_length': 61, 'stfts': np.array([[]], dtype=np.complex64)}

output1 = tf.signal.inverse_stft(**input)

@tf.function
def fun_wrapper(x):
    return tf.signal.inverse_stft(**x)

output2 = fun_wrapper(input)

print(np.allclose(output1, output2))
print(np.max(np.subtract(output1, output2)))
```


### Relevant log output

```shell
### for tf.signal.stft
False
(1.2623837153272947e+180+2.19373012209e-312j)

False
(6.443468248812391e+278-3.2e-322j)

False
(2.347922071768121e+228+1.74e-321j)

### for tf.signal.inverse_stft
False
1.4412957e+32

False
7.529253e+23

False
7800730000.0
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'comp:autograph', 'TF 2.12']",2023-05-09T20:14:39Z,1,0,https://github.com/tensorflow/tensorflow/issues/60549,Runtime Error
389,tensorflow/tensorflow,tf.linalg.matrix_rank  results has different results with or without @tf.function for numpy inputs under tensorflow-cpu,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.14.0-dev20230509

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

N/A

### GPU model and memory

_No response_

### Current Behaviour?

`tf.linalg.matrix_rank` has different results with or without `@tf.function` when the input is a numpy tensor and **tensorFlow-cpu** is used. 

Interestingly, this issue does not occur when the numpy array is explicitly converted to a TensorFlow tensor before being passed as an argument to tf.linalg.matrix_rank. This explicit conversion shouldn't be necessary, as per the TensorFlow tutorial (https://www.tensorflow.org/tutorials/customization/basics#:~:text=TensorFlow%20operations%20automatically%20convert%20NumPy%20ndarrays%20to%20Tensors), which states that ""TensorFlow operations automatically convert NumPy ndarrays to Tensors"". This discrepancy seems to indicate a bug that prevents the utilization of this automatic conversion feature.

This issue was previously raised and discussed under issue (#57959), where the proposed solution was the explicit conversion of numpy arrays to TensorFlow tensors. While this solution works, it does not align with the functionality of TensorFlow's automatic conversion of numpy arrays to tensors, and it requires users to perform an additional step that should not be necessary.

In essence, this bug seems to affect the user's ability to leverage TensorFlow's automatic conversion of numpy arrays to tensors, particularly when using TensorFlow-CPU.

I open this issue because the same behavior still exists in the latest nightly version and I believe it should not be a user issue according to the tutorial.

The reproduction colab link is here: https://colab.research.google.com/drive/1wEYxe5b-m7_3pqBP1iTrjSvydMd_jD_B?usp=sharing. 

### Standalone code to reproduce the issue

```shell
import numpy as np
import tensorflow as tf

print(tf.__version__)

input = {'name': 'matrix_rank', 'a': np.array([[-7.24721292e+307,  4.66389010e+307, -5.40181227e+307,
         7.28793100e+307,  5.19885794e+307],
       [-5.74381106e+307,  2.21923437e+307,  4.96898538e+307,
         4.26402766e+307,  7.42174751e+307],
       [-2.62810171e+307,  1.71425915e+307, -6.99349881e+307,
        -8.11519519e+307,  4.04358640e+307],
       [-8.52726304e+307,  1.44214314e+307, -4.53927548e+307,
        -4.79571993e+307, -4.59672928e+307]])}
print(input['a'].dtype)

output1 = tf.linalg.matrix_rank(**input)
print(output1)

@tf.function
def fun_wrapper(x):
    return tf.linalg.matrix_rank(**x)

output2 = fun_wrapper(input)
print(output2)
```


### Relevant log output

```shell
2.14.0-dev20230509
float64
tf.Tensor(0, shape=(), dtype=int32)
tf.Tensor(4, shape=(), dtype=int32)
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'comp:tf.function']",2023-05-09T19:42:32Z,2,0,https://github.com/tensorflow/tensorflow/issues/60547,UI/UX Bug
390,tensorflow/tensorflow,"XLA size-inference integration bug (tf.where, tf.TensorArray, slice, loop)","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.14.0.dev20230502

### Custom Code

No

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

The attached code produces difference results for `jit_compile=True` and `jit_compile=False`, the XLA version produces the wrong results. The bug reproduces on both TF 2.12 and nightly (2.14.0.dev20230502) on Google Colab.

It appears to be an issue with `tf.size(tf.where(x == 1))`. While the visible front-end output is correct, I suspect there is an integration bug on the compiler back-end. When combined with a slice that is too big x[0:16] (`x.shape = (3, 5)`), plus many more code details, the output has same size as the input (`5` in the example). As if `tf.size(tf.where(x == 1)) == tf.size(x) == 5`, when actually `tf.size(tf.where(x == 1)) == 2`. I suspect this because changing the input shape to `(4, )` changes the output shape to `(4, )`.

This bug was very subtle and difficult to find, especially because `tf.print()` is not supported by XLA. I hope you appreciate it.

### Standalone code to reproduce the issue

Google Colab link: https://colab.research.google.com/drive/1cccEssWkCVLO515uf3RoOuEMDp5V-2ZZ?usp=sharing

```python
import tensorflow as tf

def fn(x, y):
    token_idx_to_mask = tf.where(x == 1)
    n_samples = tf.size(token_idx_to_mask)
    x_repeated = tf.repeat(tf.expand_dims(x, 0), n_samples, axis=0)

    predict_all_array = tf.TensorArray(x_repeated.dtype, size=1, infer_shape=False, element_shape=(None, ))
    for batch_i in tf.range(1):
        x_batch = x_repeated[0:(batch_i + 1)*16, ...]
        y_batch = x_batch[:, y]
        predict_all_array = predict_all_array.write(batch_i, y_batch)
    predicts_all = predict_all_array.concat()

    return n_samples, predicts_all

fn_jit = tf.function(reduce_retracing=True, jit_compile=True)(fn)
fn_std = tf.function(reduce_retracing=True)(fn)

print('XLA': fn_jit(tf.constant([0, 1, 1, 0, 0], dtype=tf.dtypes.int32), tf.constant(0)))
print('STD': fn_std(tf.constant([0, 1, 1, 0, 0], dtype=tf.dtypes.int32), tf.constant(0)))
```


### Relevant log output

```shell
XLA: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 0, 0, 0, 0], dtype=int32)>)
STD: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)
```

Expected results would be:

```shell
XLA: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)
STD: (<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>)
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.12']",2023-05-02T16:34:56Z,2,0,https://github.com/tensorflow/tensorflow/issues/60472,Runtime Error
391,tensorflow/tensorflow,Graph/memory corruption involving custom_gradient of functions involving pow,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.9.1, 2.11, presumably more

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 18.04, 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I have implemented `custom_gradient`s for `tf.function`s that smooth various Nth roots to avoid the singularity at `x=0`. I expect the value computed by the custom gradient to be used whenever that function is differentiated, and not substituted for any other value.

**HOWEVER** when the following conditions hold:
- input is a complex dtype
- the `GradientTape` is persistent
- the `tf.function` being differentiated invokes `tf.pow` directly (or indirectly through `**` operator)
- the argument is `0`

Then the custom gradient is invoked and the proper value is computed, but then discarded somewhere between returning from my custom gradient and returning from the gradient tape's internal computation.

Note: this problem does not appear when wrapping `sqrt()` as that has its own gradient implementation that presumably avoids `pow()`.

This seems likely related to numerical issues highlighted in tensorflow/tfjs#346, PLUS, some kind of graph/memory corruption, although why/how that would be conditional on the argument is beyond me.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf


@tf.function
@tf.custom_gradient
def cbrt(p):
  third = 1. / 3.
  real_part_nonnegative = tf.math.real(p) >= 0
  signs = tf.cast(tf.where(real_part_nonnegative, 1.0, -1.0), p.dtype)
  p = signs * p
  unsigned_root = p ** third

  @tf.function
  def grad(upstream):
    unsigned_root_plus_eps = unsigned_root + 1e-4
    denom = 3 * unsigned_root_plus_eps * unsigned_root_plus_eps + 1e-4
    droot_dp = (1. / denom)
    grad_root = droot_dp * upstream
    tf.print(""custom gradient:"", tf.math.real(grad_root), tf.math.imag(grad_root))
    return grad_root

  root = signs * unsigned_root
  return root, grad
  

if __name__ == ""__main__"":
  tf.config.run_functions_eagerly(True)
  x = tf.Variable([-1e-2, -1e-10, 0, 1e-10, 1e-2], dtype=tf.complex64)
  with tf.GradientTape(persistent=True) as tape:
    cbrt_x = cbrt(x)
  gradients = tape.gradient(cbrt_x, x)
  tf.print(""all good:"", tf.math.real(gradients), tf.math.imag(gradients))

  tf.config.run_functions_eagerly(False)
  with tf.GradientTape() as tape:
    cbrt_x = cbrt(x)
  gradients = tape.gradient(cbrt_x, x)
  tf.print(""all good:"", tf.math.real(gradients), tf.math.imag(gradients))

  @tf.function
  def now_again_in_graph_mode(y):
    with tf.GradientTape(persistent=True) as tape:
      cbrt_y = cbrt(y)
    return tape.gradient(cbrt_y, y)
  gradients = now_again_in_graph_mode(x)
  tf.print(""chaos reigns:"", tf.math.real(gradients), tf.math.imag(gradients))
```


### Relevant log output

```shell
custom gradient: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]
all good: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]
custom gradient: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]
all good: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]
custom gradient: [7.16964388 9905.4209 9997.00098 9905.4209 7.16964388] [0 0 0 0 0]
chaos reigns: [7.16964388 9905.4209 -nan 9905.4209 7.16964388] [0 0 -nan 0 0]
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:autograph', 'TF 2.12']",2023-05-02T13:48:05Z,8,0,https://github.com/tensorflow/tensorflow/issues/60468,Runtime Error
392,tensorflow/tensorflow,AutoGraph error when function is inside a match ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230501

### Custom Code

No

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

3.10.11

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When a `tf.data.Dataset.map` function is defined inside a `match` scope and used by `tf.data.Dataset.apply` the AutoGraph fails. This should not happen, as the match statement should be irrelevant to the function definition. I suspect this error happens in more AutoGraph cases than just `tf.data.Dataset.map` via `tf.data.Dataset.apply`.

The issue is reproduced on Google Colab with both nightly (2.13.0-dev20230501) and 2.12.

### Standalone code to reproduce the issue

See https://colab.research.google.com/drive/1x07aUgISrYQohsG7FYmc70LeYjVcGixG#scrollTo=auSxkDSinh3R

```python
import tensorflow as tf
parameter = 'a'

match parameter:
    case 'a':
        def fn(ds):
            return ds.map(lambda x: x + 1)

dataset = tf.data.Dataset.range(1) \
    .apply(fn)

for x in dataset:
    print(x)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 427, in converted_call
    converted_f = _convert_actual(target_entity, program_ctx)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py"", line 269, in _convert_actual
    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 282, in transform
    return self.transform_function(obj, user_context)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 466, in transform_function
    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 342, in transform_function
    node, source = parser.parse_entity(fn, future_features=future_features)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/parser.py"", line 145, in parse_entity
    return _parse_lambda(entity)
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/parser.py"", line 275, in _parse_lambda
    lambda_nodes.extend(
  File ""/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/pyct/parser.py"", line 275, in <genexpr>
    lambda_nodes.extend(
  File ""/usr/lib/python3.10/ast.py"", line 390, in walk
    todo.extend(iter_child_nodes(node))
  File ""/usr/lib/python3.10/ast.py"", line 272, in iter_child_nodes
    for name, field in iter_fields(node):
  File ""/usr/lib/python3.10/ast.py"", line 260, in iter_fields
    for field in node._fields:
AttributeError: 'NoneType' object has no attribute '_fields'
WARNING:tensorflow:AutoGraph could not transform <function fn.<locals>.<lambda> at 0x7f9fc56e9870> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'NoneType' object has no attribute '_fields'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2.13.0-dev20230501
Converted call: <function fn.<locals>.<lambda> at 0x7f9fc56e9870>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)
    kwargs: {}

Not allowed: <method-wrapper '__call__' of function object at 0x7f9fc56e9870>: default rule
Not allowed: <function fn.<locals>.<lambda> at 0x7f9fc56e9870>: default rule
<function fn.<locals>.<lambda> at 0x7f9fc56e9870> is not cached for subkey ConversionOptions[{}]
Error transforming entity <function fn.<locals>.<lambda> at 0x7f9fc56e9870>
WARNING: AutoGraph could not transform <function fn.<locals>.<lambda> at 0x7f9fc56e9870> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'NoneType' object has no attribute '_fields'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
tf.Tensor(1, shape=(), dtype=int64)
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:autograph', 'TF 2.12']",2023-05-01T14:17:26Z,4,0,https://github.com/tensorflow/tensorflow/issues/60458,Runtime Error
393,tensorflow/tensorflow,Inconsistency in SyncBatchNormalization/BatchNormalization(synchronized=True) Results during Distributed Training on CPUs,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

_No response_

### Current Behaviour?

I am reporting an issue encountered during the distributed training of a model with `SyncBatchNormalization` layers on CPUs. The issue presents as a relatively large inconsistency between the results produced when training the same model on a single device versus multiple devices.

We've prepared a reproduction script demonstrating this issue. We set the weights of the `SyncBatchNormalization`/`BatchNormalization` layer to reproduce this bug. After conducting a one-step training, the predictions from the identical model trained on a single CPU and two CPUs exhibited relatively large differences (measured by L-infinity distance, Linf = 0.00074467063). As a comparison, we build another model with exactly the same architecture but removing the `SyncBatchNormalization` layer, and the prediction difference from this model trained on a single CPU and two CPUs is Linf = 1.0131771e-09.

Another comparison experiment is on GPUs. We trained the same model containing `SyncBatchNormalization` layer on a single GPU and two GPUs. The Linf of model’s prediction result is 2.5331974e-07. It’s much smaller than 1CPU vs 2CPU (Linf = 0.00074467063). This is weird because I would expect the CPU executions to be more deterministic than GPUs, and I would expect larger inconsistencies in the GPU runs if the inconsistencies are caused by variance.

To ensure a controlled environment, we used the same training data for one-step training on both setups, and then evaluated the model. To guarantee that the distributed trainings are expected to produce the same results given different number of devices, we use `MirroredStrategy`, keep the global batch size the same, and use `tf.keras.losses.Reduction.AUTO` reduction. To make the difference more apparent given a limited amount of training data, we deliberately chose a relatively high learning rate (lr=10).

It's noteworthy that `SyncBatchNormalization` has been deprecated as of TensorFlow version 2.12. Therefore, we also conducted the same experiment using TensorFlow's nightly build (2.13.0-dev20230428), replacing `SyncBatchNormalization()` with `BatchNormalization(synchronized=True)`. This experiment still manifested the same inconsistency. The prediction from the model containing batchnorm layer trained on a single CPU and two CPUs exhibits a substantial difference of Linf = 0.00010111928, while for the model removing batchnorm layer it’s Linf = 6.212488e-36.


### Standalone code to reproduce the issue

```shell
For TF 2.11.0 CPU experiments, the model contains `SyncBatchNormalization` layer:
https://colab.research.google.com/drive/11EseXxq_uHweY7omt7JCr1mc-42Kf1H5?usp=sharing

For TF 2.11.0 GPU experiments, the model contains `SyncBatchNormalization` layer (fix seed to generate same inputs):
https://colab.research.google.com/drive/1m_gCWzb_OKVTYAMMs8YbdKUYM3aINNPV?usp=sharing 

For TF nightly 2.13.0-dev20230428, the model contains `BatchNormalization(synchronized=True)` layer:
https://colab.research.google.com/drive/1N-aXPcfckVb8fPDSmghMlzBOFEGq6RzM?usp=sharing
```


### Relevant log output

For TF 2.11.0:

```shell
contain syncbatchnorm:  True
1CPU vs 2CPU: 0.00074467063

contain syncbatchnorm:  False
1CPU vs 2CPU: 1.0131771e-09

contain syncbatchnorm:  True
1GPU vs 2GPU: 2.5331974e-07
```

For TF nightly 2.13.0-dev20230428:

```shell
contain syncbatchnorm:  True
1CPU vs 2CPU: 0.00010111928

contain syncbatchnorm:  False
1CPU vs 2CPU: 6.212488e-36
```

</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:dist-strat', 'TF 2.11']",2023-04-29T03:41:12Z,6,0,https://github.com/tensorflow/tensorflow/issues/60438,UI/UX Bug
394,tensorflow/tensorflow,"The dim size of inferred shape in GraphProperties is less than -1, which is inconsistent with TensorShapeProto","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.3.0

### GCC/Compiler version

11.3.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

### Behavior
Viewed dim size less than -1 after called `GraphProperties::InferStatically`, while in [TensorShapeProto](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/tensor_shape.proto#L17) the comment says either the dim size is greater than 0 or -1 (meaning unknown). 
```
message Dim {
    // Size of the tensor in that dimension.
    // This value must be >= -1, but values of -1 are reserved for ""unknown""
    // shapes (values of -1 mean ""unknown"" dimension). 
    int64 size = 1;
};
```
The two are inconsistent.
### The Cause
In [this code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_properties.cc#L118) which constructs the inferred dims, it assigns a negative id to unknown dimensions, starting at -2. 
```c++
// Assign a negative id to unknown dimensions, starting at -2 (the -1 id
// reserved by TensorFlow).
void ExtractValue(DimensionHandle d, int64_t* result) {
  if (!InferenceContext::ValueKnown(d)) {
    *result = -counter;
    counter++;
  } else {
    ...
  }
}
```
It can be seen from the code that size<=-1 means unknown, but the unknown size in [this code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/utils/symbolic_shapes.cc#L42) is identified with `==-1`.
`bool IsUnknown(const TensorShapeProto::Dim& dim) { return dim.size() == -1; }`
And the [`ShapeIsSymbolicallyDefined`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/utils/symbolic_shapes.cc#L44), which calls `IsUnknown `may cause bugs.
```c++
bool ShapeIsSymbolicallyDefined(const TensorShapeProto& shape) {
  return !shape.unknown_rank() &&
         std::all_of(
             shape.dim().begin(), shape.dim().end(),
             [](const TensorShapeProto::Dim& dim) { return !IsUnknown(dim); });
}
```
[`ShapesSymbolicallyEqual`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/utils/symbolic_shapes.cc#L76) function is the same problem. 
### Questions:
1. The inferred dim size is less than -1, inconsistent with TensorShapeProto, is this expected?
2. Is the `IsUnknown` function reasonable?
3. Do the `ShapeIsSymbolicallyDefined` and `ShapesSymbolicallyEqual` cause undefined behavior? I see some graph optimization path will call this function after shape inference.

### Standalone code to reproduce the issue
I manually print the shape inference log in the Tensorflow source code. First, patch my log into the Tensorflow source code, then compile the source code, and finally run the python script I gave. It can be seen from the log that the result of shape inference has dimensions less than -1.
```shell
git checkout v2.12.0

# apply patch
echo 'diff --git a/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc b/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc
index 40c27828d26..899a9d059cb 100644
--- a/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc
+++ b/tensorflow/core/grappler/optimizers/arithmetic_optimizer.cc
@@ -4452,6 +4452,26 @@ Status ArithmeticOptimizer::Optimize(Cluster* /*cluster*/,
     VLOG(1) << ""Shape inference failed."" << status.error_message();
   }

+  for (auto& node : optimized_graph_->node()) {
+    VLOG(0) << ""node_name: "" << node.name();
+    const auto& input_properties =
+        graph_properties_->GetInputProperties(node.name());
+    for (int i = 0; i < input_properties.size(); i++) {
+      auto& property = input_properties[i];
+      VLOG(0) << ""input"" << i << "": "";
+      const TensorShapeProto& tsp = property.shape();
+      if (tsp.unknown_rank()) {
+        VLOG(0) << ""unknown shape"";
+        continue;
+      }
+      VLOG(0) << ""input_rank: "" << tsp.dim_size();
+      for (int j = 0; j < tsp.dim_size(); j++) {
+        VLOG(0) << ""dim"" << j << "" size: "" << tsp.dim(j).size();
+      }
+    }
+    VLOG(0);
+  }
+
   // Perform the optimizations.
   TF_RETURN_IF_ERROR(SimplifyArithmeticOps(can_use_shapes));
   *optimized_graph = std::move(*optimized_graph_);
' | git apply

# build
bazel build //tensorflow/tools/pip_package:build_pip_package
...(continue to build)
```

```python
import numpy as np
import tensorflow as tf

@tf.function(input_signature=[tf.TensorSpec(shape=(None, None), dtype=tf.float32)])
def fun(x):
    y = tf.constant(np.ones((2, 4)), dtype=tf.float32)
    return tf.add(x, y)

output = fun(np.ones((2, 4), dtype=np.float32))
print(""output: "", output)
```


### Relevant log output

```shell
node_name: Add
input0:
input_rank: 2
dim0 size: 2
dim1 size: 4
input1:
input_rank: 2
dim0 size: -2
dim1 size: -3
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux', 'TF 2.12']",2023-04-24T04:07:40Z,2,0,https://github.com/tensorflow/tensorflow/issues/60406,Runtime Error
395,tensorflow/tensorflow,The inferred shape of `tf.RaggedTensor.row_lengths(axis=2)` in Keras graph is incorrect for ragged tensor with uniform row lengths,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

TF 2.12, TF nightly 2.13.0-dev20230420

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Assume a Keras graph gets a ragged tensor with uniform row lengths in the axis 1, so for example
```python
inputs = tf.keras.layers.Input([64, None], ragged=True)
```

Then `inputs.row_lengths(axis=2)` is a `KerasTensor` with a correct spec `RaggedTensorSpec(TensorShape([None, 64]), ...)`.

However, when you index the first axis using a ""full"" slice, i.e.
```python
inputs.row_lengths(axis=2)[:]
```
you should get the same tensor -- but you get a `KerasTensor` with an incorrect spec `RaggedTensorSpec(TensorShape([1, 64]), ...)`.

### Standalone code to reproduce the issue

A Colab notebook reproducing the issue both in TF 2.12.0 and in TF nightly 2.13.0-dev20230420 can be found at https://colab.research.google.com/drive/1RKvNdB_81yKZkfzDpefIPJU2pgqZuYUY?usp=sharing

The full source also follows:

```python
inputs = tf.keras.layers.Input([64, None], ragged=True)
print(inputs)

print(inputs.row_lengths(axis=1))
print(inputs.row_lengths(axis=1)[:]) # OK, is the same as above

print(inputs.row_lengths(axis=2))
print(inputs.row_lengths(axis=2)[:]) # Problem, should be the same as above
```


### Relevant log output

```python
KerasTensor(type_spec=RaggedTensorSpec(TensorShape([None, 64, None]), tf.float32, 2, tf.int64), name='input_1', description=""created by layer 'input_1'"")
KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None), name='input.row_lengths/sub:0', description=""created by layer 'input.row_lengths'"")
KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.int64, name=None), name='tf.__operators__.getitem/strided_slice:0', description=""created by layer 'tf.__operators__.getitem'"")
KerasTensor(type_spec=RaggedTensorSpec(TensorShape([None, 64]), tf.int64, 1, tf.int64), description=""created by layer 'input.row_lengths_2'"")
KerasTensor(type_spec=RaggedTensorSpec(TensorShape([1, 64]), tf.int64, 1, tf.int64), description=""created by layer 'tf.__operators__.ragged_getitem'"")
```
</details>","['stat:awaiting response', 'type:bug', 'comp:keras', 'comp:ops', 'TF 2.12']",2023-04-22T18:30:48Z,4,0,https://github.com/tensorflow/tensorflow/issues/60400,Logical Bug
396,tensorflow/tensorflow,//tensorflow/python/client:session_partial_run_test is flaky,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

git HEAD

### Custom Code

No

### OS Platform and Distribution

Ubuntu 20.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

5.3.0

### GCC/Compiler version

10.2.1

### CUDA/cuDNN version

n/a

### GPU model and memory

n/a

### Current Behaviour?

Unit test reports as FLAKY or FAILED.

See https://source.cloud.google.com/results/invocations/dea422ff-7e14-4fc1-b324-0129ecd7ffbc/log or https://github.com/tensorflow/tensorflow/actions/runs/4731924097/jobs/8397430880#step:5:23224

### Standalone code to reproduce the issue

```shell
docker exec tf bazel --bazelrc=/usertools/cpu.bazelrc test --config=rbe --config=pycpp --config=build_event_export
```


### Relevant log output

```shell
======================================================================
ERROR: testPartialRunMissingPlaceholderFeedExceptionDist (__main__.PartialRunTest)
PartialRunTest.testPartialRunMissingPlaceholderFeedExceptionDist
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1379, in _do_call
    return fn(*args)
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1369, in _prun_fn
    return self._call_tf_sessionprun(handle, feed_dict, fetch_list)
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1460, in _call_tf_sessionprun
    return tf_session.TF_SessionPRun_wrapper(self._session, handle, feed_dict,
tensorflow.python.framework.errors_impl.InternalError: From /job:localhost/replica:0/task:0:
ValidateDevices called before initialization.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1629, in decorated
    return f(self, *args, **kwargs)
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session_partial_run_test.py"", line 269, in testPartialRunMissingPlaceholderFeedExceptionDist
    self.RunTestPartialRunMissingPlaceholderFeedException(
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session_partial_run_test.py"", line 119, in RunTestPartialRunMissingPlaceholderFeedException
    sess.partial_run(handle, fetches[0])
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1026, in partial_run
    return self._run(handle, fetches, feed_dict, None, None)
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1192, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1375, in _do_run
    return self._do_call(_prun_fn, handle, feeds, fetches)
  File ""/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/client/session_partial_run_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1398, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

From /job:localhost/replica:0/task:0:
ValidateDevices called before initialization.

----------------------------------------------------------------------
Ran 25 tests in 2.625s

FAILED (errors=1, skipped=1)
================================================================================
```
</details>","['stat:awaiting response', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux', 'TF 2.12']",2023-04-18T13:30:50Z,3,0,https://github.com/tensorflow/tensorflow/issues/60351,Runtime Error
397,tensorflow/tensorflow,TPU Tensorflow mapping string label to int with ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12

### Custom Code

Yes

### OS Platform and Distribution

Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

None

### GPU model and memory

None

### Current Behaviour?

I currently get an error when trying to get my batch from a tf.dataset. I am mapping the string label in the tfrecord into int with tf.lookup.StaticHashTable. 
```
InternalError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
```

Because of that I can't get the batch of my dataset, and train a model with TPU. It works fine with GPU.

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1vAADMl5fBulmSnbmbOTrMjyzAYAgHhFl?authuser=1#scrollTo=_zv9OlXbIqDf
```


### Relevant log output

```shell
AttributeError                            Traceback (most recent call last)

/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)
    786         # Fast path for the case `self._structure` is not a nested structure.
--> 787         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    788       except AttributeError:

AttributeError: 'tuple' object has no attribute '_from_compatible_tensor_list'


During handling of the above exception, another exception occurred:

InternalError                             Traceback (most recent call last)

13 frames

InternalError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:""2023-04-16T09:15:30.550805248+00:00""}
Executing non-communication op <MakeIterator> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.


During handling of the above exception, another exception occurred:

InternalError                             Traceback (most recent call last)

/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/executor.py in wait(self)
     63   def wait(self):
     64     """"""Waits for ops dispatched in this executor to finish.""""""
---> 65     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     66 
     67   def clear_error(self):

InternalError: failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:37044: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:""2023-04-16T09:15:30.550805248+00:00""}
Executing non-communication op <MakeIterator> originally returned UnavailableError, and was replaced by InternalError to avoid invoking TF network error handling logic.
```
</details>","['stat:awaiting response', 'type:bug', 'comp:tpus', 'TF 2.12']",2023-04-16T09:22:35Z,4,0,https://github.com/tensorflow/tensorflow/issues/60336,Runtime Error
398,tensorflow/tensorflow,ImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

tf 2.13.0

### Custom Code

Yes

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

3.9

### Bazel version

6.1.1

### GCC/Compiler version

9.5.0

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I build the lastest TensorFlow code from source successfully with
`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

Then I generate a TensorFlow whl successfully with
`./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg`

But when I pip install this whl, import tensorflow got ""ImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory"". 

I found the whl file generated is only 80.64M. But I think it should be about 200M.

But there has libtensorflow_cc.so.2 file under path tensorflow/bazel-bin/tensorflow. I don't know why it wasn't packed into the whl file.

### Standalone code to reproduce the issue

```shell
import tensorflow as tf
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/__init__.py"", line 38, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/python/__init__.py"", line 36, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 26, in <module>
    self_check.preload_check()
  File ""/data1/envs/xtc3.9/lib/python3.9/site-packages/tensorflow/python/platform/self_check.py"", line 63, in preload_check
    from tensorflow.python.platform import _pywrap_cpu_feature_guard
ImportError: libtensorflow_cc.so.2: cannot open shared object file: No such file or directory
```
</details>","['type:bug', 'type:build/install', 'subtype: ubuntu/linux', 'TF 2.13']",2023-04-14T09:42:40Z,16,0,https://github.com/tensorflow/tensorflow/issues/60326,Dependency Issue
399,tensorflow/tensorflow,"Stride-1 tf.nn.conv2d with XLA is 1.5x slower then without XLA, as far as stride-2 tf.nn.depthwise_conv2d","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0, 2.13.0-dev20230412

### Custom Code

Yes

### OS Platform and Distribution

Google Colab

### Mobile device

_No response_

### Python version

Google Colab

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current Behaviour?

See example below to reproduce. Here is speed test results:

stride-1 conv 40.66
stride-1 conv_jit 64.11  // conv2d is slower with JIT but only if stride=1
stride-2 conv 40.18
stride-2 conv_jit 28.05  // when stride=2 it is FASTER with JIT

stride-1 dwconv 9.82
stride-1 dwconv_jit 5.72  // dwconv is faster with JIT but only if stride=1
stride-2 dwconv 2.59
stride-2 dwconv_jit 4.2  // when stride=2 it is SLOWER with JIT

### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/drive/1zqqPVVKt4ILRA1rCoWjB1uOtB3D0hDc-?usp=sharing
```


### Relevant log output

_No response_</details>","['stat:awaiting response', 'type:bug', 'comp:xla', 'type:performance', 'TF 2.12']",2023-04-13T10:01:27Z,3,0,https://github.com/tensorflow/tensorflow/issues/60312,Performance Issue
400,tensorflow/tensorflow,Training stopping because of  BufferError: Existing exports of data: object cannot be re-sized or something wrong with tornado,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

NAME=""CentOS Linux"" VERSION=""7 (Core)""

### Mobile device

NAME=""CentOS Linux"" VERSION=""7 (Core)""

### Python version

3.9.16

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

 11.8.0 

### GPU model and memory

_No response_

### Current Behaviour?

The model training would just stop abruptly 

https://colab.research.google.com/drive/1WiqyF7dCdnNBIANEY80Pxw_mVz4fyV-S?usp=sharing

### Standalone code to reproduce the issue

```shell
Voxelmoprh library training
```


### Relevant log output

```shell
(tf) vr-lab@pop-os:~$ jupyter notebook

  _   _          _      _
 | | | |_ __  __| |__ _| |_ ___
 | |_| | '_ \/ _` / _` |  _/ -_)
  \___/| .__/\__,_\__,_|\__\___|
       |_|
                       
Read the migration plan to Notebook 7 to learn about the new features and the actions to take if you are using extensions.

https://jupyter-notebook.readthedocs.io/en/latest/migrate_to_notebook7.html

Please note that updating to Notebook 7 might break some of your extensions.

[I 00:02:49.290 NotebookApp] Serving notebooks from local directory: /home/vr-lab
[I 00:02:49.290 NotebookApp] Jupyter Notebook 6.5.4 is running at:
[I 00:02:49.290 NotebookApp] http://localhost:8888/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466
[I 00:02:49.290 NotebookApp]  or http://127.0.0.1:8888/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466
[I 00:02:49.290 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 00:02:49.334 NotebookApp] 
    
    To access the notebook, open this file in a browser:
        file:///home/vr-lab/.local/share/jupyter/runtime/nbserver-405435-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466
     or http://127.0.0.1:8888/?token=697572ae046e4388d22c7be946cefcb261064994d2f99466
[I 00:03:15.170 NotebookApp] Kernel started: 4915aa8a-d4aa-4d50-885f-810d53eae7db, name: python3
[I 00:03:20.670 NotebookApp] Kernel restarted: 4915aa8a-d4aa-4d50-885f-810d53eae7db
[W 00:03:20.684 NotebookApp] Replacing stale connection: 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d
[W 00:03:21.180 NotebookApp] zmq message arrived on closed channel
[I 00:03:21.181 NotebookApp] Starting buffering for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d
[I 00:03:21.183 NotebookApp] Restoring connection for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d
[I 00:03:21.689 NotebookApp] Replaying 1 buffered messages
[E 00:03:21.761 NotebookApp] Uncaught exception, closing connection.
    Traceback (most recent call last):
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 702, in _handle_events
        self._handle_write()
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 976, in _handle_write
        self._write_buffer.advance(num_bytes)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 182, in advance
        assert 0 < size <= self._size
    AssertionError
[W 00:03:21.764 NotebookApp] Write error on <socket.socket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>: [Errno 9] Bad file descriptor
[W 00:03:21.766 NotebookApp] zmq message arrived on closed channel
[W 00:03:21.767 NotebookApp] zmq message arrived on closed channel
Exception in callback None()
handle: <Handle cancelled>
Traceback (most recent call last):
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/asyncio/events.py"", line 80, in _run
    self._context.run(self._callback, *self._args)
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/platform/asyncio.py"", line 206, in _handle_events
    handler_func(fileobj, events)
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 702, in _handle_events
    self._handle_write()
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 976, in _handle_write
    self._write_buffer.advance(num_bytes)
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 182, in advance
    assert 0 < size <= self._size
AssertionError
[I 00:03:21.768 NotebookApp] Starting buffering for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d
2023-04-11 00:03:22.084618: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-11 00:03:22.225493: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[I 00:03:22.803 NotebookApp] Restoring connection for 4915aa8a-d4aa-4d50-885f-810d53eae7db:e6146c4b818f471185049a02ac632f6d
[I 00:03:22.803 NotebookApp] Replaying 1 buffered messages
2023-04-11 00:03:22.815590: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/vr-lab/anaconda3/envs/tf/lib/:/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib
2023-04-11 00:03:22.815709: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/vr-lab/anaconda3/envs/tf/lib/:/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib
2023-04-11 00:03:22.815716: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-04-11 00:03:25.015062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12776 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:af:00.0, compute capability: 8.6
2023-04-11 00:03:40.078576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8600
[I 00:05:15.159 NotebookApp] Saving file at /Music/HybridMorph Please don't delete/HybridMorph_proof of concept.ipynb
Task exception was never retrieved
future: <Task finished name='Task-76' coro=<WebSocketProtocol13.write_message.<locals>.wrapper() done, defined at /home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py:1090> exception=WebSocketClosedError()>
Traceback (most recent call last):
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1092, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/asyncio/tasks.py"", line 256, in __step
    result = coro.send(None)
  File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1094, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
[E 01:03:52.904 NotebookApp] Exception in callback <bound method WebSocketMixin.send_ping of ZMQChannelsHandler(4915aa8a-d4aa-4d50-885f-810d53eae7db)>
    Traceback (most recent call last):
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/ioloop.py"", line 921, in _run
        val = self.callback()
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/base/zmqhandlers.py"", line 188, in send_ping
        self.ping(b'')
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 445, in ping
        self.ws_connection.write_ping(data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1101, in write_ping
        self._write_frame(True, 0x9, data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 540, in write
        self._write_buffer.append(data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 157, in append
        b += data  # type: ignore
    BufferError: Existing exports of data: object cannot be re-sized
[E 01:13:22.812 NotebookApp] Uncaught exception in ZMQStream callback
    Traceback (most recent call last):
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
        f = callback(*args, **kwargs)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
        return callback(self, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 540, in write
        self._write_buffer.append(data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 157, in append
        b += data  # type: ignore
    BufferError: Existing exports of data: object cannot be re-sized
[E 01:13:22.815 NotebookApp] Uncaught exception in zmqstream callback
    Traceback (most recent call last):
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 634, in _handle_events
        self._handle_recv()
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 663, in _handle_recv
        self._run_callback(callback, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
        f = callback(*args, **kwargs)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
        return callback(self, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 540, in write
        self._write_buffer.append(data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 157, in append
        b += data  # type: ignore
    BufferError: Existing exports of data: object cannot be re-sized
[E 01:13:22.815 NotebookApp] Exception in callback functools.partial(<function ZMQStream._update_handler.<locals>.<lambda> at 0x7f1de4ff4b80>)
    Traceback (most recent call last):
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/ioloop.py"", line 740, in _run_callback
        ret = callback()
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 718, in <lambda>
        self.io_loop.add_callback(lambda: self._handle_events(self.socket, 0))
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 634, in _handle_events
        self._handle_recv()
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 663, in _handle_recv
        self._run_callback(callback, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 584, in _run_callback
        f = callback(*args, **kwargs)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/zmq/eventloop/zmqstream.py"", line 308, in stream_callback
        return callback(self, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/services/kernels/handlers.py"", line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/notebook/base/zmqhandlers.py"", line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/websocket.py"", line 1061, in _write_frame
        return self.stream.write(frame)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 540, in write
        self._write_buffer.append(data)
      File ""/home/vr-lab/anaconda3/envs/tf/lib/python3.9/site-packages/tornado/iostream.py"", line 157, in append
        b += data  # type: ignore
    BufferError: Existing exports of data: object cannot be re-sized
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.12']",2023-04-13T05:24:00Z,28,0,https://github.com/tensorflow/tensorflow/issues/60309,Runtime Error
401,tensorflow/tensorflow,Respect Keras layer names for output operations in Concrete Function,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Feature Request

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

When converting a Keras model to concrete function, you can preserve the input name by creating a named TensorSpec, but the outputs are always created for you by just slapping `tf.identity` on top of whatever you had there, even if it was a custom named `tf.identity` operation. Since many converters rely on concrete functions to make their own representation (TFLite, ONNX, CoreML, etc), this behavior messes up the output operation names, often making them inconsistent with each other. 
There's currently no workaround for that. You *can* access previous graph nodes by calling a layer  named like {model_name}/{output_layer_name} when doing inference on frozen graph itself, but it won't help you in any way to convert the model.

So I'd be happy to see one of those things as a solution to that:
1) Add an option to explicitly specify the TensorSpec for outputs, just the way we do it for inputs. This would be the most obvious and convenient way of doing it from a user standpoint
2) Don't add new identity operations on top of existing ones. More of a kludge, but would get the job done
3) Add an option to rename operations in concrete function post factum.
4) Add an option to cut off the operations in concrete function past a certain node. 
5) Add an option to convert a graph into a concrete function. Since you can directly modify graphs, this could work as well


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

# Build simple model
inputs = tf.keras.Input((224, 224, 3), name='custom_input_layer')
x = tf.keras.layers.Flatten()(inputs)
x = tf.keras.layers.Dense(512, activation='relu')(x)
x = tf.keras.layers.Dense(256, activation='relu')(x)
x = tf.keras.layers.Dense(128, activation='relu')(x)
x = tf.keras.layers.Dense(1, activation='sigmoid', name='custom_output_layer')(x)

model = tf.keras.Model(inputs=inputs, outputs=x, name='my_custom_model_name')
model.summary()

input_tensors = [tf.TensorSpec(shape=inp.shape, dtype=tf.float32, name=inp.name) for inp in model.inputs]
concrete_function = tf.function(lambda x: model(x)).get_concrete_function(x=input_tensors)

print(concrete_function.inputs)  # we can see 'custom_input_layer:0' is there. So is the 'true' output 'my_custom_model_name/custom_output_layer/BiasAdd/ReadVariableOp/resource:0'
print(concrete_function.outputs)  # pesky Identity node gets inserted
```


### Relevant log output

```shell
Model: ""my_custom_model_name""
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 custom_input_layer (InputLa  [(None, 224, 224, 3)]    0         
 yer)                                                            
                                                                 
 flatten (Flatten)           (None, 150528)            0         
                                                                 
 dense (Dense)               (None, 512)               77070848  
                                                                 
 dense_1 (Dense)             (None, 256)               131328    
                                                                 
 dense_2 (Dense)             (None, 128)               32896     
                                                                 
 custom_output_layer (Dense)  (None, 1)                129       
                                                                 
=================================================================
Total params: 77,235,201
Trainable params: 77,235,201
Non-trainable params: 0
_________________________________________________________________
[<tf.Tensor 'custom_input_layer:0' shape=(None, 224, 224, 3) dtype=float32>, <tf.Tensor 'my_custom_model_name/dense/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_1/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_1/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_2/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/dense_2/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/custom_output_layer/MatMul/ReadVariableOp/resource:0' shape=() dtype=resource>, <tf.Tensor 'my_custom_model_name/custom_output_layer/BiasAdd/ReadVariableOp/resource:0' shape=() dtype=resource>]
[<tf.Tensor 'Identity:0' shape=(None, 1) dtype=float32>]
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.12']",2023-04-11T08:14:38Z,9,1,https://github.com/tensorflow/tensorflow/issues/60289,UI/UX Bug
402,tensorflow/tensorflow,Collected Tensorflow profiles are not recognized in Tensorboard,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf 2.13.0-dev20230406

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 22.04

### Mobile device

n/a

### Python version

3.9.16

### Bazel version

n/a

### GCC/Compiler version

n/a

### CUDA/cuDNN version

11.8.0/8.6.0.163

### GPU model and memory

NVIDIA GeForce RTX 3080 Ti 12GiB

### Current Behaviour?

I am following the tutorial at https://www.tensorflow.org/tutorials/quickstart/beginner

I modified the code according to the instructions at https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras in order to enable profiling for a range of batches during training.

With this change, training seems to proceed as normal, with the logs indicating that a profiler session is created, and a profile is collected.

The logs directory contains one non-empty `plugins/profile/<date>/<host>.xplane.pb` file.

But when I run tensorboard (either main or tb-nightly) on the logs, it fails to detect a profile (the Profile tab is missing from the UI). I also confirm I ran `pip install -U tensorboard-plugin-profile` first.

I would have expected one of these two outcomes: either (a) tensorboard would show me the profiles, or (b) if something went wrong either when collecting or displaying the profiles, an error message would have indicated it so I can fix the issue.

### Standalone code to reproduce the issue

```shell
# The code is at https://www.tensorflow.org/tutorials/quickstart/beginner
# I change the model.fit() call to use the Tensorboard callback to collect a profile:

log_dir = ""logs/fit/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = tf.keras.callbacks.TensorBoard(
    log_dir=log_dir,
    histogram_freq=1,
    profile_batch=(500, 600))
model.fit(x_train, y_train, epochs=5, callbacks=[tensorboard_callback])
```

### Relevant log output

```shell
2023-04-06 23:17:28.048863: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2023-04-06 23:17:28.048880: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
2023-04-06 23:17:28.048915: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1671] Profiler found 1 GPUs
2023-04-06 23:17:28.237604: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
2023-04-06 23:17:28.237742: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1805] CUPTI activity buffer flushed
Epoch 1/5
2023-04-06 23:17:28.747772: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f08c0180cf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-04-06 23:17:28.747785: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6
2023-04-06 23:17:28.751189: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-04-06 23:17:28.834436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:426] Loaded cuDNN version 8600
2023-04-06 23:17:28.868033: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2023-04-06 23:17:28.900180: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 522/1875 [=======>......................] - ETA: 5s - loss: 0.4875 - accuracy: 0.8590
2023-04-06 23:17:30.991051: I tensorflow/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.
2023-04-06 23:17:30.991106: I tensorflow/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.
 645/1875 [=========>....................] - ETA: 4s - loss: 0.4499 - accuracy: 0.8701
2023-04-06 23:17:31.542500: I tensorflow/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.
2023-04-06 23:17:31.545123: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_tracer.cc:1805] CUPTI activity buffer flushed
2023-04-06 23:17:31.570874: I tensorflow/compiler/xla/backends/profiler/gpu/cupti_collector.cc:541]  GpuTracer has collected 6158 callback api events and 5891 activity events. 
2023-04-06 23:17:31.598454: I tensorflow/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.
1875/1875 [==============================] - 8s 4ms/step - loss: 0.3017 - accuracy: 0.9121
Epoch 2/5
1875/1875 [==============================] - 7s 4ms/step - loss: 0.1441 - accuracy: 0.9570
Epoch 3/5
1875/1875 [==============================] - 7s 4ms/step - loss: 0.1075 - accuracy: 0.9685
Epoch 4/5
1875/1875 [==============================] - 6s 3ms/step - loss: 0.0878 - accuracy: 0.9732
Epoch 5/5
1875/1875 [==============================] - 6s 3ms/step - loss: 0.0737 - accuracy: 0.9771
```
</details>","['comp:tensorboard', 'stat:awaiting response', 'type:bug', 'TF 2.12']",2023-04-07T03:33:16Z,6,1,https://github.com/tensorflow/tensorflow/issues/60262,Runtime Error
403,tensorflow/tensorflow,Use after free in propagator_state.cc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

tf2.12

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Pointer `next_iter` from function `PropagatorState::FrameState::IncrementIteration` is passed 
as the 1st parameter into `ActivateLoopInvs` where it is passed as the 1st parameter into 
`AdjustOutstandingOpsLocked`, then inside this function it is passed into `CleanupIterations` 
where it is deleted.

Then in `PropagatorState::FrameState::IncrementIteration` this possibly freed pointer is used 
in `return`-statement.

This behavior was introduced by https://github.com/tensorflow/tensorflow/commit/ae2a0e5c473f2a575767262021c26852d22886f8. 
Before this commit, no `return` was performed on the possibly freed pointer.
```


### Standalone code to reproduce the issue

```shell
Bug was found by Svace static analysis tool.
```


### Relevant log output

_No response_</details>","['stat:awaiting response', 'type:bug', 'comp:runtime', 'TF 2.12']",2023-04-06T11:55:35Z,10,0,https://github.com/tensorflow/tensorflow/issues/60255,Logical Bug
404,tensorflow/tensorflow,"In tf.data.experimental.enable_debug_mode, tf.data.Dataset.ragged_batch fails with an error","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

TF 2.12.0, TF nightly 2.13.0-dev20230404

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

Consider the following code creating ragged batches using `tf.data.Dataset.ragged_batch`:

```python
data = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2], [3]]))
list(data.ragged_batch(2))
```

The above code works fine in normal mode. However, if you enable debug mode using `tf.data.experimental.enable_debug_mode()`, the same code crashes with an error.

### Standalone code to reproduce the issue

I reproduced the error in https://colab.research.google.com/drive/1nf1BHjssx2YhF0ZbbgPg1QSALSS4Z89r?usp=sharing , both for TF 2.12.0 and TF nightly 2.13.0-dev20230404.

The code for triggering the bug is the following:

```python
tf.data.experimental.enable_debug_mode()
data = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2], [3]]))
list(data.ragged_batch(2))
```

### Relevant log output

Here is the error printed by TF 2.12.0

```
---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

<ipython-input-3-34b7e4bb8c4b> in <cell line: 4>()
      2 tf.data.experimental.enable_debug_mode()
      3 data = tf.data.Dataset.from_tensor_slices(tf.ragged.constant([[1, 2], [3]]))
----> 4 list(data.ragged_batch(2))

3 frames

/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6651 def raise_from_not_ok_status(e, name):
   6652   e.message += ("" name: "" + str(name if name is not None else """"))
-> 6653   raise core._status_to_exception(e) from None  # pylint: disable=protected-access
   6654 
   6655 

InvalidArgumentError: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: Value [1 2] is not convertible to a tensor with dtype <dtype: 'variant'> and shape ().
Traceback (most recent call last):

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/util/structure.py"", line 347, in reduce_fn
    component = ops.convert_to_tensor(component, spec.dtype)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/profiler/trace.py"", line 183, in wrapped
    return func(*args, **kwargs)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py"", line 1440, in convert_to_tensor
    return tensor_conversion_registry.convert(

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 209, in convert
    return overload(dtype, name)  #  pylint: disable=not-callable

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py"", line 1335, in __tf_tensor__
    return super().__tf_tensor__(dtype, name)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py"", line 967, in __tf_tensor__
    raise ValueError(

ValueError: Tensor conversion requested dtype variant for Tensor with dtype int32: <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>


During handling of the above exception, another exception occurred:


Traceback (most recent call last):

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/script_ops.py"", line 266, in __call__
    return func(device, token, args)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/script_ops.py"", line 144, in __call__
    outputs = self._call(device, args)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/script_ops.py"", line 151, in _call
    ret = self._func(*args)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/impl/api.py"", line 643, in wrapper
    return func(*args, **kwargs)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py"", line 213, in py_function_wrapper
    ret = structure.to_tensor_list(self._output_structure, ret)

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/util/structure.py"", line 410, in to_tensor_list
    return _to_tensor_list_helper(

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/util/structure.py"", line 360, in _to_tensor_list_helper
    return functools.reduce(

  File ""/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/util/structure.py"", line 349, in reduce_fn
    raise ValueError(

ValueError: Value [1 2] is not convertible to a tensor with dtype <dtype: 'variant'> and shape ().


	 [[{{node EagerPyFunc}}]] [Op:IteratorGetNext] name:
```
</details>","['stat:awaiting response', 'type:bug', 'comp:data', 'TF 2.12']",2023-04-05T10:26:05Z,1,0,https://github.com/tensorflow/tensorflow/issues/60239,Runtime Error
405,tensorflow/tensorflow,Distributed training using multiple GPUs hangs when enabling eager execution,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04.5 LTS

### Mobile device

_No response_

### Python version

3.8.10 (docker tensorflow:devel-gpu)

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

_No response_

### Current Behaviour?

```shell
After adding the following code to enable eager execution:
`
tf.config.run_functions_eagerly(True)
tf.data.experimental.enable_debug_mode()
`

Distributed training using MirroredStrategy with more than 2 GPUs hangs forever.

Using 1 GPU does not trigger the bug. Using MirroredStrategy with multiple logical CPUs also does not trigger the bug.

Whether adding the second line (`tf.data.experimental.enable_debug_mode()`) or not does not matter.

The model we used is a simple Sequential model trained on CIFAR-10 dataset.
The model structure is given as follows:
`
Input - GlobalMaxPool2D - BatchNormalization- Dense
`

Current behavior:
Using tf 2.11.0, the program hangs at the line of code:
`loss = model.fit(train_input, train_label)`
Also, the program cannot be stopped gracefully by keyboard interruption.

Using tf nightly, the program reports an error (see below).

Expected behavior:
The program should print the values of loss as an eager tensor.
```


### Standalone code to reproduce the issue

```shell
The code is also available at https://colab.research.google.com/drive/1BiAxZEO9IGak_4XOeZcyXRhJEByZ-X6B?usp=sharing

import keras
import tensorflow as tf

tf.config.run_functions_eagerly(True)
tf.data.experimental.enable_debug_mode()

if __name__ == ""__main__"":
    # Using at least 2 GPU triggers the bug
    strategy = tf.distribute.MirroredStrategy([""/GPU:0"", ""/GPU:1""])

    # Training data, batch_size=240
    train_input = tf.random.uniform(shape=(240, 32, 32, 3))
    train_label = tf.random.uniform(shape=(240, 10), minval=0, maxval=2, dtype=tf.int32)

    with strategy.scope():
        model = keras.Sequential([
            keras.layers.Input(shape=(32, 32, 3)),
            keras.layers.GlobalMaxPool2D(),
            keras.layers.BatchNormalization(axis=-1),
            keras.layers.Dense(10, ), ])
        optimizer = keras.optimizers.Adam(learning_rate=0.1)
        loss = tf.keras.losses.MeanSquaredError(
            reduction=tf.keras.losses.Reduction.NONE)
        model.compile(optimizer=optimizer, loss=loss)

    # One step training
    loss = model.fit(train_input, train_label)
    print(loss)
```


### Relevant log output

```shell
-------------- Output using tf 2.11.0 -----------------------------
tf-docker /mnt/src/reproduce > python ./reproduce_eager.py 
2023-04-05 03:23:45.884503: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-05 03:23:48.001421: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-05 03:23:53.074754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9636 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5
2023-04-05 03:23:53.076188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9636 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3f:00.0, compute capability: 7.5
2023-04-05 03:23:53.077442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9636 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:40:00.0, compute capability: 7.5
2023-04-05 03:23:53.078666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9636 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5
2023-04-05 03:23:53.079887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 9636 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:61:00.0, compute capability: 7.5
2023-04-05 03:23:53.081109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 9636 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:62:00.0, compute capability: 7.5
2023-04-05 03:23:53.082252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 9636 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:63:00.0, compute capability: 7.5
2023-04-05 03:23:53.083395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 9636 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:64:00.0, compute capability: 7.5
WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.
(program hangs here)
--------------------------------------------------------------------

-------------- Output using tf nightly -----------------------------
tf-docker /mnt/src/reproduce > python ./reproduce_eager.py 
2023-04-05 03:29:24.607254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-05 03:29:25.559092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-05 03:29:32.898888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9598 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5
2023-04-05 03:29:32.900404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9598 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3f:00.0, compute capability: 7.5
2023-04-05 03:29:32.901638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 9598 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:40:00.0, compute capability: 7.5
2023-04-05 03:29:32.902870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 9598 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:41:00.0, compute capability: 7.5
2023-04-05 03:29:32.904072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 9598 MB memory:  -> device: 4, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:61:00.0, compute capability: 7.5
2023-04-05 03:29:32.905328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 9598 MB memory:  -> device: 5, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:62:00.0, compute capability: 7.5
2023-04-05 03:29:32.906501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 9598 MB memory:  -> device: 6, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:63:00.0, compute capability: 7.5
2023-04-05 03:29:32.907653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 9598 MB memory:  -> device: 7, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:64:00.0, compute capability: 7.5
2023-04-05 03:29:33.578635: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_9' with dtype int32 and shape [240,10]
         [[{{node Placeholder/_9}}]]
2023-04-05 03:29:33.578971: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_9' with dtype int32 and shape [240,10]
         [[{{node Placeholder/_9}}]]
WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.
2023-04-05 03:29:35.301668: E tensorflow/core/common_runtime/base_collective_executor.cc:249] BaseCollectiveExecutor::StartAbort UNKNOWN: Error invoking NCCL: unhandled cuda error
Traceback (most recent call last):
  File ""./reproduce_eager.py"", line 27, in <module>
    loss = model.fit(train_input, train_label)
  File ""/usr/local/lib/python3.8/dist-packages/keras/src/utils/traceback_utils.py"", line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 364, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 771, in get
    raise self._value
  File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 125, in worker
    result = (True, func(*args, **kwds))
  File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 48, in mapstar
    return list(map(*args))
tensorflow.python.framework.errors_impl.UnknownError: {{function_node __wrapped__CollectiveReduceV2_Nordering_token_1_device_/job:localhost/replica:0/task:0/device:GPU:1}} Collective ops is aborted by: Error invoking NCCL: unhandled cuda error
The error could be from a previous operation. Restart your program to reset. [Op:CollectiveReduceV2] name:
--------------------------------------------------------------------
```
</details>","['stat:awaiting response', 'type:bug', 'comp:gpu', 'TF 2.11']",2023-04-05T03:33:55Z,6,0,https://github.com/tensorflow/tensorflow/issues/60236,Runtime Error
406,tensorflow/tensorflow,A check fail can be triggered in ThreadUnsafeUnigramCandidateSampler,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230331

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.ThreadUnsafeUnigramCandidateSampler` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""GPU:0""):
    num_true = 11
    num_sampled = 2
    unique = False
    range_max = 7612169259283414040
    seed = -111
    seed2 = -11
    true_classes = tf.saturate_cast(tf.random.uniform([12, 11], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int64)
    res = tf.raw_ops.ThreadUnsafeUnigramCandidateSampler(
        num_true=num_true,
        num_sampled=num_sampled,
        unique=unique,
        range_max=range_max,
        seed=seed,
        seed2=seed2,
        true_classes=true_classes,
    )
```


### Relevant log output

```shell
2023-04-01 16:33:33.009606: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-01 16:33:33.057487: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-01 16:33:33.874853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-01 16:33:35.397082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-01 16:33:40.359234: F tensorflow/core/kernels/range_sampler.cc:183] Check failed: range < kint32max (7612169259283414040 vs. 2147483647)
Aborted (core dumped)
```
</details>","['awaiting review', 'type:bug', 'comp:ops', 'TF 2.12']",2023-04-01T08:36:16Z,2,0,https://github.com/tensorflow/tensorflow/issues/60200,Runtime Error
407,tensorflow/tensorflow,A check fail can be triggered in LearnedUnigramCandidateSampler,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.13.0-dev20230331

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
The following code can trigger a crash in `tf.raw_ops.LearnedUnigramCandidateSampler` due to check-fail in the latest version of TensorFlow.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
with tf.device(""GPU:0""):
    num_true = 13
    num_sampled = 48
    unique = True
    range_max = 3031324185113192368
    seed = 93
    seed2 = 11
    true_classes = tf.saturate_cast(tf.random.uniform([14, 13], minval=-1024, maxval=1024, dtype=tf.int64), dtype=tf.int64)
    res = tf.raw_ops.LearnedUnigramCandidateSampler(
        num_true=num_true,
        num_sampled=num_sampled,
        unique=unique,
        range_max=range_max,
        seed=seed,
        seed2=seed2,
        true_classes=true_classes,
    )
```


### Relevant log output

```shell
2023-04-01 16:20:32.160750: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-04-01 16:20:32.211959: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-01 16:20:33.026789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-01 16:20:34.550122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14561 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:2f:00.0, compute capability: 7.0
2023-04-01 16:20:39.689581: F tensorflow/core/kernels/range_sampler.cc:183] Check failed: range < kint32max (3031324185113192368 vs. 2147483647)
Aborted (core dumped)
```
</details>","['stat:awaiting response', 'type:bug', 'comp:ops', 'TF 2.12']",2023-04-01T08:23:48Z,4,0,https://github.com/tensorflow/tensorflow/issues/60197,Runtime Error
408,tensorflow/tensorflow,tf.data.Dataset.from_generator crashes with abortion,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

2.13.0-dev20230208

### Custom Code

Yes

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.data.Dataset.from_generator crashes with abortion
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
ds = tf.data.Dataset.from_tensors([1]).repeat(-1)
def gen():
  for _ in ds:
    yield _
ds = tf.data.Dataset.from_generator(
    gen, output_types=tf.int32)
list(ds.take(2).as_numpy_iterator())
```


### Relevant log output

```shell
2023-03-28 12:06:28.440209: F tensorflow/tsl/platform/default/env.cc:74] Check failed: ret == 0 (11 vs. 0)Thread tf_data_private_threadpool creation via pthread_create() failed.
Aborted (core dumped)
```
</details>","['stat:awaiting response', 'type:bug', 'comp:data', 'TF 2.12']",2023-03-28T17:06:53Z,7,0,https://github.com/tensorflow/tensorflow/issues/60149,Logical Bug
409,tensorflow/tensorflow,"Importing TF 2.12, then torch, hangs, but not the other way around","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.12.0

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04.5

### Mobile device

_No response_

### Python version

3.8.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.8

### GPU model and memory

_No response_

### Current Behaviour?

```shell
If I import tensorflow and then import torch, the torch import line hangs forever without completing. On the other hand, if I import torch first and then import tensorflow there is no problem.

The hang is so severe that no amount of ctr-c can kill it. You have to kill the python process from a separate terminal to free the hung terminal. 

This issue does not exist in tensorflow 2.11.1 or earlier. It also doesn't happen when using older versions of torch like 1.13.1. Since torch followed by tf works but tf followed by torch doesn't, this seems like an issue tf is causing.
```


### Standalone code to reproduce the issue

```shell
docker pull tensorflow/tensorflow:2.12.0-gpu
docker run -it tensorflow/tensorflow:2.12.0-gpu

pip install torch==2.0.0+cu118 torchvision==0.15.1+cu118 torchaudio==2.0.1+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html

python

import tensorflow as tf
import torch
```


### Relevant log output

_No response_</details>","['stat:awaiting response', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux', 'TF 2.12']",2023-03-24T23:56:20Z,16,6,https://github.com/tensorflow/tensorflow/issues/60109,UI/UX Bug
410,tensorflow/tensorflow,Linking external/org_tensorflow/tensorflow/libtensorflow_cc.so.2.13.0 failed: (Exit 1): gcc failed: error executing command ,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

Head

### Custom Code

No

### OS Platform and Distribution

Linux

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

[Dynamic Pywrap PR 58734](https://github.com/tensorflow/tensorflow/pull/58734) seems to introduce a a TensorFlow Serving linking error.


### Standalone code to reproduce the issue

Run
```shell
$ git clone git@github.com:tensorflow/serving.git
$ cd serving
$ sudo ./tools/run_in_docker.sh bazel test --action_env=TF_REVISION=6147c03eb9af1e5d2ae155045b33e909ef96944e tensorflow_serving/... &> /tmp/tfs_bazel_output.log
```
and search `Linking external` in /tmp/tfs_bazel_output.log



### Relevant log output

```shell
/usr/local/google/home/rostam/Workspace/tmp/serving/.cache/_bazel_root/7c4195127656842bcf1efda48d8fd065/external/org_tensorflow/tensorflow/BUILD:1214:21: Linking external/org_tensorflow/tensorflow/libtensorflow_cc.so.2.12.0 failed: (Exit 1): gcc failed: error executing command 
  (cd /usr/local/google/home/rostam/Workspace/tmp/serving/.cache/_bazel_root/7c4195127656842bcf1efda48d8fd065/execroot/tf_serving && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    TF_REVISION=6147c03eb9af1e5d2ae155045b33e909ef96944e \
  /usr/bin/gcc @bazel-out/k8-opt/bin/external/org_tensorflow/tensorflow/libtensorflow_cc.so.2.12.0-2.params)

```","['stat:awaiting response', 'type:bug', 'type:build/install', 'subtype: ubuntu/linux', 'TF 2.12']",2023-03-22T02:34:16Z,2,0,https://github.com/tensorflow/tensorflow/issues/60061,Runtime Error
411,tensorflow/tensorflow,Missing Window GPU prebuilt binary for 2.11.0,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

tf 2.11

### Custom Code

No

### OS Platform and Distribution

Windows

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-windows-x86_64-2.11.0.zip is missing, which is breaking TensorFlow Rust for Windows with GPU: https://github.com/tensorflow/rust/issues/400
```


### Standalone code to reproduce the issue

```shell
N/A
```


### Relevant log output

_No response_</details>","['stat:awaiting response', 'type:bug', 'type:build/install', 'subtype:windows', 'TF 2.11']",2023-02-28T03:46:40Z,10,1,https://github.com/tensorflow/tensorflow/issues/59828,UI/UX Bug
412,tensorflow/tensorflow,XLA inference fails complaning about branch shape mismatches,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

binary

### Tensorflow Version

2.10.0

### Custom Code

Yes

### OS Platform and Distribution

Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

A100 40 GB

### Current Behaviour?

```shell
I have the following function. 

First, I initialize a CLIP-based text encoder:


from keras_cv.models.stable_diffusion.text_encoder import TextEncoder

MAX_PROMPT_LENGTH = 77
text_encoder = TextEncoder(MAX_PROMPT_LENGTH)


Then, I am using the `text_encoder` like so in a function that I use to serialize the `text_encoder` as a `SavedModel`:

```python
from keras_cv.models.stable_diffusion.constants import _UNCONDITIONAL_TOKENS
import tensorflow as tf 

signature_dict = {
    ""tokens"": tf.TensorSpec(shape=[None, 77], dtype=tf.int32, name=""tokens""),
}

def text_encoder_exporter(model: tf.keras.Model):
    BATCH_SIZE = 3
    MAX_PROMPT_LENGTH = 77
    POS_IDS = tf.convert_to_tensor([list(range(MAX_PROMPT_LENGTH))], dtype=tf.int32)
    UNCONDITIONAL_TOKENS = tf.convert_to_tensor([_UNCONDITIONAL_TOKENS], dtype=tf.int32)

    @tf.function(input_signature=[signature_dict])
    def serving_fn(inputs):
        # context
        encoded_text = model([inputs[""tokens""], POS_IDS], training=False)
        encoded_text = tf.squeeze(encoded_text)

        if tf.rank(encoded_text) == 2:
            encoded_text = tf.repeat(
                tf.expand_dims(encoded_text, axis=0), BATCH_SIZE, axis=0
            )

        # unconditional context
        unconditional_context = model([UNCONDITIONAL_TOKENS, POS_IDS], training=False)

        unconditional_context = tf.repeat(unconditional_context, BATCH_SIZE, axis=0)
        return {""context"": encoded_text, ""unconditional_context"": unconditional_context}

    return serving_fn
```

Serialization:

```python
tf.saved_model.save(
    text_encoder,
    ""./text_encoder/1/"",
    signatures={""serving_default"": text_encoder_exporter(text_encoder)},
)
```

Now, while attempting to XLA-compile:

```python
from tensorflow.python.saved_model import tag_constants

batch_size = 3
saved_model_loaded = tf.saved_model.load(
    ""./text_encoder/1/"", tags=[tag_constants.SERVING]
)
text_encoder_predict_fn = saved_model_loaded.signatures[""serving_default""]
# Raises error
xla_text_encoder_predict_fn = tf.function(text_encoder_predict_fn, jit_compile=True)
xla_text_encoder_predict_fn(
    tokens=tf.ones((batch_size, MAX_PROMPT_LENGTH), tf.int32)
).keys()
```
```


### Standalone code to reproduce the issue

```shell
https://colab.research.google.com/gist/sayakpaul/d7dafc252752a6c1ce10e85d8162b8ea/scratchpad.ipynb
```


### Relevant log output

```shell
/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153       raise e.with_traceback(filtered_tb) from None
    154     finally:
    155       del filtered_tb

/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50   try:
     51     ctx.ensure_initialized()
---> 52     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                         inputs, attrs, num_outputs)
     54   except core._NotOkStatusException as e:

InvalidArgumentError: left_branch_shape.rank() != right_branch_shape.rank() (4 vs 3)
	 [[{{function_node __inference_serving_fn_55983}}{{node cond}}]] [Op:__inference_function_60000]
```
</details>","['stat:awaiting response', 'type:bug', 'comp:keras', 'comp:xla', 'TF 2.10']",2023-02-27T05:43:50Z,5,0,https://github.com/tensorflow/tensorflow/issues/59818,Runtime Error
413,tensorflow/tensorflow,AutoGraph did convert this function: NameError: name 'Tuple' is not defined,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

No

### Source

source

### Tensorflow Version

2.11.0

### Custom Code

Yes

### OS Platform and Distribution

Mac, Linux, Google Colab

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

</details>

### Current Behaviour?

`tf.function` fails when type annotations are used which are locally imported. I get:

```
Cause: name 'Tuple' is not defined
```

Note:

* It seems important that `Tuple` is locally imported. If it is imported globally in the module, there does not seem to be a problem.
* When I use `from __future__ import annotations`, there is also no error. But I assume because this will just not evaluate it directly, but it still lacks the `Tuple` reference, although it's maybe really not relevant then.


### Standalone code to reproduce the issue

```python
import tensorflow as tf

print(""TensorFlow:"", tf.__version__)

tf.compat.v1.disable_eager_execution()

session = tf.compat.v1.Session()


# https://www.tensorflow.org/guide/function


def f(x: tf.Tensor):
  from typing import Tuple

  @tf.function
  def local_func(x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
    while tf.reduce_sum(x) > 1:
      tf.print(x)
      x = tf.tanh(x)
    return x, x

  return local_func(x)


session.run(f(tf.random.uniform([5])))
```


Colab link: https://colab.research.google.com/drive/1K69XH_RsU-Ux-RBfUd0B4eR8iJFTXoFM?usp=sharing

### Relevant log output

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 427, in converted_call
    converted_f = _convert_actual(target_entity, program_ctx)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py"", line 269, in _convert_actual
    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 282, in transform
    return self.transform_function(obj, user_context)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 490, in transform_function
    transformed_fn = factory.instantiate(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 213, in instantiate
    new_fn = bound_factory(**self._extra_locals)  # pylint:disable=not-callable
  File ""/tmp/__autograph_generated_filem5lq6_hq.py"", line 6, in inner_factory
    def tf__local_func(x: tf.Tensor) -> Tuple[(tf.Tensor, tf.Tensor)]:
NameError: name 'Tuple' is not defined
WARNING:tensorflow:AutoGraph could not transform <function f.<locals>.local_func at 0x7f15f5ff3af0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: name 'Tuple' is not defined
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
TensorFlow: 2.11.0
Converted call: <function f.<locals>.local_func at 0x7f15f5ff3af0>
    args: (<tf.Tensor 'x:0' shape=(5,) dtype=float32>,)
    kwargs: {}

<function f.<locals>.local_func at 0x7f15f5ff3af0> is not cached for subkey ConversionOptions[{}]
Source code of <function f.<locals>.local_func at 0x7f15f5ff3af0>:

@tf.function
def local_func(x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
  while tf.reduce_sum(x) > 1:
    tf.print(x)
    x = tf.tanh(x)
  return x, x


Transformed <function f.<locals>.local_func at 0x7f15f5ff3af0>:

# coding=utf-8
def tf__local_func(x: tf.Tensor) -> Tuple[(tf.Tensor, tf.Tensor)]:
    with ag__.FunctionScope('local_func', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:
        do_return = False
        retval_ = ag__.UndefinedReturnValue()

        def get_state():
            return (x,)

        def set_state(vars_):
            nonlocal x
            (x,) = vars_

        def loop_body():
            nonlocal x
            ag__.converted_call(ag__.ld(tf).print, (ag__.ld(x),), None, fscope)
            x = ag__.converted_call(ag__.ld(tf).tanh, (ag__.ld(x),), None, fscope)

        def loop_test():
            return (ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.ld(x),), None, fscope) > 1)
        ag__.while_stmt(loop_test, loop_body, get_state, set_state, ('x',), {})
        try:
            do_return = True
            retval_ = (ag__.ld(x), ag__.ld(x))
        except:
            do_return = False
            raise
        return fscope.ret(retval_, do_return)

Error transforming entity <function f.<locals>.local_func at 0x7f15f5ff3af0>
WARNING: AutoGraph could not transform <function f.<locals>.local_func at 0x7f15f5ff3af0> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: name 'Tuple' is not defined
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:tf.function', 'TF 2.11']",2023-02-24T14:12:46Z,7,0,https://github.com/tensorflow/tensorflow/issues/59796,Runtime Error
414,tensorflow/tensorflow,Status of int8 dot/conv with XLA+CUDA,"Are tensorcore-accelerated int8 dot/convs with XLA accessible via tensorflow APIs? https://github.com/tensorflow/tensorflow/pull/30771 suggests that int8 convs are supported, and https://github.com/tensorflow/tensorflow/issues/49140 also suggest that this functionality exists. However, the tensorflow convolution operations don't allow int8 arguments. The matmul operator allows int8 x int8 -> int32 via the output_type argument, and I've successfully compiled with XLA using this option. However, when doing this, I'm not getting the throughput that I'd expect with int8 tensorcores. I was expecting something approaching 2x throughput compared to fp16 for large matmuls ([8192, 8192] inputs). Admittedly my benchmarking is not very rigorous, but I'd be curious to know if my expectation of ~2x throughput is reasonable and whether XLA has the functionality to facilitate it.

Here's a minimal implementation of an XLA-compiled int8 matmul:
```python
@tf.function(jit_compile=True)
def int8_matmul(x, w):
    return tf.matmul(x, w, output_type=tf.int32)


x = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)
w = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)
y = int8_matmul(x, w)

print(int8_matmul.experimental_get_compiler_ir(x, w)(stage=""optimized_hlo""))
# HloModule a_inference_int8_matmul_17__.10, alias_passthrough_params=true, entry_computation_layout={(s8[2048,2048]{1,0},s8[2048,2048]{1,0})->s32[2048,2048]{1,0}}

# ENTRY %a_inference_int8_matmul_17__.10 (arg0.1: s8[2048,2048], arg1.2: s8[2048,2048]) -> s32[2048,2048] {
#   %arg0.1 = s8[2048,2048]{1,0} parameter(0), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %arg1.2 = s8[2048,2048]{1,0} parameter(1), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %copy = s8[2048,2048]{0,1} copy(s8[2048,2048]{1,0} %arg1.2), metadata={op_name=""XLA_Args""}
#   ROOT %cublas-gemm.1 = s32[2048,2048]{1,0} custom-call(s8[2048,2048]{1,0} %arg0.1, s8[2048,2048]{0,1} %copy), custom_call_target=""__cublas$gemm"", metadata={op_type=""BatchMatMulV3"" op_name=""MatMul"" source_file=""int8_xla.py"" source_line=22}, backend_config=""{\""alpha_real\"":1,\""alpha_imag\"":0,\""beta\"":0,\""dot_dimension_numbers\"":{\""lhs_contracting_dimensions\"":[\""1\""],\""rhs_contracting_dimensions\"":[\""0\""],\""lhs_batch_dimensions\"":[],\""rhs_batch_dimensions\"":[]},\""precision_config\"":{\""operand_precision\"":[\""DEFAULT\"",\""DEFAULT\""]},\""epilogue\"":\""DEFAULT\""}""
# }
```

And here's my attempt at implementing a 'canonical' quantized linear layer.  I think this is a fairly standard sequence of operations for a quantized layer, and I believe TensorRT would be able to fuse this entire operation.
```python
@tf.function(jit_compile=True)
def int8_linear_layer(x, w, b, s):
    # read in x and w as pre-quantized int8 tensors
    y = tf.matmul(x, w, output_type=tf.int32)

    # add bias and apply activation in fp32
    y = tf.cast(y, tf.float32)
    y = y + b
    y = tf.nn.relu(y)

    # quantize and store output as int8
    y = tf.round(y / s)
    y = tf.clip_by_value(y, -128, 127)
    y = tf.cast(y, tf.int8)
    return y


x = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)
w = tf.cast(tf.random.uniform([2048, 2048], minval=-127, maxval=127, dtype=tf.int32), tf.int8)
b = tf.random.normal([2048], dtype=tf.float32)  # bias
s = tf.random.normal([], dtype=tf.float32)  # per-tensor quantization scale for output activation
y = int8_linear_layer(x, w, b, s)

print(int8_linear_layer.experimental_get_compiler_ir(x, w, b, s)(stage=""optimized_hlo""))
# HloModule a_inference_int8_linear_layer_65__.32, alias_passthrough_params=true, entry_computation_layout={(s8[2048,2048]{1,0},s8[2048,2048]{1,0},f32[2048]{0},f32[])->s8[2048,2048]{1,0}}

# %fused_computation (param_0.2: f32[], param_1.4: f32[2048], param_2.7: s32[2048,2048]) -> s8[2048,2048] {
#   %constant_2 = f32[] constant(-128), metadata={op_type=""Maximum"" op_name=""clip_by_value"" source_file=""int8_xla.py"" source_line=52}
#   %broadcast.4 = f32[2048,2048]{1,0} broadcast(f32[] %constant_2), dimensions={}, metadata={op_type=""Maximum"" op_name=""clip_by_value"" source_file=""int8_xla.py"" source_line=52}
#   %param_2.7 = s32[2048,2048]{1,0} parameter(2)
#   %convert.1 = f32[2048,2048]{1,0} convert(s32[2048,2048]{1,0} %param_2.7), metadata={op_type=""Cast"" op_name=""Cast"" source_file=""int8_xla.py"" source_line=46}
#   %param_1.4 = f32[2048]{0} parameter(1)
#   %broadcast.3 = f32[2048,2048]{1,0} broadcast(f32[2048]{0} %param_1.4), dimensions={1}, metadata={op_type=""AddV2"" op_name=""add"" source_file=""int8_xla.py"" source_line=47}
#   %add.0 = f32[2048,2048]{1,0} add(f32[2048,2048]{1,0} %convert.1, f32[2048,2048]{1,0} %broadcast.3), metadata={op_type=""AddV2"" op_name=""add"" source_file=""int8_xla.py"" source_line=47}
#   %constant_1 = f32[] constant(0), metadata={op_type=""Relu"" op_name=""Relu"" source_file=""int8_xla.py"" source_line=48}
#   %broadcast.2 = f32[2048,2048]{1,0} broadcast(f32[] %constant_1), dimensions={}, metadata={op_type=""Relu"" op_name=""Relu""}
#   %maximum.0 = f32[2048,2048]{1,0} maximum(f32[2048,2048]{1,0} %add.0, f32[2048,2048]{1,0} %broadcast.2), metadata={op_type=""Relu"" op_name=""Relu""}
#   %param_0.2 = f32[] parameter(0)
#   %broadcast.1 = f32[2048,2048]{1,0} broadcast(f32[] %param_0.2), dimensions={}, metadata={op_type=""RealDiv"" op_name=""truediv"" source_file=""int8_xla.py"" source_line=51}
#   %divide.0 = f32[2048,2048]{1,0} divide(f32[2048,2048]{1,0} %maximum.0, f32[2048,2048]{1,0} %broadcast.1), metadata={op_type=""RealDiv"" op_name=""truediv"" source_file=""int8_xla.py"" source_line=51}
#   %round-nearest-even.0 = f32[2048,2048]{1,0} round-nearest-even(f32[2048,2048]{1,0} %divide.0), metadata={op_type=""Round"" op_name=""Round"" source_file=""int8_xla.py"" source_line=51}
#   %constant_0 = f32[] constant(127), metadata={op_type=""Minimum"" op_name=""clip_by_value/Minimum"" source_file=""int8_xla.py"" source_line=52}
#   %broadcast.0 = f32[2048,2048]{1,0} broadcast(f32[] %constant_0), dimensions={}, metadata={op_type=""Minimum"" op_name=""clip_by_value/Minimum"" source_file=""int8_xla.py"" source_line=52}
#   %clamp.1 = f32[2048,2048]{1,0} clamp(f32[2048,2048]{1,0} %broadcast.4, f32[2048,2048]{1,0} %round-nearest-even.0, f32[2048,2048]{1,0} %broadcast.0), metadata={op_type=""Maximum"" op_name=""clip_by_value"" source_file=""int8_xla.py"" source_line=52}
#   ROOT %convert.0 = s8[2048,2048]{1,0} convert(f32[2048,2048]{1,0} %clamp.1), metadata={op_type=""Cast"" op_name=""Cast_1"" source_file=""int8_xla.py"" source_line=53}
# }

# ENTRY %a_inference_int8_linear_layer_65__.32 (arg0.1: s8[2048,2048], arg1.2: s8[2048,2048], arg2.3: f32[2048], arg3.4: f32[]) -> s8[2048,2048] {
#   %arg3.4 = f32[] parameter(3), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %arg2.3 = f32[2048]{0} parameter(2), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %arg0.1 = s8[2048,2048]{1,0} parameter(0), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %arg1.2 = s8[2048,2048]{1,0} parameter(1), parameter_replication={false}, metadata={op_name=""XLA_Args""}
#   %copy = s8[2048,2048]{0,1} copy(s8[2048,2048]{1,0} %arg1.2), metadata={op_name=""XLA_Args""}
#   %cublas-gemm.1 = s32[2048,2048]{1,0} custom-call(s8[2048,2048]{1,0} %arg0.1, s8[2048,2048]{0,1} %copy), custom_call_target=""__cublas$gemm"", metadata={op_type=""BatchMatMulV3"" op_name=""MatMul"" source_file=""int8_xla.py"" source_line=43}, backend_config=""{\""alpha_real\"":1,\""alpha_imag\"":0,\""beta\"":0,\""dot_dimension_numbers\"":{\""lhs_contracting_dimensions\"":[\""1\""],\""rhs_contracting_dimensions\"":[\""0\""],\""lhs_batch_dimensions\"":[],\""rhs_batch_dimensions\"":[]},\""precision_config\"":{\""operand_precision\"":[\""DEFAULT\"",\""DEFAULT\""]},\""epilogue\"":\""DEFAULT\""}""
#   ROOT %fusion = s8[2048,2048]{1,0} fusion(f32[] %arg3.4, f32[2048]{0} %arg2.3, s32[2048,2048]{1,0} %cublas-gemm.1), kind=kLoop, calls=%fused_computation, metadata={op_type=""Cast"" op_name=""Cast_1"" source_file=""int8_xla.py"" source_line=53}
# }
```

System info: Ubuntu 20.04.5 LTS, TF 2.11.0 via pip, A100 GPU, CUDA Version 12.0","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'comp:xla', 'type:performance']",2023-02-02T19:47:49Z,9,0,https://github.com/tensorflow/tensorflow/issues/59530,Logical Bug
415,tensorflow/tensorflow,"ConvertFusedBatchNorm returns uninitialized value when data_format = ""NDHWC""","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Have you reproduced the bug with TF nightly?

Yes

### Source

source

### Tensorflow Version

v1.12.1-88697-g620bee79ab3 2.12.0-dev20230201

### Custom Code

No

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

Python 3.10

### Bazel version

5.3.0

### GCC/Compiler version

gcc-11

### CUDA/cuDNN version

CUDA-11.8/cudnn-8.7.0/TensorRT-8.5.3

### GPU model and memory

RTX3090

### Current Behaviour?

```shell
See code snippet:
https://github.com/tensorflow/tensorflow/blob/4aec415b3f06b19c380d1a0ca92cc2de0d74cc21/tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc#L4399-L4436

In the case of NDHWC layout (triggered by the code below) an uninitialized value is returned from ConvertFusedBatchNorm which causes an exception to be raised.

I would expect it to build correctly. Changing ConvertFusedBatchNorm to do the same thing for NDHWC as for NHWC gets rid of the crash, but I don't know if this is correct.
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import (
    BatchNormalization,
    Conv3D,
    Dense,
    Flatten,
    Input,
)
from tensorflow.keras.models import Model
from tensorflow.python.compiler.tensorrt import trt_convert as trt

inputs = Input(shape=(24, 24, 64, 1), name=""x"")
x = inputs
x = Conv3D(16, (3, 3, 3), activation=""relu"", padding=""same"")(x)
x = BatchNormalization()(x)
x = Flatten()(x)
x = Dense(128, activation=""relu"")(x)
x = Dense(128)(x)
m = Model(inputs=[inputs], outputs=[x])

m.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)

model_dir = ""/tmp/model""
tf.keras.models.save_model(m, model_dir)

converter = trt.TrtGraphConverterV2(input_saved_model_dir=model_dir,
                        precision_mode=trt.TrtPrecisionMode.FP16)

trt_func = converter.convert()

def input_fn():
    a = np.random.rand(1024, 24, 24, 64, 1).astype(np.float32)
    yield [a]

converter.build(input_fn=input_fn)
```


### Relevant log output

```shell
2023-02-02 11:32:14.336729: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1104] TF-TRT Warning: Engine creation for TRTEngineOp_000_000 failed. The native segment will be used instead. Reason: INVALID_ARGUMENT: Rank of perm for transpose does not match with that of the input.
```
</details>","['stat:awaiting response', 'type:bug', 'subtype: ubuntu/linux', 'comp:gpu:tensorrt', 'TF 2.11']",2023-02-02T11:20:23Z,6,0,https://github.com/tensorflow/tensorflow/issues/59525,Runtime Error
416,tensorflow/tensorflow,Add warning in `Dataset.shuffle` for silent data leakage," ### Issue Type

Documentation Feature Request

Add a pitfall warning in the docs of ""tf.data.Dataset"", where the `shuffle` method using together with `reshuffle_each_iteration=True` (which is **True by default**) might lead to validation data leakage.

### Tensorflow Version

2.9, 2.10, 2.11

### Issue description

Using shuffle method with `reshuffle_each_iteration=True`, followed by take/skip methods to generate train/test/validation sets could lead to validation/test data leaking into the training set (as **the full dataset is shuffled after each epoch**, which undergoes yet another split and therefore the perimeter between training and validation sets is broken).

### Standalone code to reproduce the issue

```python
dataset = tf.data.Dataset.from_tensor_slices((features, labels))
dataset = dataset.shuffle(buffer_size=BUFFER_SIZE)             # ""reshuffle_each_iteration"" is True by default
dataset = dataset.batch(BATCH_SIZE)

train_dataset = dataset.take(TRAIN_BATCH_SIZE)
val_dataset = dataset.skip(TRAIN_BATCH_SIZE)

model.fit(train_dataset, validation_data=val_dataset, batch_size=BATCH_SIZE, epochs=EPOCHS)
```

### Pitfall analysis

This data leakage is **dangerously silent** for the following reason:

In the [Dataset doc](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) under the `shuffle` method, it reads:

> shuffle(
>     buffer_size, seed=None, reshuffle_each_iteration=None, name=None
> )

where the `reshuffle_each_iteration=None` is **super misleading**, as it be easily misinterpreted that reshuffled is off by default (**which is not true at all**).

The shuffle method, which called `shuffle_op._shuffle`:
https://github.com/tensorflow/tensorflow/blob/4dacf3f368eb7965e9b5c3bbdd5193986081c3b2/tensorflow/python/data/ops/dataset_ops.py#L1472-L1473
  
which then called `_ShuffleDataset`:
  https://github.com/tensorflow/tensorflow/blob/b756c44e3f3ed52ccb4f05736569b95f4481eea0/tensorflow/python/data/ops/shuffle_op.py#L25-L32
  
which finally init the class `_ShuffleDataset`:
https://github.com/tensorflow/tensorflow/blob/b756c44e3f3ed52ccb4f05736569b95f4481eea0/tensorflow/python/data/ops/shuffle_op.py#L35-L50

has the following dangerous definition:
```
if reshuffle_each_iteration is None:
      reshuffle_each_iteration = True
```

As a result, the default `reshuffle_each_iteration is None` would be interpreted to `reshuffle_each_iteration = True` (which is truly unexpected).
","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.11']",2023-01-17T10:29:54Z,14,3,https://github.com/tensorflow/tensorflow/issues/59279,Performance Issue
417,tensorflow/tensorflow,Deleting legacy Java client from TensorFlow main repository,"TensorFlow main repository still contains the old [Java client](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java) based on TF1.x that has been replaced a few years ago by the [new version](https://github.com/tensorflow/java) maintained by SIG-JVM.

This is very misleading for users who wants to discover the capabilities of running TensorFlow models on Java (just this week a new example of such [question](https://discuss.tensorflow.org/t/what-does-it-mean-for-the-java-api-that-warning-this-api-is-deprecated-and-will-be-removed-in-a-future-version-of-tensorflow-after-the-replacement-is-stable/12757) appeared on the forum).

This issue is to start the process of deleting this client for good in TF main repo. We could start by replacing this [README](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/README.md) for simply saying that this client is deprecated and provide links to the new repo. Then we can proceed to the folder deletion, making sure it won't break any code, CI jobs or external scripts (like the documentation one).

If need be, we at SIG-JVM can take care of pushing a series of pull requests to achieve this goal. 

CC\ @bhack , @craigacp","['stat:awaiting tensorflower', 'type:bug', 'comp:apis']",2022-11-03T01:16:18Z,16,0,https://github.com/tensorflow/tensorflow/issues/58424,Runtime Error
418,tensorflow/tensorflow,GPU memory usage depends on data size,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.6-2.10

### Custom Code

Yes

### OS Platform and Distribution

Linux Fedora 36

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.7

### GPU model and memory

RTX 2070, 8GiB

### Current Behaviour?


Training a small model on small batches runs out of GPU memory if the total amount of data is large.

The same code works fine on TF 2.5, but fails on 2.6, 2.7, 2.8, and 2.9.



### Standalone code to reproduce the issue


Here is a handy script: https://gist.github.com/Dapid/3f58697edf91c6610ed5e7b681db440f

In short, a tiny model is trained on a fixed array of data. If the total amount of data is small, it runs; otherwise, it crashes.

Running `python benchmark.py` works, but `python benchmark.py --big` doesn't. Using the `tf.data` API doesn't make a difference.



### Relevant log output

```shell
-09-06 10:52:22.131172: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 15.26GiB (rounded to 16384000000)requested by op _EagerConst
```

Defining `TF_GPU_ALLOCATOR=cuda_malloc_async`:

```shell
2022-09-06 10:58:16.939018: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000000 exceeds 10% of free system memory.
2022-09-06 10:58:23.313713: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 16384000000 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 1111293952/8369799168
2022-09-06 10:58:23.313737: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                      6184304640
InUse:                        67126312
MaxInUse:                    201327628
NumAllocs:                          13
MaxAllocSize:                 67108864
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-09-06 10:58:23.313745: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-09-06 10:58:23.313749: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 5
2022-09-06 10:58:23.313753: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 2
2022-09-06 10:58:23.313755: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-09-06 10:58:23.313758: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16384, 1
2022-09-06 10:58:23.313761: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 67108864, 1
```

</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.9']",2022-09-06T09:00:12Z,9,0,https://github.com/tensorflow/tensorflow/issues/57623,Runtime Error
419,tensorflow/tensorflow,Convert FloorModOp to tosa,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

5.1

### GCC/Compiler version

9

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
from source code: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/tosa/transforms/legalize_common.cc#L2457
floormod(x, y) = x / y - floor(x / y)

from doc:https://tensorflow.google.cn/api_docs/python/tf/raw_ops/FloorMod?version=nightly
floormod(x, y) = x - floor(x / y) * y

I wonder if I misunderstood
```


### Standalone code to reproduce the issue

```shell
tosa ir I get:
'
module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 561 : i32}, tf_saved_model.semantics} {
  func.func @serving_default(%arg0: tensor<11x10xf32> {tf_saved_model.index_path = [""model_input1""]}, %arg1: tensor<f32> {tf_saved_model.index_path = [""model_input2""]}) -> (tensor<11x10xf32> {tf_saved_model.index_path = [""model_output""]}) attributes {tf.entry_function = {control_outputs = """", inputs = ""model_input1:0,model_input2:0"", outputs = ""FloorMod:0""}, tf_saved_model.exported_names = [""serving_default""]} {
    %0 = ""tosa.reciprocal""(%arg1) : (tensor<f32>) -> tensor<f32>
    %1 = ""tosa.reshape""(%0) {new_shape = [1, 1]} : (tensor<f32>) -> tensor<1x1xf32>
    %2 = ""tosa.mul""(%arg0, %1) {shift = 0 : i32} : (tensor<11x10xf32>, tensor<1x1xf32>) -> tensor<11x10xf32>
    %3 = ""tosa.floor""(%2) : (tensor<11x10xf32>) -> tensor<11x10xf32>
    %4 = ""tosa.sub""(%2, %3) : (tensor<11x10xf32>, tensor<11x10xf32>) -> tensor<11x10xf32>
    return %4 : tensor<11x10xf32>
  }
}
'
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'awaiting PR merge', 'TF 2.9', 'comp:lite-tosa']",2022-08-30T04:40:53Z,11,1,https://github.com/tensorflow/tensorflow/issues/57520,Runtime Error
420,tensorflow/tensorflow,[BUG] Gradient tape *inside* tf.function broken for tf.Variable argument.," 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.7, 2.9, nightly

### Custom Code

Yes

### OS Platform and Distribution

Ubuntu 22.04

### Mobile device

_No response_

### Python version

3.1)

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
tf.function (since TF 2.7 I believe) now remembers the storage of the variable and treats them as tensors (using the `__tf_retrace__` mechanism). This works fine whenever the value of the variable is needed and not it's identity. However, when taking the gradient, the identity matters, not the value.

This means when decorating a function that takes the gradient of y with respect to x (x being a parameter here), it traces the function, uses the value of the TensorLike object and **creates the gradient with respect to the parameter's identity (watching the parameter)**.
Calling the same function with a different parameter won't retrace (!) and just use the value, however since it doesn't retrace, it won't re-calculate the gradient with respect to the *identity* of the new variable given as argument but instead use the remembered gradient.

Basically, the problem arises since tf.function and tensor-like objects function functionally (i.e. gradient with respect to argument one, two etc) and tf.Variable, that are global statebased objects. Using a statebased object as a ""functional"" object breaks the correct behavior of GradientTape.

## Bug or feature?

I do understand the merits of this in a function that doesn't take gradients. However, it breaks for the logic of the gradient.

## Possible fixes

There are workarounds, i.e. to always give parameters in a dict with their name, the names triggering a retrace.
```


### Standalone code to reproduce the issue

```shell
Link to colab with the code below: https://colab.research.google.com/drive/1z26ZIoiFEBO9icGJ5bYusu4XY5u-vpkG?usp=sharing


import tensorflow as tf

x1 = tf.Variable(2.0)
x2 = tf.Variable(4.0)


def f():
    res = x1 + x2 ** 2 / 2
    return res


def grad(param):
    with tf.GradientTape(watch_accessed_variables=False) as tape:
        tape.watch(param)
        value = f()
    return tape.gradient(value, param)

jitted_grad = tf.function(grad)

y1 = grad(x1)
y1_jit = jitted_grad(x1)
assert abs(y1 - 1.0) < 1e-5  # because d x1 / dx1 = 1
assert abs(y1_jit - 1.0) < 1e-5
y2 = grad(x2)
y2_jit = jitted_grad(x2)
print(f""y2: {y2}, should be 4"")  # but is 1 because it uses the derivative of x1
print(f""y2_jit: {y2_jit}, should also be 4"")  # but is 1 because it uses the derivative of x1
assert abs(y2 - 4.0) < 1e-5  # because d / dx x**2/2 = x -> 4
assert abs(y2_jit - 4.0) < 1e-5  # fails!
```
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.9']",2022-08-22T13:35:38Z,4,0,https://github.com/tensorflow/tensorflow/issues/57365,Logical Bug
421,tensorflow/tensorflow,AutoGraph cannot handle python 3.10's structural pattern matching,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.2

### Custom Code

No

### OS Platform and Distribution

macOS 12.5

### Mobile device

none

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

none

### GPU model and memory

Apple's METAL (though unlikely to be relevant here)

### Current Behaviour?

Trying to AutoGraph a function containing a [PEP 634/635/636 structural pattern matching statement](https://peps.python.org/pep-0634/) from python 3.10 (i.e. `match/case`) results in the `WARNING:tensorflow:AutoGraph could not transform <function test at 0x2c4c99120> and will run it as-is.`

### Standalone code to reproduce the issue

```python
import tensorflow as tf

@tf.function
def test(x, selector):
    match selector:
        case ""square"":
            return x**2
        case ""double"":
            return x*2.
        case _:
            raise ValueError
            
test(tf.linspace(-1,1,10),""double"")
```


### Relevant log output

```shell
INFO:tensorflow:Error transforming entity <function test at 0x2c4c99120>
Traceback (most recent call last):
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py"", line 427, in converted_call
    converted_f = _convert_actual(target_entity, program_ctx)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py"", line 269, in _convert_actual
    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 282, in transform
    return self.transform_function(obj, user_context)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 466, in transform_function
    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 359, in transform_function
    result = self.transform_ast(node, context)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py"", line 237, in transform_ast
    node = self.initial_analysis(node, ctx)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py"", line 223, in initial_analysis
    graphs = cfg.build(node)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/cfg.py"", line 970, in build
    visitor.visit(node)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ast.py"", line 410, in visit
    return visitor(node)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/cfg.py"", line 766, in visit_FunctionDef
    self._process_function_def(node, is_lambda=False)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/cfg.py"", line 757, in _process_function_def
    self.visit(stmt)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ast.py"", line 410, in visit
    return visitor(node)
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ast.py"", line 414, in generic_visit
    for field, value in iter_fields(node):
  File ""/Users/yves/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ast.py"", line 252, in iter_fields
    for field in node._fields:
AttributeError: 'NoneType' object has no attribute '_fields'
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:autograph', 'TF 2.9']",2022-08-16T06:28:12Z,6,2,https://github.com/tensorflow/tensorflow/issues/57166,Runtime Error
422,tensorflow/tensorflow, Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

No

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
When running `model.predict` I am getting the warning. What does that mean and how can I fix it ?
```


### Standalone code to reproduce the issue

```shell
The model is a keras.functional.Functional model where the first layers are from tensorflow hub pretrained models. I have two dense layers in the end. compiling, fitting do not give any warning however when running `model.predict([""examplestring""])` I get the warning.
```


### Relevant log output

```shell
2022-08-09 15:37:17.301561: W tensorflow/core/common_runtime/forward_type_inference.cc:231] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected a subtype of type_id: TFT_TENSOR
args {
  type_id: TFT_LEGACY_VARIANT
}
 for input 2 of a homogeneous container 1001, got type_id: TFT_RAGGED
args {
  type_id: TFT_INT32
}

        while inferring type of node 'model/preprocessing/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/bert_pack_inputs/PartitionedCall/map/while/body/_337/map/while/TensorArrayV2Write/TensorListSetItem'
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF 2.9']",2022-08-09T11:41:24Z,30,0,https://github.com/tensorflow/tensorflow/issues/57052,Runtime Error
423,tensorflow/tensorflow,AttributeError: '_UserObject' object has no attribute 'add_slot',"Issue Type: Bug
Source: binary
Tensorflow Version: 2.6
Custom Code: Yes


# Current Behaviour?

I have model, saved using `SavedModel` format. The model is trained on Kaggle TPU and saved in the following ways:

```python
# in kaggle tpu, it must be saved in the following way!
save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')
final_model.save('./final_model', options=save_locally)
 ```

Now, it seems like, if I try to load this model in the following way, 

```python
model = tf.saved_model.load(""final_model"")

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/tmp/ipykernel_17/3033429325.py in <module>
----> 1 model = tf.saved_model.load(""final_model"")
...
...
AttributeError: '_UserObject' object has no attribute 'add_slot'
```

But if I do as follows, it works.

```
model = tf.keras.models.load_model(""final_model"")
```

But for some reason, I need to make `tf.saved_model.load` API work instead `tf.keras.models.load_model` API. What approach should we take here?
","['stat:awaiting tensorflower', 'type:bug', 'comp:dist-strat', '2.6.0']",2022-08-03T08:44:46Z,16,3,https://github.com/tensorflow/tensorflow/issues/56997,Runtime Error
424,tensorflow/tensorflow,Differentiate a tuple with `tape.jacobian`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

Yes

### OS Platform and Distribution

MacOs

### Mobile device

-

### Python version

3.9

### Bazel version

-

### GCC/Compiler version

-

### CUDA/cuDNN version

-

### GPU model and memory

-

### Current Behaviour?

```shell
I expect to able to differentiate a function returning a tuple of TensorFlow tensors, but unfortunately it is not possible because tuples have no shape. Could you provide me a work around for this? Maybe something with ragged tensors is possible here?
```


### Standalone code to reproduce the issue

```shell
import tensorflow as tf

def circuit(params):
    return params[0] ** 2, params[1::] ** 3

x = tf.Variable([0.1, 0.2, 0.3])

with tf.GradientTape() as tape:
    out = circuit(x)

jac = tape.jacobian(out, x)
```


### Relevant log output

```shell
Traceback (most recent call last):
  File ""/Users/.../test.py"", line 14, in <module>
    jac = tape.jacobian(out, x)
  File ""/Users/.../lib/python3.9/site-packages/tensorflow/python/eager/backprop.py"", line 1183, in jacobian
    target_static_shape = target.shape
AttributeError: 'tuple' object has no attribute 'shape'
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.9']",2022-07-26T18:03:56Z,3,0,https://github.com/tensorflow/tensorflow/issues/56912,Runtime Error
425,tensorflow/tensorflow,"`Error in PredictCost() for the op: op: ""CropAndResize""` when using the `tf.image.crop_and_resize` op","<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9.1

### Custom Code

Yes

### OS Platform and Distribution

RedHat Linux Enterprise 8.4

### Mobile device

_No response_

### Python version

3.9.7

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA: 11.6

### GPU model and memory

V100, 32GB

### Current Behaviour?


I wanted to code the equivalent of [RandomResizedCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html) from torchvision.
I used the model from [this official keras tutorial](https://keras.io/examples/vision/nnclr/#random-resized-crops) and integrated it in my data pipeline.
When using it, I got the following warning:

```
2022-07-26 14:10:05.665573: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: 40 } dim { size: 40 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 16 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 16 } dim { size: 16 } dim { size: 3 } } }
2022-07-26 14:10:05.815003: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
```

At first, I didn't care because my code was running fine, but I had a CPU memory error after about 9 epochs of ImageNet training. This suggests that there is somehow a memory leak during training.

The same behaviour (although on GPU from that I understand) was also observed in this [SO question](https://stackoverflow.com/q/72642906/4332585).



### Standalone code to reproduce the issue


Unfortunately, the warning does not appear on Colab, but here is a link with the appropriate minimal example anyway: https://colab.research.google.com/drive/1QHa4kxPLfCjkfDvmwv9wj5yq19za5SpA?usp=sharing

However, locally (on my laptop without GPU) and on my server, the warning is thrown.

The full code is the following:

```python

import tensorflow as tf

class RandomResizedCrop(tf.keras.layers.Layer):
    # taken from
    # https://keras.io/examples/vision/nnclr/#random-resized-crops
    def __init__(self, scale, ratio, crop_shape):
        super(RandomResizedCrop, self).__init__()
        self.scale = scale
        self.log_ratio = (tf.math.log(ratio[0]), tf.math.log(ratio[1]))
        self.crop_shape = crop_shape

    def call(self, images):
        batch_size = tf.shape(images)[0]

        random_scales = tf.random.uniform(
            (batch_size,),
            self.scale[0],
            self.scale[1]
        )
        random_ratios = tf.exp(tf.random.uniform(
            (batch_size,),
            self.log_ratio[0],
            self.log_ratio[1]
        ))

        new_heights = tf.clip_by_value(
            tf.sqrt(random_scales / random_ratios),
            0,
            1,
        )
        new_widths = tf.clip_by_value(
            tf.sqrt(random_scales * random_ratios),
            0,
            1,
        )
        height_offsets = tf.random.uniform(
            (batch_size,),
            0,
            1 - new_heights,
        )
        width_offsets = tf.random.uniform(
            (batch_size,),
            0,
            1 - new_widths,
        )

        bounding_boxes = tf.stack(
            [
                height_offsets,
                width_offsets,
                height_offsets + new_heights,
                width_offsets + new_widths,
            ],
            axis=1,
        )
        images = tf.image.crop_and_resize(
            images,
            bounding_boxes,
            tf.range(batch_size),
            self.crop_shape,
        )
        return images

import tensorflow_datasets as tfds


ds = tfds.load('cifar10', split='train', as_supervised=True)
image_width = 16
crop = RandomResizedCrop(
    scale=(0.08, 1.0),
    ratio=(0.75, 1.33),
    crop_shape=(image_width, image_width),
)
data_aug_list = [
    tf.keras.layers.ZeroPadding2D(padding=4),
    crop,
]
data_aug_layer = tf.keras.models.Sequential(data_aug_list)
ds = ds.map(
  lambda x, y: (data_aug_layer(x[None], training=True)[0], y),
  num_parallel_calls=tf.data.experimental.AUTOTUNE,
)
ds = ds.shuffle(
  buffer_size=1000,  # For now a hardcoded value
  reshuffle_each_iteration=True,
).batch(
  32,
  num_parallel_calls=tf.data.experimental.AUTOTUNE,
)
ds = ds.prefetch(
  buffer_size=tf.data.experimental.AUTOTUNE,
)

res = next(iter(ds))  # warning is thrown here
```


### Relevant log output

```shell
2022-07-25 15:28:12.176012: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 16:47:53.335022: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 18:07:19.352253: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 19:26:43.163855: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 20:46:07.180534: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 22:05:27.767552: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-25 23:24:49.840029: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 00:44:13.601504: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 02:03:37.488186: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 03:22:57.948498: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
2022-07-26 04:42:09.083767: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:690] Error in PredictCost() for the op: op: ""CropAndResize"" attr { key: ""T"" value { type: DT_UINT8 } } attr { key: ""extrapolation_value"" value { f: 0 } } attr { key: ""method"" value { s: ""bilinear"" } } inputs { dtype: DT_UINT8 shape { dim { size: 1 } dim { size: -11 } dim { size: -12 } dim { size: 3 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } value { dtype: DT_INT32 tensor_shape { dim { size: 2 } } int_val: 224 } } device { type: ""CPU"" vendor: ""GenuineIntel"" model: ""101"" frequency: 2500 num_cores: 10 environment { key: ""cpu_instruction_set"" value: ""AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2"" } environment { key: ""eigen"" value: ""3.4.90"" } l1_cache_size: 32768 l2_cache_size: 1048576 l3_cache_size: 28835840 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 224 } dim { size: 224 } dim { size: 3 } } }
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=1562147.0. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: r10i0n2: task 0: Out Of Memory
srun: launch/slurm: _step_signal: Terminating StepId=1562147.0
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=1562147.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.9']",2022-07-26T12:43:07Z,10,0,https://github.com/tensorflow/tensorflow/issues/56904,Performance Issue
426,tensorflow/tensorflow,Nondeterministic result on TPU,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

2.9

### Custom Code

Yes

### OS Platform and Distribution

Linux Ubuntu 20.04

### Mobile device

_No response_

### Python version

3.9

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

TPU v3-8

### Current Behaviour?

```shell
Use ""TF_DETERMINISTIC_OPS = 1"" or ""tf.config.experimental.enable_op_determinism()"" can get determistic result on GPU.

But the results are nondetermistic on cloud TPU.
```


### Standalone code to reproduce the issue

```shell
https://github.com/edwardyehuang/CAR

The code on repo above can get determistic result on GPU, but the result is nondetermistic on TPU.
```


### Relevant log output

```shell
TPU 1st RUN (1000 steps):

1000/1000 [==============================] - 506s 395ms/step - loss: 1.6268 - IOU: 0.3178 - g_1_orl: 0.6300 - g_1_sal: 0.0168 - val_loss: 1.3395 - val_IOU: 0.2370

TPU 2nd RUN (1000 steps):

1000/1000 [==============================] - 645s 448ms/step - loss: 1.6488 - IOU: 0.3095 - g_1_orl: 0.6314 - g_1_sal: 0.0162 - val_loss: 1.7416 - val_IOU: 0.1793
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:tpus', 'TF 2.9']",2022-07-21T08:52:46Z,20,1,https://github.com/tensorflow/tensorflow/issues/56847,Logical Bug
427,tensorflow/tensorflow,Wrong is_gpu flag computation in graph_memory.cc,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

source

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

I work for Sonar and as part of our internal tests, we run our static analyzers on multiple open source codes. While reviewing our tests we saw the following code that is almost certainly not intended:
On the following line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/costs/graph_memory.cc#L175
the method `std::string::find` will return `std::string::npos` if the searched substring is not present. Because `std::string::npos` is different from 0, casting `std::string::find` as a boolean does not return what one would expect and there is no way the test on this line returns anything other than true, whether there is a GPU or not.


### Standalone code to reproduce the issue

I don't know how this bug impacts the functional behavior of the tool. It might be completely innocuous, given its age.
I am also unsure whether it should be fixed by removing the flag altogether to keep the current years-old behavior or fixed to actually handle non-GPU devices.


### Relevant log output

_No response_</details>","['type:bug', 'comp:core', 'TF 2.9']",2022-06-16T14:45:48Z,2,0,https://github.com/tensorflow/tensorflow/issues/56481,Logical Bug
428,tensorflow/tensorflow,Use of Keras `jit_compile` in a distribution strategy causes a `std::system_error`,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.9.1

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

3.10

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

11.2/8.1.1.33

### GPU model and memory

_No response_

### Current Behaviour?


The following error is thrown during training after a number of steps / epochs 
```shell
terminate called after throwing an instance of 'std::system_error'
what():  Resource temporarily unavailable
```
I am able to reproduce this error in colab with my sample code



### Standalone code to reproduce the issue

```python
import keras
import tensorflow as tf

def build_model_() -> keras.Model:
    input = tf.keras.layers.Input(shape=(5,), name='input_a')
    x = tf.keras.layers.Dense(512, activation = 'relu')(input)
    x = tf.keras.layers.Dense(512, activation = 'relu')(x)
    output = tf.keras.layers.Dense(1, name='output')(x)
    model = tf.keras.models.Model(inputs=input, outputs=output)
    return model


strategy = tf.distribute.MirroredStrategy()
print(f""Can see {strategy.num_replicas_in_sync} gpus"")
with strategy.scope():
  model = build_model_()
  model.compile(loss = 'mse', jit_compile=True)

BATCH_SIZE_PER_REPLICA = 1024
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA

dataset = tf.data.Dataset.from_tensors(
    (tf.ones(5), 1)
).repeat(10_000_000).batch(GLOBAL_BATCH_SIZE).with_options(options)

history = model.fit(
    x = dataset,
    epochs=7,
    verbose = 1,
)
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:dist-strat', 'TF 2.9']",2022-06-10T18:14:23Z,17,0,https://github.com/tensorflow/tensorflow/issues/56423,Runtime Error
429,tensorflow/tensorflow,tf.data batching slows down on Windows,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

Yes

### OS Platform and Distribution

Windows 10

### Mobile device

_No response_

### Python version

3.8

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

CUDA 11.2 cuDNN 8.1.0

### GPU model and memory

Nvidia A40 48GB, 128GB System RAM

### Current Behaviour?


While training a keras model using a [custom tensorflow dataset](https://www.tensorflow.org/datasets/add_dataset), the training steps slow down once the remaining sample count in the first epoch is smaller than the shuffle buffer size. The slowdown carries over to the following epochs and takes longer and longer.

I would expect the opposite behavior since all remaining samples should already be in the buffer and do not need to be loaded anymore. I have tested it on multiple Windows systems and this slowdown occurred on all of them. However, it does not happen on Linux-based systems.

I then used the tensorboard profiling plugin to investigate what is causing the slowdown. As you can see here, it seems to be an input-related problem:
![profiler_1](https://user-images.githubusercontent.com/1509163/166112234-b795f11e-934f-4631-86b1-c8d79e1e1448.png)

From the input operations you can see that `Iterator::Root::Prefetch::BatchV2` takes the most time (this is also the case when removing the prefetching):
![profiler_2](https://user-images.githubusercontent.com/1509163/166112243-798a33a9-b329-4103-9d24-b0b70306411d.png)

The slowdown can also be seen in the trace viewer:
![profiler_3](https://user-images.githubusercontent.com/1509163/166112328-9e637031-1166-4ac6-afef-47a8a534bf9b.png)

Here is a more detailed comparison of `BatchV2` durations from different steps:
![profiler_4](https://user-images.githubusercontent.com/1509163/166112335-119bb08e-7cd2-4a4b-a149-0de1580332a4.png)
![profiler_5](https://user-images.githubusercontent.com/1509163/166112342-dd0ea8a4-9d79-4ec3-adc9-87ac133e118e.png)


Here is the [profiling_data.zip](https://github.com/tensorflow/tensorflow/files/8597248/profiling_data.zip).



### Standalone code to reproduce the issue

In order to reproduce and profile this issue the following code can be used. When running the `main.py` for the first time it will generate the dummy dataset which takes around 20 minutes to write 24GB of dummy data.

**dummy_dataset.py**
```python
import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds


class DummyDataset(tfds.core.GeneratorBasedBuilder):
    VERSION = tfds.core.Version('1.0.0')
    RELEASE_NOTES = {
        '1.0.0': 'Initial release.',
    }

    def _info(self) -> tfds.core.DatasetInfo:
        return tfds.core.DatasetInfo(
            builder=self,
            features=tfds.features.FeaturesDict({
                'audio': tfds.features.Audio(shape=(16000,), dtype=tf.float32),
                'label': tfds.features.Tensor(shape=(2,), dtype=tf.float32),
            }),
            supervised_keys=('audio', 'label')
        )

    def _split_generators(self, dl_manager: tfds.download.DownloadManager):
        return {
            'train': self._generate_examples(),
        }

    def _generate_examples(self):
        for i in range(400000): # If sample count is increased, issue starts later (tested with 1M+)
            yield i, {
                'audio': np.random.sample(16000).astype(dtype=np.float32),
                'label': tf.one_hot(np.random.choice(2), depth=2).numpy()
            }
```



**main.py**
```python
from dummy_dataset import DummyDataset
import tensorflow_datasets as tfds
import tensorflow as tf
import os
import datetime
from tensorflow import keras

# Configuration
shuffle_buffer = 300000 # Uses ~30GB RAM, can be lowered to 150000 if only 16GB RAM available
batch_size = 512
epochs = 3

optimizer = keras.optimizers.Adam(learning_rate=0.001)
losses = ['categorical_crossentropy']
metrics = [keras.metrics.CategoricalAccuracy()]

folder_name = datetime.datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")

# Prepare dataset (takes ~20min for the first time to generate ~24GB of dummy data)
ds_train, ds_info = tfds.load(
    name='DummyDataset',
    split='train',
    as_supervised=True,
    with_info=True,
    shuffle_files=True,
)
ds_train = ds_train.shuffle(shuffle_buffer)
ds_train = ds_train.batch(batch_size)
ds_train = ds_train.prefetch(tf.data.AUTOTUNE)

# Configure profiling
sample_count = ds_info.splits['train'].num_examples
slowdown_start = int((sample_count - shuffle_buffer) / batch_size)
profile_stop = slowdown_start + 150

callbacks = [keras.callbacks.TensorBoard(
    log_dir=os.path.join(os.getcwd(), 'tensorboard', folder_name),
    profile_batch=[slowdown_start, profile_stop]
)]

# Build and train model
input_layer = keras.layers.Input(shape=(16000,), dtype=tf.float32, name='audio_input')
output_layer = keras.layers.Dense(2, activation='softmax', name='prediction')(input_layer)
model = keras.Model(inputs=input_layer, outputs=output_layer, name='dummy_model')

model.compile(optimizer=optimizer, loss=losses, metrics=metrics)
model.summary()

print(f'Expected slowdown to start at batch {slowdown_start}, profiling batches {slowdown_start}-{profile_stop}')

model.fit(x=ds_train, epochs=epochs, verbose=1, callbacks=callbacks)
```

**requirements.txt**
```
tensorflow==2.8.0
tensorflow-datasets==4.5.2
tensorboard-plugin-profile
numpy
```


### Relevant log output

_No response_</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.8']",2022-04-30T15:34:42Z,5,0,https://github.com/tensorflow/tensorflow/issues/55815,Performance Issue
430,tensorflow/tensorflow,tf.make_tensor_proto() does not respect byte order of numpy input array,"<details><summary>Click to expand!</summary> 
 
 ### Issue Type

Bug

### Source

binary

### Tensorflow Version

tf 2.8

### Custom Code

No

### OS Platform and Distribution

_No response_

### Mobile device

_No response_

### Python version

_No response_

### Bazel version

_No response_

### GCC/Compiler version

_No response_

### CUDA/cuDNN version

_No response_

### GPU model and memory

_No response_

### Current Behaviour?

```shell
Byte order is not checked when creating a TensorProto with tf.make_tensor_proto() from a numpy.ndarray with non-native byte order. This leads to wrong tensor data.

I would expect that byte order is checked and swapped if necessary before assigning to tensor_proto.tensor_content in [python/framework/tensor_util.py](https://github.com/tensorflow/tensorflow/blob/v2.8.0/tensorflow/python/framework/tensor_util.py#L523)
```


### Standalone code to reproduce the issue

```shell
x = np.ones(shape=(1, 2), dtype=np.float32)
x_bswap = x.astype('>f4') # assuming native byte order is little endian

y = tf.make_ndarray(tf.make_tensor_proto(x))
y_bswap = tf.make_ndarray(tf.make_tensor_proto(x_bswap))

print(y)
print(y_bswap)

assert np.array_equal(x, y)
assert np.array_equal(x, y_bswap)
```


### Relevant log output

```shell
[[1. 1.]]
[[4.6006e-41 4.6006e-41]]
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
in <cell line: 7>()
      5 print(y_bswap)
      6 assert np.array_equal(x, y)
----> 7 assert np.array_equal(x, y_bswap)

AssertionError:
```
</details>","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.8']",2022-04-28T13:00:16Z,4,0,https://github.com/tensorflow/tensorflow/issues/55789,Runtime Error
431,tensorflow/tensorflow,[XLA:GPU] Failed to determine best cudnn convolution algorithm,"I got `Failed to determine best cudnn convolution algorithm`  error when running `facebook/wav2vec2-base-960h` model using torch_xla on GPU in fp16 mode. This error only occurs when using fp16 and fp32 works fine.

Minimal HLO to reproduce:
```llvm
HloModule Test

ENTRY main {
  x = f16[44,768,1,49]{3,2,1,0} parameter(0)
  y = f16[44,768,1,50]{3,2,1,0} parameter(1)
  ROOT %convolution.10022 = f16[1,128,48,768]{3,2,1,0} convolution(f16[44,768,1,49]{3,2,1,0} %x, f16[44,768,1,50]{3,2,1,0} %y), window={size=1x50 pad=0_0x64_64}, dim_labels=fb01_io01->01bf, batch_group_count=16
}
```

The following is the full log file.
```
(base) ubuntu@xla-p3-8x:~/src/tensorflow/xla_benchmark$ ../bazel-bin/tensorflow/compiler/xla/tools/replay_computation_gpu --use_fake_data=true --num_runs=1 --print_result=false conv.hlo 
2022-04-15 00:10:23.507808: I tensorflow/compiler/xla/service/platform_util.cc:69] platform Host present but no XLA compiler available: could not find registered compiler for platform Host -- check target linkage (hint: try adding tensorflow/compiler/jit:xla_cpu_jit as a dependency)
2022-04-15 00:10:24.852458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-15 00:10:24.935392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-15 00:10:24.944338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-15 00:10:24.967499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-15 00:10:24.983119: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-15 00:10:25.002219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-15 00:10:25.025011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-15 00:10:25.050733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-15 00:10:25.052806: I tensorflow/compiler/xla/service/service.cc:174] XLA service 0x562e5c814b50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2022-04-15 00:10:25.052832: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-04-15 00:10:25.052839: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-04-15 00:10:25.052844: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-04-15 00:10:25.052849: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-04-15 00:10:25.052854: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (4): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-04-15 00:10:25.052859: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (5): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-04-15 00:10:25.052863: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (6): Tesla V100-SXM2-32GB, Compute Capability 7.0
2022-04-15 00:10:25.052868: I tensorflow/compiler/xla/service/service.cc:182]   StreamExecutor device (7): Tesla V100-SXM2-32GB, Compute Capability 7.0
conv.hlo: is not HloSnapshot. Trying HloProto.
conv.hlo: is not HloProto. Trying HLO text.
2022-04-15 00:10:25.053972: I tensorflow/compiler/xla/tools/replay_computation.cc:470] Compiling 1 modules in parallel.
2022-04-15 00:10:25.622602: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8204
2022-04-15 00:10:26.108045: I tensorflow/compiler/xla/tools/replay_computation.cc:487] Done compiling; now running the modules.
2022-04-15 00:10:26.108867: E tensorflow/compiler/xla/tools/replay_computation.cc:491] Compilation failed: UNKNOWN: Failed to determine best cudnn convolution algorithm for:
%cudnn-conv = (f16[1,128,48,768]{3,1,0,2}, u8[0]{0}) custom-call(f16[704,48,1,49]{0,3,2,1} %bitcast.2, f16[48,768,1,50]{0,3,2,1} %pad), window={size=1x50 pad=0_0x64_64}, dim_labels=fb01_io01->01bf, feature_group_count=16, custom_call_target=""__cudnn$convForward"", backend_config=""{\""conv_result_scale\"":1,\""activation_mode\"":\""0\"",\""side_input_scale\"":0}""

Original error: UNKNOWN: CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(3520): 'op' CUDNN_BACKEND_OPERATION: cudnnFinalize Failed

To ignore this failure and try to use a fallback algorithm (which may have suboptimal performance), use XLA_FLAGS=--xla_gpu_strict_conv_algorithm_picker=false.  Please also file a bug for the root cause of failing autotuning.: hlo { hlo_module { name: ""Test"" entry_computation_name: ""main"" computations { name: ""main"" instructions { name: ""x"" opcode: ""parameter"" shape { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 49 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } metadata { } frontend_attributes { } } instructions { name: ""y"" opcode: ""parameter"" shape { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 50 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } metadata { } parameter_number: 1 id: 1 frontend_attributes { } } instructions { name: ""convolution.10022"" opcode: ""convolution"" shape { element_type: F16 dimensions: 1 dimensions: 128 dimensions: 48 dimensions: 768 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } metadata { } window { dimensions { size: 1 stride: 1 window_dilation: 1 base_dilation: 1 } dimensions { size: 50 stride: 1 padding_low: 64 padding_high: 64 window_dilation: 1 base_dilation: 1 } } convolution_dimension_numbers { kernel_output_feature_dimension: 1 kernel_spatial_dimensions: 2 kernel_spatial_dimensions: 3 input_batch_dimension: 1 output_batch_dimension: 2 output_feature_dimension: 3 input_spatial_dimensions: 2 input_spatial_dimensions: 3 output_spatial_dimensions: 0 output_spatial_dimensions: 1 } id: 2 operand_ids: 0 operand_ids: 1 feature_group_count: 1 precision_config { operand_precision: DEFAULT operand_precision: DEFAULT } batch_group_count: 16 frontend_attributes { } } program_shape { parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 49 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 50 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } result { element_type: F16 dimensions: 1 dimensions: 128 dimensions: 48 dimensions: 768 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameter_names: ""x"" parameter_names: ""y"" } id: 2 root_id: 2 } host_program_shape { parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 49 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameters { element_type: F16 dimensions: 44 dimensions: 768 dimensions: 1 dimensions: 50 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } result { element_type: F16 dimensions: 1 dimensions: 128 dimensions: 48 dimensions: 768 layout { minor_to_major: 3 minor_to_major: 2 minor_to_major: 1 minor_to_major: 0 format: DENSE } is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false is_dynamic_dimension: false } parameter_names: ""p0"" parameter_names: ""p1"" } entry_computation_id: 2 input_output_alias { } dynamic_parameter_binding { } } }
```

Tested with tensorflow commit 75861c43005523e2552bb3f85b2f0defc16ea9cf, CUDA 11.4, CUDNN 8.2.","['stat:awaiting tensorflower', 'type:bug', 'comp:xla']",2022-04-15T00:15:40Z,5,0,https://github.com/tensorflow/tensorflow/issues/55633,Runtime Error
432,tensorflow/tensorflow,TFLite Converter: how to skip Dequantize node in FP16 models for GPU Delegate?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yocto dunfell 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: aarch64
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.6.1
- Python version: 3.9.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Mali G72


**Describe the current behavior**
I converted mobilenet_v2_1.0_224 [https://storage.googleapis.com/download.tensorflow.org/models/tflite_11_05_08/mobilenet_v2_1.0_224_quant.tgz](url) to fp16 tflite model using the steps mentioned here [https://www.tensorflow.org/lite/performance/post_training_quantization#float16_quantization](url)

the converted mobilenet_v2_fp16.tflite model has multiple Dequantize steps inserted within which seem to convert float16 to float32, even though the model is supposed to be quantized to float16.
Below is a snippet of the model visualization from netron. The inputs to the converted model are float32 and its outputs are also float32. 
 
![test](https://user-images.githubusercontent.com/78979784/155630251-9e58343d-95a9-420c-8101-86fed9165372.PNG)


All the dequantize nodes have float16 input and float32 output. Node 0 dequantize node properties are visualized below:

![image](https://user-images.githubusercontent.com/78979784/155630719-1c989e54-22d8-4423-846f-93eb911c22ff.png)

all dequantize nodes have similar properties where the inputs to the layer comprise of only weights and/or bias. 

now according to the official documentation, _""By default, a float16 quantized model will ""dequantize"" the weights values to float32 when run on the CPU. (Note that the GPU delegate will not perform this dequantization, since it can operate on float16 data.)""_ This means dequantize step is added just for cpu execution and gpu will not perform or override this node. But the question I have is, the dequantize node is already present, can a delegate simply skip a node in a model architecture during execution runtime, since the node is a part of the model graph and every node is executed with no op delegated to CPU? 


What is the main use of this dequantize node? And why is a dequantization step inserted during fp16 quantization? Is there a way to skip these dequantize nodes in the GPU delegate during post-training float16 quantization itself, for I want to run the model only on GPU and not CPU? The node seems to be executed by GPU delegate, as none of the actions are delegated to the CPU at runtime. Or are they being simply ignored by the GPU delegate?

mobilenet_v2_fp16.tflite model is attached for your reference.
[mobilenet_v2_fp16.zip](https://github.com/tensorflow/tensorflow/files/8137542/mobilenet_v2_fp16.zip)
 
thanks



","['stat:awaiting tensorflower', 'type:bug', 'TFLiteConverter', '2.6.0']",2022-02-25T01:01:05Z,8,0,https://github.com/tensorflow/tensorflow/issues/54559,UI/UX Bug
433,tensorflow/tensorflow,InvalidArgumentError: Node '.../while_grad': Connecting to invalid output 4 of source node while which has 4 outputs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): 2.8.0 (Colab)
- Python version: 3.7.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

I get the exception
```
Node 'gradients/while_grad/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).
```
when executing a `NoOp` without any control dependencies.

But also when executing my real code, I get the same exception. I'm not sure if these are two separate issues or the same.

**Describe the expected behavior**

A NoOp without control dependencies should not depend on anything, so I would never expect such exception.

For my real code, I also would not expect such exception. 

**Standalone code to reproduce the issue**

Code (to be executed with disabled eager mode):
```
v = tf.Variable(1.)

def cond(i, x):
  return tf.less(i, 10)

def body(i, x):
  return i + 1, x * 1.

j, y = tf.while_loop(cond, body, [0., v])
loss = tf.reduce_sum(y ** 2)

session.run(tf.compat.v1.global_variables_initializer())

opt = tf.compat.v1.train.GradientDescentOptimizer(0.1)
opt_op = opt.minimize(loss)

no_op = tf.no_op()
session.run(no_op)  # here it crashes already!

print(session.run((j, y, opt_op)))
```

Colab: https://colab.research.google.com/drive/1jjXpz8SAeU-8Cg6nuWu7tjxK_sAU5lVc?usp=sharing

**Other info / logs**

When executing locally, I additionally see this log output:
```
...
2022-02-19 23:07:57.636413: W tensorflow/c/c_api.cc:349] Operation '{name:'while' id:11 op device:{requested: '', assigned: ''} def:{{{node while}} = StatelessWhile[T=[DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_VARIANT], _lower_using_switch_merge=true, _num_original_outputs=5, _read_only_resource_inputs=[], _stateful_parallelism=false, body=while_body_8_rewritten[], cond=while_cond_7_rewritten[], output_shapes=[[], [], [], [], []], parallel_iterations=10](while/loop_counter, while/maximum_iterations, Const, ReadVariableOp, gradients/while_grad/Placeholder_1_0/accumulator:0)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
Traceback (most recent call last):
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1380, in _do_call
    return fn(*args)
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1362, in _run_fn
    self._extend_graph()
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1403, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'gradients/while_grad/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/az/Programmierung/playground/tf-while-v2.py"", line 37, in <module>
    main()
  File ""/Users/az/Programmierung/playground/tf-while-v2.py"", line 31, in main
    session.run(no_op)
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 970, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1193, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1373, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/Users/az/miniforge3/lib/python3.9/site-packages/tensorflow/python/client/session.py"", line 1399, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'gradients/while_grad/while_grad': Connecting to invalid output 4 of source node while which has 4 outputs. Try using tf.compat.v1.experimental.output_all_intermediates(True).
```

My hypothesis is that the first session call `session.run(tf.compat.v1.global_variables_initializer())` will somehow freeze the while loop function graph (which is strange though as it would not depend on it), and then the further code which adds the gradients causes this warning `Operation ... StatelessWhile ... was changed by setting attribute after it was run by a session`.

I don't really understand why the first session call does that even though it is independent from the loop.

I don't really understand why it causes the error for the NoOp execution.

Is there any workaround?
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.8']",2022-02-19T22:24:17Z,9,0,https://github.com/tensorflow/tensorflow/issues/54458,Runtime Error
434,tensorflow/tensorflow,SelfAdjointEigV2 GPU operation takes a lot of temporary memory.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.9.0
- Python version: 3.8.10
- CUDA/cuDNN version: 11.5
- GPU model and memory: GTX 1660 Ti

**Describe the current behavior**
A single call to `tf.linalg.eigh()` takes a linear amount of memory in batch size, despite being processed matrix by matrix. I think the ScratchSpace is only freed in the end of the call after ALL matrices in the batch are processed, instead of on the fly matrix, per matrix. This I conclude from the enormous amount of allocations reported by the allocator during the OOM, and looking at the code.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): maybe
- Briefly describe your candidate solution(if contributing):
  - **Best solution:** reuse the ScratchSpace for every matrix.  
  - **Next best solution:** free the ScratchSpace after every matrix in the batch.

**Standalone code to reproduce the issue**
```py
import tensorflow as tf
tensor = tf.random.uniform((1024 * 256, 4, 4)) # just make sure the batch size is big enough
tensor = tf.matmul(tensor, tensor, transpose_b=True)
t = tf.linalg.eigvalsh(tensor)
```

**Other info / logs**
See below the summary of the allocator:
```
2022-01-03 16:18:32.407928: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 215.4KiB (rounded to 220672)requested by op SelfAdjointEigV2

2022-01-03 16:18:32.518897: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: 
2022-01-03 16:18:32.518906: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 256 totalling 256B
2022-01-03 16:18:32.518916: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB
2022-01-03 16:18:32.518924: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 65536 totalling 64.0KiB
2022-01-03 16:18:32.518930: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 11841 Chunks of size 220672 totalling 2.43GiB
2022-01-03 16:18:32.518937: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 251648 totalling 245.8KiB
2022-01-03 16:18:32.518944: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 410880 totalling 401.2KiB
2022-01-03 16:18:32.518951: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4194304 totalling 4.00MiB
2022-01-03 16:18:32.518958: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 268435456 totalling 512.00MiB
2022-01-03 16:18:32.518964: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 2.94GiB
2022-01-03 16:18:32.518970: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 3154771968 memory_limit_: 3154771968 available bytes: 0 curr_region_allocation_bytes_: 6309543936
2022-01-03 16:18:32.518982: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: 
Limit:                      3154771968
InUse:                      3154771968
MaxInUse:                   3154771968
NumAllocs:                       11850
MaxAllocSize:                268435456
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-01-03 16:18:32.519244: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ****************************************************************************************************
2022-01-03 16:18:32.519294: F ./tensorflow/core/util/gpu_solvers.h:533] Non-OK-status: context->allocate_temp(DataTypeToEnum<Scalar>::value, shape, &scratch_tensor_, alloc_attr) status: RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[55137] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
```

Especially this line:  
```
...r.cc:1074] 11841 Chunks of size 220672 totalling 2.43GiB
```
So here, I conclude that it crashes when it's trying to process matrix number 11842 out of 16384.

Pointers:
 - ScratchSpace request in the `HeevdImpl` call (at line 635): https://github.com/tensorflow/tensorflow/blob/322cba072c9313689aeb2fc3174f18fce57194d7/tensorflow/core/util/cuda_solvers.cc#L620-L643
 - Batch processing matrix by matrix: https://github.com/tensorflow/tensorflow/blob/322cba072c9313689aeb2fc3174f18fce57194d7/tensorflow/core/kernels/linalg/self_adjoint_eig_v2_op_gpu.cc#L130-L140","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'comp:core', 'TF 2.9']",2022-01-03T15:35:17Z,5,0,https://github.com/tensorflow/tensorflow/issues/53615,Runtime Error
435,tensorflow/tensorflow,tf.data.Dataset .map().batch() pattern is not matched to use fused implementation.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.12.1-69264-g0cdf35562dc 2.9.0
- Python version: 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.5 / 8.3
- GPU model and memory: GTX1660 Ti

**Describe the current behavior**
combining `tf.data.Dataset.map()` with `.batch()` does not use the fused BatchAndMap implementation.

**Describe the expected behavior**
It does use the fused implementation. Currently, it's only possible to use the fused implementation when using the deprecated `experimental.map_and_batch()` transformation.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

```py
import os
import datetime
from tqdm import tqdm
import numpy as np

import tensorflow as tf
print('TF version', tf.__version__)

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), 'Physical GPUs,', len(logical_gpus), 'Logical GPUs')
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)


@tf.function
def do_stuff(wmat, tf_var):
    with tf.device(""/gpu:0""):
        S = tf.constant(0.0)
        for i in tf.range(4):
            fi = tf.cast(i, dtype=tf.float32)
            A = tf.math.lgamma(tf.tanh(tf.matmul(wmat + fi, tf.transpose(wmat - fi, [0, 2, 1]))))
            S += tf.reduce_sum(A)
        error = tf.reduce_mean(tf_var)
        return error, S

exp_uuid = datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")

n_batches = 512


def gen():
    for i in range(n_batches):
        with tf.device(""/cpu:0""): # Make sure it comes from CPU
            r = tf.ones((400,800))
        yield r

option_names = ['map().batch()', 'map_and_batch()']
for option in range(2):

    with tf.device(""/cpu:0""):
        dataset = tf.data.Dataset.from_generator(gen, output_types=tf.float32)

        def my_identity(x):
            with tf.device(""/cpu:0""):
                print(""my_identity input:"", x, x.device)
                y = tf.identity(x)
                print(""my_identity output:"", y, y.device)
                return y

        if option == 0:
            ## Option 0: map().batch()
            dataset = dataset.map(my_identity).batch(16)

        elif option == 1:
            ## Option 1: deprecated map_and_batch()
            dataset = dataset.apply(tf.data.experimental.map_and_batch(my_identity, 16))

    gpu_transform = tf.data.experimental.prefetch_to_device('/gpu:0', buffer_size=4)
    dataset = dataset.apply(gpu_transform)


    tf_var = tf.Variable(tf.zeros(3))
    adam = tf.keras.optimizers.Adam(1e-4)
    logpath = os.path.join('data', 'logs', 'pa_' + exp_uuid + '_' + option_names[option])

    tf.profiler.experimental.start(logpath)
    start = datetime.datetime.now()
    for b, wmat in tqdm(enumerate(dataset)):
        with tf.GradientTape() as tape:

            if b == 0:
                print('\n\n dataset element device', wmat.device)
                print('\n')

            # Do some calculations
            result = do_stuff(wmat, tf_var)

        grads = tape.gradient(result[0], [tf_var])
        adam.apply_gradients(zip(grads, [tf_var]))
    stop = datetime.datetime.now()
    tf.profiler.experimental.stop()

    print(f'\n\nOption {option_names[option]}\n===========================\n')
    print(logpath)
    print('Time lapsed=', stop - start)
    print(""\n\n"")
```

**Other info / logs**

**Option 1**:
![image](https://user-images.githubusercontent.com/845012/147661299-a7f72017-00ff-47b6-bb71-8812bd5163d3.png)
Symptoms:
 - See the blocks `Iterator::FlapMap` and `Iterator::BatchV2` stacked on top of each other.
 - The MemcpyH2D (selected, see the details panel) is comping from pagable memory, instead of pinned memory (which is what MapAndBatch does). Because of the source being pagable memory, it can't overlap with kernel computations.
 
**Option 2**:
![image](https://user-images.githubusercontent.com/845012/147661558-9f316201-a4e5-4df1-a77b-032272537321.png)
Evidence:
 - The MapAndBatch block is used.
 - The MemcopyH2D comes from pinned memory (see details pane) and overlaps with kernel computations.

The whole deal about pinned memory is to allow parallel data upload and kernel computations. So the dataset needs to be produced into pinned host memory, which then can be uploaded asynchronously by the driver without an extra copy. See https://github.com/tensorflow/tensorflow/issues/43905#issuecomment-823675760 and https://github.com/tensorflow/tensorflow/issues/43905#issuecomment-824145184 and:
https://github.com/tensorflow/tensorflow/blob/40e9b534962989af7486bc6567ca472d71eb5049/tensorflow/core/kernels/data/experimental/map_and_batch_dataset_op.cc#L522

This is a follow up on https://github.com/tensorflow/tensorflow/issues/43905.
","['comp:tensorboard', 'type:bug', 'comp:data', 'TF 2.7']",2021-12-29T12:25:16Z,12,0,https://github.com/tensorflow/tensorflow/issues/53572,Runtime Error
436,tensorflow/tensorflow,Tensorflow conv1d computed incorrectly on GPU,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
- TensorFlow installed from (source or binary):  binary
- TensorFlow version (use command below): 2.7
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda-11.2
- GPU model and memory: Tesla K80

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Computing a single filter 1d convolution on GPU with kernel_size=15, padding='valid' and strides=1 on a (1, 64, 32) input tensor (1 sample X 64 coordinates X 32 input channels) results in data from the end of the input tensor effecting the beginning of the output vector. 

This behavior only happens when performing the calculation on GPU (specifically nvidia Telsa K80). 
On CPU results are fine.

**Describe the expected behavior**
Receptive field should be limited to the kernel size. 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Issue reproduces on colab with GPU:
https://colab.research.google.com/drive/1pK1tNxWVtC9qilcX0u5sbpGmQEZiCDEX?usp=sharing


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
[Tensorflow-bug.ipynb - Colaboratory.pdf](https://github.com/tensorflow/tensorflow/files/7769397/Tensorflow-bug.ipynb.-.Colaboratory.pdf)

","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.7']",2021-12-23T13:10:33Z,6,0,https://github.com/tensorflow/tensorflow/issues/53533,UI/UX Bug
437,tensorflow/tensorflow,issue about XLA compile MirroredStrategy,"System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): I have tested on Ubuntu 18.04.
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.7.0
- Python version: 3.7
- CUDA/cuDNN version: 11.0
- GPU model and memory: Tesla P100

**Describe the current behavior**
When I train my model on multi-gpu with XLA compiling below error is occurred.

```
021-11-20 12:57:08.333476: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-20 12:57:09.302772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15397 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:3b:00.0, compute capability: 6.0
2021-11-20 12:57:09.303502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 15397 MB memory:  -> device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:b1:00.0, compute capability: 6.0
2021-11-20 12:57:10.556310: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at xla_ops.cc:241 : INVALID_ARGUMENT: Trying to access resource _AnonymousVar3 (defined @ /home/sdb/wda/tf_xla/lib/python3.7/site-packages/keras/engine/base_layer_utils.py:129) located in device /job:localhost/replica:0/task:0/device:GPU:1 from device /job:localhost/replica:0/task:0/device:GPU:0
Traceback (most recent call last):
  File ""/home/sdb/wda/TF2-jit-compile-on-multi-gpu/xla_tf_function_distributed.py"", line 59, in <module>
    train_step_dist(images, labels)
  File ""/home/sdb/wda/tf_xla/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py"", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/home/sdb/wda/tf_xla/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Trying to access resource _AnonymousVar3 (defined @ /home/sdb/wda/tf_xla/lib/python3.7/site-packages/keras/engine/base_layer_utils.py:129) located in device /job:localhost/replica:0/task:0/device:GPU:1 from device /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_train_step_dist_650]
```

Describe the expected behavior
I want to use XLA  compile MirroredStrategy， because I found that _XLA can now compile MirroredStrategy: the step function passed to`strategy.run` can now be annoted with `jit_compile=True`._ from RELEASE.md from  2.5.0

Standalone code to reproduce the issue:

```
import tensorflow as tf
tf.compat.v1.enable_eager_execution()

# Size of each input image, 28 x 28 pixels
IMAGE_SIZE = 28 * 28
# Number of distinct number labels, [0..9]
NUM_CLASSES = 10
# Number of examples in each training batch (step)
TRAIN_BATCH_SIZE = 100
# Number of training steps to run
TRAIN_STEPS = 1000

# Loads MNIST dataset.
train, test = tf.keras.datasets.mnist.load_data()
train_ds = tf.data.Dataset.from_tensor_slices(train).batch(TRAIN_BATCH_SIZE).repeat()


# Casting from raw data to the required datatypes.
def cast(images, labels):
    images = tf.cast(
        tf.reshape(images, [-1, IMAGE_SIZE]), tf.float32)
    labels = tf.cast(labels, tf.int64)
    return (images, labels)


layer = tf.keras.layers.Dense(NUM_CLASSES)
optimizer = tf.keras.optimizers.Adam()


@tf.function(jit_compile=True)
def compiled_step(images, labels):
    images, labels = cast(images, labels)

    with tf.GradientTape() as tape:
        predicted_labels = layer(images)
        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(
            logits=predicted_labels, labels=labels
        ))
    gradients = tape.gradient(loss, layer.trainable_variables)
    return loss, predicted_labels, gradients

@tf.function()
def train_step(images, labels):
    loss, pred, gradients = compiled_step(images, labels)
    optimizer.apply_gradients(zip(gradients, layer.trainable_variables))


strategy = tf.distribute.MirroredStrategy()

@tf.function(jit_compile=True)
def train_step_dist(image, labels):
    strategy.run(train_step, args=(image, labels))


for images, labels in train_ds:
    if optimizer.iterations > TRAIN_STEPS:
        break
    train_step_dist(images, labels)

```","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.7']",2021-11-20T05:06:22Z,8,0,https://github.com/tensorflow/tensorflow/issues/53140,Runtime Error
438,tensorflow/tensorflow,Issue with conversion of dilated Convolutions #29509 still happening in version 2.6.0,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.6.0

### 2. Code
Standalone code to reproduce the issue - 
```
import numpy as np
import tensorflow as tf
from tensorflow_model_optimization.python.core.quantization.keras import quantize
from tensorflow.python import keras

l = tf.keras.layers

tf.config.run_functions_eagerly(True)

def functional_model():
    """"""Builds an MNIST functional model.""""""
    inp = tf.keras.Input(shape=image_input_shape())
    x = l.Conv2D(filters=32, kernel_size=5, padding='same', activation='relu',
                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(inp)
    x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)
    # TODO(pulkitb): Add BatchNorm when transformations are ready.
    # x = l.BatchNormalization()(x)
    x = l.Conv2D(filters=64, kernel_size=5, padding='same', activation='relu',
                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)
    x = l.Conv2D(filters=64, kernel_size=3, dilation_rate=(3, 3), padding='same', activation='relu',
                 kernel_regularizer=tf.keras.regularizers.l2(l=0.0001))(x)
    x = l.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(x)
    x = l.Flatten()(x)
    x = l.Dense(1024, activation='relu')(x)
    x = l.Dropout(0.4)(x)
    out = l.Dense(10, activation='softmax')(x)

    return tf.keras.Model(inp, [out])


def image_input_shape(img_rows=28, img_cols=28):
    if tf.keras.backend.image_data_format() == 'channels_first':
        return 1, img_rows, img_cols
    else:
        return img_rows, img_cols, 1


def preprocessed_data(img_rows=28,
                      img_cols=28,
                      num_classes=10):
    """"""Get data for mnist training and evaluation.""""""
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

    if tf.keras.backend.image_data_format() == 'channels_first':
        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
    else:
        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)

    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    x_train /= 255
    x_test /= 255

    # convert class vectors to binary class matrices
    y_train = tf.keras.utils.to_categorical(y_train, num_classes)
    y_test = tf.keras.utils.to_categorical(y_test, num_classes)

    return x_train, y_train, x_test, y_test


model = functional_model()
model.summary()
x_train, y_train, x_test, y_test = preprocessed_data()

model.compile(
    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=500)
_, model_accuracy = model.evaluate(x_test, y_test, verbose=0)

print(""Quantizing model"")

quantized_model = quantize.quantize_model(model)
quantized_model.compile(
    loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

quantized_model.fit(x_train, y_train, batch_size=500)
_, quantized_model_accuracy = quantized_model.evaluate(
    x_test, y_test, verbose=0)
model.save(""/home/anurag/git/train_data/testOrig.h5"")
quantized_model.save(""/home/anurag/git/train_data/test.h5"")
converter = tf.lite.TFLiteConverter.from_keras_model(quantized_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
converter.change_concat_input_ranges = True
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tfliteModel = converter.convert()
with open(""/home/anurag/git/train_data/test.tflite"", 'wb') as outfile:
    outfile.write(tfliteModel)
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- The latency in model is high due to the fact that atrous convolution is being broken down to spacetodepth, conv2d and depth to space and then does not apply quantization to conv2d.
- Model produces correct results, but it is slower than expected.

This is the same as #29509. Issue was solved but appears again on newer releases.","['stat:awaiting tensorflower', 'type:bug', 'TFLiteConverter', '2.6.0']",2021-11-10T22:40:29Z,15,25,https://github.com/tensorflow/tensorflow/issues/53025,Performance Issue
439,tensorflow/tensorflow,Model checkpoint load ignores wrong shape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOSX 11.6 + Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below):  v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.9.6 + 3.8
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**

Saving a variable of shape (3,3,40,32) into a checkpoint, and then loading the variable with shape (3,3,1,32) from this checkpoint does not cause an error. Instead, it loads fine. The variable shape still claims to be (3,3,1,32), but when evaluated, one can see that it is indeed (3,3,40,32).

So there does not seem to be any validation of the shape during checkpoint loading.

**Describe the expected behavior**

In general, if a tensor or variable claims to be of some static shape, I think it should never be possible that its actual shape when evaluated is different.

**Standalone code to reproduce the issue**
```

import tensorflow as tf

print(""TF:"", tf.__version__)
tf.compat.v1.disable_eager_execution()

filename = ""test-ckpt-diff-shape.model""


with tf.Graph().as_default() as graph:
    with tf.compat.v1.Session(graph=graph) as session:
        shape1 = (3,3,40,32)
        v = tf.compat.v1.get_variable(name=""W"", shape=shape1)
        print(v)
        saver = tf.compat.v1.train.Saver(var_list=[v])
        session.run(tf.compat.v1.global_variables_initializer())
        saver.save(sess=session, save_path=filename)


with tf.Graph().as_default() as graph:
    with tf.compat.v1.Session(graph=graph) as session:
        shape2 = (3,3,1,32)
        v = tf.compat.v1.get_variable(name=""W"", shape=shape2)
        print(v)
        saver = tf.compat.v1.train.Saver(var_list=[v])
        saver.restore(sess=session, save_path=filename)
        v_raw = session.run(v)
        print(v)
        print(v_raw.shape)
        assert v.shape.as_list() == list(shape2)
        assert v.shape.as_list() == list(v_raw.shape)
```

**Other info / logs**

I stumbled upon this problem because it causes a seemingly unrelated error for some following 2D convolution where this variable is used as a kernel:
```
2021-09-30 12:52:25.768174: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops.cc:1
115 : Not found: No algorithm worked!
TensorFlow exception: 2 root error(s) found.
  (0) Not found: No algorithm worked!
         [[node conv0/convolution (defined at u/schmitt/src/returnn/returnn/tf/layers/basic.py:4061) ]]
         [[output/rec/while/Switch_14/_553]]
  (1) Not found: No algorithm worked!
         [[node conv0/convolution (defined at u/schmitt/src/returnn/returnn/tf/layers/basic.py:4061) ]]
0 successful operations.
```
This was reported here: https://github.com/rwth-i6/returnn/issues/703

There are a couple of these errors also reported here:
* https://github.com/tensorflow/tensorflow/issues/43174
* https://github.com/tensorflow/tensorflow/issues/45044
* https://github.com/tensorflow/tensorflow/issues/48117

I wonder if some of them have a similar source.

But anyway, this issue is not really about the convolution error, but about the checkpoint loading behavior, and the variable shape.
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', '2.6.0']",2021-10-01T19:37:38Z,6,0,https://github.com/tensorflow/tensorflow/issues/52220,Runtime Error
440,tensorflow/tensorflow,No registered 'Const' OpKernel for GPU devices with constant folding,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip binary
- TensorFlow version (use command below): v2.6.0-rc2-32-g919f693420e 2.6.0
- Python version: 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.4 / 8.2.4.15
- GPU model and memory: NVIDIA GeForce RTX 2070 

**Describe the current behavior**

The code below fails with an exception.
This is the full output:
```
TF: 2.6.0
2021-09-30 15:52:24.159169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.162278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.162637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.163155: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-30 15:52:24.163754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.164103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.164431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.456691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.457036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.457342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-09-30 15:52:24.457640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5732 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:09:00.0, compute capability: 7.5
2021-09-30 15:52:24.466132: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'Const' OpKernel for GPU devices compatible with node {{node ConstantFolding/Const_enter}}
         (OpKernel was found, but attributes didn't match) Requested Attributes: dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=""/job:localhost/replica:0/task:0/device:GPU:0""
        .  Registered:  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_VARIANT]
  device='DEFAULT'; dtype in [DT_BOOL]
  device='DEFAULT'; dtype in [DT_QUINT16]
  device='DEFAULT'; dtype in [DT_QINT16]
  device='DEFAULT'; dtype in [DT_QINT32]
  device='DEFAULT'; dtype in [DT_QUINT8]
  device='DEFAULT'; dtype in [DT_QINT8]
  device='DEFAULT'; dtype in [DT_COMPLEX128]
  device='DEFAULT'; dtype in [DT_COMPLEX64]
  device='DEFAULT'; dtype in [DT_INT8]
  device='DEFAULT'; dtype in [DT_UINT8]
  device='DEFAULT'; dtype in [DT_INT16]
  device='DEFAULT'; dtype in [DT_UINT16]
  device='DEFAULT'; dtype in [DT_UINT32]
  device='DEFAULT'; dtype in [DT_INT64]
  device='DEFAULT'; dtype in [DT_UINT64]
  device='DEFAULT'; dtype in [DT_DOUBLE]
  device='DEFAULT'; dtype in [DT_FLOAT]
  device='DEFAULT'; dtype in [DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_HALF]
  device='DEFAULT'; dtype in [DT_INT32]
  device='CPU'
  device='TPU_SYSTEM'
  device='GPU'; dtype in [DT_VARIANT]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_UINT64]
  device='GPU'; dtype in [DT_INT64]
  device='GPU'; dtype in [DT_QINT32]
  device='GPU'; dtype in [DT_UINT32]
  device='GPU'; dtype in [DT_QUINT16]
  device='GPU'; dtype in [DT_QINT16]
  device='GPU'; dtype in [DT_INT16]
  device='GPU'; dtype in [DT_UINT16]
  device='GPU'; dtype in [DT_QINT8]
  device='GPU'; dtype in [DT_INT8]
  device='GPU'; dtype in [DT_UINT8]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_BFLOAT16]
  device='GPU'; dtype in [DT_HALF]

Traceback (most recent call last):
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1375, in _do_call
    return fn(*args)
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1359, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1451, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Const' OpKernel for 'GPU' devices compatible with node {{node ConstantFolding/Const_enter}}
         (OpKernel was found, but attributes didn't match) Requested Attributes: _XlaHasReferenceVars=false, dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=""/job:localhost/replica:0/task:0/device:GPU:0""
        .  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]
  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]
  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_VARIANT]
  device='DEFAULT'; dtype in [DT_BOOL]
  device='DEFAULT'; dtype in [DT_QUINT16]
  device='DEFAULT'; dtype in [DT_QINT16]
  device='DEFAULT'; dtype in [DT_QINT32]
  device='DEFAULT'; dtype in [DT_QUINT8]
  device='DEFAULT'; dtype in [DT_QINT8]
  device='DEFAULT'; dtype in [DT_COMPLEX128]
  device='DEFAULT'; dtype in [DT_COMPLEX64]
  device='DEFAULT'; dtype in [DT_INT8]
  device='DEFAULT'; dtype in [DT_UINT8]
  device='DEFAULT'; dtype in [DT_INT16]
  device='DEFAULT'; dtype in [DT_UINT16]
  device='DEFAULT'; dtype in [DT_UINT32]
  device='DEFAULT'; dtype in [DT_INT64]
  device='DEFAULT'; dtype in [DT_UINT64]
  device='DEFAULT'; dtype in [DT_DOUBLE]
  device='DEFAULT'; dtype in [DT_FLOAT]
  device='DEFAULT'; dtype in [DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_HALF]
  device='DEFAULT'; dtype in [DT_INT32]
  device='CPU'
  device='TPU_SYSTEM'
  device='GPU'; dtype in [DT_VARIANT]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_UINT64]
  device='GPU'; dtype in [DT_INT64]
  device='GPU'; dtype in [DT_QINT32]
  device='GPU'; dtype in [DT_UINT32]
  device='GPU'; dtype in [DT_QUINT16]
  device='GPU'; dtype in [DT_QINT16]
  device='GPU'; dtype in [DT_INT16]
  device='GPU'; dtype in [DT_UINT16]
  device='GPU'; dtype in [DT_QINT8]
  device='GPU'; dtype in [DT_INT8]
  device='GPU'; dtype in [DT_UINT8]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_BFLOAT16]
  device='GPU'; dtype in [DT_HALF]

         [[ConstantFolding/Const_enter]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tf-const-gpu.py"", line 18, in <module>
    session.run(n)
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1190, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1368, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/home/az/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1394, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Const' OpKernel for 'GPU' devices compatible with node {{node ConstantFolding/Const_enter}}
         (OpKernel was found, but attributes didn't match) Requested Attributes: _XlaHasReferenceVars=false, dtype=DT_STRING, value=Tensor<type: string shape: [] values: foo>, _device=""/job:localhost/replica:0/task:0/device:GPU:0""
        .  Registered:  device='XLA_CPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]
  device='XLA_GPU_JIT'; dtype in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_COMPLEX64, DT_INT64, DT_BOOL, DT_QINT8, DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64, DT_STRING]
  device='XLA_GPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='XLA_CPU'; dtype in [DT_UINT8, DT_QUINT8, DT_UINT16, DT_INT8, DT_QINT8, DT_INT16, DT_INT32, DT_QINT32, DT_INT64, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128, DT_BOOL, DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_VARIANT]
  device='DEFAULT'; dtype in [DT_BOOL]
  device='DEFAULT'; dtype in [DT_QUINT16]
  device='DEFAULT'; dtype in [DT_QINT16]
  device='DEFAULT'; dtype in [DT_QINT32]
  device='DEFAULT'; dtype in [DT_QUINT8]
  device='DEFAULT'; dtype in [DT_QINT8]
  device='DEFAULT'; dtype in [DT_COMPLEX128]
  device='DEFAULT'; dtype in [DT_COMPLEX64]
  device='DEFAULT'; dtype in [DT_INT8]
  device='DEFAULT'; dtype in [DT_UINT8]
  device='DEFAULT'; dtype in [DT_INT16]
  device='DEFAULT'; dtype in [DT_UINT16]
  device='DEFAULT'; dtype in [DT_UINT32]
  device='DEFAULT'; dtype in [DT_INT64]
  device='DEFAULT'; dtype in [DT_UINT64]
  device='DEFAULT'; dtype in [DT_DOUBLE]
  device='DEFAULT'; dtype in [DT_FLOAT]
  device='DEFAULT'; dtype in [DT_BFLOAT16]
  device='DEFAULT'; dtype in [DT_HALF]
  device='DEFAULT'; dtype in [DT_INT32]
  device='CPU'
  device='TPU_SYSTEM'
  device='GPU'; dtype in [DT_VARIANT]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_UINT64]
  device='GPU'; dtype in [DT_INT64]
  device='GPU'; dtype in [DT_QINT32]
  device='GPU'; dtype in [DT_UINT32]
  device='GPU'; dtype in [DT_QUINT16]
  device='GPU'; dtype in [DT_QINT16]
  device='GPU'; dtype in [DT_INT16]
  device='GPU'; dtype in [DT_UINT16]
  device='GPU'; dtype in [DT_QINT8]
  device='GPU'; dtype in [DT_INT8]
  device='GPU'; dtype in [DT_UINT8]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_BFLOAT16]
  device='GPU'; dtype in [DT_HALF]

         [[ConstantFolding/Const_enter]]
```

**Describe the expected behavior**

The code below should work without error on a GPU.

**Standalone code to reproduce the issue**


```
import tensorflow as tf


print(""TF:"", tf.__version__)
tf.compat.v1.disable_eager_execution()
tf.compat.v1.disable_control_flow_v2()


with tf.compat.v1.Session() as session:
  x = tf.constant(""foo"")

  def body(i):
    with tf.control_dependencies([tf.print(x)]):
      return i + 1

  n = tf.while_loop(cond=lambda i: tf.less(i, 1), body=body, loop_vars=[0])
  session.run(n)
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', '2.6.0']",2021-09-30T13:54:11Z,4,0,https://github.com/tensorflow/tensorflow/issues/52200,Runtime Error
441,tensorflow/tensorflow,unable to create tf.variables inside a function that is decorated with @tf.function,"tf 2.5


```
@tf.function
def weight_fn():
    w = tf.Variable(tf.truncated_normal())
```

I have a function like above that would be called about **50 times**, each time it should generate a new variable and return. But according to the [rule](https://tensorflow.google.cn/guide/function#creating_tfvariables) and the hint below,

```
    ValueError: A tf.Variable created inside your tf.function has been garbage-collected. Your code needs to keep Python references to variables created inside `tf.function`s.
    
    A common way to raise this error is to create and return a variable only referenced inside your function:
    
    @tf.function
    def f():
      v = tf.Variable(1.0)
      return v
    
    v = f()  # Crashes with this error message!
    
    The reason this crashes is that @tf.function annotated function returns a **`tf.Tensor`** with the **value** of the variable when the function is called rather than the variable instance itself. As such there is no code holding a reference to the `v` created inside the function and Python garbage collects it.
    
    The simplest way to fix this issue is to create variables outside the function and capture them:
    
    v = tf.Variable(1.0)
    
    @tf.function
    def f():
      return v
    
    f()  # <tf.Tensor: numpy=1.>
    v.assign_add(1.)
    f()  # <tf.Tensor: numpy=2.>

```

I should define the weight variable outside the tf.function, which means I should manually define over 50 weight variables, each line with a weight variable.

```
w1 = tf.Variable(tf.truncated_normal())
w2 = tf.Variable(tf.truncated_normal())
w3 = tf.Variable(tf.truncated_normal())
......
w50 = tf.Variable(tf.truncated_normal())

```

Undoubtedly, this kind of behavior is really stupid, any solutions to this kind of unreasonable rule?

","['stat:awaiting tensorflower', 'type:bug', 'TF 2.5', 'comp:tf.function']",2021-08-12T12:20:06Z,5,0,https://github.com/tensorflow/tensorflow/issues/51453,Runtime Error
442,tensorflow/tensorflow,Inconsistent eager/tf.function behavior for rank 0 shape in tf.reshape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8

**Describe the current behavior**

The eager version of `tf.reshape` takes a rank 0 tensor as a shape parameter while the jitted (`tf.function` decorated) does not.

**Describe the expected behavior**

It should be consistent, either fail in both or allow in both. To be consistent with other methods, I think it should fail in both.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): maybe
- Briefly describe your candidate solution(if contributing): raise an error, such as done in other methods like `tf.random.uniform`

**Standalone code to reproduce the issue**
[executable example here](https://colab.research.google.com/drive/15wQJVDzEHc5puK6twrV9HzxcdWZWZPtC?usp=sharing)

```
import tensorflow as tf

def func():
    return tf.reshape([[42]], 1)


func_jit = tf.function(func=func)

func()  # works
func_jit()  # fails
```","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.9']",2021-08-05T10:20:10Z,8,1,https://github.com/tensorflow/tensorflow/issues/51241,Runtime Error
443,tensorflow/tensorflow,Out of memory when using MultiWorkerMirroredStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): REHEL 7.9
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.7.1
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.2
- GPU model and memory: GTX 1080Ti

**Describe the current behavior**

When running a trivial example TF starts allocation gigabytes of memory continously until it runs out of memory.
This does only happen when running on more than 2 nodes, not when running on only 1 or 2 nodes and it happens in the creation of MultiWorkerMirroredStrategy
I can also start e.g. 6 tasks on 2 nodes (3 tasks each) without problems, but 3 tasks on 3 nodes (1 task each) does not work

I also observed that it happens only one 1 of the ranks used, the other workers are fine and waiting for that one to finish init (I suppose)

**Describe the expected behavior**

It works or a reasonable error.

**Standalone code to reproduce the issue**

```

import tensorflow as tf
from mpi_cluster_resolver import MPIClusterResolver

resolver = MPIClusterResolver()
strategy = tf.distribute.MultiWorkerMirroredStrategy(cluster_resolver=resolver)
```

With 
[mpi_cluster_resolver.py.txt](https://github.com/tensorflow/tensorflow/files/6823734/mpi_cluster_resolver.py.txt)

This is so that running distributed TF is possible via SLURM/MPI on HPC clusters","['stat:awaiting tensorflower', 'type:bug', 'comp:dist-strat', 'TF 2.5']",2021-07-15T14:14:24Z,9,0,https://github.com/tensorflow/tensorflow/issues/50790,Runtime Error
444,tensorflow/tensorflow,Wrong gradient from complex determinant,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur, Version: 11.2.3 (20D91)
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7

**Describe the current behavior**
I implemented a straightforward example which illustrates the issue:
I have a 2-dimensional tensor which is mapped to a complex tensor as:

x_i -> z_i = (x_i, lamb_i * x_i)

with some real values for lambda = [lambda_1, lambda_2].
Afterwards I calculate the Jacobian (with respect to x) and its determinant, which is
also easy to do as it is just a 2x2 matrix.
In the end, I take |det|^2 as final output L.

Analytically, you would now get for the gradient:
grad L = [ 2 * lambda_1 * (1+lambda_2^2), 2 * lambda_2 * (1+lambda_1^2)

So if you insert lambda_test = [ 1.0 , 2.0]
You should obtain: grad L(lambda_test) = [ 10, 8]

However, Tensorflow yields: TF-Grad = [-14, -8.8],
which obviously is completely off. 

Additionally, we also implemented a simple numerical derivative ourselves (just the basic definition of the gradient in terms of difference quotient). This calculation does yield the correct gradient (within numerical uncertainties)

**Standalone code to reproduce the issue**
The issue can be reproduced in this [gist](https://colab.research.google.com/drive/1C8PQKBWS-ykraftMqVq6MWmOjcmdQbhH?usp=sharing).

","['stat:awaiting tensorflower', 'type:bug', 'comp:core', 'TF 2.9']",2021-06-01T09:48:41Z,6,3,https://github.com/tensorflow/tensorflow/issues/49946,Logical Bug
445,tensorflow/tensorflow,QuantizedOpsTest.testAxis fails on cascade lake CPUs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux RHEL 8.3
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.6
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): GCC 10.2.0
- CUDA/cuDNN version: None

**Describe the current behavior**

QuantizedOpsTest.testAxis fails on cascade lake systems when native optimizations are enabled

**Standalone code to reproduce the issue**

Run the TF test `//tensorflow/python:quantized_ops_test` through bazel

**Other info / logs**
```
FAIL: testAxis (__main__.QuantizedOpsTest)
QuantizedOpsTest.testAxis
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/dev/shm/build-branfosj-admin/branfosj-admin-up/TensorFlow/2.5.0/foss-2020b/tmp8qildda2-bazel-tf/1b512851602cc5932dcb1cd30c5fbde4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/quantized_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/quantized_ops_test.py"", line 94, in testAxis
    self.assertAllEqual(quantized, expected_quantized)
  File ""/dev/shm/build-branfosj-admin/branfosj-admin-up/TensorFlow/2.5.0/foss-2020b/tmp8qildda2-bazel-tf/1b512851602cc5932dcb1cd30c5fbde4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/quantized_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1253, in decorated
    return f(*args, **kwds)
  File ""/dev/shm/build-branfosj-admin/branfosj-admin-up/TensorFlow/2.5.0/foss-2020b/tmp8qildda2-bazel-tf/1b512851602cc5932dcb1cd30c5fbde4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/quantized_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2881, in assertAllEqual
    np.testing.assert_array_equal(a, b, err_msg=""\n"".join(msgs))
  File ""/rds/bear-apps/devel/eb-sjb-up/EL8/EL8-cas/software/SciPy-bundle/2020.11-foss-2020b/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 930, in assert_array_equal
    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
  File ""/rds/bear-apps/devel/eb-sjb-up/EL8/EL8-cas/software/SciPy-bundle/2020.11-foss-2020b/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 840, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Arrays are not equal

not equal where = (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0]), array([0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1,
       1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1,
       1, 1, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0]), array([0, 1, 2, 3, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,
       1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,
       3, 4, 0, 1, 2, 3, 4, 0, 0, 1, 2, 3]))
not equal lhs = array([ -64,    0,   38,  102,   38,  102,   71,   64,   64, -128,  -64,
          0,  102,   71,   64, -128, -128,  -64,    0,   38,   64, -128,
        -64,    0,    0,   38,  102,   71,    0,   38,  102,   71,   71,
         64, -128,  -64,  102,   71,   64, -128, -128,  -64,    0,   38,
         71,   64, -128,  -64,  -64,    0,   38,  102,   38,  102,   71,
         64], dtype=int8)
not equal rhs = array([-128,  -64,    0,   38,  -64,    0,   38,  102,   71,   64, -128,
        -64,    0,   38,  102,   71,   64, -128,  -64,    0,   38,  102,
         71,   64, -128,  -64,    0,   38,  102,   71,   64, -128,  -64,
          0,   38,  102,   71,   64, -128,  -64,    0,   38,  102,   71,
         64, -128,  -64,    0,   38,  102,   71,   64,  102,   71,   64,
       -128], dtype=int32)
Mismatched elements: 56 / 120 (46.7%)
Max absolute difference: 230
Max relative difference: 4.36842105
 x: array([[[[ -64,    0,   38,  102,  102],
         [  71,   64, -128,   38,  102],
         [  71,   64,   64, -128,  -64],...
 y: array([[[[-128,  -64,    0,   38,  102],
         [  71,   64, -128,  -64,    0],
         [  38,  102,   71,   64, -128],...
```

This seems to be related to https://github.com/tensorflow/tensorflow/issues/47179 which can also only be observed on cascade lake systems.","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.9']",2021-06-01T08:35:45Z,14,1,https://github.com/tensorflow/tensorflow/issues/49944,Runtime Error
446,tensorflow/tensorflow,Compilation fails on Ubuntu 20.04 when using TensorRT 8. ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.4.1, 2.5, etc
- Python version: 3.8
- Installed using virtualenv? pip? conda?: no, built from source
- Bazel version (if compiling from source): 3.1 (for TF 2.4.1), 3.7.2 (for TF 2.5.0-rcx)
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- CUDA/cuDNN version: Cuda 11.1, cudnn8 (8.0.5.39-1+cuda11.1) or Cuda-11-2, libcudnn 8.1.1, 8.2, 
- GPU model and memory: GTX-1080ti
- TensorRT (crucial): 8.0.0-1+cuda11.0, or 8.0.0-1+cuda11.3

**Describe the problem**
When compiling with support for TensorRT 8 (via libnvinfer8), compilation fails (log is below). 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
When configuring the build, make sure you build with TensorRT support, and make sure TensorRT version 8 is selected. Build TF as usual. Compilation will fail. 

If you install  TensorRT version 7 manually (from debs available for Ubuntu 18.04), compilation will complete just fine.

**Any other info / logs**
Relevant error: 
`C++ compilation of rule '//tensorflow/compiler/tf2tensorrt:tensorrt_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command`

`In file included from bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:54,
                 from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:
bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInferRuntime.h:2264:51: note: from previous declaration 'nvinfer1::IPluginRegistry* getPluginRegistry() noexcept'
 2264 | extern ""C"" TENSORRTAPI nvinfer1::IPluginRegistry* getPluginRegistry() noexcept;`

Full log here: 
[gesdm-tf2.5.0rc3-error.txt](https://github.com/tensorflow/tensorflow/files/6469944/gesdm-tf2.5.0rc3-error.txt)

","['stat:awaiting tensorflower', 'type:bug', 'subtype: ubuntu/linux', 'comp:gpu:tensorrt', 'TF 2.5']",2021-05-13T02:02:32Z,24,0,https://github.com/tensorflow/tensorflow/issues/49150,Runtime Error
447,tensorflow/tensorflow,TF_AddGradientsWithPrefix doesn't seem to lock the gradients properly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Using TF-Java
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
- TensorFlow installed from (source or binary): Maven Central binary
- TensorFlow version (use command below): v2.4.1

**Describe the current behavior**

When training a model I'm hitting a non-determinism issue, the models are initialized identically, and fed identical data in an identical order, with inter-op and intra-op parallelism both set to 1, but I can get complete training failure when building simple MLPs and CNNs on MNIST.

TF-Java uses TF_AddGradientsWithPrefix to construct the gradients. We then wrap that in Optimizers to make a higher level front end to training, similar to the v1 optimizers package, and Keras in v2. As gradients are only available in Graph mode in the C API our code is most similar to the v1 optimizers package. I think we want to have the gating behaviour set to `GATE_OP` (https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Optimizer), but that's all implemented entirely in python, and the C API codepath doesn't seem to add the required control dependencies to ensure the gradients are all computed before being back-propped any further (in Python that's performed here - https://github.com/tensorflow/tensorflow/blob/85c8b2a817f95a3e979ecd1ed95bff1dc1335cff/tensorflow/python/ops/gradients_util.py#L692, but there doesn't seem to be an equivalent in the C API here - https://github.com/tensorflow/tensorflow/blob/5dcfc51118817f27fad5246812d83e5dccdc5f72/tensorflow/cc/framework/gradients.cc#L568).

**Describe the expected behavior**

The model should train identically with identical gradients.

**Standalone code to reproduce the issue**

See this test in TF-Java - https://github.com/Craigacp/tensorflow-java/blob/03402b30e271a46438c1feff12409095737f3d3e/tensorflow-framework/src/test/java/org/tensorflow/framework/optimizers/GradientDescentTest.java#L121. This test when run complains that out of the 20 runs some of them diverged even when trained using identical graph defs on identical data, for an identical number of steps, when using single threaded computation.
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.8']",2021-04-30T19:39:37Z,3,0,https://github.com/tensorflow/tensorflow/issues/48855,Logical Bug
448,tensorflow/tensorflow,tf.io.gfile.walk broken on Windows,"It seems that the path isn't split correctly. Filenames come with an extra `\` prefix. Minimal reproducible example:

```python3
import os
import tensorflow as tf


tf.io.gfile.makedirs(""ram://folder"")
with tf.io.gfile.GFile(""ram://folder/file.txt"", mode=""w"") as f:
    f.write(""data"")

for root, _, filenames in tf.io.gfile.walk(""ram://folder""):
    for filename in filenames:
        assert tf.io.gfile.exists(os.path.join(root, filename))
```

This passes on *nix but not on Windows. Here is a quick CI run in GitHub actions showing this: https://github.com/adriangb/tensorflow-test/actions/runs/688190284

ccing @mihaimaruseac @bhack ","['stat:awaiting tensorflower', 'type:bug', 'comp:ops']",2021-03-25T23:10:59Z,38,0,https://github.com/tensorflow/tensorflow/issues/48086,UI/UX Bug
449,tensorflow/tensorflow,Incompatibility between `set_visible_devices()` and `from_dlpack()`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.8.8
- CUDA/cuDNN version: 11.2
- GPU model and memory: GTX 1080Ti 11GB

**Describe the current behavior**

After selecting which GPU to use via `tf.config.experimental.set_visible_devices()`, converting external arrays from dlpack with `tf.experimental.dlpack.from_dlpack()` only works for device 0. Using other devices results in the error `InvalidArgumentError: GPU:1 unknown device.`

**Describe the expected behavior**

Converting external arrays from dlpack with `tf.experimental.dlpack.from_dlpack()` should work for all devices.

**Standalone code to reproduce the issue**
```python
import cupy
import random

import tensorflow as tf

# gpu_to_use = 0      # Works
gpu_to_use = 1        # Errors

gpus = tf.config.experimental.list_physical_devices(""GPU"")
if gpus:
    tf.config.experimental.set_visible_devices(gpus[gpu_to_use], ""GPU"")

# Converting from TF to CuPy with dlpack works for both devices
tensor = tf.random.uniform((10,))

dltensor = tf.experimental.dlpack.to_dlpack(tensor)
array1 = cupy.fromDlpack(dltensor)

# Converting from CuPy to TF with dlpack only works for device 0
array1 = cupy.array([random.uniform(0.0, 1.0) for i in range(10)], dtype=cupy.float32)
dltensor = array1.toDlpack()
x = tf.experimental.dlpack.from_dlpack(dltensor)

# Using device 1 results in the following error

# Traceback (most recent call last):
#   File ""examples/multi-gpu/tf-dlpack-repro.py"", line 22, in <module>
#     x = tf.experimental.dlpack.from_dlpack(dltensor)
#   File ""/home/karl/miniconda3/envs/nvtabular_dev_11.0/lib/python3.8/site-packages/tensorflow/python/dlpack/dlpack.py"", line 66, in from_dlpack
#     return pywrap_tfe.TFE_FromDlpackCapsule(dlcapsule, context.context()._handle)
# tensorflow.python.framework.errors_impl.InvalidArgumentError: GPU:1 unknown device.
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.4']",2021-03-17T16:28:56Z,11,0,https://github.com/tensorflow/tensorflow/issues/47866,Runtime Error
450,tensorflow/tensorflow,unique operation on strings returns bogus indices,"**System information**
- Have I written custom code: No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.9
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.8.
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1.243
- GPU model and memory: None

**Describe the current behavior**

Executing the test that runs `unique` on strings/chars returns invalid values for the index tensor. Some values are plain out of bounds (very big or negative) and others seem to point to wrong values. This very much looks like some kind of memory corruption.
So far this has only been reproducible on a cascade lake CPU, no GPUs in the system but TF was built with CUDA support.

**Describe the expected behavior**

Valid values returned

**Standalone code to reproduce the issue**

```
    import numpy as np
    from tensorflow.python.ops import array_ops

    def testString():
        indx = np.random.randint(65, high=122, size=7000)
        x = [chr(i) for i in indx]
        y, idx = array_ops.unique(x)
        tf_y, tf_idx = (y.numpy(), idx.numpy())
        print(','.join(str(i) for i in tf_idx))

        assert len(x) == len(tf_idx)
        assert len(tf_y) == len(np.unique(x))

        for i in range(len(x)):
            assert x[i] == tf_y[tf_idx[i]].decode('ascii')

    testString()
```
This test was extracted from the TF test suite as-is.
Errors include the last assert failing as well as errors like `IndexError: index 7566446 is out of bounds for axis 0 with size 57`

**Other info / logs**

For a part of the log from the TF test suite see https://gist.github.com/boegel/26f768c82080e593add3924fc7bc76cf
The `print` of `tf_idx` shows things like:
`-15829468,126746879,293189,50530320,291592,3,-16616924,58720256,66085632,66085120,1,0,7352690,946952,1538050,51149349,10,126746628,11,10,7,33583628,-16551388,126747138,126746628,50491941,101480976,126746879,16742418,14,20,752147,19,134209791,126749222,17,126747141,3840,16,2,33585676,18,387021328,70664,219117604,486963728,27,66343024,50530320,24,50756133,27,126762239,4096,7,-16223708,126749187,33977217,20,9,4,293189,20,4,40,24,23,8,13,-16025575,31,26,2375,40,22,126749187,8,19,36,6,26,126746627,22,36,133292287,133292287,44,21,34,36,13,31,26,13,24,43,10,-16354780,31,41,419459596,38,39,8,1,43,2,44,1,4,11,2,45,7,22,41,29,6,18,39,126746630,5,8,3,1,26,14,40,36,32,8,988163,25,7,42,18,3,41,50,32,2,8,22,22,17,39,18,5,21,30,14,22,34,43,43,48,6,24,50,20,32,24,29,35,8,40,9,16,18,46,50,18,7,13,31,37,47,12,2,27,39,6,37,38,44,45,29,16,0,2,31,1,29,8,26,9,30,32,66084976,29,29,29,24,15,39,35,49,37,30,0,27,39,386169362,9,41,4,7,126749211,5,24,46,4,0,49 [...]` (remaining values look ok)


To stress that: This seems to be highly system dependent. We compile with `-march=native` and see it only on the cascade lake system. The string test is the only one that fails, all other unique tests succeed.
This sounds to me like some case of undefined behavior where an optimization corrupts the absl hashmap used.","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.8']",2021-02-16T08:06:00Z,7,1,https://github.com/tensorflow/tensorflow/issues/47179,Runtime Error
451,tensorflow/tensorflow,c_api_distributed_test creates huge amount of threads and segfaults,"**System information**
- Have I written custom code: no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.7.1
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1

**Describe the current behavior**

When running `bazel test` on a system with a large physical core count the test `//tensorflow/c/eager:c_api_distributed_test` finishes and then segfaults on exit.

When I manually set `OMP_NUM_THREADS=80` the test succeeds without a segfault but at around 85 it again crashes.

I'm unable to get a stacktrace neither through TensorFlow nor through gdb and even valgrind gives up with

> valgrind: the 'impossible' happened:
   Max number of threads is too low

It then prints the stacks of 500(!) threads. In GDB I was sometimes able to catch a part of the stack pointing to libiomp from the included llvm-OpenMP, but that was difficult and hard to reproduce. Usually the process would just be terminated even when in GDB.

Something I noticed: The ThreadPool(Device) creates a large amount of threads which don't terminate until program exit. I don't think this is intended and expect this to be the cause which triggers some limitation in the OpenMP runtime.

Also the crash does not happen when not all subtests are run (via the GTest filter), excluding any of the 5 (or 6?) makes the crash disappear

**Describe the expected behavior**

Threads exit when ThreadPool is destroyed and no crash happens.

**Standalone code to reproduce the issue**

- build with bazel
- `CUDA_VISIBLE_DEVICES=-1 gdb /dev/shm//tmpzWGWuq-bazel-tf/fdff6046a749a079864ed2bee7e018bf/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/c_api_distributed_test`

**Other info / logs**
```
Executing tests from //tensorflow/c/eager:c_api_distributed_test
-----------------------------------------------------------------------------
2021-02-08 19:58:39.296267: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
Running main() from test_main.cc
[==========] Running 6 tests from 1 test suite.
[----------] Global test environment set-up.
[----------] 6 tests from CAPI
[ RUN      ] CAPI.TestLocalFunctionWithPackedInput
2021-02-08 19:58:39.510017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-02-08 19:58:39.748990: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-02-08 19:58:39.749037: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: taurusi8028
2021-02-08 19:58:39.749045: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: taurusi8028
2021-02-08 19:58:39.749373: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.32.3
2021-02-08 19:58:39.749416: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.32.3
2021-02-08 19:58:39.749430: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.32.3
2021-02-08 19:58:39.749508: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:39.796167: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}
2021-02-08 19:58:39.841581: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:50055
2021-02-08 19:58:39.841721: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:40.095546: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}
2021-02-08 19:58:40.095796: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:31398
2021-02-08 19:58:40.095865: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:40.095922: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:40.258067: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}
2021-02-08 19:58:40.406436: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}
2021-02-08 19:58:40.406478: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}
2021-02-08 19:58:40.406627: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1
2021-02-08 19:58:40.406634: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:2
2021-02-08 19:58:40.406664: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:40.406670: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:40.408781: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59685, 1 -> localhost:50055, 2 -> localhost:31398}
2021-02-08 19:58:40.409412: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:59685
2021-02-08 19:58:40.647385: I tensorflow/core/common_runtime/eager/kernel_and_device.cc:92] Ignoring error status when releasing multi-device function handle Unimplemented: Releasing a multi-device component 
handle on a remote device is not yet implemented.
[       OK ] CAPI.TestLocalFunctionWithPackedInput (1209 ms)
[ RUN      ] CAPI.TestRemoteFunctionWithPackedInput
2021-02-08 19:58:40.647938: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:40.673207: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}
2021-02-08 19:58:40.673481: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:31692
2021-02-08 19:58:40.673544: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:40.775266: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}
2021-02-08 19:58:40.778366: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:39353
2021-02-08 19:58:40.778517: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:40.778614: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:40.850707: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}
2021-02-08 19:58:40.857370: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}
2021-02-08 19:58:40.857557: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}
2021-02-08 19:58:40.857947: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:2
2021-02-08 19:58:40.857974: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:40.858061: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1
2021-02-08 19:58:40.858100: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:40.865056: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:62634, 1 -> localhost:31692, 2 -> localhost:39353}
2021-02-08 19:58:40.866478: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:62634
[       OK ] CAPI.TestRemoteFunctionWithPackedInput (367 ms)
[ RUN      ] CAPI.DistributedFunctionGraphPassOnlyOnce
2021-02-08 19:58:41.014661: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:41.024886: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}
2021-02-08 19:58:41.025155: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:44179
2021-02-08 19:58:41.025233: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:41.101530: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}
2021-02-08 19:58:41.103305: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:57750
2021-02-08 19:58:41.103445: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:41.103492: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:41.265757: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}
2021-02-08 19:58:41.272939: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}
2021-02-08 19:58:41.273025: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}
2021-02-08 19:58:41.273080: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1
2021-02-08 19:58:41.273111: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:41.273240: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:2
2021-02-08 19:58:41.273276: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:41.275488: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:45659, 1 -> localhost:44179, 2 -> localhost:57750}
2021-02-08 19:58:41.275920: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:45659
[       OK ] CAPI.DistributedFunctionGraphPassOnlyOnce (316 ms)
[ RUN      ] CAPI.DistributedFunctionNoError
2021-02-08 19:58:41.331075: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:41.405806: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}
2021-02-08 19:58:41.406048: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:34434
2021-02-08 19:58:41.406115: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:41.445328: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}
2021-02-08 19:58:41.446777: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:37620
2021-02-08 19:58:41.446932: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:41.447020: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:41.709247: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}
2021-02-08 19:58:41.713390: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}
2021-02-08 19:58:41.713392: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}
2021-02-08 19:58:41.713504: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:2
2021-02-08 19:58:41.713531: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:41.713555: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1
2021-02-08 19:58:41.713588: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:41.714723: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:59564, 1 -> localhost:34434, 2 -> localhost:37620}
2021-02-08 19:58:41.715081: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:59564
[       OK ] CAPI.DistributedFunctionNoError (448 ms)
[ RUN      ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPC
2021-02-08 19:58:41.778430: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:41.843268: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}
2021-02-08 19:58:41.843574: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:32251
2021-02-08 19:58:41.843637: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:41.843678: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:41.896621: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}
2021-02-08 19:58:41.952439: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}
2021-02-08 19:58:41.952676: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating sync eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1
2021-02-08 19:58:41.952707: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:41.953443: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:60792, 1 -> localhost:32251}
2021-02-08 19:58:41.953942: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:60792
[       OK ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPC (178 ms)
[ RUN      ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPCAsync
2021-02-08 19:58:41.956466: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:41.980731: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}
2021-02-08 19:58:41.980969: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:32519
2021-02-08 19:58:41.981026: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-08 19:58:41.981060: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:42.347899: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}
2021-02-08 19:58:42.350811: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}
2021-02-08 19:58:42.350917: I tensorflow/core/distributed_runtime/eager/eager_service_impl.cc:270] Creating async eager service context with rendezvous_id on host taurusi8028 /job:localhost/replica:0/task:1
2021-02-08 19:58:42.350946: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2021-02-08 19:58:42.358578: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:63496, 1 -> localhost:32519}
2021-02-08 19:58:42.359091: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:63496
[       OK ] CAPI.RemoteExecuteDeleteContextWithOutstandingRPCAsync (403 ms)
[----------] 6 tests from CAPI (2921 ms total)

[----------] Global test environment tear-down
[==========] 6 tests from 1 test suite ran. (2921 ms total)
[  PASSED  ] 6 tests.

  YOU HAVE 1 DISABLED TEST

*** Received signal 11 ***
*** BEGIN MANGLED STACK TRACE ***

```

(yes the log ends here, no stack trace!)","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.4']",2021-02-09T17:43:57Z,2,0,https://github.com/tensorflow/tensorflow/issues/47047,Runtime Error
452,tensorflow/tensorflow,"Missing GPU op for zeros_like for RaggedTensorVariant, error occurs when Ragged Tensor fed thru tf.map_fn","**System information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, included below
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.10
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
TensorFlow installed from (source or binary): pip binary
TensorFlow version (use command below): v1.12.1-49539-g18d8bcbe72b 2.5.0-dev20210123
Python version: '3.8.6 | packaged by conda-forge | (default, Nov 27 2020, 19:31:52) \n[GCC 9.3.0]'
Bazel version (if compiling from source): n/a
GCC/Compiler version (if compiling from source): n/a
CUDA/cuDNN version: 11.0 / 8
GPU model and memory: TITAN X (Pascal) computeCapability: 6.1

**Describe the current behavior**

I have a keras layer `RescaleB` that accepts a ragged tensor with shape [batch, (time), in_dim]. The layer calls `map_fn` to process each example in the batch separately, scaling the values along the inner dimension by a trainable gain vector. (The details of the operation aren't critical, but the ragged tensor going into map_fn is.)

Using this layer fails with `No unary variant unary_op function found for unary variant op enum: 1 Variant type_name: RaggedTensorVariant for device type: GPU` on a node whose name ends with `rescale_b/map/while/TensorArrayV2Write/TensorListSetItem_grad/zeros_like` which suggests that the zeros_like operation 
isn't defined for Ragged Tensors on GPU?

In this simple example, i also include `RescaleA`, which accomplishes the same task using `tf.ragged.map_flat_values`, although in my real use case I need `map_fn`. This is a simplified example.

**Describe the expected behavior**

I'd expect `RescaleB` and `RescaleA` to function identically.


**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/1mHycCXJL94VuCGkXIJ0bIXtbYamyZo78

I've reproduced the issue locally with tf-nightly-gpu TF 2.5, but I can't seem to get the nightly version to see the GPU on Colab. The Colab notebook is using TF 2.4, but the issue remains in TF 2.5 nightly.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

This may be the same issue as #44231 but hopefully the additional detail here is helpful. 
","['type:bug', 'comp:keras', 'comp:gpu', 'TF 2.4', 'TF 2.16']",2021-01-24T04:27:07Z,18,0,https://github.com/tensorflow/tensorflow/issues/46635,Runtime Error
453,tensorflow/tensorflow,Add Windows build to nightly libtensorflow C packages,"The [C API page](https://www.tensorflow.org/install/lang_c#nightly_libtensorflow_c_packages) says that ""libtensorflow packages are built nightly and uploaded to GCS for all supported platforms"" but the [libtensorflow-nightly GCS bucket](https://storage.googleapis.com/libtensorflow-nightly) does not have builds for Windows. The README's list of [offical builds](https://github.com/tensorflow/tensorflow#official-builds) also indicates that Windows nightlies should be available.

Please make Windows libtensorflow nightlies available for download.","['type:bug', 'comp:apis']",2021-01-19T16:41:57Z,3,0,https://github.com/tensorflow/tensorflow/issues/46538,UI/UX Bug
454,tensorflow/tensorflow,Memory leak in Conv2D/Activation on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary, the standard docker distribution
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0
- GPU model and memory: GeForce RTX 2070, 8GB

**Describe the current behavior**
I upgraded to TF 2.4.0 from TF 2.1.2, and training a very simple convolutional network, which worked fine in 2.1.2, started running out of memory during training. I distilled a simple reproducible example that demonstrates the issue. Each training epoch consumes about 50MB of additional memory and, given enough epochs, it grows to infinity (or 32 GB in my case). It only occurs on GPU, the same thing runs fine on CPU.

**Describe the expected behavior**
Memory not growing, or growing only very little

**Standalone code to reproduce the issue**
```
import gc
import os
import psutil
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense, Conv2D, Flatten, BatchNormalization, Activation

physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)


input_tensor = tf.keras.layers.Input(shape=(512,64,1))

x = Conv2D(filters=32, kernel_size=(5,5), strides=(2,2), padding='same')(input_tensor)
# Commented out on purpose - see Note 1 below
# x = BatchNormalization()(x)
x = Activation('relu')(x)

x = Conv2D(filters=64, kernel_size=(4,4), strides=(2,2), padding='same')(x)
# Commented out on purpose - see Note 1 below
# x = BatchNormalization()(x)
x = Activation('relu')(x)

x = Conv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same')(x)
# Commented out on purpose - see Note 1 below
# x = BatchNormalization()(x)
x = Activation('relu')(x)

x = Conv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same')(x)
# Commented out on purpose - see Note 1 below
# x = BatchNormalization()(x)
x = Activation('relu')(x)

x = Flatten()(x)

x = Dense(5, activation='sigmoid')(x)

model = tf.keras.Model(inputs=input_tensor, outputs=x)


train_x = np.random.random((2048, 512, 64, 1))
train_y = np.random.random((2048, 5))

model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam())

process = psutil.Process(os.getpid())

for i in range(50):
    model.fit(train_x, train_y, epochs=1, batch_size=32, verbose=0)
    gc.collect()
    print(i, process.memory_info().rss // 1000000)
```

**Note 1**
Now, if you uncomment the BatchNormalization() layers creation, the memory problem disappears. So, it is somehow caused by the Activation layer following immediately the Conv2D

**Note 2**
The memory problem also occurs if I train multiple epochs in a single fit() call, such as 
```
model.fit(train_x, train_y, epochs=50, batch_size=32)
```
I used the for loop only to be able to call garbage collection and print the memory.

**Note 3**
A Conv2D layer with activation embedded in it, such as
```
Conv2D(filters=128, kernel_size=(4,4), strides=(2,1), padding='same', activation='relu')
```
also causes the memory issue



","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.9']",2021-01-16T04:08:45Z,28,2,https://github.com/tensorflow/tensorflow/issues/46475,Performance Issue
455,tensorflow/tensorflow,tensorflow/c/eager/c_api_test fails to find GPU implementation of MatMulOp,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.7.1
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1/7.6.4.38
- GPU model and memory: GTX1080TI

**Describe the current behavior**

The 4 tests `CAPI.TensorHandleSilentCopy*` from `tensorflow/c/eager/c_api_test` fail each with 

```
tensorflow/c/eager/c_api_test.cc:442: Failure
Expected equality of these values:
  TF_GetCode(status.get())
    Which is: 3
  TF_OK
    Which is: 0
Could not satisfy device specification '/job:localhost/replica:0/task:0/device:GPU:0'. enable_soft_placement=0. Supported device types [CPU]. All available devices [/job:localhost/replica:0/task:0/device:GPU:
0, /job:localhost/replica:0/task:0/device:CPU:0].
```

Reason seems to be that `MatMulOp` is not found for GPU which is odd as seemingly everything else works.

**Standalone code to reproduce the issue**

`/tmp/ebbuild/TensorFlow/2.4.0/fosscuda-2019b-Python-3.7.4/tmp004t3B-bazel-tf/7277201245461b79db55d0e3e6d95f77/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/c/eager/c_api_test_gpu --gtest_filter=*TensorHandleSilentCopy*`

**Other info / logs** 

For some reason this test works on our POWER9 nodes which have otherwise the exact same environment (same software versions etc), but use V100s","['stat:awaiting tensorflower', 'type:bug', 'comp:eager', 'TF 2.4']",2021-01-08T16:45:27Z,2,0,https://github.com/tensorflow/tensorflow/issues/46295,Logical Bug
456,tensorflow/tensorflow,Inexact numeric jacobian causes test failures,"Using TensorFlow 2.4 with GPUs

**Describe the current behavior**

//tensorflow/python/keras/integration_test:gradients_test fails when run on a node with GPUs. I traced the problem to inexact computation in `_compute_numeric_jacobian`

Output is:
```
FAIL: testLSTMBatchJacobian (__main__.GradientsTest)
GradientsTest.testLSTMBatchJacobian
...
AssertionError: 
Not equal to tolerance rtol=0.01, atol=1e-06
Mismatched value: a is different from b. 
not close where = (array([0]), array([0]), array([2]))
not close lhs = [0.00074506]
not close rhs = [0.00076706]
not close dif = [2.20043e-05]
not close tol = [8.670623e-06]
dtype = float32, shape = (1, 1, 6)
Mismatch: 16.7%
Max absolute difference: 2.20043e-05
Max relative difference: 0.02868646
 x: array([[[-0.013396,  0.007078,  0.000745, -0.02031 ,  0.010461,
         -0.004366]]], dtype=float32)
 y: array([[[-0.0134  ,  0.007075,  0.000767, -0.020303,  0.010446,
         -0.004386]]], dtype=float32)
```

The numeric result is `array([-0.013396,  0.007078,  0.000745, -0.02031 ,  0.010461, -0.004366], dtype=float32)`
while the eager_result, function_result and backprop_result all are `y: array([-0.0134  ,  0.007075,  0.000767, -0.020303,  0.010446, -0.004386], dtype=float32)`


**Standalone code to reproduce the issue**
Reduced extracted test code:

```
import numpy as np
import tensorflow as tf

class GradientsTest(tf.test.TestCase):
  def testLSTMBatchJacobian(self):
    class HasLSTM(tf.keras.Model):

      def __init__(self):
        super(HasLSTM, self).__init__()
        self.lstm = tf.keras.layers.LSTM(units=5)
        self.dense = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)

      def call(self, x):
        return self.dense(self.lstm(x))

    m = HasLSTM()

    def jacobian(x):
      with tf.GradientTape() as tape:
        tape.watch(x)
        y = m(x)  # pylint: disable=not-callable
      return tape.batch_jacobian(y, x)

    inp = tf.nn.l2_normalize(tf.ones([1, 2, 3]), axis=[1, 2])
    eager_result = jacobian(inp)
    #function_result = tf.function(jacobian)(inp)
    #self.assertAllClose(eager_result, function_result)
    backprop_result, numeric_result = tf.test.compute_gradient(
        m, [inp], delta=1e-3)

    self.assertAllClose(numeric_result, backprop_result, rtol=1e-2)
    self.assertAllClose(tf.reshape(numeric_result, [-1]),
                        tf.reshape(eager_result, [-1]), rtol=1e-2)

if __name__ == ""__main__"":
  tf.test.main()
```


**Other info / logs**
Increasing the delta to `1e-2` makes the test pass.

Side note: Uncommenting the `tf.function` line yields a couple wrong-looking errors/warnings:
```
WARNING:tensorflow:Using a while_loop for converting CudnnRNNBackprop
pfor.py:1052] Using a while_loop for converting CudnnRNNBackprop
I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like/pfor/ZerosLike was passed float from has_lstm/lstm/PartitionedCall:6 incompatible with expected variant.
E tensorflow/core/grappler/optimizers/meta_optimizer.cc:592] function_optimizer failed: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like/pfor/ZerosLike was passed float from has_lstm/lstm/PartitionedCall:6 incompatible with expected variant.
 W tensorflow/core/common_runtime/process_function_library_runtime.cc:805] Ignoring multi-device function optimization failure: Invalid argument: Input 0 of node loop_body/PartitionedCall/pfor/PartitionedCall/gradients/zeros_like/pfor/ZerosLike was passed float from has_lstm/lstm/PartitionedCall:6 incompatible with expected variant.
```


","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.9']",2020-12-18T14:52:12Z,4,0,https://github.com/tensorflow/tensorflow/issues/45849,Runtime Error
457,tensorflow/tensorflow,Invalid index list in batch_scatter_ops_test.py ,"**Describe the current behavior**

For current master the tests in batch_scatter_ops_test.py produce an invalid index list/tensor.

Check: https://github.com/tensorflow/tensorflow/blob/dec8e0b11f4f87693b67e125e67dfbc68d26c205/tensorflow/python/kernel_tests/batch_scatter_ops_test.py#L59-L62

The comment says, non-duplicate values are required but `randint` is used which does produce duplicates. Hence the test fails.

**Standalone code to reproduce the issue**
A reduced test code which reproduces this on my machine:

```
import numpy as np
from tensorflow.python.ops import state_ops
from tensorflow.python.ops import variables
from tensorflow.python.framework import ops

def _NumpyUpdate(ref, indices, updates):
  for i, indx in np.ndenumerate(indices):
    indx = i[:-1] + (indx,)
    ref[indx] = updates[i]

def _VariableRankTest(vtype, itype):
  np.random.seed(8)
  indices_shape = (2,)
  for extra_shape in (), (5,):
    # Generate random indices with no duplicates for easy numpy comparison
    sparse_dim = len(indices_shape) - 1
    indices = np.random.randint(indices_shape[sparse_dim], size=indices_shape, dtype=itype)
    updates = np.random.randn(*(indices_shape + extra_shape)).astype(vtype)

    old = np.random.randn(*(indices_shape + extra_shape)).astype(vtype)
    print(""indices: %s"" % indices)
    if not extra_shape:
      continue

    # Scatter via numpy
    new = old.copy()
    _NumpyUpdate(new, indices, updates)
    # Scatter via tensorflow
    ref = variables.Variable(old)
    variables.variables_initializer([ref])

    #state_ops.batch_scatter_update(ref, indices, updates)
    ref.batch_scatter_update(ops.IndexedSlices(indices=indices, values=updates))
    ref = ref.numpy()
    assert np.allclose(ref, new, rtol=1e-6, atol=1e-6), ""Failed:\nlhs: %s\nrhs: %s"" % (ref, new)

_VariableRankTest(np.float32, np.int32)
```

I see an output of `indices: [1 1]` followed by:
```
AssertionError: Failed:
lhs: [[-0.37835857 -0.79161525  0.8595481  -0.23078899 -0.06566103]
 [-2.2964916   2.4098344   1.7278361   2.2045562   0.79482764]]
rhs: [[-0.37835857 -0.79161525  0.8595481  -0.23078899 -0.06566103]
 [ 0.9764211  -1.1834271   1.9163636  -1.1233268  -0.6640355 ]]
```

**Other info / logs**
tensorflow/python/kernel_tests/scatter_ops_test.py contains a valid implementation using `arange` and `shuffle` which would fix the issue

Side note: the code raises a warning:
> tensorflow/python/ops/resource_variable_ops.py:1124: batch_scatter_update (from tensorflow.python.ops.state_ops) is deprecated and will be removed after 2018-11-29.
Instructions for updating:
Use the batch_scatter_update method of Variable instead.

Note how the date is well passed but there is not even a way to avoid this as I'm already doing what is suggested.","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'regression issue', 'TF 2.11']",2020-12-17T14:34:27Z,9,0,https://github.com/tensorflow/tensorflow/issues/45789,Runtime Error
458,tensorflow/tensorflow,Out of memory in some tests due to GPU memory limit confusion,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0rc4
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): GCC 8.3.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: V100

**Describe the current behavior**

I have V100 GPUs with ~32GB memory. During startup of the test (many tests show this) I see lines like 
` W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 31614597888 on device 0 within provided limit. [used=0, limit=1073741824]`

Some tests then fail after allocating about 1GB of memory trying to allocate more. The failure message includes the 31GB and shows almost 1GB as used.
E.g. //tensorflow/python/keras/applications:applications_test or //tensorflow/python/keras/layers:convolutional_recurrent_test

**Standalone code to reproduce the issue**
Run bazel test

**Other info / logs**
```
2020-12-14 12:27:32.863559: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]
2020-12-14 12:27:32.863607: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]
2020-12-14 12:27:42.864373: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]
2020-12-14 12:27:42.864404: W tensorflow/stream_executor/stream_executor_pimpl.cc:490] Not enough memory to allocate 30637607680 on device 0 within provided limit. [used=0, limit=1073741824]
2020-12-14 12:27:42.864421: W tensorflow/core/common_runtime/bfc_allocator.cc:433] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.00MiB (rounded to 4194304)requested by op Tanh
Current allocation summary follows.
2020-12-14 12:27:42.864434: I tensorflow/core/common_runtime/bfc_allocator.cc:972] BFCAllocator dump for GPU_0_bfc
...
2020-12-14 12:27:42.866982: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Sum Total of in-use chunks: 928.00MiB
2020-12-14 12:27:42.866989: I tensorflow/core/common_runtime/bfc_allocator.cc:1042] total_region_allocated_bytes_: 976990208 memory_limit_: 31614597888 available bytes: 30637607680 curr_region_allocation_bytes_: 63229195776
2020-12-14 12:27:42.867000: I tensorflow/core/common_runtime/bfc_allocator.cc:1048] Stats: 
Limit:                     31614597888
InUse:                       973083648
MaxInUse:                    973083648
NumAllocs:                        2238
MaxAllocSize:                 86081536
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2020-12-14 12:27:42.867013: W tensorflow/core/common_runtime/bfc_allocator.cc:441] ****************************************************************************************************
2020-12-14 12:27:42.867035: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at cwise_op_gpu_base.cc:97 : Resource exhausted: OOM when allocating tensor with shape[32,32,32,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc

```","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.9', 'comp:lite-tosa']",2020-12-14T17:09:34Z,16,1,https://github.com/tensorflow/tensorflow/issues/45664,Logical Bug
459,tensorflow/tensorflow,No registered 'ResourceScatterNdUpdate' OpKernel for 'GPU' ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.0rc4
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): GCC 8.3.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: V100

**Describe the current behavior**
A test shows that a GPU implementation for BOOL inputs of ResourceScatterNdUpdate is seemingly missing.
The test is //tensorflow/python/kernel_tests:batch_scatter_ops_test -> ScatterTest.testBooleanScatterUpdate

**Standalone code to reproduce the issue**
Run bazel test

**Other info / logs**
```
ERROR: testBooleanScatterUpdate (__main__.ScatterTest)
ScatterTest.testBooleanScatterUpdate
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/batch_scatter_ops_test.py"", line 91, in testBooleanScatterUpdate
    update0 = state_ops.batch_scatter_update(var, [1], [True])
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py"", line 340, in new_func
    return func(*args, **kwargs)
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/state_ops.py"", line 915, in batch_scatter_update
    ref, final_indices, updates, use_locking=use_locking)
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/state_ops.py"", line 368, in scatter_nd_update
    name=name))
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_state_ops.py"", line 740, in resource_scatter_nd_update
    _ops.raise_from_not_ok_status(e, name)
  File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/batch_scatter_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 6862, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'ResourceScatterNdUpdate' OpKernel for 'GPU' devices compatible with node {{node ResourceScatterNdUpdate}}
	 (OpKernel was found, but attributes didn't match) Requested Attributes: T=DT_BOOL, Tindices=DT_INT32, use_locking=true
	.  Registered:  device='GPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT64]; Tindices in [DT_INT32]
  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT32]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]
  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT64]
  device='CPU'; T in [DT_UINT64]; Tindices in [DT_INT32]
 [Op:ResourceScatterNdUpdate]
```

","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'comp:gpu', 'TF 2.9']",2020-12-14T17:04:54Z,12,0,https://github.com/tensorflow/tensorflow/issues/45663,Runtime Error
460,tensorflow/tensorflow,GFile does not create file when nothing is written.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary (pip3)
- TensorFlow version (use command below): v2.3.0-54-gfcc4b966f1 2.3.1
- Python version: 3.8.2
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**

`tf.io.gfile.GFile` does nothing to the filesystem when it is closed without any write operations:
```python
with tf.io.gfile.GFile('foo', 'wb') as fp:
  fp.write('test')  # 'foo' is created.

with tf.io.gfile.GFile('bar', 'wb'):
  pass  # 'bar' is not created.
```

**Describe the expected behavior**

The same behavior with builtin `open`: creates a file even if no write operation is invoked.

**Standalone code to reproduce the issue**
Already described above.

**Other info / logs**
NA","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.9']",2020-11-25T06:04:08Z,6,0,https://github.com/tensorflow/tensorflow/issues/45171,UI/UX Bug
461,tensorflow/tensorflow,tf.debugging.assert_shapes inconsistent behaviour for scalars,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
`tf.debugging.assert_shapes` has seemingly inconsistent behaviour for scalars
```
import tensorflow as tf

tf.debugging.assert_shapes([(0, ())])  # passes as expected
tf.debugging.assert_shapes([(0, (1,))])  # passes unexpectedly
```
I think this is inconsistent, because `tf.constant(0)` does not have shape `(1,)`.

**Describe the expected behavior**
I would expect `tf.debugging.assert_shapes` to error if the `.shape` attribute is different from the shape constraint","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.15']",2020-11-16T01:24:29Z,12,0,https://github.com/tensorflow/tensorflow/issues/44888,Runtime Error
462,tensorflow/tensorflow,BUG: Keras SaveModel does not properly save optimizer state,"EDIT: looks like this is a dupe of #42749, I'll leave this up for now in case since that issue does not have as reproducible high level example, but feel free to close.

This happens at least for Adam (does not apply to SGD for example, did not test with others).

Tested on `tf-nightly` and `tf==2.3.0`.

TL;DR: running a `tf.kerasModel` through `tf.keras.models.load(model.save)` does not properly preserve the state of optimizers for certain optimizers (see #42749 for more details).

The [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model#the_savefile_includes_2) read:

> The savefile includes:
> * The model architecture, allowing to re-instantiate the model.
> * The model weights.
> * The state of the optimizer, allowing to resume training exactly where you left off.

Full example:

```python3
import numpy as np
import tensorflow as tf
from tensorflow import keras


# Define a minimal model
inp = keras.layers.Input((1, ))
out = keras.layers.Dense(1)(inp)
m1 = keras.Model(inp, out)
m1.compile(loss=""mae"", optimizer=""adam"")

# Create some test data
X, y = np.random.random((100, )), np.random.random((100, ))

# Fit the model to the test data to get everything initialized
m1.fit(X, y, verbose=0)


def roundtrip(model: keras.Model) -> keras.Model:
    save_dir = ""/tmp/mymodel""
    model.save(save_dir)
    restored = keras.models.load_model(save_dir)
    return restored


# Create a copy of the fitted m1
m2 = roundtrip(m1)

# Weights are preserved correctly, this passes
np.testing.assert_allclose(m1.predict(X), m2.predict(X))

# New lets train once more round
m1.fit(X, y, verbose=0)
# Since optimizer weights/state is not preserved, this fit call
# results in different weights in m2, which makes the predictions differ
m2.fit(X, y, verbose=0)
try:
    np.testing.assert_allclose(m1.predict(X), m2.predict(X), rtol=0.1)  # large relative tolerance
except AssertionError:
    print(""AssertionError: model predictions differ"")

# Diagnosis: optimizer weights are not preserved
weights1 = m1.optimizer.get_weights()
m3 = roundtrip(m1)
weights3 = m3.optimizer.get_weights()

try:
    assert weights1 == weights3
except AssertionError:
    print(""AssertionError: optimizer weights differ"")
    print(f""    {weights1}\n    vs\n    {weights3}"")

# Further, we can't even restore the weights without training!
try:
    m3.optimizer.set_weights(weights1)
except Exception as e:
    print(str(e).split("" Provided weights:"")[0])
```","['type:bug', 'comp:keras', 'TF 2.3']",2020-11-07T07:15:55Z,13,0,https://github.com/tensorflow/tensorflow/issues/44670,Runtime Error
463,tensorflow/tensorflow,Concrete Function output shape sometimes changes after save/load cycle,"Output of environment capture script:

```
== check python ===================================================
python version: 3.7.8
python branch:
python build version: ('default', 'Aug 10 2020 13:15:25')
python compiler version: Clang 10.0.0 (clang-1000.10.44.4)
python implementation: CPython


== check os platform ===============================================
os: Darwin
os kernel version: Darwin Kernel Version 17.7.0: Thu Dec 20 21:47:19 PST 2018; root:xnu-4570.71.22~1/RELEASE_X86_64
os release version: 17.7.0
os platform: Darwin-17.7.0-x86_64-i386-64bit
linux distribution: ('', '', '')
linux os distribution: ('', '', '')
mac version: ('10.13.6', ('', '', ''), 'x86_64')
uname: uname_result(system='Darwin', node='...', release='17.7.0', version='Darwin Kernel Version 17.7.0: Thu Dec 20 21:47:19 PST 2018; root:xnu-4570.71.22~1/RELEASE_X86_64', machine='x86_64', processor='i386')
architecture: ('64bit', '')
machine: x86_64


== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 10.0.0 (clang-1000.10.44.4)
Target: x86_64-apple-darwin17.7.0
Thread model: posix
InstalledDir: /Library/Developer/CommandLineTools/usr/bin

== check pips ===================================================
numpy                         1.18.5
protobuf                      3.10.0
tensorflow                    2.3.1
tensorflow-estimator          2.3.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.3.1
tf.version.GIT_VERSION = v2.3.0-54-gfcc4b966f1
tf.version.COMPILER_VERSION = 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.46.4)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.3.1
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: .../python3.7/site-packages
Required-by: 

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 7, 8, 'final', 0)

== bazel version  ===============================================
```

**Describe the current behavior**

The `structured_outputs` of a concrete function might not match after saving and restoring a model containing that function via tf.saved_model. In the example below, the function doesn't change the shape of the tensor, but, when requesting a concrete function for a tensor with shape [None, 3], a more specific output [7, 3] is returned instead.

**Describe the expected behavior**

The output shape should not be more specific than the true output.

**Standalone code to reproduce the issue**
Reproduction script and stdout of `for i in $(seq 10); do echo $i; python repro.py; done` in this Gist: https://gist.github.com/gmacon/057cc64bf849c8d65974daec56a037b7

**Other info / logs**
Initially, I thought this bug was deterministic, but it's clearly actually random. Based on running the reproduction script in a loop, it appears to work correctly about half of the time. Question: does this have something to do with the traversal order of a randomized hash table?

Edit again: I initially thought it was deterministic because it was happening every time in the system I'm building. However, I was reusing the same serialized model over and over in those tests. It appears to work in about half the *saves*, not half the *loads*.

~As far as I can tell, this was working correctly in Tensorflow 2.2.~ Edit: I'm not sure about that any more.

Edit again again: I reproduced this in Tensorflow 2.0.0.","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.3']",2020-10-26T19:33:30Z,10,0,https://github.com/tensorflow/tensorflow/issues/44337,UI/UX Bug
464,tensorflow/tensorflow,Unexpected output shape when trying to convert to Frozen Graph using convert_variables_to_constants_v2,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary):  Using Deep Learning VM Instance on GCP. Not sure.
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.0
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 11.0
- GPU model and memory: Tesla T4 16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I am using this [answer](https://stackoverflow.com/a/61004639/5052482) by TF_Support on StackOverflow to convert a model to to a frozen graph. However, the output shape that I am getting for my custom model is is very different (weird) from the one that's there in my model. More information in the code snippet below.

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2

def blah():
    jumps = tf.keras.layers.Input(shape=(269, 8))
    loc_a = tf.keras.layers.Input(shape=(32))
    loc_b = tf.keras.layers.Input(shape=(256))
    loc_c = tf.keras.layers.Input(shape=(2))
    def conv_block(x):
        x = tf.keras.layers.Conv1D(filters=32, kernel_size=1, padding='same')(x)
        x = tf.keras.layers.BatchNormalization()(x)
        x = tf.keras.layers.Activation('linear')(x)
        return x
    
    x = jumps
    for i in range(1):
        x = conv_block(x)
    _loc_a = tf.keras.layers.Dense(269)(loc_a)
    _loc_b = tf.keras.layers.Dense(269)(loc_b)
    _loc_c = tf.keras.layers.Dense(269)(loc_c)
    _loc = tf.keras.layers.Concatenate()([_loc_a, _loc_b, _loc_c])
    _loc = tf.keras.layers.Dense(269)(_loc)
    _loc = tf.keras.layers.Reshape((269, 1))(_loc)
    
    x = tf.keras.layers.Concatenate()([_loc, x])
    x= tf.keras.layers.Dense(9)(x)
#     x= tf.keras.layers.Dense(9)(x)
#     x = tf.keras.layers.Flatten()(x)
    
    return tf.keras.Model(inputs=[jumps, loc_a, loc_b, loc_c], outputs=[x])

model = blah()

model.summary()

Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 32)]         0                                            
__________________________________________________________________________________________________
input_3 (InputLayer)            [(None, 256)]        0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            [(None, 2)]          0                                            
__________________________________________________________________________________________________
dense (Dense)                   (None, 269)          8877        input_2[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 269)          69133       input_3[0][0]                    
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 269)          807         input_4[0][0]                    
__________________________________________________________________________________________________
input_1 (InputLayer)            [(None, 269, 8)]     0                                            
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 807)          0           dense[0][0]                      
                                                                 dense_1[0][0]                    
                                                                 dense_2[0][0]                    
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 269, 32)      288         input_1[0][0]                    
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 269)          217352      concatenate[0][0]                
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 269, 32)      128         conv1d[0][0]                     
__________________________________________________________________________________________________
reshape (Reshape)               (None, 269, 1)       0           dense_3[0][0]                    
__________________________________________________________________________________________________
activation (Activation)         (None, 269, 32)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 269, 33)      0           reshape[0][0]                    
                                                                 activation[0][0]                 
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 269, 9)       306         concatenate_1[0][0]              
==================================================================================================
Total params: 296,891
Trainable params: 296,827
Non-trainable params: 64

# convert model to a frozen graph

full_model = tf.function(lambda x: model(x))

full_model = full_model.get_concrete_function([tf.TensorSpec(model.inputs[0].shape, model.inputs[0].dtype, name=""input_0""),
                                              tf.TensorSpec(model.inputs[1].shape, model.inputs[0].dtype, name=""input_1""),
                                              tf.TensorSpec(model.inputs[2].shape, model.inputs[0].dtype, name=""input_2""),
                                              tf.TensorSpec(model.inputs[3].shape, model.inputs[0].dtype, name=""input_3"")])

frozen_func = convert_variables_to_constants_v2(full_model)
frozen_func.graph.as_graph_def(add_shapes=True)
layers = [op.name for op in frozen_func.graph.get_operations()]
print(""-"" * 50)
# print(""Frozen model layers: "")
# for layer in layers:
#     print(layer)
print(""-"" * 50)
print(""Frozen model inputs: "")
print(frozen_func.inputs)
print(""Frozen model outputs: "")
print(frozen_func.outputs)
# Save frozen graph from frozen ConcreteFunction to hard drive
tf.io.write_graph(graph_or_graph_def=frozen_func.graph,
                  logdir=""./frozen_models"",
                  name=""frozen_graph.pb"",
                  as_text=False)

Frozen model inputs: 
[<tf.Tensor 'input_0:0' shape=(None, 269, 8) dtype=float32>, <tf.Tensor 'input_1:0' shape=(None, 32) dtype=float32>, <tf.Tensor 'input_2:0' shape=(None, 256) dtype=float32>, <tf.Tensor 'input_3:0' shape=(None, 2) dtype=float32>]
Frozen model outputs: 
[<tf.Tensor 'Identity:0' shape=(None, None, 9) dtype=float32>]
'./frozen_models/frozen_graph.pb'

```

The output shape of the Frozen Model should match the output shape of the model created. However it does not. 

The output shape of the model should be `(None, 269, 9)` whereas the output shape of the Frozen Graph is `(None, None, 9)`

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
","['stat:awaiting tensorflower', 'type:bug', 'comp:core', '2.6.0']",2020-10-07T08:02:02Z,9,0,https://github.com/tensorflow/tensorflow/issues/43839,Logical Bug
465,tensorflow/tensorflow,LookupError when computing nested gradient with UpSampling2D,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 2.3.1
- Python version: 3.7.4
- CUDA/cuDNN version: 10.1
- GPU model and memory: NVIDIA GeForce GTX 1070 8GB VRAM

**Describe the current behavior**
The provided code fails with the following error (see Colab link for full stacktrace):
`LookupError: gradient registry has no entry for: ResizeNearestNeighborGrad`

**Describe the expected behavior**
The GradientTape.gradient method should be able to compute the gradient.

**Standalone code to reproduce the issue**
[Google Colab standalone code](https://colab.research.google.com/drive/1eGMvXJxVvY7zb8OISkS1H3D90N0J380d?usp=sharing)

**Other info / logs**
I encountered the error while developing a GAN. I used Conv2DTranspose for upsampling at first but encountered artifacts. After changing Conv2DTranspose to a combination of UpSampling2D and Conv2D (as is common in GAN's) the inner gradient stopped working.","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF 2.11']",2020-09-29T11:16:25Z,18,1,https://github.com/tensorflow/tensorflow/issues/43648,Runtime Error
466,tensorflow/tensorflow,TensorArray issues in XLA,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3 (exists in 1.5 as well)
- Python version: 3.7.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I know it was documented in xla official doc that xla doesn't support TensorArray with `dynamic_size=True`, however, I still ran into issues even w/ `dynamic_size=False`(it is default).

**Describe the expected behavior**
expect the tensorarray related stuffs are supported.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import numpy as np
import tensorflow as tf
tf.compat.v1.enable_eager_execution()

@tf.function(experimental_compile=True)
def test_tensor_array_scatter_gather():
    dtype = ""float32""
    t = tf.constant(np.array([[1.0], [2.0], [3.0]]).astype(dtype))
    scatter_indices = tf.constant([2, 1, 0])
    gather_indices = tf.constant([1, 2])
    ta1 = tf.TensorArray(dtype=dtype, size=3, infer_shape=True)
    ta2 = ta1.scatter(scatter_indices, t)
    t1 = ta2.gather(gather_indices)
    return t1

test_tensor_array_scatter_gather()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
","['stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.11']",2020-09-02T05:15:04Z,4,0,https://github.com/tensorflow/tensorflow/issues/42878,UI/UX Bug
467,tensorflow/tensorflow,Memory leak when using MultiWorkerMirroredStrategy for distributed training,"**System information**

- Have I written custom code: YES
- OS Platform and Distribution: CentOS 7.3
- TensorFlow installed from: pip
- TensorFlow version: 2.3.0
- Python version:3.7.7
- CPU ONLY

**Describe the current behavior**

When I use `MultiWorkerMirroredStrategy` for distributed training, as the number of training epochs increases, memory usage of tensorflow is also increasing, until beyond the memory limitation.

But the memory usage of stand-alone(not distributed) training is always stable.

Because I use cpu only for distributed training, I can't get any memory infomation from tensorboard using profiler.

**Standalone code to reproduce the issue**

Note that I don't know how to use `MultiWorkerMirroredStrategy` in `colab`, so I just give the reproduce steps here, and it's very easy.

1. Training Code (worker.py)

```python
import os
import json
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from absl import app, flags
import numpy as np

FLAGS = flags.FLAGS
flags.DEFINE_string(""logs"", ""logs"", ""logs dir"")
flags.DEFINE_integer(""index"", 0, ""worker index"")

class ThreeLayerMLP(keras.Model):
    def __init__(self, name=None):
        super().__init__(name=name)
        self.dense_1 = layers.Dense(32, activation='relu', name='dense_1')
        self.dense_2 = layers.Dense(16, activation='relu', name='dense_2')
        self.pred_layer = layers.Dense(
            1,
            activation='sigmoid',
            name='predictions',
        )

    def call(self, inputs, training=None):
        print(inputs.shape)
        x = self.dense_1(inputs)
        x = self.dense_2(x)
        return self.pred_layer(x)


def prepare_data():
    np.random.seed(0)
    x_train, y_train = (
        np.random.random((6000000, 31)),
        np.random.randint(2, size=(6000000, 1)),
    )

    x_val, y_val = (
        np.random.random((10000, 31)),
        np.random.randint(2, size=(10000, 1)),
    )

    return ((x_train, y_train), (x_val, y_val))


def main(argv):
    del argv  # Unused args
    tf_config = {
        ""cluster"": {
            ""worker"": [""ip1:12345"", ""ip2:12345""],
        },
        ""task"": {
            ""index"": FLAGS.index,
            ""type"": ""worker""
        }
    }
    os.environ[""TF_CONFIG""] = json.dumps(tf_config)
    print(json.loads(os.environ[""TF_CONFIG""]))
    # distributed strategy
    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    BATCH_SIZE_PER_REPLICA = 128
    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
    print('Number of devices: %d' % strategy.num_replicas_in_sync)

    with strategy.scope():
        model = ThreeLayerMLP(name='3_layer_mlp')
        model.compile(
            loss=tf.keras.losses.BinaryCrossentropy(),
            optimizer=keras.optimizers.RMSprop(),
            metrics=[""AUC""],
        )

    tensorboard_callback = tf.keras.callbacks.TensorBoard(
        log_dir=FLAGS.logs,
        histogram_freq=10,
        update_freq=100,
    )

    ((x_train, y_train), (x_val, y_val)) = prepare_data()

    model.fit(
        x_train,
        y_train,
        epochs=100,
        batch_size=BATCH_SIZE,
        validation_data=(x_val, y_val),
        callbacks=[tensorboard_callback],
    )


if __name__ == '__main__':
    app.run(main)
```

2. Distributed training: change the `ip1` and `ip2` to your machine's ip in the codes above, and execute the command below seperately:

```shell
python worker.py --index=0
python worker.py --index=1
```

3. The memory change curve of distributed training in my machine is shown as below：
![image](https://user-images.githubusercontent.com/15494997/91021608-94ee1c80-e626-11ea-9adc-c775b3ff575a.png)

4. The memory usage of stand-alone training is only 3-4G.","['stat:awaiting tensorflower', 'type:bug', 'comp:dist-strat']",2020-08-24T08:27:07Z,27,1,https://github.com/tensorflow/tensorflow/issues/42616,Performance Issue
468,tensorflow/tensorflow,tf.data.experimental.dense_to_ragged_batch fails with inputs from generator with unspecified shape in TF 2.3,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Both
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows + Linux (Colab)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary (PyPI)
- TensorFlow version (use command below): 2.1, 2.2, 2.3
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory: Titan RTX/P100

**Describe the expected behavior**
In TF 2.1, 2.2, and 2.3 batching variable length elements work fine when generated from tensor slices:
```python
ds = tf.data.Dataset.from_tensor_slices(tf.range(4))

# Generate variable length elements via map.
# First batch will have length 1. Subsequent batches will have length 2.
def f(x):
  if x == 0:
    return tf.ones([1,])
  else:
    return tf.ones([2,])
ds = ds.map(f)

# Inspect individual elements.
print(""Unbatched shapes:"")
for batch in ds:
  print(batch.shape)
print()

# Batch into ragged tensors.
ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=2))

# Inspect batched elements.
print(""Batched shapes:"")
for batch in ds:
  print(batch.to_tensor().shape)
```
Outputs:
```
Unbatched shapes:
(1,)
(2,)
(2,)
(2,)

Batched shapes:
(2, 2)
(2, 2)
```

Now, in TF 2.1 and 2.2, this also works when the dataset consumes elements from a generator:
```python
# Generate elements via a generator.
# First batch will have length 1. Subsequent batches will have length 2.
def gen():
  for i in range(4):
    if i == 0:
      yield tf.ones((1,))
    else:
      yield tf.ones((2,))
ds = tf.data.Dataset.from_generator(gen, output_types=tf.float32)

# Inspect individual elements.
print(""Unbatched shapes:"")
for batch in ds:
  print(batch.shape)
print()

# Batch into ragged tensors.
ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=2))

# Inspect batched elements.
print(""Batched shapes:"")
for batch in ds:
  print(batch.to_tensor().shape)
```

Outputs:
```
Unbatched shapes:
(1,)
(2,)
(2,)
(2,)

Batched shapes:
(2, 2)
(2, 2)
```
As expected, we get identical outputs both before and after batching.


**Describe the current behavior**
In TF 2.3, the generator version results in an error:
```
[...]
InvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2]. [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-313ce2c9fab5> in <module>()
     16 
     17 print(""Batched shapes:"")
---> 18 for batch in ds:
     19   print(batch.to_tensor().shape)
[...]
InvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2].
```

The release notes for 2.3 mention:

> - tf.data.experimental.dense_to_ragged_batch works correctly with tuples.
> - tf.data.experimental.dense_to_ragged_batch to output variable ragged rank.

Presumably this issue is related to these changes.

Here's the relevant implementation for the actual batching in TF 2.2:
https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/data/experimental/ops/batching.py#L371-L426

And in TF 2.3:
https://github.com/tensorflow/tensorflow/blob/b36436b087bd8e8701ef51718179037cccdfc26e/tensorflow/python/data/experimental/ops/batching.py#L380-L452

As suggested by the changes above, the behavior prior to 2.3 is achieved again when the output shape is specified, even if unknown:
```python
# Generate elements via a generator.
# First batch will have length 1. Subsequent batches will have length 2.
def gen():
  for i in range(4):
    if i == 0:
      yield tf.ones((1,))
    else:
      yield tf.ones((2,))

# Creating the generator explicitly specifying the unknown shape.
ds = tf.data.Dataset.from_generator(
    gen,
    output_types=tf.float32,
    output_shapes=tf.TensorShape([None])
)

# Inspect individual elements.
print(""Unbatched shapes:"")
for batch in ds:
  print(batch.shape)
print()

# Batch into ragged tensors.
ds = ds.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=2))

# Inspect batched elements.
print(""Batched shapes:"")
for batch in ds:
  print(batch.to_tensor().shape)
```

Outputs:
```
Unbatched shapes:
(1,)
(2,)
(2,)
(2,)

Batched shapes:
(2, 2)
(2, 2)
```

It's definitely less convenient to have to specify the output shapes in the generator, requiring some refactoring when updating to 2.3 -- maybe the shapes could just default to unknown when batching if unspecified in the generator?

I appreciate that this is an experimental function, which is noted in the [tf.data.experimental](https://www.tensorflow.org/api_docs/python/tf/data/experimental) docs:

> Note that the tf.data.experimental API is not subject to the same backwards compatibility guarantees as tf.data, but **we will provide deprecation advice in advance of removing existing functionality**.

If this is intended behavior, then perhaps it could be documented somewhere as it does remove the ""functionality"" of being able to ragged batch elements from a generator without specified output shape :)


**Standalone code to reproduce the issue**
- [Colab: TF 2.1](https://colab.research.google.com/drive/1lm9aOqtGob7PD_LMflmCONuXrpA-M1Kg?usp=sharing) (Works)
- [Colab: TF 2.2](https://colab.research.google.com/drive/1clcicwg22-DVYirFlR_IAlbWaAn-NfUf?usp=sharing) (Works)
- [Colab: TF 2.3](https://colab.research.google.com/drive/1NK5SWwqUaMsisDBjsRe8E-gpc3kcLI5S?usp=sharing) (Fails)
- [Colab: TF 2.3 with `output_shape` specified](https://colab.research.google.com/drive/1WKYYNm96xHIMfdBn_nNwAaN_BmurVuM1?usp=sharing) (Works)


**Other info / logs**
Full traceback of the error in TF 2.3:
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   2101       ctx.executor = executor_new
-> 2102       yield
   2103     finally:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)
    757             output_types=self._flat_output_types,
--> 758             output_shapes=self._flat_output_shapes)
    759 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py in iterator_get_next(iterator, output_types, output_shapes, name)
   2609     except _core._NotOkStatusException as e:
-> 2610       _ops.raise_from_not_ok_status(e, name)
   2611     except _core._FallbackException:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6842   # pylint: disable=protected-access
-> 6843   six.raise_from(core._status_to_exception(e.code, message), None)
   6844   # pylint: enable=protected-access

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2]. [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-313ce2c9fab5> in <module>()
     16 
     17 print(""Batched shapes:"")
---> 18 for batch in ds:
     19   print(batch.to_tensor().shape)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in __next__(self)
    734 
    735   def __next__(self):  # For Python 3 compatibility
--> 736     return self.next()
    737 
    738   def _next_internal(self):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in next(self)
    770   def next(self):
    771     try:
--> 772       return self._next_internal()
    773     except errors.OutOfRangeError:
    774       raise StopIteration

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)
    762         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    763       except AttributeError:
--> 764         return structure.from_compatible_tensor_list(self._element_spec, ret)
    765 
    766   @property

/usr/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)
     97                 value = type()
     98             try:
---> 99                 self.gen.throw(type, value, traceback)
    100             except StopIteration as exc:
    101                 # Suppress StopIteration *unless* it's the same exception that

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   2103     finally:
   2104       ctx.executor = executor_old
-> 2105       executor_new.wait()
   2106 
   2107 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py in wait(self)
     65   def wait(self):
     66     """"""Waits for ops dispatched in this executor to finish.""""""
---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     68 
     69   def clear_error(self):

InvalidArgumentError: Cannot batch tensors with different shapes in component 0. First element had shape [1] and element 1 had shape [2].
```
","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.11']",2020-08-14T02:27:05Z,3,1,https://github.com/tensorflow/tensorflow/issues/42349,Runtime Error
469,tensorflow/tensorflow,tf.keras cannot weight classes when using multiple outputs,"This post is a mirror of https://github.com/keras-team/keras/issues/11735, showing the need to handle class weight for multiple outputs.

Version 2.2.0 used.

------
 
This is a minimal source code, by @GalAvineri, to reproduce the issue (please comment/uncomment the class weight line):

````python3
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Input, Dense
from tensorflow.python.data import Dataset
import tensorflow as tf
import numpy as np


def preprocess_sample(features, labels):
    label1, label2 = labels
    label1 = tf.one_hot(label1, 2)
    label2 = tf.one_hot(label2, 3)
    return features, (label1, label2)


batch_size = 32

num_samples = 1000
num_features = 10

features = np.random.rand(num_samples, num_features)
labels1 = np.random.randint(2, size=num_samples)
labels2 = np.random.randint(3, size=num_samples)

train = Dataset.from_tensor_slices((features, (labels1, labels2))).map(preprocess_sample).batch(batch_size).repeat()

# Model
inputs = Input(shape=(num_features, ))
output1 = Dense(2, activation='softmax', name='output1')(inputs)
output2 = Dense(3, activation='softmax', name='output2')(inputs)
model = Model(inputs, [output1, output2])

model.compile(loss='categorical_crossentropy', optimizer='adam')
class_weights = {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}
model.fit(train, epochs=10, steps_per_epoch=num_samples // batch_size,
         #  class_weight=class_weights
          )
````

Uncommenting yields this error:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-38-d137ff6fb3f9> in <module>
     33 class_weights = {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}
     34 model.fit(train, epochs=10, steps_per_epoch=num_samples // batch_size,
---> 35            class_weight=class_weights
     36           )

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---> 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    813           workers=workers,
    814           use_multiprocessing=use_multiprocessing,
--> 815           model=self)
    816 
    817       # Container that configures and calls `tf.keras.Callback`s.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model)
   1115     dataset = self._adapter.get_dataset()
   1116     if class_weight:
-> 1117       dataset = dataset.map(_make_class_weight_map_fn(class_weight))
   1118     self._inferred_steps = self._infer_steps(steps_per_epoch, dataset)
   1119     self._dataset = strategy.experimental_distribute_dataset(dataset)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py in _make_class_weight_map_fn(class_weight)
   1233         ""Expected `class_weight` to be a dict with keys from 0 to one less ""
   1234         ""than the number of classes, found {}"").format(class_weight)
-> 1235     raise ValueError(error_msg)
   1236 
   1237   class_weight_tensor = ops.convert_to_tensor_v2(

ValueError: Expected `class_weight` to be a dict with keys from 0 to one less than the number of classes, found {'output1': {0: 1, 1: 10}, 'output2': {0: 5, 1: 1, 2: 10}}
````","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF 2.5']",2020-07-16T10:11:48Z,55,16,https://github.com/tensorflow/tensorflow/issues/41448,Runtime Error
470,tensorflow/tensorflow,Windows - tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: ,"**System information**

OS Name:                   Microsoft Windows 10 Enterprise
OS Version:                10.0.17763 N/A Build 17763
TensorFlow installed using 'conda'.
tensorflow v2.2.0-rc4-8-g2b96f3662b 2.2.0
Python 3.6.10 |Anaconda, Inc.| (default, Jan  7 2020, 15:18:16) [MSC v.1916 64 bit (AMD64)] on win32

**Describe the current behavior**

Saving checkpoint files from tensorflow is failing on Windows 10.

```
Traceback (most recent call last):
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Git\<redacted>\tests\integration\validate_train_model.py"", line 216, in <module>
    main()
  File ""C:\Git\<redacted>\tests\integration\validate_train_model.py"", line 176, in main
    fig_save_freq = fig_save_freq)
  File ""c:\git\<redacted>\src\pointnet\model.py"", line 640, in fit
    self.save_best_model()
  File ""c:\git\<redacted>\src\pointnet\model.py"", line 493, in save_best_model
    check_interval = False)
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\training\checkpoint_management.py"", line 823, in save
    self._record_state()
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\training\checkpoint_management.py"", line 728, in _record_state
    save_relative_paths=True)
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\training\checkpoint_management.py"", line 248, in update_checkpoint_state_internal
    text_format.MessageToString(ckpt))
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 532, in atomic_write_string_to_file
    rename(temp_pathname, filename, overwrite)
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 491, in rename
    rename_v2(oldname, newname, overwrite)
  File ""C:\Users\<redacted>\Miniconda3\envs\<redacted>\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 508, in rename_v2
    compat.as_bytes(src), compat.as_bytes(dst), overwrite)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: tests\files\checkpoints\0000_00_00_00_00_00\checkpoint.tmpc6ee5d6bc5a445c884bba8c3acadf01f to: tests\files\checkpoints\0000_00_00_00_00_00\checkpoint : Access is denied.
; Input/output error
```

Problem traced to:
tensorflow.python.lib.io.file_io, line 532, function atomic_write_string_to_file

From debugging, tensorflow attempts to create, then overwrite a file while saving a checkpoint.  For some reason, the 'overwrite' parameter, although set to True, does nothing.  This causes the rename to fail (since the file seems to get created earlier in the checkpoint save process).

We tried deleting the 'checkpoint' file before the 'save', but the checkpoint file that it's trying to overwrite appears to be created as a part of the 'save' call.

I was able to get checkpoint saving working again by modifying atomic_write_string_to_file as follows.  My change checks for existence of the rename target and deletes it using os.remove if overwrite is True, rather than relying on the tensorflow custom machinery that doesn't seem to be working:

```python
def atomic_write_string_to_file(filename, contents, overwrite=True):
  if not has_atomic_move(filename):
    write_string_to_file(filename, contents)
  else:
    temp_pathname = filename + "".tmp"" + uuid.uuid4().hex
    write_string_to_file(temp_pathname, contents)
    try:
      if overwrite and os.path.exists(filename):
        os.remove(filename)
      rename(temp_pathname, filename, overwrite)
    except errors.OpError:
      delete_file(temp_pathname)
      raise
```

The stack trace we got suggested that this is the same issue as someone was reporting for tensorflow.models:
https://github.com/tensorflow/models/issues/4177

**Describe the expected behavior**

We should be able to successfully save a checkpoint on Windows 10.
","['type:bug', 'comp:gpu', 'TF 2.5']",2020-07-14T17:04:28Z,31,9,https://github.com/tensorflow/tensorflow/issues/41380,Runtime Error
471,tensorflow/tensorflow,Autograph applied to Keras Custom Loss during Eager Execution,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, x64
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): TF 2.4.0.dev20200707
- Python version: 3.7
- CUDA/cuDNN version: 10.1 / 7.6
- GPU model and memory: Bug appears on several computers with different GPU


**Describe the current behavior**
Tensorflow applies AutoGraph to keras custom loss even in eager execution, meaning that we can't debug the loss anymore (unless using tf.print). This did not happen in previous versions of Tensorflow.
Notice that it both happens when run_eagerly is set to True in model.compile() and when tf.config.run_functions_eagerly is set to True.

**Describe the expected behavior**
When run_eagerly=True is passed to the model during compilation, we should expect Tensorflow to run eagerly in the loss function.

**Standalone code to reproduce the issue**

```
import numpy as np
import tensorflow as tf
from tensorflow import keras

# Custom Model. Autograph is not applied in eager execution so debugging is possible.
class CustomModel(keras.models.Model):
	def __init__(self):
		super(CustomModel, self).__init__()
		self.layer = tf.keras.layers.Dense(3) # Can debug here

	def call(self, inputs, training=None, mask=None):
		x = self.layer(inputs) # Can debug here
		return x

# Custom Loss. AutoGraph is applied in eager execution so debugging is impossible.
class CustomLoss(keras.losses.Loss):
	def call(self, y_true, y_pred):
		x = tf.reduce_mean(tf.abs(y_pred-y_true)) # Cannot debug here
		return x

if __name__ == '__main__':
	data = np.random.random((1000, 3)).astype(np.float32)

	model = CustomModel()

	model.compile(loss=CustomLoss(), run_eagerly=True)
	model.fit(x=data, y=data, batch_size=32)
```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
","['type:bug', 'comp:keras', 'comp:autograph', 'TF 2.2']",2020-07-08T09:06:35Z,19,0,https://github.com/tensorflow/tensorflow/issues/41189,Logical Bug
472,tensorflow/tensorflow,Zero AUROC when validation set is only comprised of positive labels,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Tested on Ubuntu 18.04 and MacOS Catalina 10.15.5
- TensorFlow installed from (source or binary): installed from Conda (tensorflow-gpu) on Ubuntu and from pip on MacOS
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.7.6 (on both machines)
- CUDA/cuDNN version: 11.0 (on Ubuntu machine)
- GPU model and memory: 4 NVIDIA V100 GPUs

**Describe the current behavior**
The AUROC is zero when there are no negatives in the validation set. This may lead users to believe that the model is not performing correctly, while it is just an error related to a misuse of the AUROC metric.

**Describe the expected behavior**
The AUROC should be NaN when no negatives are present in the validation set. This way the user is aware that there is something wrong with the metric that is being used, with the possibility to add a warning since there is a 0/0 somewhere in the computation of the AUROC.

**Standalone code to reproduce the issue**
```python
from tensorflow.keras.metrics import AUC
metric = AUC()
metric.update_state([1, 1, 1], [1, 0.5, 0.3])
metric.result()
```

I believe that similar issues also happen for other metrics, as uncanny values were displayed also for Recall and Precision, but I have not identified a simple reproducible example.","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF 2.11']",2020-07-04T13:03:01Z,4,0,https://github.com/tensorflow/tensorflow/issues/41081,Runtime Error
473,tensorflow/tensorflow,“Layer is not connected” issue while accessing intermediate layer from custom callback if model is built by sub-classing,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version:
Python 3.7.3
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
CUDA 10.2
- GPU model and memory:
NVIDIA TITAN X (Pascal), ~12GB

**Describe the current behavior**
I've a simple model and need access of intermediate layers within a custom callback to get intermediate predictions. If I build the model by sub-classing, I get the error `AttributeError: Layer dense is not connected`.

**Describe the expected behavior**
It shouldn't cause any error and be able to get predictions using intermediate layers.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

X = np.ones((8,16))
y = np.sum(X, axis=1)

class CustomCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        get_output = tf.keras.backend.function(
            inputs = self.model.layers[0].input,
            outputs = self.model.layers[1].output
        )
        print(""\nLayer output: "", get_output(X))

class Model(tf.keras.Model):
    def build(self, input_shape):
        self.dense1 = tf.keras.layers.Dense(units=32)
        self.dense2 = tf.keras.layers.Dense(units=1)
        
    def call(self, input_tensor):
        x = self.dense1(input_tensor)
        x = self.dense2(x)
        return x

model = Model()
model.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')
model.fit(X,y, epochs=2, callbacks=[CustomCallback()])
```

**Other info / logs** 
Traceback:
```---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-3-dd6e118e08d6> in <module>
     11 model = Model()
     12 model.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')
---> 13 model.fit(X,y, epochs=2, callbacks=[CustomCallback()])

/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     64   def _method_wrapper(self, *args, **kwargs):
     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---> 66       return method(self, *args, **kwargs)
     67 
     68     # Running inside `run_distribute_coordinator` already.

/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    874           epoch_logs.update(val_logs)
    875 
--> 876         callbacks.on_epoch_end(epoch, epoch_logs)
    877         if self.stop_training:
    878           break

/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_epoch_end(self, epoch, logs)
    363     logs = self._process_logs(logs)
    364     for callback in self.callbacks:
--> 365       callback.on_epoch_end(epoch, logs)
    366 
    367   def on_train_batch_begin(self, batch, logs=None):

<ipython-input-2-a1f33c1e2e52> in on_epoch_end(self, epoch, logs)
      8     def on_epoch_end(self, epoch, logs=None):
      9         get_output = tf.keras.backend.function(
---> 10             inputs = self.model.layers[0].input,
     11             outputs = self.model.layers[1].output
     12         )

/opt/anaconda3/envs/brats/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in input(self)
   1806     if not self._inbound_nodes:
   1807       raise AttributeError('Layer ' + self.name +
-> 1808                            ' is not connected, no input to return.')
   1809     return self._get_node_attribute_at_index(0, 'input_tensors', 'input')
   1810 

AttributeError: Layer dense is not connected, no input to return.
```
If I build the model using functional API as shown below, it works fine:
```
initial = tf.keras.layers.Input((16,))
x = tf.keras.layers.Dense(units=32)(initial)
final = tf.keras.layers.Dense(units=1)(x)

model = tf.keras.Model(initial, final)
model.compile(optimizer='adam',loss='mean_squared_error', metrics='accuracy')
model.fit(X,y, epochs=2, callbacks=[CustomCallback()])
```
[Here's](https://stackoverflow.com/q/62668398/2679778) the stackoverflow question I created on the same issue.","['stat:awaiting tensorflower', 'type:bug', 'comp:keras', 'TF 2.11']",2020-07-01T23:15:59Z,5,1,https://github.com/tensorflow/tensorflow/issues/41009,Runtime Error
474,tensorflow/tensorflow,tf.nn.ctc_beam_search_decoder does not pick path with highest probability at next time step,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Colab Env (Linux Ubuntu 18.04)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
Binary (I think)
- TensorFlow version (use command below):
v2.2.0-0-g2b96f3662b 2.2.0
- Python version:
3.6.9
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
None

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

The built-in CTC beam search decoder sometimes chooses a less probable path to keep in the beam by default than it could've when expanding a given step. In the Colab example provided below, I give a concrete example of when the path (0,) is retained in the beam instead of (0, 1, 0) despite the latter having a greater log-probability.

**Describe the expected behavior**

(0, 1, 0) should remain in the beam; (0,) should not.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1i9gvj0VN2gMNloohbHW6ad3Ti7eiQQCM?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

When I was comparing against a simple python version [based off this Medium article](https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7), I noticed that whenever the results diverged, the root problem was always that a path that should've been kept in the beam wasn't. I suspect top-k sorting might be mucking up somewhere.

Thanks,
Sean
","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.4']",2020-06-23T20:25:10Z,8,0,https://github.com/tensorflow/tensorflow/issues/40727,UI/UX Bug
475,tensorflow/tensorflow,tf.io.gfile / GCS fails to work on OpenSUSE,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux OpenSUSE Tumbleweed
- TensorFlow installed from (source or binary): Binary (conda)
- TensorFlow version (use command below): unknown 2.1.0
- Python version: 3.7.5
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
```python
import tensorflow as tf
tf.io.gfile.listdir(""gs://some-bucket"") # replace w/ bucket of your choice
```

This code gives an error:
```
2020-06-01 15:43:56.684531: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Unavailable: Error executing an HTTP request: libcurl code 77 meaning 'Problem with the SSL CA cert (path? access rights?)', error details: error setting certificate verify locations:
  CAfile: /etc/ssl/certs/ca-certificates.crt
  CApath: none"". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".
```

After that it hangs for a while, and then raises a `NotFoundError`.

I believe this is because the libcurl packaged with tensorflow doesn't know where to find the ca-certificates bundle file on OpenSUSE, which is at `/etc/ssl/ca-bundle.pem` rather than `/etc/ssl/certs/ca-certificates.crt`. Also, I installed through miniconda so there's another equivalent file at `$CONDA_PREFIX/ssl/cacert.pem`. Neither of these seems to be found by tensorflow.

[This code](https://github.com/tensorflow/tensorflow/blob/5597c17b6a677be5264ebda7cc31404f0ae8a434/tensorflow/core/platform/cloud/curl_http_request.cc#L129-L132) suggests that the bundle file's location can be customized with the `CURL_CA_BUNDLE` env variable. However, this doesn't change the behavior as far as i can tell; the error is still raised.

**Describe the expected behavior**
It should list the contents of the bucket.","['type:bug', 'comp:core', 'TF 2.11']",2020-06-01T20:05:13Z,18,1,https://github.com/tensorflow/tensorflow/issues/40065,Runtime Error
476,tensorflow/tensorflow,Unable to log scalar summaries in XLA,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5 Beta
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.8.2
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
Forcing XLA compilation via `experimental_compile=True` in a `tf.function` raises `tensorflow.python.framework.errors_impl.InvalidArgumentError` when the function contains `tf.summary.scalar` (I haven't tried other summaries). 

**Describe the expected behavior**
Passing `experimental_compile=True` should log the scalar without raising any errors. 

**Standalone code to reproduce the issue**
```python
import tensorflow as tf


@tf.function(
    experimental_compile=True,
)
def test_summaries():
    tf.summary.scalar('testing', 12.3)

with tf.summary.create_file_writer('./logs').as_default():
    tf.summary.experimental.set_step(0)
    test_summaries()
```

**Other info / logs**
```
2020-05-22 22:30:57.072264: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-22 22:30:57.091548: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f99eef80060 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-22 22:30:57.091590: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File ""testsum.py"", line 12, in <module>
    test_summaries()
  File ""/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 576, in __call__
    result = self._call(*args, **kwds)
  File ""/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 650, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1661, in _filtered_call
    return self._call_flat(
  File ""/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1745, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 593, in call
    outputs = execute.execute(
  File ""/private/tmp/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: Function invoked by the following node is not compilable: {{node __inference_test_summaries_22}} = __inference_test_summaries_22[_XlaMustCompile=true, config_proto=""\n\007\n\003GPU\020\000\n\007\n\003CPU\020\0012\002J\0008\001"", executor_type=""""](dummy_input).
Uncompilable nodes:
testing/write_summary/tag: unsupported op: Const op with type DT_STRING is not supported by XLA.
	Stacktrace:
		Node: __inference_test_summaries_22, function: 
		Node: testing/write_summary/tag, function: __inference_test_summaries_22

testing/write_summary/summary_metadata: unsupported op: Const op with type DT_STRING is not supported by XLA.
	Stacktrace:
		Node: __inference_test_summaries_22, function: 
		Node: testing/write_summary/summary_metadata, function: __inference_test_summaries_22

testing/write_summary: unsupported op: No registered 'WriteSummary' OpKernel for XLA_CPU_JIT devices compatible with node {{node testing/write_summary}}
	Stacktrace:
		Node: __inference_test_summaries_22, function: 
		Node: testing/write_summary, function: __inference_test_summaries_22
 [Op:__inference_test_summaries_22]
```
","['comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug', 'comp:xla', 'TF 2.11']",2020-05-23T02:34:43Z,4,0,https://github.com/tensorflow/tensorflow/issues/39798,Runtime Error
477,tensorflow/tensorflow,tf.Module.name_scope overrides parent scopes,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: macOS 10.15.5
- Mobile device if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
The `name_scope` of a `tf.Module` instance has a forward slash appended to the name, which causes the scope to disregard any parent scopes. Using `with tf.Module.name_scope` and `with tf.name_scope(mymod.name_scope.name):` both result in this issue. 

**Describe the expected behavior**
Each of the following should have the same outcome:
- `with tf.Module.name_scope:`
- `with tf.name_scope(tf.Module.name_scope.name):`
- `with tf.name_scope(tf.Module.name):`

**Standalone code to reproduce the issue**
```python
import tensorflow as tf


def my_test_mod_scopes():
    mymod = tf.Module(name='mymod')
    with tf.name_scope('first_scope'):
         with mymod.name_scope:
             tf.summary.scalar('scalar1', 84.9)
         with tf.name_scope(mymod.name):
             tf.summary.scalar('scalar2', 74.3)
         with tf.name_scope(mymod.name_scope.name):
             tf.summary.scalar('scalar3', 79.7)

with tf.summary.create_file_writer('./logs').as_default():
    tf.summary.experimental.set_step(0)
    my_test_mod_scopes()
```
![Screen Shot 2020-05-07 at 21 43 20](https://user-images.githubusercontent.com/31281983/81361269-e7bdbb80-90ab-11ea-84b9-4d41f84250b8.png)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
","['comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug', 'TF 2.5']",2020-05-08T01:45:08Z,3,0,https://github.com/tensorflow/tensorflow/issues/39289,Runtime Error
478,tensorflow/tensorflow,tf.io.gfile.listdir is inconsistent between GCS dir and local dir - adds trailing slashes,"
**System information**
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.6

**Describe the current behavior**

When listing the directory items for a GCS dir, listdir returns names with trailing slashes. 
When listing the directory items for a local dir, listdir returns names without trailing slashes. 

**Describe the expected behavior**

The behavior needs to be consistent.

**Standalone code to reproduce the issue**

```
tensorflow.io.gfile.listdir('gs://bucket/dir')
>>> ['eval/', 'train/']

tensorflow.io.gfile.listdir('..')
>>> ['eval', 'train']
```","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.7']",2020-05-04T21:00:50Z,22,0,https://github.com/tensorflow/tensorflow/issues/39167,Logical Bug
479,tensorflow/tensorflow,No default summary writer available when using tf.py_function with autograph,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 3.6.8
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5 Beta
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
Using `tf.summary` returns `False` inside a `tf.py_function` when using autograph.

**Describe the expected behavior**
`tf.summary` should return `True`.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf


@tf.function
def eager_pyfunc():
    def inner_func():
        bool_out = tf.summary.scalar('myscalar', tf.constant(32.9))
        tf.print(bool_out, name='log-myscalar-success')

    tf.py_function(inner_func, [], [], name='log-myscalar')

    bool_out2 = tf.summary.scalar('myscalar2', tf.constant(52.3))
    tf.print(bool_out2, name='log-myscalar2-success')


with tf.summary.create_file_writer('./logs').as_default():
    tf.summary.experimental.set_step(0)
    eager_pyfunc()
```
outputs:
```
0
1
```
Removing the `tf.function` outputs:
```
1
1
```

**Other info / logs** 
Could be related to https://github.com/tensorflow/tensorflow/issues/26409.","['comp:tensorboard', 'stat:awaiting tensorflower', 'type:bug', 'TF 2.12']",2020-04-21T22:29:48Z,4,0,https://github.com/tensorflow/tensorflow/issues/38772,Logical Bug
480,tensorflow/tensorflow,TPU PyFunction results in UnavailableError: failed to connect to all addresses,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Modified [Colab MNIST guide](https://www.tensorflow.org/guide/tpu)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow version (use command below): `2.2-rc3`

**Describe the current behavior**
When processing pipeline for `tf.data.Dataset` contains usage of `tf.py_function` the `UnavailableError: failed to connect to all addresses` is thrown on TPU environment.

**Describe the expected behavior**
`tf.py_function` is working on TPU environments. 

**Standalone code to reproduce the issue**
[Colab notebook](https://colab.research.google.com/drive/1D7qU4f1FZqieYHdyUezUEFPWoVZZJSxi) with simplified example. In my original code the preprocessing function is more complicated. 

**Other info / logs**
Related issue: [34346](https://github.com/tensorflow/tensorflow/issues/34346).
Stacktrace:
```
---------------------------------------------------------------------------
UnavailableError                          Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   1985       ctx.executor = executor_new
-> 1986       yield
   1987     finally:

14 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)
    660       except AttributeError:
--> 661         return structure.from_compatible_tensor_list(self._element_spec, ret)
    662 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py in from_compatible_tensor_list(element_spec, tensor_list)
    229       lambda spec, value: spec._from_compatible_tensor_list(value),
--> 230       element_spec, tensor_list)
    231 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py in _from_tensor_list_helper(decode_fn, element_spec, tensor_list)
    204     value = tensor_list[i:i + num_flat_values]
--> 205     flat_ret.append(decode_fn(component_spec, value))
    206     i += num_flat_values

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py in <lambda>(spec, value)
    228   return _from_tensor_list_helper(
--> 229       lambda spec, value: spec._from_compatible_tensor_list(value),
    230       element_spec, tensor_list)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_spec.py in _from_compatible_tensor_list(self, tensor_list)
    176     assert len(tensor_list) == 1
--> 177     tensor_list[0].set_shape(self._shape)
    178     return tensor_list[0]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in set_shape(self, shape)
   1103   def set_shape(self, shape):
-> 1104     if not self.shape.is_compatible_with(shape):
   1105       raise ValueError(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in shape(self)
   1066       except core._NotOkStatusException as e:
-> 1067         six.raise_from(core._status_to_exception(e.code, e.message), None)
   1068 

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

UnavailableError: failed to connect to all addresses
Additional GRPC error information:
{""created"":""@1587494349.376555159"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3959,""referenced_errors"":[{""created"":""@1587494349.376552078"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]}

During handling of the above exception, another exception occurred:

UnavailableError                          Traceback (most recent call last)
<ipython-input-8-f9a6a321af70> in <module>()
      1 train_dataset, test_dataset = get_dataset()
----> 2 list(train_dataset.take(1))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in __next__(self)
    629 
    630   def __next__(self):  # For Python 3 compatibility
--> 631     return self.next()
    632 
    633   def _next_internal(self):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in next(self)
    668     """"""Returns a nested structure of `Tensor`s containing the next element.""""""
    669     try:
--> 670       return self._next_internal()
    671     except errors.OutOfRangeError:
    672       raise StopIteration

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)
    659         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    660       except AttributeError:
--> 661         return structure.from_compatible_tensor_list(self._element_spec, ret)
    662 
    663   @property

/usr/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)
     97                 value = type()
     98             try:
---> 99                 self.gen.throw(type, value, traceback)
    100             except StopIteration as exc:
    101                 # Suppress StopIteration *unless* it's the same exception that

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   1987     finally:
   1988       ctx.executor = executor_old
-> 1989       executor_new.wait()
   1990 
   1991 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py in wait(self)
     65   def wait(self):
     66     """"""Waits for ops dispatched in this executor to finish.""""""
---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     68 
     69   def clear_error(self):

UnavailableError: failed to connect to all addresses
Additional GRPC error information:
{""created"":""@1587494349.376555159"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3959,""referenced_errors"":[{""created"":""@1587494349.376552078"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]}
```","['stat:awaiting tensorflower', 'type:bug', 'comp:tpus', 'TF 2.9']",2020-04-21T18:42:26Z,33,20,https://github.com/tensorflow/tensorflow/issues/38762,Runtime Error
481,tensorflow/tensorflow,tf.name_scope with spaces does not raise ValueError in eager execution ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.5 Beta
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.8
- Bazel version (if compiling from source):  n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
In autograph, `tf.name_scope` does not allow spaces in the string argument. In eager execution, `tf.name_scope` *does* allow spaces in the string argument. 

**Describe the expected behavior**
The constraints on `tf.name_scope` should be the same across eager execution and autograph. 

**Standalone code to reproduce the issue**
```python
import tensorflow as tf


# eager execution since there's no @tf.function decorator
def graphcry():
    myscalar = tf.constant(83.2)  # just a random number

    with tf.name_scope('scalaragain scope'):  # doesn't work
        tf.summary.scalar('scalaragain', data=myscalar)

    with tf.name_scope('nospace_scope'):  # works
        tf.summary.scalar('nospace', data=myscalar)

graphcry()
```

**Other info / logs**
CC @jvishnuvardhan, also see #38661 
","['stat:awaiting tensorflower', 'type:bug', 'comp:eager', 'comp:ops', 'TF 2.9']",2020-04-21T14:48:08Z,6,0,https://github.com/tensorflow/tensorflow/issues/38754,Runtime Error
482,tensorflow/tensorflow,TF saved model Assertion Error ( Called a function referencing variables which have been deleted ),"I am facing an issue . When returning a ```tf.saved_model.load``` object inside a function and then try to use it, it is not working. 

I am having a file ```sample.py``` 
```
#### sample.py

import tensorflow as tf
def load_model(model_dir):

    # Load Model
    loaded = tf.saved_model.load(model_dir)
    model = loaded.signatures['serving_default']
    print(""Model Loaded"")
    return model

```

When I am executing ```main.py```

```
from sample import load_model

model_dir = 'som_path of a saved model'
model1 = load_model(model_dir)
```

If I print model.variables I am getting following error

```
AssertionError: Called a function referencing variables which have been deleted. This likely means that function-local variables were created and not referenced elsewhere in the program. This is generally a mistake; consider storing variables in an object attribute on first call.

```

But. If load the model with same code inside the function, but not using the function it works fine

```
#### main.py
loaded = tf.saved_model.load(model_dir)
model = loaded.signatures['serving_default']
```
If I print model.variables, its working as expected. ","['type:bug', 'comp:apis', 'TF 2.11']",2020-03-15T16:25:56Z,26,5,https://github.com/tensorflow/tensorflow/issues/37615,Runtime Error
483,tensorflow/tensorflow,tf.data.Dataset unusable with steps_per_epoch standard training loop,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.4

**Describe the current behavior**

This is the underlying issue for https://github.com/tensorflow/tensorflow/issues/36153

Basically:
- `Dataset` can be used as/converted to an iterator
- Once the iterator reaches the end of the dataset it does not restart -> Yields no more samples
- `steps_per_epoch` for the `fit` method of a keras model can be used to specify the number of batches to run per epoch, more exactly: The number of times the iterator is advanced/dereferenced
- `steps_per_epoch` is required for e.g. if not the full dataset should be traversed. Either due to external requirements or to avoid using the trailing (incomplete) batch.
- `steps_per_epoch` is absolutely required to be set for `MultiWorkerMirroredStrategy`: https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy
- Whether to recreate an iterator for each epoch out of the dataset is determined by `DatasetAdapter.should_recreate_iterator` at https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/engine/training_v2.py#L241-L242
- Note that the iterator absolutely must be recreated for all common use cases:
  - Iterate over full dataset
  - Iterate over full dataset except incomplete trailing batch
  - Iterate over a random subset of the full dataset per epoch
  - Not to be recreated for: Infinite datasets. **Maybe** If the full dataset should be consumed over multiple epochs. But why? What should happen if the dataset is exhausted after some epoch? Usually restart, right?

In the current TF (2.1.0) the implementation of `DatasetAdapter.should_recreate_iterator` is: `return self.get_size() is not None or steps_per_epoch is None`: https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/engine/data_adapter.py#L208

This is **wrong**. For datasets the size is always None (see https://github.com/tensorflow/tensorflow/issues/36531 which intends for this to be changed) and as motivated above having `steps_per_epoch` set is a common use case but the iterator should still be recreated on each epoch.

This was recently changed to ` (self._user_steps is None or cardinality.cardinality(self._dataset).numpy() == self._user_steps)` https://github.com/tensorflow/tensorflow/commit/6be131d0860559954c42685a87c63f16cebb2185#diff-f8dd40712ac721c1b363e1a1ec44c1a3R741-R747

This is also **wrong**. Again assume `_user_steps` is set (see above reasoning): First the `cardinality` might be unknown, e.g. for any TFDS dataset (and `TFRecordDataset`) it is unknown. I guess due to use of `interleave` in the dataset reader. Second even if the size was known it may not be equal to the number of steps. Common example: Skipping the last batch.

**Describe the expected behavior**

This is a design issue and hence hard to resolve.

In general it would be best to eliminate the UNKNOWN size of a dataset. But when reading data line-by-line from a file it might not be known upfront. So the user has to input the size of the dataset or specify explicitly `AUTO` which would iterate over the whole dataset once to get the number of samples. This can be costly but should not be possible in general (e.g. TFDS knows the number of samples)

I think the sanest approach would be to default to recreating the iterator on each epoch UNLESS the dataset is known to be infinite. This might still be wrong for cases I can't imagine right now but is correct for all cases I can think of. Maybe even allow the user to specify this, but this default is IMO way better than the current.

The other approach would be to recreate the iterator when it runs out of data after starting an epoch. This would partially solve the issue, but fails for:
- Omitting the trailing batch: It would yield the incomplete batch from the last epoch first, but that should be skipped.
- Using only some random samples per batch but wanting to shuffle before each batch: It would happily consume the rest of the samples and not see the samples used in an earlier epoch until it runs out of data

With the UNKNOWN size fixed, one could also fix the check at  https://github.com/tensorflow/tensorflow/commit/6be131d0860559954c42685a87c63f16cebb2185#diff-f8dd40712ac721c1b363e1a1ec44c1a3R741-R747 to check if `_user_steps` yields 1 epoch (and optional trailing batches) by using `size // steps == 1` which would be better than the previous approach (recreate iterator when out of data) as the use case with of Omitting the trailing batch is covered. But it would fail for the other.

So my suggestion would to
- (optional) avoid UNKNOWN sizes
- recreate iterators unless dataset is infinite by default
- allow the user to overwrite this explicitly, maybe via `with_options` of the dataset


**Code to reproduce the issue**
Some reduced example code based on e.g. https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#train_the_model_with_multiworkermirroredstrategy

```
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow as tf
from tensorflow.python.data.experimental.ops import cardinality
import numpy as np
tfds.disable_progress_bar()

# Scaling MNIST data from (0, 255] to (0., 1.]
def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

def build_and_compile_cnn_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32,
                               3,
                               activation='relu',
                               input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,
                  optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
                  metrics=['accuracy'])
    return model

BATCH_SIZE = 64
if False:
    examples = np.ones([10*BATCH_SIZE,28,28,1])
    labels = np.ones([examples.shape[0]])
    dataset = tf.data.Dataset.from_tensor_slices((examples, labels))
    num_examples = examples.shape[0]
else:
    datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
    dataset = datasets['test']
    num_examples = info.splits['test'].num_examples

x = dataset.map(scale).cache().shuffle(10000).batch(BATCH_SIZE)
model = build_and_compile_cnn_model()

card = cardinality.cardinality(x)
num_batches = sum(1 for _ in x)
full_batches = num_examples // BATCH_SIZE
print(""Samples: %s\nBatches: %s (%s full)\nCardinality: %s"" % (num_examples, num_batches, full_batches, card))
model.fit(x=x, epochs=2, steps_per_epoch=full_batches)

```

**Other info / logs**
There is a warning:
> WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 312 batches). You may need to use the repeat() function when building your dataset.

It seems that adding `repeat()` and hence creating an infinite dataset is a viable option. However the docu also states
> In TF 1.X, the idiomatic way to create epochs was through the `repeat` transformation:
>     In TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it possible to also create epochs through Python iteration:

So it seems that it should not be required and as per above explanation the return value of `DatasetAdapter.should_recreate_iterator` is not correct.","['stat:awaiting tensorflower', 'type:bug', 'comp:data', 'TF 2.10']",2020-02-07T13:30:41Z,14,2,https://github.com/tensorflow/tensorflow/issues/36539,Logical Bug
484,tensorflow/tensorflow,How can I clear GPU memory in tensorflow 2?,"

### System information
- Custom code; nothing exotic though.
- Ubuntu 18.04
- installed from source (with pip)
- tensorflow version v2.1.0-rc2-17-ge5bf8de
- 3.6
- CUDA 10.1
- Tesla V100, 32GB RAM

I created a model, nothing especially fancy in it. When I create the model, when using nvidia-smi, I can see that tensorflow takes up nearly all of the memory. When I try to fit the model with a small batch size, it successfully runs. When I fit with a larger batch size, it runs out of memory. Nothing unexpected so far. 

However, the only way I can then release the GPU memory is to restart my computer. When I run nvidia-smi I can see the memory is still used, but there is no process using a GPU. Also, If I try to run another model, it fails much sooner. 

Nothing in the first five pages of google results works. (and most solutions are for TF1)

Is there any way to release GPU memory in tensorflow 2?","['stat:awaiting tensorflower', 'type:bug', 'comp:gpu', 'TF 2.7']",2020-02-04T15:16:15Z,123,83,https://github.com/tensorflow/tensorflow/issues/36465,Logical Bug
485,tensorflow/tensorflow,AttributeError: 'Tensor' object has no attribute 'log_prob',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina (Version: 10.15.2 (19C57))
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1
- Python version: 3.7.5
- GPU model and memory: Intel Iris Pro 1536 MB

**Describe the current behavior**

I get the error

> AttributeError: 'Tensor' object has no attribute 'log_prob'

with TensorFlow Probability 0.9 (and TF 2.1).

**Describe the expected behavior**

No error.

**Code to reproduce the issue**

The following code

```
import tensorflow as tf
import tensorflow_probability as tfp
from tensorflow_probability import distributions as tfd


def get_mnist_data(normalize=True):
    img_rows, img_cols = 28, 28
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

    if tf.keras.backend.image_data_format() == 'channels_first':
        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)
        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)
        input_shape = (1, img_rows, img_cols)
    else:
        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
        input_shape = (img_rows, img_cols, 1)

    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')

    if normalize:
        x_train /= 255
        x_test /= 255

    return x_train, y_train, x_test, y_test, input_shape


def get_bayesian_cnn(input_shape, num_classes=10):
    model_input = tf.keras.layers.Input(shape=input_shape)

    # kernel_divergence_fn=None to solve a symbolic exception.
    x = tfp.layers.Convolution2DFlipout(6, kernel_size=(5, 5), padding=""SAME"", activation=tf.nn.relu,
                                        kernel_divergence_fn=None)(model_input)
    x = tf.keras.layers.Flatten()(x)
    x = tfp.layers.DenseFlipout(84, activation=tf.nn.relu)(x)
    x = tfp.layers.DenseFlipout(num_classes)(x)

    model_output = tfp.layers.DistributionLambda(lambda t: tfd.Categorical(logits=t, validate_args=True))(x)

    model = tf.keras.Model(model_input, model_output)

    return model


def neg_log_likelihood(y_true, y_pred):
    return -tf.reduce_mean(y_pred.log_prob(tf.cast(tf.argmax(y_true, axis=-1), tf.int32)))


def train():
    x_train, y_train, x_test, y_test, input_shape = get_mnist_data()

    model = get_bayesian_cnn(input_shape=input_shape)

    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=neg_log_likelihood,
                  metrics=[neg_log_likelihood])

    model.fit(x_train, y_train, batch_size=128, epochs=1, verbose=1)


if __name__ == ""__main__"":
    train()
```

**Comments**

This error seems to be due to the fact that `y_pred` is a tensor when the loss is called, while it should be a distribution. Meanwhile, I found a [question on Stack Overflow related to the third issue I mentioned above](https://stackoverflow.com/q/59743872/3924118). 

(_This is a duplicate issue of https://github.com/tensorflow/probability/issues/742, but, for completeness, I decided to open it here too._)","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.11']",2020-01-24T13:50:40Z,15,0,https://github.com/tensorflow/tensorflow/issues/36181,Runtime Error
486,tensorflow/tensorflow,tf.io.gfile.glob does not list all files in a Google Cloud Storage bucket,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): ?
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0
- Python version: 3
- Bazel version (if compiling from source): /
- GCC/Compiler version (if compiling from source): /
- CUDA/cuDNN version: /
- GPU model and memory: /

**Describe the current behavior**
When listing file with `tf.io.gfile.glob` not all images are returned. It seems it is not resolving the folders recursively. 
When using the same path with gsutils  we get the correct image count.


**Describe the expected behavior**
When using the same gs:// path with **gsutils** we get the correct amount of images. 

**Code to reproduce the issue**
In order to reproduce the behavior I prepared a Google Bucket with the following structure. The bucket is public accessible, please feel free to use it to reproduce the behavior on your end: `gs://tensorflow-issue-reproduction`

![level0](https://user-images.githubusercontent.com/1991664/69040135-93375e80-09ed-11ea-927f-da445d2bae10.png)
![level1](https://user-images.githubusercontent.com/1991664/69040139-94688b80-09ed-11ea-9e19-28af47c3bcee.png)
![level2](https://user-images.githubusercontent.com/1991664/69040146-96cae580-09ed-11ea-8894-c69a82acbbaa.png)
![level3](https://user-images.githubusercontent.com/1991664/69040150-9894a900-09ed-11ea-94dc-49fb2d9b3b9b.png)

In summary we have 4 jpg images nested in different folder levels.

TensorFlow 2 code to reproduce
```
files = tf.io.gfile.glob('gs://tensorflow-issue-reproduction/**/*.jpg')
print('file count: ', len(files))
# found files 1
```

gsutil command which works properly
```
gsutil du gs://tensorflow-issue-reproduction/**/*.jpg | wc -l
# found files 4
```

**Other info / logs**
/

Best regards
Sascha
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops']",2019-11-18T09:34:42Z,16,0,https://github.com/tensorflow/tensorflow/issues/34371,Runtime Error
487,tensorflow/tensorflow,Cannot seek on write only tf.gfile.GFile,"**System information**

-  Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0
- Python version: 3.6

**Describe the current behavior**
Calling `seek()` on a `tf.gfile.GFile` opened in write only mode raises `tensorflow.python.framework.errors_impl.PermissionDeniedError`.

**Describe the expected behavior**
GFile should support the Python IO semantics that supports seeking on a write only file.

More generally it would be preferable if GFile followed the API of Python's [`io.IOBase`](https://docs.python.org/3/library/io.html#io.IOBase).


**Code to reproduce the issue**

```
import tensorflow as tf

with tf.io.gfile.GFile('test.txt', 'w') as f:
    f.seek(0)
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""<stdin>"", line 2, in <module>
  File ""/VENV/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/VENV/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 146, in seek
    self._preread_check()
  File ""/VENV/lib/python3.6/site-packages/tensorflow/python/lib/io/file_io.py"", line 82, in _preread_check
    ""File isn't open for reading"")
tensorflow.python.framework.errors_impl.PermissionDeniedError: File isn't open for reading
```","['stat:awaiting tensorflower', 'type:bug', 'comp:apis', 'TF 2.11']",2019-08-30T19:01:28Z,12,0,https://github.com/tensorflow/tensorflow/issues/32122,Runtime Error
488,tensorflow/tensorflow,Initial read on nonexistent tf.gfile.GFile in w+ mode crashes,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: v1.14.0-rc1-22-gaf24dc9 1.14.0
- Python version: 3.7

**Describe the current behavior**
Python raises `tensorflow.python.framework.errors_impl.NotFoundError` when doing a first read (no writes before it) on a nonexistent `tf.gfile.GFile` in `w+` mode.

**Describe the expected behavior**
Read on an empty `w+` file should return an empty string.
One problem with the current behaviour is that numpy.savez() crashes when writing to a GFile.

**Code to reproduce the issue**
```
import tensorflow as tf

with tf.io.gfile.GFile('test.txt', 'w+') as f:
    f.read()
```

**Other info / logs**
```
Traceback (most recent call last):
  File ""test_gfile.py"", line 5, in <module>
    f.read()
  File ""/VENV/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 122, in read
    self._preread_check()
  File ""/VENV/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py"", line 84, in _preread_check
    compat.as_bytes(self.__name), 1024 * 512)
tensorflow.python.framework.errors_impl.NotFoundError: test.txt; No such file or directory
```","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.11']",2019-08-29T18:56:58Z,18,0,https://github.com/tensorflow/tensorflow/issues/32090,Runtime Error
489,tensorflow/tensorflow,Potential bugs found with static analysis,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.14.0
- Python version: 2.7.12
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with: 1. TF 1.0: `python -c ""import
tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""` 2. TF 2.0: `python -c
""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Issue 1:**
https://github.com/tensorflow/tensorflow/blob/1ee51a3b868a3ccd5f80724f6b9389fd0a9aed07/tensorflow/compiler/tf2xla/functionalize_cond.cc#L445-L447
`dst_copy != nullptr` is checked immediately after `if (dst_copy == nullptr) continue;`. Is one of comparisons supposed to be different, or can the `TF_RET_CHECK` be removed?

**Issue 2:**
https://github.com/tensorflow/tensorflow/blob/9d67841e6c1f852516abf6ff44490b5d5a8331af/tensorflow/contrib/ignite/kernels/dataset/ignite_dataset_iterator.cc#L126-L128
This code is unreachable, because both branches of `if` above return. Either it should be deleted or some part of `if` modified.

**Issue 3:**
https://github.com/tensorflow/tensorflow/blob/514004a2347058214d1e7b13b9769a2abdd06830/tensorflow/core/profiler/rpc/client/capture_profile.cc#L218-L220
The condition is always true. My guess is that `==` is intended instead of `!=`.

**Issue 4:**
https://github.com/tensorflow/tensorflow/blob/9590c4c32dd4346ea5c35673336f5912c6072bf2/tensorflow/core/profiler/internal/tfprof_tensor.h#L55
Should this be `void` since nothing gets returned in any branch?

**Issue 5:**
https://github.com/tensorflow/tensorflow/blob/9380a41290e8fb8b9ea85f614472deab56dbc481/tensorflow/stream_executor/stream_executor_pimpl.cc#L112-L125
`ScopedTracer` doesn't obey the rule of 3, but this `SCOPED_TRACE` macro expands to calling its copy constructor. This should be safe in practice because of copy elision, I believe, but undesirable to rely on. Unfortunately, changing to `auto tracer{MakeScopedTracer(this, &LOC##Begin, &LOC##Complete, ##__VA_ARGS__)};` will infer `initializer_list<...>` (is that right?).

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
","['stat:awaiting tensorflower', 'type:bug', 'TF 1.13', 'comp:core']",2019-07-23T11:20:49Z,3,0,https://github.com/tensorflow/tensorflow/issues/30954,UI/UX Bug
490,tensorflow/tensorflow,Many context switches / Many threads even if threading is limited,"**System information**
- Have I written custom code: No (https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/quickstart/beginner.ipynb)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux OpenSuse Leap 15.0
- TensorFlow installed from (source or binary): pip package tensorflow==2.0.0-beta1
- TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213
- Python version: 3.7.3 
- GPU model and memory: CPU only

**Describe the current behavior**
If you limit the number of threads with tf.set_inter_op_parallelism_threads(NUM_THREADS) and tf.set_intra_op_parallelism_threads(NUM_THREADS) tensorflow creates a threadpool with more threads than NUM_THREADS and runs maximal NUM_THREADS causing high amount of context switches, which is delaying execution.

**Describe the expected behavior**
Create maximal NUM_THREADS and limit context switches.

**Code to reproduce the issue**
```python
from __future__ import absolute_import, division, print_function, unicode_literals
!pip install -q tensorflow==2.0.0-beta1
import tensorflow as tf
NUM_THREADS=2
tf.config.threading.set_inter_op_parallelism_threads(NUM_THREADS)
tf.config.threading.set_intra_op_parallelism_threads(NUM_THREADS)
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
model.fit(x_train, y_train, epochs=100)
```


","['stat:awaiting tensorflower', 'type:bug', 'comp:runtime', 'TF 2.5', '2.6.0']",2019-06-19T16:22:16Z,31,2,https://github.com/tensorflow/tensorflow/issues/29968,UI/UX Bug
491,tensorflow/tensorflow,Unexpected UnicodeDecodeError: invalid continuation byte when reading lines from a file,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.5.2
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
Unexpected and undocumented runtime exception/error when handling malformed data.

**Describe the expected behavior**
Expected a ""TypeError"" or an empty list as a result.

**Code to reproduce the issue**
```
import csv
import sys
import tensorflow as tf

input_file_name = sys.argv[1]

with tf.gfile.Open(input_file_name, ""r"") as f:
  reader = csv.reader(f, delimiter=""\t"", quotechar=None)
  for line in reader:
    print(line)
```
Run with the path to the attached file as a command line argument.

**Other info / logs**

Traceback (most recent call last):
  File ""tensorflow_bug.py"", line 9, in <module>
    for line in reader:
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 220, in \_\_next\_\_
    return self.next()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 214, in next
    retval = self.readline()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 184, in readline
    return self._prepare_value(self._read_buf.ReadLineAsString())
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/lib/io/file_io.py"", line 100, in _prepare_value
    return compat.as_str_any(val)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py"", line 107, in as_str_any
    return as_str(value)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/compat.py"", line 80, in as_text
    return bytes_or_text.decode(encoding)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0: invalid continuation byte

[corrupted_file1.zip](https://github.com/tensorflow/tensorflow/files/3047460/corrupted_file1.zip)
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.11']",2019-04-05T10:29:22Z,11,0,https://github.com/tensorflow/tensorflow/issues/27537,Runtime Error
492,tensorflow/tensorflow,Optimizing slice of variable not possible,"Applying the gradient of a variable slice currently results in a `NotImplemented` error of tf.train.Optimizer.

**The following two examples are working:**
```python
### WORKING ###
X = tf.Variable(2, dtype=tf.float32)
y = tf.constant(10, dtype=""float32"")
loss = y - (X*X)

variables=[X]
gradient = tf.gradients(loss, variables)
gradient = [(g, v) for g, v in zip(gradient, variables)]
train_op = tf.train.AdamOptimizer().apply_gradients(gradient)
```

```python
### WORKING ###
big_X = tf.Variable([2,3,4], dtype=tf.float32)
X = big_X[0]
y = tf.constant(10, dtype=""float32"")
loss = y - (X*X)

train_op = train_op = tf.train.AdamOptimizer().minimize(loss)
```

**The following example throws an error:**
```python
### NOT WORKING ###
big_X = tf.Variable([2,3,4], dtype=tf.float32)
X = big_X[0]
y = tf.constant(10, dtype=""float32"")
loss = y - (X*X)

variables=[X]
gradient = tf.gradients(loss, variables)
gradient = [(g, v) for g, v in zip(gradient, variables)]
train_op = tf.train.AdamOptimizer().apply_gradients(gradient)
```
The error:
```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2963, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-22-10282dee2005>"", line 10, in <module>
    train_op = tf.train.AdamOptimizer().apply_gradients(gradient)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py"", line 605, in apply_gradients
    update_ops.append(processor.update_op(self, grad))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py"", line 189, in update_op
    raise NotImplementedError(""Trying to update a Tensor "", self._v)
NotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'strided_slice_9:0' shape=() dtype=float32>)
```
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.10.1-0-g4dcfddc5d1 1.10.1
- **Python version**: 3.6.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.11']",2018-09-04T17:44:33Z,11,0,https://github.com/tensorflow/tensorflow/issues/22059,Runtime Error
493,tensorflow/tensorflow,`tf.dynamic_stitch` gradient is incorrect,"### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)

Original reproduction code (TensorFlow 1.0)
``` python
import tensorflow as tf

x = tf.zeros((1, 3))
y = tf.dynamic_stitch([[0], [0]], [x, tf.ones((1, 3))])

with tf.Session() as sess:
    print(""y"")
    print(sess.run(y))

    analytic, numeric = tf.test.compute_gradient(x, (1, 3), y, (1, 3))
    print(""analytic"")
    print(analytic)
    print(""numeric"")
    print(numeric)
```
Updated reproduction code (TensorFlow 2.16)
``` python
import tensorflow as tf

x = tf.zeros((1, 3))

analytic, numeric = tf.test.compute_gradient(
    lambda x: tf.dynamic_stitch([[0], [0]], [x, tf.ones((1, 3))]), [x]
)
print(""analytic"")
print(analytic)
print(""numeric"")
print(numeric)
```

gives output
```
y
[[ 1.  1.  1.]]
analytic
[[ 1.  0.  0.]
 [ 0.  1.  0.]
 [ 0.  0.  1.]]
numeric
[[ 0.  0.  0.]
 [ 0.  0.  0.]
 [ 0.  0.  0.]]
```

The numeric gradient correctly shows that `x` has no impact on `y` (since the value of `x` is completely overwritten by a constant in the `dynamic_stitch`).  The analytic gradient is incorrect; it seems like the gradient calculation in `dynamic_stitch` does not handle the case where there are duplicate indices being merged.
","['stat:awaiting tensorflower', 'type:bug', 'comp:ops', 'TF 2.9']",2017-02-09T19:09:54Z,15,0,https://github.com/tensorflow/tensorflow/issues/7397,Logical Bug
